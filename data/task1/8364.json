{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 56 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels\n\nPaper Abstract: Abstract The soft Dice loss (SDL) has taken a pivotal role in numerous automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct utilization in scenarios involving soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be employed in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calibration, which supports the wider adoption of DMLs in practice. The code is available at https://github.com/zifuwanggg/JDTLosses.\n\n{retrieved_text}\n\nGenerate exactly 56 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 56\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A Novel Focal Tversky Loss Function with Improved Attention U-Net for Lesion Segmentation\",\n        \"The Lovasz-softmax loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks\",\n        \"Theoretical analysis and experimental validation of volume bias of soft Dice optimized segmentation maps in the context of inherent uncertainty\",\n        \"The Liver Tumor Segmentation Benchmark (LiTS)\",\n        \"Encoder-decoder with Atrous separable convolution for semantic image segmentation\",\n        \"MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark\",\n        \"ImageNet: A Large-Scale Hierarchical Image Database\",\n        \"Encyclopedia of Distances\",\n        \"Local Temperature Scaling for Probability Calibration\",\n        \"Optimization for medical image segmentation: Theory and Practice When Evaluating With Dice Score or Jaccard Index\",\n        \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\",\n        \"Relaxed triangle inequality ratio of the Sørensen–Dice and Tversky indexes\",\n        \"SoftSeg: Advantages of soft versus binary training for image segmentation\",\n        \"Who Said What: Modeling Individual Labelers Improves Classification\",\n        \"On calibration of modern neural networks\",\n        \"Deep residual learning for image recognition\",\n        \"The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge\",\n        \"Distilling the knowledge in a neural network\",\n        \"Knowledge Distillation from A Stronger Teacher\",\n        \"Masked Distillation with Receptive Tokens\",\n        \"Segmentation models pytorch\",\n        \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation\",\n        \"Spatially Varying Label Smoothing: Capturing Uncertainty from Expert Annotations\",\n        \"Improving Uncertainty Estimation in Convolutional Neural Networks Using Inter-rater Agreement\",\n        \"Learning calibrated medical image segmentation via multi-rater agreement modeling\",\n        \"Segment Anything\",\n        \"Label fusion and training methods for reliable representation of inter-rater uncertainty\",\n        \"Dice Loss for Data-imbalanced NLP Tasks\",\n        \"Focal Loss for Dense Object Detection\",\n        \"Meta-Cal: Well-controlled Post-hoc Calibration by Ranking\",\n        \"Metrics Reloaded: Recommendations for image analysis validation\",\n        \"A Statistical Perspective on Distillation\",\n        \"Quantification of Uncertainties in Biomedical Image Quantification Challenge\",\n        \"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\",\n        \"When Does Label Smoothing Help?\",\n        \"Noisy Image Segmentation With Soft-Dice\",\n        \"Optimal Decisions from Probabilistic Models: the Intersection-over-Union Case\",\n        \"On the relationship between calibrated predictors and unbiased volume estimation\",\n        \"A Consistent and Differentiable Lp Canonical Calibration Error Estimator\",\n        \"Efficient Medical Image Segmentation Based on Knowledge Distillation\",\n        \"Optimizing intersection-over-union in deep neural networks for image segmentation\",\n        \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n        \"Post training uncertainty calibration of deep networks for medical image segmentation\",\n        \"Tversky loss function for image segmentation using 3D fully convolutional deep networks\",\n        \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\",\n        \"Using Soft Labels to Model Uncertainty in Medical Image Segmentation\",\n        \"Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations\",\n        \"Rethinking the Inception Architecture for Computer Vision\",\n        \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\",\n        \"The Dice loss in the context of missing or empty labels: Introducing ΦΦ\\\\Phi and ϵitalic-ϵ\\\\epsilon\",\n        \"The Nature of Statistical Learning Theory\",\n        \"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels\",\n        \"Pytorch Image Models\",\n        \"The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses\",\n        \"Learning generalized intersection over union for dense pixelwise prediction\",\n        \"Deep Learning for Medical Image Segmentation: Tricks, Challenges and Future Directions\"\n    ],\n    \"reasoning\": \"Selected these 56 references because they represent foundational work and recent advances in Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}