{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: GFlowOut: Dropout with Generative Flow Networks\n\nPaper Abstract: Abstract Bayesian inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way to approximate inference and estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent research shows that the dropout mask can be seen as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GFlowOut to address these issues. GFlowOut leverages the recently proposed probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks. We empirically demonstrate that GFlowOut results in predictive distributions that generalize better to out-of-distribution data and provide uncertainty estimates which lead to better performance in downstream tasks.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Adaptive dropout for training deep neural networks\",\n        \"Flow network based generative models for non-iterative diverse candidate generation\",\n        \"Gflownet foundations\",\n        \"Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty\",\n        \"Learnable Bernoulli dropout for Bayesian deep learning\",\n        \"Deep Gaussian processes\",\n        \"Bayesian deep learning via subnetwork inference\",\n        \"Bayesian structure learning with generative flow networks\",\n        \"Contextual dropout: An efficient sample-dependent dropout module\",\n        \"On the expressiveness of approximate inference in Bayesian neural networks\",\n        \"Deep ensembles: A loss landscape perspective\",\n        \"Dropout as a Bayesian approximation: Representing model uncertainty in deep learning\",\n        \"Concrete dropout\",\n        \"Dropblock: A regularization method for convolutional networks\",\n        \"On calibration of modern neural networks\",\n        \"Deep residual learning for image recognition\",\n        \"Benchmarking neural network robustness to common corruptions and perturbations\",\n        \"Improving neural networks by preventing co-adaptation of feature detectors\",\n        \"Biological sequence design with gflownets\",\n        \"DEUP: Direct epistemic uncertainty prediction\",\n        \"Variational dropout and the local reparameterization trick\",\n        \"Accurate uncertainties for deep learning using calibrated regression\",\n        \"Is MC dropout bayesian?\",\n        \"Meta dropout: Learning to perturb latent features for generalization\",\n        \"Bayesian model selection, the marginal likelihood, and generalization\",\n        \"A practical Bayesian framework for backpropagation networks\",\n        \"Learning GFlowNets from partial episodes for improved convergence and stability\",\n        \"Trajectory balance: Improved credit assignment in GFlowNets\",\n        \"GFlowNets and variational inference\",\n        \"Uncertainty Baselines: Benchmarks for uncertainty & robustness in deep learning\",\n        \"Bayesian learning for neural networks, volume 118\",\n        \"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images\",\n        \"Structured dropout variational inference for Bayesian neural networks\",\n        \"Evaluating generalization in gflownets for molecule design\",\n        \"(more) efficient reinforcement learning via posterior sampling\",\n        \"Can you trust your modelâ€™s uncertainty? evaluating predictive uncertainty under dataset shift\",\n        \"Generative augmented flow networks\",\n        \"Analysis on the dropout effect in convolutional neural networks\",\n        \"Autodropout: Learning dropout patterns to regularize deep networks\",\n        \"The eICU Collaborative Research Database, a freely available multi-center database for critical care research\",\n        \"Evidential deep learning to quantify classification uncertainty\",\n        \"Reinforcement learning: An introduction\",\n        \"Bayesian deep learning and a probabilistic perspective of generalization\",\n        \"Soft dropout and its variational bayes approximation\",\n        \"Rx equalization for a high-speed channel based on bayesian active learning using dropout\",\n        \"Deep modular co-attention networks for visual question answering\",\n        \"Unifying generative models with gflownets\",\n        \"Generative flow networks for discrete probabilistic modeling\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in GFlowOut: Dropout with Generative Flow Networks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}