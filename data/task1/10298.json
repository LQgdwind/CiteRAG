{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments\n\nPaper Abstract: Abstract Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based approach for a domain-incremental learning task: learning permutations in the MNIST task. We first compare our rehearsal environment with other baselines to show its efficacy in promoting continual learning. Additionally, we investigate whether this environment shows forward facilitation, i.e., faster learning of later tasks. Next, we explore the robustness of our learning environment to the number of tasks, model size, and amount of data rehearsed after each task. Notably, our results show that the performance is comparable or superior to that of models trained using popular regularization methods and also to rehearsals in non-power-law environments. The benefits of this training paradigm include simplicity, learning that is agnostic to both tasks and models and the lack of a need for extra neural circuitry. In addition, because our method is orthogonal to other methods, future research can explore combining training in power-law environments with other continual learning mechanisms.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A functional approach to external graph algorithms\",\n        \"A random graph model for massive graphs\",\n        \"Reflections of the Environment in Memory\",\n        \"Need probability affects retention: A direct demonstration\",\n        \"Blowing in the wind: Using ‘North Wind and the Sun’texts to sample phoneme inventories\",\n        \"PopSignAI : Using Sign Language Recognition to Improve American Sign Language Learning in Novice Signers\",\n        \"Analyzing the spread of tweets in response to Paris attacks\",\n        \"Domain Incremental Lifelong Learning in an Open World\",\n        \"Sequential mastery of multiple visual tasks: Networks naturally learn to learn and forget to forget\",\n        \"The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]\",\n        \"An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\",\n        \"REMIND Your Neural Network to Prevent Catastrophic Forgetting\",\n        \"Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis\",\n        \"Radial structure of the Internet\",\n        \"Toward a protein–protein interaction map of the budding yeast: a comprehensive system to examine two-hybrid interactions in all possible combinations between the yeast proteins\",\n        \"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\",\n        \"FearNet: Brain-Inspired Model for Incremental Learning\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Overcoming catastrophic forgetting by incremental moment matching\",\n        \"Learning without forgetting\",\n        \"Gradient Episodic Memory for Continuum Learning\",\n        \"On the frequency distribution of retweets\",\n        \"Catastrophic interference in neural network models is mitigated when the training data reflect a power-law environmental structure\",\n        \"Packnet: Adding multiple tasks to a single network by iterative pruning\",\n        \"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem\",\n        \"Power-law scaling to assist with key challenges in artificial intelligence\",\n        \"Power law distributions in information science: Making the case for logarithmic binning\",\n        \"Never-Ending Learning\",\n        \"Mapping the NFT revolution: market trends, trade networks, and visual features\",\n        \"Mechanisms of skill acquisition and the law of practice\",\n        \"Power laws, Pareto distributions and Zipf’s law\",\n        \"Continual lifelong learning with neural networks: A review\",\n        \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n        \"iCaRL: Incremental Classifier and Representation Learning\",\n        \"Catastrophic forgetting, rehearsal and pseudorehearsal\",\n        \"Experience Replay for Continual Learning\",\n        \"Progressive Neural Networks\",\n        \"The Role of Process in the Rational Analysis of Memory\",\n        \"Progress & Compress: A scalable framework for continual learning\",\n        \"Overcoming catastrophic forgetting with hard attention to the task\",\n        \"Continual Learning with Deep Generative Replay\",\n        \"Reflections of the social environment in chimpanzee memory: applying rational analysis beyond humans\",\n        \"Discovering Structure in Multiple Learning Tasks: The TC Algorithm\",\n        \"Brain-inspired replay for continual learning with artificial neural networks\",\n        \"Three scenarios for continual learning\",\n        \"Three types of incremental learning\",\n        \"A Comprehensive Survey of Continual Learning: Theory, Method and Application\",\n        \"A Prompt Log Analysis of Text-to-Image Generation Systems\",\n        \"An Online Incremental Learning Vector Quantization\",\n        \"Continual learning through synaptic intelligence\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}