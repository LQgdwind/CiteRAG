{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 52 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Convergence of Policy Gradient for Entropy Regularized MDPs with Neural Network Approximation in the Mean-Field Regime\n\nPaper Abstract: Abstract We study the global convergence of policy gradient for infinite-horizon, continuous state and action space, and entropy-regularized Markov decision processes (MDPs). We consider a softmax policy with (one-hidden layer) neural network approximation in a mean-field regime. Additional entropic regularization in the associated mean-field probability measure is added, and the corresponding gradient flow is studied in the 2-Wasserstein metric. We show that the objective function is increasing along the gradient flow. Further, we prove that if the regularization in terms of the mean-field measure is sufficient, the gradient flow converges exponentially fast to the unique stationary solution, which is the unique maximizer of the regularized MDP objective. Lastly, we study the sensitivity of the value function along the gradient flow with respect to regularization parameters and the initial condition. Our results rely on the careful analysis of the non-linear Fokker–Planck–Kolmogorov equation and extend the pioneering work of (Mei et al., 2020) and (Agarwal et al., 2020), which quantify the global convergence rate of policy gradient for entropy-regularized MDPs in the tabular setting.\n\n{retrieved_text}\n\nGenerate exactly 52 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 52\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Optimality and approximation with policy gradient methods in Markov decision processes\",\n        \"Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime\",\n        \"Gradient flows: in metric spaces and in the space of probability measures\",\n        \"Stochastic optimal control: the discrete-time case\",\n        \"Fokker–Planck–Kolmogorov Equations, volume 207\",\n        \"Distances between transition probabilities of diffusions and applications to nonlinear Fokker–Planck–Kolmogorov equations\",\n        \"Distances between stationary distributions of diffusions and solvability of nonlinear Fokker–Planck–Kolmogorov equations\",\n        \"Convergence in variation of solutions of nonlinear Fokker–Planck–Kolmogorov equations to stationary measures\",\n        \"On ergodic properties of nonlinear Markov chains and stochastic McKean-Vlasov equations\",\n        \"Probabilistic Theory of Mean Field Games with Applications I-II\",\n        \"On the global convergence of gradient descent for over-parameterized models using optimal transport\",\n        \"Uniform in time weak propagation of chaos on the Torus\",\n        \"A weak convergence approach to the theory of large deviations\",\n        \"Taming the noise in reinforcement learning via soft updates\",\n        \"A certain class of diffusion processes associated with nonlinear parabolic equations\",\n        \"A theory of regularized Markov decision processes\",\n        \"Reinforcement learning with deep energy-based policies\",\n        \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\",\n        \"McKean-Vlasov SDEs under measure dependent Lyapunov conditions\",\n        \"Discrete-time Markov control processes: basic optimality criteria, volume 30\",\n        \"Mean-field Langevin system, optimal control and deep neural networks\",\n        \"Mean-field langevin dynamics and energy landscape of neural networks\",\n        \"Mean-field neural ODEs via relaxed optimal control\",\n        \"The variational formulation of the Fokker–Planck equation\",\n        \"An introduction to variational inequalities and their applications\",\n        \"Central limit theorem for Markov processes with spectral gap in the Wasserstein metric\",\n        \"Controlled diffusion processes, volume 14\",\n        \"A Pettis-type integral and applications to transition semigroups\",\n        \"Chaos, fractals, and noise: stochastic aspects of dynamics, volume 97\",\n        \"Softmax policy gradient methods can take exponential time to converge\",\n        \"Neural trust region/proximal policy optimization attains globally optimal policy\",\n        \"Nonlinear parabolic equations for measures\",\n        \"On uniqueness of solutions to nonlinear Fokker–Planck–Kolmogorov equations\",\n        \"On the global convergence rates of softmax policy gradient methods\",\n        \"A mean field view of the landscape of two-layer neural networks\",\n        \"Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit\",\n        \"Human-level control through deep reinforcement learning\",\n        \"A unified view of entropy-regularized Markov decision processes\",\n        \"Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality\",\n        \"Markov decision processes: discrete stochastic dynamic programming\",\n        \"Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error\",\n        \"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\",\n        \"Asymptotics of reinforcement learning with neural networks\",\n        \"Mean field analysis of deep neural networks\",\n        \"Gradient flows for regularized stochastic control problems\",\n        \"Reinforcement Learning\",\n        \"Leverage the average: an analysis of KL regularization in reinforcement learning\",\n        \"Optimal transport: old and new, volume 338\",\n        \"Grandmaster level in Starcraft II using multi-agent reinforcement learning\",\n        \"Neural policy gradient methods: Global optimality and rates of convergence\",\n        \"A proposal on machine learning via dynamical systems\",\n        \"Modeling interaction via the principle of maximum causal entropy\"\n    ],\n    \"reasoning\": \"Selected these 52 references because they represent foundational work and recent advances in Convergence of Policy Gradient for Entropy Regularized MDPs with Neural Network Approximation in the Mean-Field Regime, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}