{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sparsifying Suprema of Gaussian Processes\n\nPaper Abstract: Abstract We give a dimension-independent sparsification result for suprema of centered Gaussian processes: Let T𝑇T be any (possibly infinite) bounded set of vectors in ℝnsuperscriptℝ𝑛\\mathbb{R}^{n}, and let {𝑿t}t∈Tsubscriptsubscript𝑿𝑡𝑡𝑇\\{\\boldsymbol{X}_{t}\\}_{t\\in T} be the canonical Gaussian process on T𝑇T. We show that there is an Oε​(1)subscript𝑂𝜀1O_{\\varepsilon}(1)-size subset S⊆T𝑆𝑇S\\subseteq T and a set of real values {cs}s∈Ssubscriptsubscript𝑐𝑠𝑠𝑆\\{c_{s}\\}_{s\\in S} such that sups∈S{𝑿s+cs}subscriptsupremum𝑠𝑆subscript𝑿𝑠subscript𝑐𝑠\\sup_{s\\in S}\\{{\\boldsymbol{X}}_{s}+c_{s}\\} is an ε𝜀\\varepsilon-approximator of supt∈T𝑿tsubscriptsupremum𝑡𝑇subscript𝑿𝑡\\sup_{t\\in T}{\\boldsymbol{X}}_{t}. Notably, the size of S𝑆S is completely independent of both the size of T𝑇T and of the ambient dimension n𝑛n. We use this to show that every norm is essentially a junta when viewed as a function over Gaussian space: Given any norm ν​(x)𝜈𝑥\\nu(x) on ℝnsuperscriptℝ𝑛\\mathbb{R}^{n}, there is another norm ψ​(x)𝜓𝑥\\psi(x) which depends only on the projection of x𝑥x along Oε​(1)subscript𝑂𝜀1O_{\\varepsilon}(1) directions, for which ψ​(𝒈)𝜓𝒈\\psi({\\boldsymbol{g}}) is a multiplicative (1±ε)plus-or-minus1𝜀(1\\pm\\varepsilon)-approximation of ν​(𝒈)𝜈𝒈\\nu({\\boldsymbol{g}}) with probability 1−ε1𝜀1-\\varepsilon for 𝒈∼N​(0,In)similar-to𝒈𝑁0subscript𝐼𝑛{\\boldsymbol{g}}\\sim N(0,I_{n}). We also use our sparsification result for suprema of centered Gaussian processes to give a sparsification lemma for convex sets of bounded geometric width: Any intersection of (possibly infinitely many) halfspaces in ℝnsuperscriptℝ𝑛\\mathbb{R}^{n} that are at distance O​(1)𝑂1O(1) from the origin is ε𝜀\\varepsilon-close, under N​(0,In)𝑁0subscript𝐼𝑛N(0,I_{n}), to an intersection of only Oε​(1)subscript𝑂𝜀1O_{\\varepsilon}(1) many halfspaces. We describe applications to agnostic learning and tolerant property testing.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Asymptotic Geometric Analysis, Part I, volume 202 of Mathematical Surveys and Monographs\",\n        \"Asymptotic geometric analysis, Part II, volume 261\",\n        \"Majorizing Measures for the Optimizer\",\n        \"New notions and constructions of sparsification for graphs and hypergraphs\",\n        \"Anti-concentration and honest, adaptive confidence bands\",\n        \"Comparison and anti-concentration bounds for maxima of Gaussian random vectors\",\n        \"Lower Bounds for Convexity Testing\",\n        \"Superconcentration and related topics, volume 15 of Springer Monographs in Mathematics\",\n        \"Agnostically Learning Multi-index Models with Queries\",\n        \"Cover times, blanket times, and majorizing measures\",\n        \"Robust testing of low dimensional functions\",\n        \"Order statistics\",\n        \"Quantitative correlation inequalities via semigroup interpolation\",\n        \"Convex Influences\",\n        \"Gaussian Approximation of Convex Sets by Intersections of Halfspaces\",\n        \"The sizes of compact subsets of Hilbert space and continuity of Gaussian processes\",\n        \"Regularite des trajectoires des fonctions aleatoires gaussiennes\",\n        \"Representation, approximation and learning of submodular functions using low-rank decision trees\",\n        \"Optimal bounds on approximation of submodular and XOS functions by juntas\",\n        \"DNF sparsification and a faster deterministic counting algorithm\",\n        \"On Milman’s inequality and random subspaces which escape through a mesh in ℝn\",\n        \"Majorization of Gaussian processes and geometric applications\",\n        \"Learning geometric concepts via Gaussian surface area\",\n        \"Gaussian measures of dilatations of convex symmetric sets\",\n        \"Submodular functions and convexity\",\n        \"From DNF compression to sunflower theorems via regularity\",\n        \"Probability in Banach Spaces\",\n        \"On deterministic approximation of DNF\",\n        \"Decision list compression by mild random restrictions\",\n        \"A Polynomial Time Approximation Scheme for Computing the Supremum of Gaussian Processes\",\n        \"A new proof of A. Dvoretzky’s theorem on cross-sections of convex bodies\",\n        \"Chaining introduction with some computer science applications\",\n        \"New null space results and recovery thresholds for matrix rank minimization\",\n        \"Isometric sketching of any set via the restricted isometry property\",\n        \"Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach\",\n        \"Answer to Expectation of the maximum of gaussian random variables, Mathematics StackExchange, December 2011\",\n        \"Various thresholds for ℓ1-optimization in compressed sensing\",\n        \"Regularly random duality\",\n        \"Gauss and Cauchy measures and ε-entropy\",\n        \"Regularity of Gaussian processes\",\n        \"Sections of smooth convex bodies via majorizing measures\",\n        \"Upper and lower bounds for stochastic processes: decomposition theorems, volume 60\",\n        \"Asymptotic Convex Geometry Lecture Notes, 2018\",\n        \"A note on approximate counting for k-DNF\",\n        \"High-Dimensional Probability: An Introduction with Applications in Data Science, volume 47\",\n        \"Probability in high dimension\",\n        \"Chaining, interpolation, and convexity\",\n        \"Structure from local optima: Learning subspace juntas via higher order PCA\",\n        \"High-dimensional statistics: A non-asymptotic viewpoint, volume 48\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Sparsifying Suprema of Gaussian Processes, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}