{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 56 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Content and Context Features for Scene Image Representation\n\nPaper Abstract: Abstract Existing research in scene image classification has focused on either content features (e.g., visual information) or context features (e.g., annotations). As they capture different information about images which can be complementary and useful to discriminate images of different classes, we believe the fusion of them will improve classification results. In this paper, we propose new techniques to compute content features and context features, and then fuse them together. For content features, we design multi-scale deep features based on background and foreground information in images. For context features, we use annotations of similar images available in the web to design a codebook (filter words). Our experiments on three widely used benchmark scene datasets using Support Vector Machine (SVM) classifier reveal that our proposed context and content features produce better results than existing context and content features, respectively. The fusion of the proposed two types of features significantly outperform numerous state-of-the-art features.\n\n{retrieved_text}\n\nGenerate exactly 56 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 56\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Coordinate cnns and lstms to categorize scene images with multi-views and multi-levels of abstraction\",\n        \"Fast algorithms for sorting and searching strings\",\n        \"Enriching word vectors with subword information\",\n        \"Keras\",\n        \"Histograms of oriented gradients for human detection\",\n        \"ImageNet: a large-scale hierarchical image database\",\n        \"A bayesian hierarchical model for learning natural scene categories\",\n        \"Iot security based on iris verification using multi-algorithm feature level fusion scheme\",\n        \"Multi-scale orderless pooling of deep convolutional activation features\",\n        \"Bag of surrogate parts: one inherent feature of deep cnns\",\n        \"Bag of surrogate parts feature for visual recognition\",\n        \"Deep residual learning for image recognition\",\n        \"Caffe: Convolutional architecture for fast feature embedding\",\n        \"Saliency detection based on integrated features\",\n        \"Blocks that shout: Distinctive parts for scene classification\",\n        \"Keras-vgg16-places365\",\n        \"Convolutional neural networks for sentence classification\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"When naive bayes nearest neighbors meet convolutional neural networks\",\n        \"Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories\",\n        \"What, where and who? classifying events by scene and object recognition\",\n        \"Object bank: A high-level image representation for scene classification & semantic feature sparsification\",\n        \"Learning important spatial pooling regions for scene classification\",\n        \"Otc: A novel local descriptor for scene classification\",\n        \"Efficient estimation of word representations in vector space\",\n        \"Wordnet: a lexical database for english\",\n        \"Gist of the scene\",\n        \"Modeling the shape of the scene: a holistic representation of the spatial envelope\",\n        \"Reconfigurable models for scene recognition\",\n        \"Magnitude: A fast, efficient universal vector embedding utility package\",\n        \"Linear discriminant multi-set canonical correlations analysis (ldmcca): an efficient approach for feature fusion of finger biometrics\",\n        \"Finger-vein verification using gabor filter and sift feature matching\",\n        \"Glove: Global vectors for word representation\",\n        \"Improving the fisher kernel for large-scale image classification\",\n        \"Recognizing indoor scenes\",\n        \"Local features are not lonelyâ€“laplacian sparse coding for image classification\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Fusion of whole and part features for the classification of histopathological image of breast tissue\",\n        \"Attention-based vgg-16 model for covid-19 chest x-ray image classification\",\n        \"Unsupervised deep features for privacy image classification\",\n        \"Tag-based semantic features for scene image classification\",\n        \"Hdf: Hybrid deep features for scene image representation\",\n        \"Indoor image representation by high-level semantic features\",\n        \"Going deeper with convolutions\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"G-MS2F: GoogLeNet based multi-stage feature fusion of deep CNN for scene recognition\",\n        \"Learning semantic text features for web text aided image classification\",\n        \"Task-generic semantic convolutional neural network for web text-aided image classification\",\n        \"CENTRIST: A visual descriptor for scene categorization\",\n        \"mCENTRIST: a multi-channel feature generation mechanism for scene categorization\",\n        \"Feature fusion: parallel strategy vs. serial strategy\",\n        \"Sift descriptors modeling and application in texture image classification\",\n        \"Image classification by search with explicitly and implicitly semantic representations\",\n        \"Places: An image database for deep scene understanding\",\n        \"Places: A 10 million image database for scene recognition\",\n        \"Large margin learning of upstream scene understanding models\"\n    ],\n    \"reasoning\": \"Selected these 56 references because they represent foundational work and recent advances in Content and Context Features for Scene Image Representation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}