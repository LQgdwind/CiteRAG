{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Are We Using Autoencoders in a Wrong Way?\n\nPaper Abstract: Abstract Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Multilayer feedforward networks are universal approximators\",\n        \"When and why are deep networks better than shallow ones?\",\n        \"Unsupervised learning of hierarchical representations with convolutional deep belief networks\",\n        \"Reducing the dimensionality of data with neural networks\",\n        \"Transport analysis of infinitely deep neural network\",\n        \"Autoencoders\",\n        \"A better autoencoder for image: Convolutional autoencoder\",\n        \"Nonlinear principal component analysis using autoassociative neural networks\",\n        \"Extracting and composing robust features with denoising autoencoders\",\n        \"Research on denoising sparse autoencoder\",\n        \"K-sparse autoencoders\",\n        \"Contractive auto-encoders: Explicit invariance during feature extraction\",\n        \"Auto-association by multilayer perceptrons and singular value decomposition\",\n        \"Deep autoencoder neural networks for gene ontology annotation predictions\",\n        \"From principal subspaces to principal components with linear autoencoders\",\n        \"Exploring the latent space of autoencoders with interventional assays\",\n        \"Auto-encoding variational bayes\",\n        \"Understanding disentangling in Œ≤ùõΩ\\\\beta-VAE\",\n        \"beta-VAE: Learning basic visual concepts with a constrained variational framework\",\n        \"Categorical Reparameterization with Gumbel-Softmax\",\n        \"Learning structured output representation using deep conditional generative models\",\n        \"Isolating Sources of Disentanglement in Variational Autoencoders\",\n        \"Nonlinear Dimensionality Reduction\",\n        \"A global geometric framework for nonlinear dimensionality reduction\",\n        \"Nonlinear dimensionality reduction by locally linear embedding\",\n        \"Topological Autoencoders\",\n        \"Chart Auto-Encoders for Manifold Structured Data\",\n        \"Extendable and invertible manifold learning with geometry regularized autoencoders\",\n        \"The effect of manifold entanglement and intrinsic dimensionality on learning\",\n        \"Learning low-dimensional representations of shape data sets with diffeomorphic autoencoders\",\n        \"Siamese neural networks: An overview\",\n        \"Reducing data complexity using autoencoders with class-informed loss functions\",\n        \"Ci-vae: a class-informed deep variational autoencoder for enhanced class-specific data interpolation\",\n        \"Supervised autoencoders: Improving generalization performance with unsupervised regularizers\",\n        \"Deep Learning.\",\n        \"A discussion on the validity of manifold learning\",\n        \"Representation learning: A review and new perspectives\",\n        \"Pytorch: An imperative style, high-performance deep learning library.\",\n        \"Pytorch lightning.\",\n        \"Scikit-learn: Machine learning in Python.\",\n        \"The mnist database of handwritten digit images for machine learning research.\",\n        \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\",\n        \"Cifar-10 (canadian institute for advanced research)\",\n        \"Caltech 101.\",\n        \"Breast Cancer Wisconsin (Diagnostic).\",\n        \"beta-vae: Learning basic visual concepts with a constrained variational framework.\",\n        \"A survey of inductive biases for factorial representation-learning.\",\n        \"Challenging common assumptions in the unsupervised learning of disentangled representations.\",\n        \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\",\n        \"Enhancing variational autoencoders with mutual information neural estimation for text generation.\",\n        \"The mutual autoencoder: Controlling information in latent code representations\",\n        \"Learning joint latent representations based on information maximization.\",\n        \"On disentanglement and mutual information in semi-supervised variational auto-encoders.\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in Are We Using Autoencoders in a Wrong Way?, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}