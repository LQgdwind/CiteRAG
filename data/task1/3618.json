{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Evaluating Various Tokenizers for Arabic Text Classification\n\nPaper Abstract: Abstract The first step in any NLP pipeline is to split the text into individual tokens. The most obvious and straightforward approach is to use words as tokens. However, given a large text corpus, representing all the words is not efficient in terms of vocabulary size. In the literature, many tokenization algorithms have emerged to tackle this problem by creating subwords which in turn limits the vocabulary size in a given text corpus. Most tokenization techniques are language-agnostic i.e they don’t incorporate the linguistic features of a given language. Not to mention the difficulty of evaluating such techniques in practice. In this paper, we introduce three new tokenization algorithms for Arabic and compare them to three other baselines using unsupervised evaluations. In addition to that, we compare all the six algorithms by evaluating them on three supervised classification tasks which are sentiment analysis, news classification and poetry classification using six publicly available datasets. Our experiments show that none of the tokenization technique is the best choice overall and that the performance of a given tokenization algorithm depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset. However, some tokenization techniques are better overall as compared to others on various text classification tasks.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Classifying and diacritizing arabic poems using deep recurrent neural networks\",\n        \"Farasa: A fast and furious segmenter for arabic\",\n        \"Pre-training bert on arabic tweets: Practical considerations\",\n        \"Arbert & marbert: deep bidirectional transformers for arabic\",\n        \"A comparative study of effective approaches for arabic sentiment analysis\",\n        \"A comprehensive survey of arabic sentiment analysis\",\n        \"Arabic online handwriting recognition (aohr) a survey\",\n        \"Character-level language modeling with deeper self-attention\",\n        \"Meter classification of arabic poems using deep bidirectional recurrent neural networks\",\n        \"Metrec: A dataset for meter classification of arabic poetry\",\n        \"On the importance of tokenization in arabic embedding models\",\n        \"Arabic tweets sentimental analysis using machine learning\",\n        \"Labr: A large scale arabic book reviews dataset\",\n        \"Arabert: Transformer-based model for arabic language understanding\",\n        \"A comparative study between methods of arabic baseline detection\",\n        \"A survey of opinion mining in arabic: a comprehensive system perspective covering challenges and advances in tools, resources, models, applications, and visualizations\",\n        \"Dataset for arabic classification\",\n        \"Enriching word vectors with subword information\",\n        \"Byte pair encoding is suboptimal for language model pretraining\",\n        \"Variable-length word encodings for neural translation models\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"1.5 billion words arabic corpus\",\n        \"hulmona: The universal language model in arabic\",\n        \"Arabic sentiment analysis: studies, resources, and tools\",\n        \"Universal language model fine-tuning for text classification\",\n        \"Arabic optical characters recognition by neural network based arabic unicode\",\n        \"Subword regularization: Improving neural network translation models with multiple subword candidates\",\n        \"Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing\",\n        \"Orthographic syllable as basic unit for smt between related languages\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Efficient estimation of word representations in vector space\",\n        \"The impact of preprocessing on arabic-english statistical and neural machine translation\",\n        \"Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic\",\n        \"Glove: Global vectors for word representation\",\n        \"Language models are unsupervised multitask learners\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Challenging language-dependent segmentation for arabic: An application to machine translation and part-of-speech tagging\",\n        \"Japanese and korean voice search\",\n        \"Neural machine translation of rare words with subword units\",\n        \"Bpe and charcnns for translation of morphology: A cross-lingual comparison and analysis\",\n        \"Morphological word embeddings for arabic neural machine translation in low-resource settings\",\n        \"Morfessor 2.0: Toolkit for statistical morphological segmentation\",\n        \"Charformer: Fast character transformers via gradient-based subword tokenization\",\n        \"Multi-view subword regularization\",\n        \"Google’s neural machine translation system: Bridging the gap between human and machine translation\",\n        \"Byt5: Towards a token-free future with pre-trained byte-to-byte models\",\n        \"Learning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in Evaluating Various Tokenizers for Arabic Text Classification, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}