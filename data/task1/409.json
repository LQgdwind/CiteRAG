{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from ùí™‚Å¢(ùëõ‚Å∂) down to ùí™‚Å¢(ùëõ¬≥)\n\nPaper Abstract: Abstract We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from ùí™‚Äã(n6)ùí™superscriptùëõ6\\mathcal{O}(n^{6}) down to ùí™‚Äã(n3)ùí™superscriptùëõ3\\mathcal{O}(n^{3}). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger and DiscPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Supertagging: An approach to almost parsing\",\n        \"Proposal for a natural language processing syntactic backbone\",\n        \"Range concatenation grammars\",\n        \"The tiger treebank\",\n        \"Discontinuous constituents in trees, rules, and parsing\",\n        \"Unavoidable ill-nestedness in natural language and the adequacy of tree local-MCTAG induced dependency structures\",\n        \"Discontinuous constituency parsing with a stack-free transition system and a dynamic oracle\",\n        \"Incremental discontinuous phrase structure parsing with the GAP transition\",\n        \"Unlexicalized transition-based discontinuous constituency parsing\",\n        \"Programming Languages and Their Compilers: Preliminary Notes\",\n        \"Dependency parsing with bounded block degree and well-nestedness via Lagrangian relaxation and branch-and-bound\",\n        \"Efficient discontinuous phrase-structure parsing via the generalized maximum spanning arborescence\",\n        \"Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles\",\n        \"Deep biaffine attention for neural dependency parsing\",\n        \"Probabilistic parsing for german using sister-head dependencies\",\n        \"Efficient parsing for bilexical context-free grammars and head automaton grammars\",\n        \"A faster parsing algorithm for lexicalized tree-adjoining grammars\",\n        \"Bilexical grammars and a cubic-time probabilistic parser\",\n        \"PLCFRS parsing of English discontinuous constituents\",\n        \"Parsing as reduction\",\n        \"Discontinuous constituent parsing with pointer networks\",\n        \"Generic refinement of expressive grammar formalisms with an application to discontinuous constituent parsing\",\n        \"Parsing mildly non-projective dependency structures\",\n        \"Dependency parsing schemata and mildly non-projective dependency parsing\",\n        \"Semiring parsing\",\n        \"Less grammar, more features\",\n        \"Data-driven parsing with probabilistic linear context-free rewriting systems\",\n        \"An efficient recognition and syntax-analysis algorithm for context-free languages\",\n        \"Simple and accurate dependency parsing using bidirectional LSTM feature representations\",\n        \"Constituency parsing with a self-attentive encoder\",\n        \"Multilingual constituency parsing with self-attention and pre-training\",\n        \"Mildly non-projective dependency structures\",\n        \"Discontinuity and non-projectivity: Using mildly context-sensitive formalisms for data-driven parsing\",\n        \"Discontinuous parsing with continuous trees\",\n        \"PLCFRS parsing revisited: Restricting the fan-out to two\",\n        \"Discontinuous incremental shift-reduce parsing\",\n        \"Building a large annotated corpus of english: The penn treebank\",\n        \"Parentheticals and discontinuous constituent structure\",\n        \"Non-projective dependency parsing using spanning tree algorithms\",\n        \"Continuous or discontinuous constituents? a comparison between syntactic analyses for constituent order and their processing systems\",\n        \"On the generalized minimum spanning tree problem\",\n        \"Non-projective dependency parsing in expected linear time\",\n        \"Parsing as deduction\",\n        \"Dynamic programming for higher order parsing of gap-minding trees\",\n        \"The simple truth about dependency and phrase structure representations: An opinion piece\",\n        \"Efficient parsing for head-split dependency trees\",\n        \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\",\n        \"Introducing the spmrl 2014 shared task on parsing morphologically-rich languages\",\n        \"On multiple context-free grammars\",\n        \"An annotation scheme for free word order languages\",\n        \"Neural discontinuous constituency parsing\",\n        \"A minimal span-based neural constituency parser\",\n        \"Learning structured prediction models: A large margin approach\",\n        \"Experiments with easy-first nonprojective constituent parsing\",\n        \"Discontinuity (re)2-visited: A minimalist approach to pseudoprojective constituent parsing\",\n        \"Characterizing structural descriptions produced by various grammatical formalisms\",\n        \"Recognition and parsing of context-free languages in time n3\",\n        \"Dependency parsing as head selection\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from ùí™‚Å¢(ùëõ‚Å∂) down to ùí™‚Å¢(ùëõ¬≥), providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}