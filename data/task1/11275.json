{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Uncertainty Quantification for Neurosymbolic Programs via Compositional Conformal Prediction\n\nPaper Abstract: Abstract. Machine learning has become an effective tool for automatically annotating unstructured data (e.g., images) with structured labels (e.g., object detections). As a result, a new programming paradigm called neurosymbolic programming has emerged where users write queries against these predicted annotations. However, due to the intrinsic fallibility of machine learning models, these programs currently lack any notion of correctness. In many domains, users may want some kind of conservative guarantee that the results of their queries contain all possibly relevant instances. Conformal prediction has emerged as a promising strategy for quantifying uncertainty in machine learning by modifying models to predict sets of labels instead of individual labels. A key feature is that it provides a probabilistic guarantee that the prediction set contains the true label with high probability (called coverage). We propose a novel framework for adapting conformal prediction to neurosymbolic programs; our strategy is to represent prediction sets as abstract values in some abstract domain, and then to use abstract interpretation to propagate prediction sets through the program. Our strategy satisfies three key desiderata: (i) correctness (i.e., the program outputs a prediction set that contains the true output with high probability), (ii) compositionality (i.e., we can quantify uncertainty separately for different modules and then compose them together), and (iii) structured values (i.e., we can provide uncertainty quantification for structured values such as lists). When the full program is available ahead-of-time, we propose an optimization to our approach that incorporates conformal prediction at intermediate program points to reduce imprecision in abstract interpretation. We evaluate our approach on programs that take MNIST and MS-COCO images as input, demonstrating that it produces reasonably sized prediction sets while satisfying our coverage guarantee.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Fairsquare: probabilistic verification of program fairness\",\n        \"Neurosymbolic reinforcement learning with formally verified exploration\",\n        \"Conformal prediction: A gentle introduction\",\n        \"Uncertainty Sets for Image Classifiers using Conformal Prediction\",\n        \"Conformal prediction for reliable machine learning: theory, adaptations and applications\",\n        \"Conformal prediction beyond exchangeability\",\n        \"ImageEye: Batch Image Processing Using Program Synthesis\",\n        \"SkyQuery: an aerial drone video sensing platform\",\n        \"Practical adversarial multivalid conformal prediction\",\n        \"Measuring neural net robustness with constraints\",\n        \"Verifiable reinforcement learning via policy extraction\",\n        \"Probabilistic verification of fairness properties via concentration\",\n        \"Distribution-free, risk-controlling prediction sets\",\n        \"Neurosymbolic programming\",\n        \"Data Extraction via Semantic Regular Expression Synthesis\",\n        \"Web question answering with neurosymbolic program synthesis\",\n        \"Hyperproperties\",\n        \"Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints\",\n        \"The well-calibrated Bayesian\",\n        \"Fairness through awareness\",\n        \"Few-shot conformal prediction with auxiliary tasks\",\n        \"Rekall: Specifying video events using compositions of spatiotemporal labels\",\n        \"Terpret: A probabilistic programming language for program induction\",\n        \"Ai2: Safety and robustness certification of neural networks with abstract interpretation\",\n        \"Conformal inference for online prediction with arbitrary distribution shifts\",\n        \"Explaining and harnessing adversarial examples\",\n        \"On calibration of modern neural networks\",\n        \"Distribution-free binary classification: prediction sets, confidence intervals and calibration\",\n        \"Equality of opportunity in supervised learning\",\n        \"Safety verification of deep neural networks\",\n        \"Neurosymbolic transformers for multi-agent communication\",\n        \"Compositional learning and verification of neural network controllers\",\n        \"Verisig: verifying safety properties of hybrid systems with neural network controllers\",\n        \"Reluplex: An efficient SMT solver for verifying deep neural networks\",\n        \"Inherent trade-offs in algorithmic fairness\",\n        \"Semantics of probabilistic programs\",\n        \"Calibrated structured prediction\",\n        \"Gradient-based learning applied to document recognition\",\n        \"Microsoft coco: Common objects in context\",\n        \"When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making\",\n        \"Synthesizing Video Trajectory Queries\",\n        \"Various techniques used in connection with random digits\",\n        \"PAC confidence sets for deep neural networks via calibrated prediction\",\n        \"PAC Prediction Sets Under Covariate Shift\",\n        \"Pac prediction sets for meta-learning\",\n        \"PAC confidence predictions for deep neural network classifiers\",\n        \"A Tutorial on Conformal Prediction\",\n        \"Learning differentiable programs with admissible neural heuristics\",\n        \"Intriguing properties of neural networks\",\n        \"Conformal prediction under covariate shift\",\n        \"Human–computer collaboration for skin cancer recognition\",\n        \"Houdini: Lifelong learning as program synthesis\",\n        \"Conditional validity of inductive conformal predictors\",\n        \"Algorithmic learning in a random world\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Uncertainty Quantification for Neurosymbolic Programs via Compositional Conformal Prediction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}