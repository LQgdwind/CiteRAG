{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Learning-Augmented Private Algorithms for Multiple Quantile Release\n\nPaper Abstract: Abstract When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework—previously applied largely to improve time complexity or competitive ratios—as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. This idea is instantiated on the important task of multiple quantile release, for which we derive error guarantees that scale with a natural measure of prediction quality while (almost) recovering state-of-the-art prediction-independent guarantees. Our analysis enjoys several advantages, including minimal assumptions about the data, a natural way of adding robustness, and the provision of useful surrogate losses for two novel “meta” algorithms that learn predictions from other (potentially sensitive) data. We conclude with experiments on challenging tasks demonstrating that learning predictions across one or more instances can lead to large error reductions while preserving privacy.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"The price of differential privacy for online learning\",\n        \"Public data-assisted mirror descent for private model training\",\n        \"Customizing ML predictions for online algorithms\",\n        \"Differentially private learning with adaptive clipping\",\n        \"Learning-to-learn non-convex piecewise-Lipschitz functions\",\n        \"The primal-dual method for learning augmented algorithms\",\n        \"Private domain adaptation from a public source\",\n        \"Private estimation with public data\",\n        \"CoinPress: Practical private mean and covariance estimation\",\n        \"Concentration Inequalities: A Nonasymptotic Theory of Independence\",\n        \"A note on maximum likelihood estimation for regression models using grouped data\",\n        \"On the generalization ability of on-line learning algorithms\",\n        \"A stability-based validation procedure for differentially private machine learning\",\n        \"Faster fundamental graph algorithms via learned predictions\",\n        \"Optimal robustness-consistency tradeoffs for learning-augmented metrical task systems\",\n        \"Universal portfolios\",\n        \"Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density\",\n        \"Order Statistics\",\n        \"Learning online algorithms with distributional advice\",\n        \"Differential privacy for bayesian inference through posterior sampling\",\n        \"Faster matchings via learned duals\",\n        \"Putting the “learning” into learning-augmented algorithms for frequency estimation\",\n        \"Secretaries with advice\",\n        \"The algorithmic foundations of differential privacy\",\n        \"Rényi differential privacy mechanisms for posterior sampling\",\n        \"Differentially private quantiles\",\n        \"Iterative constructions and private data release\",\n        \"A multiplicative weights mechanism for privacy-preserving data analysis\",\n        \"Logarithmic regret algorithms for online convex optimization\",\n        \"Online page migration with ML advice\",\n        \"Differentially private online learning\",\n        \"Online algorithms for weighted paging with predictions\",\n        \"Practical and private (deep) learning without sampling or shuffling\",\n        \"Differentially private approximate quantiles\",\n        \"Learning predictions for algorithms with predictions\",\n        \"Exponentiated gradient versus gradient descent for linear predictors\",\n        \"Scaling up the accuracy of Naive-Bayes classifiers: a decision-tree hybrid\",\n        \"The case for learned index structures\",\n        \"Improving online algorithms via ML predictions\",\n        \"Online scheduling via learned weights\",\n        \"Private adaptive optimization with side information\",\n        \"Permutation predictions for non-clairvoyant scheduling\",\n        \"Leveraging public data for practical private query release\",\n        \"Renewable and cooling aware workload management for sustainable data centers\",\n        \"NLTK: The natural language toolkit\",\n        \"Competitive caching with machine learned advice\",\n        \"A survey of algorithms and analysis for adaptive online learning\",\n        \"Mechanism design via differential privacy\",\n        \"Algorithms with predictions\",\n        \"Training deep networks without learning rates through coin betting\",\n        \"GloVe: Global vectors for word representation\",\n        \"Concavity of the log likelihood\",\n        \"Near-optimal bounds for online caching with machine learned advice\",\n        \"Beyond Worst-Case Analysis of Algorithms\",\n        \"Discrete-convex-analysis-based framework for warm-starting algorithms with predictions\",\n        \"Uniform bounds for scheduling with job size estimates\",\n        \"Private posterior inference consistent with public information: A case study in small area estimation from synthetic census data\",\n        \"Online learning and online convex optimization\",\n        \"(Nearly) optimal algorithms for private online learning in full-information and bandit settings\",\n        \"Item recommendation on monotonic behavior chains\",\n        \"Competitive control with delayed imperfect information\",\n        \"Online convex programming and generalized infinitesimal gradient ascent\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in Learning-Augmented Private Algorithms for Multiple Quantile Release, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}