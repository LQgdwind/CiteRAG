{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 57 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey on Word Meta-Embedding Learning\n\nPaper Abstract: Abstract Meta-embedding (ME) learning is an emerging approach that attempts to learn more accurate word embeddings given existing (source) word embeddings as the sole input. Due to their ability to incorporate semantics from multiple source embeddings in a compact manner with superior performance, ME learning has gained popularity among practitioners in NLP. To the best of our knowledge, there exist no prior systematic survey on ME learning and this paper attempts to fill this need. We classify ME learning methods according to multiple factors such as whether they (a) operate on static or contextualised embeddings, (b) trained in an unsupervised manner or (c) fine-tuned for a particular task/domain. Moreover, we discuss the limitations of existing ME learning methods and highlight potential future research directions.\n\n{retrieved_text}\n\nGenerate exactly 57 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 57\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Joint learning of hierarchical word embeddings from a corpus and a taxonomy\",\n        \"A latent variable model approach to pmi-based word embeddings\",\n        \"Learning principled bilingual mappings of word embeddings while preserving monolingual invariance\",\n        \"Learning word meta-embeddings by autoencoding\",\n        \"Joint word representation learning using a corpus and a semantic lexicon\",\n        \"Learning linear transformations between counting-based and prediction-based word embeddings\",\n        \"Think globally, embed locally ‚Äî locally linear meta-embedding of words\",\n        \"Using kùëòk-way Co-occurrences for Learning Word Embeddings\",\n        \"Learning meta word embeddings by unsupervised weighted concatenation of source embeddings\",\n        \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n        \"Frustratingly easy meta-embedding ‚Äì computing meta-embeddings by averaging source word embeddings\",\n        \"Natural language processing (almost) from scratch\",\n        \"Word translation without parallel data\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Eigenwords: Spectral word embeddings\",\n        \"The Handbook of Brain Theory and Neural Networks\",\n        \"Improving cross-lingual word embeddings by meeting in the middle\",\n        \"Domain-Adversarial Training of Neural Networks\",\n        \"A common semantic space for monolingual and cross-lingual meta-embeddings\",\n        \"Benchmarking meta-embeddings: What works and what does not\",\n        \"Single or multiple? combining word representations independently learned from text and wordnet\",\n        \"Learning Efficient Task-Specific Meta-Embeddings with Word Prisms\",\n        \"Distilling the knowledge in a neural network\",\n        \"Improving word representations via global context and multiple word prototypes\",\n        \"Learning geometric word meta-embeddings\",\n        \"Bag of Tricks for Efficient Text Classification\",\n        \"Gender-preserving debiasing for pre-trained word embeddings\",\n        \"Dynamic meta-embeddings for improved sentence representations\",\n        \"Albert: A lite bert for self-supervised learning of language representations\",\n        \"FAME: Feature-based adversarial meta-embeddings for robust input representations\",\n        \"Improving distributional similarity with lessons learned from word embeddings\",\n        \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n        \"Efficient estimation of word representation in vector space\",\n        \"Contextual correlates of semantic similarity\",\n        \"A scalable hierarchical distributed language model\",\n        \"All-but-the-Top: Simple and Effective Postprocessing for Word Representations\",\n        \"Linear ensembles of word embedding models\",\n        \"Efficient non-parametric estimation of multiple embeddings per word in vector space\",\n        \"Meta-embedding as auxiliary task regularization\",\n        \"A survey on transfer learning\",\n        \"Large scale distributed neural network training through online distillation\",\n        \"Glove: global vectors for word representation\",\n        \"Deep contextualized word representations\",\n        \"Sentence meta-embeddings for unsupervised semantic textual similarity\",\n        \"Ensemble learning\",\n        \"Multi-prototype vector-space models of word meaning\",\n        \"Unsupervised attention-based sentence-level meta-embeddings from contextualised language models\",\n        \"Dict2vec : Learning word embeddings using lexical dictionaries\",\n        \"Attention is all you need\",\n        \"Glue: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"Learning multilingual meta-embeddings for code-switching named entity recognition\",\n        \"Hierarchical meta-embeddings for code-switching named entity recognition\",\n        \"Task-oriented domain-specific meta-embedding for text classification\",\n        \"Dynamic task-specific factors for meta-embedding\",\n        \"Xlnet: Generalized autoregressive pretraining for language understanding\",\n        \"Learning word meta-embeddings\",\n        \"On the dimensionality of word embedding\"\n    ],\n    \"reasoning\": \"Selected these 57 references because they represent foundational work and recent advances in A Survey on Word Meta-Embedding Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}