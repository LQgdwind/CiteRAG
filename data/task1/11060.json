{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing\n\nPaper Abstract: Abstract Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings. In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with dimension-independent accuracy guarantees, using O~‚Äã(D2/œµ)~ùëÇsuperscriptùê∑2italic-œµ\\widetilde{O}({D^{2}}/{\\epsilon}) samples where Dùê∑D is the ambient dimension and œµitalic-œµ\\epsilon is the fraction of corrupted datapoints. As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both the sensing matrices and the measurements. In addition, we establish a Statistical Query lower bound providing evidence that the quadratic dependence on Dùê∑D in the sample complexity is necessary for computationally efficient algorithms.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"The security of machine learning\",\n        \"On the low-rank approach for semidefinite programs arising in synchronization and community detection\",\n        \"A subsampling line-search method with second-order results\",\n        \"Poisoning Attacks against Support Vector Machines\",\n        \"Global optimality of local search for low rank matrix recovery\",\n        \"Statistical Query Algorithms and Low Degree Tests Are Almost Equivalent\",\n        \"High-Dimensional Robust Mean Estimation in Nearly-Linear Time\",\n        \"Non-convex matrix completion against a semi-random adversary\",\n        \"Quantum entropy scoring for fast robust mean estimation and improved outlier detection\",\n        \"Robust estimators in high dimensions without the computational intractability\",\n        \"Being Robust (in High Dimensions) Can Be Practical\",\n        \"Sever: A Robust Meta-Algorithm for Stochastic Optimization\",\n        \"Statistical Query Lower Bounds for List-Decodable Linear Regression\",\n        \"Streaming Algorithms for High-Dimensional Robust Statistics\",\n        \"Recent advances in algorithmic high-dimensional robust statistics\",\n        \"Algorithmic High-Dimensional Robust Statistics\",\n        \"Outlier Robust Mean Estimation with Subgaussian Rates via Stability\",\n        \"Statistical Query Lower Bounds for Robust Estimation of High-Dimensional Gaussians and Gaussian Mixtures\",\n        \"Efficient algorithms and lower bounds for robust linear regression\",\n        \"Probability‚Äîtheory and examples\",\n        \"Statistical Algorithms and a Lower Bound for Detecting Planted Cliques\",\n        \"Robust regression via mutivariate regression depth\",\n        \"Robust Matrix Sensing in the Semi-Random Model\",\n        \"Escaping from saddle points‚Äîonline stochastic gradient for tensor decomposition\",\n        \"No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis\",\n        \"Matrix completion has no spurious local minimum\",\n        \"Asymptotic behavior of dissipative systems\",\n        \"Robust statistics. The approach based on influence functions\",\n        \"Robust statistics\",\n        \"Robust estimation of a location parameter\",\n        \"Robust empirical risk minimization via Newton‚Äôs method\",\n        \"How to Escape Saddle Points Efficiently\",\n        \"On nonconvex optimization for machine learning: gradients, stochasticity, and saddle points\",\n        \"Efficient noise-tolerant Learning from Statistical Queries\",\n        \"Worldwide human relationships inferred from genome-wide patterns of variation\",\n        \"Nonconvex robust low-rank matrix recovery\",\n        \"Non-convex low-rank matrix recovery with arbitrary outliers via median-truncated gradient descent\",\n        \"Agnostic Estimation of Mean and Covariance\",\n        \"A randomized algorithm for nonconvex minimization with inexact evaluations and complexity guarantees\",\n        \"Ancestry Informative Markers for Fine-Scale Individual Assignment to Worldwide Populations\",\n        \"Robust estimation via robust gradient estimation\",\n        \"Genetic structure of human populations\",\n        \"Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis\",\n        \"Certified Defenses for Data Poisoning Attacks\",\n        \"A geometric analysis of phase retrieval\",\n        \"Complete Dictionary Recovery Over the Sphere I: Overview and the Geometric Picture\",\n        \"Mathematics and picturing of data\",\n        \"High-dimensional data analysis with low-dimensional models: Principles, computation, and applications\",\n        \"Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}