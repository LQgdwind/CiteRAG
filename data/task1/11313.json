{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 55 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Oscillations enhance time-series prediction in reservoir computing with feedback\n\nPaper Abstract: Abstract Reservoir computing, a machine learning framework used for modeling the brain, can predict temporal data with little observations and minimal computational resources. However, it is difficult to accurately reproduce the long-term target time series because the reservoir system becomes unstable. This predictive capability is required for a wide variety of time-series processing, including predictions of motor timing and chaotic dynamical systems. This study proposes oscillation-driven reservoir computing (ODRC) with feedback, where oscillatory signals are fed into a reservoir network to stabilize the network activity and induce complex reservoir dynamics. The ODRC can reproduce long-term target time series more accurately than conventional reservoir computing methods in a motor timing and chaotic time-series prediction tasks. Furthermore, it generates a time series similar to the target in the unexperienced period, that is, it can learn the abstract generative rules from limited observations. Given these significant improvements made by the simple and computationally inexpensive implementation, the ODRC would serve as a practical model of various time series data. Moreover, we will discuss biological implications of the ODRC, considering it as a model of neural oscillations and their cerebellar processors.\n\n{retrieved_text}\n\nGenerate exactly 55 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 55\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Brain oscillations in perception and memory\",\n        \"Hippocampal theta-dependent eyeblink classical conditioning: coordination of a distributed learning system\",\n        \"Memory, navigation and theta rhythm in the hippocampal-entorhinal system\",\n        \"Distinct patterns of brain oscillations underlie two basic parameters of human maze learning\",\n        \"Classical conditioning, awareness, and brain systems\",\n        \"Reservoir computing with a single delay-coupled non-linear mechanical oscillator\",\n        \"Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning\",\n        \"On bifurcations and chaos in random neural networks\",\n        \"Brain oscillations and memory\",\n        \"Convolutional sequence to sequence learning\",\n        \"Quantum reservoir computing with a single nonlinear oscillator\",\n        \"Neural Networks and Learning Machines\",\n        \"Real-time parallel processing of grammatical structure in the fronto-striatal system: A recurrent network simulation study using reservoir computing\",\n        \"Oscillatory synchronization in large-scale cortical networks predicts perception\",\n        \"Cerebellar theta oscillations are synchronized during hippocampal theta-contingent trace conditioning\",\n        \"Harnessing the power of theta: natural manipulations of cognitive performance during hippocampal theta-contingent eyeblink conditioning\",\n        \"The “echo state” approach to analysing and training recurrent neural networks\",\n        \"Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication\",\n        \"Interactions between prefrontal cortex and cerebellum revealed by trace eyelid conditioning\",\n        \"Spatiotemporal motor learning with reward-modulated Hebbian plasticity in modular reservoir computing\",\n        \"Oscillation-driven reservoir computing for long-term timing/chaotic time-series prediction\",\n        \"Reservoir computing using self-sustained oscillations in a locally connected neural network\",\n        \"Learning long-term motor timing/patterns on an orthogonal basis in random neural networks\",\n        \"Teaching recurrent neural networks to infer global temporal structure from local examples\",\n        \"Diffusion-induced chaos in reaction systems\",\n        \"Robust timing and motor patterns by taming chaos in recurrent neural networks\",\n        \"Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex\",\n        \"Deterministic nonperiodic flow\",\n        \"Cortico-striatal circuits and interval timing: coincidence detection of oscillatory processes\",\n        \"The storage of time intervals using oscillating neurons\",\n        \"Positional encoding helps recurrent neural networks handle a large vocabulary\",\n        \"Hippocampal theta-band activity and trace eyeblink conditioning in rabbits\",\n        \"Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data\",\n        \"A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics\",\n        \"Stimulus-dependent suppression of chaos in recurrent neural networks\",\n        \"At the edge of chaos: how cerebellar granular layer network dynamics can provide the basis for temporal filters\",\n        \"An equation for continuous chaos\",\n        \"Measurement of the Lyapunov spectrum from a chaotic time series\",\n        \"Oscillatory brain states and learning: Impact of hippocampal theta-contingent training\",\n        \"Nonlinear analysis of hydrodynamic instability in laminar flames—I. derivation of basic equations\",\n        \"Hippocampus and trace conditioning of the rabbit’s classically conditioned nictitating membrane response\",\n        \"Chaos in random neural networks\",\n        \"Generating coherent patterns of activity from chaotic neural networks\",\n        \"Suppression of chaos in a partially driven recurrent neural network\",\n        \"Recent advances in physical reservoir computing: A review\",\n        \"Chaos may enhance expressivity in cerebellar granular layer\",\n        \"Physical reservoir computing based on spin torque oscillator with forced synchronization\",\n        \"Chaos in neuronal networks with balanced excitatory and inhibitory activity\",\n        \"Attention is all you need\",\n        \"Learning long temporal sequences in spiking networks by multiplexing neural oscillations\",\n        \"Driving reservoir models with oscillations: a solution to the extreme structural sensitivity of chaotic networks\",\n        \"Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics\",\n        \"The impact of hippocampal lesions on trace-eyeblink conditioning and forebrain–cerebellar interactions\",\n        \"Positive feedback of efferent copy via pontine nucleus facilitates cerebellum-mediated associative learning\",\n        \"The cerebellum as a liquid state machine\"\n    ],\n    \"reasoning\": \"Selected these 55 references because they represent foundational work and recent advances in Oscillations enhance time-series prediction in reservoir computing with feedback, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}