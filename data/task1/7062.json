{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 60 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Proof.\n\nPaper Abstract: Abstract Stochastic Gradient Descent (SGD) has been the method of choice for learning large-scale non-convex models. While a general analysis of when SGD works has been elusive, there has been a lot of recent progress in understanding the convergence of Gradient Flow (GF) on the population loss, partly due to the simplicity that a continuous-time analysis buys us. An overarching theme of our paper is providing general conditions under which SGD converges, assuming that GF on the population loss converges. Our main tool to establish this connection is a general converse Lyapunov like theorem, which implies the existence of a Lyapunov potential under mild assumptions on the rates of convergence of GF. In fact, using these potentials, we show a one-to-one correspondence between rates of convergence of GF and geometrical properties of the underlying objective. When these potentials further satisfy certain self-bounding properties, we show that they can be used to provide a convergence guarantee for Gradient Descent (GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It turns out that these self-bounding assumptions are in a sense also necessary for GD/SGD to work. Using our framework, we provide a unified analysis for GD/SGD not only for classical settings like convex losses, or objectives that satisfy PŁ / KŁ properties, but also for more complex problems including Phase Retrieval and Matrix sq-root, and extending the results in the recent work of Chatterjee (2022).\n\n{retrieved_text}\n\nGenerate exactly 60 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 60\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Information-theoretic lower bounds on the oracle complexity of convex optimization\",\n        \"On the theory of policy gradient methods: Optimality, approximation, and distribution shift\",\n        \"A convergence theory for deep learning via over-parameterization\",\n        \"Simple, efficient, and neural algorithms for sparse coding\",\n        \"Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-Łojasiewicz inequality\",\n        \"On the implicit bias of initialization shape: Beyond infinitesimal mirror descent\",\n        \"Potential-function proofs for first-order methods\",\n        \"Local and global linear convergence of general low-rank matrix recovery problems\",\n        \"Lqr through the lens of first order methods: Discrete-time case\",\n        \"Convex optimization: Algorithms and complexity\",\n        \"Phase retrieval via wirtinger flow: Theory and algorithms\",\n        \"Lyapunov analysis: from dynamical systems theory to applications\",\n        \"Convergence of gradient descent for deep neural networks\",\n        \"The interplay between implicit bias and benign overfitting in two-layer linear networks\",\n        \"Nonlinear dynamical systems and control: A Lyapunov-based approach\",\n        \"Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval\",\n        \"A note on lazy training in supervised differentiable programming\",\n        \"Lyapunov functions and feedback in nonlinear control\",\n        \"Global convergence of stochastic gradient descent for some non-convex matrix problems\",\n        \"Gradient descent provably optimizes over-parameterized neural networks\",\n        \"Continuous vs. discrete optimization of deep neural networks\",\n        \"Inequalities for the trace of matrix product\",\n        \"Optimizing static linear feedback: Gradient method\",\n        \"Characterizing implicit bias in terms of optimization geometry\",\n        \"Implicit bias of gradient descent on linear convolutional networks\",\n        \"Implicit regularization in matrix factorization\",\n        \"Mirrorless mirror descent: A natural derivation of mirror descent\",\n        \"Global convergence of non-convex gradient descent for computing matrix squareroot\",\n        \"Risk and parameter convergence of logistic regression\",\n        \"Provable efficient online matrix completion via non-convex stochastic gradient descent\",\n        \"SGD: the role of implicit regularization, batch-size and multiple-epochs\",\n        \"Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition\",\n        \"Classical converse theorems in lyapunov’s second method\",\n        \"An alternative view: When does sgd escape local minima?\",\n        \"Continuous time analysis of momentum methods\",\n        \"Continuous and discrete dynamics for online learning and convex optimization\",\n        \"A Lyapunov Approach to Accelerated First-Order Optimization In Continuous and Discrete Time\",\n        \"Accelerated mirror descent in continuous and discrete time\",\n        \"On gradients of functions definable in o-minimal structures\",\n        \"A topological property of real analytic subsets\",\n        \"Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion\",\n        \"On the global convergence rates of softmax policy gradient methods\",\n        \"Leveraging non-uniformity in first-order non-convex optimization\",\n        \"Global exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic regulator\",\n        \"Stochastic approximation approach to stochastic programming\",\n        \"Problem complexity and method efficiency in optimization\",\n        \"Continuous-time models for stochastic optimization algorithms\",\n        \"Gradient methods for the minimisation of functionals\",\n        \"The implicit bias of gradient descent on separable data\",\n        \"Smoothness, low noise and fast rates\",\n        \"A differential equation for modeling nesterov’s accelerated gradient method: theory and insights\",\n        \"Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval\",\n        \"Implicit regularization in relu networks with the square loss\",\n        \"Lyapunov arguments in optimization\",\n        \"A lyapunov analysis of accelerated methods in optimization\",\n        \"Stochastic gradient descent with noise of machine learning type. part i: Discrete time analysis\",\n        \"Stochastic gradient descent with noise of machine learning type. part ii: Continuous time analysis\",\n        \"A general sample complexity analysis of vanilla policy gradient\",\n        \"Global convergence in deep learning with variable splitting via the kurdyka-łojasiewicz property\",\n        \"Revisiting the role of euler numerical integration on acceleration and stability in convex optimization\"\n    ],\n    \"reasoning\": \"Selected these 60 references because they represent foundational work and recent advances in Proof., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}