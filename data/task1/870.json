{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Temporal-Differential Learning in Continuous Environments\n\nPaper Abstract: Abstract In this paper, a new reinforcement learning (RL) method known as the method of temporal differential is introduced. Compared to the traditional temporal-difference learning method, it plays a crucial role in developing novel RL techniques for continuous environments. In particular, the continuous-time least squares policy evaluation (CT-LSPE) and the continuous-time temporal-differential (CT-TD) learning methods are developed. Both theoretical and empirical evidences are provided to demonstrate the effectiveness of the proposed temporal-differential learning methodology.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Fitted Q-iteration in continuous action-space MDPs\",\n        \"Ergodic Control of Diffusion Processes\",\n        \"Stochastic Differential Equations: Theory and Applications\",\n        \"Theory of reproducing kernels\",\n        \"Advantage updating\",\n        \"Some recent applications of reinforcement learning\",\n        \"Adaptive algorithms and stochastic approximations\",\n        \"On the functional central limit theorem and the law of the iterated logarithm for markov processes\",\n        \"Value iteration and adaptive dynamic programming for data-driven adaptive optimal control design\",\n        \"Continuous-time robust dynamic programming\",\n        \"Learning through reinforcement and replicator dynamics\",\n        \"Fifty years of maximal monotonicity\",\n        \"Deep relaxation: partial differential equations for optimizing deep neural networks\",\n        \"Neural ordinary differential equations\",\n        \"Temporal difference learning in continuous time and space\",\n        \"Reinforcement learning in continuous time and space\",\n        \"Benchmarking deep reinforcement learning for continuous control\",\n        \"A proposal on machine learning via dynamical systems\",\n        \"Reinforcement learning using a continuous time actor-critic framework with spiking neurons\",\n        \"A Liapounov bound for solutions of the Poisson equation\",\n        \"Back to the future: Generating moment implications for continuous-time markov processes\",\n        \"Advantage updating applied to a differential game\",\n        \"Lectures on stochastic processes\",\n        \"Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics\",\n        \"Robust Adaptive Dynamic Programming\",\n        \"Nonlinear Control\",\n        \"Optimal and autonomous control using reinforcement learning: A survey\",\n        \"Dynamic replication and hedging: A reinforcement learning approach\",\n        \"Actor-Critic Algorithms\",\n        \"Spectral theory and limit theorems for geometrically ergodic markov processes\",\n        \"Stochastic Stability and Control\",\n        \"Optimal discounted stochastic control for diffusion processes\",\n        \"Stochastic Approximation and Recursive Algorithms and Applications\",\n        \"Continuous control with deep reinforcement learning\",\n        \"Analysis of recursive stochastic algorithms\",\n        \"Dissipative operators in a Banach space\",\n        \"Parallel Robots\",\n        \"Monotone (nonlinear) operators in Hilbert space\",\n        \"Balancing a double inverted pendulum using optimal control and laguerre functions\",\n        \"A study of reinforcement learning in the continuous case by the means of viscosity solutions\",\n        \"Finite-time bounds for fitted value iteration\",\n        \"Least squares policy evaluation algorithms with linear function approximation\",\n        \"The adjoint Markoff process\",\n        \"Semigroups of Linear Operators and Applications to Partial Differential Equations\",\n        \"A note on the abstract cauchy problem\",\n        \"A tour of reinforcement learning: The view from continuous control\",\n        \"Rationality and intelligence\",\n        \"Artificial Intelligence: A Modern Approach\",\n        \"Deep neural networks motivated by partial differential equations\",\n        \"Nonlinear model reduction by moment matching\",\n        \"Mastering the game of go without human knowledge\",\n        \"Mathematical Control Theory: Deterministic Finite Dimensional Systems\",\n        \"Input to state stability: Basic concepts and results\",\n        \"Learning to predict by the methods of temporal differences\",\n        \"Reinforcement Learning: An Introduction\",\n        \"Asynchronous stochastic approximation and Q-learning\",\n        \"An analysis of temporal-difference learning with function approximation\",\n        \"Reinforcement learning in continuous state and action spaces\",\n        \"Reinforcement learning in continuous action spaces\",\n        \"Optimal Adaptive Control and Differential Games by Reinforcement Learning Principles\",\n        \"Prefrontal cortex as a meta-reinforcement learning system\",\n        \"Convergence results for some temporal difference methods based on least squares\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in Temporal-Differential Learning in Continuous Environments, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}