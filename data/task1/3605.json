{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Contingency-Aware Influence Maximization: A Reinforcement Learning Approach\n\nPaper Abstract: Abstract The influence maximization (IM) problem aims at finding a subset of seed nodes in a social network that maximize the spread of influence. In this study, we focus on a sub-class of IM problems, where whether the nodes are willing to be the seeds when being invited is uncertain, called contingency-aware IM. Such contingency aware IM is critical for applications for non-profit organizations in low resource communities (e.g., spreading awareness of disease prevention). Despite the initial success, a major practical obstacle in promoting the solutions to more communities is the tremendous runtime of the greedy algorithms and the lack of high performance computing (HPC) for the non-profits in the field – whenever there is a new social network, the non-profits usually do not have the HPCs to recalculate the solutions. Motivated by this and inspired by the line of works that use reinforcement learning (RL) to address combinatorial optimization on graphs, we formalize the problem as a Markov Decision Process (MDP), and use RL to learn an IM policy over historically seen networks, and generalize to unseen networks with negligible runtime at test phase. To fully exploit the properties of our targeted problem, we propose two technical innovations that improve the existing methods, including state-abstraction and theoretically grounded reward shaping. Empirical results show that our method achieves influence as high as the state-of-the-art methods for contingency-aware IM, while having negligible runtime at test phase.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Boosting reinforcement learning in competitive influence maximization with transfer learning\",\n        \"Learning explainable interventions to mitigate hiv transmission in sex workers across five states in india\",\n        \"Neural combinatorial optimization with reinforcement learning\",\n        \"Machine learning for combinatorial optimization: A methodological tour d’horizon\",\n        \"Maximizing social influence in nearly optimal time\",\n        \"Discriminative embeddings of latent variable models for structured data\",\n        \"Learning heuristics for the tsp by policy gradient\",\n        \"Mining the network value of customers\",\n        \"Talk of the network: A complex systems look at the underlying process of word-of-mouth\",\n        \"Adaptive submodularity: Theory and applications in active learning and stochastic optimization\",\n        \"Hybrid computing using a neural network with dynamic external memory\",\n        \"The application of medical artificial intelligence technology in rural areas of developing countries\",\n        \"Efficient algorithms for adaptive influence maximization\",\n        \"Efficient approximation algorithms for adaptive influence maximization\",\n        \"An efficient graph convolutional network technique for the travelling salesman problem\",\n        \"Influence maximization in unknown social networks: Learning policies for effective graph sampling\",\n        \"Maximizing the spread of influence through a social network\",\n        \"Learning combinatorial optimization algorithms over graphs\",\n        \"Adam: A method for stochastic optimization\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Monstor: An inductive approach for estimating and maximizing influence over unseen social networks\",\n        \"Attention, learn to solve routing problems!\",\n        \"Cost-effective outbreak detection in networks\",\n        \"Disco: Influence maximization meets network embedding and deep learning\",\n        \"Combinatorial optimization with graph convolutional networks and guided tree search\",\n        \"A learning-based framework to handle multi-round multi-party influence maximization on social networks\",\n        \"Gcomb: Learning budget-constrained combinatorial algorithms over billion-sized graphs\",\n        \"Learning scheduling algorithms for data processing clusters\",\n        \"Artificial intelligence for citizen services and government\",\n        \"Artificial intelligence for the public sector: opportunities and challenges of cross-sector collaboration\",\n        \"Playing atari with deep reinforcement learning\",\n        \"Human-level control through deep reinforcement learning\",\n        \"Reinforcement learning for solving the vehicle routing problem\",\n        \"Structure and tie strengths in mobile communication networks\",\n        \"Active screening for recurrent diseases: A reinforcement learning approach\",\n        \"Examining impacts of a peer-based mindfulness and yoga intervention to reduce interpersonal violence among young adults experiencing homelessness\",\n        \"Dynamic electronic toll collection via multi-agent deep reinforcement learning with edge-based graph convolutional networks\",\n        \"Using artificial intelligence to augment network-based, hiv prevention for youth experiencing homelessness\",\n        \"Network-based intervention strategies to reduce violence among homeless\",\n        \"Multi-round influence maximization\",\n        \"Influence maximization in near-linear time: A martingale approach\",\n        \"Deep reinforcement learning-based approach to tackle topic-aware influence maximization\",\n        \"Pointer networks\",\n        \"Learning from delayed rewards\",\n        \"End-to-end influence maximization in the field\",\n        \"Clinical trial of an ai-augmented intervention for hiv prevention in youth experiencing homelessness\",\n        \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"Using social networks to aid homeless shelters: Dynamic influence maximization under uncertainty\",\n        \"Please be an influencer? contingency-aware influence maximization\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Contingency-Aware Influence Maximization: A Reinforcement Learning Approach, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}