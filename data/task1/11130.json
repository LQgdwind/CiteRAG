{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 63 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Gradient Descent is Pareto-Optimal in the Oracle Complexity and Memory Tradeoff for Feasibility Problems\n\nPaper Abstract: Abstract In this paper we provide oracle complexity lower bounds for finding a point in a given set using a memory-constrained algorithm that has access to a separation oracle. We assume that the set is contained within the unit d𝑑d-dimensional ball and contains a ball of known radius ϵ>0italic-ϵ0\\epsilon>0. This setup is commonly referred to as the feasibility problem. We show that to solve feasibility problems with accuracy ϵ≥e−do​(1)italic-ϵsuperscript𝑒superscript𝑑𝑜1\\epsilon\\geq e^{-d^{o(1)}}, any deterministic algorithm either uses d1+δsuperscript𝑑1𝛿d^{1+\\delta} bits of memory or must make at least 1/(d0.01​δ​ϵ2​1−δ1+1.01​δ−o​(1))1superscript𝑑0.01𝛿superscriptitalic-ϵ21𝛿11.01𝛿𝑜11/(d^{0.01\\delta}\\epsilon^{2\\frac{1-\\delta}{1+1.01\\delta}-o(1)}) oracle queries, for any δ∈[0,1]𝛿01\\delta\\in[0,1]. Additionally, we show that randomized algorithms either use d1+δsuperscript𝑑1𝛿d^{1+\\delta} memory or make at least 1/(d2​δ​ϵ2​(1−4​δ)−o​(1))1superscript𝑑2𝛿superscriptitalic-ϵ214𝛿𝑜11/(d^{2\\delta}\\epsilon^{2(1-4\\delta)-o(1)}) queries for any δ∈[0,14]𝛿014\\delta\\in[0,\\frac{1}{4}]. Because gradient descent only uses linear memory 𝒪​(d​ln⁡1/ϵ)𝒪𝑑1italic-ϵ\\mathcal{O}(d\\ln 1/\\epsilon) but makes Ω​(1/ϵ2)Ω1superscriptitalic-ϵ2\\Omega(1/\\epsilon^{2}) queries, our results imply that it is Pareto-optimal in the oracle complexity/memory tradeoff. Further, our results show that the oracle complexity for deterministic algorithms is always polynomial in 1/ϵ1italic-ϵ1/\\epsilon if the algorithm has less than quadratic memory in d𝑑d. This reveals a sharp phase transition since with quadratic 𝒪​(d2​ln⁡1/ϵ)𝒪superscript𝑑21italic-ϵ\\mathcal{O}(d^{2}\\ln 1/\\epsilon) memory, cutting plane methods only require 𝒪​(d​ln⁡1/ϵ)𝒪𝑑1italic-ϵ\\mathcal{O}(d\\ln 1/\\epsilon) queries.\n\n{retrieved_text}\n\nGenerate exactly 63 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 63\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Acceleration by stepsize hedging I: Multi-step descent and the silver stepsize schedule\",\n        \"On vaidya’s volumetric cutting plane method for convex programming\",\n        \"A cutting plane algorithm for convex programming that uses analytic centers\",\n        \"Parallelization does not accelerate convex optimization: Adaptivity lower bounds for non-smooth convex minimization\",\n        \"Time-space tradeoffs for learning finite functions from random evaluations, with applications to polynomials\",\n        \"Solving convex programs by random walks\",\n        \"Quadratic memory is necessary for optimal query complexity in convex optimization: Center-of-mass is pareto-optimal\",\n        \"Memory-constrained algorithms for convex optimization\",\n        \"When is memorization of irrelevant training data necessary for high-accuracy learning?\",\n        \"Strong memory lower bounds for learning natural models\",\n        \"Complexity of highly parallel non-smooth convex optimization\",\n        \"Memory-query tradeoffs for randomized convex optimization\",\n        \"Branch-and-bound performance estimation programming: A unified methodology for constructing optimal optimization methods\",\n        \"Local operator theory, random matrices and banach spaces\",\n        \"Function minimization by conjugate gradients\",\n        \"Extractor-based time-space lower bounds for learning\",\n        \"Time-space lower bounds for two-pass learning\",\n        \"Memory-sample lower bounds for learning parity with noise\",\n        \"A survey of nonlinear conjugate gradient methods\",\n        \"A bound on tail probabilities for quadratic forms in independent random variables\",\n        \"An improved cutting plane method for convex optimization, convex-concave games, and its applications\",\n        \"Polynomial algorithms in linear programming\",\n        \"Time-space hardness of learning sparse parities\",\n        \"Communication-efficient algorithms for decentralized and stochastic optimization\",\n        \"Adaptive estimation of a quadratic functional by model selection\",\n        \"A faster cutting plane method and its implications for combinatorial and convex optimization\",\n        \"An algorithm for minimizing convex functions\",\n        \"On the limited memory BFGS method for large scale optimization\",\n        \"Efficient convex optimization requires superlinear memory\",\n        \"Memory limited, streaming pca\",\n        \"Mixing implies lower bounds for space bounded learning\",\n        \"Entropy Samplers and Strong Generic Lower Bounds For Space Bounded Learning\",\n        \"D-admm: A communication-efficient distributed algorithm for separable optimization\",\n        \"On parallel complexity of nonsmooth convex optimization\",\n        \"Problem complexity and method efficiency in optimization\",\n        \"Self-concordant functions and polynomial-time methods in convex programming\",\n        \"Introductory lectures on convex optimization: A basic course, volume 87\",\n        \"Updating quasi-newton matrices with limited storage\",\n        \"Near optimal memory-regret tradeoff for online learning\",\n        \"Online prediction in sub-linear space\",\n        \"Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence\",\n        \"A time-space lower bound for a large class of learning problems\",\n        \"Fast learning requires good memory: A time-space lower bound for parity learning\",\n        \"Aide: Fast and communication efficient distributed optimization\",\n        \"Sub-sampled newton methods\",\n        \"Hanson-wright inequality and sub-gaussian concentration\",\n        \"Singular values of gaussian matrices and permanent estimators\",\n        \"Communication-efficient distributed optimization using an approximate newton-type method\",\n        \"Memory-sample tradeoffs for linear regression with small error\",\n        \"Cocoa: A general framework for communication-efficient distributed optimization\",\n        \"Memory bounds for the experts problem\",\n        \"Minimax rates for memory-bounded sparse linear regression\",\n        \"Memory, communication, and statistical queries\",\n        \"Topics in random matrix theory, volume 132\",\n        \"The method of inscribed ellipsoids\",\n        \"A new algorithm for minimizing convex functions over convex sets\",\n        \"High-dimensional probability\",\n        \"Condition numbers of random triangular matrices\",\n        \"Memory and communication efficient distributed stochastic optimization with minibatch prox\",\n        \"Gradient sparsification for communication-efficient distributed optimization\",\n        \"Open problem: The oracle complexity of convex optimization with limited memory\",\n        \"Evaluation of the information complexity of mathematical programming problems\",\n        \"Communication-efficient algorithms for statistical optimization\"\n    ],\n    \"reasoning\": \"Selected these 63 references because they represent foundational work and recent advances in Gradient Descent is Pareto-Optimal in the Oracle Complexity and Memory Tradeoff for Feasibility Problems, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}