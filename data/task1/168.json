{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies\n\nPaper Abstract: Abstract Branch and Bound (B&B) is the exact tree search method typically used to solve Mixed-Integer Linear Programming problems (MILPs). Learning branching policies for MILP has become an active research area, with most works proposing to imitate the strong branching rule and specialize it to distinct classes of problems. We aim instead at learning a policy that generalizes across heterogeneous MILPs: our main hypothesis is that parameterizing the state of the B&B search tree can aid this type of generalization. We propose a novel imitation learning framework, and introduce new input features and architectures to represent branching. Experiments on MILP benchmark instances clearly show the advantages of incorporating an explicit parameterization of the state of the search tree to modulate the branching decisions, in terms of both higher accuracy and smaller B&B trees. The resulting policies significantly outperform the current state-of-the-art method for “learning to branch” by effectively allowing generalization to generic unseen instances.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Constraint Integer Programming\",\n        \"Hybrid Branching, 309–311\",\n        \"Branching rules revisited\",\n        \"Mixed Integer Programming: Analyzing 12 Years of Progress, 449–481\",\n        \"A Machine Learning-Based Approximation of Strong Branching\",\n        \"Finding Cuts in the TSP (A Preliminary Report)\",\n        \"Learning to Branch\",\n        \"Machine Learning for Combinatorial Optimization: a Methodological Tour d’Horizon\",\n        \"Experiments in mixed-integer programming\",\n        \"From feasibility to improvement to proof: three phases of solving mixed-integer programs\",\n        \"An updated mixed-integer programming library: MIPLIB 3\",\n        \"Learning Action Representations for Reinforcement Learning\",\n        \"Lifelong Learning with a Changing Action Set\",\n        \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\",\n        \"DASH: Dynamic Approach for Switching Heuristics\",\n        \"Reinforcement Learning in Large Discrete Action Spaces\",\n        \"Learning MILP Resolution Outcomes Before Reaching Time-Limit\",\n        \"Measuring the Impact of Branching Rules for Mixed-Integer Programming\",\n        \"Exact Combinatorial Optimization with Graph Convolutional Neural Networks\",\n        \"The SCIP Optimization Suite 6.0\",\n        \"MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming Library\",\n        \"Explaining and Harnessing Adversarial Examples\",\n        \"Cuts, Primal Heuristics, and Learning to Branch for the Time-Dependent Traveling Salesman Problem\",\n        \"Learning to Search in Branch and Bound Algorithms\",\n        \"Long short-term memory\",\n        \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\n        \"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\",\n        \"Learning to Branch in Mixed Integer Programming\",\n        \"Adam: A method for stochastic optimization\",\n        \"MIPLIB 2010\",\n        \"Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks\",\n        \"An Automatic Method of Solving Discrete Programming Problems\",\n        \"An abstract model for branching and its application to mixed integer programming\",\n        \"Feature Pyramid Networks for Object Detection\",\n        \"A Computational Study of Search Strategies for Mixed Integer Programming\",\n        \"Branch And Bound—Why Does It Work?\",\n        \"Mixed integer programming computation\",\n        \"Performance Variability in Mixed-Integer Programming, chapter 1, 1–12\",\n        \"Performance Variability in Mixed-Integer Programming, chapter Chapter 1, 1–12\",\n        \"On learning and branching: a survey\",\n        \"PySCIPOpt: Mathematical Programming in Python with the SCIP Optimization Suite\",\n        \"Learning in Gated Neural Networks\",\n        \"Mogrifier LSTM\",\n        \"MILPlib\",\n        \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n        \"Efficient Training of Artificial Neural Networks for Autonomous Navigation\",\n        \"Code for the relpscost branching rule in SCIP\",\n        \"Guiding Combinatorial Optimization with UCT\",\n        \"Beyond Skip Connections: Top-Down Modulation for Object Detection\",\n        \"ExGate: Externally Controlled Gating for Feature-based Attention in Artificial Neural Networks\",\n        \"Learning to Search via Retrospective Imitation\",\n        \"Visualizing High-Dimensional Data Using t-SNE\",\n        \"Integer programming\",\n        \"Group Normalization\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}