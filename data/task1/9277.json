{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 219 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Titre de la thèse\n\nPaper Abstract: \n\n{retrieved_text}\n\nGenerate exactly 219 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 219\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Attention is all you need\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Comparing machines and humans on a visual categorization test\",\n        \"A taxonomy of external and internal attention\",\n        \"Describing multimedia content using attention-based encoder-decoder networks\",\n        \"Les passions de l’âme\",\n        \"Physiological optics\",\n        \"Gestalt psychology today\",\n        \"Reflexes of the brain (trad. s. belsky)\",\n        \"Modeling visual attention via selective tuning\",\n        \"Conditioned reflexes\",\n        \"Bayesian surprise attracts human attention\",\n        \"A feature-integration theory of attention\",\n        \"Perception and communication\",\n        \"Toward a theory of memory and attention\",\n        \"A model for visual shape recognition\",\n        \"A neural model of attention, reinforcement and discrimination learning\",\n        \"Parallel versus serial processing in rapid pattern discrimination\",\n        \"Shifts in selective visual attention: towards the underlying neural circuitry\",\n        \"Attention: Some theoretical considerations\",\n        \"Selective attention in man\",\n        \"Perceptual load as a major determinant of the locus of selection in visual attention\",\n        \"Diluting the burden of load: perceptual load effects are simply dilution effects\",\n        \"The attention system of the human brain\",\n        \"On the control of automatic processes: a parallel distributed processing account of the stroop effect\",\n        \"Slam: A connectionist model for attention in visual selection tasks\",\n        \"A theory of visual attention\",\n        \"Neural mechanisms of selective visual attention\",\n        \"Computational modeling of spatial attention\",\n        \"Control of goal-directed and stimulus-driven attention in the brain\",\n        \"Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia\",\n        \"The attention system of the human brain: 20 years after\",\n        \"Orthogonal representations for robust context-dependent task performance in brains and neural networks\",\n        \"Curriculum learning for human compositional generalization\",\n        \"Bio-inspired visual attention and object recognition\",\n        \"How biological attention mechanisms improve task performance in a large-scale visual system model\",\n        \"Orienting of attention\",\n        \"Neural systems control of spatial orienting\",\n        \"Selective attention and the organization of visual information\",\n        \"Shifting visual attention between objects and locations: evidence from normal and parietal lesion subjects\",\n        \"Does visual attention select objects or locations?\",\n        \"Object-based attentional selection—grouped arrays or spatially invariant representations?: Comment on vecera and farah (1994)\",\n        \"Color segregation and selective attention in a nonsearch task\",\n        \"Movement and visual attention: the spotlight metaphor breaks down\",\n        \"Perceptual organization and focused attention: The role of objects and proximity in visual processing\",\n        \"Visual parsing and response competition: The effect of grouping factors\",\n        \"Objects and attributes in divided attention: Surface and boundary systems\",\n        \"Computational modelling of visual attention\",\n        \"Global effects of feature-based attention in human visual cortex\",\n        \"Object-based attention: A tutorial review\",\n        \"The distinct modes of vision offered by feedforward and recurrent processing\",\n        \"Visual attention: bottom-up versus top-down\",\n        \"Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices\",\n        \"Automatic guidance of attention from working memory\",\n        \"On the difference between working memory and attentional set\",\n        \"Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\",\n        \"Neural network model for selective attention in visual pattern recognition and associative recall\",\n        \"Recognition and segmentation of connected characters with selective attention\",\n        \"SCAN: A scalable model of attentional selection\",\n        \"Learning to generate artificial fovea trajectories for target detection\",\n        \"Integration of bottom-up and top-down cues for visual attention using non-linear relaxation\",\n        \"A neural model combining attentional orienting to object recognition: Preliminary explorations on the interplay between where and what\",\n        \"A selective attention-based method for visual pattern recognition with application to handwritten digit recognition and face recognition\",\n        \"Attentional selection for object recognition—a gentle way\",\n        \"Scene analysis with saccadic eye movements: top-down and bottom-up modeling\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Effective approaches to attention-based neural machine translation\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Long short-term memory\",\n        \"On the properties of neural machine translation: Encoder–decoder approaches\",\n        \"Quasi-recurrent neural networks\",\n        \"Learning phrase representations using RNN encoder–decoder for statistical machine translation\",\n        \"Recent trends in deep learning based natural language processing [review article]\",\n        \"Deep learning\",\n        \"Neural turing machines\",\n        \"Long short-term memory-networks for machine reading\",\n        \"A decomposable attention model for natural language inference\",\n        \"BERT: pre-training of deep bidirectional transformers for language understanding\",\n        \"Convolutional sequence to sequence learning\",\n        \"Self-attention with relative position representations\",\n        \"Stand-alone self-attention in vision models\",\n        \"Attention augmented convolutional networks\",\n        \"Rethinking and improving relative position encoding for vision transformer\",\n        \"Position information in transformers: An overview\",\n        \"Adaptive input representations for neural language modeling\",\n        \"Learning deep transformer models for machine translation\",\n        \"Texture synthesis by non-parametric sampling\",\n        \"Non-local neural networks\",\n        \"Bilateral filtering for gray and color images\",\n        \"Image transformer\",\n        \"Scaling local self-attention for parameter efficient visual backbones\",\n        \"Bottleneck transformers for visual recognition\",\n        \"Deep residual learning for image recognition\",\n        \"End-to-end object detection with transformers\",\n        \"A^ 2-nets: Double attention networks\",\n        \"Squeeze-and-excitation networks\",\n        \"Compact generalized non-local network\",\n        \"Efficient attention: Attention with linear complexities\",\n        \"Ccnet: Criss-cross attention for semantic segmentation\",\n        \"Relating transformers to models and neural representations of the hippocampal formation\",\n        \"The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Training data-efficient image transformers & distillation through attention\",\n        \"Cvt: Introducing convolutions to vision transformers\",\n        \"Cmt: Convolutional neural networks meet vision transformers\",\n        \"Incorporating convolution designs into visual transformers\",\n        \"Levit: a vision transformer in convnet’s clothing for faster inference\",\n        \"Coatnet: Marrying convolution and attention for all data sizes\",\n        \"Conformer: Local features coupling global representations for visual recognition\",\n        \"Localvit: Bringing locality to vision transformers\",\n        \"Elsa: Enhanced local self-attention for vision transformer\",\n        \"Convit: Improving vision transformers with soft convolutional inductive biases\",\n        \"Conviformers: Convolutionally guided vision transformer\",\n        \"Scaling vision transformers\",\n        \"Generating long sequences with sparse transformers\",\n        \"Energy and policy considerations for modern deep learning research\",\n        \"Drawing early-bird tickets: Toward more efficient training of deep networks\",\n        \"Emergent symbols through binding in external memory\",\n        \"What do we perceive in a glance of a real-world scene?\",\n        \"Visual turing test for computer vision systems\",\n        \"Beyond the feedforward sweep: feedback computations in the visual cortex\",\n        \"On the ability to inhibit thought and action: A users’ guide to the stop signal paradigm\",\n        \"Mental rotation of three-dimensional objects\",\n        \"Spatial attention and the apprehension of spatial relations\",\n        \"Visual attention and the apprehension of spatial relations: The case of depth\",\n        \"Attentional coding of categorical relations in scene perception: evidence from the flicker paradigm\",\n        \"Perceiving spatial relations via attentional tracking and shifting\",\n        \"Retinotopic mapping of categorical and coordinate spatial relation processing in early visual cortex\",\n        \"Recruitment of anterior dorsolateral prefrontal cortex in human reasoning: a parametric study of relational complexity\",\n        \"Differential role of anterior prefrontal and premotor cortex in the processing of relational information\",\n        \"Working memory for relations among objects\",\n        \"Contextual effects in visual working memory reveal hierarchically structured memory representations\",\n        \"Same-different conceptualization: a machine vision perspective\",\n        \"Unsupervised learning by program synthesis\",\n        \"Not-so-clevr: learning same–different relations strains feedforward neural networks\",\n        \"Solving the same-different task with convolutional neural networks\",\n        \"Evaluating the progress of deep learning for visual relational concepts\",\n        \"25 years of cnns: Can we compare to human abstraction capabilities?\",\n        \"Can deep convolutional neural networks learn same-different relations?\",\n        \"Five points to check when comparing visual perception in humans and machines\",\n        \"Program synthesis performance constrained by non-linear spatial relations in synthetic visual reasoning test\",\n        \"Differential involvement of eeg oscillatory components in sameness versus spatial-relation visual reasoning tasks\",\n        \"Same/different in visual reasoning\",\n        \"Learning same and different relations: cross-species comparisons\",\n        \"Adam: A method for stochastic optimization\",\n        \"Shortcut learning in deep neural networks\",\n        \"Mlp-mixer: An all-mlp architecture for vision\",\n        \"Recurrent vision transformer for solving visual reasoning problems, 2021b\",\n        \"Resnet with one-neuron hidden layers is a universal approximator\",\n        \"The Algebraic Mind: Integrating Connectionism and Cognitive Science\",\n        \"Learning what and where to attend\",\n        \"Recurrent neural circuits for contour detection\",\n        \"Do neural networks for segmentation understand insideness?\",\n        \"Parallel encoding within and between elementary stimulus dimensions\",\n        \"Selective attention and cognitive control\",\n        \"Selective attention gates visual processing in the extrastriate cortex\",\n        \"Attentional modulation of visual motion processing in cortical areas mt and mst\",\n        \"Object-based attention in the primary visual cortex of the macaque monkey\",\n        \"fmri evidence for objects as the units of attentional selection\",\n        \"Mechanisms of visual attention in the human cortex\",\n        \"Visual attention: The past 25 years\",\n        \"Attention improves performance primarily by reducing interneuronal correlations\",\n        \"Spatial attention decorrelates intrinsic activity fluctuations in macaque area v4\",\n        \"Dynamic shifts of visual receptive fields in cortical area mt by spatial attention\",\n        \"Time course of attention reveals different mechanisms for spatial and feature-based attention in area v4\",\n        \"Neuronal effects of spatial and feature attention differ due to normalization\",\n        \"Attentional modulation of visual processing\",\n        \"Feature binding, attention and object perception\",\n        \"Inferotemporal cortex and object vision\",\n        \"Are cortical models really bound by the “binding problem”?\",\n        \"Serial deployment of attention during visual search\",\n        \"Event files: Evidence for automatic integration of stimulus-response episodes\",\n        \"The theta-gamma neural code\",\n        \"Binding by random bursts: A computational model of cognitive control\",\n        \"Time-based binding as a solution to and a limitation for flexible cognition\",\n        \"Covert orienting in the split brain reveals hemispheric specialization for object-based attention\",\n        \"Action recognition using visual attention\",\n        \"ABC-CNN: an attention based convolutional neural network for visual question answering\",\n        \"Stacked attention networks for image question answering\",\n        \"Ask, attend and answer: Exploring question-guided spatial attention for visual question answering\",\n        \"End-to-end instance segmentation and counting with recurrent attention\",\n        \"Deep networks with internal selective attention through feedback connections\",\n        \"Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning\",\n        \"Global-and-local attention networks for visual recognition\",\n        \"Cbam: Convolutional block attention module\",\n        \"Deformable DETR: Deformable transformers for End-to-End object detection\",\n        \"Attention over learned object embeddings enables complex visual reasoning\",\n        \"Layer normalization\",\n        \"On the binding problem in artificial neural networks\",\n        \"Different binding strategies for the different stages of visual recognition\",\n        \"Structure mapping in analogy and similarity\",\n        \"Modeling visual problem solving as analogical reasoning\",\n        \"Cognitive abilities and their interplay: Reasoning, crystallized intelligence, working memory components, and sustained attention\",\n        \"The relationship between attention and working memory\",\n        \"Fluid intelligence is related to capacity in memory as well as attention: Evidence from middle childhood and adulthood\",\n        \"Understanding the Computational Demands Underlying Visual Reasoning\",\n        \"Vision using routines: A functional account of vision\",\n        \"Raven: A dataset for relational and analogical visual reasoning\",\n        \"Measuring abstract reasoning in neural networks\",\n        \"Hierarchical rule induction network for abstract visual reasoning\",\n        \"A closer look at generalisation in raven\",\n        \"Stratified rule-aware network for abstract visual reasoning\",\n        \"Training data-efficient image transformers and distillation through attention\",\n        \"Recurrent models of visual attention\",\n        \"A simple neural network module for relational reasoning\",\n        \"Instance normalization: The missing ingredient for fast stylization\",\n        \"Optuna: A next-generation hyperparameter optimization framework\",\n        \"Attention on abstract visual reasoning\",\n        \"The concepts of ‘sameness’ and ‘difference’in an insect\",\n        \"Ducklings imprint on the relational concept of “same or different”\",\n        \"A benchmark for compositional visual reasoning\",\n        \"V-prom: A benchmark for visual reasoning using visual progressive matrices\",\n        \"On the measure of intelligence\",\n        \"Early convolutions help transformers see better\",\n        \"Attention in psychology, neuroscience, and machine learning\",\n        \"Visit: A neural model of covert visual attention\",\n        \"Gamr: A guided attention model for (visual) reasoning\",\n        \"Visual routines\",\n        \"Learning representations that support extrapolation\"\n    ],\n    \"reasoning\": \"Selected these 219 references because they represent foundational work and recent advances in Titre de la thèse, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}