{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations\n\nPaper Abstract: Abstract Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Abductive and deductive change\",\n        \"Conversational neuro-symbolic commonsense reasoning\",\n        \"Self-supervised knowledge triplet learning for zero-shot question answering\",\n        \"Abductive commonsense reasoning\",\n        \"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow\",\n        \"Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering\",\n        \"Comet: Commonsense transformers for automatic knowledge graph construction\",\n        \"Flexible generation of natural language deductions\",\n        \"A large annotated corpus for learning natural language inference\",\n        \"Language models are few-shot learners\",\n        \"Probabilistic semantics for cost based abduction\",\n        \"Deep reasoning networks for unsupervised pattern de-mixing with constraint reasoning\",\n        \"Zero-shot commonsense question answering with cloze translation and consistency optimization\",\n        \"Back to square one: Artifact detection, training and commonsense disentanglement in the winograd schema\",\n        \"Social chemistry 101: Learning to reason about social and moral norms\",\n        \"Posterior regularization for structured latent variable models\",\n        \"Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets\",\n        \"Explanation\",\n        \"Improving unsupervised commonsense reasoning using knowledge-enabled natural language inference\",\n        \"Rationalizing neural predictions\",\n        \"The winograd schema challenge\",\n        \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Unsupervised text generation by learning from search\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Knowledge-driven data construction for zero-shot evaluation in commonsense question answering\",\n        \"Show your work: Scratchpads for intermediate computation with language models\",\n        \"Approaches to abductive reasoning: an overview\",\n        \"The book of why: the new science of cause and effect\",\n        \"Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning\",\n        \"Robust textual inference via learning and abductive reasoning\",\n        \"Explain yourself! leveraging language models for commonsense reasoning\",\n        \"Thinking like a skeptic: Defeasible inference in natural language\",\n        \"Atomic: An atlas of machine commonsense for if-then reasoning\",\n        \"Unsupervised commonsense question answering with self-talk\",\n        \"Recent advances in natural language inference: A survey of benchmarks, resources, and approaches\",\n        \"Proofwriter: Generating implications, proofs, and abductive statements over natural language\",\n        \"Abductive reasoning: Logic, visual thinking, and coherence\",\n        \"Rationales for sequential predictions\",\n        \"Investigating gender bias in language models using causal mediation analysis\",\n        \"Glue: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"SemEval-2020 task 4: Commonsense validation and explanation\",\n        \"Does it make sense? and why? a pilot study for sense making and explanation\",\n        \"Finetuned language models are zero-shot learners\",\n        \"Chain of thought prompting elicits reasoning in large language models\",\n        \"Transformers: State-of-the-art natural language processing\",\n        \"Winowhy: A deep diagnosis of essential commonsense knowledge for answering winograd schema challenge\",\n        \"ECNU-SenseMaker at SemEval-2020 task 4: Leveraging heterogeneous knowledge resources for commonsense validation and explanation\",\n        \"Think before you speak: Learning to generate implicit knowledge for response generation by self-talk\",\n        \"Probing commonsense explanation in dialogue response generation\",\n        \"Towards interpretable natural language understanding with explanations as latent variables\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}