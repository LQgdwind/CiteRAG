{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 416 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: An Overview of Multi-agent Reinforcement Learning from Game Theoretical Perspective\n\nPaper Abstract: Abstract Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. Our work is separated into two parts. From §§\\S1 to §§\\S4, we present the self-contained fundamental knowledge of MARL, including problem formulations, basic solutions, and existing challenges. Specifically, we present the MARL formulations through two representative frameworks, namely, stochastic games and extensive-form games, along with different variations of games that can be addressed. The goal of this part is to enable the readers, even those with minimal related background, to grasp the key ideas in MARL research. From §§\\S5 to §§\\S9, we present an overview of recent developments of MARL algorithms. Starting from new taxonomies for MARL methods, we conduct a survey of previous survey papers. In later sections, we highlight several modern topics in MARL research, including Q-function factorisation, multi-agent soft learning, networked multi-agent MDP, stochastic potential games, zero-sum continuous games, online MDP, turn-based stochastic games, policy space response oracle, approximation methods in general-sum games, and mean-field type learning in games with infinite agents. Within each topic, we select both the most fundamental and cutting-edge algorithms. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.\n\n{retrieved_text}\n\nGenerate exactly 416 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 416\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Politex: Regret bounds for policy iteration using expert prediction\",\n        \"A multiagent reinforcement learning algorithm with non-linear dynamics\",\n        \"Wasserstein robust reinforcement learning\",\n        \"The equivalence of linear programs and zero-sum games\",\n        \"A cooperative multi-agent transportation management and route guidance system\",\n        \"Local saddle point optimization: A curvature exploitation approach\",\n        \"Model-free q-learning designs for linear discrete-time zero-sum games with application to h-infinity control\",\n        \"Optimizing fixed-size stochastic controllers for pomdps and decentralized pomdps\",\n        \"Fitted q-learning in mean-field games\",\n        \"Q-learning in regularized mean-field games\",\n        \"A maximum principle for sdes of mean-field type\",\n        \"Decentralized q-learning for stochastic teams and games\",\n        \"Gambling in a rigged casino: The adversarial multi-armed bandit problem\",\n        \"The nonstochastic multiarmed bandit problem\",\n        \"Near-optimal regret bounds for reinforcement learning\",\n        \"Minimax regret bounds for reinforcement learning\",\n        \"Emergent tool use from multi-agent autocurricula\",\n        \"Open-ended learning in symmetric zero-sum games\",\n        \"The mechanics of n-player differentiable games\",\n        \"Re-evaluating evaluation\",\n        \"On the theory of dynamic programming\",\n        \"Consistency of vanishingly smooth fictitious play\",\n        \"Mixed equilibria and dynamical systems arising from fictitious play in perturbed games\",\n        \"Partitioning procedures for solving mixed-variable program-ming problems, numerische matkematic 4\",\n        \"Learning deep architectures for AI\",\n        \"Mean field games and mean field type control theory, volume 101\",\n        \"Brown’s original fictitious play\",\n        \"Policy iteration for decentralized control of markov decision processes\",\n        \"The complexity of decentralized control of markov decision processes\",\n        \"The dynamic programming algorithm\",\n        \"Neuro-dynamic programming\",\n        \"Approximating game-theoretic optimal strategies for full-scale poker\",\n        \"An analog of the minimax theorem for vector payoffs\",\n        \"Variational inference: A review for statisticians\",\n        \"Evolutionary dynamics of multi-agent learning: A survey\",\n        \"The statistical mechanics of strategic interaction\",\n        \"Stochastic approximation with two time scales\",\n        \"Reinforcement learning in markovian evolutionary games\",\n        \"Decision-theoretic planning: Structural assumptions and computational leverage\",\n        \"Convergence problems of general-sum multiagent reinforcement learning\",\n        \"Convergence and no-regret in multiagent learning\",\n        \"An analysis of stochastic game theory for multiagent reinforcement learning\",\n        \"Rational and convergent learning in stochastic games\",\n        \"Multiagent learning using a variable learning rate\",\n        \"R-max-a general polynomial time algorithm for near-optimal reinforcement learning\",\n        \"On the computation of equilibria in discounted stochastic dynamic games\",\n        \"Dynamic thresholding and pruning for regret minimization\",\n        \"Deep counterfactual regret minimization\",\n        \"Regret-based pruning in extensive-form games\",\n        \"Reduced space and faster convergence in imperfect-information games via pruning\",\n        \"Superhuman ai for heads-up no-limit poker: Libratus beats top professionals\",\n        \"Superhuman ai for multiplayer poker\",\n        \"Language models are few-shot learners\",\n        \"A survey of monte carlo tree search methods\",\n        \"Global convergence of policy gradient for sequential zero-sum linear quadratic dynamic games\",\n        \"A general stochastic maximum principle for sdes of mean-field type\",\n        \"Efficient monte carlo counterfactual regret minimization in games with many player actions\",\n        \"Multi-agent reinforcement learning: An overview\",\n        \"Provably efficient exploration in policy optimization\",\n        \"Neural temporal-difference learning converges to global optima\",\n        \"Sophisticated experience-weighted attraction learning and strategic teaching in repeated games\",\n        \"A cognitive hierarchy model of games\",\n        \"Multiagent systems in automotive applications\",\n        \"Flows and decompositions of games: Harmonic and potential games\",\n        \"Learning to rank: from pairwise approach to listwise approach\",\n        \"The master equation and the convergence problem in mean field games\",\n        \"Learning in mean field games: the fictitious play\",\n        \"Mean field game of controls and an application to trade crowding\",\n        \"Probabilistic analysis of mean-field games\",\n        \"Forward–backward stochastic differential equations and controlled mckean–vlasov dynamics\",\n        \"Probabilistic Theory of Mean Field Games with Applications I-II\",\n        \"Control of mckean–vlasov dynamics versus mean field games\",\n        \"Mean field games with common noise\",\n        \"A probabilistic weak formulation of mean field games and applications\",\n        \"Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods\",\n        \"Model-free mean-field reinforcement learning: mean-field mdp and mean-field q-learning\",\n        \"On the convergence problem in mean field games: a two state model without uniqueness\",\n        \"Prediction, learning, and games\",\n        \"Is reinforcement learning worth the hype? 2020\",\n        \"Convergence analysis of gradient-based learning with non-uniform learning rates in non-cooperative multi-agent settings\",\n        \"Settling the complexity of two-player nash equilibrium\",\n        \"Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism\",\n        \"The dynamics of reinforcement learning in cooperative multiagent systems\",\n        \"Complexity results about nash equilibria\",\n        \"Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents\",\n        \"New complexity results about nash equilibria\",\n        \"Neural correlates of depth of strategic reasoning in medial prefrontal cortex\",\n        \"Information set monte carlo tree search\",\n        \"A survey on transfer learning for multiagent reinforcement learning systems\",\n        \"Distributed optimal power flow for smart microgrids\",\n        \"The complexity of computing a nash equilibrium\",\n        \"Training gans with optimism\",\n        \"The limit points of (optimistic) gradient descent in min-max optimization\",\n        \"Three-player games are hard\",\n        \"Pilco: A model-based and data-efficient approach to policy search\",\n        \"Selection of equilibria in a linear quadratic mean-field game\",\n        \"A review on the applications of multiagent systems in wireless sensor networks\",\n        \"Solving stochastic games\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Learning to act in decentralized partially observable mdps\",\n        \"Optimally solving dec-pomdps as continuous-state mdps\",\n        \"Online learning in markov decision processes with changing cost sequences\",\n        \"Mckean-vlasov optimal control: the dynamic programming principle\",\n        \"Approximate fictitious play for mean field games\",\n        \"On the convergence of model free learning in mean field games\",\n        \"Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria\",\n        \"Experts in a markov decision process\",\n        \"Online markov decision processes\",\n        \"Learning rates for q-learning\",\n        \"Global convergence of policy gradient methods for the linear quadratic regulator\",\n        \"Total expected discounted reward mdps: existence of optimal policies\",\n        \"Convergence of learning dynamics in stackelberg games\",\n        \"Competitive Markov decision processes\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"On the connection between symmetric n𝑛n-player games and mean field games\",\n        \"Learning with opponent-learning awareness\",\n        \"Counterfactual multi-agent policy gradients\",\n        \"Stabilising experience replay for deep multi-agent reinforcement learning\",\n        \"A decision-theoretic generalization of on-line learning and an application to boosting\",\n        \"Actor-critic provably finds nash equilibria of linear-quadratic mean-field games\",\n        \"The theory of learning in games, volume 2\",\n        \"Learning mixed equilibria\",\n        \"Consistency and cautious fictitious play\",\n        \"Partially observable mean field reinforcement learning\",\n        \"A comprehensive survey on safe reinforcement learning\",\n        \"On the mckean-vlasov limit for interacting diffusions\",\n        \"Distributed Artificial Intelligence: Volume II, volume 2\",\n        \"Generalized sampling and variance in counterfactual regret minimization\",\n        \"Bounded rationality: The adaptive toolbox\",\n        \"Gradient-based algorithms for finding nash equilibria in extensive form games\",\n        \"Finding equilibria in large sequential games of imperfect information\",\n        \"Discrete–time stochastic control and dynamic potential games: the Euler–Equation approach\",\n        \"Generative adversarial nets\",\n        \"Explaining and harnessing adversarial examples\",\n        \"No-regret algorithms for online convex programs\",\n        \"Balancing two-player stochastic games with soft q-learning\",\n        \"Correlated q-learning\",\n        \"Dynamic programming principles for learning mfcs\",\n        \"Q-learning for mean-field controls\",\n        \"Mean field games and applications\",\n        \"Multiagent planning with factored mdps\",\n        \"Coordinated reinforcement learning\",\n        \"Learning mean-field games\",\n        \"A general framework for learning mean-field games\",\n        \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\",\n        \"Finite mean field games: fictitious play and convergence to a first order continuous mean field game\",\n        \"Dream to control: Learning behaviors by latent imagination\",\n        \"Learning latent dynamics for planning from pixels\",\n        \"Approximation to bayes risk in repeated play\",\n        \"Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)\",\n        \"Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor\",\n        \"Simple adaptive strategies: from regret-matching to uncoupled dynamics, volume 4\",\n        \"A reinforcement procedure leading to correlated equilibrium\",\n        \"Fictitious self-play in extensive-form games\",\n        \"Deep reinforcement learning from self-play in imperfect-information games\",\n        \"Neural replicator dynamics\",\n        \"Homotopy methods to compute equilibria in game theory\",\n        \"Stationary equilibria in stochastic games: Structure, selection, and computation\",\n        \"A survey of learning in multiagent environments: Dealing with non-stationarity\",\n        \"A survey and critique of multiagent deep reinforcement learning\",\n        \"Gans trained by a two time-scale update rule converge to a local nash equilibrium\",\n        \"Differential topology, volume 33\",\n        \"On the global convergence of stochastic fictitious play\",\n        \"Nash q-learning for general-sum stochastic games\",\n        \"Multiagent reinforcement learning: theoretical framework and an algorithm\",\n        \"Mean-field langevin dynamics and energy landscape of neural networks\",\n        \"Large population stochastic dynamic games: closed-loop mckean-vlasov systems and the nash certainty equivalence principle\",\n        \"Distributed Artificial Intelligence: Volume I, volume 1\",\n        \"Mean field equilibria of dynamic auctions with learning\",\n        \"Human-level performance in 3d multiplayer games with population-based reinforcement learning\",\n        \"An overview of cooperative and competitive multiagent learning\",\n        \"Feature-based q-learning for two-player stochastic games\",\n        \"What is local optimality in nonconvex-nonconcave minimax optimization?\",\n        \"Regret minimization for partially observable deep reinforcement learning\",\n        \"Finding optimal abstract strategies in extensive-form games\",\n        \"Efficient nash equilibrium approximation through monte carlo counterfactual regret minimization\",\n        \"Anonymous sequential games\",\n        \"More is the same; phase transitions and mean field theories\",\n        \"Reinforcement learning: A survey\",\n        \"Learning dynamics in games with stochastic perturbations\",\n        \"Graphical games\",\n        \"Graphical models for game theory\",\n        \"Swarm intelligence\",\n        \"The General Theory of Employment, Interest and Money\",\n        \"Brain function and adaptive systems: a heterostatic theory\",\n        \"Reinforcement learning in robotics: A survey\",\n        \"Bandit based monte-carlo planning\",\n        \"Sparse cooperative q-learning\",\n        \"The complexity of two-person zero-sum games in extensive form\",\n        \"Finding mixed strategies with small supports in extensive form games\",\n        \"Actor-critic algorithms\",\n        \"An accelerated inexact proximal point method for solving nonconvex-concave min-max problems\",\n        \"Rethinking formal models of partially observable multiagent decision making\",\n        \"Reputation and imperfect information\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Extensive games\",\n        \"A simplified two-person poker\",\n        \"Determinantal point processes for machine learning\",\n        \"Potential Game Theory\",\n        \"Mean field games via controlled martingale problems: existence of markovian equilibria\",\n        \"Limit theory for controlled mckean–vlasov dynamics\",\n        \"On the convergence of closed-loop nash equilibria to the mean field game limit\",\n        \"Mean field and n-agent games for optimal investment under relative performance criteria\",\n        \"Learning in zero-sum team markov games using factored value functions\",\n        \"Monte carlo sampling for regret minimization in extensive games\",\n        \"A unified game-theoretic approach to multiagent reinforcement learning\",\n        \"Batch reinforcement learning\",\n        \"Mean field games\",\n        \"An algorithm for distributed reinforcement learning in cooperative multi-agent systems\",\n        \"Dynamic programming for mean-field type control\",\n        \"Deep learning\",\n        \"A mean field game of portfolio trading and its consequences on perceived correlations\",\n        \"Equilibrium points of bimatrix games\",\n        \"Convergent multiple-timescales reinforcement learning algorithms in normal form games\",\n        \"Individual q-learning in normal form games\",\n        \"Generalised weakened fictitious play\",\n        \"Reinforcement learning and control as probabilistic inference: Tutorial and review\",\n        \"Local-effect games\",\n        \"Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning\",\n        \"Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient\",\n        \"Deep reinforcement learning: An overview\",\n        \"Sampled fictitious play is hannan consistent\",\n        \"Continuous control with deep reinforcement learning\",\n        \"On gradient descent ascent for nonconvex-concave minimax problems\",\n        \"Convergence of monte carlo tree search in simultaneous move games\",\n        \"The weighted majority algorithm\",\n        \"Markov games as a framework for multi-agent reinforcement learning\",\n        \"Friend-or-foe q-learning in general-sum games\",\n        \"Value-function reinforcement learning in markov games\",\n        \"Neural proximal/trust region policy optimization attains globally optimal policy\",\n        \"Computing approximate equilibria in sequential adversarial games by exploitability descent\",\n        \"Multi-agent actor-critic for mixed cooperative-competitive environments\",\n        \"Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications\",\n        \"A mean-field analysis of deep resnet and beyond: Towards provable optimization via overparameterization from depth\",\n        \"Natural actor-critic converges globally for hierarchical linear quadratic regulator\",\n        \"Learning parametric closed-loop policies for markov potential games\",\n        \"Average reward reinforcement learning: Foundations, algorithms, and empirical results\",\n        \"The empirical bayes envelope and regret minimization in competitive markov decision processes\",\n        \"Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems\",\n        \"Understanding and using linear programming\",\n        \"On evolution\",\n        \"On the convergence of gradient-based learning in continuous games\",\n        \"Policy gradient in linear quadratic dynamic games has no convergence guarantees\",\n        \"On finding local nash equilibria (and only local nash equilibria) in zero-sum games\",\n        \"Propagation of chaos for a class of non-linear parabolic equations\",\n        \"Planning in the presence of cost functions controlled by an adversary\",\n        \"Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile\",\n        \"Learning in games with continuous action sets and unknown payoff functions\",\n        \"Which training methods for gans do actually converge?\",\n        \"The numerics of gans\",\n        \"Stochastic potential games\",\n        \"Algorithmic game theory lecture notes\",\n        \"Steps toward artificial intelligence\",\n        \"Theory of neural-analog reinforcement systems and its application to the brain model problem\",\n        \"Human-level control through deep reinforcement learning\",\n        \"Potential games\",\n        \"Deepstack: Expert-level artificial intelligence in heads-up no-limit poker\",\n        \"Robust reinforcement learning\",\n        \"Mean-field markov decision processes with common noise and open-loop controls\",\n        \"Application impact of multi-agent systems and technologies: A survey\",\n        \"A generalized training approach for multiagent learning\",\n        \"Finite-time bounds for fitted value iteration\",\n        \"Gradient descent gan optimization is locally stable\",\n        \"Non-cooperative games\",\n        \"Decentralized stochastic control with partial history sharing: A common information approach\",\n        \"Achieving geometric convergence for distributed optimization over time-varying graphs\",\n        \"Distributed subgradient methods for multi-agent optimization\",\n        \"Problem complexity and method efficiency in optimization\",\n        \"Online markov decision processes under bandit feedback\",\n        \"A unified view of entropy-regularized markov decision processes\",\n        \"Zur theorie der gesellschaftsspiele\",\n        \"Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications\",\n        \"Solving a class of non-convex min-max games using iterative first order methods\",\n        \"Game theory and multi-agent reinforcement learning\",\n        \"Convergence to the mean field game limit: a case study\",\n        \"A concise introduction to decentralized POMDPs, volume 1\",\n        \"α𝛼\\\\alpha-rank: Multi-agent evaluation by evolution\",\n        \"Navigating the landscape of games\",\n        \"A review of cooperative multi-agent deep reinforcement learning\",\n        \"Variational regret bounds for reinforcement learning\",\n        \"A course in game theory\",\n        \"Openai five, 2018\",\n        \"Cooperative multi-agent learning: The state of the art\",\n        \"The complexity of markov decision processes\",\n        \"Comparative evaluation of multi-agent deep reinforcement learning algorithms\",\n        \"Mixed-strategy learning with continuous action sets\",\n        \"Actor-critic fictitious play in simultaneous move multistage games\",\n        \"Natural actor-critic\",\n        \"Discrete time mckean–vlasov control problem: a dynamic programming approach\",\n        \"Dynamic programming for optimal control of stochastic mckean–vlasov dynamics\",\n        \"Bellman equation and viscosity solutions for mean-field stochastic control problem\",\n        \"Learning against opponents with bounded memory\",\n        \"New criteria and a new algorithm for learning in multi-agent systems\",\n        \"Two-timescale algorithms for learning nash equilibria in general-sum stochastic games\",\n        \"Non-convex min-max optimization: Provable algorithms and applications in machine learning\",\n        \"Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning\",\n        \"Characterization and computation of local nash equilibria in continuous games\",\n        \"Genericity and structural stability of non-degenerate differential nash equilibria\",\n        \"Factorization machines\",\n        \"Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method\",\n        \"A survey of multi-objective sequential decision-making\",\n        \"Online convex optimization in adversarial markov decision processes\",\n        \"Promp: Proximal meta-policy search\",\n        \"Markov–nash equilibria in mean-field games with discounted cost\",\n        \"Approximate nash equilibria in partially observed stochastic games with mean-field interactions\",\n        \"Comparing uct versus cfr in simultaneous games\",\n        \"Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines\",\n        \"Deep learning in neural networks: An overview\",\n        \"Experiments on decisions under risk: The expected utility hypothesis\",\n        \"Mastering atari, go, chess and shogi by planning with a learned model\",\n        \"Trust region policy optimization\",\n        \"Proximal policy optimization algorithms\",\n        \"Spieltheoretische behandlung eines oligopolmodells mit nachfrageträgheit: Teil i: Bestimmung des dynamischen preisgleichgewichts\",\n        \"Multi-agent system applications in healthcare: current technology and future roadmap\",\n        \"Understanding machine learning: From theory to algorithms\",\n        \"Online learning and online convex optimization\",\n        \"Safe, multi-agent, reinforcement learning for autonomous driving\",\n        \"Stochastic games\",\n        \"A note on the lemke-howson algorithm\",\n        \"Soft policy gradient method for maximum entropy deep reinforcement learning\",\n        \"Multiagent systems: Algorithmic, game-theoretic, and logical foundations\",\n        \"If multi-agent learning is the answer, what is the question?\",\n        \"Global stability of dynamical systems\",\n        \"Near-optimal time and sample complexities for solving markov decision processes with a generative model\",\n        \"Solving discounted stochastic two-player games with near-optimal time and sample complexity\",\n        \"Mastering the game of go with deep neural networks and tree search\",\n        \"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play\",\n        \"Deterministic policy gradient algorithms\",\n        \"Mastering the game of go without human knowledge\",\n        \"Theories of bounded rationality\",\n        \"Nash convergence of gradient dynamics in general-sum games\",\n        \"Mean field analysis of neural networks: A law of large numbers\",\n        \"The logic of animal conflict\",\n        \"Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning\",\n        \"A mean field view of the landscape of two-layers neural networks\",\n        \"On the universality of online mirror descent\",\n        \"Actor-critic policy optimization in partially observable multiagent environments\",\n        \"Multiagent systems: A survey from a machine learning perspective\",\n        \"Reinforcement learning in stationary mean-field games\",\n        \"Multi type mean field reinforcement learning\",\n        \"Value-decomposition networks for cooperative multi-agent learning based on team reward\",\n        \"A multi-agent off-policy actor-critic algorithm for distributed reinforcement learning\",\n        \"Learning to predict by the methods of temporal differences\",\n        \"Reinforcement learning: An introduction, volume 1\",\n        \"Policy gradient methods for reinforcement learning with function approximation\",\n        \"Smooth fictitious play in n×\\\\times 2 potential games\",\n        \"Apprenticeship learning using linear programming\",\n        \"Algorithms for reinforcement learning\",\n        \"A unified analysis of value-function-based reinforcement-learning algorithms\",\n        \"Maa*: A heuristic search algorithm for solving decentralized pomdps\",\n        \"Topics in propagation of chaos\",\n        \"Solving heads-up limit texas hold’em\",\n        \"Transfer learning for reinforcement learning domains: A survey\",\n        \"Pettingzoo: Gym for multi-agent reinforcement learning\",\n        \"Temporal difference learning and td-gammon\",\n        \"Efficient algorithms for smooth minimax optimization\",\n        \"Animal intelligence: an experimental study of the associative processes in animals\",\n        \"A regularized opponent model with maximum entropy objective\",\n        \"Hierarchical pomdp controller optimization by likelihood maximization\",\n        \"An optimization approach for approximate nash equilibria\",\n        \"Evolutionary game theory and multi-agent reinforcement learning\",\n        \"What evolutionary game theory tells us about multiagent learning\",\n        \"A generalised method for empirical game theoretic analysis\",\n        \"Multiagent learning: Basics, challenges, and prospects\",\n        \"Approximate equilibrium computation for discrete-time linear-quadratic mean-field games\",\n        \"Reinforcement learning and markov decision processes\",\n        \"Grandmaster level in starcraft ii using multi-agent reinforcement learning\",\n        \"Starcraft ii: A new challenge for reinforcement learning\",\n        \"No-regret dynamics and fictitious play\",\n        \"Theory of games and economic behavior\",\n        \"Theory of games and economic behavior (commemorative edition)\",\n        \"Neural policy gradient methods: Global optimality and rates of convergence\",\n        \"Reinforcement learning to play an optimal nash equilibrium in team markov games\",\n        \"Solving games with functional regret estimation\",\n        \"The optimal reward baseline for gradient-based reinforcement learning\",\n        \"Online reinforcement learning in stochastic games\",\n        \"Multiagent systems: a modern approach to distributed artificial intelligence\",\n        \"L’hypothèse du champ moléculaire et la propriété ferromagnétique\",\n        \"Methods for empirical game-theoretic analysis\",\n        \"Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning\",\n        \"Probabilistic recursive reasoning for multi-agent reinforcement learning\",\n        \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"An introduction to multiagent systems\",\n        \"Rollout sampling policy iteration for decentralized pomdps\",\n        \"Monte-carlo expectation maximization for decentralized pomdps\",\n        \"Multiagent planning with trembling-hand perfect equilibrium in multiagent pomdps\",\n        \"Online learning in markov decision processes with adversarially chosen transition probability distributions\",\n        \"Learning deep mean field games for modeling large population behavior\",\n        \"Mean field multi-agent reinforcement learning\",\n        \"Alpha-alpha-rank: Scalable multi-agent evaluation through evolution\",\n        \"Multi-agent determinantal q-learning\",\n        \"Provably global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost\",\n        \"A theoretical analysis of deep q-learning\",\n        \"A new complexity result on solving the markov decision problem\",\n        \"The simplex method is strongly polynomial for the markov decision problem with a fixed discount rate\",\n        \"Learning team-optimality for decentralized stochastic control and dynamic games\",\n        \"The evolution of conventions\",\n        \"Markov decision processes with arbitrary reward processes\",\n        \"Dynamic potential games in communications: Fundamentals and applications\",\n        \"On an application of set theory to the theory of the game of chess\",\n        \"Multi-agent learning with policy prediction\",\n        \"Convergence of gradient methods on bilinear zero-sum games\",\n        \"Bi-level actor-critic for multi-agent coordination\",\n        \"Robust multi-agent reinforcement learning with model uncertainty\",\n        \"Networked multi-agent reinforcement learning in continuous spaces\",\n        \"Multi-agent reinforcement learning: A selective overview of theories and algorithms\",\n        \"Policy optimization provably converges to nash equilibria in zero-sum linear quadratic games\",\n        \"Finite-sample analysis for decentralized batch multi-agent reinforcement learning with networked agents\",\n        \"Fully decentralized multi-agent reinforcement learning with networked agents\",\n        \"Distributed off-policy actor-critic reinforcement learning with policy consensus\",\n        \"Analysis and improvement of policy gradient estimation\",\n        \"Factorized q-learning for large-scale multi-agent systems\",\n        \"Online learning in episodic markovian decision processes by relative entropy policy search\",\n        \"Online convex programming and generalized infinitesimal gradient ascent\",\n        \"Cyclic equilibria in markov games\",\n        \"Regret minimization in games with incomplete information\"\n    ],\n    \"reasoning\": \"Selected these 416 references because they represent foundational work and recent advances in An Overview of Multi-agent Reinforcement Learning from Game Theoretical Perspective, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}