{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 183 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Mixed-Precision Neural Networks: A Survey\n\nPaper Abstract: Abstract Mixed-precision Deep Neural Networks achieve the energy efficiency and throughput needed for hardware deployment, particularly when the resources are limited, without sacrificing accuracy. However, the optimal per-layer bit precision that preserves accuracy is not easily found, especially with the abundance of models, datasets, and quantization techniques that creates an enormous search space. In order to tackle this difficulty, a body of literature has emerged recently, and several frameworks that achieved promising accuracy results have been proposed. In this paper, we start by summarizing the quantization techniques used generally in literature. Then, we present a thorough survey of the mixed-precision frameworks, categorized according to their optimization techniques such as reinforcement learning and quantization techniques like deterministic rounding. Furthermore, the advantages and shortcomings of each framework are discussed, where we present a juxtaposition. We finally give guidelines for future mixed-precision frameworks.\n\n{retrieved_text}\n\nGenerate exactly 183 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 183\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep neural networks for object detection\",\n        \"Deeptest: Automated testing of deep-neural-network-driven autonomous cars\",\n        \"Toward collaborative inferencing of deep neural networks on internet-of-things devices\",\n        \"Cadd-splice—improving genome-wide variant effect prediction using deep learning-derived splice scores\",\n        \"Enabling cognitive smart cities using big data and machine learning: Approaches and challenges\",\n        \"Importance estimation for neural network pruning\",\n        \"Pruning filters for efficient convnets\",\n        \"Learning both weights and connections for efficient neural network\",\n        \"Optimal brain damage\",\n        \"Designing energy-efficient convolutional neural networks using energy-aware pruning\",\n        \"Exploring the regularity of sparse structure in convolutional neural networks\",\n        \"Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size\",\n        \"Mobilenetv2: Inverted residuals and linear bottlenecks\",\n        \"Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"Squeezenext: Hardware-aware neural network design\",\n        \"Efficient methods and hardware for deep learning\",\n        \"Searching for mobilenetv3\",\n        \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\",\n        \"Distilling the knowledge in a neural network\",\n        \"Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy\",\n        \"Model compression via distillation and quantization\",\n        \"Dreaming to distill: Data-free knowledge transfer via deepinversion\",\n        \"Experimental determination of precision requirements for back-propagation training of artificial neural networks\",\n        \"Lsq+: Improving low-bit quantization through learnable offsets and better initialization\",\n        \"One weight bitwidth to rule them all\",\n        \"Binarized neural networks\",\n        \"Quantization and training of neural networks for efficient integer-arithmetic-only inference\",\n        \"Xnor-net: Imagenet classification using binary convolutional neural networks\",\n        \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks\",\n        \"Incremental network quantization: Towards lossless cnns with low-precision weights\",\n        \"Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network\",\n        \"Hawq: Hessian aware quantization of neural networks with mixed-precision\",\n        \"Bit-mixer: Mixed-precision networks with runtime bit-width selection\",\n        \"Weight discretization paradigm for optical neural networks\",\n        \"Zeroq: A novel zero shot quantization framework\",\n        \"1.1 computing’s energy problem (and what we can do about it)\",\n        \"Rethinking differentiable search for mixed-precision neural networks\",\n        \"Binary neural networks: A survey\",\n        \"Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients\",\n        \"Pact: Parameterized clipping activation for quantized neural networks\",\n        \"Low-bit quantization of neural networks for efficient inference\",\n        \"Post-training 4-bit quantization of convolution networks for rapid-deployment\",\n        \"Quantizing deep convolutional networks for efficient inference: A whitepaper\",\n        \"Towards efficient training for neural network quantization\",\n        \"Ternary neural networks with fine-grained quantization\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"Value-aware quantization for training and inference of neural networks\",\n        \"Mxqn: mixed quantization for reducing bit-width of weights and activations in deep convolutional neural networks\",\n        \"Releq: A reinforcement learning approach for deep quantization of neural networks\",\n        \"Learning and generalization in overparameterized neural networks, going beyond two layers\",\n        \"M. Mahoney et al\",\n        \"“A survey on methods and theories of quantized neural networks\",\n        \"“A survey of quantization methods for efficient neural network inference\",\n        \"“Binaryconnect: Training deep neural networks with binary weights during propagations\",\n        \"K. Gopalakrishnan\",\n        \"“Training and inference with integers in deep neural networks\",\n        \"“Towards the limit of network quantization\",\n        \"“Quantized convolutional neural networks for mobile devices\",\n        \"“Weighted-entropy-based quantization for deep neural networks\",\n        \"“Compressing deep convolutional networks using vector quantization\",\n        \"“Ternary weight networks\",\n        \"“Extremely low bit neural network: Squeeze the last bit out with admm\",\n        \"“Adaptive quantization of neural networks\",\n        \"“Loss-aware binarization of deep networks\",\n        \"“Fixed-point feedforward deep neural network design using weights+ 1\",\n        \"“Trained ternary quantization\",\n        \"“Multilayer feedforward neural networks with single powers-of-two weights\",\n        \"“Variational network quantization\",\n        \"“Rounding methods for neural networks with low resolution synaptic weights\",\n        \"“Neural networks with few multiplications\",\n        \"“Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights.” in NIPS\",\n        \"“Learning discrete weights using the local reparameterization trick\",\n        \"“An introduction to variational methods for graphical models\",\n        \"“Mixed precision quantization of convnets via differentiable neural architecture search\",\n        \"“Joint training of low-precision neural network with quantization interval parameters\",\n        \"“Training compact neural networks with binary weights and low precision activations\",\n        \"“1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns\",\n        \"“Scalable distributed dnn training using commodity gpu cloud computing\",\n        \"“Qsgd: Communication-efficient sgd via gradient quantization and encoding\",\n        \"“Communication quantization for data-parallel training of deep neural networks\",\n        \"“Evoq: Mixed precision quantization of dnns via sensitivity guided evolutionary search\",\n        \"“Deep learning with low precision by half-wave gaussian quantization\",\n        \"“Explicit loss-error-aware quantization for low-bit deep neural networks\",\n        \"“Discovering low-precision networks close to full-precision networks for efficient embedded inference\",\n        \"“Nvidia 8-bit inference with tensorrt\",\n        \"“Integer quantization for deep learning inference: Principles and empirical evaluation\",\n        \"“Hmq: Hardware friendly mixed precision quantization block for cnns\",\n        \"“Search what you want: Barrier panelty nas for mixed precision quantization\",\n        \"“Apq: Joint search for network architecture\",\n        \"“Joint neural architecture search and quantization\",\n        \"“Differentiable soft quantization: Bridging full-precision and low-bit neural networks\",\n        \"“Towards mixed-precision quantization of neural networks via constrained optimization\",\n        \"“Opq: Compressing deep neural networks with one-shot pruning-quantization\",\n        \"“Constructing energy-efficient mixed-precision neural networks through principal component analysis for edge intelligence\",\n        \"S.-M. Moosavi-Dezfooli\",\n        \"“Mixed-precision quantized neural network with progressively decreasing bitwidth for image classification and object detection\",\n        \"“Simple augmentation goes a long way: Adrl for dnn quantization\",\n        \"“Haq: Hardware-aware automated quantization with mixed precision\",\n        \"“Bayesian bits: Unifying quantization and pruning\",\n        \"“Differentiable quantization of deep neural networks\",\n        \"“Stochastic layer-wise precision in deep neural networks\",\n        \"V. Chandrasekhar\",\n        \"“Estimating or propagating gradients through stochastic neurons for conditional computation\",\n        \"“Dropout: a simple way to prevent neural networks from overfitting\",\n        \"“Categorical reparameterization with gumbel-softmax\",\n        \"“Gradient-based learning applied to document recognition\",\n        \"“Imagenet classification with deep convolutional neural networks\",\n        \"Constrained optimization and Lagrange multiplier methods. Academic press\",\n        \"“Clip-q: Deep network compression learning by in-parallel pruning-quantization\",\n        \"“Syq: Learning symmetric quantization for efficient deep neural networks\",\n        \"“Learning sparse neural networks through l​_​0𝑙_0l\\\\_0 regularization\",\n        \"“Auto-encoding variational bayes\",\n        \"“Stochastic backpropagation and approximate inference in deep generative models\",\n        \"“Relaxed quantization for discretized neural networks\",\n        \"“Learned step size quantization\",\n        \"“Up or down? adaptive rounding for post-training quantization\",\n        \"“Trained uniform quantization for accurate and efficient neural network inference on fixed-point hardware\",\n        \"“Mixed precision dnns: All you need is a good parametrization\",\n        \"“Fixed point quantization of deep convolutional networks\",\n        \"“Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm\",\n        \"“Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1\",\n        \"“Hawq-v2: Hessian aware trace-weighted quantization of neural networks\",\n        \"“Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix\",\n        \"“Fully quantized network for object detection\",\n        \"“Focal loss for dense object detection\",\n        \"“Microsoft coco: Common objects in context\",\n        \"“A low effort approach to structured cnn design using pca\",\n        \"“Hybrid binary networks: optimizing for accuracy\",\n        \"“Low-bit quantization of neural networks for efficient inference.” in ICCV Workshops\",\n        \"“Improving neural network quantization without retraining using outlier channel splitting\",\n        \"“Data-free quantization through weight equalization and bias correction\",\n        \"“The knowledge within: Methods for data-free model compression\",\n        \"“Tvm: end-to-end optimization stack for deep learning\",\n        \"“Pulp: a linear programming toolkit for python\",\n        \"” https://github.com/NVIDIA/cutlass\",\n        \"“Deep residual learning for image recognition\",\n        \"“Improving post training neural quantization: Layer-wise calibration and integer programming\",\n        \"“Learning compression from limited unlabeled data\",\n        \"“Deep neural network compression by in-parallel pruning-quantization\",\n        \"“Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach\",\n        \"“Towards accurate binary convolutional neural network\",\n        \"“Autoq: Automated kernel-wise neural network quantization\",\n        \"G. Hinton et al\",\n        \"“Identity mappings in deep residual networks\",\n        \"“Imagenet: A large-scale hierarchical image database\",\n        \"“Learning transferable architectures for scalable image recognition\",\n        \"“A comparative analysis of selection schemes used in genetic algorithms\",\n        \"“Efficient neural architecture search via parameters sharing\",\n        \"L. Van Der Maaten\",\n        \"“Progressive neural architecture search\",\n        \"“Regularized evolution for image classifier architecture search\",\n        \"“Darts: Differentiable architecture search\",\n        \"“Mixed precision neural architecture search for energy efficient deep learning\",\n        \"“Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"“Very deep convolutional networks for large-scale image recognition\",\n        \"“mixup: Beyond empirical risk minimization\",\n        \"“Neural architecture optimization\",\n        \"“Design automation for efficient deep learning computing\",\n        \"“Amc: Automl for model compression and acceleration on mobile devices\",\n        \"“Once-for-all: Train one network and specialize it for efficient deployment\",\n        \"“Single path one-shot neural architecture search with uniform sampling\",\n        \"“Proxylessnas: Direct neural architecture search on target task and hardware\",\n        \"“Interior point methods in semidefinite programming with applications to combinatorial optimization\",\n        \"E. Zheltonozhskii\",\n        \"“Ssd: Single shot multibox detector\",\n        \"“High-capacity expert binary networks\",\n        \"“Learning to quantize deep networks by optimizing quantization intervals with task loss\",\n        \"“Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks\",\n        \"“Wrpn: Wide reduced-precision networks\",\n        \"“Proximal policy optimization algorithms\",\n        \"“Stripes: Bit-serial deep neural network computing\",\n        \"“A unified framework of dnn weight pruning and weight clustering/quantization using admm\",\n        \"“Continuous control with deep reinforcement learning\",\n        \"“Adam: A method for stochastic optimization\",\n        \"“Bismo: A scalable bit-serial matrix multiplication overlay for reconfigurable computing\",\n        \"“Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n        \"“Data-efficient hierarchical reinforcement learning\",\n        \"“On learning-based methods for design-space exploration with high-level synthesis\",\n        \"“Primal: Power inference using machine learning\",\n        \"“Two-step quantization for low-bit neural networks\",\n        \"“Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation\",\n        \"“Balanced binary neural networks with gated residual\",\n        \"“Pruning algorithms-a survey\"\n    ],\n    \"reasoning\": \"Selected these 183 references because they represent foundational work and recent advances in Mixed-Precision Neural Networks: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}