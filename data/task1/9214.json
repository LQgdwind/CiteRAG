{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Memory-Constrained Algorithms for Convex Optimization via Recursive Cutting-Planes\n\nPaper Abstract: Abstract We propose a family of recursive cutting-plane algorithms to solve feasibility problems with constrained memory, which can also be used for first-order convex optimization. Precisely, in order to find a point within a ball of radius œµitalic-œµ\\epsilon with a separation oracle in dimension dùëëd‚Äîor to minimize 111-Lipschitz convex functions to accuracy œµitalic-œµ\\epsilon over the unit ball‚Äîour algorithms use ùí™‚Äã(d2p‚Äãln‚Å°1œµ)ùí™superscriptùëë2ùëù1italic-œµ\\mathcal{O}(\\frac{d^{2}}{p}\\ln\\frac{1}{\\epsilon}) bits of memory, and make ùí™‚Äã((C‚Äãdp‚Äãln‚Å°1œµ)p)ùí™superscriptùê∂ùëëùëù1italic-œµùëù\\mathcal{O}((C\\frac{d}{p}\\ln\\frac{1}{\\epsilon})^{p}) oracle calls, for some universal constant C‚â•1ùê∂1C\\geq 1. The family is parametrized by p‚àà[d]ùëùdelimited-[]ùëëp\\in[d] and provides an oracle-complexity/memory trade-off in the sub-polynomial regime ln‚Å°1œµ‚â´ln‚Å°dmuch-greater-than1italic-œµùëë\\ln\\frac{1}{\\epsilon}\\gg\\ln d. While several works gave lower-bound trade-offs (impossibility results) [29, 5]‚Äîwe explicit here their dependence with ln‚Å°1œµ1italic-œµ\\ln\\frac{1}{\\epsilon}, showing that these also hold in any sub-polynomial regime‚Äîto the best of our knowledge this is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with œµ‚â§1/ditalic-œµ1ùëë\\epsilon\\leq 1/\\sqrt{d}. The algorithms divide the dùëëd variables into pùëùp blocks and optimize over blocks sequentially, with approximate separation vectors constructed using a variant of Vaidya‚Äôs method. In the regime œµ‚â§d‚àíŒ©‚Äã(d)italic-œµsuperscriptùëëŒ©ùëë\\epsilon\\leq d^{-\\Omega(d)}, our algorithm with p=dùëùùëëp=d achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"On Vaidya‚Äôs volumetric cutting plane method for convex programming\",\n        \"Towards a practical volumetric cutting plane method for convex programming\",\n        \"Non-euclidean restricted memory level method for large-scale convex optimization\",\n        \"Solving convex programs by random walks\",\n        \"Quadratic Memory is Necessary for Optimal Query Complexity in Convex Optimization: Center-of-Mass is Pareto-Optimal\",\n        \"Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality\",\n        \"The Convergence of a Class of Double-rank Minimization Algorithms 1. General Considerations\",\n        \"Convex Optimization: Algorithms and Complexity\",\n        \"A geometric alternative to Nesterov‚Äôs accelerated gradient descent\",\n        \"Acceleration Methods\",\n        \"Space lower bounds for linear prediction in the streaming model\",\n        \"Detecting Correlations with Little Memory and Communication\",\n        \"Fast linear algebra is stable\",\n        \"Optimized first-order methods for smooth convex minimization\",\n        \"Performance of first-order methods for smooth convex minimization: a novel approach\",\n        \"An Optimal First Order Method Based on Optimal Quadratic Averaging\",\n        \"On the optimality of the random hyperplane rounding technique for MAX CUT\",\n        \"A new approach to variable metric algorithms\",\n        \"A Family of Variable-Metric Methods Derived by Variational Means\",\n        \"An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games, and Its Applications\",\n        \"Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales\",\n        \"Bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization\",\n        \"Communication-efficient algorithms for decentralized and stochastic optimization\",\n        \"Bundle Methods for Machine Learning\",\n        \"A faster cutting plane method and its implications for combinatorial and convex optimization\",\n        \"New variants of bundle methods\",\n        \"Nonsmooth optimization via quasi-Newton methods\",\n        \"On the limited memory BFGS method for large scale optimization\",\n        \"Efficient convex optimization requires superlinear memory\",\n        \"Mixing Implies Lower Bounds for Space Bounded Learning\",\n        \"D-ADMM: A Communication-Efficient Distributed Algorithm for Separable Optimization\",\n        \"Problem Complexity and Method Efficiency in Optimization\",\n        \"A method of solving a convex programming problem with convergence rate O‚Äã(1/k2)ùëÇ1superscriptùëò2O(1/k^{2})\",\n        \"Introductory lectures on convex optimization: A basic course\",\n        \"Updating Quasi-Newton Matrices with Limited Storage\",\n        \"A long step cutting plane algorithm that uses the volumetric barrier\",\n        \"A Time-Space Lower Bound for a Large Class of Learning Problems\",\n        \"AIDE: Fast and Communication Efficient Distributed Optimization\",\n        \"The Fastest Known Globally Convergent First-Order Method for Minimizing Strongly Convex Functions\",\n        \"Communication-Efficient Distributed Optimization using an Approximate Newton-type Method\",\n        \"Conditioning of Quasi-Newton Methods for Function Minimization\",\n        \"Memory-Sample Tradeoffs for Linear Regression with Small Error\",\n        \"CoCoA: A General Framework for Communication-Efficient Distributed Optimization\",\n        \"An optimal gradient method for smooth strongly convex minimization\",\n        \"Bundle Methods for Regularized Risk Minimization\",\n        \"A new algorithm for minimizing convex functions over convex sets\",\n        \"Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox\",\n        \"Gradient Sparsification for Communication-Efficient Distributed Optimization\",\n        \"Open problem: The oracle complexity of convex optimization with limited memory\",\n        \"Communication-efficient algorithms for statistical optimization\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Memory-Constrained Algorithms for Convex Optimization via Recursive Cutting-Planes, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}