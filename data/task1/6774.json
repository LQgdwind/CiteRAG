{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Concentration of polynomial random matrices via Efron-Stein inequalities\n\nPaper Abstract: Abstract Analyzing concentration of large random matrices is a common task in a wide variety of fields. Given independent random variables, several tools are available to bound the norms of random matrices whose entries are linear in the variables, such as the matrix-Bernstein inequality. However, for many recent applications, we need to bound the norms of random matrices whose entries are polynomials in the variables. Such matrices arise naturally in the analysis of spectral algorithms (e.g., Hopkins et al. [STOC 2016], Moitra and Wein [STOC 2019]), and in lower bounds for semidefinite programs based on the Sum-of-Squares (SoS) hierarchy (e.g. Barak et al. [FOCS 2016], Jones et al. [FOCS 2021]). In this work, we present a general framework to obtain such bounds, based on the beautiful matrix Efron-Stein inequalities developed by Paulin, Mackey and Tropp [Annals of Probability 2016]. The Efron-Stein inequality bounds the norm of a random matrix by the norm of another potentially simpler (but still random) matrix. We view the latter matrix as arising by ‚Äúdifferentiating‚Äù the starting matrix. By recursively differentiating, our framework reduces the main task to bounding the norms of far simpler matrices. These simpler matrices are in fact deterministic matrices in the case of Rademacher random variables and hence, bounding their norm is a far easier task. In general for non-Rademacher random variables, the task reduces to the much easier task of scalar concentration. Moreover, in the setting of polynomial matrices, our main result also generalizes the work of Paulin, Mackey and Tropp. As applications of our basic framework, we recover known bounds in the literature, especially for simple ‚Äútensor networks‚Äù and ‚Äúdense graph matrices‚Äù. As applications of our general framework, we derive bounds for ‚Äúsparse graph matrices‚Äù. The sparse graph matrix bounds were obtained only recently by Jones et al. [FOCS 2021] using a nontrivial application of the trace power method, and was a core component in their work. We expect this framework will also be helpful for other applications involving concentration phenomena for nonlinear random matrices.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Matrix poincar√© inequalities and concentration\",\n        \"Matrix hypercontractivity, streaming algorithms and ldcs: the large alphabet case\",\n        \"Graph matrices: Norm bounds and applications\",\n        \"Concentration inequalities for non-lipschitz functions with bounded derivatives of higher order\",\n        \"A hypercontractive inequality for matrix-valued functions with applications to quantum computing and ldcs\",\n        \"Hypercontractivity, sum-of-squares proofs, and their applications\",\n        \"Moment inequalities for functions of independent random variables\",\n        \"Spectral radii of sparse random matrices\",\n        \"Higher order concentration of measure\",\n        \"A nearly tight sum-of-squares lower bound for the planted clique problem\",\n        \"Polynomial-time power-sum decomposition of polynomials\",\n        \"Matrix Poincar√©, œÜùúë\\\\varphi-Sobolev inequalities, and quantum ensembles\",\n        \"Concentration inequalities with exchangeable pairs\",\n        \"Stein‚Äôs method for concentration inequalities\",\n        \"Exponential decay of matrix œÜùúë\\\\varphi-entropies on markov semigroups with applications to dynamical evolutions of quantum ensembles\",\n        \"Improved sum-of-squares lower bounds for hidden clique and hidden submatrix problems\",\n        \"The eigenvalues of random symmetric matrices\",\n        \"Sum-of-squares lower bounds for sherrington-kirkpatrick via planted affine planes\",\n        \"Scalar poincar√© implies matrix poincar√©\",\n        \"Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms\",\n        \"SoS and planted clique: Tight analysis of mpw moments at all degrees and an optimal lower bound at degree four\",\n        \"Statistical inference and the sum of squares method\",\n        \"Jensen‚Äôs operator inequality\",\n        \"Tensor principal component analysis via sum-of-square proofs\",\n        \"A robust spectral algorithm for overcomplete tensor decomposition\",\n        \"Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors\",\n        \"From Poincar√© inequalities to nonlinear matrix concentration\",\n        \"Nonlinear matrix concentration via semigroup methods\",\n        \"Symmetrized Fourier Analysis of Convex Relaxations for Combinatorial Optimization Problems\",\n        \"Sum-of-squares lower bounds for sparse independent set\",\n        \"Exact nuclear norm, completion and decomposition for random overcomplete tensors via degree-4 sos\",\n        \"Estimates of moments and tails of gaussian chaoses\",\n        \"Bounds on the norms of uniform low degree graph matrices\",\n        \"Sum-of-squares lower bounds for planted clique\",\n        \"Lifting sum-of-squares lower bounds: degree-2 to degree-4\",\n        \"Spectral methods from tensor networks\",\n        \"Some topics in analysis of Boolean functions\",\n        \"Analysis of boolean functions\",\n        \"A survey of certain trace inequalities\",\n        \"Efron‚Äìstein inequalities for random matrices\",\n        \"Machinery for proving sum-of-squares lower bounds on certification problems\",\n        \"Sub-exponential time sum-of-squares lower bounds for principal components analysis\",\n        \"Combinatorial Optimization via the Sum of Squares Hierarchy\",\n        \"Nonlinear Random Matrices and Applications to the Sum of Squares Hierarchy\",\n        \"Tight lower bounds for planted clique in the degree-4 SOS program\",\n        \"Bernstein-like concentration and moment inequalities for polynomials of independent random variables: multilinear case\",\n        \"Fast and robust tensor decomposition with applications to dictionary learning\",\n        \"A bound for the error in the normal approximation to the distribution of a sum of dependent random variables\",\n        \"Approximate computation of expectations\",\n        \"An introduction to matrix concentration inequalities\",\n        \"Spectral norm of random matrices\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Concentration of polynomial random matrices via Efron-Stein inequalities, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}