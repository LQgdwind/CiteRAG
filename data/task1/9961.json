{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding\n\nPaper Abstract: Abstract Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than one week using ten GPUs. The results showed that our fine-tuned model could synthesize SCs with an average BLEU (BiLingual Evaluation Understudy) score of 0.557. However, many codes in the auto-completed SCs were vulnerable. Using the code before the vulnerable line of 176 SCs containing different types of vulnerabilities to auto-complete the code, we found that more than 70% of the auto-completed codes were insecure. Thus, we further fine-tuned the model on other 941 vulnerable SCs containing the same types of vulnerabilities and applied vulnerability-constrained decoding. The fine-tuning took only one hour with four GPUs. We then auto-completed the 176 SCs again and found that our approach could identify 62% of the code to be generated as vulnerable and avoid generating 67% of them, indicating the approach could efficiently and effectively avoid vulnerabilities in the auto-completed code.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"code2vec: Learning distributed representations of code\",\n        \"Mapping language to code in programmatic context\",\n        \"Deep contextualized word representations\",\n        \"Improving language understanding by generative pre-training\",\n        \"Your ai pair programmer\",\n        \"Competition-level code generation with alphacode\",\n        \"Pangu-coder: Program synthesis with function-level language modeling\",\n        \"Evaluating large language models trained on code\",\n        \"Introducing chatgpt\",\n        \"Beyond the safeguards: Exploring the security risks of chatgpt\",\n        \"Asleep at the keyboard? assessing the security of github copilotâ€™s code contributions\",\n        \"How secure is code generated by chatgpt?\",\n        \"A survey of attacks on ethereum smart contracts (sok)\",\n        \"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\",\n        \"Foundations of nlp explained\",\n        \"Why Smart Contracts Reported as Vulnerable were not Exploited?\",\n        \"Making Smart Contracts Smarter\",\n        \"Slither: a static analysis framework for smart contracts\",\n        \"Mythril: an open-source security analysis tool for ethereum smart contracts\",\n        \"Contractfuzzer: Fuzzing smart contracts for vulnerability detection\",\n        \"Sfuzz: An efficient adaptive fuzzer for solidity smart contracts\",\n        \"Smartian: Enhancing smart contract fuzzing with static and dynamic data-flow analyses\",\n        \"The adverse effects of code duplication in machine learning models of code\",\n        \"What is the jaccard index?\",\n        \"The pile: An 800gb dataset of diverse text for language modeling\",\n        \"Eleutherai\",\n        \"Fun and dystopia with ai-based code generation using gpt-j-6b\",\n        \"Roformer: Enhanced transformer with rotary position embedding\",\n        \"How effective are smart contract analysis tools? evaluating smart contract static analysis tools using bug injection\",\n        \"Empirical review of automated analysis tools on 47,587 ethereum smart contracts\",\n        \"Beam search strategies for neural machine translation\",\n        \"Deepspeed\",\n        \"Codebleu: a method for automatic evaluation of code synthesis\",\n        \"Crystalbleu: Precisely and efficiently measuring the similarity of code\",\n        \"Safemath\",\n        \"Detecting gender bias in transformer-based models: A case study on bert\",\n        \"Introducing microsoft security copilot: Empowering defenders at the speed of ai\",\n        \"Secure-by-construction synthesis of cyber-physical systems\",\n        \"Lexically constrained decoding for sequence generation using grid beam search\",\n        \"Fast lexically constrained decoding with dynamic beam allocation for neural machine translation\",\n        \"Improved lexically constrained decoding for translation and monolingual rewriting\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Coderl: Mastering code generation through pretrained models and deep reinforcement learning\",\n        \"Execution-based code generation using deep reinforcement learning\",\n        \"Offline reinforcement learning as one big sequence modeling problem\",\n        \"Decrypted\",\n        \"Empirical evaluation of smart contract testing: What is the best choice?\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}