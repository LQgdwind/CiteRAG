{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 63 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Generalizing Graph Neural Networks on Out-Of-Distribution Graphs\n\nPaper Abstract: Abstract Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training graphs and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. This learning mechanism inherits from the common characteristics of machine learning approaches. However, such spurious correlations may change in the wild testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNN models. To this end, in this paper, we argue that the spurious correlation exists among subgraph-level units and analyze the degeneration of GNN in causal view. Based on the causal view analysis, we propose a general causal representation framework for stable GNN, called StableGNN. The main idea of this framework is to extract high-level representations from raw graph data first and resort to the distinguishing ability of causal inference to help the model get rid of spurious correlations. Particularly, to extract meaningful high-level representations, we exploit a differentiable graph pooling layer to extract subgraph-based representations by an end-to-end manner. Furthermore, inspired by the confounder balancing techniques from causal inference, based on the learned high-level representations, we propose a causal variable distinguishing regularizer to correct the biased training distribution by learning a set of sample weights. Hence, GNNs would concentrate more on the true connection between discriminative substructures and labels. Extensive experiments are conducted on both synthetic datasets with various distribution shift degrees and eight real-world OOD graph datasets. The results well verify that the proposed model StableGNN not only outperforms the state-of-the-arts but also provides a flexible framework to enhance existing GNNs. In addition, the interpretability experiments validate that StableGNN could leverage causal structures for predictions.\n\n{retrieved_text}\n\nGenerate exactly 63 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 63\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Identification and estimation of local average treatment effects\",\n        \"Invariant risk minimization\",\n        \"Approximate residual balancing: De-biased inference of average treatment effects in high dimensions\",\n        \"A meta-transfer objective for learning to disentangle causal mechanisms\",\n        \"Beyond low-frequency information in graph convolutional networks\",\n        \"Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity\",\n        \"Exploring the landscape of spatial robustness\",\n        \"Graph u-nets\",\n        \"Neural message passing for quantum chemistry\",\n        \"Measuring statistical dependence with hilbert-schmidt norms\",\n        \"Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies\",\n        \"Inductive representation learning on large graphs\",\n        \"Deep residual learning for image recognition\",\n        \"Benchmarking neural network robustness to common corruptions and perturbations\",\n        \"Open graph benchmark: Datasets for machine learning on graphs\",\n        \"Using auc and accuracy in evaluating learning algorithms\",\n        \"Batch Normalization. Accelerating deep network training by reducing internal covariate shift\",\n        \"Hierarchical generation of molecular graphs using structural motifs\",\n        \"Does invariant risk minimization capture invariance?\",\n        \"Universal invariant and equivariant graph neural networks\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Out-of-distribution generalization via risk extrapolation (rex)\",\n        \"Stable prediction across unknown environments\",\n        \"Stable prediction with model misspecification and agnostic distribution shift\",\n        \"Building machines that learn and think like people\",\n        \"Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling\",\n        \"Rdkit: Open-source cheminformatics\",\n        \"Graph classification using structural attention\",\n        \"Self-attention graph pooling\",\n        \"A reduction of a graph to a canonical form and an algebra arising during this reduction\",\n        \"A pac-bayesian approach to generalization bounds for graph neural networks\",\n        \"Generative causal explanations for graph neural networks\",\n        \"Discovering causal signals in images\",\n        \"Parameterized explainer for graph neural network\",\n        \"Domain generalization using a mixture of multiple latent domains\",\n        \"Geometric deep learning on graphs and manifolds using mixture model cnns\",\n        \"Explainability methods for graph convolutional neural networks\",\n        \"Learning to learn single domain generalization\",\n        \"The risks of invariant risk minimization\",\n        \"The graph neural network model\",\n        \"Modeling relational data with graph convolutional networks\",\n        \"Toward causal representation learning\",\n        \"Causally regularized learning with agnostic data selection bias\",\n        \"Stable learning via sample reweighting\",\n        \"Efficient graphlet kernels for large graph comparison\",\n        \"Weisfeiler-lehman graph kernels\",\n        \"Supervised feature selection via dependence estimation\",\n        \"Feature selection via dependence maximization\",\n        \"One pixel attack for fooling deep neural networks\",\n        \"Test-time training for out-of-distribution generalization\",\n        \"Graph attention networks\",\n        \"Learning robust representations by projecting superficial statistics out\",\n        \"Simplifying graph convolutional networks\",\n        \"Moleculenet: a benchmark for molecular machine learning\",\n        \"Representation learning on graphs with jumping knowledge networks\",\n        \"How powerful are graph neural networks?\",\n        \"Hierarchical graph representation learning with differentiable pooling\",\n        \"Gnnexplainer: Generating explanations for graph neural networks\",\n        \"Link prediction based on graph neural networks\",\n        \"An end-to-end deep learning architecture for graph classification\",\n        \"Deep stable learning for out-of-distribution generalization\",\n        \"Counterfactual prediction for bundle treatment\",\n        \"Stable weights that balance covariates for estimation with incomplete outcome data\"\n    ],\n    \"reasoning\": \"Selected these 63 references because they represent foundational work and recent advances in Generalizing Graph Neural Networks on Out-Of-Distribution Graphs, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}