{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Deep Evidential Learning for Bayesian Quantile Regression\n\nPaper Abstract: Abstract It is desirable to have accurate uncertainty estimation from a single deterministic forward-pass model, as traditional methods for uncertainty quantification are computationally expensive. However, this is difficult because single forward-pass models do not sample weights during inference and often make assumptions about the target distribution, such as assuming it is Gaussian. This can be restrictive in regression tasks, where the mean and standard deviation are inadequate to model the target distribution accurately. This paper proposes a deep Bayesian quantile regression model that can estimate the quantiles of a continuous target distribution without the Gaussian assumption. The proposed method is based on evidential learning, which allows the model to capture aleatoric and epistemic uncertainty with a single deterministic forward-pass model. This makes the method efficient and scalable to large models and datasets. We demonstrate that the proposed method achieves calibrated uncertainties on non-Gaussian distributions, disentanglement of aleatoric and epistemic uncertainty, and robustness to out-of-distribution samples.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Variational Inference for Nonparametric Bayesian Quantile Regression\",\n        \"Deep evidential regression\",\n        \"Pattern recognition and machine learning, volume 4\",\n        \"Weight uncertainty in neural network\",\n        \"Gaussian Process Quantile Regression using Expectation Propagation\",\n        \"Bayesian scale mixtures of normals linear regression and Bayesian quantile regression with big data and variable selection\",\n        \"Distributional Reinforcement Learning With Quantile Regression\",\n        \"High-Dimensional Variable Selection for Quantile Regression Based on Variational Bayesian Method\",\n        \"Bayesian quantile regression with approximate likelihood\",\n        \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning\",\n        \"Concrete dropout\",\n        \"Deep bayesian active learning with image data\",\n        \"Gradient conjugate priors and multi-layer neural networks\",\n        \"Noise contrastive priors for functional uncertainty\",\n        \"Probabilistic backpropagation for scalable learning of bayesian neural networks\",\n        \"The apolloscape dataset for autonomous driving\",\n        \"Modeling Censored Mobility Demand Through Censored Quantile Regression Neural Networks\",\n        \"What uncertainties do we need in bayesian deep learning for computer vision?\",\n        \"Adam: A method for stochastic optimization\",\n        \"Variational dropout and the local reparameterization trick\",\n        \"Regression Quantiles\",\n        \"The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance\",\n        \"Gibbs sampling methods for Bayesian quantile regression\",\n        \"Simple and scalable predictive uncertainty estimation using deep ensembles\",\n        \"Simple and principled uncertainty estimation with deterministic deep learning via distance awareness\",\n        \"Predictive uncertainty estimation via prior networks\",\n        \"The Unreasonable Effectiveness of Deep Evidential Regression\",\n        \"Multivariate deep evidential regression\",\n        \"Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting\",\n        \"Estimating the mean and variance of the target probability distribution\",\n        \"Improving Evidential Deep Learning via Multi-Task Learning\",\n        \"Can you trust your modelâ€™s uncertainty? evaluating predictive uncertainty under dataset shift\",\n        \"Evidential Conditional Neural Processes\",\n        \"Learn to Accumulate Evidence from All Training Samples: Theory and Practice\",\n        \"High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach\",\n        \"Uncertainty in neural networks: Approximately bayesian ensembling\",\n        \"Beyond expectation: Deep joint mean and quantile regression for spatiotemporal problems\",\n        \"U-net: Convolutional networks for biomedical image segmentation\",\n        \"Evidential Deep Learning to Quantify Classification Uncertainty\",\n        \"Indoor Segmentation and Support Inference from RGBD Images\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"Neural Spline Search for Quantile Probabilistic Modeling\",\n        \"Efficient object localization using convolutional networks\",\n        \"Bayesian Quantile and Expectile Optimisation\",\n        \"Bayesian quantile inference\",\n        \"Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation\",\n        \"Uncertainty estimation using a single deep deterministic neural network\",\n        \"Bayesian deep learning and a probabilistic perspective of generalization\",\n        \"Bayesian quantile regression\",\n        \"A Three-Parameter Asymmetric Laplace Distribution and Its Extension\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Deep Evidential Learning for Bayesian Quantile Regression, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}