{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 389 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Dissociating language and thought in large language models\n\nPaper Abstract: Abstract Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence—knowledge of linguistic rules and patterns—and functional linguistic competence—understanding and using language in the world. We ground this distinction in human neuroscience, showing that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. In short, LLMs are good models of language but incomplete models of human thought. * The two lead authors contributed equally to this work.\n\n{retrieved_text}\n\nGenerate exactly 389 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 389\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Can language models encode perceptual structure without grounding? a case study in color\",\n        \"Core Syntax: A Minimalist Approach\",\n        \"The Human Amygdala and Emotion\",\n        \"The Social Brain: Neural Basis of Social Knowledge\",\n        \"Origins of the brain networks for advanced mathematics in expert mathematicians\",\n        \"A distinct cortical network for mathematical knowledge in the human brain\",\n        \"Against stored abstractions: A radical exemplar model of language acquisition\",\n        \"Learning to compose neural networks for question answering\",\n        \"The Contribution of Grammar, Vocabulary and Theory of Mind in Pragmatic Language Competence in Children with Autistic Spectrum Disorders\",\n        \"What’s in a look?\",\n        \"Computation of Conditional Probability Statistics by 8-Month-Old Infants\",\n        \"Discovering Event Structure in Continuous Narrative Perception and Memory\",\n        \"Representation of Real-World Event Schemas during Narrative Perception\",\n        \"Is problem solving dependent on language?\",\n        \"Is relational reasoning dependent on language? A voxel-based lesion symptom mapping study\",\n        \"On the proper role of linguistically-oriented deep net analysis in linguistic theorizing\",\n        \"Distributional memory: A general framework for corpus-based semantics\",\n        \"Spared musical abilities in a conductor with global aphasia and ideomotor apraxia\",\n        \"Functionalism and the competition model\",\n        \"Voxel-based lesion-symptom mapping\",\n        \"Neural responses to grammatically and lexically degraded speech\",\n        \"Language and spatial reorientation: evidence from severe aphasia\",\n        \"Probing classifiers: Promises, shortcomings, and advances\",\n        \"Synthetic and natural noise both break neural machine translation\",\n        \"Climbing towards NLU: On meaning, form, and understanding in the age of data\",\n        \"No evidence for a special role of language in feature-based categorization\",\n        \"Using deep neural networks to learn syntactic agreement\",\n        \"Poverty of the stimulus revisited\",\n        \"Experience grounds language\",\n        \"What are large language models supposed to model?\",\n        \"No evidence for differences among language regions in their temporal receptive windows\",\n        \"A functional dissociation between language and multiple-demand systems revealed in patterns of BOLD signal fluctuations\",\n        \"Syntactic processing is distributed across the language system\",\n        \"Systematic inequalities in language technology performance across the world’s languages\",\n        \"How children learn the meanings of words\",\n        \"From exemplar to grammar: A probabilistic analogy-based model of language learning\",\n        \"On the opportunities and risks of foundation models\",\n        \"How relevant is the turing test in the age of sophisbots?\",\n        \"Improving language models by retrieving from trillions of tokens, February 2022a\",\n        \"Improving language models by retrieving from trillions of tokens\",\n        \"A framework for the cooperation of learning algorithms\",\n        \"The dangers of underclaiming: Reasons for caution when reporting how nlp systems fail\",\n        \"Parallel distributed networks resolved at high resolution reveal close juxtaposition of distinct regions\",\n        \"Localizing syntactic predictions using recurrent neural network grammars\",\n        \"The mental representation of grammatical relations, volume 1\",\n        \"Is syntactic knowledge probabilistic? Experiments with the English dative alternation\",\n        \"Sur le siège de la faculté du langage articulé (15 juin)\",\n        \"Language Models are Few-Shot Learners\",\n        \"Sparks of artificial general intelligence: Early experiments with gpt-4\",\n        \"Language and identity\",\n        \"Identity and interaction: A sociocultural linguistic approach\",\n        \"The brain’s default network: Updated anatomy, physiology and evolving insights\",\n        \"The Brain’s Default Network\",\n        \"Frequency and the emergence of linguistic structure, volume 45\",\n        \"Explanatory models in neuroscience: Part 1–taking mechanistic abstraction seriously\",\n        \"The cognitive functions of language\",\n        \"Brains and algorithms partially converge in natural language processing\",\n        \"Language model behavior: A comprehensive survey\",\n        \"Probabilistic models of cognition: Conceptual foundations\",\n        \"What Don’t RNN Language Models Learn About Filler-Gap Dependencies?\",\n        \"Look at that! bert can be easily distracted from paying attention to morphosyntax\",\n        \"The human language system does not support music processing, June 2021\",\n        \"A structured distributional model of sentence meaning and processing\",\n        \"Syntactic Structures\",\n        \"Aspects of the Theory of Syntax\",\n        \"Linguistics and cognitive science: problems and mysteries\",\n        \"PaLM: Scaling language modeling with pathways\",\n        \"Deep reinforcement learning from human preferences\",\n        \"Creating language: Integrating evolution, acquisition, and processing\",\n        \"Distributional Learning as a Theory of Language Acquisition\",\n        \"Arenas of language use\",\n        \"Using language\",\n        \"Electra: Pre-training text encoders as discriminators rather than generators\",\n        \"Training verifiers to solve math word problems\",\n        \"At the core of reasoning: Dissociating deductive and non-deductive load\",\n        \"Crawling the internal knowledge-base of language models\",\n        \"Language universals and linguistic typology: Syntax and morphology\",\n        \"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\",\n        \"Are all languages equally hard to language-model?\",\n        \"An amazing four doctoral dissertations\",\n        \"Episodic Memory in Lifelong Language Learning\",\n        \"What’s so special about BERT’s layers? a closer look at the NLP pipeline in monolingual and multilingual models\",\n        \"Parallel systems for social and spatial reasoning within the cortical apex, April 2022\",\n        \"Functional Organization of Social Perception and Cognition in the Superior Temporal Sulcus\",\n        \"The Representation of Semantic Information Across Human Cerebral Cortex During Listening Versus Reading Is Invariant to Stimulus Modality\",\n        \"The role of language in intelligence\",\n        \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n        \"and Morton Ann Gernsbacher\",\n        \"Cortical tracking of hierarchical linguistic structures in connected speech\",\n        \"Parallel distributed networks dissociate episodic and social functions within the individual\",\n        \"Brain-like functional specialization emerges spontaneously in deep neural networks\",\n        \"The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour\",\n        \"The Structure of Cognition: Attentional Episodes in Mind and Brain\",\n        \"Integrated Intelligence from Distributed Brain Activity\",\n        \"Analyzing Individual Neurons in Pre-trained Language Models\",\n        \"Faith and fate: Limits of transformers on compositionality\",\n        \"Measuring and improving consistency in pretrained language models\",\n        \"Amnesic probing: Behavioral explanation with amnesic counterfactuals\",\n        \"Finding structure in time\",\n        \"Learning and development in neural networks: the importance of starting small\",\n        \"Vector space models of word meaning and phrase meaning: A survey\",\n        \"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\",\n        \"Probing for semantic evidence of composition by means of simple classification tasks\",\n        \"Structures, not strings: linguistics as part of the cognitive sciences\",\n        \"Language and thought are not the same thing: evidence from neuroimaging and neurological patients: Language versus thought\",\n        \"New method for fMRI investigations of language: defining ROIs functionally in individual subjects\",\n        \"Functional specificity for high-level linguistic processing in the human brain\",\n        \"Lexical and syntactic representations in the brain: An fMRI investigation with multi-voxel pattern analyses\",\n        \"Syntactic processing in the human brain: what we know, what we don’t know, and a suggestion for how to proceed\",\n        \"Broad domain generality in focal regions of frontal and parietal cortex\",\n        \"Neural correlate of the construction of sentence meaning\",\n        \"Lack of selectivity for syntax relative to word meanings throughout the language network\",\n        \"Effects of contextual relevance on pragmatic inference during conversation: An fMRI study\",\n        \"Understanding particularized and generalized conversational implicatures: Is theory-of-mind necessary?\",\n        \"What Does the Frontomedian Cortex Contribute to Language Processing: Coherence or Theory of Mind?\",\n        \"Causal analysis of syntactic agreement mechanisms in neural language models\",\n        \"Functional neuroanatomy of intuitive physical inference\",\n        \"Other minds in the brain: a functional imaging study of “theory of mind” in story comprehension\",\n        \"A tripartite structure of pragmatic language abilities: comprehension of social conventions, intonation processing, and causal reasoning\",\n        \"Three ideal observer models for rule learning in simple languages\",\n        \"Large language models as models of human cognition, Jul 2023\",\n        \"Predicting pragmatic reasoning in language games\",\n        \"On comprehending sentences: Syntactic parsing strategies\",\n        \"Subcognition and the limits of the Turing test\",\n        \"The Turing Test: the first 50 years\",\n        \"Reading the mind in cartoons and stories: an fMRI study of ’theory of mind’ in verbal and nonverbal tasks\",\n        \"Understanding social reasoning in language models with language models\",\n        \"SyntaxGym: An Online Platform for Targeted Evaluation of Language Models\",\n        \"Decisions, decisions: Infant language learning when multiple generalizations are possible\",\n        \"Constraints on sentence comprehension\",\n        \"Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information\",\n        \"A human universal: the capacity to learn a language\",\n        \"Distributed neural system for general intelligence revealed by lesion mapping\",\n        \"Constructions work\",\n        \"Explain me this: Creativity, competition, and the partial productivity of constructions\",\n        \"Shared computational principles for language processing in humans and deep language models\",\n        \"Reporting bias and knowledge acquisition\",\n        \"Coordination Among Neural Modules Through a Shared Global Workspace, March 2022\",\n        \"Semantic projection recovers rich human knowledge of multiple object features from word embeddings\",\n        \"Utterer’s meaning and intention\",\n        \"Logic and conversation\",\n        \"Language as a mechanism for reasoning about possibilities\",\n        \"Colorless green recurrent networks dream hierarchically\",\n        \"Retrieval augmented language model pre-training\",\n        \"Peter Hagoort and Stephen C\",\n        \"The construction system of the brain\",\n        \"Non-literal language processing is jointly supported by the language and Theory of Mind networks: Evidence from a novel meta-analytic fMRI approach, March 2022\",\n        \"A hierarchy of linguistic predictions during natural language comprehension\",\n        \"A structural probe for finding syntax in word representations\",\n        \"Narrow syntax and the language of thought\",\n        \"Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training\",\n        \"A Systematic Assessment of Syntactic Generalization in Neural Language Models\",\n        \"A fine-grained comparison of pragmatic language understanding in humans and language models\",\n        \"Precision fMRI reveals that the language-selective network supports both phrase-structure building and lexical access during language production\",\n        \"Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models\",\n        \"Understanding English language variation in US schools\",\n        \"Toward racial justice in linguistics: Interdisciplinary insights into theorizing race in the discipline and diversifying the profession\",\n        \"Learning by abstraction: The neural state machine\",\n        \"BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language\",\n        \"Syntactic and Semantic Modulation of Neural Activity during Auditory Sentence Comprehension\",\n        \"Compositionality decomposed: how do neural networks generalise?\",\n        \"Perceiving social interactions in the posterior superior temporal sulcus\",\n        \"Comprehension of computer code relies primarily on domain-general executive resources\",\n        \"The Language Network is Recruited but Not Required for Nonverbal Event Semantics\",\n        \"The nature of the language faculty and its implications for evolution of language (reply to fitch, hauser, and chomsky)\",\n        \"Foundations of language: Brain, meaning, grammar, evolution\",\n        \"Discourse-level comprehension engages medial frontal Theory of Mind brain regions even for expository texts\",\n        \"Localizing Pain Matrix and Theory of Mind networks with both verbal and non-verbal stimuli\",\n        \"Computational language modeling and the promise of in silico experimentation\",\n        \"Everyday conversation requires cognitive inference: Neural bases of comprehending implicated meanings in conversations\",\n        \"Inferring and executing programs for visual reasoning\",\n        \"Speech-accompanying gestures are not processed by the language-processing mechanisms\",\n        \"Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition\",\n        \"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition\",\n        \"The fusiform face area: a module in human extrastriate cortex specialized for face perception\",\n        \"Scaling laws for neural language models\",\n        \"Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly\",\n        \"Event knowledge in large language models: the gap between the impossible and the unlikely\",\n        \"A pleasant three days in Philadelphia: Arguments for a pseudopartitive analysis\",\n        \"On the impact of various types of noise on neural machine translation\",\n        \"Entity tracking in language models\",\n        \"Testing for grammatical category abstraction in neural language models\",\n        \"Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models\",\n        \"Language as a social cue\",\n        \"Accent trumps race in guiding children’s social preferences\",\n        \"Algebra in a man with severe aphasia\",\n        \"Neural networks can learn patterns of island-insensitivity in norwegian\",\n        \"Theory of mind may have spontaneously emerged in large language models\",\n        \"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference, September 2021\",\n        \"LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better\",\n        \"Common and Distinct Neural Substrates for Pragmatic, Semantic, and Syntactic Processing of Spoken Sentences: An fMRI Study\",\n        \"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\",\n        \"Language in context\",\n        \"The emergence of number and syntax units in LSTM language models\",\n        \"Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans, October 2021\",\n        \"Can language models learn from explanations in context?\",\n        \"A usage-based model\",\n        \"How not to disagree: The emergence of structure from usage\",\n        \"Deep learning and linguistic representation\",\n        \"Probing for the usage of grammatical number\",\n        \"Alessandro Lenci\",\n        \"Lists with and without syntax: A new approach to measuring the neural processing of syntax\",\n        \"Linguistic and other psychological aspects of paroxysmal aphasia\",\n        \"i’m sorry dave, i’m afraid i can’t do that\\\": Linguistics, statistics, and natural language processing circa 2001\",\n        \"Functional selectivity for social interaction perception in the human superior temporal sulcus during natural viewing\",\n        \"Distributional semantics in linguistic and cognitive research\",\n        \"Topographic Mapping of a Hierarchy of Temporal Receptive Windows Using a Narrated Story\",\n        \"Presumptive meanings: The theory of generalized conversational implicature\",\n        \"Stephen C Levinson\",\n        \"Large language models can be strong differentially private learners\",\n        \"Holistic evaluation of language models\",\n        \"What can linguistics and deep learning contribute to each other? Response to Pater\",\n        \"Syntactic Structure from Deep Learning\",\n        \"Assessing the ability of lstms to learn syntax-sensitive dependencies\",\n        \"Llm+ p: Empowering large language models with optimal planning proficiency\",\n        \"Evaluating verifiability in generative search engines, 2023b\",\n        \"Relational Memory-Augmented Language Models\",\n        \"Things not written in text: Exploring spatial commonsense from visual signals\",\n        \"Computer code comprehension shares neural resources with formal logical inference in the fronto-parietal network\",\n        \"Rearranging the familiar: Testing compositional generalization in recurrent networks\",\n        \"Unit testing for concepts in neural networks\",\n        \"Influence paths for characterizing subject-verb number agreement in LSTM language models\",\n        \"The lexical nature of syntactic ambiguity resolution\",\n        \"Neural systems underlying British Sign Language and audio-visual English processing in native users\",\n        \"The competition model\",\n        \"A discerning several thousand judgments: GPT-3 rates the article + adjective + numeral + noun construction\",\n        \"Emergent linguistic structure in artificial neural networks trained by self-supervision\",\n        \"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision\",\n        \"The next decade in ai: Four steps towards robust artificial intelligence\",\n        \"Beyond the turing test\",\n        \"CamemBERT: a tasty French language model\",\n        \"Targeted syntactic evaluation of language models\",\n        \"Syntax-Sensitive Regions of the Posterior Inferior Frontal Gyrus and the Posterior Temporal Lobe Are Differentially Recruited by Production and Perception\",\n        \"The role of the IFG and pSTS in syntactic prediction: Evidence from a parametric study of hierarchical structure in fMRI\",\n        \"The temporal dynamics of structure and content in sentence comprehension: Evidence from fMRI-constrained MEG\",\n        \"Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks\",\n        \"Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks\",\n        \"How much do language models copy from their training data? evaluating linguistic novelty in text generation using RAVEN\",\n        \"Embers of autoregression: Understanding large language models through the problem they are trained to solve\",\n        \"Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference\",\n        \"Shared language: overlap and segregation of the neuronal infrastructure for speaking and listening revealed by functional MRI\",\n        \"Locating and editing factual associations in gpt\",\n        \"Primary progressive aphasia\",\n        \"Primary progressive aphasia and the evolving neurology of the language network\",\n        \"Augmented language models: a survey\",\n        \"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers\",\n        \"The quantization model of neural scaling\",\n        \"What kind of language is hard to language-model?\",\n        \"COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models\",\n        \"Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network\",\n        \"Functional neuroanatomy of deductive inference: A language-independent distributed network\",\n        \"The boundaries of language and thought in deductive inference\",\n        \"Thought beyond language: neural dissociation of algebra and natural language\",\n        \"An analysis of the Turing test\",\n        \"Causal analysis of syntactic agreement neurons in multilingual language models\",\n        \"Neurophysiological dynamics of phrase-structure building during sentence processing\",\n        \"Does BERT rediscover a classical NLP pipeline?\",\n        \"Colorless green ideas learn furiously: Chomsky and the two cultures of statistical learning\",\n        \"Productivity and reuse in language: A theory of linguistic computation and storage\",\n        \"Productivity and reuse in language\",\n        \"Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?\",\n        \"ChatGPT: Optimizing Language Models for Dialogue, November 2022\",\n        \"Training language models to follow instructions with human feedback, March 2022\",\n        \"Cortical representation of the constituent structure of sentences\",\n        \"Are NLP Models really able to Solve Simple Math Word Problems?\",\n        \"Mapping language models to grounded conceptual spaces\",\n        \"Generative linguistics and neural networks at 60: Foundation, friction, and fusion\",\n        \"Where do you know what you know? The representation of semantic knowledge in the human brain\",\n        \"Functionally distinct language and Theory of Mind networks are synchronized at rest and during language comprehension\",\n        \"Olessia Jouravlev\",\n        \"The learnability of abstract syntactic principles\",\n        \"Language Models as Knowledge Bases?\",\n        \"Modern language models refute chomsky’s approach to language\",\n        \"Turing test: 50 years later\",\n        \"Beyond Hemispheric Dominance: Brain Regions Underlying the Joint Lateralization of Language and Arithmetic to the Left Hemisphere\",\n        \"Words and rules: The ingredients of language\",\n        \"Natural language and natural selection\",\n        \"The faculty of language: what’s special about it?\",\n        \"On language and connectionism: Analysis of a parallel distributed processing model of language acquisition\",\n        \"How multilingual is multilingual BERT?\",\n        \"Invariant representation of physical stability in the human brain\",\n        \"Measuring and narrowing the compositionality gap in language models\",\n        \"High-level language processing regions are not engaged in action observation or imitation\",\n        \"Why think step-by-step? reasoning emerges from the locality of experience\",\n        \"Empirical assessment of stimulus poverty arguments\",\n        \"Language models are unsupervised multitask learners\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Can LSTM learn to capture agreement? the case of Basque\",\n        \"Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages\",\n        \"Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction\",\n        \"On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT\",\n        \"Immediate disambiguation of lexically ambiguous words during reading: Evidence from eye movements\",\n        \"Can fMRI reveal the representation of syntactic structure in the brain?\",\n        \"Selective and Invariant Neural Responses to Spoken and Written Narratives\",\n        \"High-level language brain regions are sensitive to sub-lexical regularities, June 2021\",\n        \"Neural networks for modelling and control\",\n        \"HiStruct+: Improving extractive text summarization with hierarchical structure information\",\n        \"Parallel distributed processing\",\n        \"Learning the past tenses of English verbs: Implicit rules or parallel distributed processing\",\n        \"Aphasia and the Relationship of Language and Brain\",\n        \"Pattern induction by infant language learners\",\n        \"Statistical learning by 8-month-old infants\",\n        \"WinoGrande: An Adversarial Winograd Schema Challenge at Scale\",\n        \"Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs, October 2022\",\n        \"Uniquely human social cognition\",\n        \"It’s the thought that counts: specific brain regions for one component of theory of mind\",\n        \"Overlapping and non-overlapping brain regions for theory of mind and self reflection in individual subjects\",\n        \"Toolformer: Language models can teach themselves to use tools\",\n        \"Integrative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence\",\n        \"The neural architecture of language: Integrative modeling converges on predictive processing\",\n        \"When a sentence does not introduce a discourse entity, transformer-based models still sometimes refer to it\",\n        \"Invariant representations of mass in the human brain\",\n        \"Minding language models’ (lack of) theory of mind: A plug-and-play multi-character belief tracker\",\n        \"A new fun and robust version of an fMRI localizer for the frontotemporal language system\",\n        \"fMRI reveals language-specific predictive coding during naturalistic sentence comprehension\",\n        \"‘Constituent length’ effects in fMRI do not provide evidence for abstract syntactic processing, November 2021\",\n        \"Robust Effects of Working Memory Demand during Naturalistic Language Comprehension in Language-Selective Cortex\",\n        \"No evidence of theory of mind reasoning in the human language network, July 2022b\",\n        \"Graded sensitivity to structure and meaning throughout the human language network\",\n        \"Clever hans or neural theory of mind? stress testing social reasoning in large language models\",\n        \"Dynamic reconfiguration of the default mode network during narrative comprehension\",\n        \"From \\\"thought and language\\\" to \\\"thinking for speaking\\\"\",\n        \"Retrieval and unification of syntactic structure in sentence comprehension: an FMRI study using word-category ambiguity\",\n        \"Two types of modified cardinals\",\n        \"Core Knowledge\",\n        \"Neural evidence that utterance-processing entails mentalizing: the case of irony\",\n        \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\",\n        \"Roformer: Enhanced transformer with rotary position embedding, 2021\",\n        \"Generating text with recurrent neural networks\",\n        \"oLMpics-on what language model pre-training captures\",\n        \"Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge\",\n        \"Integration of visual and linguistic information in spoken language comprehension\",\n        \"Sociality and interaction envelope organize visual action representations\",\n        \"CxGBERT: BERT meets construction grammar\",\n        \"BERT rediscovers the classical NLP pipeline\",\n        \"Memorization without overfitting: Analyzing the training dynamics of large language models\",\n        \"Origins of human communication\",\n        \"Do large language models know what humans know?\",\n        \"CxLM: A construction and context-aware language model\",\n        \"When does syntax mediate neural language model performance? evidence from dropout probes\",\n        \"Computing Machinery and Intelligence\",\n        \"Large language models fail on trivial alterations to theory-of-mind tasks\",\n        \"Pragmatics in action: indirect requests engage theory of mind areas and the cortical motor network\",\n        \"Strategies of discourse comprehension\",\n        \"Single-stage prediction models do not explain the magnitude of syntactic disambiguation difficulty\",\n        \"Quantity doesn’t buy quality syntax with neural language models\",\n        \"Aphasic language, aphasic thought: an investigation of propositional thinking in an a-propositional aphasic\",\n        \"Evidence for cognition without grammar from causal reasoning and ’theory of mind’ in an agrammatic aphasic patient\",\n        \"Severe impairment in grammar does not preclude theory of mind\",\n        \"Agrammatic but numerate\",\n        \"Attention is All you Need\",\n        \"Analyzing the structure of attention in a transformer language model\",\n        \"Neural responses to visually observed social interactions\",\n        \"Universal adversarial triggers for attacking and analyzing NLP\",\n        \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\",\n        \"SuperGLUE: A stickier benchmark for general-purpose language understanding systems\",\n        \"A compact and language-sensitive multilingual translation method\",\n        \"Extending multilingual BERT to low-resource languages\",\n        \"What artificial neural networks can tell us about human language acquisition\",\n        \"Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs\",\n        \"BLiMP: The Benchmark of Linguistic Minimal Pairs for English\",\n        \"Frequency effects on syntactic rule learning in transformers\",\n        \"Chain of thought prompting elicits reasoning in large language models\",\n        \"The better your syntax, the better your semantics? probing pretrained language models for the English comparative correlative\",\n        \"Construction grammar provides unique insight into neural language models\",\n        \"Eliza—a computer program for the study of natural language communication between man and machine\",\n        \"Der aphasische Symptomencomplex: eine psychologische Studie auf anatomischer Basis\",\n        \"What do RNN language models learn about filler–gap dependencies?\",\n        \"Using computational models to test syntactic learnability\",\n        \"Communicating without a functioning language system: Implications for the role of language in mentalizing\",\n        \"Relevance theory\",\n        \"Language Mapping in Aphasia\",\n        \"Philosophical investigations\",\n        \"From word models to world models: Translating from natural language to the probabilistic language of thought\",\n        \"Fluid intelligence loss linked to restricted regions of damage within frontal and parietal cortex\",\n        \"Fluid intelligence is supported by the multiple-demand system not the language system\",\n        \"Causal distillation for language models\",\n        \"Performance-optimized hierarchical models predict neural responses in higher visual cortex\",\n        \"Task representations in neural networks trained to perform many cognitive tasks\",\n        \"How poor is the stimulus? evaluating hierarchical generalization in neural networks trained on child-directed speech\",\n        \"Amplification of local changes along the timescale processing hierarchy\",\n        \"Neural-symbolic vqa: Disentangling reasoning from vision and language understanding\",\n        \"Including signed languages in natural language processing\",\n        \"Word frequency does not predict grammatical knowledge in language models\",\n        \"HellaSwag: Can a Machine Really Finish Your Sentence?\",\n        \"On the Paradox of Learning to Reason from Data, May 2022\",\n        \"When do you need billions of words of pretraining data?\",\n        \"Mixture-of-Experts with Expert Choice Routing, October 2022\",\n        \"Unsupervised neural network models of the ventral visual stream\"\n    ],\n    \"reasoning\": \"Selected these 389 references because they represent foundational work and recent advances in Dissociating language and thought in large language models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}