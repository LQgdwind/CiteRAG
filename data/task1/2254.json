{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 282 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Comprehensive Study of Deep Video Action Recognition\n\nPaper Abstract: Abstract Video action recognition is one of the representative tasks for video understanding. Over the last decade, we have witnessed great advancements in video action recognition thanks to the emergence of deep learning. But we also encountered new challenges, including modeling long-range temporal information in videos, high computation costs, and incomparable results due to datasets and evaluation protocol variances. In this paper, we provide a comprehensive survey of over 200 existing papers on deep learning for video action recognition. We first introduce the 17 video action recognition datasets that influenced the design of models. Then we present video action recognition models in chronological order: starting with early attempts at adapting deep learning, then to the two-stream networks, followed by the adoption of 3D convolutional kernels, and finally to the recent compute-efficient models. In addition, we benchmark popular methods on several representative datasets and release code for reproducibility. In the end, we discuss open problems and shed light on opportunities for video action recognition to facilitate new research ideas.\n\n{retrieved_text}\n\nGenerate exactly 282 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 282\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"YouTube-8M: A Large-Scale Video Classification Benchmark\",\n        \"Self-Supervised MultiModal Versatile Networks\",\n        \"Self-Supervised Learning by Cross-Modal Audio-Video Clustering\",\n        \"NetVLAD: CNN Architecture for Weakly Supervised Place Recognition\",\n        \"Sequential Deep Learning for Human Action Recognition\",\n        \"SpeedNet: Learning the Speediness in Videos\",\n        \"Understanding and Simplifying One-Shot Architecture Search\",\n        \"Efficient Video Classification Using Fewer Frames\",\n        \"Dynamic Image Networks for Action Recognition\",\n        \"Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications\",\n        \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\",\n        \"A short note about kinetics-600\",\n        \"A short note on the kinetics-700 human action dataset\",\n        \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\n        \"Temporal Attentive Alignment for Large-Scale Video Domain Adaptation\",\n        \"RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning\",\n        \"A Simple Framework for Contrastive Learning of Visual Representations\",\n        \"Improved Baselines with Momentum Contrastive Learning\",\n        \"Webly Supervised Learning of Convolutional Networks\",\n        \"Multi-Fiber Networks for Video Recognition\",\n        \"Appending Adversarial Frames for Universal Video Attack\",\n        \"Generalized Rank Pooling for Activity Recognition\",\n        \"P-CNN: Pose-based CNN Features for Action Recognition\",\n        \"Why Can’t I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition\",\n        \"PoTion: Pose MoTion Representation for Action Recognition\",\n        \"MARS: Motion-Augmented RGB Stream for Action Recognition\",\n        \"AutoAugment: Learning Augmentation Strategies From Data\",\n        \"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines\",\n        \"Rescaling Egocentric Vision\",\n        \"ImageNet: A Large-Scale Hierarchical Image Database\",\n        \"Improved Regularization of Convolutional Neural Networks with Cutout\",\n        \"Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification\",\n        \"Spatio-Temporal Channel Correlation Networks for Action Classification\",\n        \"Large Scale Holistic Video Understanding\",\n        \"Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification\",\n        \"Deep Temporal Linear Encoding Networks\",\n        \"Long-term Recurrent Convolutional Networks for Visual Recognition and Description\",\n        \"Omni-sourced Webly-supervised Learning for Video Recognition\",\n        \"Temporal Cycle-Consistency Learning\",\n        \"ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding\",\n        \"Watching a small portion could be as good as watching all: Towards efficient video classification\",\n        \"End-to-End Learning of Motion Representation for Video Understanding\",\n        \"More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation\",\n        \"X3D: Expanding Architectures for Efficient Video Recognition\",\n        \"SlowFast Networks for Video Recognition\",\n        \"Spatiotemporal Residual Networks for Video Action Recognition\",\n        \"Spatiotemporal Multiplier Networks for Video Action Recognition\",\n        \"What Have We Learned From Deep Representations for Action Recognition?\",\n        \"Deep insights into convolutional networks for video recognition\",\n        \"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\n        \"Discriminative Hierarchical Rank Pooling for Activity Recognition\",\n        \"Self-supervised video representation learning with odd-one-out networks\",\n        \"Modeling Video Evolution For Action Recognition\",\n        \"Learning End-to-end Video Classification with Rank-Pooling\",\n        \"Multi-modal Transformer for Video Retrieval\",\n        \"Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition\",\n        \"Exploring Semantic Inter-Class Relationships (SIR) for Zero-Shot Action Recognition\",\n        \"Webly-supervised video recognition by mutually voting for relevant web images and web video frames\",\n        \"Modality Distillation with Multiple Stream Networks for Action Recognition\",\n        \"Large-Scale Weakly-Supervised Pre-Training for Video Action Recognition\",\n        \"Unsupervised representation learning by predicting image rotations\",\n        \"Coot: Cooperative hierarchical transformer for video-text representation learning\",\n        \"ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification\",\n        \"Distinit: Learning video representations without a single labeled video\",\n        \"Separate Visual Pathways for Perception and Action\",\n        \"Explaining and harnessing adversarial examples\",\n        \"Watching the World Go By: Representation Learning from Unlabeled Videos\",\n        \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\",\n        \"The” Something Something” Video Database for Learning and Evaluating Visual Common Sense\",\n        \"AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions\",\n        \"Video representation learning by dense predictive coding\",\n        \"Memory-augmented dense predictive coding for video representation learning\",\n        \"Self-supervised co-training for video representation learning\",\n        \"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\",\n        \"Momentum contrast for unsupervised visual representation learning\",\n        \"Deep Residual Learning for Image Recognition\",\n        \"Going Deeper into Action Recognition: A Survey\",\n        \"Long Short-Term Memory\",\n        \"Determining Optical Flow\",\n        \"Searching for MobileNetV3\",\n        \"Squeeze-and-Excitation Networks\",\n        \"What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets\",\n        \"Densely Connected Convolutional Networks\",\n        \"Timeception for complex action recognition\",\n        \"Adversarial attacks for optical flow-based action recognition classifiers\",\n        \"Space-time correspondence as a contrastive random walk\",\n        \"Objects2action: Classifying and Localizing Actions without Any Video Example\",\n        \"Deep Domain Adaptation in Action Space\",\n        \"Action genome: Actions as compositions of spatio-temporal scene graphs\",\n        \"3D Convolutional Neural Networks for Human Action Recognition\",\n        \"STM: SpatioTemporal and Motion Encoding for Action Recognition\",\n        \"Black-box adversarial attacks on video recognition models\",\n        \"Self-supervised spatiotemporal feature learning via video rotation prediction\",\n        \"Shuffle and Attend: Video Domain Adaptation\",\n        \"Zero-Shot Action Recognition in Videos: A Survey\",\n        \"Aggregating Local Descriptors into a Compact Image Representation\",\n        \"AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos\",\n        \"Large-Scale Video Classification with Convolutional Neural Networks\",\n        \"The Kinetics Human Action Video Dataset\",\n        \"EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition\",\n        \"Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles\",\n        \"Fine-grained action recognition in assembly work scenes by drawing attention to the hands\",\n        \"Human Action Recognition and Prediction: A Survey\",\n        \"Cooperative learning of audio and video models from self-supervised synchronization, 2018\",\n        \"SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition\",\n        \"ImageNet Classification with Deep Convolutional Neural Networks\",\n        \"The language of actions: Recovering the syntax and semantics of goal-directed human activities\",\n        \"HMDB: A Large Video Database for Human Motion Recognition\",\n        \"Motionsqueeze: Neural motion feature learning for video understanding\",\n        \"Resource Efficient 3D Convolutional Neural Networks\",\n        \"Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition\",\n        \"The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition\",\n        \"Deep Local Video Feature for Action Recognition\",\n        \"Unsupervised Representation Learning by Sorting Sequence, 2017\",\n        \"Motion Feature Network: Fixed Motion Filter for Action Recognition\",\n        \"The ava-kinetics localized human actions video dataset\",\n        \"Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation\",\n        \"Adversarial perturbations against real-time video classification systems\",\n        \"Joint-task self-supervised learning for temporal correspondence\",\n        \"Directional temporal modeling for action recognition\",\n        \"TEA: Temporal Excitation and Aggregation for Action Recognition\",\n        \"VLAD3: Encoding Dynamics of Deep Features for Action Recognition\",\n        \"RESOUND: Towards Action Recognition without Representation Bias\",\n        \"VideoLSTM Convolves, Attends and Flows for Action Recognition\",\n        \"Fast AutoAugment\",\n        \"Training Kinetics in 15 Minutes: Large-scale Distributed Training on Videos\",\n        \"TSM: Temporal Shift Module for Efficient Video Understanding\",\n        \"Use What You Have: Video Retrieval using Representations from Collaborative Experts\",\n        \"DARTS: Differentiable Architecture Search\",\n        \"Forecasting human object interaction: Joint prediction of motor attention and egocentric activity\",\n        \"TEINet: Towards an Efficient Architecture for Video Recognition\",\n        \"TAM: Temporal Adaptive Module for Video Recognition\",\n        \"Unsupervised learning of long-term motion dynamics for videos\",\n        \"TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition\",\n        \"Going deeper into first-person activity recognition\",\n        \"Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions\",\n        \"End-to-End Learning of Visual Representations from Uncurated Instructional Videos\",\n        \"Learning a text-video embedding from incomplete and heterogeneous data\",\n        \"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\n        \"Shuffle and learn: unsupervised learning using temporal order verification\",\n        \"Moments in Time Dataset: One Million Videos for Event Understanding\",\n        \"Multi-moments in time: Learning and interpreting models for multi-action video understanding\",\n        \"Audio-visual instance discrimination with cross-modal agreement\",\n        \"Multi-Modal Domain Adaptation for Fine-Grained Action Recognition\",\n        \"ActionFlowNet: Learning Motion Representation for Action Recognition\",\n        \"Temporal Difference Networks for Video Action Recognition\",\n        \"Multimodal deep learning\",\n        \"Weakly supervised action localization by sparse temporal pooling network\",\n        \"Joint Action Recognition and Pose Estimation From Video\",\n        \"Representation learning with contrastive predictive coding\",\n        \"Adversarial Cross-Domain Action Recognition with Co-Attention\",\n        \"Context encoders: Feature learning by inpainting\",\n        \"Multi-modal self-supervision from generalized data transformations\",\n        \"W-talc: Weakly-supervised temporal activity localization and classification\",\n        \"Video Action Recognition Via Neural Architecture Searching\",\n        \"Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice\",\n        \"Action Recognition with Stacked Fisher Vectors\",\n        \"DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition\",\n        \"Efficient Neural Architecture Search via Parameter Sharing\",\n        \"Tiny Video Networks\",\n        \"Evolving Losses for Unsupervised Video Representation Learning\",\n        \"Evolving Space-Time Neural Architectures for Videos\",\n        \"Representation Flow for Action Recognition\",\n        \"Avid dataset: Anonymized videos from diverse countries, 2020\",\n        \"Learning multimodal representations for unseen activities, 2020\",\n        \"Spatiotemporal contrastive video representation learning\",\n        \"Zero-Shot Action Recognition with Error-Correcting Output Codes\",\n        \"Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks\",\n        \"Designing Network Design Spaces\",\n        \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n        \"Weakly supervised action learning with rnn based fine-to-coarse modeling\",\n        \"Video Activity Recognition: State-of-the-Art\",\n        \"Recognizing fine-grained and composite activities using hand-centric features and script data\",\n        \"Multi-modal pyramid feature combination for human action recognition\",\n        \"Avlnet: Learning audio-visual language representations from instructional videos, 2020\",\n        \"Assemblenet++: Assembling modality representations via attention connections\",\n        \"AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures\",\n        \"Image Classification with the Fisher Vector: Theory and Practice\",\n        \"Temporal aggregate representations for long-range video understanding\",\n        \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding\",\n        \"Temporal interlacing network, 2020\",\n        \"Learning Long-Term Dependencies for Action Recognition With a Biologically-Inspired Deep Network\",\n        \"Autoloc: Weakly-supervised temporal action localization in untrimmed videos\",\n        \"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition\",\n        \"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding\",\n        \"Two-Stream Convolutional Networks for Action Recognition in Videos\",\n        \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n        \"A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection\",\n        \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\n        \"D3D: Distilled 3D Networks for Video Action Recognition\",\n        \"Lsta: Long short-term attention for egocentric action recognition\",\n        \"Human Action Recognition across Datasets by Foreground-Weighted Histogram Decomposition\",\n        \"Learning video representations using contrastive bidirectional transformer, 2019\",\n        \"VideoBERT: A Joint Model for Video and Language Representation Learning\",\n        \"Lattice Long Short-Term Memory for Human Action Recognition\",\n        \"Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition\",\n        \"Going Deeper with Convolutions\",\n        \"Intriguing Properties of Neural Networks\",\n        \"Convolutional Learning of Spatio-temporal Features\",\n        \"Contrastive multiview coding\",\n        \"Learning Spatiotemporal Features with 3D Convolutional Networks\",\n        \"Video Classification With Channel-Separated Convolutional Networks\",\n        \"A Closer Look at Spatiotemporal Convolutions for Action Recognition\",\n        \"Action Recognition in Video Sequences using Deep Bi-Directional LSTM With CNN Features\",\n        \"Long-term Temporal Convolutions for Action Recognition\",\n        \"Attention is All You Need\",\n        \"Anticipating visual representations from unlabeled video\",\n        \"Action Recognition by Dense Trajectories\",\n        \"Action Recognition with Improved Trajectories\",\n        \"Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics\",\n        \"Hallucinating idt descriptors and i3d optical flow features for action recognition with cnns\",\n        \"Appearance-and-Relation Networks for Video Classification\",\n        \"Action Recognition With Trajectory-Pooled Deep-Convolutional Descriptors\",\n        \"CUHK and SIAT Submission for THUMOS15 Action Recognition Challenge\",\n        \"Untrimmednets for weakly supervised action recognition and detection\",\n        \"Towards Good Practices for Very Deep Two-Stream ConvNets\",\n        \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\n        \"Non-Local Neural Networks\",\n        \"Videos as Space-Time Region Graphs\",\n        \"Learning Correspondence from the Cycle-Consistency of Time\",\n        \"Attentionnas: Spatiotemporal attention cell search for video classification, 2020\",\n        \"Symbiotic attention for egocentric action recognition with object-centric alignment\",\n        \"Pulling actions out of context: Explicit separation for effective combination\",\n        \"Spatiotemporal Pyramid Network for Video Action Recognition\",\n        \"Heuristic black-box adversarial attacks on video recognition models\",\n        \"Mimetics: Towards understanding human actions out of context\",\n        \"Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions\",\n        \"Long-Term Feature Banks for Detailed Video Understanding\",\n        \"A Multigrid Method for Efficiently Training Video Models\",\n        \"Compressed Video Action Recognition\",\n        \"Harnessing Object and Scene Semantics for Large-Scale Video Understanding\",\n        \"Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification\",\n        \"AdaFrame: Adaptive Frame Selection for Fast Video Recognition\",\n        \"Unsupervised feature learning via non-parametric instance discrimination\",\n        \"Deep Learning for Video Classification and Captioning\",\n        \"Audiovisual SlowFast Networks for Video Recognition\",\n        \"Aggregated Residual Transformations for Deep Neural Networks\",\n        \"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification\",\n        \"Self-supervised spatiotemporal learning via video clip order prediction\",\n        \"Dual Many-to-One-Encoder-based Transfer Learning for Cross-Dataset Human Action Recognition\",\n        \"Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation\",\n        \"Transductive Zero-Shot Action Recognition by Word-Vector Embedding\",\n        \"A Discriminative CNN Video Representation for Event Detection\",\n        \"Sparse black-box video attack with reinforcement learning\",\n        \"Spatial temporal graph convolutional networks for skeleton-based action recognition\",\n        \"Video Representation Learning with Visual Tempo Consistency\",\n        \"Temporal Pyramid Network for Action Recognition\",\n        \"Hierarchical contrastive motion learning for video action recognition, 2020\",\n        \"Describing Videos by Exploiting Temporal Structure\",\n        \"End-to-end learning of action detection from frame glimpses in videos\",\n        \"Two-Stream SR-CNNs for Action Recognition in Videos\",\n        \"Beyond Short Snippets: Deep Networks for Video Classification\",\n        \"CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features\",\n        \"A Duality based Approach for Realtime TV-L1 Optical Flow\",\n        \"Real-time Action Recognition with Enhanced Motion Vector CNNs\",\n        \"Pan: Towards fast action recognition via learning persistence of appearance, 2020\",\n        \"Mixup: Beyond Empirical Risk Minimization\",\n        \"ResNeSt: Split-Attention Networks\",\n        \"Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior\",\n        \"A Comprehensive Survey of Vision-Based Human Action Recognition Methods\",\n        \"Colorful image colorization\",\n        \"Split-brain autoencoders: Unsupervised learning by cross-channel prediction\",\n        \"V4D:4D Convolutional Neural Networks for Video-level Representation Learning\",\n        \"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\",\n        \"Hierarchically decoupled spatial-temporal contrast for self-supervised video representation learning, 2020\",\n        \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\",\n        \"Trajectory Convolution for Action Recognition\",\n        \"Temporal Relational Reasoning in Videos\",\n        \"Places: A 10 million Image Database for Scene Recognition\",\n        \"MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition\",\n        \"Faster recurrent networks for efficient video classification\",\n        \"Bidirectional Multirate Reconstruction for Temporal Modeling in Videos\",\n        \"Uncovering Temporal Context for Video Question Answering\",\n        \"ActBERT: Learning Global-Local Video-Text Representations\",\n        \"A3d: Adaptive 3d networks for video action recognition, 2020\",\n        \"A Key Volume Mining Deep Framework for Action Recognition\",\n        \"Hidden Two-Stream Convolutional Networks for Action Recognition\",\n        \"Towards Universal Representation for Unseen Action Recognition\",\n        \"Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition\",\n        \"Random Temporal Skipping for Multirate Video Analysis\",\n        \"Chained Multi-Stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection\",\n        \"ECO: Efficient Convolutional Network for Online Video Understanding\"\n    ],\n    \"reasoning\": \"Selected these 282 references because they represent foundational work and recent advances in A Comprehensive Study of Deep Video Action Recognition, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}