{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 249 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey on Evaluation of Large Language Models\n\nPaper Abstract: Abstract. Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.\n\n{retrieved_text}\n\nGenerate exactly 249 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 249\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Benchmarking Arabic AI with Large Language Models\",\n        \"Mega: Multilingual evaluation of generative ai\",\n        \"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models\",\n        \"A general language assistant as a laboratory for alignment\",\n        \"Benchmarking Foundation Models with Language-Model-as-an-Examiner\",\n        \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\",\n        \"Comparing automatic and human evaluation of NLG systems\",\n        \"Cross-Validation\",\n        \"Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models\",\n        \"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3’s personality instruments results\",\n        \"On the opportunities and risks of foundation models\",\n        \"What is intelligence?\",\n        \"Class-based n-gram models of natural language\",\n        \"Language models are few-shot learners\",\n        \"Sparks of artificial general intelligence: Early experiments with gpt-4\",\n        \"Evaluating the feasibility of ChatGPT in healthcare: an analysis of multiple clinical and research scenarios\",\n        \"Do Large Language Models Understand Chemistry? A Conversation with ChatGPT\",\n        \"Evaluating large language models trained on code\",\n        \"Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study\",\n        \"The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations\",\n        \"INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models\",\n        \"Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Deep reinforcement learning from human preferences\",\n        \"Frederick Naylor\",\n        \"Evaluating Language Models for Mathematics through Interactions\",\n        \"Support-vector networks\",\n        \"Uncovering ChatGPT’s Capabilities in Recommender Systems\",\n        \"Can large language models provide feedback to students? a case study on chatgpt\",\n        \"Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination\",\n        \"Can ChatGPT pass high school exams on English language comprehension\",\n        \"How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?\",\n        \"Toxicity in chatgpt: Analyzing persona-assigned language models\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Satyapriya Krishna\",\n        \"Alpacafarm: A simulation framework for methods that learn from human feedback\",\n        \"Analysis of large-language model versus human performance for genetics questions\",\n        \"Recommender Systems in the Era of Large Language Models (LLMs)\",\n        \"Ddxplus: A new dataset for automatic medical diagnosis\",\n        \"Should chatgpt be biased? challenges and risks of bias in large language models\",\n        \"GPT-3: Its nature, scope, limits, and consequences\",\n        \"Baby steps in evaluating the capacities of large language models\",\n        \"Mathematical capabilities of chatgpt\",\n        \"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\",\n        \"Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models’ Reasoning Performance\",\n        \"Estimation of prediction error by using K-fold cross-validation\",\n        \"Perceptron-based learning algorithms\",\n        \"Adaptive Testing of Computer Vision Models\",\n        \"Introduction to the special issue on statistical language modeling\",\n        \"Making pre-trained language models better few-shot learners\",\n        \"Suchin Gururangan\",\n        \"Selective classification for deep neural networks\",\n        \"Trueteacher: Learning factual consistency evaluation with large language models\",\n        \"Large Language Models Are Not Abstract Reasoners\",\n        \"How does CHATGPT perform on the United States Medical Licensing Examination? the implications of large language models for medical education and knowledge assessment\",\n        \"Moral foundations theory: The pragmatic validity of moral pluralism\",\n        \"Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation\",\n        \"What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks\",\n        \"Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models – and Disappeared in GPT-4\",\n        \"Evaluation of AI Chatbots for Patient-Specific EHR Questions\",\n        \"Equality of opportunity in supervised learning\",\n        \"The political ideology of conversational AI: Converging evidence on ChatGPT’s pro-environmental, left-libertarian orientation\",\n        \"Can Large Language Models Understand Real-World Complex Instructions?\",\n        \"Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests\",\n        \"Measuring coding challenge competence with apps\",\n        \"Aligning ai with shared human values\",\n        \"Measuring massive multitask language understanding\",\n        \"Cuad: An expert-annotated nlp dataset for legal contract review\",\n        \"Measuring mathematical problem solving with the math dataset\",\n        \"Evaluating large language models on a highly-specialized topic, radiation oncology physics\",\n        \"TRUE: Re-evaluating factual consistency evaluation\",\n        \"Choice-75: A Dataset on Decision Branching in Script Learning\",\n        \"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench\",\n        \"Language is not all you need: Aligning perception with language models\",\n        \"C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models\",\n        \"TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models\",\n        \"Open-source Large Language Models Leaderboard\",\n        \"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers\",\n        \"Bring Your Own Data! Self-Supervised Evaluation for Large Language Models\",\n        \"Online question and answer sessions: How students support their own and other students’ processes of inquiry in a text-based learning environment\",\n        \"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models\",\n        \"Beavertails: Towards improved safety alignment of llm via a human-preference dataset\",\n        \"Structgpt: A general framework for large language model to reason over structured data\",\n        \"Assessing the accuracy and reliability of AI-generated medical responses: an evaluation of the Chat-GPT model\",\n        \"Language Models (Mostly) Know What They Know\",\n        \"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\",\n        \"ChatGPT for good? On opportunities and challenges of large language models for education\",\n        \"covLLM: Large Language Models for COVID-19 Biomedical Literature\",\n        \"Dynabench: Rethinking benchmarking in NLP\",\n        \"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models\",\n        \"Natural Questions: a Benchmark for Question Answering Research\",\n        \"Evaluating the use of large language model in identifying top research questions in gastroenterology\",\n        \"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning\",\n        \"Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design\",\n        \"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets\",\n        \"An Evaluation of Log Parsing with ChatGPT\",\n        \"Deep learning\",\n        \"Can Large Language Models Infer and Disagree Like Humans?\",\n        \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Seed-bench: Benchmarking multimodal llms with generative comprehension\",\n        \"CMMLU: Measuring massive multitask language understanding in Chinese\",\n        \"API-Bank: A Benchmark for Tool-Augmented LLMs\",\n        \"Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights\",\n        \"A Survey on Out-of-Distribution Evaluation of Neural NLP Models\",\n        \"AlpacaEval: An Automatic Evaluator of Instruction-following Models\",\n        \"Evaluating object hallucination in large vision-language models\",\n        \"Holistic evaluation of language models\",\n        \"Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models\",\n        \"UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation\",\n        \"Can large language models reason about medical questions?\",\n        \"Truthfulqa: Measuring how models mimic human falsehoods\",\n        \"LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models\",\n        \"M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models\",\n        \"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning\",\n        \"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4\",\n        \"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\",\n        \"MMBench: Is Your Multi-modal Model an All-around Player?\",\n        \"Summary of chatgpt/gpt-4 research and perspective towards the future of large language models\",\n        \"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings\",\n        \"Can chatgpt forecast stock price movements? Return predictability and large language models\",\n        \"New trends in machine translation using large language models: Case examples with chatgpt\",\n        \"Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising results, limitations, and potential\",\n        \"Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking\",\n        \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\",\n        \"MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization\",\n        \"Dynamic benchmarking of masked language models on temporal concept drift with multiple views\",\n        \"What is artificial intelligence\",\n        \"Bing Chat\",\n        \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\",\n        \"Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence\",\n        \"Adversarial NLI: A new benchmark for natural language understanding\",\n        \"Codegen: An open large language model for code with multi-turn program synthesis\",\n        \"Why we need new evaluation metrics for NLG\",\n        \"ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models\",\n        \"GPT-4 Technical Report\",\n        \"Human-like problem-solving abilities in large language models using ChatGPT\",\n        \"ThoughtSource: A central hub for large language model reasoning data\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Understanding the Capabilities of Large Language Models for Automated Planning\",\n        \"Unifying Large Language Models and Knowledge Graphs: A Roadmap\",\n        \"Talm: Tool augmented language models\",\n        \"Vishakh Padmakumar\",\n        \"Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs\",\n        \"Validity problems comparing values across cultures and possible solutions\",\n        \"Measuring and Modifying Factual Knowledge in Large Language Models\",\n        \"Adversarially constructed evaluation sets are more challenging, but may not be fair\",\n        \"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer\",\n        \"Is ChatGPT a general-purpose natural language processing task solver?\",\n        \"Tool Learning with Foundation Models\",\n        \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\",\n        \"Improving language understanding by generative pre-training\",\n        \"A Survey of Hallucination in Large Foundation Models\",\n        \"Marco Tulio Ribeiro and Scott Lundberg\",\n        \"Beyond accuracy: Behavioral testing of NLP models with CheckList\",\n        \"The Two Word Test: A Semantic Benchmark for Large Language Models\",\n        \"The Self-Perception and Political Biases of ChatGPT\",\n        \"Personality Traits in Large Language Models\",\n        \"Assessing the accuracy of responses by the language model ChatGPT to questions regarding bariatric surgery\",\n        \"Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples\",\n        \"ARB: Advanced Reasoning Benchmark for Large Language Models\",\n        \"Toolformer: Language models can teach themselves to use tools\",\n        \"Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education\",\n        \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface\",\n        \"Moral mimicry: Large language models produce moral rationalizations tailored to political identity\",\n        \"Large Language Models Encode Clinical Knowledge\",\n        \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\",\n        \"Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs\",\n        \"ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks\",\n        \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\",\n        \"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent\",\n        \"EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models\",\n        \"Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models\",\n        \"Trialling a large language model (ChatGPT) in general practice with the Applied Knowledge Test: observational study demonstrating opportunities and limitations in primary care\",\n        \"Lamda: Language models for dialog applications\",\n        \"Dynatask: A framework for creating dynamic AI benchmark tasks\",\n        \"Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback\",\n        \"Open-source tools learning benchmarks\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Computing machinery and intelligence\",\n        \"On the Planning Abilities of Large Language Models–A Critical Investigation\",\n        \"Large Language Models Still Can’t Plan (A Benchmark for LLMs on Planning and Reasoning about Change)\",\n        \"Emiel Van Miltenburg\",\n        \"Attention is all you need\",\n        \"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\",\n        \"Superglue: A stickier benchmark for general-purpose language understanding systems\",\n        \"GLUE: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\",\n        \"GPT-J-6B: A 6 billion parameter autoregressive language model\",\n        \"Adversarial glue: A multi-task benchmark for robustness evaluation of language models\",\n        \"Evaluating open question answering evaluation\",\n        \"Chain-of-thought prompting for responding to in-depth dialogue questions with LLM\",\n        \"Generalizing to unseen domains: A survey on domain generalization\",\n        \"Document-level machine translation with large language models\",\n        \"Large language models are not fair evaluators\",\n        \"Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction\",\n        \"CMB: A Comprehensive Medical Benchmark in Chinese\",\n        \"Emotional Intelligence of Large Language Models\",\n        \"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback\",\n        \"Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation\",\n        \"Exploring Vision-Language Models for Imbalanced Learning\",\n        \"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\",\n        \"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today\",\n        \"Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study\",\n        \"Emergent abilities of large language models\",\n        \"CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?\",\n        \"A prompt pattern catalog to enhance prompt engineering with chatgpt\",\n        \"Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation\",\n        \"Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting\",\n        \"An Empirical Study on Challenging Math Problem Solving with GPT-4\",\n        \"Autoformalization with large language models\",\n        \"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks\",\n        \"Ask Again, Then Fail: Large Language Models’ Vacillations in Judgement\",\n        \"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views\",\n        \"CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility\",\n        \"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models\",\n        \"Large language models can rate news outlet credibility\",\n        \"Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective\",\n        \"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark\",\n        \"KoLA: Carefully Benchmarking World Knowledge of Large Language Models\",\n        \"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models\",\n        \"Mm-vet: Evaluating large multimodal models for integrated capabilities\",\n        \"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations\",\n        \"How well do Large Language Models perform in Arithmetic tasks?\",\n        \"Glm-130b: An open bilingual pre-trained model\",\n        \"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning\",\n        \"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation\",\n        \"Opt: Open pre-trained transformer language models\",\n        \"Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models\",\n        \"Bertscore: Evaluating text generation with bert\",\n        \"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models\",\n        \"Sentiment Analysis in the Era of Large Language Models: A Reality Check\",\n        \"Wider and deeper llm networks are fairer llm evaluators\",\n        \"Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models\",\n        \"SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\",\n        \"MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning\",\n        \"CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models\",\n        \"A survey of large language models\",\n        \"On Evaluating Adversarial Robustness of Large Vision-Language Models\",\n        \"LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset\",\n        \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\",\n        \"Towards a unified multi-dimensional evaluator for text generation\",\n        \"Agieval: A human-centric benchmark for evaluating foundation models\",\n        \"Large language models are human-level prompt engineers\",\n        \"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts\",\n        \"Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective\",\n        \"Exploring ai ethics of chatgpt: A diagnostic analysis\",\n        \"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex\",\n        \"Fine-tuning language models from human preferences\",\n        \"Can Large Language Models Transform Computational Social Science?\"\n    ],\n    \"reasoning\": \"Selected these 249 references because they represent foundational work and recent advances in A Survey on Evaluation of Large Language Models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}