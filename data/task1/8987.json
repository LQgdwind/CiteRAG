{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 158 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Source Code Data Augmentation for Deep Learning: A Survey\n\nPaper Abstract: Abstract The increasingly popular adoption of deep learning models in many critical source code tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start with an introduction of data augmentation in source code and then provide a discussion on major representative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques useful in real-world source code scenarios and downstream tasks. Finally, we outline the prevailing challenges and potential opportunities for future research. In essence, we aim to demystify the corpus of existing literature on source code DA for deep learning, and foster further exploration in this sphere. Complementing this, we present a continually updated GitHub repository that hosts a list of update-to-date papers on DA for source code modeling, accessible at https://github.com/terryyz/DataAug4Code.\n\n{retrieved_text}\n\nGenerate exactly 158 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 158\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Summarize and generate to back-translate: Unsupervised translation of programming languages\",\n        \"Santacoder: donâ€™t reach for the stars!\",\n        \"A survey of machine learning for big code and naturalness\",\n        \"Typilus: Neural type hints\",\n        \"Self-supervised bug detection and repair\",\n        \"Mining source code repositories at massive scale using language modeling\",\n        \"code2seq: Generating sequences from structured representations of code\",\n        \"Adversarial robustness of program synthesis models\",\n        \"Can differential testing improve automatic speech recognition systems?\",\n        \"Program synthesis with large language models\",\n        \"Augmentedcode: Examining the effects of natural language resources in code retrieval models\",\n        \"Adversarial robustness for code\",\n        \"Generative code modeling with graphs\",\n        \"Deep learning based vulnerability detection: Are we there yet?\",\n        \"Evaluating large language models trained on code\",\n        \"Exploring data augmentation for code generation tasks\",\n        \"Varclr: Variable semantic representation pre-training via contrastive learning\",\n        \"Path-sensitive code embedding via contrastive learning for software vulnerability detection\",\n        \"Readsum: Retrieval-augmented adaptive transformer for source code summarization\",\n        \"srcml: An infrastructure for the exploration, analysis, and manipulation of source code: A tool demonstration\",\n        \"Randaugment: practical automated data augmentation with a reduced search space\",\n        \"A kernel theory of modern data augmentation\",\n        \"Detecting cryptography misuses with machine learning: Graph embeddings, transfer learning and data augmentation in source code related tasks\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Mossad: Defeating software plagiarism detection\",\n        \"Nl-augmenter: A framework for task-sensitive natural language augmentation\",\n        \"Towards learning (dis)-similarity of source code from program contrasts\",\n        \"Concord: Clone-aware contrastive learning for source code\",\n        \"Mixcode: Enhancing code classification by mixup-based data augmentation\",\n        \"Boosting source code learning with data augmentation: An empirical study\",\n        \"Deepdebug: Fixing python bugs using stack traces, backtranslation, and code skeletons\",\n        \"Data augmentation for low-resource neural machine translation\",\n        \"Ac/c++ code vulnerability dataset with code changes and cve summaries\",\n        \"A survey of data augmentation approaches for nlp\",\n        \"Codebert: A pre-trained model for programming and natural languages\",\n        \"Discrete adversarial attack to models of code\",\n        \"Deep api learning\",\n        \"Apicom: Automatic api completion via prompt learning and adversarial training-based data augmentation\",\n        \"Graphcode{bert}: Pre-training code representations with data flow\",\n        \"Simscood: Systematic analysis of out-of-distribution behavior of source code models\",\n        \"Mixgen: A new multi-modal data augmentation\",\n        \"Neuro-symbolic zero-shot code cloning with cross-language intermediate representation\",\n        \"Deep learning type inference\",\n        \"Semantic robustness of models of source code\",\n        \"Codes: A distribution shift benchmark dataset for source code learning\",\n        \"Summarizing source code with transferred api knowledge\",\n        \"Cosqa: 20,000+ web queries for code search and question answering\",\n        \"Api method recommendation without worrying about the task-api knowledge gap\",\n        \"Towards better multilingual code search through cross-lingual contrastive learning\",\n        \"Codesearchnet challenge: Evaluating the state of semantic code search\",\n        \"Contrastive code representation learning\",\n        \"Clawsat: Towards both robust and accurate code models\",\n        \"Inferfix: End-to-end program repair with llms\",\n        \"What do pre-trained code models know about code?\",\n        \"Exploring representation-level augmentation for code search\",\n        \"Rethinking negative pairs in code search\",\n        \"Starcoder: may the source be with you!\",\n        \"Coderetriever: A large scale contrastive pre-training method for code search\",\n        \"Multi-target backdoor attacks for code pre-trained models\",\n        \"Semantic-preserving adversarial code comprehension\",\n        \"Ropgen: Towards robust code authorship attribution via automatic coding style transformation\",\n        \"Cross-lingual transfer learning for statistical type inference\",\n        \"Unleashing the power of compiler intermediate representation to enhance neural program embeddings\",\n        \"Codeqa: A question answering dataset for source code comprehension\",\n        \"Matcha: Enhancing visual language pretraining with math reasoning and chart derendering\",\n        \"Retrieval-augmented generation for code summarization via hybrid gnn\",\n        \"Contrabert: Enhancing code pre-trained models via contrastive learning\",\n        \"Uncovering and quantifyingsocialbiases incodegeneration\",\n        \"Reacc: A retrieval-augmented code completion framework\",\n        \"CodeXGLUE: A machine learning benchmark dataset for code understanding and generation\",\n        \"Mulcs: Towards a unified deep representation for multilingual code search\",\n        \"Automatic documentation generation via source code summarization of method context\",\n        \"Pointer sentinel mixture models\",\n        \"An enhanced data augmentation approach to support multi-class code readability classification\",\n        \"Improving multi-class code readability classification with an enhanced data augmentation approach (130)\",\n        \"The effectiveness of data augmentation in code readability classification\",\n        \"Augmented language models: a survey\",\n        \"A parallel corpus of python functions and documentation strings for automated code documentation and code generation\",\n        \"Two case studies of open source software development: Apache and mozilla\",\n        \"Retrieval-based prompt selection for code-related few-shot learning\",\n        \"Codegen: An open large language model for code with multi-turn program synthesis\",\n        \"Conditional image synthesis with auxiliary classifier gans\",\n        \"Measuring the impact of programming language distribution\",\n        \"Multipas: applying program transformations to introductory programming assignments for data augmentation\",\n        \"Bleu: a method for automatic evaluation of machine translation\",\n        \"Contrastive learning with keyword-based data augmentation for code search and code question answering\",\n        \"Retrieval augmented code generation and summarization\",\n        \"Pathways to leverage transcompiler based data augmentation for cross-language clone detection\",\n        \"A search-based testing framework for deep neural networks of source code embedding\",\n        \"Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks\",\n        \"Misleading authorship attribution of source code using adversarial learning\",\n        \"Programtransformer: A tool for generating semantically equivalent transformed programs\",\n        \"On the generalizability of neural program models with respect to semantic-preserving program transformations\",\n        \"Sunny and dark outside?! improving answer consistency in vqa through entailed question generation\",\n        \"Probabilistic model for code with decision trees\",\n        \"Codebleu: a method for automatic evaluation of code synthesis\",\n        \"Unsupervised translation of programming languages\",\n        \"Leveraging automated unit tests for unsupervised code translation\",\n        \"Improving neural machine translation models with monolingual data\",\n        \"Bash comment generation via data augmentation and semantic-aware codebert\",\n        \"Cocosoda: Effective contrastive learning for code search\",\n        \"Improving imbalanced learning by pre-finetuning with data augmentation\",\n        \"Cross-modal contrastive learning for code search\",\n        \"Paraphrasing techniques for maritime qa system\",\n        \"A survey on image data augmentation for deep learning\",\n        \"Mufin: Improving neural repair models with back-translation\",\n        \"Do not have enough data? an easy data augmentation for code summarization\",\n        \"Strata: Simple, gradient-free attacks for models of code\",\n        \"Generating adversarial computer programs using optimized obfuscations\",\n        \"Neural machine translation: A review\",\n        \"Vipergpt: Visual inference via python execution for reasoning\",\n        \"Towards a big data curated benchmark of inter-project code clones\",\n        \"Semantic equivalent adversarial data augmentation for visual question answering\",\n        \"Domain adaptive code completion via language models and decoupled domain databases\",\n        \"Generating adversarial examples of source code classification models via q-learning-based markov decision process\",\n        \"Adversarial attacks on neural models of code via code difference reduction\",\n        \"Augmenting api documentation with insights from stack overflow\",\n        \"What do they capture? a structural analysis of pre-trained language models for source code\",\n        \"Improving automatic source code summarization via deep reinforcement learning\",\n        \"Bridging pre-trained models and downstream tasks for source code understanding\",\n        \"Leveraging causal inference for explainable automatic program repair\",\n        \"Two birds with one stone: Boosting code generation and code search via a generative adversarial network\",\n        \"Recode: Robustness evaluation of code generation models\",\n        \"Want to reduce labeling cost? gpt-3 can help\",\n        \"Automatically learning semantic features for defect prediction\",\n        \"Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair\",\n        \"Heloc: Hierarchical contrastive learning of source code representation\",\n        \"Test-driven multi-task learning with functionally equivalent code transformation for neural code generation\",\n        \"Plot2api: recommending graphic api from plot via semantic parsing guided neural network\",\n        \"Time series data augmentation for deep learning: A survey\",\n        \"On the generalization effects of linear transformations in data augmentation\",\n        \"Rustgen: An augmentation approach for generating compilable rust code with large language models\",\n        \"Data augmentation for code translation with comparable corpora and multiple references\",\n        \"Incorporating external knowledge through pre-training for natural language to code generation\",\n        \"Exploitgen: Template-augmented exploit code generation based on codebert\",\n        \"How important are good method names in neural code generation? a model robustness perspective\",\n        \"Assessing and improving syntactic adversarial robustness of pre-trained models for code translation\",\n        \"A pre-training method for enhanced code representation based on multimodal contrastive learning\",\n        \"Natural attack for pre-trained models of code\",\n        \"Break-it-fix-it: Unsupervised learning for program repair\",\n        \"Tram: A token-level retrieval-augmented mechanism for source code summarization\",\n        \"Adversarial examples for models of code\",\n        \"A syntactic neural model for general-purpose code generation\",\n        \"Gpt3mix: Leveraging large-scale language models for text augmentation\",\n        \"Bashexplainer: Retrieval-augmented bash code comment generation based on fine-tuned codebert\",\n        \"Data augmentation by program transformation\",\n        \"mixup: Beyond empirical risk minimization\",\n        \"Generating adversarial examples for holding robustness of source code processing models\",\n        \"Retrieval-based neural source code summarization\",\n        \"Training deep code comment generation models via data augmentation\",\n        \"Combo: Pre-training representations of binary code using contrastive learning\",\n        \"D2a: A dataset built for ai-based vulnerability detection methods using differential analysis\",\n        \"Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks\",\n        \"Adversarial robustness of deep code comment generation\",\n        \"Large language models are state-of-the-art evaluators of code generation\",\n        \"Exploring ai ethics of chatgpt: A diagnostic analysis\",\n        \"On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex\",\n        \"Evaluation of contrastive learning with various code representations for code clone detection\"\n    ],\n    \"reasoning\": \"Selected these 158 references because they represent foundational work and recent advances in Source Code Data Augmentation for Deep Learning: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}