{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Approximating Single-Source Personalized PageRank with Absolute Error Guarantees1footnote 11footnote 1This text is the full version of a paper that has been accepted for publication at the 27th International Conference on Database Theory (ICDT 2024).\n\nPaper Abstract: Abstract Personalized PageRank (PPR) is an extensively studied and applied node proximity measure in graphs. For a pair of nodes sğ‘ s and tğ‘¡t on a graph G=(V,E)ğºğ‘‰ğ¸G=(V,E), the PPR value Ï€â€‹(s,t)ğœ‹ğ‘ ğ‘¡\\pi(s,t) is defined as the probability that an Î±ğ›¼\\alpha-discounted random walk from sğ‘ s terminates at tğ‘¡t, where the walk terminates with probability Î±ğ›¼\\alpha at each step. We study the classic Single-Source PPR query, which asks for PPR approximations from a given source node sğ‘ s to all nodes in the graph. Specifically, we aim to provide approximations with absolute error guarantees, ensuring that the resultant PPR estimates Ï€^â€‹(s,t)^ğœ‹ğ‘ ğ‘¡\\hat{\\pi}(s,t) satisfy maxtâˆˆVâ¡|Ï€^â€‹(s,t)âˆ’Ï€â€‹(s,t)|â‰¤Îµsubscriptğ‘¡ğ‘‰^ğœ‹ğ‘ ğ‘¡ğœ‹ğ‘ ğ‘¡ğœ€\\max_{t\\in V}\\big{|}\\hat{\\pi}(s,t)-\\pi(s,t)\\big{|}\\leq\\varepsilon for a given error bound Îµğœ€\\varepsilon. We propose an algorithm that achieves this with high probability, with an expected running time of â€¢ O~â€‹(m/Îµ)~ğ‘‚ğ‘šğœ€\\widetilde{O}\\big{(}\\sqrt{m}/\\varepsilon\\big{)} for directed graphs333O~â€‹(â‹…)~ğ‘‚â‹…\\widetilde{O}(\\cdot) suppresses polylogâ¡(n)polylogğ‘›\\operatorname{polylog}(n) factors., where m=|E|ğ‘šğ¸m=|E|; â€¢ O~â€‹(dmax/Îµ)~ğ‘‚subscriptğ‘‘ğœ€\\widetilde{O}\\big{(}\\sqrt{d_{\\max}}/\\varepsilon\\big{)} for undirected graphs, where dmaxsubscriptğ‘‘d_{\\max} is the maximum node degree in the graph; â€¢ O~â€‹(nÎ³âˆ’1/2/Îµ)~ğ‘‚superscriptğ‘›ğ›¾12ğœ€\\widetilde{O}\\left(n^{\\gamma-1/2}/\\varepsilon\\right) for power-law graphs, where n=|V|ğ‘›ğ‘‰n=|V| and Î³âˆˆ(12,1)ğ›¾121\\gamma\\in\\left(\\frac{1}{2},1\\right) is the extent of the power law. These sublinear bounds improve upon existing results. We also study the case when degree-normalized absolute error guarantees are desired, requiring maxtâˆˆVâ¡|Ï€^â€‹(s,t)/dâ€‹(t)âˆ’Ï€â€‹(s,t)/dâ€‹(t)|â‰¤Îµdsubscriptğ‘¡ğ‘‰^ğœ‹ğ‘ ğ‘¡ğ‘‘ğ‘¡ğœ‹ğ‘ ğ‘¡ğ‘‘ğ‘¡subscriptğœ€ğ‘‘\\max_{t\\in V}\\big{|}\\hat{\\pi}(s,t)/d(t)-\\pi(s,t)/d(t)\\big{|}\\leq\\varepsilon_{d} for a given error bound Îµdsubscriptğœ€ğ‘‘\\varepsilon_{d}, where the graph is undirected and dâ€‹(t)ğ‘‘ğ‘¡d(t) is the degree of node tğ‘¡t. We give an algorithm that provides this error guarantee with high probability, achieving an expected complexity of O~â€‹(âˆ‘tâˆˆVÏ€â€‹(s,t)/dâ€‹(t)/Îµd)~ğ‘‚subscriptğ‘¡ğ‘‰ğœ‹ğ‘ ğ‘¡ğ‘‘ğ‘¡subscriptğœ€ğ‘‘\\widetilde{O}\\left(\\sqrt{\\sum_{t\\in V}\\pi(s,t)/d(t)}\\big{/}\\varepsilon_{d}\\right). This improves over the previously known Oâ€‹(1/Îµd)ğ‘‚1subscriptğœ€ğ‘‘O(1/\\varepsilon_{d}) complexity.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Local computation of pagerank contributions\",\n        \"Detecting sharp drops in pagerank and a simplified local partitioning algorithm\",\n        \"Local graph partitioning using pagerank vectors\",\n        \"Using pagerank to locally partition a graph\",\n        \"On the choice of kernel and labelled data in semi-supervised learning methods\",\n        \"Quick detection of top-k personalized pagerank lists\",\n        \"Fast personalized pagerank on mapreduce\",\n        \"Fast incremental and personalized pagerank\",\n        \"Emergence of scaling in random networks\",\n        \"Bookmark-coloring algorithm for personalized pagerank computing\",\n        \"Scaling graph neural networks with approximate pagerank\",\n        \"Directed scale-free graphs\",\n        \"The anatomy of a large-scale hypertextual web search engine\",\n        \"Survey: Concentration inequalities and martingale inequalities: A survey\",\n        \"Efficient processing of network proximity queries via chebyshev acceleration\",\n        \"Towards scaling fully personalized pagerank: Algorithms, lower bounds, and experiments\",\n        \"Variational perspective on local graph clustering\",\n        \"Fast and exact top-k search for random walk with restart\",\n        \"Efficient ad-hoc search for personalized pagerank\",\n        \"Efficient personalized pagerank with accuracy assurance\",\n        \"Pagerank beyond the web\",\n        \"Distributed algorithms on exact personalized pagerank\",\n        \"Parallel personalized pagerank on dynamic graphs\",\n        \"Massively parallel algorithms for personalized pagerank\",\n        \"Random generation of combinatorial structures from a uniform distribution\",\n        \"Bepi: Fast and memory-efficient method for billion-scale random walk with restart\",\n        \"Predict then propagate: Graph neural networks meet personalized pagerank\",\n        \"Efficient personalized pagerank computation: The power of variance-reduced monte carlo approaches\",\n        \"Efficient personalized pagerank computation: A spanning forests sampling based approach\",\n        \"Index-free approach with theoretical guarantee for efficient random walk with restart query\",\n        \"Distributed algorithms for fully personalized pagerank on large graphs\",\n        \"Personalized pagerank estimation and search: A bidirectional approach\",\n        \"Fast-ppr: scaling personalized pagerank estimation for large graphs\",\n        \"Personalized pagerank to a target node\",\n        \"Computing personalized pagerank quickly by exploiting graph structures\",\n        \"Efficient pagerank tracking in evolving networks\",\n        \"Asymmetric transitivity preserving graph embedding\",\n        \"Realtime top-k personalized pagerank over large graphs on gpus\",\n        \"Bear: Block elimination approach for random walk with restart on large graphs\",\n        \"Verse: Versatile graph embeddings from similarity measures\",\n        \"New fast method for generating discrete random numbers with arbitrary frequency distributions\",\n        \"Approximate graph propagation\",\n        \"Personalized pagerank to a target node, revisited\",\n        \"Parallelizing approximate single-source personalized pagerank queries on shared memory\",\n        \"Hubppr: Effective indexing for approximate personalized pagerank\",\n        \"Efficient algorithms for approximate single-source personalized pagerank queries\",\n        \"Fora: Simple and effective approximate single-source personalized pagerank\",\n        \"Prsim: Sublinear time simrank computation on large power-law graphs\",\n        \"Topppr: Top-k personalized pagerank queries with precision guarantees on large graphs\",\n        \"Unifying the global and local approaches: An efficient power iteration with forward push\",\n        \"Fast and unified local search for random walk based k-nearest-neighbor query in large graphs\",\n        \"Local higher-order graph clustering\",\n        \"Scalable graph embeddings via sparse transpose proximities\",\n        \"Fast and accurate random walk with restart on dynamic graphs with guarantees\",\n        \"Tpa: Fast, scalable, and accurate method for approximate random walk with restart on billion scale graphs\",\n        \"Irwr: incremental random walk with restart\",\n        \"Random walk with restart over dynamic graphs\",\n        \"Incremental and accuracy-aware personalized pagerank through scheduled approximation\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Approximating Single-Source Personalized PageRank with Absolute Error Guarantees1footnote 11footnote 1This text is the full version of a paper that has been accepted for publication at the 27th International Conference on Database Theory (ICDT 2024)., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}