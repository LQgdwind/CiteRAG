{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A locality-based approach for coded computation\n\nPaper Abstract: Abstract Modern distributed computation infrastructures are often plagued by unavailabilities such as failing or slow servers. These unavailabilities adversely affect the tail latency of computation in distributed infrastructures. While replicating computation is a simple approach to provide resilience, it entails significant resource overhead. Coded computation has emerged as a resource-efficient alternative, wherein multiple units of data are encoded to create parity units and the function to be computed is applied to each of these units on distinct servers. If some of the function outputs are unavailable, a decoder can use the available ones to decode the unavailable ones. Existing coded computation approaches are resource efficient only for simple variants of linear functions such as multilinear, with even the class of low degree polynomials necessitating the same multiplicative overhead as replication for practically relevant straggler tolerance. In this paper, we present a new approach to model coded computation via the lens of locality of codes. We introduce a generalized notion of locality, denoted computational locality, building upon the locality of an appropriately defined code. We then show an equivalence between computational locality and the required number of workers for coded computation and leverage results from the well-studied locality of codes to design coded computation schemes. Specifically, we show that recent results on coded computation of multivariate polynomials can be derived using local recovery schemes for Reed-Muller codes. We then present coded computation schemes for multivariate polynomials that adaptively exploit locality properties of input data—an inadmissible technique under existing frameworks. These schemes require fewer workers than the lower bound under existing coded computation frameworks, showing that the existing multiplicative overhead on the number of servers is not fundamental for coded computation of nonlinear functions.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"The tail at scale\",\n        \"Erasure coding vs. replication: A quantitative comparison\",\n        \"Speeding up distributed machine learning using codes\",\n        \"A unified coding framework for distributed computing with straggling servers\",\n        \"Coded computation over heterogeneous clusters\",\n        \"Rateless codes for near-perfect load balancing in distributed matrix-vector multiplication\",\n        \"Train where the data is: A case for bandwidth efficient coded training\",\n        \"Coded convolution for parallel and distributed computing within a deadline\",\n        \"Short-dot: Computing large linear transforms distributedly using coded short dot products\",\n        \"Hierarchical coded computation\",\n        \"A unified coded deep neural network training strategy based on generalized polydot codes\",\n        \"High-dimensional coded matrix multiplication\",\n        \"Polynomial codes: an optimal design for high-dimensional coded matrix multiplication\",\n        \"On the optimal recovery threshold of coded matrix multiplication\",\n        \"Coded sparse matrix multiplication\",\n        \"Straggler-proofing massive-scale distributed matrix multiplication with d-dimensional product codes\",\n        \"Exploitation of stragglers in coded computation\",\n        \"Locally recoverable coded matrix multiplication\",\n        \"Slack squeeze coded computing for adaptive straggler mitigation\",\n        \"Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding\",\n        \"Hierarchical coding for distributed computing\",\n        \"Cross subspace alignment codes for coded distributed batch computation\",\n        \"Entangled polynomial codes for secure, private, and batch distributed matrix multiplication: Breaking the ”cubic” barrier\",\n        \"Lagrange coded computing: Optimal design for resiliency, security and privacy\",\n        \"Locally decodable codes\",\n        \"On the locality of codeword symbols\",\n        \"Locally repairable codes\",\n        \"A family of optimal locally recoverable codes\",\n        \"Codes with locality for two erasures\",\n        \"On taking advantage of multiple requests in error correcting codes\",\n        \"Lagrange coded computing: Optimal design for resiliency, security, and privacy\",\n        \"Timely-throughput optimal coded computing over cloud networks\",\n        \"Gradient coding: Avoiding stragglers in distributed learning\",\n        \"Improving distributed gradient descent using reed-solomon codes\",\n        \"Near-optimal straggler mitigation for distributed gradient methods\",\n        \"DRACO: Byzantine-resilient distributed training via redundant gradients\",\n        \"Gradient coding using the stochastic block model\",\n        \"Communication-computation efficient gradient coding\",\n        \"Gradient coding from cyclic MDS codes and expander graphs\",\n        \"Erasurehead: Distributed gradient descent without delays using approximate gradient coding\",\n        \"Approximate gradient coding via sparse random graphs\",\n        \"Straggler mitigation in distributed optimization through data encoding\",\n        \"Learning a code: Machine learning for approximate non-linear coded computation\",\n        \"Parity models: Erasure-coded resilience for prediction serving systems\",\n        \"Collage Inference: Tolerating Stragglers in Distributed Neural Network Inference using Coding\",\n        \"Application of boolean algebra to switching circuit design and to error detection\",\n        \"A class of multiple-error-correcting codes and the decoding scheme\",\n        \"Efficient checking of computations\",\n        \"Hiding instances in multioracle queries\",\n        \"Self-testing/correcting for polynomials and for approximate functions\",\n        \"Highly resilient correctors for polynomials\",\n        \"Error correction for algebraic block codes\",\n        \"mixup: Beyond empirical risk minimization\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in A locality-based approach for coded computation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}