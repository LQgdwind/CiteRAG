{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 235 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions\n\nPaper Abstract: Abstract. Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.\n\n{retrieved_text}\n\nGenerate exactly 235 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 235\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep multimodal subspace clustering networks\",\n        \"Persistent anti-muslim bias in large language models\",\n        \"VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers\",\n        \"Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing\",\n        \"Analyzing the Behavior of Visual Question Answering Models\",\n        \"Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach\",\n        \"Language2pose: Natural language grounded pose forecasting\",\n        \"Domain-adversarial neural networks\",\n        \"Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text\",\n        \"A probabilistic framework to incorporate mixed-data type features: Matrix factorization with multimodal side information\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"Multimodal coordination of sound and movement in music and speech\",\n        \"Neuro-Symbolic Visual Reasoning: Disentangling Visual from Reasoning\",\n        \"Spice: Semantic propositional image caption evaluation\",\n        \"Neural module networks\",\n        \"Deep canonical correlation analysis\",\n        \"Audio-to-text alignment for speech recognition with very limited resources\",\n        \"Gated multimodal units for information fusion\",\n        \"Multimodal fusion for multimedia analysis: a survey\",\n        \"Dbpedia: A nucleus for a web of open data\",\n        \"Comparison of redundancy and relevance measures for feature selection in tissue classification of CT images\",\n        \"Co-training and expansion: Towards bridging theory and practice\",\n        \"Multimodal machine learning: A survey and taxonomy\",\n        \"On the Benefits of Early Fusion in Multimodal Representation Learning\",\n        \"The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations\",\n        \"Image-music-text\",\n        \"Analysis of representations for domain adaptation\",\n        \"Mutan: Multimodal tucker fusion for visual question answering\",\n        \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\",\n        \"Representation Learning: A Review and New Perspectives\",\n        \"Multimodal datasets: misogyny, pornography, and malignant stereotypes\",\n        \"Experience Grounds Language\",\n        \"Combining labeled and unlabeled data with co-training\",\n        \"A theory of consciousness from a theoretical computer science perspective: Insights from the Conscious Turing Machine\",\n        \"Freebase: a collaboratively created graph database for structuring human knowledge\",\n        \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n        \"HoME: a Household Multimodal Environment\",\n        \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges\",\n        \"Gender shades: Intersectional accuracy disparities in commercial gender classification\",\n        \"A survey on multimodal data-driven smart healthcare systems: approaches and applications\",\n        \"Juan C Caicedo and Fabio A González\",\n        \"Matthew Jagielski\",\n        \"Prithvijit Chattopadhyay\",\n        \"Kanthashree Mysore Sathyendra\",\n        \"Andrew Rouditchenko\",\n        \"VisualGPT: Data-efficient adaptation of pretrained language models for image captioning\",\n        \"Jingqiang Chen and Hai Zhuge\",\n        \"What comprises a good talking-head video generation?: A survey and benchmark\",\n        \"Tadas Baltrušaitis\",\n        \"Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers\",\n        \"Taylor Berg-Kirkpatrick\",\n        \"Louis-Philippe Morency\",\n        \"Emilie Delaherche and Mohamed Chetouani\",\n        \"Emily Denton and Rob Fergus\",\n        \"Challenges in real-life emotion annotation and machine learning based detection\",\n        \"Multimodal safety-critical scenarios generation for decision-making algorithms evaluation\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Cross-modal data programming enables rapid medical machine learning\",\n        \"Notes on noise contrastive estimation and negative sampling\",\n        \"Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention\",\n        \"Haoqi Fan and Jiatong Zhou\",\n        \"Mohammad Amin Sadeghi\",\n        \"Andreas Foltyn and Jessica Deuschel\",\n        \"Predictive learning via rule ensembles\",\n        \"Marc’Aurelio Ranzato\",\n        \"Razieh Khamsehashari\",\n        \"Perceptual Score: What Data Modalities Does Your Model Perceive?\",\n        \"Suchin Gururangan\",\n        \"Shortcut learning in deep neural networks\",\n        \"Pratyay Banerjee\",\n        \"Douglas Summers-Stay\",\n        \"Towards transparent ai systems: Interpreting visual question answering models\",\n        \"KAT: A Knowledge Augmented Transformer for Vision-and-Language\",\n        \"Abigale J Stangl\",\n        \"The social impact of deepfakes\",\n        \"Decoupling the role of data, attention, and losses in multimodal transformers\",\n        \"Jack Hessel and Lillian Lee\",\n        \"beta-vae: Learning basic visual concepts with a constrained variational framework\",\n        \"Multimodal Co-Training for Selecting Good Examples from Webly Labeled Video\",\n        \"Learning to compose and reason with language tree structures for visual grounding\",\n        \"Deep multimodal multilinear fusion with high-order polynomial pooling\",\n        \"Unsupervised multimodal representation learning across medical images and reports\",\n        \"Disentangling by Partitioning: A Representation Learning Framework for Multimodal Sensory Data\",\n        \"Multimodal adversarial network for cross-modal retrieval\",\n        \"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\",\n        \"Multimodal representation learning for recommendation in Internet of Things\",\n        \"Learning by abstraction: The neural state machine\",\n        \"Compositional attention networks for machine reasoning\",\n        \"Drew A Hudson and Christopher D Manning\",\n        \"Evidential Sparsification of Multimodal Latent Spaces in Conditional Variational Autoencoders\",\n        \"Perceiver: General perception with iterative attention\",\n        \"Multimodal human–computer interaction: A survey\",\n        \"You said that?: Synthesising talking faces from audio\",\n        \"Mohammad Hasanuzzaman\",\n        \"Jonathan Schwarz\",\n        \"MIMIC-III, a freely accessible critical care database\",\n        \"Bharath Hariharan\",\n        \"One model to learn them all\",\n        \"Deep fragment embeddings for bidirectional image sentence mapping\",\n        \"Conjugate mixture models for clustering multimodal data\",\n        \"Approximate graph Laplacians for multimodal data clustering\",\n        \"Joint patch clustering-based dictionary learning for multimodal image fusion\",\n        \"Embedded multimodal interfaces in robotics: applications, future trends, and societal implications\",\n        \"Stephen Mussmann\",\n        \"Multimodal images in the brain\",\n        \"Visual genome: Connecting language and vision using crowdsourced dense image annotations\",\n        \"An overview of sequence comparison: Time warps, string edits, and macromolecules\",\n        \"Kernel and nonlinear canonical correlation analysis\",\n        \"Krishnan Srinivasan\",\n        \"Visualbert: A simple and performant baseline for vision and language\",\n        \"VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles\",\n        \"Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text\",\n        \"Pre-Trained Language Models for Interactive Decision-Making\",\n        \"Xiang Lisa Li and Percy Liang\",\n        \"Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness\",\n        \"Yao-Hung Hubert Tsai\",\n        \"Gunjan Chhablani\",\n        \"HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning\",\n        \"Leslie Yufan Chen\",\n        \"PolyViT: Co-training Vision Transformers on Images, Videos and Audio\",\n        \"Temporal fusion transformers for interpretable multi-horizon time series forecasting\",\n        \"Alberto Garcia-Duran\",\n        \"Varun Bharadhwaj Lakshminarasimhan\",\n        \"Pretrained transformers as universal computation engines\",\n        \"Gregory Farquhar\",\n        \"DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations\",\n        \"Smil: Multimodal learning with severely missing modality\",\n        \"Multisensory spatial interactions: a window onto functional integration in the human brain\",\n        \"An overview on clustering methods\",\n        \"Nicholas Johnston\",\n        \"Joshua B Tenenbaum\",\n        \"Alexander Toshev\",\n        \"Spoken language interaction with robots: Recommendations for future research\",\n        \"Ruslan Salakhutdinov\",\n        \"A taxonomy of relationships between images and text\",\n        \"Alexander M Rush\",\n        \"Luke Merrick and Ankur Taly\",\n        \"Pratul P Srinivasan\",\n        \"WordNet: a lexical database for English\",\n        \"Antoine Veillard\",\n        \"A comprehensive survey on multimodal medical signals fusion for smart healthcare systems\",\n        \"Jonathan Munro and Dima Damen\",\n        \"Multimodal mental imagery\",\n        \"Large-scale concept ontology for multimedia\",\n        \"Grounding language for transfer in deep reinforcement learning\",\n        \"A hybrid latent space data fusion method for multimodal emotion recognition\",\n        \"A review of relational machine learning for knowledge graphs\",\n        \"Modeling multimodal human-computer interaction\",\n        \"Characterization and classification of semantic image-text relations\",\n        \"Multimodal abstractive summarization for how2 videos\",\n        \"Decoding brain representations by multimodal learning of neural activity and visual features\",\n        \"Lisa Anne Hendricks\",\n        \"Chandra Bhagavatula\",\n        \"Communication goes multimodal\",\n        \"Valentin Vielzeuf\",\n        \"Chris M Cervantes\",\n        \"A review of affective computing: From unimodal analysis to multimodal fusion\",\n        \"Sandhini Agarwal\",\n        \"Language Models are Unsupervised Multitask Learners\",\n        \"AmirAli Bagher Zadeh\",\n        \"Jose Costa Pereira\",\n        \"Fastspeech: Fast, robust and controllable text to speech\",\n        \"Andreas Blattmann\",\n        \"Measuring Social Biases in Grounded Vision and Language Embeddings\",\n        \"Subhashini Venugopalan\",\n        \"Multimodal graph networks for compositional generalization in visual question answering\",\n        \"Audiovisual synchronization and fusion using canonical correlation analysis\",\n        \"Oleksandr Maksymets\",\n        \"The graph neural network model\",\n        \"Symeon Papadopoulos\",\n        \"Michael Cogswell\",\n        \"Logic tensor networks: Deep learning and logical reasoning from data and knowledge\",\n        \"A mathematical theory of communication\",\n        \"Toward multimodal human–computer interface\",\n        \"A survey of heterogeneous information network analysis\",\n        \"Variational mixture-of-experts autoencoders for multimodal deep generative models\",\n        \"Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n        \"Make-a-video: Text-to-video generation without text-video data\",\n        \"FLAVA: A Foundational Language And Vision Alignment Model\",\n        \"Zero-shot learning through cross-modal transfer\",\n        \"Kvl-bert: Knowledge enhanced visual-and-linguistic bert for visual commonsense reasoning\",\n        \"An information theoretic framework for multi-view learning\",\n        \"Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models\",\n        \"Nlvr2 visual bias analysis\",\n        \"Reinforcement learning: An introduction\",\n        \"Synthesizing obama: learning lip sync from audio\",\n        \"Multimodal logical inference system for visual-textual entailment\",\n        \"Mgat: Multimodal graph attention network for recommendation\",\n        \"Rainer Stiefelhagen\",\n        \"Canonical correlation analysis\",\n        \"What makes for good views for contrastive learning?\",\n        \"Akshay Krishnamurthy\",\n        \"Deep canonical time warping for simultaneous alignment and representation learning of sequences\",\n        \"Learning factorized multimodal representations\",\n        \"Multimodal few-shot learning with frozen language models\",\n        \"Corpus-based learning of analogies and semantic relations\",\n        \"Multimodality and reading: The construction of meaning through image-text interaction\",\n        \"Multimodal research in vision and language: A review of current and emerging trends\",\n        \"Jeffrey P Bigham\",\n        \"Neural discrete representation learning\",\n        \"Analyzing differentiable fuzzy logic operators\",\n        \"Guillem Cucurull\",\n        \"Order-embeddings of images and language\",\n        \"Subspace clustering\",\n        \"Show and tell: Lessons learned from the 2015 mscoco image captioning challenge\",\n        \"Sarah Adel Bargal\",\n        \"Multimodal graph-based reranking for web image search\",\n        \"M2Lens: Visualizing and explaining multimodal models for sentiment analysis\",\n        \"Asli Celikyilmaz\",\n        \"Order embeddings and character-level convolutions for multimodal alignment\",\n        \"Face-to-Face Contrastive Learning for Social Intelligence Question-Answering\",\n        \"Nonnegative decomposition of multivariate information\",\n        \"Shibani Santurkar\",\n        \"Multimodal generative models for scalable weakly-supervised learning\",\n        \"Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks\",\n        \"Multimodal end-to-end autonomous driving\",\n        \"Adaptive Cross-Modal Few-shot Learning\",\n        \"Multi-view intact space learning\",\n        \"Ruslan Salakhudinov\",\n        \"MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records\",\n        \"Clevrer: Collision events for video representation and reasoning\",\n        \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\n        \"Speaker identification on the SCOTUS corpus\",\n        \"Tensor fusion network for multimodal sentiment analysis\",\n        \"Navonil Mazumder\",\n        \"Foundations of multimodal co-learning\",\n        \"and Louis-Philippe Morency\",\n        \"Leonidas J Guibas\",\n        \"Merlot: Multimodal neural script knowledge models\",\n        \"Socratic models: Composing zero-shot multimodal reasoning with language\",\n        \"Ching-Yao Chuang\",\n        \"Multimodal feature fusion by relational reasoning and attention for visual question answering\",\n        \"Multi-Modal Knowledge Graph Construction and Application: A Survey\",\n        \"Building a large-scale multimodal knowledge base system for answering visual queries\",\n        \"Encoder-agnostic adaptation for conditional language generation\"\n    ],\n    \"reasoning\": \"Selected these 235 references because they represent foundational work and recent advances in Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}