{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Can Large Language Models Detect Misinformation in Scientific News Reporting?\n\nPaper Abstract: Abstract. Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database, a comprehensive collection of COVID-19 and historical coronavirus research. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. Our first architecture, SERIf, uses three modules: Summarization, Evidence Retrieval, and Inference to classify the news article as fake or true; the second architecture, SIf, bypasses the explicit evidence retrieval module while keeping the other two, and the third, direct-to-inference architecture, D2I, where both the summarization and the explicit evidence retrieval module are removed. For each of these architectures, we use several prompt engineering strategies including: zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B. We note that the SIf architecture does best while the naive D2I does worst in terms of accuracy and F-1 scores.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Gpt-4 technical report\",\n        \"Writing strategies for science communication: Data and computational analysis\",\n        \"# Scamdemic,# Plandemic, or# Scaredemic: what Parler social media platform tells us about COVID-19 vaccine\",\n        \"Language models are few-shot learners\",\n        \"Truth, lies, and automation: How language models could change disinformation\",\n        \"Overview of the first workshop on scholarly document processing (SDP)\",\n        \"Can LLM-Generated Misinformation Be Detected?\",\n        \"MMCoVaR: multimodal COVID-19 vaccine focused data repository for fake news detection and a baseline architecture for classification\",\n        \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\",\n        \"A COVID-19 rumor dataset\",\n        \"Climate-fever: A dataset for verification of real-world climate claims\",\n        \"A survey on automated fact-checking\",\n        \"Answering the call for a standard reliability measure for coding data\",\n        \"Rouge: A package for automatic evaluation of summaries\",\n        \"Autodan: Generating stealthy jailbreak prompts on aligned large language models\",\n        \"Jailbreaking chatgpt via prompt engineering: An empirical study\",\n        \"S2ORC: The semantic scholar open research corpus\",\n        \"Leveraging media and health communication strategies to overcome the COVID-19 infodemic\",\n        \"Jailbreaking chatgpt on release day\",\n        \"The danger of misinformation in the COVID-19 crisis\",\n        \"Measuring sentence-level and aspect-level (un) certainty in science communications\",\n        \"Semeval-2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup\",\n        \"Scientific claim verification with VerT5erini\",\n        \"Natural Language Annotation for Machine Learning: A guide to corpus-building for applications\",\n        \"Fine-tuning aligned language models compromises safety, even when users do not intend to!\",\n        \"Covid-19 vaccination hesitancy\",\n        \"Public trust, misinformation and COVID-19 vaccination willingness in Latin America and the Caribbean: today’s key challenges\",\n        \"COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic\",\n        \"COVID-19 fake news infodemic research dataset (COVID19-FNIR dataset)\",\n        \"Can science literacy help individuals identify misinformation in everyday life?\",\n        \"do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models\",\n        \"A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication\",\n        \"The science of detecting llm-generated texts\",\n        \"Bing conversational experiences and image creator terms\",\n        \"FEVER: a Large-scale Dataset for Fact Extraction and VERification\",\n        \"Examining the impact of sharing COVID-19 misinformation online on mental health\",\n        \"Fact checking: Task definition and dataset construction\",\n        \"Scientific Fact-Checking: A Survey of Resources and Approaches\",\n        \"Fact or fiction: Verifying scientific claims\",\n        \"Overview and insights from the SCIVER shared task on scientific claim verification\",\n        \"SciFact-open: Towards open-domain scientific claim verification\",\n        \"Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence\",\n        \"Cord-19: The covid-19 open research dataset\",\n        \"liar, liar pants on fire”: A new benchmark dataset for fake news detection\",\n        \"Semi-supervised exaggeration detection of health science press releases\",\n        \"Modeling information change in science communication with semantically matched paraphrases\",\n        \"Generating scientific claims for zero-shot scientific fact checking\",\n        \"Evidence-aware fake news detection with graph neural networks\",\n        \"Exploring the limits of chatgpt for query or aspect-based text summarization\",\n        \"Extractive summarization via chatgpt for faithful summary generation\",\n        \"Benchmarking Large Language Models for News Summarization\",\n        \"AnswerFact: Fact checking in product question answering\",\n        \"Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in Can Large Language Models Detect Misinformation in Scientific News Reporting?, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}