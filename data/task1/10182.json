{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 215 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Universal Time-Series Representation Learning: A Survey\n\nPaper Abstract: Abstract. Time-series data exists in every corner of real-world systems and services, ranging from satellites in the sky to wearable devices on human bodies. Learning representations by extracting and inferring valuable information from these time series is crucial for understanding the complex dynamics of particular phenomena and enabling informed decisions. With the learned representations, we can perform numerous downstream analyses more effectively. Among several approaches, deep learning has demonstrated remarkable performance in extracting hidden patterns and features from time-series data without manual feature engineering. This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for time series. According to the proposed taxonomy, we comprehensively review existing studies and discuss their intuitions and insights into how these methods enhance the quality of learned representations. Finally, as a guideline for future studies, we summarize commonly used experimental setups and datasets and discuss several promising research directions. An up-to-date corresponding resource is available at https://github.com/itouchz/awesome-deep-time-series-representations.\n\n{retrieved_text}\n\nGenerate exactly 215 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 215\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Large-scale indexing, discovery, and ranking for the internet of things (iot)\",\n        \"Anomaly detection for iot time-series data: A survey\",\n        \"A survey of physics-based attack detection in cyber-physical systems\",\n        \"Deep learning-based anomaly detection in cyber-physical systems: Progress and opportunities\",\n        \"Neural network–based financial volatility forecasting: A systematic review\",\n        \"Ai in finance: Challenges, techniques, and opportunities\",\n        \"Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities\",\n        \"A survey on deep learning for human activity recognition\",\n        \"Time-series data mining\",\n        \"Time-series forecasting with deep learning: A survey\",\n        \"Deep learning for time series classification: A review\",\n        \"Time series extrinsic regression: Predicting numeric values from time series data\",\n        \"Deep learning for anomaly detection in time-series data: Review, analysis, and guidelines\",\n        \"Time Series Data Representation and Dimensionality Reduction Techniques, pages 267–284\",\n        \"A transformer-based framework for multivariate time series representation learning\",\n        \"TS2Vec: Towards universal representation of time series\",\n        \"Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion\",\n        \"Recurrent neural networks for edge intelligence: A survey\",\n        \"A review of recurrent neural network-based methods in computational physiology\",\n        \"An empirical survey of data augmentation for time series classification with neural networks\",\n        \"Transformers in time series: A survey\",\n        \"A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection\",\n        \"Latent odes for irregularly-sampled time series\",\n        \"A review of deep learning methods for irregularly sampled medical time series data\",\n        \"Label-efficient time series representation learning: A review\",\n        \"Self-supervised learning for time series analysis: Taxonomy, progress, and prospects\",\n        \"A survey on time-series pre-trained models\",\n        \"Unsupervised representation learning for time series: A review\",\n        \"Self-supervised representation learning: Introduction, advances, and challenges\",\n        \"Recursive time series data augmentation\",\n        \"TimesNet: Temporal 2d-variation modeling for general time series analysis\",\n        \"Time series data augmentation for deep learning: A survey\",\n        \"A review of unsupervised feature learning and deep learning for time-series modeling\",\n        \"Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data\",\n        \"A deep multi-task representation learning method for time series classification and retrieval\",\n        \"A real-time action representation with temporal encoding and deep compression\",\n        \"Attentive neural controlled di01fferential equations for time-series classification and forecasting\",\n        \"Crosspyramid: Neural ordinary differential equations architecture for partially-observed time-series\",\n        \"EXIT: Extrapolation and interpolation-based neural controlled differential equations for time-series classification and forecasting\",\n        \"FEAT: A general framework for feature-aware multivariate time-series representation learning\",\n        \"Latent ordinary differential equations for irregularly-sampled time series\",\n        \"Learning disentangled representations of satellite image time series\",\n        \"MARINA: An mlp-attention model for multivariate time-series analysis\",\n        \"Memory-augmented dense predictive coding for video representation learning\",\n        \"Multi-task self-supervised time-series representation learning\",\n        \"TriBERT: Human-centric audio-visual representation learning\",\n        \"Unsupervised scalable representation learning for multivariate time series\",\n        \"Multilevel wavelet decomposition network for interpretable time series analysis\",\n        \"Ti-MAE: Self-supervised masked time series autoencoders\",\n        \"UniTS: A universal time series analysis framework with self-supervised representation learning\",\n        \"T-Rep: Representation learning for time series using time-embeddings\",\n        \"Delta: deep local pattern representation for time-series clustering and classification using visual perception\",\n        \"Learning robust and consistent time series representations: A dilated inception-based approach\",\n        \"Audio word2vec: Sequence-to-sequence autoencoding for unsupervised learning of audio segmentation and representation\",\n        \"One Fits All: Power general time series analysis by pretrained lm\",\n        \"One Transformer for All Time Series: Representing and training with time-dependent heterogeneous tabular data\",\n        \"SSAN: Separable self-attention network for video representation learning\",\n        \"Contrastive shapelet learning for unsupervised multivariate time series representation learning\",\n        \"Effectively modeling time series with simple discrete state spaces\",\n        \"Learning representations of multivariate time series with missing data\",\n        \"Modeling irregular time series with continuous recurrent units\",\n        \"Modeling temporal data as continuous functions with stochastic process diffusion\",\n        \"Neural continuous-discrete state space models for irregularly-sampled time series\",\n        \"Random warping series: A random features method for time-series embedding\",\n        \"TE-ESN: time encoding echo state network for prediction based on irregularly sampled time series data\",\n        \"Towards explainable representation of time-evolving graphs via spatial-temporal graph attention networks\",\n        \"Hypertime: Implicit neural representations for time series\",\n        \"Multivariate time series representation learning via hierarchical correlation pooling boosted graph neural network\",\n        \"Towards learning disentangled representations for time series\",\n        \"Time series domain adaptation via sparse associative structure alignment\",\n        \"Weakly paired associative learning for sound and image representations via bimodal associative memory\",\n        \"Contrast everything: A hierarchical contrastive framework for medical time-series\",\n        \"ContiFormer: Continuous-time transformer for irregular time series modeling\",\n        \"TriD-MAE: A generic pre-trained model for multivariate time series with missing values\",\n        \"A multi-scale decomposition mlp-mixer for time series analysis\",\n        \"FITS: Modeling time series with 10​k10𝑘10k parameters\",\n        \"End-to-end incomplete time-series modeling from linear memory of latent variables\",\n        \"WHEN: A wavelet-dtw hybrid attention network for heterogeneous time series analysis\",\n        \"Sparse binary transformers for multivariate time series modeling\",\n        \"Multi-time attention networks for irregularly sampled time series\",\n        \"Temporal aggregate representations for long-range video understanding\",\n        \"TARNet: Task-aware reconstruction for time-series transformer\",\n        \"TCGL: Temporal contrastive graph for self-supervised video representation learning\",\n        \"Time series continuous modeling for imputation and forecasting with implicit neural representations\",\n        \"NuTime: Numerically multi-scaled embedding for large-scale time series pretraining\",\n        \"Representation learning via global temporal alignment and cycle-consistency\",\n        \"Wave2vec: Deep representation learning for clinical temporal data\",\n        \"Decoupling local and global representations of time series\",\n        \"Sqn2Vec: learning sequence representation via sequential patterns with a gap constraint\",\n        \"SimMTM: A simple pre-training framework for masked time-series modeling\",\n        \"Cross-architecture self-supervised video representation learning\",\n        \"Learning by aligning videos in time\",\n        \"Self-supervised spatiotemporal representation learning by exploiting video continuity\",\n        \"Self-supervised video representation learning by pace prediction\",\n        \"Self-supervised video representation learning by uncovering spatio-temporal statistics\",\n        \"TransRank: Self-supervised video representation learning via ranking-based transformation recognition\",\n        \"Learning decomposed spatial relations for multi-variate time-series modeling\",\n        \"Contrastive spatio-temporal pretext learning for self-supervised video representation\",\n        \"Cycle-contrast for self-supervised video representation learning\",\n        \"Dual contrastive learning for spatio-temporal representation\",\n        \"Frame-wise action representations for long videos via sequence contrastive learning\",\n        \"Hierarchically decoupled spatial-temporal contrast for self-supervised video representation learning\",\n        \"Learning from untrimmed videos: Self-supervised video representation learning with hierarchical consistency\",\n        \"Learning representations from audio-visual spatial alignment\",\n        \"Modeling video as stochastic processes for fine-grained video representation learning\",\n        \"On temporal granularity in self-supervised video representation learning\",\n        \"PrimeNet: Pre-training for irregular multivariate time series\",\n        \"RSPNet: Relative speed perception for unsupervised video representation learning\",\n        \"Self-supervised contrastive pre-training for time series via time-frequency consistency\",\n        \"Self-supervised time series representation learning with temporal-instance similarity distillation\",\n        \"Spatiotemporal contrastive video representation learning\",\n        \"TCLR: Temporal contrastive learning for video representation\",\n        \"TempCLR: Temporal alignment representation with contrastive learning\",\n        \"Time-equivariant contrastive video representation learning\",\n        \"TimeAutoML: Autonomous representation learning for multivariate irregularly sampled time series\",\n        \"TimeCLR: A self-supervised contrastive learning framework for univariate time series representation\",\n        \"TS-Rep: Self-supervised time series representation learning from robot sensor data\",\n        \"Unsupervised representation learning for time series with temporal neighborhood coding\",\n        \"Context consistency regularization for label sparsity in time series\",\n        \"FOCAL: Contrastive learning for multimodal time-series sensing signals in factorized orthogonal latent space\",\n        \"A co-training approach for noisy time series learning\",\n        \"TEST: Text prototype aligned embedding to activate llm’s ability for time series\",\n        \"Time-series representation learning via temporal and contextual contrasting\",\n        \"Exploring temporally dynamic data augmentation for video recognition\",\n        \"Time series contrastive learning with information-aware augmentations\",\n        \"Frequency selective augmentation for video representation learning\",\n        \"Finding order in chaos: A novel data augmentation method for time series in contrastive learning\",\n        \"Contrastive learning of global and local video representations\",\n        \"Long short view feature decomposition via contrastive video representation learning\",\n        \"Cross-modal mutual learning for audio-visual speech recognition and manipulation\",\n        \"Irregularly-sampled time series modeling with spline networks\",\n        \"Multi-view integrative attention-based deep representation learning for irregular clinical time-series data\",\n        \"Direct embedding of temporal network edges via time-decayed line graphs\",\n        \"Learning video representations from large language models\",\n        \"Long short-term memory\",\n        \"Learning phrase representations using rnn encoder-decoder for statistical machine translation\",\n        \"An empirical evaluation of generic convolutional and recurrent networks for sequence modeling\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Deep learning for time series classification and extrinsic regression: A current survey\",\n        \"Attention is all you need\",\n        \"A survey on principles, models and methods for learning from irregularly sampled time series\",\n        \"Experimental analysis of the effect of autoencoder architectures on time series representation learning\",\n        \"Yoav Goldberg and Omer Levy\",\n        \"Evaluation methods for representation learning: A survey\",\n        \"Informer: Beyond efficient transformer for long sequence time-series forecasting\",\n        \"Modeling long-and short-term temporal patterns with deep neural networks\",\n        \"Monash time series forecasting archive\",\n        \"The ucr time series archive\",\n        \"The uea multivariate time series classification archive, 2018\",\n        \"A public domain dataset for human activity recognition using smartphones\",\n        \"Early prediction of sepsis from clinical data: the physionet/computing in cardiology challenge 2019\",\n        \"Time series extrinsic regression\",\n        \"Matrix profile viii: domain agnostic online semantic segmentation at superhuman performance levels\",\n        \"Clasp: parameter-free time series segmentation\",\n        \"Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress\",\n        \"Timesead: Benchmarking deep multivariate time-series anomaly detection\",\n        \"Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding\",\n        \"TSB-UAD: an end-to-end benchmark suite for univariate time-series anomaly detection\",\n        \"Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications\",\n        \"DCdetector: Dual attention contrastive representation learning for time series anomaly detection\",\n        \"Context-aware deep time-series decomposition for anomaly detection in businesses\",\n        \"Choose wisely: An extensive evaluation of model selection for anomaly detection in time series\",\n        \"Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection\",\n        \"Towards a rigorous evaluation of time-series anomaly detection\",\n        \"Do you know existing accuracy metrics overrate time-series anomaly detections?\",\n        \"Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100\",\n        \"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\n        \"The sound of pixels\",\n        \"Diffusion convolutional recurrent neural network: Data-driven traffic forecasting\",\n        \"Time-series generative adversarial networks\",\n        \"Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012\",\n        \"Af classification from a short single lead ecg recording: The physionet/computing in cardiology challenge 2017\",\n        \"Physionet: components of a new research resource for complex physiologic signals\",\n        \"Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state\",\n        \"uwave: Accelerometer-based personalized gesture recognition and its applications\",\n        \"Lip reading in the wild\",\n        \"Esc: Dataset for environmental sound classification\",\n        \"Ucf101: A dataset of 101 human actions classes from videos in the wild\",\n        \"Hmdb: a large video database for human motion recognition\",\n        \"The kinetics human action video dataset\",\n        \"Mmact: A large-scale dataset for cross modal human action understanding\",\n        \"A dataset of neonatal eeg recordings with seizure annotations\",\n        \"Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification\",\n        \"Revisiting time series outlier detection: Definitions and benchmarks\",\n        \"Robust anomaly detection for multivariate time series through stochastic recurrent neural network\",\n        \"Practical approach to asynchronous multivariate time series anomaly detection and localization\",\n        \"Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding\",\n        \"SWaT: A water treatment testbed for research and training on ics security\",\n        \"WADI: a water distribution testbed for research in the design of secure cyber physical systems\",\n        \"Real-world anomaly detection in surveillance videos\",\n        \"Oops! predicting unintentional action in video\",\n        \"The deepfake detection challenge (DFDC) dataset\",\n        \"Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals\",\n        \"Coherence-based label propagation over time series for accelerated active learning\",\n        \"Are all frames equal? active sparse labeling for video action detection\",\n        \"Concept drift detection in data stream mining: A literature review\",\n        \"Recent advances in concept drift adaptation methods for deep learning\",\n        \"Adatime: A benchmarking suite for domain adaptation on time series data\",\n        \"Domain adaptation for time series forecasting via attention sharing\",\n        \"AutoGT: Automated graph transformer architecture search\",\n        \"Neural architecture search: Insights from 1000 papers\",\n        \"AutoAI-TS: Autoai for time series forecasting\",\n        \"Differentiable neural architecture search for high-dimensional time series forecasting\",\n        \"AutoTS: Automatic time series forecasting model design based on two-stage pruning\",\n        \"Efficient automated deep learning for time series forecasting\",\n        \"LightCTS: A lightweight framework for correlated time series forecasting\",\n        \"Neural architecture search for time series classification\",\n        \"RNTS: Robust neural temporal search for time series classification\",\n        \"AutoTransformer: Automatic transformer architecture design for time series classification\",\n        \"Large models for time series and spatio-temporal data: A survey and outlook\",\n        \"Recurrent neural networks for multivariate time series with missing values\",\n        \"Neural ordinary differential equations\",\n        \"Interpolation-prediction networks for irregularly sampled time series\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Scaling up visual and vision-language representation learning with noisy text supervision\"\n    ],\n    \"reasoning\": \"Selected these 215 references because they represent foundational work and recent advances in Universal Time-Series Representation Learning: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}