{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 52 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Blackbox Approach to Best of Both Worlds in Bandits and Beyond\n\nPaper Abstract: Abstract Best-of-both-worlds algorithms for online learning which achieve near-optimal regret in both the adversarial and the stochastic regimes have received growing attention recently. Existing techniques often require careful adaptation to every new problem setup, including specialised potentials and careful tuning of algorithm parameters. Yet, in domains such as linear bandits, it is still unknown if there exists an algorithm that can simultaneously obtain O‚Äã(log‚Å°(T))ùëÇùëáO(\\log(T)) regret in the stochastic regime and O~‚Äã(T)~ùëÇùëá\\tilde{O}(\\sqrt{T}) regret in the adversarial regime. In this work, we resolve this question positively and present a general reduction from best of both worlds to a wide family of follow-the-regularized-leader (FTRL) and online-mirror-descent (OMD) algorithms. We showcase the capability of this reduction by transforming existing algorithms that are only known to achieve worst-case guarantees into new algorithms with best-of-both-worlds guarantees in contextual bandits, graph bandits and tabular Markov decision processes.\n\n{retrieved_text}\n\nGenerate exactly 52 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 52\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Regret balancing for bandit and rl model selection\",\n        \"Competing in the dark: An efficient algorithm for bandit linear optimization\",\n        \"Corralling a band of bandit algorithms\",\n        \"From bandits to experts: A tale of domination and independence\",\n        \"Online learning with feedback graphs: Beyond bandits\",\n        \"Better best of both worlds bounds for bandits with switching costs\",\n        \"Corralling stochastic bandit algorithms\",\n        \"An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits\",\n        \"Finite-time analysis of the multiarmed bandit problem\",\n        \"The nonstochastic multiarmed bandit problem\",\n        \"The best of both worlds: Stochastic and adversarial bandits\",\n        \"Towards minimax policies for online linear optimization with bandit feedback\",\n        \"Improved path-length regret bounds for bandits\",\n        \"Simultaneously learning stochastic and adversarial bandits under the position-based model\",\n        \"Dynamic balancing for model selection in bandits and rl\",\n        \"Towards best-of-all-worlds online learning with feedback graphs\",\n        \"Adapting to misspecification in contextual bandits\",\n        \"A second-order bound with excess losses\",\n        \"Better algorithms for stochastic bandits with adversarial corruptions\",\n        \"Follow-the-perturbed-leader achieves best-of-both-worlds for bandit problems\",\n        \"Adaptive best-of-both-worlds algorithm for heavy-tailed multi-armed bandits\",\n        \"On optimal robustness to adversarial corruption in online decision problems\",\n        \"Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds\",\n        \"Tight first-and second-order regret bounds for adversarial linear bandits\",\n        \"Adversarially robust multi-armed bandit algorithm with variance-dependent regret bounds\",\n        \"Nearly optimal best-of-both-worlds algorithms for online learning with feedback graphs\",\n        \"Simultaneously learning stochastic and adversarial episodic mdps with known transition\",\n        \"The best of both worlds: stochastic and adversarial episodic mdps with unknown transition\",\n        \"Simultaneously learning stochastic and adversarial bandits with general graph feedback\",\n        \"Asymptotically efficient adaptive allocation rules\",\n        \"Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps\",\n        \"Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously\",\n        \"Homework 3 solution, introduction to online optimization/learning\",\n        \"Corralling a larger band of bandits: A case study on switching regret for linear bandits\",\n        \"Stochastic bandits robust to adversarial corruptions\",\n        \"Improved analysis of the tsallis-inf algorithm in stochastically constrained adversarial bandits and stochastic bandits with adversarial corruptions\",\n        \"A best-of-both-worlds algorithm for bandits with delayed feedback\",\n        \"On the optimality of the hedge algorithm in the stochastic regime\",\n        \"Best of both worlds model selection\",\n        \"Optimization, learning, and games with predictable sequences\",\n        \"An algorithm for stochastic and adversarial bandits with switching costs\",\n        \"A near-optimal best-of-both-worlds algorithm for online learning with feedback graphs\",\n        \"Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences\",\n        \"An improved parametrization and analysis of the exp3++ algorithm for stochastic and adversarial bandits\",\n        \"One practical algorithm for both stochastic and adversarial bandits\",\n        \"Best-of-both-worlds algorithms for partial monitoring\",\n        \"More adaptive algorithms for adversarial bandits\",\n        \"A model selection approach for corruption robust reinforcement learning\",\n        \"Connections between mirror descent, thompson sampling and the information ratio\",\n        \"Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits\",\n        \"An optimal algorithm for stochastic and adversarial bandits\",\n        \"Beating stochastic and adversarial semi-bandits optimally and simultaneously\"\n    ],\n    \"reasoning\": \"Selected these 52 references because they represent foundational work and recent advances in A Blackbox Approach to Best of Both Worlds in Bandits and Beyond, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}