{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 101 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Document-editing Assistants and Model-based Reinforcement Learning as a Path to Conversational AI\n\nPaper Abstract: Abstract Intelligent assistants that follow commands or answer simple questions, such as Siri and Google search, are among the most economically important applications of AI. Future conversational AI assistants promise even greater capabilities and a better user experience through a deeper understanding of the domain, the user, or the user’s purposes. But what domain and what methods are best suited to researching and realizing this promise? In this article we argue for the domain of voice document editing and for the methods of model-based reinforcement learning. The primary advantages of voice document editing are that the domain is tightly scoped and that it provides something for the conversation to be about (the document) that is delimited and fully accessible to the intelligent assistant. The advantages of reinforcement learning in general are that its methods are designed to learn from interaction without explicit instruction and that it formalizes the purposes of the assistant. Model-based reinforcement learning is needed in order to genuinely understand the domain of discourse and thereby work efficiently with the user to achieve their goals. Together, voice document editing and model-based reinforcement learning comprise a promising research direction for achieving conversational AI. Keywords: conversational AI, intelligent assistants, dialogue systems, human-computer interaction, reinforcement learning, model-based reinforcement learning\n\n{retrieved_text}\n\nGenerate exactly 101 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 101\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"An application of reinforcement learning to aerobatic helicopter flight\",\n        \"Voice annotation and editing in a workstation environment\",\n        \"A plan-based approach to speech act recognition\",\n        \"Intelligent tutoring systems\",\n        \"A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems\",\n        \"A comparison of direct and model-based reinforcement learning\",\n        \"Autonomous helicopter control using reinforcement learning policy search methods\",\n        \"Towards zero-shot frame semantic parsing for domain scaling\",\n        \"Interactional Feedback and the Impact of Attitude and Motivation on Noticing L2 Form\",\n        \"Safe model-based reinforcement learning with stability guarantees\",\n        \"The composition of messages in speech-graphics interactive systems\",\n        \"Speech to text conversion\",\n        \"Learning end-to-end goal-oriented dialog\",\n        \"An intelligent assistant for navigation of visually impaired people\",\n        \"Belief systems and language understanding\",\n        \"Sub-domain modelling for dialogue management with hierarchical reinforcement learning\",\n        \"Learning and querying fast generative models for reinforcement learning\",\n        \"Plan recognition and its use in understanding dialog\",\n        \"Mixed-initiative man-computer instructional dialogues\",\n        \"Computer text-editing: An information-processing analysis of a routine cognitive skill\",\n        \"More than just another pretty face: Embodied conversational interface agents\",\n        \"Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons\",\n        \"Policy adaptation for deep reinforcement learning-based dialogue management\",\n        \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models\",\n        \"Autonomous Helicopter Flight Using Reinforcement Learning\",\n        \"On Knowing What to Say: Planning Speech Acts\",\n        \"Search engines: Information retrieval in practice, volume 520\",\n        \"Human-computer dialogue simulation using hidden Markov models\",\n        \"Deep reinforcement learning for multi-domain dialogue systems\",\n        \"Scaling up deep reinforcement learning for multi-domain dialogue systems\",\n        \"Robot assistants: Applications and evolution\",\n        \"Looking Back at Google’s Research Efforts in 2018\",\n        \"PILCO: A model-based and data-efficient approach to policy search\",\n        \"Gaussian processes for data-efficient learning in robotics and control\",\n        \"Towards end-to-end reinforcement learning of dialogue agents for information access\",\n        \"The ubiquity of model-based reinforcement learning\",\n        \"Method and apparatus for editing documents through voice recognition\",\n        \"The Best Dictation Software for 2019\",\n        \"Augmenting human intellect: A conceptual framework\",\n        \"NeurIPS 2019 ConvAI Workshop Recap\",\n        \"Sponsored search: A brief history\",\n        \"Learning robust dialog policies in noisy environments\",\n        \"Doc2Dial: A Framework for Dialogue Composition Grounded in Business Documents\",\n        \"Reinforcement-learning based dialogue system for human–robot interactions with socially-inspired rewards\",\n        \"Natural language interactions with artificial experts\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"Learning to communicate with deep multi-agent reinforcement learning\",\n        \"Counterintuitive behavior of social systems\",\n        \"Isis—a knowledge-based system for factory scheduling\",\n        \"Learning to Predict Without Looking Ahead: World Models Without Forward Prediction\",\n        \"Neural Approaches to Conversational AI\",\n        \"N.; Rojas-Barahona\",\n        \"J. D.; Kaptanoglu\",\n        \"C. L.; and Thomaz\",\n        \"H.; and Davidson\",\n        \"L.; and Ostendorf\",\n        \"E. J.; and Bowling\",\n        \"N.; Ghandeharioun\",\n        \"J.; Hassan Awadallah\",\n        \"R. H.; Czechowski\",\n        \"P. H.; and Popovich\",\n        \"J.; Murray-Smith\",\n        \"K.; Chockalingam\",\n        \"N. T.; and Baroni\",\n        \"J.; and Jurafsky\",\n        \"J.; and Celikyilmaz\",\n        \"R. T.; MacGlashan\",\n        \"S.; and Bennington\",\n        \"W.; and Georgila\",\n        \"J. A.; Geramifard\",\n        \"D.; and Riedmiller\",\n        \"M. I.; and Sastry\",\n        \"J.; and Ipeirotis\",\n        \"G.; Painter-Wakefield\",\n        \"C. J.; and Young\",\n        \"J. P.; and Sutton\",\n        \"R. S.; Mathewson\",\n        \"R. S.; and Mathewson\",\n        \"B.; Ramakrishnan\",\n        \"M. E.; Hirschberg\",\n        \"D.; and Sutskever\",\n        \"D. E.; Smolensky\",\n        \"D.; Schrittwieser\",\n        \"T.; Dulac-Arnold\",\n        \"T.; Schrittwieser\",\n        \"D. J.; and Walker\",\n        \"E. M.; Williamson\",\n        \"P.-H.; Budzianowski\",\n        \"Y.-N.; and Rudnicky\",\n        \"R. S.; Szepesvári\",\n        \"G.; and Breazeal\",\n        \"D.; and Schapire\",\n        \"P. M.; and Sutton\",\n        \"T. B.; and Deisenroth\",\n        \"M.; and De Freitas\",\n        \"M.; Springenberg\",\n        \"G.; Budzianowski\",\n        \"M.; Rojas Barahona\",\n        \"R.; and Schmidhuber\",\n        \"B.; and Williams\",\n        \"Dynamic Reward-based Dueling Deep Dyna-Q: Robust Policy Learning in Noisy Environments\"\n    ],\n    \"reasoning\": \"Selected these 101 references because they represent foundational work and recent advances in Document-editing Assistants and Model-based Reinforcement Learning as a Path to Conversational AI, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}