{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 195 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution View [Advanced Review]\n\nPaper Abstract: Abstract The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black-box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end users to evaluate the model decision making to know whether to accept or reject predictions and recommendations before an action is taken. In this review, we focus on the interpretability of the DL models in healthcare. We start by introducing the methods for interpretability in depth and comprehensively as a methodological reference for future researchers or clinical practitioners in this field. Besides the methods’ details, we also include a discussion of advantages and disadvantages of these methods and which scenarios each of them is suitable for, so that interested readers can know how to compare and choose among them for use. Moreover, we discuss how these methods, originally developed for solving general-domain problems, have been adapted and applied to healthcare problems and how they can help physicians better understand these data-driven technologies. Overall, we hope this survey can help researchers and practitioners in both artificial intelligence (AI) and clinical fields understand what methods we have for enhancing the interpretability of their DL models and choose the optimal one accordingly.\n\n{retrieved_text}\n\nGenerate exactly 195 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 195\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Normlime: A new feature importance metric for explaining deep neural networks\",\n        \"Interpretable machine learning in healthcare\",\n        \"Threat of adversarial attacks on deep learning in computer vision: A survey\",\n        \"Attentive state-space modeling of disease progression\",\n        \"Publicly available clinical bert embeddings\",\n        \"Longitudinal adversarial attack on electronic health records data\",\n        \"Towards better understanding of gradient-based attribution methods for deep neural networks\",\n        \"Explaining deep neural networks with a polynomial time algorithm for shapley value approximation\",\n        \"Deep learning algorithm predicts diabetic retinopathy progression in individual patients\",\n        \"Deep learning and machine learning in hydrological processes climate change and earth systems a systematic review\",\n        \"Evaluating recurrent neural network explanations\",\n        \"Readmission prediction using deep learning on electronic health records\",\n        \"Synthesizing robust adversarial examples\",\n        \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Interpretable representation learning for healthcare via capturing disease progression through time\",\n        \"Scibert: A pretrained language model for scientific text\",\n        \"Evaluating and aggregating feature-based model explanations\",\n        \"Explainable machine learning in deployment\",\n        \"neurophysiological correlates of concussion: Deep learning for clinical assessment\",\n        \"On identifiability in transformers\",\n        \"Towards evaluating the robustness of neural networks\",\n        \"Activation atlas\",\n        \"Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission\",\n        \"Adversarial attacks and defences: A survey\",\n        \"Explaining image classifiers by counterfactual generation\",\n        \"Ecgadv: Generating adversarial electrocardiogram to misguide arrhythmia classification system\",\n        \"Learning to explain: An information-theoretic perspective on model interpretation\",\n        \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models\",\n        \"Retain: An interpretable predictive model for healthcare using reverse time attention mechanism\",\n        \"Parsevasl networks: Improving robustness to adversarial examples\",\n        \"What does bert look at? an analysis of bert’s attention\",\n        \"Towards interpretability of segmentation networks by analyzing deepdreams\",\n        \"Extracting tree-structured representations of trained networks\",\n        \"A randomized gradient-free attack on relu networks\",\n        \"Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks\",\n        \"Real time image saliency for black box classifiers\",\n        \"Towards a rigorous science of interpretable machine learning\",\n        \"Enslaving the algorithm: From a “right to an explanation” to a “right to better decisions”?\",\n        \"Testing the robustness of attribution methods for convolutional neural networks in mri-based alzheimer’s disease classification\",\n        \"On the interpretability of machine learning-based model for predicting hypertension\",\n        \"Interpretability in healthcare: A comparative study of local machine learning interpretability techniques\",\n        \"Dermatologist-level classification of skin cancer with deep neural networks\",\n        \"A guide to deep learning in healthcare\",\n        \"Deep learning for time series classification: a review\",\n        \"Detecting adversarial samples from artifacts\",\n        \"What can AI do for me? evaluating machine learning interpretations in cooperative play\",\n        \"Adversarial attacks on medical machine learning\",\n        \"Adversarial attacks against medical deep learning systems\",\n        \"Explainable deep learning for augmentation of small RNA expression profiles\",\n        \"All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously\",\n        \"Camp: Co-attention memory networks for diagnosis prediction in healthcare\",\n        \"Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives\",\n        \"Potential biases in machine learning algorithms using electronic health record data\",\n        \"Explaining and harnessing adversarial examples\",\n        \"On the (statistical) detection of adversarial examples\",\n        \"Towards deep neural network architectures robust to adversarial examples\",\n        \"Algorithmic bias: From discrimination discovery to fairness-aware data mining\",\n        \"Machine learning interpretability with H2O driverless ai\",\n        \"Deep learning models for electrocardiograms are susceptible to adversarial attack\",\n        \"Explaining black box predictions and unveiling data artifacts through influence functions\",\n        \"Machine learning explainability in breast cancer survival\",\n        \"Uncertainty-aware attention for reliable interpretation and prediction\",\n        \"Distilling the knowledge in a neural network\",\n        \"Universal adversarial attacks on deep neural networks for medical image classification\",\n        \"Causability and explainability of artificial intelligence in medicine\",\n        \"Evaluating feature importance estimates\",\n        \"Locally interpretable models and effects based on supervised partitioning (LIME-SUP)\",\n        \"Graphlime: Local interpretable model explanations for graph neural networks\",\n        \"Adversarial attacks to machine learning-based smart healthcare systems\",\n        \"Benchmarking deep learning interpretability in time series predictions\",\n        \"Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?\",\n        \"Attention is not explanation\",\n        \"Feature relevance quantification in explainable ai: A causal problem\",\n        \"Is BERT really robust? a strong baseline for natural language attack on text classification and entailment\",\n        \"Thinking, fast and slow\",\n        \"An attention based deep learning model of clinical events in the intensive care unit\",\n        \"Interpreting interpretability: Understanding data scientists’ use of interpretability tools for machine learning\",\n        \"Deep learning and explainable ai in healthcare using ehr\",\n        \"Examples are not enough, learn to criticize! criticism for interpretability\",\n        \"Visually interpretable deep network for diagnosis of breast masses on mammograms\",\n        \"Apache ii: a severity of disease classification system\",\n        \"Understanding black-box predictions via influence functions\",\n        \"Survey of explainable machine learning with visual and granular methods beyond quasi-explanations\",\n        \"An explanation method for black-box machine learning survival models using the chebyshev distance\",\n        \"What is interpretable? using machine learning to design interpretable decision-support systems\",\n        \"Interpretable & explorable approximations of black box models\",\n        \"Faithful and customizable explanations of black box models\",\n        \"Deep learning\",\n        \"Biobert: a pre-trained biomedical language representation model for biomedical text mining\",\n        \"Pathflowai: A high-throughput workflow for preprocessing, deep learning and interpretation in digital pathology\",\n        \"Understanding neural networks through representation erasure\",\n        \"Machine learning–based interpretation and visualization of nonlinear interactions in prostate cancer survival\",\n        \"Leveraging auxiliary measures: a deep multi-task neural network for predictive modeling in clinical research\",\n        \"Behrt: transformer for electronic health records\",\n        \"A tailored deep convolutional neural network design for detection of covid-19 cases from chest radiography images\",\n        \"The mythos of model interpretability\",\n        \"Towards explainable nlp: A generative explanation framework for text classification\",\n        \"Safetynet: Detecting and rejecting adversarial examples robustly\",\n        \"Consistent individualized feature attribution for tree ensembles\",\n        \"A unified approach to interpreting model predictions\",\n        \"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\",\n        \"Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks\",\n        \"Understanding adversarial attacks on deep learning based medical image analysis systems\",\n        \"Towards deep learning models resistant to adversarial attacks\",\n        \"Explainable reinforcement learning through a causal lens\",\n        \"Combining patient visual timelines with deep learning to predict mortality\",\n        \"The role of artificial intelligence in precision medicine\",\n        \"Machine learning in the next five years\",\n        \"Asynchronous methods for deep reinforcement learning\",\n        \"Playing atari with deep reinforcement learning\",\n        \"An investigation of interpretability techniques for deep learning in predictive process analytics\",\n        \"Systematic poisoning attacks on and defenses for machine learning in healthcare\",\n        \"Ultra-strong machine learning: comprehensibility of programs learned with ilp\",\n        \"Explainable prediction of medical codes from clinical text\",\n        \"The inevitable application of big data to health care\",\n        \"Beyond word importance: Contextual decomposition to extract interactions from lstms\",\n        \"Cross-domain transferability of adversarial perturbations\",\n        \"Explainable ai for medical imaging: deep-learning cnn ensemble for classification of estrogen receptor status from breast mri\",\n        \"The limitations of deep learning in adversarial settings\",\n        \"Distillation as a defense to adversarial perturbations against deep neural networks\",\n        \"Explainable artificial intelligence models using real-world electronic health record data: a systematic scoping review\",\n        \"Meaningful explanations of black box ai decision systems\",\n        \"Automatic brain tumor grading from mri data using convolutional neural networks and quality assessment\",\n        \"Neural network interpretation of the parkinson’s disease diagnosis from spect imaging\",\n        \"Model agnostic supervised local explanations\",\n        \"Attend and diagnose: Clinical time series analysis using attention models\",\n        \"Explain yourself! leveraging language models for commonsense reasoning\",\n        \"Model agnostic multilevel explanations\",\n        \"Deep learning in glaucoma with optical coherence tomography: a review\",\n        \"” why should i trust you?” explaining the predictions of any classifier\",\n        \"Model-agnostic interpretability of machine learning\",\n        \"Anchors: High-precision model-agnostic explanations\",\n        \"Interpretations are useful: penalizing explanations to align neural networks with prior knowledge\",\n        \"Explaining classifications for individual instances\",\n        \"Predicting inpatient medication orders from electronic health record data\",\n        \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n        \"Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy\",\n        \"Ann-dt: an algorithm for extraction of decision trees from artificial neural networks\",\n        \"Proximal policy optimization algorithms\",\n        \"Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"Kdruleex: A novel approach for enhancing user comprehensibility using rule extraction\",\n        \"Financial time series forecasting with deep learning: A systematic literature review: 2005–2019\",\n        \"Understanding adversarial training: Increasing local stability of supervised models through robust optimization\",\n        \"Alime: Autoencoder based approach for local interpretability\",\n        \"A value for n-person games\",\n        \"ILIME: local and global interpretable model-agnostic explainer of black-box decision\",\n        \"An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification\",\n        \"A modified perturbed sampling method for local interpretable model-agnostic explanation\",\n        \"Deep interpretable mortality model for intensive care unit risk prediction\",\n        \"Learning important features through propagating activation differences\",\n        \"Not just a black box: Learning important features through propagating activation differences\",\n        \"Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n        \"Interpretation of deep learning using attributions: application to ophthalmic diagnosis\",\n        \"Explainable deep learning models in medical image analysis\",\n        \"Striving for simplicity: The all convolutional net\",\n        \"Identify susceptible locations in medical records via adversarial attacks on deep predictive models\",\n        \"The shapley taylor interaction index\",\n        \"The many shapley values for model explanation\",\n        \"Axiomatic attribution for deep networks\",\n        \"Clinical intervention prediction and understanding with deep neural networks\",\n        \"Intriguing properties of neural networks\",\n        \"A taxonomy and terminology of adversarial machine learning\",\n        \"I read but don’t agree: Privacy policy benchmarking using machine learning and the eu gdpr\",\n        \"What clinicians want: Contextualizing explainable machine learning for clinical end use\",\n        \"Detecting statistical interactions from neural network weights\",\n        \"How does this interaction affect me? interpretable attribution for feature interactions\",\n        \"Deep reinforcement learning with double q-learning\",\n        \"Attention is all you need\",\n        \"The importance of interpretability and visualization in machine learning for applications in medicine and health care\",\n        \"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned\",\n        \"Explaining instance classifications with interactions of subsets of feature values\",\n        \"Defending against adversarial attack towards deep neural networks via collaborative multi-task training\",\n        \"A review of deep learning for renewable energy forecasting\",\n        \"Explainable diagnosis of secondary pulmonary tuberculosis by graph rank-based average pooling neural network\",\n        \"Do no harm: a roadmap for responsible machine learning for health care\",\n        \"Interpretable recurrent neural networks using sequential sparse recovery\",\n        \"Transformers: State-of-the-art natural language processing\",\n        \"Interpretable classification from skin cancer histology slides using deep learning: A retrospective multicenter study\",\n        \"An interpretable knowledge transfer model for knowledge base completion\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Explaining therapy predictions with layer-wise relevance propagation in neural networks\",\n        \"On the (in) fidelity and sensitivity of explanations\",\n        \"Gnn explainer: A tool for post-hoc explanation of graph neural networks\",\n        \"Deep neural network or dermatologist?\",\n        \"Adversarial examples: Attacks and defenses for deep learning\",\n        \"Dlime: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems\",\n        \"Visualizing and understanding convolutional networks\",\n        \"Visualizing and understanding convolutional neural networks\",\n        \"Anc: attention network for covid-19 explainable diagnosis based on convolutional block attention module\",\n        \"Fine-grained sentiment analysis with faithful attention\",\n        \"Learning deep features for discriminative localization\",\n        \"Opening the black box of artificial intelligence for clinical decision support: A study predicting stroke outcome\",\n        \"Deepred–rule extraction from deep neural networks\",\n        \"Visualizing deep neural network decisions: Prediction difference analysis\"\n    ],\n    \"reasoning\": \"Selected these 195 references because they represent foundational work and recent advances in Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution View [Advanced Review], providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}