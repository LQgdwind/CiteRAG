{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 52 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Generating and Evaluating Explanations of Attended and Error-Inducing Input Regions for VQA Models\n\nPaper Abstract: Abstract [Summary]Attention maps, a popular heatmap-based explanation method for Visual Question Answering (VQA), are supposed to help users understand the model by highlighting portions of the image/question used by the model to infer answers. However, we see that users are often misled by current attention map visualizations that point to relevant regions despite the model producing an incorrect answer. Hence, we propose Error Maps that clarify the error by highlighting image regions where the model is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly leading to an incorrect answer, and hence, improve usersâ€™ understanding of those cases. To evaluate our new explanations, we further introduce a metric that simulates usersâ€™ interpretation of explanations to evaluate their potential helpfulness to understand model correctness. We finally conduct user studies to see that our new explanations help users understand model correctness better than baselines by an expected 30% and that our proxy helpfulness metrics correlate strongly (Ï>0.97ðœŒ0.97\\rho>0.97) with how well users can predict model correctness.\n\n{retrieved_text}\n\nGenerate exactly 52 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 52\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Reading the mind in the eyes or reading between the lines? Theory of mind predicts collective intelligence equally well online and face-to-face\",\n        \"Explanation and justification in machine learning: A survey\",\n        \"Explanation in artificial intelligence: Insights from the social sciences\",\n        \"Beyond accuracy: The role of mental models in human-AI team performance\",\n        \"DARPAâ€™s explainable artificial intelligence program\",\n        \"Bottom-up and top-down attention for image captioning and visual question answering\",\n        \"Multimodal explanations: Justifying decisions and pointing to the evidence\",\n        \"Stacked attention networks for image question answering\",\n        \"Hierarchical question-image co-attention for visual question answering\",\n        \"It Takes Two to Tango: Towards Theory of AIâ€™s Mind\",\n        \"A Study on Multimodal and Interactive Explanations for Visual Question Answering\",\n        \"Can you explain that? Lucid explanations help human-AI collaborative image retrieval\",\n        \"Generating visual explanations\",\n        \"Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"Attention is all you need\",\n        \"Deep residual learning for image recognition\",\n        \"Mask r-cnn\",\n        \"Vqa: Visual question answering\",\n        \"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\",\n        \"The Impact of Explanations on AI Competency Prediction in VQA\",\n        \"Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"A model of inexact reasoning in medicine\",\n        \"Explainable artificial intelligence for training and tutoring\",\n        \"An explainable artificial intelligence system for small-unit tactical behavior\",\n        \"Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)\",\n        \"Do explanations make VQA models more predictable to a human?\",\n        \"Explicit bias discovery in visual question answering models\",\n        \"Manipulating and measuring model interpretability\",\n        \"Anticipating Visual Representations from Unlabeled Video\",\n        \"Predicting Motivations of Actions by Leveraging Text\",\n        \"Real-time inference of complex mental states from facial expressions and head gestures\",\n        \"A Bayesian computer vision system for modeling human interactions\",\n        \"Who is Mistaken?\",\n        \"Learning to act properly: Predicting and explaining affordances from images\",\n        \"Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration\",\n        \"Pythia v0.1: the Winning Entry to the VQA Challenge 2018\",\n        \"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\",\n        \"Ask, attend and answer: Exploring question-guided spatial attention for visual question answering\",\n        \"Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering\",\n        \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Stacked Self-Attention Networks for Visual Question Answering\",\n        \"VisualBERT: A Simple and Performant Baseline for Vision and Language\",\n        \"Rise: Randomized input sampling for explanation of black-box models\",\n        \"Explaining Image Classifiers by Counterfactual Generation\",\n        \"Guided zoom: Questioning network evidence for fine-grained classification\",\n        \"Grounding visual explanations\",\n        \"Exploring human-like attention supervision in visual question answering\",\n        \"Predicting failures of vision systems\",\n        \"Towards transparent systems: Semantic characterization of failure modes\",\n        \"When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making\",\n        \"U-cam: Visual explanation using uncertainty based class activation maps\"\n    ],\n    \"reasoning\": \"Selected these 52 references because they represent foundational work and recent advances in Generating and Evaluating Explanations of Attended and Error-Inducing Input Regions for VQA Models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}