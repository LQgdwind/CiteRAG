{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 212 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning\n\nPaper Abstract: Abstract Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.\n\n{retrieved_text}\n\nGenerate exactly 212 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 212\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Minds, brains, and programs\",\n        \"Artificial general intelligence, volume 2\",\n        \"In two minds: dual-process accounts of reasoning\",\n        \"Dual-process theories of higher cognition: Advancing the debate\",\n        \"Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering\",\n        \"Ok-vqa: A visual question answering benchmark requiring external knowledge\",\n        \"Scienceqa: A novel resource for question answering on scholarly articles\",\n        \"Gqa: A new dataset for real-world visual reasoning and compositional question answering\",\n        \"Touchstone: Evaluating vision-language models by language models\",\n        \"Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs\",\n        \"Sparkles: Unlocking chats across multiple images for multimodal instruction-following models\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"Visual instruction tuning\",\n        \"Empowering vision-language models to follow interleaved vision-language instructions\",\n        \"Multimodal few-shot learning with frozen language models\",\n        \"Socratic models: Composing zero-shot multimodal reasoning with language\",\n        \"Rt-2: Vision-language-action models transfer web knowledge to robotic control\",\n        \"Pali-x: On scaling up a multilingual vision and language model\",\n        \"Palm-e: An embodied multimodal language model\",\n        \"Visual chatgpt: Talking, drawing and editing with visual foundation models\",\n        \"Mm-react: Prompting chatgpt for multimodal reasoning and action\",\n        \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\",\n        \"Llama-adapter: Efficient fine-tuning of language models with zero-init attention\",\n        \"Kosmos-2: Grounding multimodal large language models to the world\",\n        \"mplug-owl: Modularization empowers large language models with multimodality\",\n        \"Mathematical capabilities of chatgpt\",\n        \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\",\n        \"Survey of hallucination in natural language generation\",\n        \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\",\n        \"Nature language reasoning, a survey\",\n        \"Towards reasoning in large language models: A survey\",\n        \"What is reasoning? what is an argument?\",\n        \"A concise introduction to logic\",\n        \"Informal logic and the theory of reasoning\",\n        \"Reasoning and the logic of things: The cambridge conferences lectures of 1898\",\n        \"Logical reasoning in formal and everyday reasoning tasks\",\n        \"Logical reasoning\",\n        \"Admissibility of logical inference rules\",\n        \"Belief rule-base inference methodology using the evidential reasoning approach-rimer\",\n        \"Deductive reasoning\",\n        \"Inductive reasoning and kolmogorov complexity\",\n        \"Abduction\",\n        \"Analogical reasoning: What develops? a review of research and theory\",\n        \"Analogy and analogical reasoning\",\n        \"Introduction to logic and critical thinking\",\n        \"Training verifiers to solve math word problems\",\n        \"Are nlp models really able to solve simple math word problems?\",\n        \"A diverse corpus for evaluating and developing English math word problem solvers\",\n        \"MAWPS: A math word problem repository\",\n        \"Mathqa: Towards interpretable math word problem solving with operation-based formalisms\",\n        \"Program induction by rationale generation: Learning to solve and explain algebraic word problems\",\n        \"Measuring mathematical problem solving with the math dataset\",\n        \"Hellaswag: Can a machine really finish your sentence?\",\n        \"Winogrande: An adversarial winograd schema challenge at scale\",\n        \"Social iqa: Commonsense reasoning about social interactions\",\n        \"Piqa: Reasoning about physical commonsense in natural language\",\n        \"Commongen: A constrained text generation challenge for generative commonsense reasoning\",\n        \"Cosmos qa: Machine reading comprehension with contextual commonsense reasoning\",\n        \"Abductive commonsense reasoning\",\n        \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\",\n        \"On the paradox of learning to reason from data\",\n        \"Folio: Natural language reasoning with first-order logic\",\n        \"Proofwriter: Generating implications, proofs, and abductive statements over natural language\",\n        \"Challenging big-bench tasks and whether chain-of-thought can solve them\",\n        \"Textworld: A learning environment for text-based games\",\n        \"Alfworld: Aligning text and embodied environments for interactive learning\",\n        \"Webshop: Towards scalable real-world web interaction with grounded language agents\",\n        \"Babyai: A platform to study the sample efficiency of grounded language learning\",\n        \"Minedojo: Building open-ended embodied agents with internet-scale knowledge\",\n        \"Do as i can, not as i say: Grounding language in robotic affordances\",\n        \"Microsoft coco: Common objects in context\",\n        \"Nocaps: Novel object captioning at scale\",\n        \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\",\n        \"Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models, 2023\",\n        \"Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023\",\n        \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2023\",\n        \"Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023\",\n        \"Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts\",\n        \"mplug-docowl: Modularized multimodal large language model for document understanding, 2023\",\n        \"Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond\",\n        \"M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models\",\n        \"Mmbench: Is your multi-modal model an all-around player?, 2023\",\n        \"Mme: A comprehensive evaluation benchmark for multimodal large language models\",\n        \"Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark, 2023\",\n        \"Halle-switch: Controlling object hallucination in large vision language models, 2023\",\n        \"Visit-bench: A benchmark for vision-language instruction following inspired by real-world use, 2023\",\n        \"Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023\",\n        \"Tiny lvlm-ehub: Early multimodal experiments with bard, 2023\",\n        \"What matters in training a gpt4-style language model with multimodal inputs?, 2023\",\n        \"Learn to explain: Multimodal reasoning via thought chains for science question answering\",\n        \"Winoground: Probing vision and language models for visio-linguistic compositionality, 2022\",\n        \"Raven: A dataset for relational and analogical visual reasoning\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Microsoft coco: Common objects in context, 2015\",\n        \"Lvis: A dataset for large vocabulary instance segmentation, 2019\",\n        \"Bleu: a method for automatic evaluation of machine translation\",\n        \"Cider: Consensus-based image description evaluation, 2015\",\n        \"ROUGE: A package for automatic evaluation of summaries\",\n        \"Judging llm-as-a-judge with mt-bench and chatbot arena, 2023\",\n        \"Gpt-4 technical report, 2023\",\n        \"Fool your (vision and) language model with embarrassingly simple permutations, 2023\",\n        \"Solving quantitative reasoning problems with language models\",\n        \"Explain yourself! leveraging language models for commonsense reasoning\",\n        \"Chain-of-thought prompting elicits reasoning in large language models\",\n        \"Least-to-most prompting enables complex reasoning in large language models\",\n        \"Large language models are zero-shot reasoners\",\n        \"Complexity-based prompting for multi-step reasoning\",\n        \"Automatic chain of thought prompting in large language models\",\n        \"Pal: Program-aided language models\",\n        \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022\",\n        \"React: Synergizing reasoning and acting in language models, 2023\",\n        \"Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023\",\n        \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\",\n        \"Language models are few-shot learners\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Bloom: A 176b-parameter open-access multilingual language model, 2023\",\n        \"Training compute-optimal large language models\",\n        \"Llama: Open and efficient foundation language models, 2023\",\n        \"Opt: Open pre-trained transformer language models\",\n        \"GLM: General language model pretraining with autoregressive blank infilling\",\n        \"Stanford alpaca: An instruction-following llama model\",\n        \"Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023\",\n        \"Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023\",\n        \"Improved baselines with visual instruction tuning\",\n        \"Otter: A multi-modal model with in-context instruction tuning, 2023\",\n        \"Cogvlm: Visual expert for pretrained language models, 2023\",\n        \"Training a helpful and harmless assistant with reinforcement learning from human feedback\",\n        \"Evaluating large language models trained on code\",\n        \"Finetuned language models are zero-shot learners\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Laion-5b: An open large-scale dataset for training next generation image-text models\",\n        \"Multimodal c4: An open, billion-scale corpus of images interleaved with text\",\n        \"Instruction tuning with gpt-4\",\n        \"Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning\",\n        \"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts\",\n        \"Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021\",\n        \"Visual genome: Connecting language and vision using crowdsourced dense image annotations\",\n        \"A-okvqa: A benchmark for visual question answering using world knowledge\",\n        \"Ocr-vqa: Visual question answering by reading text in images\",\n        \"Textcaps: a dataset for image captioning with reading comprehension\",\n        \"Coyo-700m: Image-text pair dataset\",\n        \"Datacomp: In search of the next generation of multimodal datasets\",\n        \"Referitgame: Referring to objects in photographs of natural scenes\",\n        \"Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023\",\n        \"Im2text: Describing images using 1 million captioned photographs\",\n        \"Generation and comprehension of unambiguous object descriptions\",\n        \"Llavar: Enhanced visual instruction tuning for text-rich image understanding\",\n        \"Mitigating hallucination in large multi-modal models via robust instruction tuning\",\n        \"Dvqa: Understanding data visualizations via question answering\",\n        \"Docvqa: A dataset for vqa on document images\",\n        \"Chartqa: A benchmark for question answering about charts with visual and logical reasoning\",\n        \"Ai2d-rst: A multimodal corpus of 1000 primary school science diagrams\",\n        \"Openflamingo: An open-source framework for training large autoregressive vision-language models\",\n        \"Introducing idefics: An open reproduction of state-of-the-art visual language model, August 2023\",\n        \"Mimic-it: Multi-modal in-context instruction tuning, 2023\",\n        \"Mmicl: Empowering vision-language model with multi-modal in-context learning, 2023\",\n        \"Catastrophic interference in connectionist networks: The sequential learning problem\",\n        \"Grounding language models to images for multimodal inputs and outputs\",\n        \"A survey for in-context learning\",\n        \"What makes good in-context examples for GPT-3?\",\n        \"Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity\",\n        \"Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering\",\n        \"From images to textual prompts: Zero-shot visual question answering with frozen large language models\",\n        \"Language models with image descriptors are strong few-shot video-language learners\",\n        \"A survey of embodied ai: From simulators to research tasks\",\n        \"Habitat: A platform for embodied ai research\",\n        \"Cliport: What and where pathways for robotic manipulation\",\n        \"Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action\",\n        \"Collaborating with language models for embodied reasoning\",\n        \"Statler: State-maintaining language models for embodied reasoning\",\n        \"Inner monologue: Embodied reasoning through planning with language models\",\n        \"Code as policies: Language model programs for embodied control\",\n        \"Robotic skill acquisition via instruction augmentation with vision-language models\",\n        \"Distilling internet-scale vision-language models into embodied agents\",\n        \"Object scene representation transformer\",\n        \"Rt-1: Robotics transformer for real-world control at scale\",\n        \"Talm: Tool augmented language models\",\n        \"Toolformer: Language models can teach themselves to use tools, 2023\",\n        \"Art: Automatic multi-step reasoning and tool-use for large language models\",\n        \"Tool learning with foundation models, 2023\",\n        \"Augmented language models: a survey, 2023\",\n        \"Foundation models for decision making: Problems, methods, and opportunities, 2023\",\n        \"Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023\",\n        \"Creator: Disentangling abstract and concrete reasonings of large language models through tool creation, 2023\",\n        \"Webgpt: Browser-assisted question-answering with human feedback, 2022\",\n        \"Chameleon: Plug-and-play compositional reasoning with large language models, 2023\",\n        \"Tool documentation enables zero-shot tool-usage with large language models, 2023\",\n        \"Lamda: Language models for dialog applications, 2022\",\n        \"Tree of thoughts: Deliberate problem solving with large language models, 2023\",\n        \"Gorilla: Large language model connected with massive apis\",\n        \"Visual programming: Compositional visual reasoning without training\",\n        \"Dsfd: dual shot face detector\",\n        \"Per-pixel classification is not all you need for semantic segmentation\",\n        \"Vilt: Vision-and-language transformer without convolution or region supervision\",\n        \"High-resolution image synthesis with latent diffusion models\",\n        \"Adding conditional control to text-to-image diffusion models\",\n        \"Llama-adapter v2: Parameter-efficient visual instruction model, 2023\",\n        \"mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023\",\n        \"Infimm:, 2024\",\n        \"Gemini: a family of highly capable multimodal models\",\n        \"Introducing our multimodal models, 2023\",\n        \"Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023\",\n        \"Scaling instruction-finetuned language models, 2022\",\n        \"Obelisc: An open web-scale filtered dataset of interleaved image-text documents\",\n        \"Generative pretraining in multimodality\",\n        \"Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models\",\n        \"Vx2text: End-to-end learning of video-based text generation from multimodal inputs\",\n        \"Towards fast adaptation of pretrained contrastive models for multi-channel video-language retrieval\",\n        \"Proximal policy optimization algorithms\",\n        \"Direct preference optimization: Your language model is secretly a reward model\",\n        \"Aligning large multimodal models with factually augmented rlhf\"\n    ],\n    \"reasoning\": \"Selected these 212 references because they represent foundational work and recent advances in Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}