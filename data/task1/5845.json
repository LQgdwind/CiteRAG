{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities\n\nPaper Abstract: Abstract. Multimodal sentiment analysis has been studied under the assumption that all modalities are available. However, such a strong assumption does not always hold in practice, and most of multimodal fusion models may fail when partial modalities are missing. Several works have addressed the missing modality problem; but most of them only considered the single modality missing case, and ignored the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer Encoder (TATE) network to handle the problem of missing uncertain modalities. Specifically, we design a tag encoding module to cover both the single modality and multiple modalities missing cases, so as to guide the networkâ€™s attention to those missing modalities. Besides, we adopt a new space projection pattern to align common vectors. Then, a Transformer encoder-decoder network is utilized to learn the missing modality features. At last, the outputs of the Transformer encoder are used for the final sentiment classification. Extensive experiments are conducted on CMU-MOSI and IEMOCAP datasets, showing that our method can achieve significant improvements compared with several baselines.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text\",\n        \"Autoencoders, unsupervised learning, and deep architectures\",\n        \"Openface 2.0: Facial behavior analysis toolkit\",\n        \"IEMOCAP: Interactive emotional dyadic motion capture database\",\n        \"Deep adversarial learning for multi-modality missing data completion\",\n        \"Multimodal sentiment analysis with word-level fusion and reinforcement learning\",\n        \"Twins: Revisiting the Design of Spatial Attention in Vision Transformers\",\n        \"Transformer-xl: Attentive language models beyond a fixed-length context\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Semi-supervised deep generative modelling of incomplete multi-modality emotional data\",\n        \"Iterative network pruning with uncertainty regularization for lifelong sentiment classification\",\n        \"Generative adversarial nets\",\n        \"Online early-late fusion based on adaptive HMM for sign language recognition\",\n        \"Misa: Modality-invariant and-specific representations for multimodal sentiment analysis\",\n        \"Deep residual learning for image recognition\",\n        \"Hypergraph attention networks for multimodal learning\",\n        \"Adam: A method for stochastic optimization\",\n        \"Auto-encoding variational bayes\",\n        \"Reformer: The efficient transformer\",\n        \"Visualizing data using t-SNE\",\n        \"Analyzing multimodal sentiment via acoustic-and visual-LSTM with channel-aware temporal convolution network\",\n        \"Multimodal sentiment analysis using hierarchical fusion with context modeling\",\n        \"Spectral regularization algorithms for learning large incomplete matrices\",\n        \"librosa: Audio and music signal analysis in python\",\n        \"Training strategies to handle missing modalities for audio-visual expression recognition\",\n        \"Found in translation: Learning robust joint representations by cyclic translations between modalities\",\n        \"Multimodal sentiment analysis: Addressing key issues and setting up the baselines\",\n        \"VIGAN: Missing view imputation with generative adversarial networks\",\n        \"Clustering on multiple incomplete datasets via collective kernel learning\",\n        \"Web table retrieval using multimodal deep learning\",\n        \"QuTI! Quantifying Text-Image Consistency in Multimodal Documents\",\n        \"Sentiment analysis and topic recognition in video transcriptions\",\n        \"Missing modalities imputation via cascaded residual autoencoder\",\n        \"Attention is all you need\",\n        \"Extracting and composing robust features with denoising autoencoders\",\n        \"Investigating dynamic routing in tree-structured LSTM for sentiment analysis\",\n        \"Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis\",\n        \"Social image sentiment analysis by exploiting multimodal content and heterogeneous relations\",\n        \"A co-memory network for multimodal sentiment analysis\",\n        \"Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association\",\n        \"Entity-sensitive attention and fusion network for entity-level multimodal sentiment classification\",\n        \"Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality\",\n        \"Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis\",\n        \"Transformer-based feature reconstruction network for robust multimodal sentiment analysis\",\n        \"Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages\",\n        \"Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph\",\n        \"Knowledge guided capsule attention network for aspect-based sentiment analysis\",\n        \"Deep partial multi-view learning\",\n        \"Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities\",\n        \"Joint aspect-sentiment analysis with minimal user guidance\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}