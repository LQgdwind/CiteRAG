{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 64 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Generative AI for Controllable Protein Sequence Design: A Survey\n\nPaper Abstract: Abstract The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finally, we identify the unresolved challenges and highlight research opportunities that merit deeper exploration.\n\n{retrieved_text}\n\nGenerate exactly 64 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 64\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Protein generation with evolutionary diffusion: sequence is all you need\",\n        \"Model-based reinforcement learning for biological sequence design\",\n        \"Design by directed evolution\",\n        \"Structured denoising diffusion models in discrete state-spaces\",\n        \"A neural probabilistic language model\",\n        \"Flow network based generative models for non-iterative diverse candidate generation\",\n        \"Introduction to protein structure\",\n        \"Language models are few-shot learners\",\n        \"Transformer-based protein generation with regularized latent space optimization\",\n        \"xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein\",\n        \"Uniprot: a worldwide hub of protein knowledge\",\n        \"Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations\",\n        \"Robust deep learning–based protein sequence design using proteinmpnn\",\n        \"Diffusion models beat gans on image synthesis\",\n        \"Controllable protein design with language models\",\n        \"Protgpt2 is a deep unsupervised language model for protein design\",\n        \"Knowledge-design: Pushing the limit of protein deign via knowledge refinement\",\n        \"Pifold: Toward effective and efficient protein inverse folding\",\n        \"Proteininvbench: Benchmarking protein inverse folding on diverse tasks, models, and metrics\",\n        \"Automatic chemical design using a data-driven continuous representation of molecules\",\n        \"Generative adversarial networks\",\n        \"Protein design with guided discrete diffusion\",\n        \"Generating functional protein variants with variational autoencoders\",\n        \"Rita: a study on scaling up generative protein sequence models\",\n        \"Denoising diffusion probabilistic models\",\n        \"Autoregressive diffusion models\",\n        \"Learning inverse folding from millions of predicted structures\",\n        \"Generative models for graph-based protein design\",\n        \"Biological sequence design with gflownets\",\n        \"Learning from protein structure with geometric vector perceptrons\",\n        \"Scaling laws for neural language models\",\n        \"Ctrl: A conditional transformer language model for controllable generation\",\n        \"Auto-encoding variational bayes\",\n        \"Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles\",\n        \"Diffusion-lm improves controllable text generation\",\n        \"Disentangled wasserstein autoencoder for t-cell receptor engineering\",\n        \"A text-guided protein design framework\",\n        \"Large language models generate functional protein sequences across diverse families\",\n        \"De novo protein design using geometric vector field networks\",\n        \"Progen2: exploring the boundaries of protein language models\",\n        \"Observed antibody space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences\",\n        \"Cath–a hierarchic classification of protein domain structures\",\n        \"Msa transformer\",\n        \"Proximal exploration for model-guided protein sequence design\",\n        \"Expanding functional protein sequence spaces using generative adversarial networks\",\n        \"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\",\n        \"An evolution-based model for designing chorismate mutase enzymes\",\n        \"Generative power of a protein language model trained on multiple sequence alignments\",\n        \"Taking the human out of the loop: A review of bayesian optimization\",\n        \"Adalead: A simple and robust adaptive greedy search algorithm for sequence design\",\n        \"Deep unsupervised learning using nonequilibrium thermodynamics\",\n        \"Accelerating bayesian optimization for biological sequence design with denoising autoencoders\",\n        \"Proteinrl: Reinforcement learning with generative protein language models for property-directed sequence design\",\n        \"Reinforcement learning: An introduction\",\n        \"Amortized bayesian optimization over discrete spaces\",\n        \"Discovering highly potent antimicrobial peptides with deep generative model hydramp\",\n        \"Attention is all you need\",\n        \"Self-play reinforcement learning guides protein engineering\",\n        \"Instructprotein: Aligning human and protein language via knowledge instruction\",\n        \"Signal peptides generated by attention-based neural networks\",\n        \"Protein sequence design with deep generative models\",\n        \"Graph denoising diffusion for inverse protein folding\",\n        \"Structure-informed language models are protein designers\",\n        \"Sample-efficient multi-objective molecular optimization with gflownets\"\n    ],\n    \"reasoning\": \"Selected these 64 references because they represent foundational work and recent advances in Generative AI for Controllable Protein Sequence Design: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}