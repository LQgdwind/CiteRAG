{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 699 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\n\nPaper Abstract: Abstract Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.\n\n{retrieved_text}\n\nGenerate exactly 699 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 699\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"russian proverbs\",\n        \"Persistent anti-Muslim bias in large language models\",\n        \"A survey of neural networks and formal languages\",\n        \"VQA: Visual question answering\",\n        \"Learning convex optimization models\",\n        \"A very unlikely chess game\",\n        \"Asking clarifying questions in open-domain information-seeking conversations\",\n        \"Learning continuous semantic representations of symbolic expressions\",\n        \"A survey of machine learning for big code and naturalness\",\n        \"code2seq: Generating sequences from structured representations of code\",\n        \"Structural language models of code\",\n        \"A survey on approaches to computational humor generation\",\n        \"MathQA: Towards interpretable math word problem solving with operation-based formalisms\",\n        \"Toward automated quest generation in text-adventure games\",\n        \"Bringing stories alive: Generating interactive fiction worlds\",\n        \"Concrete problems in AI safety\",\n        \"Optnet: Differentiable optimization as a layer in neural networks\",\n        \"More is different\",\n        \"ColBERT: Using BERT sentence embedding for humor detection\",\n        \"On the cross-lingual transferability of monolingual representations\",\n        \"Efficient large scale language modeling with mixtures of experts\",\n        \"Big BiRD: A large, fine-grained, bigram relatedness dataset for examining semantic composition\",\n        \"A general language assistant as a laboratory for alignment\",\n        \"Generating fact checking explanations\",\n        \"Humor in language\",\n        \"Program synthesis with large language models\",\n        \"Celex2 ldc96l14\",\n        \"Explaining neural scaling laws\",\n        \"Real or fake? Learning to discriminate machine from human generated text\",\n        \"Deepcoder: Learning to write programs\",\n        \"Extended gloss overlaps as a measure of semantic relatedness\",\n        \"ITEM2VEC: Neural item embedding for collaborative filtering\",\n        \"Big data’s disparate impact\",\n        \"Teaching classification boundaries to humans\",\n        \"The Pushshift Reddit dataset\",\n        \"The relationship between inference skills and reading comprehension\",\n        \"Neural path planning: Fixed time, near-optimal path generation via oracle imitation\",\n        \"Climbing towards NLU: On meaning, form, and understanding in the age of data\",\n        \"On the dangers of stochastic parrots: Can language models be too big?\",\n        \"The child’s learning of english morphology\",\n        \"Neural-symbolic learning and reasoning: A survey and interpretation\",\n        \"Critical thinking for language models\",\n        \"Abductive commonsense reasoning\",\n        \"Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge\",\n        \"On the ability and limitations of transformers to recognize formal languages\",\n        \"On the practical ability of recurrent neural networks to recognize hierarchical languages\",\n        \"Deep API programmer: Learning to program with APIs\",\n        \"Neural language models are effective plagiarists\",\n        \"The inference of regular LISP programs from examples\",\n        \"Multimodal datasets: Misogyny, pornography, and malignant stereotypes\",\n        \"A clustering approach for nearly unsupervised recognition of nonliteral language\",\n        \"The importance of suppressing domain style in authorship analysis, 2020\",\n        \"PIQA: reasoning about physical commonsense in natural language\",\n        \"Predicting human metaphor paraphrase judgments with deep neural networks\",\n        \"GPT-NeoX-20B: An open-source autoregressive language model\",\n        \"Large dataset and language model fun-tuning for humor recognition\",\n        \"Language (technology) is power: A critical survey of ‘‘bias’’ in NLP\",\n        \"Russian Proverbs and Sayings and Their English Equivalents\",\n        \"Vector forms as a foreign language, 24 June 2019\",\n        \"They Never Said It: A Book of Fake Quotes, Misquotes, and Misleading Attributions\",\n        \"Man is to computer programmer as woman is to homemaker? Debiasing word embeddings\",\n        \"On the opportunities and risks of foundation models, 2021\",\n        \"Identifying and reducing gender bias in word-level language models, 2019\",\n        \"COMET: Commonsense transformers for automatic knowledge graph construction, 2019\",\n        \"D3 data-driven documents\",\n        \"What will it take to fix benchmarking in natural language understanding?\",\n        \"A large annotated corpus for learning natural language inference\",\n        \"Rosetta stone linguistic problems\",\n        \"Programming with a differentiable Forth interpreter\",\n        \"GPT-3 creative fiction\",\n        \"Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts\",\n        \"Verification of forecasts expressed in terms of probability\",\n        \"Non-linear mapping for improved identification of 1300+ languages\",\n        \"Language models are few-shot learners\",\n        \"Ravens attribute visual access to unseen competitors\",\n        \"The WMT’18 morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English\",\n        \"On a family of Turing machines and the related programming language\",\n        \"Inference making ability and its relation to comprehension failure\",\n        \"Eliciting good teaching from humans for machine learners\",\n        \"Semantics derived automatically from language corpora contain human-like biases\",\n        \"Does the chimpanzee have a theory of mind? 30 years later\",\n        \"Machine translation of Klingon, 2010\",\n        \"Extracting training data from large language models\",\n        \"Labeling documents with timestamps: Learning from their time expressions\",\n        \"Studying cultural differences in emoji usage across the East and the West\",\n        \"Simplicity: A unifying principle in cognitive science?\",\n        \"Developing self-awareness in robots via inner speech\",\n        \"Touchdown: Natural language navigation and spatial reasoning in visual street environments\",\n        \"Generative pretraining from pixels\",\n        \"Evaluating large language models trained on code, 2021\",\n        \"Humor recognition using deep learning\",\n        \"Transformers play chess, 2020\",\n        \"Execution-guided neural program synthesis\",\n        \"Generating long sequences with sparse transformers, 2019\",\n        \"On measuring gender bias in translation of gender-neutral pronouns\",\n        \"QuAC: Question answering in context, 2018\",\n        \"On the measure of intelligence, 2019\",\n        \"Abstraction and reasoning challenge, 2020\",\n        \"The algebraic theory of context-free languages\",\n        \"PaLM: Scaling language modeling with pathways, 2022\",\n        \"CycleGAN, a master of steganography, 2017\",\n        \"The Google similarity distance\",\n        \"Unified scaling laws for routed language models, 2022\",\n        \"BoolQ: Exploring the surprising difficulty of natural yes/no questions, 2019\",\n        \"Think you have solved question answering? Try ARC, the AI2 reasoning challenge, 2018\",\n        \"Transformers as soft reasoners over language, 2020\",\n        \"Self recognition in a jumping spider: Portia labiata females discriminate between their own draglines and those of conspecifics\",\n        \"Training verifiers to solve math word problems\",\n        \"General-purpose declarative inductive programming with domain-specific background knowledge for data wrangling automation, 2018\",\n        \"Automated data transformation with inductive programming and dynamic background knowledge\",\n        \"Introduction to Logic\",\n        \"Snips voice platform: An embedded spoken language understanding system for private-by-design voice interfaces, 2018\",\n        \"The trouble with bias\",\n        \"Metagol system, 2016\",\n        \"Meta-interpretive learning of data transformation programs\",\n        \"Learning higher-order logic programs\",\n        \"Emoji usage in TV conversation\",\n        \"TextWorld: A learning environment for text-based games, 2018\",\n        \"White Chicago cops use force more often than Black officers\",\n        \"Playing text-based games with common sense, 2020\",\n        \"Language modeling with gated convolutional networks\",\n        \"Finding contradictions in text\",\n        \"Did it happen? The pragmatic complexity of veridicality assessment\",\n        \"The CommitmentBank: Investigating projection in naturally occurring discourse\",\n        \"Indexing by latent semantic analysis\",\n        \"When redundancy is useful: A Bayesian approach to “overinformative” referring expressions\",\n        \"Calibration of pre-trained transformers\",\n        \"On measuring and mitigating biased inferences of word embeddings\",\n        \"RobustFill: Neural program learning under noisy I/O\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding, 2018\",\n        \"Quasar: Datasets for question answering by search and reading, 2017\",\n        \"Sequence-based prediction of protein--protein interaction sites with l1-logreg classifier\",\n        \"NL-Augmenter: A framework for task-sensitive natural language augmentation, 2021\",\n        \"Queens are powerful too: Mitigating gender bias in dialogue generation, 2019\",\n        \"Learning syllogism with Euler neural-networks, 2020\",\n        \"GLaM: Efficient scaling of language models with mixture-of-experts, 2021\",\n        \"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs\",\n        \"RoFT: A tool for evaluating human detection of machine-generated text\",\n        \"To test machine comprehension, start by defining comprehension\",\n        \"FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization\",\n        \"Neural path hunter: Reducing hallucination in dialogue systems via path grounding, 2021\",\n        \"How can self-attention networks recognize Dyck-n languages?, 2020\",\n        \"Misspelling oblivious word embeddings, 2019\",\n        \"Compositional morpheme embeddings with affixes as functions and stems as arguments\",\n        \"Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language\",\n        \"Semantic relatedness of Wikipedia concepts -- benchmark data and a working solution\",\n        \"emoji2vec: Learning emoji representations from their description\",\n        \"Parsimonious morpheme segmentation with an application to enriching word embeddings, 2019\",\n        \"Semantic sort: A supervised approach to personalized semantic relatedness, 2013\",\n        \"Measuring and improving consistency in pretrained language models, 2021\",\n        \"Learning to learn programs from examples: Going beyond program structure\",\n        \"Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning, 2020\",\n        \"Can neural networks understand logical entailment?, 2018\",\n        \"Making sense of sensory input, 2019\",\n        \"Question answering as an automatic evaluation metric for news article summarization\",\n        \"Text editing by command, 2020\",\n        \"Hierarchical neural story generation\",\n        \"Beyond English-centric multilingual machine translation\",\n        \"Humor detection via an internal and external neural network\",\n        \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021\",\n        \"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\",\n        \"WordNet: An Electronic Lexical Database\",\n        \"Synthesizing data structure transformations from input-output examples\",\n        \"Controlling other people: The impact of power on stereotyping\",\n        \"The Cattell-Horn-Carroll theory of cognitive abilities\",\n        \"An introduction to inductive programming\",\n        \"The Language of Thought\",\n        \"Connectionism and cognitive architecture: A critical analysis\",\n        \"The Elements of Eloquence: Secrets of the Perfect Turn of Phrase\",\n        \"Whodunnit? Crime drama as a case for natural language understanding\",\n        \"“The penny drops”: Investigating insight through the medium of cryptic crosswords\",\n        \"Teaching GPT-2 transformer a sense of humor: How to fine-tune large transformer models on a single GPU in PyTorch\",\n        \"Go figure: A meta evaluation of factuality in summarization, 2020\",\n        \"Computing semantic relatedness using Wikipedia-based explicit semantic analysis\",\n        \"Predictability and surprise in large generative models, 2022\",\n        \"Neural metaphor detection in context\",\n        \"The pile: An 800GB dataset of diverse text for language modeling, 2021\",\n        \"Niklas Muennighoff\",\n        \"Neurosymbolic AI: The 3rd wave, 2020\",\n        \"Evaluating models’ local decision boundaries via contrast sets, 2020\",\n        \"The TUNA-REG challenge 2009: Overview and evaluation results\",\n        \"TerpreT: A probabilistic programming language for program induction, 2016\",\n        \"Knowledge-aware assessment of severity of suicide risk for early intervention\",\n        \"SyntaxGym: An online platform for targeted evaluation of language models\",\n        \"RealToxicityPrompts: Evaluating neural toxic degeneration in language models\",\n        \"The GEM benchmark: Natural language generation, its evaluation and metrics\",\n        \"The roles of similarity in transfer: Separating retrievability from inferential soundness\",\n        \"Conversational implicatures in English dialogue: Annotated dataset\",\n        \"Injecting numerical reasoning skills into language models\",\n        \"Transformer feed-forward layers are key-value memories, 2020b\",\n        \"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies\",\n        \"Irony detection in a multilingual context\",\n        \"A report on the 2020 sarcasm detection shared task, 2020\",\n        \"ePiC: Employing proverbs in context as a benchmark for abstract language understanding, 2021\",\n        \"Color naming across languages reflects color use\",\n        \"Assessing BERT’s syntactic abilities, 2019\",\n        \"Structural equation methods in the social sciences\",\n        \"Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them\",\n        \"Identifying sarcasm in Twitter: A closer look\",\n        \"Pragmatic language interpretation as probabilistic inference\",\n        \"Topical-chat: Towards knowledge-grounded open-domain conversations\",\n        \"Are neural open-domain dialog systems robust to speech recognition errors in the dialog history? An empirical study\",\n        \"Choice of plausible alternatives (COPA), 2010\",\n        \"English gigaword\",\n        \"Neural Turing machines, 2014\",\n        \"Hybrid computing using a neural network with dynamic external memory\",\n        \"Progress report on program-understanding systems (AIM-240), 1974\",\n        \"Application of theorem proving to problem solving\",\n        \"Stochastic optimization of sorting networks via continuous relaxations, 2019\",\n        \"Universal neural machine translation for extremely low resource languages, 2018\",\n        \"Colorless green recurrent networks dream hierarchically\",\n        \"Automating string processing in spreadsheets using input-output examples\",\n        \"Spreadsheet data manipulation using examples\",\n        \"Inductive programming meets the real world\",\n        \"Program synthesis\",\n        \"On calibration of modern neural networks\",\n        \"Disfl-QA: A benchmark dataset for understanding disfluencies in question answering\",\n        \"English Proverbs and Sayings\",\n        \"The proverb as a mitigating and politeness strategy in Akan discourse\",\n        \"The argument reasoning comprehension task: Identification and reconstruction of implicit warrants\",\n        \"Theoretical limitations of self-attention in neural sequence models\",\n        \"It’s all in the name: Mitigating gender bias with name-based counterfactual data substitution\",\n        \"Actual causality\",\n        \"ECONET: Effective continual pretraining of language models for event temporal reasoning, 2020\",\n        \"When in Rome, do as the Romans do: Proverbs as a part of EFL teaching\",\n        \"Context-free transductions with neural stacks, 2018\",\n        \"An advanced test of theory of mind: Understanding of story characters thoughts and feelings by able autistic, mentally handicapped, and normal children and adults\",\n        \"The MovieLens datasets: History and context\",\n        \"Policy-driven neural response generation for knowledge-grounded dialog systems\",\n        \"A survey on recent approaches for natural language processing in low-resource scenarios\",\n        \"On the projection problem for presuppositions\",\n        \"Tracking the world state with recurrent entity networks, 2016\",\n        \"Women also snowboard: Overcoming bias in captioning models\",\n        \"A baseline for detecting misclassified and out-of-distribution examples in neural networks\",\n        \"Using pre-training can improve model robustness and uncertainty\",\n        \"Aligning AI with shared human values, 2020\",\n        \"Measuring coding challenge competence with APPS, 2021a\",\n        \"Measuring massive multitask language understanding\",\n        \"Measuring mathematical problem solving with the MATH dataset, 2021c\",\n        \"Scaling laws for autoregressive generative modeling, 2020\",\n        \"The weirdest people in the world?\",\n        \"Teaching machines to read and comprehend\",\n        \"Scaling laws for transfer, 2021\",\n        \"3D-DEEP: 3-dimensional deep-learning based on elevation patterns for road scene interpretation\",\n        \"TaPas: Weakly supervised table parsing via pre-training\",\n        \"Deep learning scaling is predictable, empirically, 2017\",\n        \"Beyond human-level accuracy: Computational challenges in deep learning\",\n        \"RNNs can generate bounded hierarchical languages with optimal memory, 2020\",\n        \"Algorithmic regulation and the rule of law\",\n        \"Training compute-optimal large language models, 2022\",\n        \"Analogy and relational reasoning\",\n        \"A Proverb in Mind: The Cognitive Science of Proverbial Wit and Wisdom\",\n        \"Smelling themselves: Dogs investigate their own odours longer when modified in an “olfactory mirror” test\",\n        \"Learning to solve arithmetic word problems with verb categorization\",\n        \"Bridging anaphora resolution as question answering\",\n        \"Global inference for bridging anaphora resolution\",\n        \"National name report 2018\",\n        \"National name report 2019\",\n        \"National name report 2020\",\n        \"Introduction to Paremiology: A Comprehensive Guide to Proverb Studies\",\n        \"Gamepad: A learning environment for theorem proving, 2018\",\n        \"GQA: A new dataset for real-world visual reasoning and compositional question answering, 2019\",\n        \"Lexical semantic relatedness with random graph walks\",\n        \"A Treatise of Human Nature\",\n        \"Compositionality decomposed: How do neural networks generalise?\",\n        \"Can self-awareness be taught? Monkeys pass the mirror test -- again\",\n        \"David Huynh and Stefano Mazzocchi\",\n        \"Emojineering part 1: Machine learning for emoji trends\",\n        \"Automatic detection of generated text is easiest when humans are fooled\",\n        \"AI safety via debate, 2018\",\n        \"Leveraging passage retrieval with generative models for open domain question answering, 2020\",\n        \"Indic-transformers: An analysis of transformer language models for indian languages, 2020\",\n        \"Roget’s Thesaurus as a lexical resource for natural language processing\",\n        \"Learning to execute instructions in a Minecraft dialogue\",\n        \"Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition\",\n        \"Semantic similarity based on corpus statistics and lexical taxonomy\",\n        \"Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment\",\n        \"How can we know what language models know?\",\n        \"CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning, 2016\",\n        \"Robust encodings: A framework for combating adversarial typos, 2020\",\n        \"Harnessing context incongruity for sarcasm detection\",\n        \"Automatic sarcasm detection: A survey\",\n        \"TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension, 2017\",\n        \"Inferring algorithmic patterns with stack-augmented recurrent nets\",\n        \"Template guided text generation for task-oriented dialogue\",\n        \"Rogue-Gym: A new challenge for generalization in reinforcement learning\",\n        \"Wrangler: Interactive visual specification of data transformation scripts\",\n        \"Critique of Pure Reason\",\n        \"Prolegomena to Any Future Metaphysics\",\n        \"Scaling laws for neural language models, 2020\",\n        \"The unreasonable effectiveness of recurrent neural networks\",\n        \"Simple and phrasal implicatives\",\n        \"Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2019\",\n        \"Are pretrained language models symbolic reasoners over knowledge?, 2020\",\n        \"Learning the difference that makes a difference with counterfactually-augmented data, 2019\",\n        \"Alignment of language agents, 2021\",\n        \"The misgendering machines: Trans/HCI implications of automatic gender recognition\",\n        \"How do humans teach: On curriculum learning and teaching dimension\",\n        \"ParsiNLU: A suite of language understanding challenges for persian, 2020a\",\n        \"UNIFIEDQA: Crossing format boundaries with a single QA system\",\n        \"A large self-annotated corpus for sarcasm, 2017\",\n        \"Dynabench: Rethinking benchmarking in NLP\",\n        \"Cooperation and codenames: Understanding natural language processing via codenames\",\n        \"Character-aware neural language models, 2015\",\n        \"Evaluating approaches to personalizing language models\",\n        \"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate\",\n        \"Inductive programming: A survey of program synthesis techniques\",\n        \"Intentional action and side effects in ordinary language\",\n        \"A surprisingly robust trick for the Winograd schema challenge\",\n        \"The NarrativeQA reading comprehension challenge\",\n        \"MultiEmo: Multilingual, multilevel, multidomain sentiment analysis corpus of consumer reviews\",\n        \"Counterlogicals as counterconventionals\",\n        \"Against conventional wisdom\",\n        \"Authorship verification as a one-class classification problem\",\n        \"Sprichwörter und zweisprachige lexikographie: Deutsch-schwedische und deutsch-finnische wörtebücher im vergleich\",\n        \"A burstiness-aware approach for document dating\",\n        \"Self-Aware Computing Systems\",\n        \"The aha! moment: The cognitive neuroscience of insight\",\n        \"WikiHow: A large scale text summarization dataset, 2018\",\n        \"All the news that’s fit to fabricate: AI-generated text as a tool of media misinformation\",\n        \"Quality at a glance: An audit of web-crawled multilingual datasets\",\n        \"Hurdles to progress in long-form question answering, 2021\",\n        \"Evaluating the factual consistency of abstractive text summarization\",\n        \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing\",\n        \"Natural questions: A benchmark for question answering research\",\n        \"The NetHack learning environment, 2020\",\n        \"Giving GPT-3 a Turing test\",\n        \"RACE: Large-scale ReAding comprehension dataset from examinations\",\n        \"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks, 2017\",\n        \"Word meaning in minds and machines, 2020\",\n        \"Building machines that learn and think like people\",\n        \"Metaphors We Live By\",\n        \"The emergence of number and syntax units in LSTM language models\",\n        \"Can RNNs learn recursive nested subject-verb agreements?, 2021a\",\n        \"Mechanisms for handling nested dependencies in neural-network language models and humans\",\n        \"Deep learning for symbolic mathematics, 2019\",\n        \"ALBERT: A lite BERT for self-supervised learning of language representations, 2019\",\n        \"Revisiting the evaluation of theory of mind through question answering\",\n        \"Language models as fact checkers?\",\n        \"Towards few-shot fact-checking via perplexity\",\n        \"Scalable agent alignment via reward modeling: A research direction, 2018\",\n        \"Solving logic puzzles: From robust processing to precise semantics\",\n        \"Zero-shot relation extraction via reading comprehension\",\n        \"TR9856: A multi-word term relatedness benchmark\",\n        \"Investigating memorization of conspiracy theories in text generation, 2021\",\n        \"MLQA: Evaluating cross-lingual extractive question answering\",\n        \"Retrieval-augmented generation for knowledge-intensive NLP tasks, 2020b\",\n        \"Question and answer test-train overlap in open-domain question answering datasets, 2020c\",\n        \"UNQOVERing stereotyping biases via underspecified questions\",\n        \"DailyDialog: A manually labelled multi-turn dialogue dataset\",\n        \"DELPHI: Accurate deep ensemble model for protein interaction sites prediction\",\n        \"An approach for measuring semantic similarity between words using multiple information sources\",\n        \"A meaning-based statistical English math word problem solver\",\n        \"Towards debiasing sentence representations\",\n        \"Learning to contrast the counterfactual samples for robust visual question answering\",\n        \"Birds have four legs?! NumerSense: probing numerical commonsense knowledge of pre-trained language models\",\n        \"Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge, 2021a\",\n        \"Reasoning over paragraph effects in situations\",\n        \"TruthfulQA: Measuring how models mimic human falsehoods, 2021b\",\n        \"Program induction by rationale generation: Learning to solve and explain algebraic word problems, 2017\",\n        \"Assessing the ability of LSTMs to learn syntax-sensitive dependencies\",\n        \"What makes good in-context examples for GPT-3?, 2021a\",\n        \"LogiQA: A challenge dataset for machine reading comprehension with logical reasoning, 2020a\",\n        \"Can small and synthetic benchmarks drive modeling innovation? A retrospective study of question answering modeling approaches, 2021b\",\n        \"A token-level reference-free hallucination detection benchmark for free-form text generation, 2021c\",\n        \"Interpretable multi-step reasoning with knowledge extraction on complex healthcare question answering, 2020b\",\n        \"RoBERTa: A robustly optimized BERT pretraining approach, 2019\",\n        \"Multilingual denoising pre-training for neural machine translation, 2020c\",\n        \"SemEval-2015 task 5: QA TempEval - evaluating temporal information understanding with question answering\",\n        \"Content preserving text generation with attribute controls, 2018\",\n        \"Scruples: A corpus of community ethical judgments on 32,000 real-life anecdotes, 2020\",\n        \"Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark, 2021\",\n        \"Gender bias in neural natural language processing\",\n        \"What’s in the box? An analysis of undesirable content in the Common Crawl corpus\",\n        \"A survey of reinforcement learning informed by natural language\",\n        \"EventPlus: A temporal event understanding pipeline, 2021\",\n        \"Language models as few-shot learner for task-oriented dialogue systems, 2020\",\n        \"Few-shot bot: Prompt-based learning for dialogue systems, 2021\",\n        \"Low-resource languages: A review of past work and future challenges, 2020\",\n        \"Automatic prediction of discourse connectives\",\n        \"Encode, tag, realize: High-precision text editing, 2019\",\n        \"A BERT-based approach for automatic humor detection and scoring\",\n        \"The next decade in AI: Four steps towards robust artificial intelligence, 2020\",\n        \"GPT-3, bloviator: OpenAI’s language generator has no idea what it’s talking about\",\n        \"The Penn Treebank: Annotating predicate argument structure\",\n        \"Collective classification for fine-grained information status\",\n        \"Inclusive data visualization for people with disabilities: A call to action\",\n        \"Research community dynamics behind popular AI benchmarks\",\n        \"Targeted syntactic evaluation of language models\",\n        \"Recipe1M+: A dataset for learning cross-modal embeddings for cooking recipes and food images\",\n        \"Annotating character relationships in literary texts, 2015\",\n        \"Suicide risk assessment with multi-level dual-context language and BERT\",\n        \"On measuring social biases in sentence encoders\",\n        \"OpenAI API alchemy: Emoji storytelling\",\n        \"On faithfulness and factuality in abstractive summarization, 2020\",\n        \"Context based spelling correction\",\n        \"The application of convolution neural network based cell segmentation during cryopreservation\",\n        \"Image-based recommendations on styles and substitutes, 2015\",\n        \"The natural language decathlon: Multitask learning as question answering, 2018\",\n        \"Extending machine language models toward human-level language understanding, 2019\",\n        \"Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks\",\n        \"Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference, 2019\",\n        \"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks\",\n        \"Acquisition of chess knowledge in AlphaZero, 2021\",\n        \"Documenting geographically and contextually diverse data sources: The BigScience catalogue of language data and resources, 2022\",\n        \"USR: An unsupervised and reference free evaluation metric for dialog generation\",\n        \"Phraseologie des schwedischen\",\n        \"Interactive optimal teaching with unknown learners\",\n        \"A framework for the computational linguistic analysis of dehumanization\",\n        \"Temporal information extraction for question answering using syntactic dependencies in an LSTM-based architecture\",\n        \"Pointer sentinel mixture models, 2016\",\n        \"On the linguistic capacity of real-time counter automata, 2020\",\n        \"Andere zeiten, andere lehren\\\": Sprach-und kulturgeschichtliche betrachtungen zum sprichwort\",\n        \"Making computers laugh: Investigations in automatic humor recognition\",\n        \"The effect of natural distribution shift on question answering models\",\n        \"Automatic disambiguation of English puns\",\n        \"SemEval-2017 task 7: Detection and interpretation of English puns\",\n        \"An effective, low-cost measure of semantic relatedness obtained from Wikipedia links\",\n        \"National name statistical analysis, 2018\",\n        \"Cross-task generalization via natural language crowdsourcing instructions, 2021\",\n        \"Natural reference to objects in a visual domain\",\n        \"Generating expressions that refer to visible objects\",\n        \"Playing Atari with deep reinforcement learning, 2013\",\n        \"CLaC at CLPsych 2019: Fusion of neural features and predicted class probabilities for suicide risk assessment based on online posts\",\n        \"Introducing the LCC metaphor datasets\",\n        \"Lexical cohesion computed by thesaural relations as an indicator of the structure of text\",\n        \"A corpus and evaluation framework for deeper understanding of commonsense stories, 2016\",\n        \"Structure here, bias there: Hierarchical generalization by jointly learning syntactic transformations\",\n        \"Applying the naïve Bayes classifier with kernel density estimation to the prediction of protein–protein interaction sites\",\n        \"Comprehending complex concepts\",\n        \"StereoSet: Measuring stereotypical bias in pretrained language models, 2020\",\n        \"Obtaining well calibrated probabilities using Bayesian binning\",\n        \"Stress test evaluation for natural language inference, 2018\",\n        \"More data can hurt for linear regression: Sample-wise double descent\",\n        \"The deep bootstrap framework: Good online learners are good offline generalizers\",\n        \"Deep double descent: Where bigger models and more data hurt\",\n        \"History of Kannada Literature: Readership Lectures\",\n        \"Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization\",\n        \"The Montreal Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment\",\n        \"Participatory research for low-resourced machine translation: A case study in African languages\",\n        \"Evaluating theory of mind in question answering\",\n        \"Posterior calibration and exploratory analysis for natural language processing models\",\n        \"Comparisons of sequence labeling algorithms and extensions\",\n        \"DisSent: Learning sentence representations from explicit discourse relations\",\n        \"Proverb comprehension as a function of reading proficiency in preadolescents\",\n        \"Generating natural anagrams: Towards language generation under hard combinatorial constraints\",\n        \"The chess transformer: Mastering play using generative language models, 2020\",\n        \"The things that we have to do\\\": Ethics and instrumentality in humanitarian communication\",\n        \"Show your work: Scratchpads for intermediate computation with language models\",\n        \"Klaus Oberauer and Oliver Wilhelm\",\n        \"Andrea Weidenfeld\",\n        \"Standard Occupational Classification of the People’s Republic of China\",\n        \"iSarcasm: A dataset of intended sarcasm\",\n        \"Type-and-example-directed program synthesis\",\n        \"Revisions that improve cohesion in multi-document summaries: A preliminary study\",\n        \"Can you trust your model’s uncertainty? Evaluating predictive uncertainty under dataset shift\",\n        \"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics\",\n        \"Sarcasm detection using context separators in online discourse, 2020\",\n        \"A review of speaker diarization: Recent advances with deep learning, 2021\",\n        \"BBQ: A hand-built bias benchmark for question answering, 2021\",\n        \"Are NLP models really able to solve simple math word problems?\",\n        \"Carbon emissions and large neural network training, 2021\",\n        \"Figurative language\",\n        \"Learning algorithms via neural logic networks, 2019\",\n        \"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\",\n        \"Causality: Models, Reasoning, and Inference\",\n        \"Deep and dense sarcasm detection, 2019\",\n        \"True few-shot learning with language models, 2021\",\n        \"Don’t patronize me! An annotated dataset with patronizing and condescending language towards vulnerable communities\",\n        \"Language models as knowledge bases?, 2019\",\n        \"Data cleaning: A case study with OpenRefine and Trifacta Wrangler\",\n        \"Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?, 2020\",\n        \"Fleet system, 2020\",\n        \"Distributed representations and nested compositional structure\",\n        \"Holographic Reduced Representations: Distributed Representation for Cognitive Structures\",\n        \"A general psychoevolutionary theory of emotion\",\n        \"Program synthesis from polymorphic refinement types\",\n        \"Generative language modeling for automated theorem proving, 2020\",\n        \"Knowledge derived from Wikipedia for computing semantic relatedness\",\n        \"SemEval 2015, task 7: Diachronic text evaluation\",\n        \"MELD: A multimodal multi-party dataset for emotion recognition in conversations\",\n        \"A transformer-based approach to irony and sarcasm detection\",\n        \"Few-shot instruction prompts for pretrained language models to detect social biases, 2021\",\n        \"Joking riddles: A developmental index of children’s humor\",\n        \"An analysis of the adaptation speed of causal models, 2020\",\n        \"The specification language TimeML, 2004\",\n        \"What are the most popular names chinese parents give their babies? a perspective from big data\",\n        \"TIMEDIAL: Temporal commonsense reasoning in dialog\",\n        \"Main trends in recent philosophy: Two dogmas of empiricism\",\n        \"The North American computational linguistics olympiad (NACLO)\",\n        \"Language models are unsupervised multitask learners\",\n        \"A word at a time: Computing word relatedness using temporal semantic analysis\",\n        \"Scaling language models: Methods, analysis & insights from training Gopher\",\n        \"Word sense disambiguation: A unified evaluation framework and empirical comparison\",\n        \"Resolving complex cases of definite pronouns: The Winograd schema challenge\",\n        \"A survey on computational metaphor processing\",\n        \"SQuAD: 100,000+ questions for machine comprehension of text\",\n        \"Know what you don’t know: Unanswerable questions for SQuAD\",\n        \"Med-BERT: Pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction\",\n        \"Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset\",\n        \"Folk psychology as a theory\",\n        \"CoQA: A conversational question answering challenge\",\n        \"Neural programmer-interpreters, 2015\",\n        \"Semi-supervised multitask learning for sequence labeling, 2017\",\n        \"Sentence-BERT: Sentence embeddings using siamese BERT-networks, 2019\",\n        \"Multi-prototype vector-space models of word meaning\",\n        \"Neural joke generation, 2017\",\n        \"Using information content to evaluate semantic similarity in a taxonomy\",\n        \"Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language\",\n        \"Choice of plausible alternatives: An evaluation of commonsense causal reasoning\",\n        \"A constructive prediction of the generalization error across scales, 2019\",\n        \"How well do NLI models capture verb veridicality?\",\n        \"Causation and causal inference in epidemiology\",\n        \"Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for NLP, 2021\",\n        \"XTREME-R: Towards more challenging and nuanced multilingual evaluation\",\n        \"Gender bias in coreference resolution\",\n        \"Comparing conventions\",\n        \"Number-space mapping in the newborn chick resembles humans’ mental number line\",\n        \"The child as hacker: Building more human-like models of learning\",\n        \"The child as hacker\",\n        \"and PDP Research Group (eds\",\n        \"A neural attention model for abstractive sentence summarization\",\n        \"Artificial Intelligence: A Modern Approach\",\n        \"How good is your tokenizer? On the monolingual performance of multilingual language models, 2020\",\n        \"PuzzLing Machines: A challenge on learning from small data\",\n        \"Robsut wrod reocginiton via semi-character recurrent neural network, 2016\",\n        \"WINOGRANDE: An adversarial Winograd schema challenge at scale\",\n        \"Automatic detection of satire in Twitter: A psycholinguistic-based approach\",\n        \"Masked language model scoring\",\n        \"Temporal reasoning in natural language processing: A survey\",\n        \"The New York Times annotated corpus LDC2008T19\",\n        \"Multitask prompted training enables zero-shot task generalization\",\n        \"A simple neural network module for relational reasoning, 2017\",\n        \"Symbolic behaviour in artificial intelligence, 2021\",\n        \"Social IQa: Commonsense reasoning about social interactions\",\n        \"Social bias frames: Reasoning about social and power implications of language\",\n        \"Analysing mathematical reasoning abilities of neural models, 2019\",\n        \"Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP, 2021\",\n        \"Language tasks and language games: On methodology in current natural language processing research, 2019\",\n        \"Get your vitamin C! Robust fact verification with contrastive evidence\",\n        \"Programming puzzles\",\n        \"Towards causal representation learning, 2021\",\n        \"Cryopreservation aims to engineer novel ways to freeze, store, and thaw organs\",\n        \"Get to the point: Summarization with pointer-generator networks\",\n        \"BLEURT: Learning robust metrics for text generation, 2020\",\n        \"Learning a SAT solver from single-bit supervision, 2018\",\n        \"Does he wink or does he nod? A challenging benchmark for evaluating word understanding of language models, 2021\",\n        \"Evaluating the ability of LSTMs to learn context-free grammars, 2018\",\n        \"How grammatical is character-level neural machine translation? Assessing MT quality with contrastive translation pairs, 2016\",\n        \"Revisiting low-resource neural machine translation: A case study\",\n        \"Neural machine translation of rare words with subword units, 2015\",\n        \"Diagram understanding in geometry questions\",\n        \"Counterfactual learning in networks: An empirical study of model dependence, 2021\",\n        \"All your questions answered\",\n        \"Inferring LISP programs from examples\",\n        \"Adafactor: Adaptive learning rates with sublinear memory cost, 2018\",\n        \"The woman worked as a babysitter: On biases in language generation\",\n        \"Neural logic reasoning, 2020\",\n        \"Expert, crowdsourced, and machine assessment of suicide risk via online postings\",\n        \"EmoTag1200: Understanding the association between emojis and emotions\",\n        \"Retrieval augmentation reduces hallucination in conversation, 2021\",\n        \"Automatic metaphor interpretation as a paraphrasing task\",\n        \"Metaphor corpus annotated for source-target domain mappings\",\n        \"Mining discourse markers for unsupervised sentence representation learning\",\n        \"DiscSense: Automated semantic analysis of discourse markers\",\n        \"Zero-shot recommendation as language modeling\",\n        \"SPRINGS: Prediction of protein-protein interaction sites using artificial neural networks\",\n        \"Predicting a correct program in programming by example\",\n        \"Transforming spreadsheet data types using examples\",\n        \"COM2SENSE: A commonsense reasoning benchmark with complementary sentences\",\n        \"CLUTRR: A diagnostic benchmark for inductive reasoning from text, 2019\",\n        \"Closing brackets with recurrent neural networks\",\n        \"The synthesis of LISP programs from examples: A survey\",\n        \"On the proper treatment of connectionism\",\n        \"Recursive deep models for semantic compositionality over a sentiment treebank\",\n        \"Release strategies and the social impacts of language models, 2019\",\n        \"Early detection of freeze damage in navel orange fruit using nondestructive low intensity ultrasound coupled with machine learning\",\n        \"Pragmatics, modularity and mind-reading\",\n        \"Patching gender: Non-binary utopias in HCI\",\n        \"Causation, Prediction, and Search\",\n        \"Inferring interpersonal relations in narrative summaries\",\n        \"Evaluating gender bias in machine translation\",\n        \"A Method for Linguistic Metaphor Identification: From MIP to MIPVU\",\n        \"Neural networks -- a model of boolean functions\",\n        \"Learning to summarize from human feedback, 2020\",\n        \"Watching a language model learning chess\",\n        \"Metaphoric paraphrase generation, 2020\",\n        \"Wikirelate! Computing semantic relatedness using Wikipedia\",\n        \"Prerequisite skills for reading comprehension: Multi-perspective analysis of MCTest datasets and systems\",\n        \"Executing instructions in situated collaborative interactions\",\n        \"Evolution and impact of bias in human and machine learning algorithm interaction\",\n        \"Sequence to sequence learning with neural networks, 2014\",\n        \"The bitter lesson\",\n        \"LSTM networks can perform dynamic counting\",\n        \"Memory-augmented recurrent neural networks can learn generalized Dyck languages, 2019b\",\n        \"ChePT -- applying deep neural transformer models to chess move prediction and self-commentary, 2021\",\n        \"You reap what you sow: On the challenges of bias evaluation under multilingual settings\",\n        \"oLMpics -- on what language model pre-training captures, 2019a\",\n        \"CommonsenseQA: A question answering challenge targeting commonsense knowledge\",\n        \"Understanding the capabilities, limitations, and societal impact of large language models, 2021\",\n        \"Learning to recommend quotes for writing\",\n        \"The teaching size: Computable teachers and learners for universal languages\",\n        \"Learning what makes a difference from counterfactual examples and gradient supervision, 2020\",\n        \"Analog retrieval by constraint satisfaction\",\n        \"Representing numbers in NLP: A survey and a vision\",\n        \"Learning to interpret natural language commands through human-robot dialog\",\n        \"Killing, letting die, and the trolley problem\",\n        \"LaMDA: Language models for dialog applications, 2022\",\n        \"Survey on collaborative filtering, content-based filtering and hybrid recommendation system\",\n        \"FEVER: A large-scale dataset for fact extraction and VERification\",\n        \"Metaphor paraphrasing and word sense disambiguation: Toward a new approach to automated metaphor, 2021\",\n        \"Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective\",\n        \"Learning chess blindfolded: Evaluating language models on state tracking, 2021\",\n        \"Correlation-based network analysis combined with machine learning techniques highlight the role of the gaba shunt in brachypodium sylvaticum freezing tolerance\",\n        \"Neural arithmetic logic units, 2018\",\n        \"Omiotis: A thesaurus-based measure of text relatedness\",\n        \"Text relatedness based on a word thesaurus\",\n        \"Metaphor detection with cross-lingual model transfer\",\n        \"Computing machinery and intelligence\",\n        \"Dating documents using graph convolution networks\",\n        \"Fine-grained temporal relation extraction, 2019\",\n        \"Temporal reasoning in natural language inference\",\n        \"Fill in the BLANC: Human-free quality estimation of document summaries\",\n        \"Attention is all you need\",\n        \"The use of spatial relations in referring expression generation\",\n        \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\",\n        \"Computational argumentation quality assessment in natural language\",\n        \"Does GPT-2 know your phone number?\",\n        \"GLUE: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"SuperGLUE: A stickier benchmark for general-purpose language understanding systems\",\n        \"Asking and answering questions to evaluate the factual consistency of summaries\",\n        \"GPT-J-6B: A 6 billion parameter autoregressive language model, May 2021\",\n        \"Learning to count objects with few exemplar annotations, 2019b\",\n        \"Continuity of topic, interaction, and query: Learning to quote in online conversations\",\n        \"SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver, 2019c\",\n        \"Learning language games through interaction\",\n        \"It’s going to be okay: Measuring access to support in online communities\",\n        \"TalkDown: A corpus for condescension detection in context\",\n        \"Who uses web search for what: And how\",\n        \"Wechsler Adult Intelligence Scale–Fourth Edition (WAIS–IV)\",\n        \"Finetuned language models are zero-shot learners, 2021\",\n        \"Chain of thought prompting elicits reasoning in large language models\",\n        \"Humor detection: A transformer gets the last laugh\",\n        \"Towards AI-complete question answering: A set of prerequisite toy tasks, 2015\",\n        \"Lexicosyntactic inference in neural models\",\n        \"Revisiting the strange stories: Revealing mentalizing impairments in autism\",\n        \"A broad-coverage challenge corpus for sentence understanding through inference\",\n        \"Cognitive and emotional demands of black humour processing: The role of intelligence, aggressiveness and mood\",\n        \"Language models are few-shot multilingual learners\",\n        \"Understanding natural language\",\n        \"Philosophical investigations\",\n        \"Some additional experiments extending the tech report ‘‘assessing BERT’s syntactic abilities’’ by Yoav Goldberg, 2019\",\n        \"Huggingface’s transformers: State-of-the-art natural language processing, 2019\",\n        \"An embedding method for unseen words considering contextual information and morphological information\",\n        \"Learning data transformation rules through examples: Preliminary results\",\n        \"Applying the transformer to character-level transduction, 2020\",\n        \"Google’s neural machine translation system: Bridging the gap between human and machine translation, 2016\",\n        \"The causal-neural connection: Expressiveness, learnability, and inference, 2021\",\n        \"On hallucination and predictive uncertainty in conditional language generation, 2021\",\n        \"Recipes for safety in open-domain chatbots, 2020a\",\n        \"AutoQA: From databases to QA semantic parsers with only synthetic training data\",\n        \"Incorporating latent meanings of morphological compositions to enhance word embeddings\",\n        \"ByT5: Towards a token-free future with pre-trained byte-to-byte models, 2021a\",\n        \"mT5: A massively multilingual pre-trained text-to-text transformer\",\n        \"Who’s to say what’s funny? A computer using language models and deep learning, that’s who!, 2017\",\n        \"Humor recognition and humor anchor extraction\",\n        \"Learning to prove theorems via interacting with proof assistants, 2019\",\n        \"Explainable artificial intelligence via Bayesian teaching\",\n        \"WikiWalk: Random walks on Wikipedia for semantic relatedness\",\n        \"Optimizing sentence modeling and selection for document summarization\",\n        \"Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task\",\n        \"CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases\",\n        \"SParC: Cross-domain semantic parsing in context\",\n        \"ReClor: A reading comprehension dataset requiring logical reasoning, 2020\",\n        \"Learning the Dyck language with attention-based Seq2Seq models\",\n        \"ActivityNet-QA: A dataset for understanding complex web videos via question answering, 2019d\",\n        \"Artificial intelligence as a positive and negative factor in global risk\",\n        \"Learning to execute, 2014\",\n        \"Figure me out: A gold standard dataset for metaphor interpretation\",\n        \"From recognition to cognition: Visual commonsense reasoning, 2018\",\n        \"HellaSwag: Can a machine really finish your sentence?\",\n        \"Defending against neural fake news, 2019b\",\n        \"The gap of semantic parsing: A survey on automatic math word problem solvers\",\n        \"Hurtful words: Quantifying biases in clinical contextual word embeddings\",\n        \"WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge\",\n        \"Multi-agent reinforcement learning: A selective overview of theories and algorithms, 2019a\",\n        \"Reasoning about goals, steps, and temporal ordering with WikiHow\",\n        \"Tweet sarcasm detection using deep neural network\",\n        \"ReCoRD: Bridging the gap between human and machine commonsense reading comprehension, 2018a\",\n        \"Irony detection via sentiment-based transfer learning\",\n        \"Learning to count objects in natural images for visual question answering, 2018b\",\n        \"When do you need billions of words of pretraining data?, 2020e\",\n        \"Gender bias in coreference resolution: Evaluation and debiasing methods\",\n        \"Calibrate before use: Improving few-shot performance of language models, 2021\",\n        \"Going on a vacation\\\" takes longer than \\\"going for a walk\\\": A study of temporal commonsense understanding, 2019\",\n        \"Detecting hallucinated content in conditional neural sequence generation, 2020\",\n        \"Learning to ask unanswerable questions for machine reading comprehension\",\n        \"Machine teaching: An inverse problem to machine learning and an approach toward optimal education\",\n        \"Designing effective sparse expert models, 2022\"\n    ],\n    \"reasoning\": \"Selected these 699 references because they represent foundational work and recent advances in Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}