{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 148 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Unsupervised Representation Learning for Time Series: A Review\n\nPaper Abstract: Abstract Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.\n\n{retrieved_text}\n\nGenerate exactly 148 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 148\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A systematic literature review of iot time series anomaly detection solutions\",\n        \"Anomaly detection for iot time-series data: A survey\",\n        \"A blockchain-based trusted upload scheme for the internet of things nodes\",\n        \"Automatic data imputation in time series processing using neural networks for industry and medical datasets\",\n        \"A case driven study of the use of time series classification for flexibility in industry 4.0\",\n        \"Financial time series forecasting with deep learning : A systematic literature review: 2005-2019\",\n        \"A review of deep learning methods for irregularly sampled medical time series data\",\n        \"An evaluation of time series summary statistics as features for clinical prediction tasks\",\n        \"Multivariate time series classification with WEASEL+MUSE\",\n        \"The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances\",\n        \"The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances\",\n        \"Deep learning for time series forecasting: A survey\",\n        \"A survey on semi-parametric machine learning technique for time series forecasting\",\n        \"An experimental review on deep learning architectures for time series forecasting\",\n        \"Caseload prediction using graphical evolutionary game theory and time series analysis\",\n        \"Learning representations for time series clustering\",\n        \"I-SEA: importance sampling and expected alignment-based deep distance metric learning for time series analysis and embedding\",\n        \"Energy-efficient sensory data collection based on spatiotemporal correlation in iot networks\",\n        \"Weakly-supervised sensor-based activity segmentation and recognition via learning from distributions\",\n        \"Deep learning for time series classification: a review\",\n        \"“XAI methods for neural time series classification: A brief review\",\n        \"“Approaches and applications of early classification of time series: A review\",\n        \"“A review on distance based time series classification\",\n        \"“Time series compression: a survey\",\n        \"“Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data\",\n        \"“A survey on time series data mining\",\n        \"“A review of unsupervised feature learning and deep learning for time-series modeling\",\n        \"“Towards learning disentangled representations for time series\",\n        \"“Online deep clustering for unsupervised representation learning\",\n        \"“Crowd evolution method based on intelligence level clustering\",\n        \"“A survey on knowledge integration techniques with artificial neural networks for seq-2-seq/time series models\",\n        \"“Timenet: Pre-trained deep recurrent neural network for time series classification\",\n        \"“Unsupervised representation learning for time series with temporal neighborhood coding\",\n        \"“Self-supervised ECG representation learning for emotion recognition\",\n        \"“Self-supervised learning of graph neural networks: A unified review\",\n        \"“Survey on self-supervised learning: Auxiliary pretext tasks and contrastive learning methods in imaging\",\n        \"“Contrastive self-supervised learning: A survey on different architectures\",\n        \"“What makes good contrastive learning on small-scale wearable-based tasks?” in KDD. ACM\",\n        \"“Self-supervised learning of pretext-invariant representations\",\n        \"“Revisiting self-supervised visual representation learning\",\n        \"“Self-training with noisy student improves imagenet classification\",\n        \"“Boosting contrastive self-supervised learning with false negative cancellation\",\n        \"“A simple framework for contrastive learning of visual representations\",\n        \"“Bootstrap your own latent - A new approach to self-supervised learning\",\n        \"“Momentum contrast for unsupervised visual representation learning\",\n        \"“Rspnet: Relative speed perception for unsupervised video representation learning\",\n        \"“TCLR: temporal contrastive learning for video representation\",\n        \"“Active contrastive learning of audio-visual video representations\",\n        \"“Spatiotemporal contrastive video representation learning\",\n        \"“Unsupervised scalable representation learning for multivariate time series\",\n        \"“Time-series representation learning via temporal and contextual contrasting\",\n        \"“Ts2vec: Towards universal representation of time series\",\n        \"“Timesnet: Temporal 2d-variation modeling for general time series analysis\",\n        \"“Understanding contrastive representation learning through alignment and uniformity on the hypersphere\",\n        \"“Big self-supervised models are strong semi-supervised learners\",\n        \"“Representation learning with contrastive predictive coding\",\n        \"“Exploring simple siamese representation learning\",\n        \"“Prototypical contrastive learning of unsupervised representations\",\n        \"“Unsupervised learning of visual features by contrasting cluster assignments\",\n        \"“MHCCL: masked hierarchical cluster-wise contrastive learning for multivariate time series\",\n        \"“Unsupervised feature extraction by time-contrastive learning and nonlinear ICA\",\n        \"“Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap\",\n        \"“Clustering-friendly representation learning via instance discrimination and feature decorrelation\",\n        \"“Clustering based contrastive learning for improving face representations\",\n        \"“Time series - A taxonomy based survey\",\n        \"“Clustering of time series data - a survey\",\n        \"“Deep clustering for unsupervised learning of visual features\",\n        \"“Representation learning with deconvolution for multivariate time series classification and visualization\",\n        \"“Time-series generative adversarial networks\",\n        \"“Timevae: A variational auto-encoder for multivariate time series generation\",\n        \"“Self-supervised representation learning from electroencephalography signals\",\n        \"“Efficient parameter-free clustering using first neighbor relations\",\n        \"“Hierarchically clustered representation learning\",\n        \"“Tensorized multi-view subspace representation learning\",\n        \"“A comprehensive survey on deep clustering: Taxonomy\",\n        \"“Learning representations for incomplete time series clustering\",\n        \"“Learning representations for time series clustering\",\n        \"“Unsupervised deep embedding for clustering analysis\",\n        \"“Towards the generalization of contrastive self-supervised learning\",\n        \"“Supporting clustering with contrastive learning\",\n        \"“Contrastive clustering\",\n        \"“SLIC: self-supervised learning with iterative clustering for human action videos\",\n        \"“Reducing the dimensionality of data with neural networks\",\n        \"“Analysis of different rnn autoencoder variants for time series classification and machine prognostics\",\n        \"“Improving clinical predictions through unsupervised time series representation learning\",\n        \"“Unsupervised representation learning for point clouds: A survey\",\n        \"P. Robles-Granda\",\n        \"“Cross reconstruction transformer for self-supervised time series representation learning\",\n        \"“Representation learning using a multi-branch transformer for industrial time series anomaly detection\",\n        \"“Simmtm: A simple pre-training framework for masked time-series modeling\",\n        \"“Preservational learning improves self-supervised medical image models by reconstructing diverse contexts\",\n        \"“Rdcgan: Unsupervised representation learning with regularized deep convolutional generative adversarial networks\",\n        \"“Generative adversarial networks in time series: A survey and taxonomy\",\n        \"“Large scale adversarial representation learning\",\n        \"“Generative adversarial networks: An overview\",\n        \"“Real-valued (medical) time series generation with recurrent conditional gans\",\n        \"“MAD-GAN: multivariate anomaly detection for time series data with generative adversarial networks\",\n        \"“Conditional GAN for timeseries generation\",\n        \"“Wasserstein GAN\",\n        \"“Conditional generative adversarial nets\",\n        \"“Out-of-distribution representation learning for time series classification\",\n        \"“wav2vec: Unsupervised pre-training for speech recognition\",\n        \"“Cass: A channel-aware self-supervised representation learning framework for multivariate time series classification\",\n        \"“A transformer-based framework for multivariate time series representation learning\",\n        \"“Timeclr: A self-supervised contrastive learning framework for univariate time series representation\",\n        \"“Mixing up contrastive learning: Self-supervised representation learning for time series\",\n        \"“Self-supervised speech representation learning: A review\",\n        \"“An empirical study of graph contrastive learning\",\n        \"“Unsupervised feature learning via non-parametric instance-level discrimination\",\n        \"“Contrasting the landscape of contrastive and non-contrastive learning\",\n        \"“Understanding self-supervised learning dynamics without contrastive pairs\",\n        \"“Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion\",\n        \"“Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting\",\n        \"“Contrastive learning inverts the data generating process\",\n        \"“Arcl: Enhancing contrastive learning with augmentation-robust representations\",\n        \"“Learning fair graph representations via automated data augmentations\",\n        \"“Time series data augmentation for deep learning: A survey\",\n        \"“An empirical survey of data augmentation for time series classification with neural networks\",\n        \"“Utilizing expert features for contrastive learning of time-series representations\",\n        \"“Towards domain-agnostic contrastive learning\",\n        \"“Contrastive learning with stronger augmentations\",\n        \"“What should not be contrastive in contrastive learning\",\n        \"“Autoaugment: Learning augmentation strategies from data\",\n        \"“Fast autoaugment\",\n        \"“Robust contrastive learning against noisy views\",\n        \"“Robust audio-visual instance discrimination\",\n        \"“What makes for good views for contrastive learning?” in NeurIPS\",\n        \"“Rethinking minimal sufficient representation in contrastive learning\",\n        \"“Semppl: Predicting pseudo-labels for better contrastive representations\",\n        \"“Soft neighbors are positive supporters in contrastive visual representation learning\",\n        \"“Neighborhood contrastive learning for novel class discovery\",\n        \"“With a little help from my friends: Nearest-neighbor contrastive learning of visual representations\",\n        \"A. Krishnamurthy\",\n        \"“Do more negative samples necessarily hurt in contrastive learning?” in ICML\",\n        \"“Learning robust and consistent time series representations: A dilated inception-based approach\",\n        \"“The trade-off between universality and label efficiency of representations from contrastive learning\",\n        \"“Simcse: Simple contrastive learning of sentence embeddings\",\n        \"“Few-shot classification with contrastive learning\",\n        \"“FSCE: few-shot object detection via contrastive proposal encoding\",\n        \"“Learning a few-shot embedding model with contrastive learning\",\n        \"“Reluplex: An efficient SMT solver for verifying deep neural networks\",\n        \"“Scaling provable adversarial defenses\",\n        \"“Robustness verification for contrastive learning\",\n        \"“Investigating why contrastive learning benefits robustness against label noise\",\n        \"“Deep residual learning for image recognition\",\n        \"“Inceptiontime: Finding alexnet for time series classification\",\n        \"“Randaugment: Practical automated data augmentation with a reduced search space\",\n        \"“Adaptive weighting scheme for automatic time-series data augmentation\"\n    ],\n    \"reasoning\": \"Selected these 148 references because they represent foundational work and recent advances in Unsupervised Representation Learning for Time Series: A Review, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}