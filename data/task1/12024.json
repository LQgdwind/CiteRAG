{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 225 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era\n\nPaper Abstract: Abstract. The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.\n\n{retrieved_text}\n\nGenerate exactly 225 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 225\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Image2stylegan: How to embed images into the stylegan latent space?\",\n        \"Image2stylegan++: How to edit the embedded images?\",\n        \"A-star: Test-time attention segregation and retention for text-to-image synthesis\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"COSMOS: catching out-of-context image misuse using self-supervised learning\",\n        \"Blended latent diffusion\",\n        \"Blended diffusion for text-driven editing of natural images\",\n        \"Openflamingo: An open-source framework for training large autoregressive vision-language models\",\n        \"Visual prompting via image inpainting\",\n        \"Text2live: Text-driven layered image and video editing\",\n        \"Label-Efficient Semantic Segmentation with Diffusion Models\",\n        \"Mip-nerf 360: Unbounded anti-aliased neural radiance fields\",\n        \"Seeing what a gan cannot generate\",\n        \"Shaping Images: Scholarly Perspectives on Image Manipulation\",\n        \"SonicDiffusion: Audio-Driven Image Generation and Editing with Pretrained Diffusion Models\",\n        \"iEdit: Localised Text-guided Image Editing with Weak Supervision\",\n        \"SpeechPainter: Text-conditioned Speech Inpainting\",\n        \"Fast approximate energy minimization via graph cuts\",\n        \"Instructpix2pix: Learning to follow image editing instructions\",\n        \"Language models are few-shot learners\",\n        \"Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing\",\n        \"Emerging properties in self-supervised vision transformers\",\n        \"Pix2video: Video editing using image diffusion\",\n        \"Stablevideo: Text-driven consistency-aware diffusion video editing\",\n        \"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors\",\n        \"Learning to Follow Object-Centric Image Editing Instructions Faithfully\",\n        \"LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models\",\n        \"Muse: Text-To-Image Generation via Masked Generative Transformers\",\n        \"Maskgit: Masked generative image transformer\",\n        \"A survey on evaluation of large language models\",\n        \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\",\n        \"Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers\",\n        \"GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting\",\n        \"Language-based image editing with recurrent attentive models\",\n        \"Photoverse: Tuning-free image customization with text-to-image diffusion models\",\n        \"Rethinking atrous convolution for semantic image segmentation\",\n        \"EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models\",\n        \"Learning to predict 3d objects with an interpolation-based differentiable renderer\",\n        \"Masked-attention mask transformer for universal image segmentation\",\n        \"Sequential attention GAN for interactive image editing\",\n        \"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models\",\n        \"Stargan: Unified generative adversarial networks for multi-domain image-to-image translation\",\n        \"Stargan v2: Diverse image synthesis for multiple domains\",\n        \"Empirical evaluation of gated recurrent neural networks on sequence modeling\",\n        \"Flexit: Towards flexible semantic image translation\",\n        \"Diffedit: Diffusion-based semantic image editing with mask guidance\",\n        \"Vqgan-clip: Open domain image generation and editing with natural language guidance\",\n        \"Emu: Enhancing image generation models using photogenic needles in a haystack\",\n        \"GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models\",\n        \"Giannis Daras and Alex Dimakis\",\n        \"Srirangaraj Setlur\",\n        \"Olivier Pietquin\",\n        \"Prafulla Dhariwal and Alexander Nichol\",\n        \"Christos Tzelepis\",\n        \"Cogview2: Faster and better text-to-image generation via hierarchical transformers\",\n        \"Alexander Kolesnikov\",\n        \"Samira Ebrahimi Kahou\",\n        \"Diffusion self-guidance for controllable image generation\",\n        \"Parmida Atighehchian\",\n        \"Vse++: Improving visual-semantic embeddings with hard negatives\",\n        \"An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control\",\n        \"Layoutgpt: Compositional visual planning and generation with large language models\",\n        \"William Yang Wang\",\n        \"Amit Haim Bermano\",\n        \"Encoder-based domain tuning for fast personalization of text-to-image models\",\n        \"Stylegan-nada: Clip-guided domain adaptation of image generators\",\n        \"Alexander Marcel Schubert\",\n        \"Yonatan Belinkov\",\n        \"Making llama see and draw with seed tokenizer\",\n        \"Instructdiffusion: A generalist modeling interface for vision tasks\",\n        \"Self-supervised pretraining of visual features in the wild\",\n        \"Natalia Neverova\",\n        \"Textbooks are all you need\",\n        \"Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation\",\n        \"InstructME: An instruction guided music edit and remix framework with latent diffusion models\",\n        \"Exploring Text-Guided Single Image Editing for Remote Sensing Images\",\n        \"Chartllama: A multimodal llm for chart understanding and generation\",\n        \"StyleBooth: Image Style Editing with Multimodal Instruction\",\n        \"Aleksander Holynski\",\n        \"Ganspace: Discovering interpretable gan controls\",\n        \"Imagen video: High definition video generation with diffusion models\",\n        \"Classifier-free diffusion guidance\",\n        \"Video diffusion models\",\n        \"Andrey Zhmoginov\",\n        \"Zeyuan Allen-Zhu\",\n        \"BlenderAlchemy: Editing 3D Graphics with Vision-Language Models\",\n        \"Mulan: A joint embedding of music audio and natural language\",\n        \"Noise2music: Text-conditioned music generation with diffusion models\",\n        \"Xun Huang and Serge Belongie\",\n        \"SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models\",\n        \"Vladimir Kulikov\",\n        \"Drew A Hudson and Christopher D Manning\",\n        \"H친kon Hukkel친s and Frank Lindseth\",\n        \"Globally and locally consistent image completion\",\n        \"Sebastian Borgeaud\",\n        \"Andrew Zisserman\",\n        \"Text2human: Text-driven controllable human image generation\",\n        \"Bharath Hariharan\",\n        \"Prateksha Udhayanan\",\n        \"Instruct 3d-to-3d: Text instruction guided 3d-to-3d conversion\",\n        \"Layered neural atlases for consistent video editing\",\n        \"Byoung-Tak Zhang\",\n        \"Spencer Whitehead\",\n        \"Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis\",\n        \"Melgan: Generative adversarial networks for conditional waveform synthesis\",\n        \"Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection\",\n        \"Lisa: Reasoning segmentation via large language model\",\n        \"S칮ren Kaae S칮nderby\",\n        \"Iulia Raluca Turc\",\n        \"Thomas Lukasiewicz\",\n        \"Lightweight generative adversarial networks for text-guided image manipulation\",\n        \"Text Guided Image Editing with Automatic Concept Locating and Forgetting\",\n        \"MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers\",\n        \"A Survey of Multimodal Composite Editing and Retrieval\",\n        \"InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following\",\n        \"Content-based controls for music large language modeling\",\n        \"Text-Driven Image Editing via Learnable Regions\",\n        \"Julian Eisenschlos\",\n        \"Francesco Piccinno\",\n        \"Antonio Torralba\",\n        \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n        \"Grounding dino: Marrying dino with grounded pre-training for open-set object detection\",\n        \"Vmamba: Visual state space model\",\n        \"Cristian Rodriguez-Opazo\",\n        \"Neural volumes: learning dynamic renderable volumes from images\",\n        \"Layered neural rendering for retiming people in video\",\n        \"Associating objects and their effects in video through coordination games\",\n        \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Timo L칲ddecke and Alexander Ecker\",\n        \"Latent consistency models: Synthesizing high-resolution images with few-step inference\",\n        \"Jenny Rose Finkel\",\n        \"InstructGIE: Towards Generalizable Image Editing\",\n        \"Nerf: Representing scenes as neural radiance fields for view synthesis\",\n        \"Hierarchical granularity transfer learning\",\n        \"Takeru Miyato and Masanori Koyama\",\n        \"Freeze the discriminator: a simple baseline for fine-tuning gans\",\n        \"Dreamix: Video diffusion models are general video editors\",\n        \"Seyed-Mohsen Moosavi-Dezfooli\",\n        \"Alberto Baldrati\",\n        \"Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation\",\n        \"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image Editing\",\n        \"SNeRF: stylized neural implicit representations for 3D scenes\",\n        \"Prafulla Dhariwal\",\n        \"Maria-Elena Nilsback and Andrew Zisserman\",\n        \"Christopher Olah\",\n        \"Carroll Wainwright\",\n        \"Jonathan T Barron\",\n        \"Krishna Kumar Singh\",\n        \"Fice: Text-conditioned fashion image editing with guided gan inversion\",\n        \"Infobench: Evaluating instruction following ability in large language models\",\n        \"Sandhini Agarwal\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Aristide Baratin\",\n        \"Hierarchical text-conditional image generation with clip latents\",\n        \"Aaron Van den Oord\",\n        \"Lajanugen Logeswaran\",\n        \"Grounded sam: Assembling open-world models for diverse visual tasks\",\n        \"Andr치s B칩dis-Szomor칰\",\n        \"Pivotal tuning for latent-based editing of real images\",\n        \"Eric Michael Smith\",\n        \"Andreas Blattmann\",\n        \"Michael Rubinstein\",\n        \"NeRF-Insert: 3D Local Editing with Multimodal Control Signals\",\n        \"Kamyar Ghasemipour\",\n        \"Image super-resolution via iterative refinement\",\n        \"Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing\",\n        \"Mitchell Wortsman\",\n        \"Laion-400m: Open dataset of clip-filtered 400 million image-text pairs\",\n        \"Shant Navasardyan\",\n        \"Sebastian Goodman\",\n        \"AudioScenic: Audio-Driven Video Scene Editing\",\n        \"Interfacegan: Interpreting the disentangled face representation learned by gans\",\n        \"Adversarial scene editing: Automatic object removal from weak supervision\",\n        \"Emu edit: Precise image editing via recognition and generation tasks\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"How to bridge the gap between modalities: A comprehensive survey on multimodal large language model\",\n        \"Generative modeling by estimating gradients of the data distribution\",\n        \"Jascha Sohl-Dickstein\",\n        \"Vincent Vanhoucke\",\n        \"Pratul Srinivasan\",\n        \"LocInv: Localization-aware Inversion for Text-Guided Image Editing\",\n        \"Emergent correspondence from image diffusion\",\n        \"Pontus Stenetorp\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Designing an encoder for stylegan image manipulation\",\n        \"Attacks and defenses for generative diffusion models: A comprehensive survey\",\n        \"Unitune: Text-driven image editing by fine tuning a diffusion model on a single image\",\n        \"Thuan Hoang Nguyen\",\n        \"Slidergan: Synthesizing expressive face images by sliding 3d blendshape parameters\",\n        \"P+limit-from洧녞P+: Extended Textual Conditioning in Text-to-Image Generation\",\n        \"Instructedit: Improving automatic masks for diffusion-based image editing with user instructions\",\n        \"Ceslee Montgomery\",\n        \"Zero-shot video editing using off-the-shelf image diffusion models\",\n        \"Ethical Dimensions of App Designs: A Case Study of Photo-and Video-Editing Apps\",\n        \"Xiaolong Wang and Abhinav Gupta\",\n        \"Videocomposer: Compositional video synthesis with motion controllability\",\n        \"Audit: Audio editing by following instructions with latent diffusion models\",\n        \"Chain-of-thought prompting elicits reasoning in large language models\",\n        \"Dialogpaint: A dialog-based image editing model\",\n        \"Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery\",\n        \"Fairy: Fast parallelized instruction-guided video-to-video synthesis\",\n        \"Visual chatgpt: Talking, drawing and editing with visual foundation models\",\n        \"The Fashion IQ Dataset: Retrieving Images by Combining Side Information and Relative Natural Language Feedback\",\n        \"Stan Weixian Lei\",\n        \"Ruslan Salakhudinov\",\n        \"Restart sampling for improving generative processes\",\n        \"Bikhyat Adhikari\",\n        \"Semantic hierarchy emerges in deep generative representations for scene synthesis\",\n        \"Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing\",\n        \"An Improved Baseline for Reasoning Segmentation with Large Language Model\",\n        \"Inst-inpaint: Instructing to remove objects with diffusion models\",\n        \"CoCa: Contrastive Captioners are Image-Text Foundation Models\",\n        \"Chatface: Chat-guided real face editing via diffusion latent space manipulation\",\n        \"Soundstream: An end-to-end neural audio codec\",\n        \"Multimodal image synthesis and editing: The generative AI era\",\n        \"Dimitris Metaxas\",\n        \"TIE: Revolutionizing Text-based Image Editing for Complex-Prompt Following and High-Fidelity Editing\",\n        \"Prospect: Prompt spectrum for attribute-aware personalization of diffusion models\",\n        \"Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning\",\n        \"EffiVED: Efficient Video Editing via Text-instruction Diffusion Models\",\n        \"InstructBrush: Learning Attention-based Instruction Optimization for Image Editing\",\n        \"Semantic understanding of scenes through the ade20k dataset\",\n        \"Philipp Kr칛henb칲hl\",\n        \"Oluwasanmi O Koyejo\",\n        \"Segment everything everywhere all at once\"\n    ],\n    \"reasoning\": \"Selected these 225 references because they represent foundational work and recent advances in Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}