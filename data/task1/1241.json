{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 59 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Accelerated Stochastic Gradient-free and Projection-free Methods\n\nPaper Abstract: Abstract In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique. Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of O‚Äã(d‚Äãn‚Äãœµ‚àí2)ùëÇùëëùëõsuperscriptitalic-œµ2O(d\\sqrt{n}\\epsilon^{-2}) for finding an œµitalic-œµ\\epsilon-stationary point in the finite-sum problem, which improves the exiting best result by a factor of O‚Äã(n‚Äãœµ‚àí2)ùëÇùëõsuperscriptitalic-œµ2O(\\sqrt{n}\\epsilon^{-2}), and has the function query complexity of O‚Äã(d‚Äãœµ‚àí3)ùëÇùëësuperscriptitalic-œµ3O(d\\epsilon^{-3}) in the stochastic problem, which improves the exiting best result by a factor of O‚Äã(œµ‚àí1)ùëÇsuperscriptitalic-œµ1O(\\epsilon^{-1}). To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique of STORM, which still reaches the function query complexity of O‚Äã(d‚Äãœµ‚àí3)ùëÇùëësuperscriptitalic-œµ3O(d\\epsilon^{-3}) in the stochastic problem without relying on any large batches. In particular, we present an accelerated framework of the Frank-Wolfe methods based on the proposed momentum accelerated technique. The extensive experimental results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.\n\n{retrieved_text}\n\nGenerate exactly 59 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 59\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Katyusha: The first direct acceleration of stochastic gradient methods\",\n        \"Variance reduction for faster non-convex optimization\",\n        \"Zeroth-order (non)-convex stochastic optimization via conditional gradient and gradient updates\",\n        \"A frank-wolfe framework for efficient and effective adversarial attacks\",\n        \"Curvilinear distance metric learning\",\n        \"Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization\",\n        \"Momentum-based variance reduction in non-convex sgd\",\n        \"Saga: A fast incremental gradient method with support for non-strongly convex composite objectives\",\n        \"Optimal rates for zero-order convex optimization: The power of two function evaluations\",\n        \"Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator\",\n        \"An algorithm for quadratic programming\",\n        \"On the information-adaptive variants of the admm: an iteration complexity perspective\",\n        \"Stochastic first-and zeroth-order methods for nonconvex stochastic programming\",\n        \"Accelerated gradient methods for nonconvex nonlinear and stochastic programming\",\n        \"Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization\",\n        \"Simple black-box adversarial attacks\",\n        \"Stochastic conditional gradient++\",\n        \"Projection-free online learning\",\n        \"Variance-reduced and projection-free stochastic optimization\",\n        \"Deep residual learning for image recognition\",\n        \"Zeroth-order stochastic alternating direction method of multipliers for nonconvex nonsmooth optimization\",\n        \"Nonconvex zeroth-order stochastic admm methods with lower function query complexity\",\n        \"Faster gradient-free proximal stochastic methods for nonconvex nonsmooth optimization\",\n        \"Momentum-based policy gradient methods\",\n        \"On the convergence properties of the projected gradient method for convex optimization\",\n        \"Revisiting frank-wolfe: Projection-free sparse convex optimization\",\n        \"Improved zeroth-order variance reduced algorithms and analysis for nonconvex optimization\",\n        \"Accelerating stochastic gradient descent using predictive variance reduction\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Convergence rate of frank-wolfe for non-convex objectives\",\n        \"On the global linear convergence of frank-wolfe optimization variants\",\n        \"Block-coordinate frank-wolfe optimization for structural svms\",\n        \"Conditional gradient sliding for convex optimization\",\n        \"Mnist handwritten digit database\",\n        \"Non-convex finite-sum optimization via scsg methods\",\n        \"An accelerated proximal coordinate gradient method\",\n        \"Zeroth-order online alternating direction method of multipliers: Convergence analysis and applications\",\n        \"Zeroth-order stochastic variance reduction for nonconvex optimization\",\n        \"Zeroth-order stochastic projected gradient descent for nonconvex optimization\",\n        \"Derivative-free methods for policy optimization: Guarantees for linear quadratic systems\",\n        \"Introductory Lectures on Convex Programming Volume I: Basic course\",\n        \"Random gradient-free minimization of convex functions\",\n        \"Sarah: A novel method for machine learning problems using stochastic recursive gradient\",\n        \"Stochastic recursive gradient algorithm for nonconvex optimization\",\n        \"Stochastic proximal gradient descent with acceleration techniques\",\n        \"Non-convex conditional gradient sliding\",\n        \"Stochastic variance reduction for nonconvex optimization\",\n        \"A stochastic gradient method with an exponential convergence _rate for finite training sets\",\n        \"Towards gradient free and projection free stochastic optimization\",\n        \"Complexities in projection-free stochastic non-convex minimization\",\n        \"A hybrid stochastic optimization framework for stochastic composite nonconvex optimization\",\n        \"Graphical models, exponential families, and variational inference\",\n        \"Spiderboost: A class of faster variance-reduced algorithms for nonconvex optimization\",\n        \"Spiderboost and momentum: Faster variance reduction algorithms\",\n        \"Stochastic recursive gradient-based methods for projection-free online learning\",\n        \"Frank-wolfe method is automatically adaptive to error bound condition\",\n        \"Conditional gradient methods via stochastic path-integrated differential estimator\",\n        \"One sample stochastic frank-wolfe\",\n        \"Stochastic nested variance reduction for nonconvex optimization\"\n    ],\n    \"reasoning\": \"Selected these 59 references because they represent foundational work and recent advances in Accelerated Stochastic Gradient-free and Projection-free Methods, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}