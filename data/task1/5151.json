{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 253 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Systems\n\nPaper Abstract: Abstract. Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures, after decades of research efforts. Near-bank PIM architectures place simple cores close to DRAM banks. Recent research demonstrates that they can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the Sparse Matrix Vector Multiplication (SpMV) kernel. SpMV has been characterized as one of the most significant and thoroughly studied scientific computation kernels. It is primarily a memory-bound kernel with intensive memory accesses due its algorithmic nature, the compressed matrix format used, and the sparsity patterns of the input matrices given. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make three key contributions. First, we implement a wide variety of software strategies on SpMV for a multithreaded PIM core, including (1) various compressed matrix formats, (2) load balancing schemes across parallel threads and (3) synchronization approaches, and characterize the computational limits of a single multithreaded PIM core. Second, we design various load balancing schemes across multiple PIM cores, and two types of data partitioning techniques to execute SpMV on thousands of PIM cores: (1) 1D-partitioned kernels to perform the complete SpMV computation only using PIM cores, and (2) 2D-partitioned kernels to strive a balance between computation and data transfer costs to PIM-enabled memory. Third, we compare SpMV execution on a real-world PIM system with 2528 PIM cores to an Intel Xeon CPU and an NVIDIA Tesla V100 GPU to study the performance and energy efficiency of various devices, i.e., both memory-centric PIM systems and conventional processor-centric CPU/GPU systems, for the SpMV kernel. SparseP software package provides 25 SpMV kernels for real PIM systems supporting the four most widely used compressed matrix formats, i.e., CSR, COO, BCSR and BCOO, and a wide range of data types. SparseP is publicly and freely available at https://github.com/CMU-SAFARI/SparseP. Our extensive evaluation using 26 matrices with various sparsity patterns provides new insights and recommendations for software designers and hardware architects to efficiently accelerate the SpMV kernel on real PIM systems.\n\n{retrieved_text}\n\nGenerate exactly 253 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 253\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Co-ML: A Case for Collaborative ML Acceleration Using near-Data Processing\",\n        \"Compute Caches\",\n        \"A Scalable Processing-In-Memory Accelerator for Parallel Graph Processing\",\n        \"PIM-Enabled Instructions: A Low-overhead, Locality-Aware Processing-in-Memory Architecture\",\n        \"A Sparse Matrix‐Vector Multiplication Method with Low Preprocessing Cost\",\n        \"In-Memory Low-Cost Bit-Serial Addition Using Commodity DRAM Technology\",\n        \"NetDIMM: Low-Latency Near-Memory Network Interface Architecture\",\n        \"Accelerating Genome Analysis: A Primer on an Ongoing Journey\",\n        \"Opportunities and Challenges of Performing Vector Operations inside the DRAM\",\n        \"CoCoPeLia: Communication-Computation Overlap Prediction for Efficient Linear Algebra on GPUs\",\n        \"GraphiDe: A Graph Processing Accelerator Leveraging In-DRAM-Computing\",\n        \"PANTHER: A Programmable Architecture for Neural Network Training Harnessing Energy-Efficient ReRAM\",\n        \"PUMA: A Programmable Ultra-Efficient Memristor-Based Accelerator for Machine Learning Inference\",\n        \"Copernicus: Characterizing the Performance Implications of Compression Formats Used in Sparse Workloads\",\n        \"ALRESCHA: A Lightweight Reconfigurable Sparse-Computation Accelerator\",\n        \"Chameleon: Versatile and PracticalNear-DRAM Acceleration Architecture for Large Memory Systems\",\n        \"Pattern-Based Sparse Matrix Representation for Memory-Efficient SMVM Kernels\",\n        \"Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors\",\n        \"Sparse Matrix Partitioning for Optimizing SpMV on CPU-GPU Heterogeneous Platforms\",\n        \"Sparse Matrix Format Selection with Multiclass SVM for SpMV on GPU\",\n        \"BestSF: A Sparse Meta-Format for Optimizing SpMV on GPU\",\n        \"SlimSell: A Vectorizable Graph Representation for Breadth-First Search\",\n        \"ReVAMP: ReRAM based VLIW Architecture for In-Memory Computing\",\n        \"Communication Balancing in Parallel Sparse Matrix-Vector Multiplication\",\n        \"Numerical Methods for Least Squares Problems\",\n        \"Sparse Matrix Solvers on the GPU: Conjugate Gradients and Multigrid\",\n        \"Scalable Matrix Computations on Large Scale-Free Graphs Using 2D Graph Partitioning\",\n        \"Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks\",\n        \"Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks\",\n        \"CoNDA: Efficient Cache Coherence Support for Near-data Accelerators\",\n        \"LazyPIM: An Efficient Cache Coherence Mechanism for Processing-in-Memory\",\n        \"DR-STRaNGe: End-to-End System Design for DRAM-based True Random Number Generators\",\n        \"Exact Sparse Matrix-Vector Multiplication on GPU’s and Multicore Architectures\",\n        \"The Anatomy of a Large-scale Hypertextual Web Search Engine\",\n        \"Parallel Sparse Matrix-Vector and Matrix-Transpose-Vector Multiplication Using Compressed Sparse Blocks\",\n        \"Reduced-Bandwidth Multithreaded Algorithms for Sparse Matrix-Vector Multiplication\",\n        \"Performance Analysis of Multicore and Multinodal Implementation of SpMV Operation\",\n        \"GenASM: A High-Performance, Low-Power Approximate String Matching Acceleration Framework for Genome Sequence Analysis\",\n        \"Hypergraph-Partitioning-Based Decomposition for Parallel Sparse-Matrix Vector Multiplication\",\n        \"Low-Cost Inter-Linked Subarrays (LISA): Enabling Fast Inter-Subarray Data Movement in DRAM\",\n        \"Improving DRAM performance by parallelizing refreshes with accesses\",\n        \"PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory\",\n        \"Accelerating Bandwidth-Bound Deep Learning Inference with Main-Memory Accelerators\",\n        \"Near Data Acceleration with Concurrent Host Access\",\n        \"McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge\",\n        \"Concurrent Data Structures with Near-Data-Processing: An Architecture-Aware Implementation\",\n        \"Model-Driven Autotuning of Sparse Matrix-Vector Multiply on GPUs\",\n        \"OpenMP: An Industry-Standard API for Shared-Memory Programming\",\n        \"GraphH: A Processing-in-Memory Architecture for Large-Scale Graph Processing\",\n        \"The University of Florida Sparse Matrix Collection\",\n        \"DrAcc: a DRAM based Accelerator for Accurate CNN Inference\",\n        \"The True Processing In Memory Accelerator\",\n        \"Andrew Lumsdaine\",\n        \"Alexandros Daglis\",\n        \"Arun Subramaniyan\",\n        \"Performance Analysis and Optimization of Sparse Matrix-Vector Multiplication on Intel Xeon Phi\",\n        \"Performance Analysis and Optimization of Sparse Matrix-Vector Multiplication on Modern Multi- and Many-Core Processors\",\n        \"BASMAT: Bottleneck-Aware Sparse Matrix-Vector Multiplication Auto-Tuning on GPGPUs\",\n        \"Conflict-Free Symmetric Sparse Matrix-Vector Multiplication on Multicore Architectures\",\n        \"Vasileios Karakasis\",\n        \"An Introduction to Algebraic Multigrid\",\n        \"hypre: A Library of High Performance Preconditioners\",\n        \"Katherine Morrow\",\n        \"Ricardo Quislant\",\n        \"Mohammad Sadrosadati\",\n        \"Valeria Cardellini\",\n        \"A High Memory Bandwidth FPGA Accelerator for Sparse Matrix-Vector Multiplication\",\n        \"Niladrish Chatterjee\",\n        \"Duality Cache for Data Parallel Acceleration\",\n        \"Anupam Chattopadhyay\",\n        \"Georgios Tziantzioulis\",\n        \"Practical Near-Data Processing for In-Memory Analytics Frameworks\",\n        \"HRL: Efficient and Flexible Reconfigurable Logic for Near-Data Processing\",\n        \"TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory\",\n        \"Amirali Boroumand\",\n        \"Combining HTM with RCU to Speed up Graph Coloring on Multicore Platforms\",\n        \"Nandita Vijaykumar\",\n        \"Christina Giannoula\",\n        \"Kornilios Kourtis\",\n        \"Johannes Langguth\",\n        \"Accelerating SpMV on FPGAs by Compressing Nonzero Values\",\n        \"PrIM Benchmark Suite\",\n        \"SparseP Software Package\",\n        \"iPIM: Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture\",\n        \"DLUX: A LUT-Based Near-Bank Accelerator for Data Center Deep Learning Training Workloads\",\n        \"A Performance Modeling and Optimization Analysis Tool for Sparse Matrix-Vector Multiplication on GPUs\",\n        \"Hsien-Hsin S. Lee\",\n        \"Burhan Ahmad Mudassar\",\n        \"João Dinis Ferreira\",\n        \"Shahar Kvatinsky\",\n        \"Hoang Anh Du Nguyen\",\n        \"EIE: Efficient Inference Engine on Compressed Deep Neural Network\",\n        \"Learning Both Weights and Connections for Efficient Neural Networks\",\n        \"Accelerating Dependent Cache Misses with an Enhanced Memory Controller\",\n        \"Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads\",\n        \"Hadi Asghari-Moghaddam\",\n        \"Jesmin Jahan Tithi\",\n        \"PASTIX: A High-Performance Parallel Direct Solver for Sparse Symmetric Positive Definite Systems\",\n        \"Aravind Sukumaran-Rajam\",\n        \"Auto-Tuning Strategies for Parallelizing Sparse Matrix-Vector (SpMV) Multiplication on Multi- and Many-Core Processors\",\n        \"Ramprakash Reddy Puli\",\n        \"Medal: Scalable DIMM based Near Data Processing Accelerator for DNA Seeding Algorithm\",\n        \"Centaur: A Chiplet-based\",\n        \"Optimizing Sparse Matrix Vector Multiplication on SMP\",\n        \"Katherine Yelick\",\n        \"Manoj Maramreddy\",\n        \"Intel Xeon Silver 4110 Processor\",\n        \"HBM DRAM Technology and Architecture\",\n        \"An Energy-Efficient VLSI Architecture for Pattern Recognition via Deep Embedding of Computation in SRAM\",\n        \"Perfomance Models for Blocked Sparse Matrix-Vector Multiplication Kernels\",\n        \"Cellular Logic-in-Memory Arrays\",\n        \"Semi-Two-Dimensional Partitioning for Parallel Sparse Matrix-Vector Multiplication\",\n        \"Benjamin Youngjae Cho\",\n        \"A Survey of Resource Management for Processing-In-Memory and Near-Memory Processing Architectures\",\n        \"Jukka K Nurminen\",\n        \"Sudhakar Yalamanchili\",\n        \"Damla Senol Cali\",\n        \"The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices\",\n        \"D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers with Low Latency and High Throughput\",\n        \"Functionality-Based Processing-in-Memory Accelerator for Deep Convolutional Neural Networks\",\n        \"A Case for Exploiting Subarray-Level Parallelism (SALP) in DRAM\",\n        \"Itpackv 2D User’s Guide\",\n        \"TACO: A Tool to Generate Tensor Algebra Kernels\",\n        \"Optimizing Sparse Matrix-Vector Multiplication Using Index and Value Compression\",\n        \"Sparse Matrix-vector Multiplication on GPGPU Clusters: A New Storage Format and a Scalable Implementation\",\n        \"Parallel Hash Table Design for NDP Systems\",\n        \"MAGIC—Memristor-Aided Logic\",\n        \"Memristor-Based IMPLY Logic Design Procedure\",\n        \"TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning\",\n        \"25.4 A 20nm 6GB Function-In-Memory DRAM\",\n        \"Evaluation Criteria for Sparse Matrix Storage Formats\",\n        \"Variant Calling Parallelization on Processor-in-Memory Architecture\",\n        \"Jean-Francois Roy\",\n        \"Gennady Pekhimenko\",\n        \"Adaptive Runtime Tuning of Parallel Sparse Matrix-Vector Multiplication on Distributed Memory Systems\",\n        \"Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology: Industrial Product\",\n        \"SNAP: A General-Purpose Network Analysis and Graph-Mining Library\",\n        \"Logic Operations in Memory Using a Memristive Akers Array\",\n        \"SMAT: An Input Adaptive Auto-Tuner for Sparse Matrix-Vector Multiplication\",\n        \"Performance Analysis and Optimization for SpMV on GPU Using Probabilistic Modeling\",\n        \"Krishna T Malladi\",\n        \"Pinatubo: A Processing-In-Memory Architecture for Bulk Bitwise Operations in Emerging Non-Volatile Memories\",\n        \"Design Space Exploration for Sparse Matrix-Matrix Multiplication on FPGAs\",\n        \"Amazon.com Recommendations: Item-to-Item Collaborative Filtering\",\n        \"Sparse Convolutional Neural Networks\",\n        \"Towards Efficient SpMV on Sunway Manycore Architectures\",\n        \"An Efficient GPU General Sparse Matrix-Matrix Multiplication for Irregular Data\",\n        \"CSR5: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication\",\n        \"Mikhail Smelyanskiy\",\n        \"Concurrent Data Structures for Near-Memory Computing\",\n        \"Mohammad Bakhshalipour\",\n        \"AdELL: An Adaptive Warp-Balancing ELL Format for Efficient Sparse Matrix-Vector Multiplication on GPUs\",\n        \"Efficient Multithreaded Untransposed\",\n        \"Salvatore Filippone\",\n        \"Optimizing Sparse Matrix–Vector Product Computations Using Unroll and Jam\",\n        \"Merge-Based Parallel Sparse Matrix-Vector Multiplication\",\n        \"Eriko Nurvitadhi\",\n        \"Automatically Tuning Sparse Matrix-Vector Multiplication for GPU Architectures\",\n        \"Maleen Abeydeera\",\n        \"Enabling Practical Processing In and Near Memory for Data-Intensive Computing\",\n        \"Processing Data Where It Makes Sense: Enabling In-Memory Computation\",\n        \"A Modern Primer on Processing in Memory\",\n        \"OrderLight: Lightweight Memory-Ordering Primitive for Efficient Fine-Grained PIM Computations\",\n        \"GraphPIM: Enabling Instruction-Level PIM Offloading in Graph Computing Frameworks\",\n        \"Active Memory Cube: A Processing-in-Memory Architecture for Exascale Systems\",\n        \"Variable-Sized Blocks for Locality-Aware SpMV\",\n        \"Load-Balancing in Sparse Matrix-Vector Multiplication\",\n        \"Dheevatsa Mudigere\",\n        \"Processing in Storage Class Memory\",\n        \"TileSpMV: A Tiled Algorithm for Sparse Matrix-Vector Multiplication on GPUs\",\n        \"A Sparse Matrix Vector Multiply Accelerator for Support Vector Machine\",\n        \"Ganesh Venkatesh\",\n        \"NVIDIA System Management Interface Program\",\n        \"NVIDIA Tesla V100 GPU Architecture\",\n        \"Konstantinos Kanellopoulos\",\n        \"A. Giray Yağlikçi\",\n        \"Effects of Ordering Strategies and Programming Paradigms on Sparse Matrix Computations\",\n        \"Scalability of Hybrid Sparse Matrix Dense Vector (SpMV) Multiplication\",\n        \"Jonathan Beaumont\",\n        \"Antonio Puglielli\",\n        \"TRiM: Enhancing Processor-Memory Interfaces with Scalable Tensor Reduction in Memory\",\n        \"A Medium-Grain Method for Fast 2D Bipartitioning of Sparse Matrices\",\n        \"Sparse Matrix-Vector Multiplication on the Single-Chip Cloud Computer Many-Core Processor\",\n        \"Marcos Fernández\",\n        \"Improving Performance of Sparse Matrix-Vector Multiplication\",\n        \"A Survey of Indexing Techniques for Sparse Matrices\",\n        \"Rajeev Balasubramonian\",\n        \"Sudarshan Srinivasan\",\n        \"Mehdi Modarressi\",\n        \"Sparso: Context-Driven Optimizations of Sparse Linear Algebra\",\n        \"Krylov Subspace Methods on Supercomputers\",\n        \"Efficient SpMV Operation for Large and Highly Sparse Matrices Using Scalable Multi-Way Merge Parallelization\",\n        \"Sunder: Enabling Low-Overhead and Scalable Near-Data Pattern Matching Acceleration\",\n        \"List-of-list Sparse Matrix\",\n        \"Louis-Noel Pouchet\",\n        \"Scan Primitives for GPU Computing\",\n        \"Rachata Ausavarungnirun\",\n        \"Simple Operations in Memory to Reduce Data Movement\",\n        \"In-DRAM Bulk Bitwise Execution Engine\",\n        \"Naveen Muralimanohar\",\n        \"McDRAM: Low Latency and Energy-Efficient Matrix Computations in DRAM\",\n        \"Ahsan Javed Awan\",\n        \"Dionysios Diamantopoulos\",\n        \"Giovanni Mariani\",\n        \"6 New Facts About Facebook\",\n        \"GraphR: Accelerating Graph Processing Using ReRAM\",\n        \"Globally Homogeneous\",\n        \"A Logic-in-Memory Computer\",\n        \"ClSpMV: A Cross-Platform OpenCL SpMV Framework on GPUs\",\n        \"ABC-DIMM: Alleviating the Bottleneck of Communication in DIMM-based Near-Memory Processing with Inter-DIMM Broadcast\",\n        \"Optimizing SpMV for Diagonal Sparse Matrices on GPU\",\n        \"Design and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture\",\n        \"Huynh Phung Huyng\",\n        \"Characterizing the Behavior of Sparse Algorithms on Caches\",\n        \"Improving the Memory-System Performance of Sparse-Matrix Vector Multiplication\",\n        \"An Energy Efficient Column-Major Backend for FPGA SpMV Accelerators\",\n        \"Introduction to UPMEM PIM. Processing-in-memory (PIM) on DRAM Accelerator (White Paper)\",\n        \"UPMEM User Manual. Version 2021.3\",\n        \"A Two-Dimensional Data Distribution Method for Parallel Sparse Matrix-Vector Multiplication\",\n        \"A New Approach for Sparse Matrix Vector Product on NVIDIA GPUs\",\n        \"Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply\",\n        \"OSKI: A Library of Automatically Tuned Sparse Matrix Kernels\",\n        \"Automatic Performance Tuning of Sparse Matrix Kernels\",\n        \"Fast Sparse Matrix-Vector Multiplication by Exploiting Variable Block Structure\",\n        \"On Improving the Performance of Sparse Matrix-Vector Multiplication\",\n        \"Accelerating Sparse Matrix Computations via Data Compression\",\n        \"Sieve: Scalable In-situ DRAM-based Accelerator Designs for Massively Parallel k-mer Matching\",\n        \"Efficient PageRank and SpMV Computation on AMD GPUs\",\n        \"Xiaobo Sharon Hu\",\n        \"CASpMV: A Customized and Accelerative SpMV Framework for the Sunway TaihuLight\",\n        \"CVR: Efficient Vectorization of SpMV on X86 Processors\",\n        \"SpaceA: Sparse Matrix Vector Multiplication on Processing-in-Memory Accelerator\",\n        \"ELP2IM: Efficient and Low Power Bitwise Operation Processing in DRAM\",\n        \"YaSpMV: Yet Another SpMV Framework on GPUs\",\n        \"A Hybrid Computing Method of SpMV on CPU–GPU Heterogeneous Computing Systems\",\n        \"Optimization of Quasi-Diagonal Matrix-Vector Multiplication on GPU\",\n        \"Performance Optimization Using Partitioned SpMV on GPUs and Multicore CPUs\",\n        \"Srinivasan Parthasarathy\",\n        \"Pejman Lotfi-Kamran\",\n        \"Mahdi Nazm Bojnordi\",\n        \"Alexander Lyashevsky\",\n        \"Gamma: Leveraging Gustavson’s Algorithm to Accelerate Sparse Matrix Multiplication\",\n        \"Christos Kozyrakis\",\n        \"Cambricon-X: An Accelerator for Sparse Neural Networks\",\n        \"SpArch: Efficient Architecture for Sparse Matrix Multiplication\",\n        \"Exploring Better Speculation and Data Locality in Sparse Matrix-Vector Multiplication on Intel Xeon\",\n        \"Bridging the Gap between Deep Learning and Sparse Matrix Format Selection\",\n        \"Overhead-Conscious Format Selection for SpMV-Based Applications\",\n        \"RRAM-based TCAMs for Pattern Search\",\n        \"Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach\",\n        \"Accelerating Sparse Matrix-Matrix Multiplication with 3D-Stacked Logic-In-Memory Hardware\",\n        \"GraphQ: Scalable PIM-Based Graph Processing\",\n        \"Massively Parallel Skyline Computation for Processing-in-Memory Architectures\"\n    ],\n    \"reasoning\": \"Selected these 253 references because they represent foundational work and recent advances in SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Systems, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}