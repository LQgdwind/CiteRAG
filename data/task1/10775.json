{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs\n\nPaper Abstract: Abstract Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization problem and employ a standardized set of genetic algorithms for generating eligible prompts. Compared to the baseline AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4% higher without ONION defense and 85.2% higher with ONION defense. SMJ’s better performance in all three semantic meaningfulness metrics of Jailbreak Prompt, Similarity, and Outlier, also means that SMJ is resistant to defenses that use those metrics as thresholds.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\",\n        \"Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023\",\n        \"Qlora: Efficient finetuning of quantized llms\",\n        \"Generative language models and automated influence operations: Emerging threats and potential mitigations\",\n        \"Large language models can be used to effectively scale spear phishing campaigns\",\n        \"Generating transferable 3d adversarial point cloud via random perturbation factorization\",\n        \"Catastrophic jailbreak of open-source llms via exploiting generation\",\n        \"Adversarial example generation with syntactically controlled paraphrase networks\",\n        \"Baseline defenses for adversarial attacks against aligned language models\",\n        \"Exploiting programmatic behavior of llms: Dual-use through standard security attacks\",\n        \"Fine-tuned deberta-v3 for prompt injection detection, 2023\",\n        \"Multi-step jailbreaking privacy attacks on chatgpt\",\n        \"Privacy-enhancing face obfuscation guided by semantic-aware attribution maps\",\n        \"Exploring inconsistent knowledge distillation for object detection with data augmentation\",\n        \"Efficient adversarial attacks for visual object tracking\",\n        \"Generate more imperceptible adversarial examples for object detection\",\n        \"A large-scale multiple-objective method for black-box attack against object detection\",\n        \"Imitated detectors: Stealing knowledge of black-box object detectors\",\n        \"Parallel rectangle flip attack: A query-based black-box attack against object detection\",\n        \"Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning\",\n        \"Perceptual-sensitive gan for generating adversarial patches\",\n        \"Spatiotemporal attacks for embodied agents\",\n        \"Bias-based universal adversarial patch attack for automatic check-out\",\n        \"X-adv: Physical adversarial object attacks against x-ray prohibited item detection\",\n        \"Exploring the relationship between architectural design and adversarially robust generalization\",\n        \"Pre-trained trojan attacks for visual recognition\",\n        \"Improving adversarial transferability by stable diffusion\",\n        \"Autodan: Generating stealthy jailbreak prompts on aligned large language models\",\n        \"Llama 2 - acceptable use policy - meta ai\",\n        \"Introducing chatgpt\",\n        \"Usage policies\",\n        \"Moderation\",\n        \"Training language models to follow instructions with human feedback\",\n        \"To chatgpt, or not to chatgpt: That is the question!\",\n        \"Onion: A simple and effective defense against textual backdoor attacks\",\n        \"Turn the combination lock: Learnable textual backdoor attacks via word substitution\",\n        \"Semantic textual similarity\",\n        \"Pretrained models\",\n        \"” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models\",\n        \"Improving robust fariness via balance adversarial training\",\n        \"Llama 2: Open foundation and fine-tuned chat models, 2023\",\n        \"Dual attention suppression attack: Generate adversarial camouflage in physical world\",\n        \"Adaptive perturbation generation for multiple backdoors detection\",\n        \"Transferable adversarial attacks for image and video object detection\",\n        \"Transformers: State-of-the-art natural language processing\",\n        \"Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\",\n        \"Judging llm-as-a-judge with mt-bench and chatbot arena\",\n        \"Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions\",\n        \"Universal and transferable adversarial attacks on aligned language models\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}