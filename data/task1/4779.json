{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search\n\nPaper Abstract: Abstract The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists. Experiment results demonstrate that SPANN is 2Ã—\\times faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality 90%percent9090\\% with same memory cost in three billion-scale datasets. It can reach 90%percent9090\\% recall@1 and recall@10 in just around one millisecond with only 32GB memory cost. Code is available at: https://github.com/microsoft/SPTAG.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Benchmarking nearest neighbors.\",\n        \"Billion-scale anns benchmarks.\",\n        \"Datasets for approximate nearest neighbor search.\",\n        \"Lightweight approximate nearest neighbor algorithm (n2).\",\n        \"Nsg : Navigating spread-out graph for approximate nearest neighbor search.\",\n        \"Spacev1b: A billion-scale vector dataset for text descriptors.\",\n        \"The inverted multi-index.\",\n        \"Efficient indexing of billion-scale datasets of deep descriptors.\",\n        \"Revisiting the inverted indices for billion-scale approximate nearest neighbors.\",\n        \"Shape indexing using approximate nearest-neighbour search in high-dimensional spaces.\",\n        \"Multidimensional binary search trees used for associative searching.\",\n        \"SPTAG: A library for fast approximate nearest neighbor search,\",\n        \"Random projection trees and low dimensional manifolds.\",\n        \"Locality-sensitive hashing scheme based on p-stable distributions.\",\n        \"Pyramid: A general framework for distributed similarity search on large-scale datasets.\",\n        \"Efficient k-nearest neighbor graph construction for generic similarity measures.\",\n        \"Optimal analysis of best fit bin packing.\",\n        \"An Algorithm for Finding Best Matches in Logarithmic Expected Time.\",\n        \"Fast approximate nearest neighbor search with the navigating spreading-out graphs.\",\n        \"Accelerating large-scale inference with anisotropic vector quantization.\",\n        \"Fast approximate nearest-neighbor search with k-nearest neighbor graph.\",\n        \"Pruned bi-directed k-nearest neighbor graph for proximity search.\",\n        \"Optimization of indexing based on k-nearest neighbor graph for proximity search in high-dimensional data.\",\n        \"Fast image search for learned metrics.\",\n        \"Product quantization for nearest neighbor search.\",\n        \"Searching in one billion vectors: re-rank with source coding.\",\n        \"Billion-scale similarity search with gpus.\",\n        \"Locally optimized product quantization for approximate nearest neighbor search.\",\n        \"Learning to hash with binary reconstructive embeddings.\",\n        \"Fast clustering with flexible balance constraints.\",\n        \"An investigation of practical approximate nearest neighbor algorithms.\",\n        \"Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.\",\n        \"Scalable Nearest Neighbour Algorithms for High Dimensional Data.\",\n        \"Scalable recognition with a vocabulary tree.\",\n        \"Graph-based nearest neighbor search: From practice to theory.\",\n        \"Hm-ann: Efficient billion-point nearest neighbor search on heterogeneous memory.\",\n        \"Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips).\",\n        \"Refinements to nearest-neighbor searching in k-dimensional trees.\",\n        \"Diskann: Fast accurate billion-point nearest neighbor search on a single node.\",\n        \"Singleton indexes for nearest neighbor search.\",\n        \"The relative neighbourhood graph of a finite planar set.\",\n        \"Scalable k-nn graph construction for visual descriptors.\",\n        \"Query-driven iterated neighborhood graph search for large scale indexing.\",\n        \"Trinary-projection trees for approximate nearest neighbor search.\",\n        \"Composite quantization.\",\n        \"A survey on learning to hash.\",\n        \"Spectral hashing.\",\n        \"Complementary hashing for approximate nearest neighbor search.\",\n        \"Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces.\",\n        \"Grip: Multi-store capacity-optimized high-performance nearest neighbor search for vector search engine.\",\n        \"Composite quantization for approximate nearest neighbor search.\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}