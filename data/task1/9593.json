{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 171 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods\n\nPaper Abstract: Abstract Representation learning refers to the problem of transforming high-dimensional and possibly noisy raw inputs to compact and informative representations of the data. Unlike traditional hand-engineered features, representation learning can automatically capture the underlying structure and patterns hidden in the surface features, which can be useful for improving performance on downstream tasks and improving generalization. This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. This problem is challenging because the sequences can be very diverse. For example, if the series is a continuous signal like a speech recording, sources of variation include (but are not limited to) linguistic material, different speakers, and distinct recording environments. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. In this thesis, I focus on speech data, but the methods can also be applied to other domains. In Chapter 3, I systematically study autoencoders for acoustic representation learning, and I show that speech recognition can benefit from the representations learned by a variational autoencoder (VAE) with a feedforward encoder when the VAE is trained using a much larger unlabeled dataset. To the best of my knowledge, this is the first work to experimentally show that VAEs learn representations that significantly improve speech recognition and outperform autoencoding approaches that do not use variational inference. I also explore in depth what makes VAEs powerful for representation learning. I find that their KL divergence term provides regularization effects which may play a critical role in representation learning. In Chapter 4, I contribute new techniques for variational representation learning models in a multi-view learning setting. Specifically, I extend variational canonical correlation analysis (VCCA), an unsupervised multi-view representation learning method, with more informative sample-specific priors. My proposed extension of VCCA learns representations that improve speech recognition performance, when we have access to multi-view training data consisting of audio paired with articulatory measurements. I also explore two specific problems in multi-view learning — cross-domain multi-view learning and label embedding. I investigate learning an acoustic encoder using an additional modality (view) when we don’t have access to paired in-domain multi-view data. I explore multiple transfer learning techniques and optimization techniques to exploit the learned acoustic-articulatory mapping in the dataset where acoustic-articulatory pairs are accessible. I show that such acoustic-articulatory mapping information can improve speech recognition, especially with small training datasets. For label embedding, I directly treat the label as a second view and use the structural information hidden in the label to assist the representation learning. In Chapter 5, I study multitask recurrent representation learning, combining supervised and unsupervised losses, and present several technical improvements to variational sequential models. Unlike other variational sequential models developed prior to my work that focus on generation tasks and typically use unidirectional recurrent layers as their encoders, I propose to use variational bidirectional recurrent layers as an encoder to learn better-performing representations for downstream tasks. I also propose to factor the representation into a discriminative component and a reconstruction-specific component. I also propose to update the priors dynamically, discuss the resulting benefits and limitations in depth, and propose potential extensions. My experimental results show that the proposed multitask representation learning framework works well for speech recognition, and also text tasks like entity recognition and text chunking. In Chapter 6 I focus on unsupervised sequential representation learning. I compare multiple unsupervised learning approaches, including autoencoding, contrastive learning, and masked reconstruction. I also contribute a new approach, multi-view masked reconstruction. My experimental study shows that making representations invariant to different domains and robust to distortions can improve speech recognition performance and generalization to unseen data. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; Instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. I believe some of the findings in this thesis can still be useful today, and in Chapter 7 I conclude those findings and discuss potential future work.\n\n{retrieved_text}\n\nGenerate exactly 171 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 171\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Tensorflow: A system for large-scale machine learning\",\n        \"What regularized auto-encoders learn from the data-generating distribution\",\n        \"A comparison of unidirectional and bidirectional lstm networks for human activity recognition\",\n        \"Deep variational information bottleneck\",\n        \"Fixing a broken elbo\",\n        \"Linear and nonlinear ica based on mutual information - the misep method\",\n        \"Deep canonical correlation analysis\",\n        \"Black box variational inference for state space models\",\n        \"Bidirectional recurrent neural network language models for automatic speech recognition\",\n        \"Multi-view cca-based acoustic features for phonetic recognition across speakers and domains\",\n        \"A probabilistic interpretation of canonical correlation analysis\",\n        \"vq-wav2vec: Self-supervised learning of discrete speech representations\",\n        \"wav2vec 2.0: A framework for self-supervised learning of speech representations\",\n        \"Unsupervised speech recognition\",\n        \"End-to-end attention-based large vocabulary speech recognition\",\n        \"Contrastively disentangled sequential variational autoencoder\",\n        \"Self-organizing neural network that discovers surfaces in random-dot stereograms\",\n        \"Mine: mutual information neural estimation\",\n        \"An information-maximization approach to blind separation and blind deconvolution\",\n        \"Greedy layer-wise training of deep networks\",\n        \"Representation learning: A review and new perspectives\",\n        \"Estimating or propagating gradients through stochastic neurons for conditional computation\",\n        \"Fully-convolutional siamese networks for object tracking\",\n        \"Training with noise is equivalent to tikhonov regularization\",\n        \"Generating sentences from a continuous space\",\n        \"Signature verification using a \\\"siamese\\\" time delay neural network\",\n        \"Language models are few-shot learners\",\n        \"Deep clustering for unsupervised learning of visual features\",\n        \"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition\",\n        \"Variational sequential labelers for semi-supervised learning\",\n        \"A simple framework for contrastive learning of visual representations\",\n        \"Big self-supervised models are strong semi-supervised learners\",\n        \"Beyond triplet loss: a deep quadruplet network for person re-identification\",\n        \"Semi-supervised asr by end-to-end self-training\",\n        \"On the properties of neural machine translation: Encoder-decoder approaches\",\n        \"Unsupervised speech representation learning using wavenet autoencoders\",\n        \"Gated feedback recurrent neural networks\",\n        \"A recurrent latent variable model for sequential data\",\n        \"An unsupervised autoregressive model for speech representation learning\",\n        \"Deep bidirectional and unidirectional lstm recurrent neural network for network-wide traffic speed prediction\",\n        \"Improving deep neural networks for lvcsr using rectified linear units and dropout\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Unsupervised learning of disentangled representations from video\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Nice: Non-linear independent components estimation\",\n        \"Density estimation using real nvp\",\n        \"Tutorial on variational autoencoders\",\n        \"The zero resource speech challenge 2019: Tts without t\",\n        \"Variational recurrent auto-encoders\",\n        \"Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition\",\n        \"Sequential neural models with stochastic layers\",\n        \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning\",\n        \"A theoretically grounded application of dropout in recurrent neural networks\",\n        \"William M Fisher\",\n        \"Understanding the difficulty of training deep feedforward neural networks\",\n        \"Deep learning, volume 1\",\n        \"Generative adversarial nets\",\n        \"Z-forcing: Training stochastic recurrent networks\",\n        \"Sequence transduction with recurrent neural networks\",\n        \"Bidirectional lstm networks for improved phoneme classification and recognition\",\n        \"Hierarchical subsampling networks\",\n        \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\",\n        \"Bootstrap your own latent: A new approach to self-supervised learning\",\n        \"Unsupervised learning of spoken language with visual context\",\n        \"Lagging inference networks and posterior collapse in variational autoencoders\",\n        \"Multi-view recurrent neural acoustic word embeddings\",\n        \"Multilingual models for compositional distributed semantics\",\n        \"Reducing the dimensionality of data with neural networks\",\n        \"Learning deep representations by mutual information estimation and maximization\",\n        \"Long short-term memory\",\n        \"Deep metric learning using triplet network\",\n        \"Elbo surgery: yet another way to carve up the variational evidence lower bound\",\n        \"Learning consistent deep generative models from sparse data via prediction constraints\",\n        \"Relations between two sets of variates\",\n        \"Unsupervised learning of disentangled and interpretable representations from sequential data\",\n        \"Hierarchical generative modeling for controllable speech synthesis\",\n        \"Toward controlled generation of text\",\n        \"Nonlinear independent component analysis: Existence and uniqueness results\",\n        \"Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models\",\n        \"Categorical reparameterization with gumbel-softmax\",\n        \"Principal component analysis\",\n        \"A segmental framework for fully-unsupervised large-vocabulary speech recognition\",\n        \"Semi-supervised end-to-end speech recognition\",\n        \"Semi-supervised end-to-end speech recognition using text-to-speech and autoencoders\",\n        \"Joint ctc-attention based end-to-end speech recognition using multi-task learning\",\n        \"Adam: A method for stochastic optimization\",\n        \"Auto-encoding variational bayes\",\n        \"Semi-supervised learning with deep generative models\",\n        \"Variational dropout and the local reparameterization trick\",\n        \"Improved variational inference with inverse autoregressive flow\",\n        \"Glow: Generative flow with invertible 1x1 convolutions\",\n        \"Deep kalman filters\",\n        \"Temporal ensembling for semi-supervised learning\",\n        \"Deep learning under privileged information using heteroscedastic dropout\",\n        \"Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks\",\n        \"On large-vocabulary speaker-independent continuous speech recognition\",\n        \"Prototypical contrastive learning of unsupervised representations\",\n        \"Disentangled sequential autoencoder\",\n        \"Deep contextualized acoustic representations for semi-supervised speech recognition\",\n        \"Self-organization in a perceptual network\",\n        \"Autoencoder for words\",\n        \"Large-margin softmax loss for convolutional neural networks\",\n        \"Phoneme recognition on the timit database\",\n        \"Unsupervised data augmentation with naive augmentation and without unlabeled data\",\n        \"Speech enhancement based on deep denoising autoencoder\",\n        \"Rectifier nonlinearities improve neural network acoustic models\",\n        \"The concrete distribution: A continuous relaxation of discrete random variables\",\n        \"Adversarial autoencoders\",\n        \"Stacked convolutional auto-encoders for hierarchical feature extraction\",\n        \"Neural variational inference for text processing\",\n        \"Virtual adversarial training: A regularization method for supervised and semi-supervised learning\",\n        \"Regularizing deep networks by modeling and predicting label structure\",\n        \"Sensory coding: information maximization and redundancy reduction\",\n        \"Learning disentangled representations with semi-supervised deep generative models\",\n        \"Speaker-adaptation for hybrid hmm-ann continuous speech recognition system\",\n        \"Sparse autoencoder\",\n        \"f-gan: Training generative neural samplers using variational divergence minimization\",\n        \"Representation learning with contrastive predictive coding\",\n        \"Librispeech: an asr corpus based on public domain audio books\",\n        \"Specaugment: A simple data augmentation method for automatic speech recognition\",\n        \"Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks\",\n        \"Context encoders: Feature learning by inpainting\",\n        \"The design for the wall street journal-based csr corpus\",\n        \"Glove: Global vectors for word representation\",\n        \"Semi-supervised sequence tagging with bidirectional language models\",\n        \"Deep contextualized word representations\",\n        \"Efficient learning of sparse representations with an energy-based model\",\n        \"The kaldi speech recognition toolkit\",\n        \"Improving language understanding by generative pre-training\",\n        \"Language models are unsupervised multitask learners\",\n        \"Tighter variational bounds are not necessarily better\",\n        \"Multi-task self-supervised learning for robust speech recognition\",\n        \"Preventing posterior collapse with delta-vaes\",\n        \"Generating diverse high-fidelity images with vq-vae-2\",\n        \"Variational inference with normalizing flows\",\n        \"Stochastic backpropagation and approximate inference in deep generative models\",\n        \"Higher order contractive auto-encoder\",\n        \"Contractive auto-encoders: Explicit invariance during feature extraction\",\n        \"Markov chain monte carlo and variational inference: Bridging the gap\",\n        \"Introduction to the conll-2000 shared task: Chunking\",\n        \"Introduction to the conll-2003 shared task: Language-independent named entity recognition\",\n        \"wav2vec: Unsupervised pre-training for speech recognition\",\n        \"Bidirectional recurrent neural networks\",\n        \"Autorec: Autoencoders meet collaborative filtering\",\n        \"Piecewise latent variables for neural variational text processing\",\n        \"Variational bi-lstms\",\n        \"Amortized inference regularization\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"End-to-end neural segmental models for speech recognition\",\n        \"Acoustic feature learning via deep variational canonical correlation analysis\",\n        \"Acoustic feature learning using cross-domain articulatory measurements\",\n        \"Neural discrete representation learning\",\n        \"Attention is all you need\",\n        \"Extracting and composing robust features with denoising autoencoders\",\n        \"Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion\",\n        \"Dropout training as adaptive regularization\",\n        \"Fast dropout training\",\n        \"On deep multi-view representation learning\",\n        \"Deep variational canonical correlation analysis\",\n        \"Unsupervised pre-training of bidirectional speech encoders via masked reconstruction, 2020\",\n        \"X-ray microbeam speech production database\",\n        \"X-ray microbeam speech production database user’s handbook\",\n        \"Unsupervised data augmentation for consistency training\",\n        \"Self-training with noisy student improves imagenet classification\",\n        \"A survey on multi-view learning\",\n        \"Xlnet: Generalized autoregressive pretraining for language understanding\",\n        \"Adaptation of context-dependent deep neural networks for automatic speech recognition\",\n        \"Towards deeper understanding of variational autoencoding models\",\n        \"Improved representation learning for acoustic event classification using tree-structured ontology\",\n        \"Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction\",\n        \"Advances in all-neural speech recognition\"\n    ],\n    \"reasoning\": \"Selected these 171 references because they represent foundational work and recent advances in Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}