{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation\n\nPaper Abstract: Abstract We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotion-guided navigation taking into account social and proxemic constraints. To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47%percent82.4782.47\\% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Human-aware robot navigation: A survey\",\n        \"Speech based emotion classification\",\n        \"Speech emotion analysis in noisy real-world environment\",\n        \"Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild\",\n        \"Affective body expression perception and recognition: A survey\",\n        \"M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues\",\n        \"Analysis of emotion recognition using facial expressions, speech and multimodal information\",\n        \"Emoticon: Context-aware multimodal emotion recognition using frege’s principle\",\n        \"An intelligent and generic approach for detecting human emotions: a case study with facial expressions\",\n        \"Emotional expressions reconsidered: challenges to inferring emotion from human facial movements\",\n        \"Emotion recognition through gait on mobile devices\",\n        \"Step: Spatial temporal graph convolutional networks for emotion perception from gaits\",\n        \"Toward sociable robots\",\n        \"A survey of socially interactive robots\",\n        \"Glmp-realtime pedestrian path prediction using global and local movement patterns\",\n        \"Cmetric: A driving behavior measure using centrality functions\",\n        \"Artificial Emotion and Social Robotics\",\n        \"Generalized velocity obstacles\",\n        \"Optimal reciprocal collision avoidance for multi-agent navigation\",\n        \"Reciprocal velocity obstacles for real-time multi-agent navigation\",\n        \"Dronet: Learning to fly by driving\",\n        \"Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning\",\n        \"Crowdmove: Autonomous mapless navigation in crowded scenarios\",\n        \"Pedestrian dominance modeling for socially-aware robot navigation\",\n        \"From proxemics theory to socially-aware navigation: A survey\",\n        \"Pedestrian vision and collision avoidance behavior: Investigation of the information process space of pedestrians using an eye tracker\",\n        \"A framework towards a socially aware mobile robot motion in human-centered dynamic environment\",\n        \"Deep learning for emotion recognition in faces\",\n        \"Emotion recognition using facial expressions\",\n        \"Identifying emotions from walking using affective and deep features\",\n        \"The liar’s walk: Detecting deception with gait and gesture\",\n        \"The emotionally intelligent robot: Improving socially-aware human prediction in crowded environments\",\n        \"How are you feeling? multimodal emotion learning for socially-assistive robot navigation\",\n        \"Going deeper into action recognition: A survey\",\n        \"Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping\",\n        \"Spatial temporal graph convolutional networks for skeleton-based action recognition\",\n        \"A large-scale varying-view rgb-d action dataset for arbitrary-view human action recognition\",\n        \"Learning perceived emotion using affective and deep features for mental health applications\",\n        \"Learning 3d human pose from structure and motion\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"The effect of facial expressions on peripersonal and interpersonal spaces\",\n        \"Personal space, evasive movement and pedestrian level of service\",\n        \"Adam: A method for stochastic optimization\",\n        \"Recognizing emotions conveyed by human gait\",\n        \"Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices\",\n        \"Identifying emotions from non-contact gaits information based on microsoft kinects\",\n        \"Body expression recognition from animated 3d skeleton\",\n        \"Enhanced skeleton visualization for view invariant human action recognition\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}