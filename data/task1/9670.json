{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 282 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Brain-Inspired Computational Intelligence via Predictive Coding\n\nPaper Abstract: Abstract Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.\n\n{retrieved_text}\n\nGenerate exactly 282 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 282\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"High-resolution image synthesis with latent diffusion models\",\n        \"Photorealistic text-to-image diffusion models with deep language understanding\",\n        \"Imagen video: High definition video generation with diffusion models\",\n        \"Hierarchical text-conditional image generation with clip latents\",\n        \"Human-level play in the game of diplomacy by combining language models with strategic reasoning\",\n        \"Mastering the game of stratego with model-free multiagent reinforcement learning\",\n        \"Llama 2: Open foundation and fine-tuned chat models\",\n        \"Evaluating large language models trained on code\",\n        \"Language models are few-shot learners\",\n        \"ImageNet classification with deep convolutional neural networks\",\n        \"Learning representations by back-propagating errors\",\n        \"Deep residual learning for image recognition\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Mlp-mixer: An all-mlp architecture for vision\",\n        \"New types of deep neural network learning for speech recognition and related applications: An overview\",\n        \"Mastering the game of Go without human knowledge\",\n        \"DeepStack: Expert-level artificial intelligence in heads-up no-limit poker\",\n        \"Grandmaster level in starcraft ii using multi-agent reinforcement learning\",\n        \"Attention is all you need\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Towards biologically plausible deep learning\",\n        \"Equilibrium propagation: Bridging the gap between energy-based models and backpropagation\",\n        \"The forward-forward algorithm: Some preliminary investigations\",\n        \"Toward next-generation artificial intelligence: Catalyzing the NeuroAI revolution\",\n        \"The carbon impact of artificial intelligence\",\n        \"Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision\",\n        \"Deep learning: A critical appraisal\",\n        \"Rebooting AI: Building Artificial Intelligence We Can Trust\",\n        \"Competitive learning: From interactive activation to adaptive resonance\",\n        \"The recent excitement about neural networks\",\n        \"Theories of error back-propagation in the brain\",\n        \"The neural coding framework for learning generative models\",\n        \"Backpropagation and the brain\",\n        \"Towards a predictive processing implementation of the common model of cognition\",\n        \"Designing ecosystems of intelligence from first principles\",\n        \"How active inference could help revolutionise robotics\",\n        \"Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects\",\n        \"Predictive coding under the free-energy principle\",\n        \"Predictive coding: Towards a future of deep learning beyond backpropagation?\",\n        \"Learning on arbitrary graph topologies via predictive coding\",\n        \"Communication dynamics in complex brain networks\",\n        \"A theoretical framework for inference learning\",\n        \"How robust are deep neural networks?\",\n        \"Inferring neural activity before plasticity: A foundation for learning beyond backpropagation\",\n        \"Understanding predictive coding as an adaptive trust-region method\",\n        \"Predictive coding, variational autoencoders, and biological connections\",\n        \"Machine Learning: Discriminative and Generative, volume 755\",\n        \"To recognize shapes, first learn to generate images\",\n        \"Neuroanimator: Fast neural network emulation and control of physics-based models\",\n        \"Does the brain do inverse graphics\",\n        \"Predictive coding—I\",\n        \"Predictive coding: A fresh view of inhibition in the retina\",\n        \"Dynamic predictive coding by the retina\",\n        \"An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity\",\n        \"Learning and inference in the brain\",\n        \"Hierarchical models in the brain\",\n        \"Dynamic model of visual recognition predicts neural response properties in the visual cortex\",\n        \"An optimal estimation approach to visual perception and learning\",\n        \"General duality between optimal control and estimation\",\n        \"Linear theory for control of nonlinear stochastic systems\",\n        \"Path integral control and bounded rationality\",\n        \"The free-energy principle: A unified brain theory?\",\n        \"Variational message passing\",\n        \"Surfing Uncertainty: Prediction, Action, and the Embodied Mind\",\n        \"The Predictive Mind\",\n        \"Active Inference: The Free Energy Principle in Mind, Brain, and Behavior\",\n        \"World models and predictive coding for cognitive and developmental robotics: frontiers and challenges\",\n        \"Active inference in robotics and artificial agents: Survey and challenges\",\n        \"The role of occam’s razor in knowledge discovery\",\n        \"Predictive coding—II\",\n        \"A theory of cortical responses\",\n        \"A review of predictive coding algorithms\",\n        \"DEM: A variational treatment of dynamic systems\",\n        \"Variational free energy and the Laplace approximation\",\n        \"Dendritic cortical microcircuits approximate the backpropagation algorithm\",\n        \"Incremental predictive coding: A parallel and fully automatic learning algorithm\",\n        \"Deep predictive coding network with local recurrent processing for object recognition\",\n        \"Robust graph representation learning via predictive coding\",\n        \"Brain computation by assemblies of neurons\",\n        \"Associative memories via predictive coding\",\n        \"Recurrent predictive coding models for associative memory employing covariance learning\",\n        \"Universal Hopfield networks: A general framework for single-shot associative memory models\",\n        \"Hopfield networks is all you need\",\n        \"BayesPCN: A continually learnable predictive coding associative memory\",\n        \"On the relationship between variational inference and auto-associative memory\",\n        \"Predictive coding approximates backprop along arbitrary computation graphs\",\n        \"Can the brain do backpropagation? — Exact implementation of backpropagation in predictive coding networks\",\n        \"Reverse differentiation via predictive coding\",\n        \"Contrastive Hebbian learning in the continuous Hopfield model\",\n        \"How auto-encoders could provide credit assignment in deep networks via target propagation\",\n        \"Difference target propagation\",\n        \"Backpropagation at the infinitesimal inference limit of energy-based models: Unifying predictive coding, equilibrium propagation, and contrastive Hebbian learning\",\n        \"A theoretical framework for inference and learning in predictive coding networks\",\n        \"Lifelong neural predictive coding: Learning cumulatively online without forgetting\",\n        \"Continual learning of recurrent neural networks by locally aligning distributed representations\",\n        \"Statistical analysis of stochastic gradient methods for generalized linear models\",\n        \"Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics\",\n        \"The electrical constants of a crustacean nerve fibre\",\n        \"Theory of physiological properties of dendrites\",\n        \"Mathematical Foundations of Neuroscience, volume 35\",\n        \"Sparse coding with an overcomplete basis set: A strategy employed by v1?\",\n        \"Development of low entropy coding in a recurrent network\",\n        \"Reconciling predictive coding and biased competition models of cortical function\",\n        \"Predictive coding as a model of biased competition in visual attention\",\n        \"Unsupervised learning of overlapping image components using divisive input modulation\",\n        \"Algorithms for non-negative matrix factorization\",\n        \"Object perception as Bayesian inference\",\n        \"Predictive coding beyond Gaussian assumptions\",\n        \"The graphical brain: Belief propagation and active inference\",\n        \"Convolutional neural generative coding: Scaling predictive coding to natural images\",\n        \"Cogngen: Building the kernel for a hyperdimensional predictive processing cognitive architecture\",\n        \"Faster convergence in deep-predictive-coding networks to learn deeper representations\",\n        \"Context dependent encoding using convolutional dynamic networks\",\n        \"A hierarchical predictive coding model of object recognition in natural images\",\n        \"Cognitive architectures for sensory processing\",\n        \"Temporal context in object recognition\",\n        \"Pyramid methods in image processing\",\n        \"Deep predictive coding networks\",\n        \"Parallel flow in deep predictive coding networks\",\n        \"Minimum error entropy Kalman filter\",\n        \"Modified Kalman filter based method for training state-recurrent multilayer perceptrons\",\n        \"Learning to adapt by minimizing discrepancy\",\n        \"Dynamic predictive coding: A new model of hierarchical sequence learning and prediction in the cortex\",\n        \"Generalised filtering\",\n        \"Continual competitive memory: A neural system for online task-free lifelong learning\",\n        \"Action understanding and active inference\",\n        \"Perceptions as hypotheses: Saccades as experiments\",\n        \"Active inference, eye movements and oculomotor delays\",\n        \"Perception and hierarchical dynamics\",\n        \"Bayesian filtering with multiple internal models: Toward a theory of social intelligence\",\n        \"A neural model of binocular saccade planning and vergence control\",\n        \"Cognitive architecture for video games\",\n        \"Backprop-free reinforcement learning with active neural generative coding\",\n        \"Active predicting coding: Brain-inspired reinforcement learning for sparse reward robotic control problems\",\n        \"Exploring robotic minds: actions, symbols, and consciousness as self-organizing dynamic phenomena\",\n        \"Adaptive robot body learning and estimation through predictive coding\",\n        \"An empirical study of active inference on a humanoid robot\",\n        \"Active predictive coding: A unified neural framework for learning hierarchical world models for perception and planning\",\n        \"Active predictive coding networks: A neural solution to the problem of learning reference frames and part-whole hierarchies\",\n        \"A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics\",\n        \"Maze learning using a hyperdimensional predictive processing cognitive architecture\",\n        \"Augmenting bert-style models with predictive coding to improve discourse-level representations\",\n        \"Probabilistic models of the brain: Perception and neural function\",\n        \"Hierarchical Bayesian inference in the visual cortex\",\n        \"Bayesian brain: Probabilistic approaches to neural coding\",\n        \"The cybernetic bayesian brain\",\n        \"The history of the future of the bayesian brain\",\n        \"Whatever next? predictive brains, situated agents, and the future of cognitive science\",\n        \"The epistemological problem for automata\",\n        \"On the computational architecture of the neocortex\",\n        \"Perceptual illusions and brain models\",\n        \"Perceptions as hypotheses\",\n        \"Free energy and dendritic self-organization\",\n        \"Canonical microcircuits for predictive coding\",\n        \"Neural elements for predictive coding\",\n        \"Neuronal coding of prediction errors\",\n        \"Midbrain dopamine neurons encode a quantitative reward prediction error signal\",\n        \"Somato-dendritic synaptic plasticity and error-backpropagation in active dendrites\",\n        \"Where is the error? hierarchical predictive coding through dendritic error computation\",\n        \"Evaluating the neurophysiological evidence for predictive processing as a model of perception\",\n        \"Predictive coding explains binocular rivalry: An epistemological review\",\n        \"Illusory motion reproduced by deep neural networks trained for prediction\",\n        \"Repetition suppression and its contextual determinants in predictive coding\",\n        \"Attention, uncertainty, and free-energy\",\n        \"Cerebral hierarchies: predictive processing, precision and the pulvinar\",\n        \"Evidence of a predictive coding hierarchy in the human brain listening to speech\",\n        \"A tutorial on the free-energy framework for modelling perception and learning\",\n        \"Predictive coding: A theoretical and experimental review\",\n        \"Active inference, sensory attenuation and illusions\",\n        \"Free energy, precision and learning: the role of cholinergic neuromodulation\",\n        \"Attention or salience?\",\n        \"Prediction and memory: A predictive coding account\",\n        \"Preventing deterioration of classification accuracy in predictive coding networks\",\n        \"Spiking neural predictive coding for continual learning from data streams\",\n        \"Tightening the biological constraints on gradient-based predictive coding\",\n        \"Learning spiking neural systems with the event-driven forward-forward process\",\n        \"Deep learning hardware: Past, present, and future\",\n        \"Memristor-the missing circuit element\",\n        \"The missing memristor found\",\n        \"Neuromorphic spintronics\",\n        \"Training end-to-end analog neural networks with equilibrium propagation\",\n        \"Dynamical memristors for higher-complexity neuromorphic computing\",\n        \"Breeding brains? patients’ and laymen’s perspectives on cerebral organoids\",\n        \"Neuronal cultures playing pong: First steps toward advanced screening and biological computing\",\n        \"Organoid intelligence (oi): the new frontier in biocomputing and intelligence-in-a-dish\",\n        \"Predictive coding as a model of response properties in cortical area v1\",\n        \"Reflections on agranular architecture: Predictive coding in the motor cortex\",\n        \"The sentient organoid?\",\n        \"A single-spike model of predictive coding\",\n        \"Bayesian model selection and model averaging\",\n        \"Correlation between neural spike trains increases with firing rate\",\n        \"Networks of spiking neurons: the third generation of neural network models\",\n        \"The spinnaker project\",\n        \"A million spiking-neuron integrated circuit with a scalable communication network and interface\",\n        \"Loihi: A neuromorphic manycore processor with on-chip learning\",\n        \"A model of predictive coding based on spike timing\",\n        \"Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type\",\n        \"A quantitative description of membrane current and its application to conduction and excitation in nerve\",\n        \"Pc-snn: Supervised learning with local hebbian synaptic plasticity based on predictive coding in spiking neural networks\",\n        \"Dendritic predictive coding: A theory of cortical computation with spiking neurons\",\n        \"(Non-)Convergence results for predictive coding networks\",\n        \"A perspective view and survey of meta-learning\",\n        \"Meta-learning with memory-augmented neural networks\",\n        \"Meta-learning in neural networks: A survey\",\n        \"Fast inference in sparse coding algorithms with applications to object recognition\",\n        \"Dynamic expectation maximization algorithm for estimation of linear systems with colored noise\",\n        \"Predictive coding, precision and synchrony\",\n        \"Predprop: Bidirectional stochastic optimization with precision weighted predictive coding\",\n        \"Understanding and improving optimization in predictive coding networks\",\n        \"Curvature-sensitive predictive coding with approximate laplace monte carlo\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"Adam: A method for stochastic optimization\",\n        \"Visual feature extraction by a multilayered network of analog threshold elements\",\n        \"On the difficulty of training recurrent neural networks\",\n        \"Generative modeling by estimating gradients of the data distribution\",\n        \"On variational message passing on factor graphs\",\n        \"How to grow a mind: Statistics, structure, and abstraction\",\n        \"Toward the neural implementation of structure learning\",\n        \"An active inference approach to modeling structure learning: Concept learning as an example case\",\n        \"Structure learning in predictive processing needs revision\",\n        \"Morphogenesis as Bayesian inference: A variational approach to pattern formation and control in complex biological systems\",\n        \"Ten simple rules for dynamic causal modeling\",\n        \"A tutorial on bayesian nonparametric models\",\n        \"Comparing dynamic causal models using aic, bic and free energy\",\n        \"Post hoc bayesian model selection\",\n        \"Hierarchical dirichlet processes\",\n        \"Nonparametric Bayesian Models of Lexican Acquisition\",\n        \"Computational phenotyping in psychiatry: a worked example\",\n        \"Sparse distributed memory\",\n        \"Vector symbolic architectures: A new building material for artificial general intelligence\",\n        \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors\",\n        \"A neural network walks into a lab: towards using deep nets as models for human behavior\",\n        \"Active inference and learning\",\n        \"Deep predictive coding networks for video prediction and unsupervised learning\",\n        \"Short-term precipitation prediction with skip-connected prednet\",\n        \"Afa-prednet: The action modulation within predictive coding\",\n        \"Prednet and predictive coding: A critical review\",\n        \"Predictive codes for forthcoming perception in the frontal cortex\",\n        \"Neural repetition suppression reflects fulfilled perceptual expectations\",\n        \"The Organization of Behavior\",\n        \"Random synaptic feedback weights support error backpropagation for deep learning\",\n        \"Highly nonrandom features of synaptic connectivity in local cortical circuits\",\n        \"Long short-term memory\",\n        \"Linking structure and function in macroscale brain networks\",\n        \"Application of graph theory for identifying connectivity patterns in human brain networks: a systematic review\",\n        \"Network geometry\",\n        \"Credit assignment through time: Alternatives to backpropagation\",\n        \"Learning long-term dependencies with gradient descent is difficult\",\n        \"Biologically motivated algorithms for propagating local target representations\",\n        \"Back propagation is sensitive to initial conditions\",\n        \"On weight initialization in deep neural networks\",\n        \"Old can be gold: Better gradient flow can make vanilla-gcns great again\",\n        \"Efficient backprop\",\n        \"Layer normalization\",\n        \"Target propagation in recurrent neural networks\",\n        \"Training language models using target-propagation\",\n        \"Investigating backpropagation alternatives when learning to dynamically count with recurrent neural networks\",\n        \"A theoretical framework for target propagation, 2020\",\n        \"Towards scaling difference target propagation by learning backprop targets, 2022\",\n        \"Fixed-weight difference target propagation\",\n        \"Backpropagation-free deep learning with recursive local representation alignment\",\n        \"Deep credit assignment by aligning local representations\",\n        \"Training products of experts by minimizing contrastive divergence\",\n        \"Equivalence of equilibrium propagation and recurrent backpropagation\",\n        \"Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias\",\n        \"Generalization of equilibrium propagation to vector field dynamics\",\n        \"Training dynamical binary neural networks with equilibrium propagation\",\n        \"Holomorphic equilibrium propagation computes exact gradients through finite size oscillations\",\n        \"Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation\",\n        \"Training a spiking neural network with equilibrium propagation\",\n        \"Eqspike: spike-driven equilibrium propagation for neuromorphic implementations\",\n        \"Direct feedback alignment provides learning in deep neural networks\",\n        \"Hardware beyond backpropagation: a photonic co-processor for direct feedback alignment\",\n        \"Silicon photonic architecture for training deep neural networks with direct feedback alignment\",\n        \"Feedback alignment in deep convolutional networks\",\n        \"Direct feedback alignment scales to modern deep learning tasks and architectures\",\n        \"How important is weight symmetry in backpropagation?\",\n        \"Biologically-plausible learning algorithms can scale to large datasets\",\n        \"The predictive forward-forward algorithm\",\n        \"Signal propagation: The framework for learning and inference in a forward pass\",\n        \"Error-driven input modulation: solving the credit assignment problem without a backward pass\"\n    ],\n    \"reasoning\": \"Selected these 282 references because they represent foundational work and recent advances in Brain-Inspired Computational Intelligence via Predictive Coding, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}