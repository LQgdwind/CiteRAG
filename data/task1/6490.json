{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Actor-Critic based Improper Reinforcement Learning\n\nPaper Abstract: Abstract We consider an improper reinforcement learning setting where a learner is given MùëÄM base controllers for an unknown Markov decision process, and wishes to combine them optimally to produce a potentially new controller that can outperform each of the base ones. This can be useful in tuning across controllers, learnt possibly in mismatched or simulated environments, to obtain a good controller for a given target environment with relatively few trials. Towards this, we propose two algorithms: (1) a Policy Gradient-based approach; and (2) an algorithm that can switch between a simple Actor-Critic (AC) based scheme and a Natural Actor-Critic (NAC) scheme depending on the available information. Both algorithms operate over a class of improper mixtures of the given controllers. For the first case, we derive convergence rate guarantees assuming access to a gradient oracle. For the AC-based approach we provide convergence rate guarantees to a stationary point in the basic AC case and to a global optimum in the NAC case. Numerical results on (i) the standard control theoretic benchmark of stabilizing an cartpole; and (ii) a constrained queueing task show that our improper policy optimization algorithm can stabilize the system even when the base policies at its disposal are unstable.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Regret bounds for the adaptive control of linear quadratic systems\",\n        \"Optimality and approximation with policy gradient methods in markov decision processes\",\n        \"Boosting for control of dynamical systems\",\n        \"Gambling in a rigged casino: The adversarial multi-armed bandit problem\",\n        \"Near-optimal regret bounds for reinforcement learning\",\n        \"Optimizing over a restricted policy class in mdps\",\n        \"Analysis of a target-based actor-critic algorithm with linear function approximation\",\n        \"Successor features for transfer in reinforcement learning\",\n        \"Dynamic programming and optimal control 3rd edition, volume ii\",\n        \"Global optimality guarantees for policy gradient methods\",\n        \"A finite time analysis of temporal difference learning with linear function approximation\",\n        \"Natural actor‚Äìcritic algorithms\",\n        \"Almost sure stability of stochastic linear systems with ergodic parameters\",\n        \"Stochastic Approximation\",\n        \"Logarithmic regret for learning linear quadratic regulators efficiently\",\n        \"Black-box control for linear dynamical systems\",\n        \"More data speeds up training time in learning halfspaces over sparse vectors\",\n        \"From average case complexity to improper learning complexity\",\n        \"On the Sample Complexity of the Linear Quadratic Regulator\",\n        \"Regret analysis of a markov policy gradient algorithm for multi-arm bandits\",\n        \"Probability: Theory and examples\",\n        \"Global convergence of policy gradient methods for the linear quadratic regulator\",\n        \"Online convex optimization in the bandit setting: Gradient descent without a gradient\",\n        \"On the properties of the softmax function with application in game theory and reinforcement learning\",\n        \"Thompson Sampling for Learning Parameterized Markov Decision Processes\",\n        \"Efficient reinforcement learning for high dimensional linear quadratic systems\",\n        \"Approximately optimal approximate reinforcement learning\",\n        \"Nonlinear Control\",\n        \"Efficient learning by implicit exploration in bandit problems with side observations\",\n        \"Actor-critic algorithms\",\n        \"Asymptotically efficient adaptive allocation rules\",\n        \"Softmax policy gradient methods can take exponential time to converge\",\n        \"The weighted majority algorithm\",\n        \"Les √©quations aux d√©riv√©es partielles (paris, 1962)\",\n        \"Multi-agent actor-critic for mixed cooperative-competitive environments\",\n        \"Popular ensemble methods: An empirical study\",\n        \"Certainty equivalence is efficient for linear quadratic control\",\n        \"On the global convergence rates of softmax policy gradient methods\",\n        \"Hybrid mac protocols for low-delay scheduling\",\n        \"Throughput optimal decentralized scheduling with single-bit state feedback for a class of queueing systems\",\n        \"Explore no more: Improved high-probability regret bounds for non-stochastic bandits\",\n        \"(more) efficient reinforcement learning via posterior sampling\",\n        \"Learning unknown markov decision processes: A thompson sampling approach\",\n        \"Natural actor-critic\",\n        \"Data-driven model-free slip control of anti-lock braking systems using reinforcement q-learning\",\n        \"On-line q-learning using connectionist systems\",\n        \"Trust region policy optimization\",\n        \"Proximal policy optimization algorithms\",\n        \"Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps\",\n        \"Mastering the game of go with deep neural networks and tree search\",\n        \"Artificial intelligence: Learning to play Go from scratch\",\n        \"Reinforcement Learning: An Introduction\",\n        \"Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning\",\n        \"Policy gradient methods for reinforcement learning with function approximation\",\n        \"Stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks\",\n        \"Ensemble algorithms in reinforcement learning\",\n        \"Ensemble network architecture for deep reinforcement learning\",\n        \"Improving sample complexity bounds for (natural) actor-critic algorithms\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Actor-Critic based Improper Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}