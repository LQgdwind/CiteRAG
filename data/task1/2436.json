{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling\n\nPaper Abstract: Abstract A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining labeled data can be costly, especially for multiple language varieties and dialects. We propose to self-train pre-trained language models in zero- and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as ~10% F1 (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from the unlabeled DA examples used for self-training. Our work opens up opportunities for developing DA models exploiting only MSA resources and it can be extended to other languages and tasks.111Our code and fine-tuned models can be accessed at: https://github.com/mohammadKhalifa/zero-shot-arabic-dialects\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Integrating rule-based system with classification for Arabic named entity recognition\",\n        \"You tweet what you speak: A city-level dataset of arabic dialects\",\n        \"ARBERT & MARBERT: Deep bidirectional transformers for Arabic\",\n        \"Nadi 2020: The first nuanced arabic dialect identification shared task\",\n        \"Micro-dialect identification in diaglossic and code-switched environments\",\n        \"AraNet: A Deep Learning Toolkit for Arabic Social Media\",\n        \"Deep learning for arabic nlp: A survey\",\n        \"Transfer learning for Arabic named entity recognition with deep neural networks\",\n        \"Part-of-speech tagging for Arabic gulf dialect using bi-lstm\",\n        \"Understanding and detecting dangerous speech in social media\",\n        \"Arabert: Transformer-based model for Arabic language understanding\",\n        \"Anersys: An Arabic named entity recognition system based on maximum entropy\",\n        \"A neural probabilistic language model\",\n        \"The MADAR shared task on Arabic fine-grained dialect identification\",\n        \"Electra: Pre-training text encoders as discriminators rather than generators\",\n        \"Unsupervised cross-lingual representation learning at scale\",\n        \"The multilingual Internet: Language, culture, and communication online\",\n        \"Named entity recognition using cross-lingual resources: Arabic as an example\",\n        \"Multi-dialect arabic pos tagging: a crf approach\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"A robust self-learning framework for cross-lingual text classification\",\n        \"Arabic named entity recognition using deep learning approach\",\n        \"Deep models for Arabic dialect identification on benchmarked data\",\n        \"Character-aware neural networks for Arabic named entity recognition for social media\",\n        \"Combination of active learning and self-training for cross-lingual sentiment classification with density analysis of unlabelled samples\",\n        \"Computer-mediated discourse 2.0\",\n        \"Character convolutions for arabic named entity recognition with long short-term memory networks\",\n        \"Cross-lingual transfer learning for pos tagging without cross-lingual resources\",\n        \"Self-training and co-training applied to spanish named entity recognition\",\n        \"Neural architectures for named entity recognition\",\n        \"Cross-lingual language model pretraining\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"End-to-end sequence labeling via bi-directional lstm-cnns-crf\",\n        \"The penn Arabic treebank: Building a large-scale annotated Arabic corpus\",\n        \"Developing an egyptian Arabic treebank: Impact of dialectal morphology on annotation and tool development\",\n        \"Efficient estimation of word representations in vector space\",\n        \"Tides extraction (ace) 2003 multilingual training data\",\n        \"CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing\",\n        \"Cross-lingual name tagging and linking for 282 languages\",\n        \"Self-training without reranking for parser domain adaptation and its impact on semantic role labeling\",\n        \"A survey of Arabic named entity recognition and classification\",\n        \"A hybrid approach to Arabic named entity recognition\",\n        \"Natural language processing for dialectical arabic: A survey\",\n        \"Mass: Masked sequence to sequence pre-training for language generation\",\n        \"Small and practical bert models for sequence labeling\",\n        \"Predicting the effectiveness of self-training: Application to sentiment classification\",\n        \"Attention is all you need\",\n        \"Semi-supervised learning for part-of-speech tagging of mandarin transcribed speech\",\n        \"Neural cross-lingual named entity recognition with minimal resources\",\n        \"Can artificial neural networks learn language models?\",\n        \"Enhancing pre-trained language representations with rich knowledge for machine reading comprehension\",\n        \"The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content\",\n        \"Arabic dialect identification\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}