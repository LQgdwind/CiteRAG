{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction\n\nPaper Abstract: Abstract We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: https://github.com/pavancm/GREED.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Perceptual quality assessment of high frame rate video\",\n        \"A study of high frame rate video formats\",\n        \"Subjective and objective quality assessment of high frame rate videos\",\n        \"Video quality assessment by reduced reference spatio-temporal entropic differencing\",\n        \"Toward a practical perceptual video quality metric\",\n        \"SpEED-QA: Spatial efficient entropic differencing for image and video quality\",\n        \"Spatiotemporal feature integration and model fusion for full reference video quality assessment\",\n        \"Objective video quality assessment methods: A classification, review, and performance comparison\",\n        \"Image quality assessment: from error visibility to structural similarity\",\n        \"Multiscale structural similarity for image quality assessment\",\n        \"FSIM: A feature similarity index for image quality assessment\",\n        \"A new standardized method for objectively measuring video quality\",\n        \"Temporal video quality model accounting for variable frame delay distortions\",\n        \"Video quality assessment on mobile devices: Subjective, behavioral and objective studies\",\n        \"Motion tuned spatio-temporal quality assessment of natural videos\",\n        \"A structural similarity metric for video based on motion models\",\n        \"A spatiotemporal most-apparent-distortion model for video quality assessment\",\n        \"Most apparent distortion: full-reference image quality assessment and the role of strategy\",\n        \"Attention driven foveated video quality assessment\",\n        \"A full reference video quality measure based on motion differences and saliency maps evaluation\",\n        \"An optical flow-based full reference video quality assessment algorithm\",\n        \"Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network\",\n        \"A neural network model of spatial distortion sensitivity for video quality estimation\",\n        \"C3DVQA: Full-reference video quality assessment with 3d convolutional neural network\",\n        \"Perceptual aliasing factors and the impact of frame rate on video quality\",\n        \"Temporal motion smoothness and the impact of frame rate variation on video quality\",\n        \"A frame rate dependent video quality metric based on temporal wavelet decomposition and spatiotemporal pooling\",\n        \"Capturing video frame rate variations via entropic differencing\",\n        \"Blind prediction of natural video quality\",\n        \"Image information and visual quality\",\n        \"RRED indices: Reduced reference entropic differencing for image quality assessment\",\n        \"The statistics of natural images\",\n        \"No-reference image quality assessment in the spatial domain\",\n        \"Making a “completely blind” image quality analyzer\",\n        \"Adaptive wavelet thresholding for image denoising and compression\",\n        \"Wavelet-based texture retrieval using generalized gaussian density and Kullback-Leibler distance\",\n        \"On the sum of generalized gaussian random signals\",\n        \"New results on the sum of two generalized Gaussian random variables\",\n        \"Exposing image splicing with inconsistent local noise variances\",\n        \"Encoding for streaming sites\",\n        \"New support vector algorithms\",\n        \"LIBSVM: A library for support vector machines\",\n        \"Entropy-based algorithms for best basis selection\",\n        \"Study of subjective and objective quality assessment of video\",\n        \"Final report from the video quality experts group on the validation of objective quality metrics for video quality assessment\",\n        \"Spatial and temporal contrast-sensitivity functions of the visual system\",\n        \"Vis3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}