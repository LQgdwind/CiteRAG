{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 63 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Solving Dynamic Principal-Agent Problems with a Rationally Inattentive Principal\n\nPaper Abstract: Abstract Principal-Agent (PA) problems describe a broad class of economic relationships characterized by misaligned incentives and asymmetric information. The Principal’s problem is to find optimal incentives given the available information, e.g., a manager setting optimal wages for its employees. Whereas the Principal is often assumed rational, comparatively little is known about solutions when the Principal is boundedly rational, especially in the sequential setting, with multiple Agents, and with multiple information channels. Here, we develop RIRL, a deep reinforcement learning framework that solves such complex PA problems with a rationally inattentive Principal. Such a Principal incurs a cost for paying attention to information, which can model forms of bounded rationality. We use RIRL to analyze rich economic phenomena in manager-employee relationships. In the single-step setting, 1) RIRL yields wages that are consistent with theoretical predictions; and 2) non-zero attention costs lead to simpler but less profitable wage structures, and increased Agent welfare. In a sequential setting with multiple Agents, RIRL shows opposing consequences of the Principal’s inattention to different information channels: 1) inattention to Agents’ outputs closes wage gaps based on ability differences; and 2) inattention to Agents’ efforts induces a social dilemma dynamic in which Agents work harder, but essentially for free. Moreover, RIRL reveals non-trivial relationships between the Principal’s inattention and Agent types, e.g., if Agents are prone to sub-optimal effort choices, payment schedules are more sensitive to the Principal’s attention cost. As such, RIRL can reveal novel economic relationships and enables progress towards understanding the effects of bounded rationality in dynamic settings.111Example code is available at https://github.com/salesforce/RIRL\n\n{retrieved_text}\n\nGenerate exactly 63 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 63\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Feudal multi-agent hierarchies for cooperative reinforcement learning\",\n        \"Job market signaling and employer learning\",\n        \"Emergent tool use from multi-agent autocurricula\",\n        \"Modeling human categorization of natural images using deep feature representations\",\n        \"Mine: mutual information neural estimation\",\n        \"Agent-based modeling: Methods and techniques for simulating human systems\",\n        \"Cognitive model priors for predicting human decisions\",\n        \"Explore, discover and learn: Unsupervised discovery of state-covering skills\",\n        \"Cognitive biases\",\n        \"Experimental tests of rational inattention\",\n        \"Bounded rationality in principal-agent relationships\",\n        \"Bounded rationality for relaxing best response and mutual consistency: The quantal hierarchy model of decision-making\",\n        \"Diversity is all you need: Learning skills without a reward function\",\n        \"Behavioral Inattention\",\n        \"Variability and mutability, contribution to the study of statistical distributions and relations. studi cconomico-giuridici della r. universita de cagliari (1912). reviewed in: Light, rj, margolin, bh: An analysis of variance for categorical data\",\n        \"Generative adversarial nets\",\n        \"Soft q-learning with mutual-information regularization\",\n        \"Variational intrinsic control\",\n        \"An analysis of the principal-agent problem\",\n        \"Deep learning for predicting human strategic behavior\",\n        \"Risk aversion, performance pay, and the principal-agent problem\",\n        \"Long Short-Term Memory\",\n        \"Rationally inattentive inverse reinforcement learning explains youtube commenting behavior\",\n        \"Moral hazard in teams\",\n        \"Managerial incentive problems: A dynamic perspective\",\n        \"Aggregation and Linearity in the Provision of Intertemporal Incentives\",\n        \"Route choice, travel time variability, and rational inattention\",\n        \"Auto-encoding variational bayes\",\n        \"Preference reversals due to myopic discounting of delayed reward\",\n        \"Neural networks for predicting human interactions in repeated games\",\n        \"Complexity of computing optimal stackelberg strategies in security resource allocation games\",\n        \"Brain-like object recognition with high-performing shallow recurrent anns\",\n        \"Bounded rationality via recursion\",\n        \"Mutual-information regularization in markov decision processes and actor-critic learning\",\n        \"Multi-agent reinforcement learning in sequential social dilemmas\",\n        \"A neural network walks into a lab: towards using deep nets as models for human behavior\",\n        \"Busines Cycle Dynamics Under Rational Inattention\",\n        \"Rational inattention: A review\",\n        \"Quantal response equilibria for normal form games\",\n        \"The political evolution of principal-agent models\",\n        \"The optimal structure of incentives and authority within an organization\",\n        \"Variational information maximisation for intrinsically motivated reinforcement learning\",\n        \"Information-Theoretic Bounded Rationality\",\n        \"An information-theoretic on-line update principle for perception-action coupling\",\n        \"Risk aversion in the small and in the large\",\n        \"A continuous-time version of the principal-agent problem\",\n        \"Learning expensive coordination: An event-based deep rl approach\",\n        \"M3rl: Mind-aware multi-agent management reinforcement learning\",\n        \"Mastering the game of go without human knowledge\",\n        \"Models of man; social and rational\",\n        \"Implications of rational inattention\",\n        \"Job Market Signaling\",\n        \"Agent and Principal\",\n        \"An information-theoretic approach to curiosity-driven reinforcement learning\",\n        \"Reinforcement learning: An introduction\",\n        \"Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist\",\n        \"Advances in Prospect Theory: Cumulative Representation of Uncertainty\",\n        \"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II\",\n        \"Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning\",\n        \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"A Study of AI Population Dynamics with Million-agent Reinforcement Learning\",\n        \"The ai economist: Improving equality and productivity with ai-driven tax policies\",\n        \"The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning\"\n    ],\n    \"reasoning\": \"Selected these 63 references because they represent foundational work and recent advances in Solving Dynamic Principal-Agent Problems with a Rationally Inattentive Principal, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}