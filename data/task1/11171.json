{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 65 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: An analysis of the effects of sharing research data, code, and preprints on citations\n\nPaper Abstract: Abstract Calls to make scientific research more open have gained traction with a range of societal stakeholders. Open Science practices include but are not limited to the early sharing of results via preprints and openly sharing outputs such as data and code to make research more reproducible and extensible. Existing evidence shows that adopting Open Science practices has effects in several domains. In this study, we investigate whether adopting one or more Open Science practices leads to significantly higher citations for an associated publication, which is one form of academic impact. We use a novel dataset known as Open Science Indicators, produced by PLOS and DataSeer, which includes all PLOS publications from 2018 to 2023 as well as a comparison group sampled from the PMC Open Access Subset. In total, we analyze circa 122’000 publications. We calculate publication and author-level citation indicators and use a broad set of control variables to isolate the effect of Open Science Indicators on received citations. We show that Open Science practices are adopted to different degrees across scientific disciplines. We find that the early release of a publication as a preprint correlates with a significant positive citation advantage of about 20.2%percent20.220.2\\% (±.7plus-or-minus.7\\pm.7) on average. We also find that sharing data in an online repository correlates with a smaller yet still positive citation advantage of 4.3%percent4.34.3\\% (±.8plus-or-minus.8\\pm.8) on average. However, we do not find a significant citation advantage for sharing code. Further research is needed on additional or alternative measures of impact beyond citations. Our results are likely to be of interest to researchers, as well as publishers, research funders, and policymakers.\n\n{retrieved_text}\n\nGenerate exactly 65 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 65\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"The Unacknowledged Convergence of Open Source, Open Access, and Open Science\",\n        \"Wikipedia and the Politics of Openness\",\n        \"A Genealogy of Open Access: Negotiations between Openness and Access to Research\",\n        \"Open Science Now: A Systematic Literature Review for an Integrated Definition\",\n        \"Philosophy of Open Science\",\n        \"UNESCO Recommendation on Open Science\",\n        \"Assessment of Transparency Indicators across the Biomedical Literature: How Open Is Open?\",\n        \"Establishing Institutional Scores with the Rigor and Transparency Index: Large-scale Analysis of Scientific Reporting Quality\",\n        \"Promoting Open Science: A Holistic Approach to Changing Behaviour\",\n        \"Towards Wide-Scale Adoption of Open Science Practices: The Role of Open Science Communities\",\n        \"A Survey of Funders’ and Institutions’ Needs for Understanding Researchers’ Open Research Practices\",\n        \"PLOS Open Science Indicators Principles and Definitions\",\n        \"The Citation Advantage of Linking Publications to Research Data\",\n        \"PathOS Deliverable 1.2: Scoping Review of Open Science Impact\",\n        \"Open Access Research Outputs Receive More Diverse Citations\",\n        \"All the Research That’s Fit to Print: Open Access and the News Media\",\n        \"Open Science Is a Research Accelerator\",\n        \"Open Science Saves Lives: Lessons from the COVID-19 Pandemic\",\n        \"Estimating the Reproducibility of Psychological Science\",\n        \"The Societal Impact of Open Science–a Scoping Review\",\n        \"The Economic Impacts of Open Science: A Rapid Evidence Assessment\",\n        \"Cost-Benefit Analysis for FAIR Research Data: Cost of Not Having FAIR Research Data\",\n        \"The Invisible Workload of Open Research\",\n        \"Dynamics of Cumulative Advantage and Threats to Equity in Open Science: A Scoping Review\",\n        \"Open Science and Epistemic Diversity: Friends or Foes?\",\n        \"Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?\",\n        \"Sharing Detailed Research Data Is Associated with Increased Citation Rate\",\n        \"Data Reuse and the Open Data Citation Advantage\",\n        \"Linking to Data: Effect on Citation Rates in Astronomy\",\n        \"A Study of the Impact of Data Sharing on Article Citations Using Journal Policies as a Natural Experiment\",\n        \"Long-Term Availability of Data Associated with Articles in PLOS ONE\",\n        \"Data Sharing Practices and Data Availability upon Request Differ across Scientific Disciplines\",\n        \"Practical Challenges for Researchers in Data Sharing\",\n        \"Code Sharing Is Associated with Research Impact in Image Processing\",\n        \"Code Sharing in Psychological Methods and Statistics: An Overview and Associations with Conventional and Alternative Research Metrics\",\n        \"The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices\",\n        \"Papers with Code or without Code? Impact of GitHub Repository Usability on the Diffusion of Machine Learning Research\",\n        \"How Open Science Helps Researchers Succeed\",\n        \"Releasing a Preprint Is Associated with More Attention and Citations for the Peer-Reviewed Article\",\n        \"The Relationship between bioRxiv Preprints, Citations and Altmetrics\",\n        \"Is Preprint the Future of Science? A Thirty Year Journey of Online Preprint Services\",\n        \"The Evolving Role of Preprints in the Dissemination of COVID-19 Research and Their Impact on the Science Communication Landscape\",\n        \"Unreviewed Science in the News: The Evolution of Preprint Media Coverage from 2014–2021\",\n        \"The Experiences of COVID-19 Preprint Authors: A Survey of Researchers about Publishing and Receiving Feedback on Their Work during the Pandemic\",\n        \"Characterization of Comments about bioRxiv and medRxiv Preprints\",\n        \"On the Value of Preprints: An Early Career Researcher Perspective\",\n        \"Preprinting Is Positively Associated with Early Career Researcher Status in Ecology and Evolution\",\n        \"No Evidence of Important Difference in Summary Treatment Effects between COVID-19 Preprints and Peer-Reviewed Publications: A Meta-Epidemiological Study\",\n        \"Consistency of Covid-19 Trial Preprints with Published Reports and Impact for Decision Making: Retrospective Review\",\n        \"Downstream Retraction of Preprinted Research in the Life and Medical Sciences\",\n        \"PLOS Open Science Indicators (Version 5)\",\n        \"PMC Open Access Subset\",\n        \"Self-Selected or Mandated, Open Access Increases Citation Impact for Higher Quality Research\",\n        \"Does Interdisciplinary Research Lead to Higher Citation Impact? The Different Effect of Proximal and Distal Interdisciplinarity\",\n        \"Bias against Novelty in Science: A Cautionary Tale for Users of Bibliometric Indicators\",\n        \"The Chaperone Effect in Scientific Publishing\",\n        \"Author Name Disambiguation in MEDLINE\",\n        \"PubMed and beyond: A Survey of Web Tools for Searching Biomedical Literature\",\n        \"A Brief Survey of Automatic Methods for Author Name Disambiguation\",\n        \"Author Name Disambiguation for PubMed\",\n        \"Entity Linking for Biomedical Literature\",\n        \"Author Name Disambiguation: What Difference Does It Make in Author-based Citation Analysis?\",\n        \"Distortive Effects of Initial-based Name Disambiguation on Measurements of Large-scale Coauthorship Networks\",\n        \"Advancing Code Sharing in the Computational Biology Community\",\n        \"The Rise of GitHub in Scholarly Publications\"\n    ],\n    \"reasoning\": \"Selected these 65 references because they represent foundational work and recent advances in An analysis of the effects of sharing research data, code, and preprints on citations, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}