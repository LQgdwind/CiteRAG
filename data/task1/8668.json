{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n\nPaper Abstract: Abstract When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research. 00footnotetext: â€ Corresponding author. liuyuisanai@gmail.com00footnotetext: First author contact: lt.wang@mail.utoronto.ca\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"The option-critic architecture\",\n        \"Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective\",\n        \"Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning\",\n        \"Accelerating robotic reinforcement learning via parameterized action primitives\",\n        \"How would surround vehicles move? a unified framework for maneuver classification and motion prediction\",\n        \"Epsilon: An efficient planning system for automated vehicles in highly interactive environments\",\n        \"The promise of hierarchical reinforcement learning\",\n        \"Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization & topology analysis\",\n        \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\",\n        \"Deep q-learning from demonstrations\",\n        \"Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms\",\n        \"The construction of movement with behavior-specific and behavior-independent modules\",\n        \"Compositional imitation learning: Explaining and executing one task at a time\",\n        \"Learning motor primitives for robotics\",\n        \"Robot learning from demonstration by constructing skill trees\",\n        \"A software package for sequential quadratic programming\",\n        \"Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning\",\n        \"Adaptive sampling-based motion planning with a non- conservatively defensive strategy for autonomous driving\",\n        \"Cirl: Controllable imitative reinforcement learning for vision-based self-driving\",\n        \"Improved deep reinforcement learning with expert demonstrations for urban autonomous driving\",\n        \"From motor control to team play in simulated humanoid football\",\n        \"Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios\",\n        \"Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks\",\n        \"Neural probabilistic motor primitives for humanoid control\",\n        \"Catch & carry: reusable neural controllers for vision-guided whole-body tasks\",\n        \"Overcoming exploration in reinforcement learning with demonstrations\",\n        \"Awac: Accelerating online reinforcement learning with offline datasets\",\n        \"Learning to simulate self-driven particles system with coordinated policy optimization\",\n        \"Accelerating reinforcement learning with learned skill priors\",\n        \"Keyframing the future: Keyframe discovery for visual prediction and planning\",\n        \"Reinforcement learning of motor skills with policy gradients\",\n        \"Learning complex dexterous manipulation with deep reinforcement learning and demonstrations\",\n        \"Learning transferable motor skills with hierarchical latent mixture policies\",\n        \"Reinforcement learning with sparse rewards using guidance from offline demonstration\",\n        \"A reduction of imitation learning and structured prediction to no-regret online learning\",\n        \"Proximal policy optimization algorithms\",\n        \"Discovering motor programs by recomposing demonstrations\",\n        \"Safety-enhanced autonomous driving using interpretable sensor fusion transformer\",\n        \"Learning to drive using inverse reinforcement learning and deep q-networks\",\n        \"Mastering the game of go with deep neural networks and tree search\",\n        \"Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning\",\n        \"Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards\",\n        \"Grandmaster level in starcraft ii using multi-agent reinforcement learning\",\n        \"Hierarchical adaptable and transferable networks (hatn) for driving behavior prediction\",\n        \"Socially-compatible behavior design of autonomous vehicles with verification on real human data\",\n        \"Transferable and adaptable driving behavior prediction\",\n        \"Social interactions for autonomous driving: A review and perspectives\",\n        \"Learning compositional models of robot skills for task and motion planning\",\n        \"Efficient uncertainty-aware decision-making for automated driving using guided branching\",\n        \"Accelerating reinforcement learning for autonomous driving using task-agnostic and ego-centric motion skills\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}