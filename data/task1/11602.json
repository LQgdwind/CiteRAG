{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 55 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Effective Authorship Attribution: Integrating Class-Incremental Learning\n\nPaper Abstract: Abstract Authorship Attribution (AA) is the process of attributing an unidentified document to its true author from a predefined group of known candidates, each possessing multiple samples. The nature of AA necessitates accommodating emerging new authors, as each individual must be considered unique. This uniqueness can be attributed to various factors, including their stylistic preferences, areas of expertise, gender, cultural background, and other personal characteristics that influence their writing. These diverse attributes contribute to the distinctiveness of each author, making it essential for AA systems to recognize and account for these variations. However, current AA benchmarks commonly overlook this uniqueness and frame the problem as a closed-world classification, assuming a fixed number of authors throughout the system’s lifespan and neglecting the inclusion of emerging new authors. This oversight renders the majority of existing approaches ineffective for real-world applications of AA, where continuous learning is essential. These inefficiencies manifest as current models either resist learning new authors or experience catastrophic forgetting, where the introduction of new data causes the models to lose previously acquired knowledge. To address these inefficiencies, we propose redefining AA as Class-Incremental Learning (CIL), where new authors are introduced incrementally after the initial training phase, allowing the system to adapt and learn continuously. To achieve this, we briefly examine subsequent CIL approaches introduced in other domains. Moreover, we have adopted several well-known CIL methods, along with an examination of their strengths and weaknesses in the context of AA. Additionally, we outline potential future directions for advancing CIL AA systems. As a result, our paper can serve as a starting point for evolving AA systems from closed-world models to continual learning through CIL paradigms.\n\n{retrieved_text}\n\nGenerate exactly 55 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 55\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Authorship attribution\",\n        \"Computational methods in authorship attribution\",\n        \"A survey of modern authorship attribution methods\",\n        \"Authorship attribution using word sequences\",\n        \"Effects of age and gender on blogging.\",\n        \"Authorship identification using ensemble learning\",\n        \"Stylometric analysis for authorship attribution on twitter\",\n        \"The word2vec graph model for author attribution and genre detection in literary analysis\",\n        \"Same or different? diff-vectors for authorship analysis\",\n        \"Authorship attribution for forensic investigation with thousands of authors\",\n        \"Code authorship attribution: Methods and challenges\",\n        \"Malware authorship attribution: Unmasking the culprits behind malicious software\",\n        \"Sentiment analysis algorithms and applications: A survey\",\n        \"Text classification algorithms: A survey\",\n        \"Semantic and Similarity Measure Methods for Plagiarism Detection of Students’ Assignments\",\n        \"A survey on few-shot class-incremental learning\",\n        \"Class-incremental learning using diffusion model for distillation and replay\",\n        \"Dense network expansion for class incremental learning\",\n        \"Few-shot class-incremental learning via compact and separable features for fine-grained vehicle recognition\",\n        \"Efficient data perturbation for privacy preserving and accurate data stream mining\",\n        \"Class-incremental learning: survey and performance evaluation on image classification\",\n        \"Deep class-incremental learning: A survey\",\n        \"Online continual learning in image classification: An empirical survey\",\n        \"The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects\",\n        \"Catastrophic forgetting in connectionist networks\",\n        \"Catastrophic interference in connectionist networks: The sequential learning problem\",\n        \"Incremental learning using conditional adversarial networks\",\n        \"Continual learning with deep generative replay\",\n        \"Riemannian walk for incremental learning: Understanding forgetting and intransigence\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Rotate your networks: Better weight consolidation and less catastrophic forgetting\",\n        \"Gradient episodic memory for continual learning\",\n        \"Distilling the knowledge in a neural network\",\n        \"icarl: Incremental classifier and representation learning\",\n        \"Learning without forgetting\",\n        \"Learning a unified classifier incrementally via rebalancing\",\n        \"Augmented geometric distillation for data-free incremental person reid\",\n        \"T5 meets tybalt: Author attribution in early modern english drama using large language models\",\n        \"Bertaa: Bert fine-tuning for authorship attribution\",\n        \"Deesil: Deep-shallow incremental learning\",\n        \"Lifelong machine learning with deep streaming linear discriminant analysis\",\n        \"Open set authorship attribution toward demystifying victorian periodicals\",\n        \"Cross-domain authorship attribution: Author identification using a multi-aspect ensemble approach.\",\n        \"Moving towards open set incremental learning: Readily discovering new authors\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Authorship attribution and verification with many authors and limited data\",\n        \"Authorship attribution with topic models\",\n        \"Whodunit? learning to contrast for authorship attribution\",\n        \"Rcv1: A new benchmark collection for text categorization research\",\n        \"Memory aware synapses: Learning what (not) to forget\",\n        \"Semantic drift compensation for class-incremental learning\",\n        \"Profiling haters on twitter using statistical and contextualized embeddings\",\n        \"Source code authorship attribution using long short-term memory based networks\",\n        \"Recognizing and imitating programmer style: Adversaries in program authorship attribution\",\n        \"Large-scale and language-oblivious code authorship identification\"\n    ],\n    \"reasoning\": \"Selected these 55 references because they represent foundational work and recent advances in Towards Effective Authorship Attribution: Integrating Class-Incremental Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}