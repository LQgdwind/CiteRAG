{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Structured Voronoi Sampling\n\nPaper Abstract: Abstract Gradient-based sampling algorithms have demonstrated their effectiveness in text generation, especially in the context of controlled text generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (svs). In an experimental setup where the reference distribution is known, we show that the empirical distribution of svs samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, svs is able to generate fluent and diverse samples while following the control targets significantly better than other methods. https://github.com/AfraAmini/svs\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Transport Equation and Cauchy Problem for Non-Smooth Vector Fields\",\n        \"Mathematical Methods of Classical Mechanics\",\n        \"Pyro: Deep universal probabilistic programming\",\n        \"Stan: A probabilistic programming language\",\n        \"Diffusion models beat gans on image synthesis\",\n        \"A measure-theoretic characterization of tight language models\",\n        \"Hybrid Monte Carlo\",\n        \"RealToxicityPrompts: Evaluating neural toxic degeneration in language models\",\n        \"Hafez: an interactive poetry generation system\",\n        \"Oops I took a gradient: Scalable sampling for discrete distributions\",\n        \"Monte Carlo sampling methods using Markov chains and their applications\",\n        \"Denoising diffusion probabilistic models\",\n        \"Towards decoding as continuous optimisation in neural machine translation\",\n        \"The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo\",\n        \"The curious case of neural text degeneration\",\n        \"Learning to write with cooperative discriminators\",\n        \"Hamiltonian-preserving schemes for the Liouville equation with discontinuous potentials\",\n        \"GeDi: Generative discriminator guided sequence generation\",\n        \"Controlled text generation as continuous optimization with multiple constraints\",\n        \"Constrained sampling from language models via langevin dynamics in embedding spaces\",\n        \"The Variational Principles of Mechanics\",\n        \"Diffusion-LM improves controllable text generation\",\n        \"DExperts: Decoding-time controlled text generation with experts and anti-experts\",\n        \"RoBERTa: A robustly optimized BERT pretraining approach\",\n        \"Reflected diffusion models\",\n        \"A new rejection sampling method for truncated multivariate Gaussian random variables restricted to convex sets\",\n        \"Locally typical sampling\",\n        \"Equation of state calculations by fast computing machines\",\n        \"Reflection, refraction, and Hamiltonian monte carlo\",\n        \"Probabilistic inference using Markov chain Monte Carlo methods\",\n        \"MCMC using Hamiltonian dynamics\",\n        \"Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods\",\n        \"The E2E dataset: New challenges for end-to-end generation\",\n        \"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics\",\n        \"Auxiliary-variable exact hamiltonian monte carlo samplers for binary distributions\",\n        \"Composable effects for flexible and accelerated probabilistic programming in numpyro\",\n        \"COLD decoding: Energy-based constrained text generation with Langevin dynamics\",\n        \"Enhanced gradient-based MCMC in discrete spaces\",\n        \"Palette: Image-to-image diffusion models\",\n        \"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP\",\n        \"Societal biases in language generation: Progress and challenges\",\n        \"Improved techniques for training score-based generative models\",\n        \"Accurate and efficient simulations of hamiltonian mechanical systems with discontinuous potentials\",\n        \"Neural text generation with unlikelihood training\",\n        \"Bayesian learning via stochastic gradient Langevin dynamics\",\n        \"Transformers: State-of-the-art natural language processing\",\n        \"FUDGE: Controlled text generation with future discriminators\",\n        \"A Langevin-like sampler for discrete distributions\",\n        \"Continuous relaxations for discrete hamiltonian monte carlo\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Structured Voronoi Sampling, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}