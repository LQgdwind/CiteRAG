{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 199 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses\n\nPaper Abstract: Abstract As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.\n\n{retrieved_text}\n\nGenerate exactly 199 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 199\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep learning with differential privacy\",\n        \"Turning your weakness into a strength: Watermarking deep neural networks by backdooring\",\n        \"Venomave: Clean-label poisoning against speech recognition\",\n        \"Bullseye polytope: A scalable clean-label poisoning attack with improved transferability\",\n        \"Baffle: Backdoor detection via feedback-based federated learning\",\n        \"Possible malware found hidden inside images from the ImageNet dataset\",\n        \"Synthesizing robust adversarial examples\",\n        \"On the impossibility of cryptography with tamperable randomness\",\n        \"How to backdoor federated learning\",\n        \"A little is enough: Circumventing defenses for distributed learning\",\n        \"Influence functions in deep learning are fragile\",\n        \"Analyzing federated learning through an adversarial lens\",\n        \"Support vector machines under adversarial label noise\",\n        \"Poisoning attacks against support vector machines\",\n        \"Evasion attacks against machine learning at test time\",\n        \"Machine learning with adversaries: Byzantine tolerant gradient descent\",\n        \"Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff\",\n        \"Dp-instahide: Provably defusing poisoning and backdoor attacks with differentially private data augmentations\",\n        \"Analysis of causative attacks against svms learning from data streams\",\n        \"Leaf: A benchmark for federated settings\",\n        \"Understanding distributed poisoning attack in federated learning\",\n        \"Data poisoning attacks to local differential privacy protocols\",\n        \"Learning from untrusted data\",\n        \"Detecting backdoor attacks on deep neural networks by activation clustering\",\n        \"Backdoor attacks on federated meta-learning\",\n        \"Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks\",\n        \"Robust covariance and scatter matrix estimation under huber’s contamination model\",\n        \"BadNL: Backdoor attacks against nlp models\",\n        \"Targeted backdoor attacks on deep learning systems using data poisoning\",\n        \"Refit: a unified watermark removal framework for deep learning systems with limited data\",\n        \"Distributed statistical machine learning in adversarial settings: Byzantine gradient descent\",\n        \"Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition\",\n        \"Sentinet: Detecting localized universal attack against deep learning systems\",\n        \"Emnist: Extending mnist to handwritten letters\",\n        \"Certified adversarial robustness via randomized smoothing\",\n        \"A backdoor attack against lstm-based text classification systems\",\n        \"Recent advances in algorithmic high-dimensional robust statistics\",\n        \"Being robust (in high dimensions) can be practical\",\n        \"Robust estimators in high-dimensions without the computational intractability\",\n        \"Sever: A robust meta-algorithm for stochastic optimization\",\n        \"Trojan attack on deep generative models in autonomous driving\",\n        \"The automatic robustness of minimum distance functionals\",\n        \"Calibrating noise to sensitivity in private data analysis\",\n        \"Foggysight: A scheme for facial lookup privacy\",\n        \"Robust physical-world attacks on deep learning visual classification\",\n        \"Poisoning attacks to graph-based recommender systems\",\n        \"Local model poisoning attacks to byzantine-robust federated learning\",\n        \"Influence function based data poisoning attacks to top-n recommender systems\",\n        \"Learning to confuse: Generating training time adversarial data with auto-encoder\",\n        \"Preventing unauthorized use of proprietary data: Poisoning for secure dataset release\",\n        \"Model inversion attacks that exploit confidence information and basic countermeasures\",\n        \"Attack-resistant federated learning with residual-based reweighting\",\n        \"Mitigating sybils in federated learning poisoning\",\n        \"Robust estimation and generative adversarial nets\",\n        \"Generative adversarial nets for robust scatter estimation: A proper scoring rule perspective\",\n        \"Face-off: Adversarial face obfuscation\",\n        \"Strip: A defence against trojan attacks on deep neural networks\",\n        \"Backdoor attacks and countermeasures on deep learning: a comprehensive review\",\n        \"Inverting gradients–How easy is it to break privacy in federated learning?\",\n        \"Witches’ brew: Industrial scale data poisoning via gradient matching\",\n        \"What doesn’t kill you makes you robust (er): Adversarial training against poisons and backdoors\",\n        \"Maxup: A simple way to improve generalization of neural network training\",\n        \"Generative adversarial nets\",\n        \"Badnets: Identifying vulnerabilities in the machine learning model supply chain\",\n        \"Practical poisoning attacks on neural networks\",\n        \"Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems\",\n        \"Robust statistics: the approach based on influence functions, volume 196\",\n        \"On the effectiveness of mitigating data poisoning attacks with gradient shaping\",\n        \"Targeted poisoning attacks on social recommender systems\",\n        \"Unlearnable examples: Making personal data unexploitable\",\n        \"One-pixel signature: Characterizing cnn models for backdoor detection\",\n        \"Metapoison: Practical general-purpose clean-label data poisoning\",\n        \"Robust estimation of a location parameter\",\n        \"Robust statistics, volume 523\",\n        \"Manipulating machine learning: Poisoning attacks and countermeasures for regression learning\",\n        \"Subpopulation data poisoning attacks\",\n        \"Auditing differentially private machine learning: How private is private sgd?\",\n        \"Model-reuse attacks on deep learning systems\",\n        \"Certified robustness of nearest neighbors against data poisoning attacks\",\n        \"Intrinsic certified robustness of bagging against data poisoning attacks\",\n        \"Learning in the presence of malicious errors\",\n        \"Auto-encoding variational bayes\",\n        \"Trojdrl: Evaluation of backdoor attacks on deep reinforcement learning\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Learning halfspaces with malicious noise\",\n        \"Understanding black-box predictions via influence functions\",\n        \"Stronger data poisoning attacks break data sanitization defenses\",\n        \"Universal litmus patterns: Revealing backdoor attacks in cnns\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Adversarial machine learning–industry perspectives\",\n        \"Agnostic estimation of mean and covariance\",\n        \"Tiny imagenet visual recognition challenge\",\n        \"Certified robustness to adversarial examples with differential privacy\",\n        \"Deep partition aggregation: Provable defense against general poisoning attacks\",\n        \"Data poisoning attacks on factorization-based collaborative filtering, 2016\",\n        \"Principled approaches to robust machine learning and beyond\",\n        \"Rsa: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets\",\n        \"Invisible backdoor attacks on deep neural networks via steganography and regularization\",\n        \"Learning to detect malicious clients for robust federated learning\",\n        \"Backdoor learning: A survey\",\n        \"Data poisoning attacks on stochastic bandits, 2019\",\n        \"Fine-pruning: Defending against backdooring attacks on deep neural networks\",\n        \"Removing backdoor-based watermarks in neural networks with limited data\",\n        \"Backdoor attacks and defenses in feature-partitioned collaborative learning\",\n        \"Delving into transferable adversarial examples and black-box attacks\",\n        \"Trojaning attack on neural networks\",\n        \"Neural trojans\",\n        \"Biometric backdoors: A poisoning attack against unsupervised template updating\",\n        \"Threats to federated learning: A survey\",\n        \"Nic: Detecting adversarial samples with neural network invariant checking\",\n        \"Data poisoning attacks in contextual bandits\",\n        \"Data poisoning against differentially-private learners: attacks and defenses\",\n        \"Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners\",\n        \"Learning under p𝑝p-tampering attacks\",\n        \"The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure\",\n        \"Data poisoning attacks in multi-party learning\",\n        \"Using machine teaching to identify optimal training-set attacks on machine learners\",\n        \"Spambayes: Effective open-source, bayesian based, email classification system\",\n        \"The hidden vulnerability of distributed learning in byzantium\",\n        \"Towards data poisoning attacks in crowd sensing systems\",\n        \"Universal adversarial perturbations\",\n        \"Towards poisoning of deep learning algorithms with back-gradient optimization\",\n        \"Poisoning attacks with generative adversarial nets\",\n        \"Exploiting machine learning to subvert your spam filter\",\n        \"Sok: Security and privacy in machine learning\",\n        \"Detection of adversarial training examples in poisoning attacks through anomaly detection\",\n        \"Label sanitization against label flipping poisoning attacks\",\n        \"Deep k-nn defense against clean-label data poisoning attacks\",\n        \"Robust aggregation for federated learning\",\n        \"A taxonomy and survey of attacks against machine learning\",\n        \"Robust estimation via robust gradient estimation\",\n        \"Defending neural backdoors via generative distribution modeling\",\n        \"Language models are unsupervised multitask learners\",\n        \"Backdoors in neural models of source code\",\n        \"Certified robustness to label-flipping attacks via randomized smoothing\",\n        \"Deepsigns: A generic watermarking framework for ip protection of deep learning models\",\n        \"Hidden trigger backdoor attacks\",\n        \"Baaan: Backdoor attacks against autoencoder and gan-based machine learning models\",\n        \"Facehack: Triggering backdoored facial recognition systems using facial characteristics\",\n        \"You autocomplete me: Poisoning vulnerabilities in neural code completion\",\n        \"Just how toxic is data poisoning? A unified benchmark for backdoor and data poisoning attacks\",\n        \"Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"Exploring backdoor poisoning attacks against malware classifiers\",\n        \"Poison frogs! Targeted clean-label poisoning attacks on neural networks\",\n        \"Fawkes: Protecting personal privacy against unauthorized deep learning models\",\n        \"Tensorclog: An imperceptible poisoning attack on deep neural network applications\",\n        \"Robust regression using repeated medians\",\n        \"Poisoning attacks on algorithmic fairness\",\n        \"Robust learning: Information theory and algorithms\",\n        \"Certified defenses for data poisoning attacks\",\n        \"Resilience: A criterion for learning in the presence of arbitrary outliers\",\n        \"Data poisoning attacks on federated machine learning\",\n        \"Natural backdoor attack on text data\",\n        \"Poisoned classifiers are not only backdoored, they are fundamentally broken\",\n        \"Can you really backdoor federated learning?\",\n        \"Intriguing properties of neural networks\",\n        \"Bypassing backdoor detection algorithms in deep learning\",\n        \"An embarrassingly simple approach for trojan attack in deep neural networks\",\n        \"Data poisoning attacks against federated learning systems\",\n        \"The space of transferable adversarial examples\",\n        \"Spectral signatures in backdoor attacks\",\n        \"A survey of sampling from contaminated distributions\",\n        \"Label-consistent backdoor attacks\",\n        \"Embedding watermarks into deep neural networks\",\n        \"Learning disjunction of conjunctions\",\n        \"Extracting and composing robust features with denoising autoencoders\",\n        \"Data poisoning attacks against differentially private recommender systems\",\n        \"Microsoft chatbot is taught to swear on Twitter\",\n        \"Customizing triggers with concealed data poisoning\",\n        \"Neural cleanse: Identifying and mitigating backdoor attacks in neural networks\",\n        \"Practical detection of trojan neural networks: Data-limited and data-free cases\",\n        \"Backdoor attacks against transfer learning with pre-trained deep learning models\",\n        \"Stop-and-go: Exploring backdoor attacks on deep reinforcement learning-based traffic congestion control systems\",\n        \"Rab: Provable robustness against backdoor attacks\",\n        \"Backdoor attacks on facial recognition in the physical world\",\n        \"Mitigating backdoor attacks in federated learning\",\n        \"Graph backdoor\",\n        \"Is feature selection secure against training data poisoning?\",\n        \"DBA: Distributed backdoor attacks against federated learning\",\n        \"Detecting ai trojans using meta neural analysis\",\n        \"Generative poisoning attack method against neural networks\",\n        \"Design of intentional backdoors in sequential models\",\n        \"Latent backdoor attacks on deep neural networks\",\n        \"Byzantine-robust distributed learning: Towards optimal statistical rates\",\n        \"Adversarial examples: Attacks and defenses for deep learning\",\n        \"Cutmix: Regularization strategy to train strong classifiers with localizable features\",\n        \"mixup: Beyond empirical risk minimization\",\n        \"Poisoning attack in federated learning using generative adversarial nets\",\n        \"Protecting intellectual property of deep neural networks with watermarking\",\n        \"A game-theoretic analysis of label flipping attacks on distributed support vector machines\",\n        \"Trojaning language models for fun and profit\",\n        \"Online data poisoning attacks\",\n        \"Backdoor attacks to graph neural networks\",\n        \"Efficient label contamination attacks against black-box learning models\",\n        \"Generalized resilience and robust statistics\",\n        \"Transferable clean-label poisoning attacks on deep neural nets\",\n        \"Deep leakage from gradients\",\n        \"Gangsweep: Sweep out neural backdoors by gan\",\n        \"General notions of statistical depth function\"\n    ],\n    \"reasoning\": \"Selected these 199 references because they represent foundational work and recent advances in Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}