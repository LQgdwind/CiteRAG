{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 254 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: From Show to Tell: A Survey on Deep Learning-based Image Captioning\n\nPaper Abstract: Abstract Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.\n\n{retrieved_text}\n\nGenerate exactly 254 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 254\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"“Language and visual perception associations: meta-analytic connectivity modeling of Brodmann Area 37\",\n        \"Language and visual perception associations: meta-analytic connectivity modeling of Brodmann Area 37\",\n        \"Automatic image captioning\",\n        \"Every picture tells a story: Generating sentences from images\",\n        \"Im2text: Describing images using 1 million captioned photographs\",\n        \"DeViSE: a deep visual-semantic embedding model\",\n        \"Unifying visual-semantic embeddings with multimodal neural language models\",\n        \"Deep fragment embeddings for bidirectional image sentence mapping\",\n        \"I2T: Image parsing to text description\",\n        \"Generating image descriptions using dependency relational patterns\",\n        \"Corpus-guided sentence generation of natural images\",\n        \"Composing simple image descriptions using web-scale n-grams\",\n        \"Choosing linguistics over vision to describe images\",\n        \"Midge: Generating image descriptions from computer vision detections\",\n        \"BabyTalk: Understanding and generating simple image descriptions\",\n        \"Treetalk: Composition and compression of trees for image descriptions\",\n        \"Automatic description generation from images: A survey of models, datasets, and evaluation measures\",\n        \"A survey on automatic image caption generation\",\n        \"A Comprehensive Survey of Deep Learning for Image Captioning\",\n        \"Framing image description as a ranking task: Data, models and evaluation metrics\",\n        \"Vision to Language: Methods, Metrics and Datasets\",\n        \"A survey on deep neural network-based image captioning\",\n        \"Image captioning: a comprehensive survey\",\n        \"Show and tell: A neural image caption generator\",\n        \"Going deeper with convolutions\",\n        \"Deep visual-semantic alignments for generating image descriptions\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\n        \"Long-term recurrent convolutional networks for visual recognition and description\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation\",\n        \"From captions to visual concepts and back\",\n        \"Guiding the Long-Short Term Memory model for Image Caption Generation\",\n        \"Image captioning with semantic attention\",\n        \"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\n        \"An Empirical Study of Language CNN for Image Captioning\",\n        \"StructCap: Structured Semantic Embedding for Image Captioning\",\n        \"GroupCap: Group-based Image Captioning with Structured Relevance and Diversity Constraints\",\n        \"Self-critical sequence training for image captioning\",\n        \"Deep residual learning for image recognition\",\n        \"Boosting image captioning with attributes\",\n        \"Semantic Compositional Networks for Visual Captioning\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Knowing when to look: Adaptive attention via a visual sentinel for image captioning\",\n        \"Rethinking the form of latent states in image captioning\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present\",\n        \"Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition\",\n        \"Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style\",\n        \"Stack-Captioning: Coarse-to-Fine Learning for Image Captioning\",\n        \"Review Networks for Caption Generation\",\n        \"“Recurrent Fusion Network for Image Captioning\",\n        \"“Seeing with Humans: Gaze-Assisted Neural Image Captioning\",\n        \"“Paying attention to descriptions generated by image captioning models\",\n        \"“Top-down visual saliency guided by captions\",\n        \"“Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention\",\n        \"“Boosted attention: Leveraging human attention for image captioning\",\n        \"“Bottom-up and top-down attention for image captioning and visual question answering\",\n        \"“Faster R-CNN: Towards real-time object detection with region proposal networks\",\n        \"“Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\n        \"“Reflective Decoding Network for Image Captioning\",\n        \"“Look Back and Predict Forward in Image Captioning\",\n        \"“Adaptively Aligned Image Captioning via Adaptive Attention Time\",\n        \"and Tell: Image Captioning with Recall Mechanism\",\n        \"“Context-aware visual policy network for fine-grained image captioning\",\n        \"“Areas of Attention for Image Captioning\",\n        \"“Exploring Visual Relationship for Image Captioning\",\n        \"“Aligning linguistic words and visual semantic units for image captioning\",\n        \"“Semi-supervised classification with graph convolutional networks\",\n        \"“Auto-Encoding Scene Graphs for Image Captioning\",\n        \"“Improving Image Captioning with Better Use of Captions\",\n        \"“Hierarchy Parsing for Image Captioning\",\n        \"“Attention is all you need\",\n        \"“Learning to Collocate Neural Modules for Image Captioning\",\n        \"“Entangled Transformer for Image Captioning\",\n        \"“Image Captioning: Transforming Objects into Words\",\n        \"“Normalized and Geometry-Aware Self-Attention Network for Image Captioning\",\n        \"“Attention on Attention for Image Captioning\",\n        \"“X-Linear Attention Networks for Image Captioning\",\n        \"“Meshed-Memory Transformer for Image Captioning\",\n        \"“Image captioning through image transformer\",\n        \"“Prophet Attention: Predicting Attention with Future Attention\",\n        \"“SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability\",\n        \"“Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network\",\n        \"“Dual-Level Collaborative Transformer for Image Captioning\",\n        \"“Aligning visual regions and textual concepts for semantic-grounded image representations\",\n        \"E. Learned-Miller\",\n        \"“RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words\",\n        \"“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n        \"“Training data-efficient image transformers & distillation through attention\",\n        \"“CPTR: Full Transformer Network for Image Captioning\",\n        \"“Learning Transferable Visual Models From Natural Language Supervision\",\n        \"“SimVLM: Simple visual language model pretraining with weak supervision\",\n        \"“How Much Can CLIP Benefit Vision-and-Language Tasks?” arXiv preprint arXiv:2107.06383\",\n        \"“ClipCap: CLIP Prefix for Image Captioning\",\n        \"“Universal Captioner: Long-Tail Vision-and-Language Model Training through Content-Style Separation\",\n        \"“Lxmert: Learning cross-modality encoder representations from transformers\",\n        \"“Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"“Oscar: Object-semantics aligned pre-training for vision-language tasks\",\n        \"“Unified Vision-Language Pre-Training for Image Captioning and VQA\",\n        \"“BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"“VinVL: Revisiting visual representations in vision-language models\",\n        \"“Scaling Up Vision-Language Pre-training for Image Captioning\",\n        \"“Long Short-Term Memory\",\n        \"“Neural Baby Talk\",\n        \"“AutoCaption: Image Captioning with Neural Architecture Search\",\n        \"“Convolutional image captioning\",\n        \"“Improving language understanding by generative pre-training\",\n        \"“Fast Image Caption Generation with Position Alignment\",\n        \"“Non-autoregressive image captioning with counterfactuals-critical multi-agent learning\",\n        \"“Iterative Back Modification for Faster Image Captioning\",\n        \"“Fast Sequence Generation with Multi-Agent Reinforcement Learning\",\n        \"Statistical Machine Translation. Cambridge University Press\",\n        \"“Sequence level training with recurrent neural networks\",\n        \"“Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"“BLEU: a method for automatic evaluation of machine translation\",\n        \"“Rouge: A package for automatic evaluation of summaries\",\n        \"“Deep reinforcement learning-based image captioning with embedding reward\",\n        \"“Improved Image Captioning via Policy Gradient Optimization of SPIDEr\",\n        \"“SPICE: Semantic Propositional Image Caption Evaluation\",\n        \"C. Lawrence Zitnick\",\n        \"“Actor-Critic Sequence Training for Image Captioning\",\n        \"“Self-critical n-step Training for Image Captioning\",\n        \"“XGPT: Cross-modal Generative Pre-Training for Image Captioning\",\n        \"“VirTex: Learning Visual Representations From Textual Annotations\",\n        \"“Neural Machine Translation of Rare Words with Subword Units\",\n        \"“Microsoft COCO: Common Objects in Context\",\n        \"“From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\n        \"“Conceptual captions: A cleaned\",\n        \"“Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts\",\n        \"“Captioning Images Taken by People Who Are Blind\",\n        \"“Learning deep representations of fine-grained visual descriptions\",\n        \"“Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards\",\n        \"F. Moreno-Noguer\",\n        \"everyone! context driven entity-aware captioning for news images\",\n        \"“TextCaps: a Dataset for Image Captioning with Reading Comprehension\",\n        \"“Connecting vision and language with localized narratives\",\n        \"“Zero-Shot Text-to-Image Generation\",\n        \"“YFCC100M: The new data in multimedia research\",\n        \"“WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\",\n        \"“Scaling up visual and vision-language representation learning with noisy text supervision\",\n        \"“Caltech-UCSD Birds 200\",\n        \"“Automated flower classification over a large number of classes\",\n        \"adapt and tell: Adversarial training of cross-domain image captioner\",\n        \"“Generating visual explanations\",\n        \"“Grounding visual explanations\",\n        \"“Generative adversarial text to image synthesis\",\n        \"“Transparent human evaluation for image captioning\",\n        \"“METEOR: An automatic metric for MT evaluation with improved correlation with human judgments\",\n        \"“NNEval: Neural network based evaluation metric for image captioning\",\n        \"“Learning to evaluate image captioning\",\n        \"“Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale\",\n        \"L. Anne Hendricks\",\n        \"“Measuring the diversity of automatic image descriptions\",\n        \"“Describing like humans: on diversity in image captioning\",\n        \"“On Diversity in Image Captioning: Metrics and Methods\",\n        \"“Object Hallucination in Image Captioning\",\n        \"Omission: A Fine-grained Evaluation for Image Captioning\",\n        \"“Towards unique and informative captioning of images\",\n        \"“From word embeddings to document distances\",\n        \"N. Ikizler-Cinbis\",\n        \"Control and Tell: A Framework for Generating Controllable and Grounded Captions\",\n        \"“Explore and Explain: Self-supervised Navigation and Recounting\",\n        \"“ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT\",\n        \"“Improving image captioning evaluation by considering inter references variance\",\n        \"“FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation\",\n        \"“UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning\",\n        \"“A neural compositional paradigm for image captioning\",\n        \"“Towards Diverse and Natural Image Descriptions via a Conditional GAN\",\n        \"“BERTScore: Evaluating Text Generation with BERT\",\n        \"“BERTTune: Fine-Tuning Neural Machine Translation with BERTScore\",\n        \"“TIGEr: text-to-image grounding for image caption evaluation\",\n        \"“Stacked Cross Attention for Image-Text Matching\",\n        \"“CLIPScore: A Reference-free Evaluation Metric for Image Captioning\",\n        \"“Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data\",\n        \"“Captioning Images with Diverse Objects\",\n        \"“nocaps: novel object captioning at scale\",\n        \"“Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects\",\n        \"“Pointing Novel Objects in Image Captioning\",\n        \"“Decoupled Novel Object Captioner\",\n        \"“Guided Open Vocabulary Image Captioning with Constrained Beam Search\",\n        \"“VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\",\n        \"“Unpaired image captioning by language pivoting\",\n        \"“Unsupervised image captioning\",\n        \"“Towards unsupervised image captioning with shared multimodal embeddings\",\n        \"“Unpaired image captioning via scene graph alignments\",\n        \"“Recurrent relational memory network for unsupervised image captioning\",\n        \"“Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach\",\n        \"“Unpaired Image Captioning with Semantic-Constrained Self-Learning\",\n        \"“RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning\",\n        \"“DenseCap: Fully convolutional Localization Networks for Dense Captioning\",\n        \"“Dense captioning with joint inference and visual context\",\n        \"“Learning object context for dense captioning\",\n        \"“Context and attribute grounded dense captioning\",\n        \"“Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning\",\n        \"“A hierarchical approach for generating descriptive image paragraphs\",\n        \"“Recurrent topic-transition GAN for visual paragraph generation\",\n        \"“Show and Tell More: Topic-Oriented Multi-Sentence Image Captioning\",\n        \"“Diverse and coherent paragraph generation from images\",\n        \"“Curiosity-driven reinforcement learning for diverse visual paragraph generation\",\n        \"“TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\",\n        \"“Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning\",\n        \"“Improving OCR-based Image Captioning by Incorporating Geometrical Relationship\",\n        \"“Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps\",\n        \"“Towards Accurate Text-based Image Captioning with Content Diversity Exploration\",\n        \"“Learning to describe differences between pairs of similar images\",\n        \"“Robust Change Captioning\",\n        \"“Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning\",\n        \"“Image Difference Captioning with Instance-Level Fine-Grained Feature Representation\",\n        \"“Viewpoint-Agnostic Change Captioning With Cycle Consistency\",\n        \"“Image Change Captioning by Learning from an Auxiliary Task\",\n        \"“Diverse Beam Search for Improved Description of Complex Scenes\",\n        \"“Contrastive learning for image captioning\",\n        \"“Generating diverse and descriptive image captions using visual paraphrases\",\n        \"“Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space\",\n        \"“Sequential latent spaces for modeling the intention during diverse image captioning\",\n        \"“Variational structured semantic inference for diverse image captioning\",\n        \"“Diverse image captioning with context-object split latent spaces\",\n        \"diverse and accurate image captioning guided by part-of-speech\",\n        \"“Multilingual image description with neural sequence models\",\n        \"“COCO-CN for cross-lingual image tagging\",\n        \"“Cross-lingual image caption generation\",\n        \"“Multi30K: Multilingual English-German Image Descriptions\",\n        \"“Fluency-guided cross-lingual image captioning\",\n        \"“Unpaired cross-lingual image caption generation with self-supervised rewards\",\n        \"“On the Automatic Generation of Medical Imaging Reports\",\n        \"“Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation\",\n        \"“Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation\",\n        \"“Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation\",\n        \"“Automatic Caption Generation for News Images\",\n        \"“Transform and Tell: Entity-Aware News Image Captioning\",\n        \"“Visual News: Benchmark and Challenges in News Image Captioning\",\n        \"“Journalistic Guidelines Aware News Image Captioning\",\n        \"“Automatic alt-text: Computer-generated image descriptions for blind users on a social network service\",\n        \"“Attend to you: Personalized image captioning with context sequence memory networks\",\n        \"“Towards personalized image captioning via multimodal memory networks\",\n        \"“Learning Long-and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption\",\n        \"“StyleNet: Generating Attractive Visual Captions with Styles\",\n        \"“Semstyle: Learning to generate stylised image captions using unaligned text\",\n        \"“Mscap: Multi-style image captioning with unpaired stylized text\",\n        \"“MemCap: Memorizing style knowledge for image captioning\",\n        \"“Engaging image captioning via personality\",\n        \"“Intention oriented image captions with guiding objects\",\n        \"“Connecting What to Say With Where to Look by Modeling Human Attention Traces\",\n        \"“Human-like Controllable Image Captioning with Verb-specific Semantic Roles\",\n        \"“Say as you wish: Fine-grained control of image caption generation with abstract scene graphs\",\n        \"“Comprehensive image captioning via scene graph decomposition\",\n        \"“Length-controllable image captioning\",\n        \"edit and tell: A framework for editing image captions\",\n        \"“Focused evaluation for image description with binary forced-choice tasks\",\n        \"“Going beneath the surface: Evaluating image captioning for grammaticality\",\n        \"“Cross-modal Coherence Modeling for Caption Generation\",\n        \"“UNITER: UNiversal Image-TExt Representation Learning\",\n        \"“Axiomatic attribution for deep networks\"\n    ],\n    \"reasoning\": \"Selected these 254 references because they represent foundational work and recent advances in From Show to Tell: A Survey on Deep Learning-based Image Captioning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}