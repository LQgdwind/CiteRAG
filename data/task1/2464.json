{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 279 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Understanding in Artificial Intelligence\n\nPaper Abstract: Abstract Current Artificial Intelligence (AI) methods, most based on deep learning, have facilitated progress in several fields, including computer vision and natural language understanding. The progress of these AI methods is measured using benchmarks designed to solve challenging tasks, such as visual question answering. A question remains of how much understanding is leveraged by these methods and how appropriate are the current benchmarks to measure understanding capabilities. To answer these questions, we have analysed existing benchmarks and their understanding capabilities, defined by a set of understanding capabilities, and current research streams. We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.\n\n{retrieved_text}\n\nGenerate exactly 279 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 279\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep learning\",\n        \"Deep learning in neural networks: An overview\",\n        \"Learning internal representations by error propagation. parallel distributed processing, explorations in the microstructure of cognition, Foundations\",\n        \"Same-different problems strain convolutional neural networks\",\n        \"Clevr: A diagnostic dataset for compositional language and elementary visual reasoning\",\n        \"Education and mind in the knowledge age\",\n        \"Critique of Pure Reason\",\n        \"The limits of reason\",\n        \"Building machines that learn and think like people\",\n        \"The measure of intelligence\",\n        \"Playing atari with deep reinforcement learning\",\n        \"Mastering the game of go without human knowledge\",\n        \"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images\",\n        \"Is bert really robust? natural language attack on text classification and entailment\",\n        \"Measuring abstract reasoning in neural networks\",\n        \"Ai evaluation: On broken yardsticks and measurement scales\",\n        \"Building thinking machines by solving animal cognition tasks\",\n        \"Making the v in vqa matter: Elevating the role of image understanding in visual question answering\",\n        \"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision\",\n        \"Human-level concept learning through probabilistic program induction\",\n        \"Recognition-by-components: a theory of human image understanding.\",\n        \"Understanding human language: Can nlp and deep learning help?\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Microsoft coco: Common objects in context\",\n        \"The princeton shape benchmark\",\n        \"Glue: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"Squad: 100,000+ questions for machine comprehension of text\",\n        \"The lambada dataset: Word prediction requiring a broad discourse context\",\n        \"Break it down: A question understanding benchmark\",\n        \"C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset\",\n        \"Vqa: Visual question answering\",\n        \"An analysis of visual question answering algorithms\",\n        \"Deep compositional question answering with neural module networks\",\n        \"Learning to reason: End-to-end module networks for visual question answering\",\n        \"Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation\",\n        \"Clevrer: Collision events for video representation and reasoning\",\n        \"SemEval-2019 task 10: Math question answering\",\n        \"Commonsense reasoning and commonsense knowledge in artificial intelligence\",\n        \"The pascal recognising textual entailment challenge\",\n        \"A corpus and evaluation framework for deeper understanding of commonsense stories\",\n        \"Swag: A large-scale adversarial dataset for grounded commonsense inference\",\n        \"Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches\",\n        \"Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension\",\n        \"Commonsenseqa: A question answering challenge targeting commonsense knowledge\",\n        \"The narrativeqa reading comprehension challenge\",\n        \"Newsqa: A machine comprehension dataset\",\n        \"Seq2sql: Generating structured queries from natural language using reinforcement learning\",\n        \"Graph2seq: Graph to sequence learning with attention-based neural networks\",\n        \"Hotpotqa: A dataset for diverse, explainable multi-hop question answering\",\n        \"Worldtree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference\",\n        \"On the capabilities and limitations of reasoning for natural language understanding\",\n        \"Event2mind: Commonsense inference on events\",\n        \"Winograd schema-knowledge extraction using narrative chains\",\n        \"Conceptnet 5.5: An open multilingual graph of general knowledge\",\n        \"Webchild 2.0: Fine-grained commonsense knowledge distillation\",\n        \"Never-ending learning\",\n        \"Atomic: An atlas of machine commonsense for if-then reasoning\",\n        \"Cracking the contextual commonsense code: Understanding commonsense reasoning aptitude of deep contextual representations\",\n        \"Pingan smart health and SJTU at COIN - shared task: utilizing pre-trained language models and common-sense knowledge in machine reading tasks\",\n        \"Commonsense for generative multi-hop question answering tasks\",\n        \"Towards generalizable neuro-symbolic systems for commonsense question answering\",\n        \"Commonsense inference in human-robot communication\",\n        \"Sequence to sequence learning for event prediction\",\n        \"Diversity-aware event prediction based on a conditional variational autoencoder with reconstruction\",\n        \"Can a gorilla ride a camel? learning semantic plausibility from text\",\n        \"How pre-trained word representations capture commonsense physical comparisons\",\n        \"Gated graph sequence neural networks\",\n        \"Amr-to-text generation with synchronous node replacement grammar\",\n        \"Extracting common inference patterns from semi-structured explanations\",\n        \"The next decade in ai: Four steps towards robust artificial intelligence (2020)\",\n        \"A. S. d’Avila Garcez\",\n        \"A multi-world approach to question answering about real-world scenes based on uncertain input\",\n        \"Visual question answering: Datasets\",\n        \"Neural-symbolic vqa: Disentangling reasoning from vision and language understanding\",\n        \"Ddrprog: A clevr differentiable dynamic reasoning programmer\",\n        \"L. van der Maaten\",\n        \"Explainable neural computation via stack neural module networks\",\n        \"Explainable and explicit visual reasoning over scene graphs\",\n        \"Hierarchical question-image co-attention for visual question answering\",\n        \"Modeling relationships in referential expressions with compositional modular networks\",\n        \"Learning to compose task-specific tree structures\",\n        \"Task-driven modular networks for zero-shot compositional learning\",\n        \"Recursive deep models for semantic compositionality over a sentiment treebank\",\n        \"Improved semantic representations from tree-structured long short-term memory networks\",\n        \"Cooperative learning of disjoint syntax and semantics\",\n        \"The emergence of organizing structure in conceptual representation\",\n        \"Just add functions: A neural-symbolic language model\",\n        \"Differentiable perturb-and-parse: Semi-supervised parsing with a structured variational autoencoder\",\n        \"Tutorial on variational autoencoders\",\n        \"Junction tree variational autoencoder for molecular graph generation\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Glove: Global vectors for word representation\",\n        \"Learning distributed representations of concepts using linear relational embedding\",\n        \"Holographic embeddings of knowledge graphs\",\n        \"Complex embeddings for simple link prediction\",\n        \"Deep learning for learning graph representations\",\n        \"Poincaré embeddings for learning hierarchical representations\",\n        \"Wordnet: a lexical database for english\",\n        \"Hyperbolic entailment cones for learning hierarchical embeddings\",\n        \"Representation tradeoffs for hyperbolic embeddings\",\n        \"Neural embeddings of graphs in hyperbolic space\",\n        \"An information flow model for conflict and fission in small groups\",\n        \"Continuous hierarchical representations with poincaré variational auto-encoders\",\n        \"Adam: A method for stochastic optimization\",\n        \"MNIST handwritten digit database (2010). URL http://yann.lecun.com/exdb/mnist/\",\n        \"Numerically accurate hyperbolic embeddings using tiling-based models\",\n        \"Learning continuous hierarchies in the lorentz model of hyperbolic geometry\",\n        \"Inferring concept hierarchies from text corpora via hyperbolic embeddings\",\n        \"Unsupervised hierarchy matching with optimal transport over hyperbolic spaces\",\n        \"Hyperbolic disk embeddings for directed acyclic graphs\",\n        \"A differentiable gaussian-like distribution on hyperbolic space for gradient-based learning\",\n        \"Knowledge graph embedding by translating on hyperplanes\",\n        \"The encyclopedia of applied linguistics\",\n        \"The obo foundry: coordinated evolution of ontologies to support biomedical data integration\",\n        \"Knowledge graph refinement: A survey of approaches and evaluation methods\",\n        \"N. Mihindukulasooriya\",\n        \"Bootstrapping knowledge graphs from images and text\",\n        \"Link prediction based on graph neural networks\",\n        \"A review of relational machine learning for knowledge graphs\",\n        \"Reasoning with neural tensor networks for knowledge base completion\",\n        \"A comprehensive survey of graph embedding: Problems\",\n        \"Knowledge graph embedding: A survey of approaches and applications\",\n        \"Embedding entities and relations for learning and inference in knowledge bases\",\n        \"Kbgan: Adversarial learning for knowledge graph embeddings\",\n        \"Object detection with deep learning: A review\",\n        \"Centermask : Real-time anchor-free instance segmentation\",\n        \"Fcos: Fully convolutional one-stage object detection\",\n        \"Solo: Segmenting objects by locations\",\n        \"in: Proceedings of the IEEE international conference on computer vision\",\n        \"Blendmask: Top-down meets bottom-up for instance segmentation\",\n        \"Draw: A recurrent neural network for image generation\",\n        \"repeat: Fast scene understanding with generative models\",\n        \"Faster attend-infer-repeat with tractable probabilistic models\",\n        \"Generative modeling of infinite occluded objects for compositional scene representation\",\n        \"Geometric modelling for computer vision\",\n        \"Picture: A probabilistic programming language for scene perception\",\n        \"Opendr: An approximate differentiable renderer\",\n        \"Neural scene de-rendering\",\n        \"Automatic graphics program generation using attention-based hierarchical decoder\",\n        \"Learning to infer graphics programs from hand-drawn images\",\n        \"Transforming auto-encoders\",\n        \"A neural-symbolic architecture for inverse graphics improved by lifelong meta-learning\",\n        \"Monet: Unsupervised scene decomposition and representation\",\n        \"Y. Thawornwattana\",\n        \"Multi-object representation learning with iterative variational inference\",\n        \"Genesis: Generative scene inference and sampling with object-centric latent representations\",\n        \"Space: Unsupervised object-oriented scene representation via spatial attention and decomposition\",\n        \"Generative hierarchical models for parts\",\n        \"A table detection method for pdf documents based on convolutional neural networks\",\n        \"Convolutional neural networks for page segmentation of historical document images\",\n        \"Multi-scale multi-task fcn for semantic page segmentation and table detection\",\n        \"A saliency-based convolutional neural network for table and chart detection in digitized documents\",\n        \"Deepdesrt: Deep learning for detection and structure recognition of tables in document images\",\n        \"Table detection using deep learning\",\n        \"Corpus conversion service: A machine learning platform to ingest documents at scale\",\n        \"Publaynet: largest dataset ever for document layout analysis\",\n        \"Rethinking table recognition using graph neural networks\",\n        \"Graph neural network for symbol detection on document images\",\n        \"E. ShafieiBavani\",\n        \"Multi-task model for comic book image analysis\",\n        \"Layoutlm: Pre-training of text and layout for document image understanding\",\n        \"Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning\",\n        \"Deepproblog: Neural probabilistic logic programming\",\n        \"Logic tensor networks: Deep learning and logical reasoning from data and knowledge\",\n        \"Neuro-symbolic= neural+ logical+ probabilistic\",\n        \"Neural logic machines\",\n        \"Logical neural networks\",\n        \"Neural logic networks\",\n        \"Utilizing a neural logic expert system in currency option trading\",\n        \"Inductive logic programming\",\n        \"Harnessing deep neural networks with logic rules\",\n        \"Problog: A probabilistic prolog and its applications in link discovery\",\n        \"On interpretability of artificial neural networks\",\n        \"Neural module networks\",\n        \"Learning to compose neural networks for question answering\",\n        \"Genetic programming\",\n        \"Genetic programming: on the programming of computers by means of natural selection\",\n        \"A. rahman Mohamed\",\n        \"Automating string processing in spreadsheets using input-output examples\",\n        \"Making neural programming architectures generalize via recursion\",\n        \"Unsupervised learning by program synthesis\",\n        \"Z3: An efficient smt solver\",\n        \"Comparing machines and humans on a visual categorization test\",\n        \"Deepcoder: Learning to write programs\",\n        \"Automatic program synthesis of long programs with a learned garbage collector\",\n        \"Differentiable functional program interpreters\",\n        \"Terpret: A probabilistic programming language for program induction\",\n        \"Synthesizing data structure transformations from input-output examples\",\n        \"Differentiable programs with neural libraries\",\n        \"Towards synthesizing complex programs from input-output examples\",\n        \"Combining symbolic and function evaluation expressions in neural programs\",\n        \"Learning compositional neural programs with recursive tree search and planning\",\n        \"Recent advances in neural program synthesis\",\n        \"Neural turing machines\",\n        \"A. Grabska-Barwińska\",\n        \"Learning algorithmic solutions to symbolic planning tasks with a neural computer\",\n        \"Learning by abstraction: The neural state machine\",\n        \"Neural programmer-interpreters\",\n        \"Neural program lattices\",\n        \"Memo: A deep network for flexible combination of episodic memories\",\n        \"Programming with a differentiable forth interpreter\",\n        \"Mindreaders: the cognitive basis of” theory of mind”\",\n        \"Representations of commonsense knowledge\",\n        \"Experience grounds language\",\n        \"Commonsense inference in natural language processing (coin)-shared task report\",\n        \"Introductory tutorial: Commonsense reasoning for natural language processing\",\n        \"Commonsense knowledge aware conversation generation with graph attention\",\n        \"Improving question answering over incomplete kbs with knowledge-aware reader\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Language models are few-shot learners\",\n        \"Story ending generation with incremental encoding and commonsense knowledge\",\n        \"Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge\",\n        \"Kagnet: Knowledge-aware graph networks for commonsense reasoning\",\n        \"Dynamic integration of background knowledge in neural nlu systems\",\n        \"Answering science exam questions using query rewriting with background knowledge\",\n        \"Incorporating structured commonsense knowledge in story completion\",\n        \"Ranking and selecting multi-hop knowledge paths to better predict human needs\",\n        \"Improving natural language inference using external knowledge in the science questions domain\",\n        \"Reasoning about actions and state changes by injecting commonsense knowledge\",\n        \"Reasoning with heterogeneous knowledge for commonsense machine comprehension\",\n        \"A multi-attention based neural network with external knowledge for story ending predicting task\",\n        \"Careful selection of knowledge to solve open book question answering\",\n        \"Improving question answering by commonsense-based pre-training\",\n        \"Explain yourself! leveraging language models for commonsense reasoning\",\n        \"Incorporating relation knowledge into commonsense reading comprehension with multi-task learning\",\n        \"Mcscript2. 0: A machine comprehension corpus focused on script events and participants\",\n        \"Record: Bridging the gap between human and machine commonsense reading comprehension\",\n        \"Mitre at semeval-2018 task 11: Commonsense reasoning without commonsense knowledge\",\n        \"Teaching machines to read and comprehend\",\n        \"Yuanfudao at semeval-2018 task 11: Three-way attention and relational knowledge for commonsense machine comprehension\",\n        \"Enhancing pre-trained language representations with rich knowledge for machine reading comprehension\",\n        \"Stochastic answer networks for machine reading comprehension\",\n        \"Simple and effective multi-paragraph reading comprehension\",\n        \"Microsoft coco captions: Data collection and evaluation server\",\n        \"Show and tell: A neural image caption generator\",\n        \"Conceptual captions: A cleaned\",\n        \"Ok-vqa: A visual question answering benchmark requiring external knowledge\",\n        \"Referitgame: Referring to objects in photographs of natural scenes\",\n        \"Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models\",\n        \"Generation and comprehension of unambiguous object descriptions\",\n        \"Lxmert: Learning cross-modality encoder representations from transformers\",\n        \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Vl-bert: Pre-training of generic visual-linguistic representations\",\n        \"Inferring the why in images\",\n        \"Answering visual what-if questions: From actions to predicted scene descriptions\",\n        \"From recognition to cognition: Visual commonsense reasoning\",\n        \"“what happens if…” learning to predict the effect of forces in images\",\n        \"Interpretable intuitive physics model\",\n        \"Social lstm: Human trajectory prediction in crowded spaces\",\n        \"Learning to act properly: Predicting and explaining affordances from images\",\n        \"Social gan: Socially acceptable trajectories with generative adversarial networks\",\n        \"Moviegraphs: Towards understanding human-centric situations from videos\",\n        \"S. Lacoste-Julien\",\n        \"Towards automatic learning of procedures from web instructional videos\",\n        \"Krishnacam: Using a longitudinal\",\n        \"H. Bagherinezhad\",\n        \"Temporal perception and prediction in ego-centric video\",\n        \"Anticipating visual representations from unlabeled video\",\n        \"What will happen next? forecasting player moves in sports videos\",\n        \"First-person activity forecasting with online inverse reinforcement learning\",\n        \"Stair actions: A video dataset of everyday home actions\",\n        \"C. Lawrence Zitnick\",\n        \"Visualcomet: Reasoning about the dynamic context of a still image\",\n        \"Video (language) modeling: a baseline for generative models of natural videos\",\n        \"Unsupervised learning of video representations using lstms\",\n        \"Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks\",\n        \"Generating videos with scene dynamics\",\n        \"Deep multi-scale video prediction beyond mean square error\",\n        \"High fidelity video prediction with large stochastic recurrent neural networks\",\n        \"Improved conditional vrnns for video prediction\",\n        \"Patch to the future: Unsupervised visual prediction\",\n        \"Recurrent network models for human dynamics\",\n        \"The pose knows: Video forecasting by generating pose futures\",\n        \"Forecasting human dynamics from static images\",\n        \"A hierarchical representation for future action prediction\",\n        \"Relational action forecasting\",\n        \"D. Polychronopoulos\"\n    ],\n    \"reasoning\": \"Selected these 279 references because they represent foundational work and recent advances in Understanding in Artificial Intelligence, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}