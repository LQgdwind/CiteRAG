{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 46 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Control-Centric Benchmark for Video Prediction\n\nPaper Abstract: Abstract Video is a promising source of knowledge for embodied agents to learn models of the world’s dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning (VP2), includes simulated environments with 111111 task categories and 310310310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction trajectories for each task category. A central design goal of our benchmark is to expose a simple interface – a single forward prediction call – so it is straightforward to evaluate almost any action-conditioned video prediction model. We then leverage our benchmark to study the effects of scaling model size, quantity of training data, and model ensembling by analyzing five highly-performant video prediction models, finding that while scale can improve perceptual quality when modeling visually diverse settings, other attributes such as uncertainty awareness can also aid planning performance. Additional visualizations and code are at this link.\n\n{retrieved_text}\n\nGenerate exactly 46 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 46\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"ROBEL: RObotics BEnchmarks for Learning with low-cost robots\",\n        \"Fitvid: Overfitting in pixel-level video prediction\",\n        \"Simulation as an engine of physical scene understanding\",\n        \"Physion: Evaluating physical prediction from vision in humans and machines\",\n        \"Openai gym\",\n        \"The cityscapes dataset for semantic urban scene understanding\",\n        \"Robonet: Large-scale multi-robot learning\",\n        \"A tutorial on the cross-entropy method\",\n        \"Stochastic video generation with a learned prior\",\n        \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\",\n        \"Deep visual foresight for planning robot motion\",\n        \"Unsupervised learning for physical interaction through video prediction\",\n        \"D4rl: Datasets for deep data-driven reinforcement learning\",\n        \"Vision meets robotics: The kitti dataset\",\n        \"Comparing correspondences: Video prediction with correspondence-wise losses\",\n        \"Rl unplugged: A suite of benchmarks for offline reinforcement learning\",\n        \"Maskvit: Masked visual pre-training for video prediction\",\n        \"World models\",\n        \"Flexible diffusion modeling of long videos\",\n        \"Gans trained by a two time-scale update rule converge to a local nash equilibrium\",\n        \"Cem-gd: Cross-entropy method with gradient descent planner for model-based reinforcement learning\",\n        \"Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments\",\n        \"Robodesk: A multi-task reinforcement learning benchmark\",\n        \"Pytorch image quality: Metrics for image quality assessment\",\n        \"igibson 2.0: Object-centric simulation for robot learning of everyday household tasks\",\n        \"On the limited memory BFGS method for large scale optimization\",\n        \"Challenges and opportunities in offline reinforcement learning from visual observations\",\n        \"Unsupervised learning of object structure and dynamics from videos\",\n        \"Deep dynamics models for learning dexterous manipulation\",\n        \"Action-conditional video prediction using deep networks in atari games\",\n        \"Pytorch: An imperative style, high-performance deep learning library\",\n        \"Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations\",\n        \"The cross-entropy method for combinatorial and continuous optimization\",\n        \"Reconciling intuitive physics and newtonian mechanics for colliding objects\",\n        \"Unsupervised learning of video representations using lstms\",\n        \"Deepmind control suite\",\n        \"Towards accurate generative models of video: A new metric & challenges\",\n        \"High fidelity video prediction with large stochastic recurrent neural networks\",\n        \"Masked conditional video diffusion for prediction, generation, and interpolation\",\n        \"Image quality assessment: from error visibility to structural similarity\",\n        \"Aggressive driving with model predictive path integral control\",\n        \"Videogpt: Video generation using vq-vae and transformers\",\n        \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning\",\n        \"MOPO: model-based offline policy optimization\",\n        \"The unreasonable effectiveness of deep features as a perceptual metric\",\n        \"robosuite: A modular simulation framework and benchmark for robot learning\"\n    ],\n    \"reasoning\": \"Selected these 46 references because they represent foundational work and recent advances in A Control-Centric Benchmark for Video Prediction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}