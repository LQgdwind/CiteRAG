{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 278 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Visual Tuning\n\nPaper Abstract: Abstract Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.\n\n{retrieved_text}\n\nGenerate exactly 278 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 278\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A survey on vision transformer\",\n        \"Transformers in vision: A survey\",\n        \"On the opportunities and risks of foundation models\",\n        \"A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\",\n        \"Visual chatgpt: Talking, drawing and editing with visual foundation models\",\n        \"Language models are few-shot learners\",\n        \"Efficient transformers: A survey\",\n        \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n        \"Meta pseudo labels\",\n        \"Coca: Contrastive captioners are image-text foundation models\",\n        \"Scaling vision transformers to 22 billion parameters\",\n        \"Palm-e: An embodied multimodal language model\",\n        \"A roadmap for big model\",\n        \"A comprehensive survey on transfer learning\",\n        \"Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks\",\n        \"Exploring versatile generative language model via parameter-efficient transfer learning\",\n        \"Lst: Ladder side-tuning for parameter and memory efficient transfer learning\",\n        \"Pca-sift: A more distinctive representation for local image descriptors\",\n        \"Brief: Binary robust independent elementary features\",\n        \"Orb: An efficient alternative to sift or surf\",\n        \"Deep learning\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Going deeper with convolutions\",\n        \"Deep residual learning for image recognition\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"A survey on deep transfer learning\",\n        \"Learning to learn\",\n        \"The kinetics human action video dataset\",\n        \"Attention is all you need\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Scaling up visual and vision-language representation learning with noisy text supervision\",\n        \"A generalist agent\",\n        \"Flava: A foundational language and vision alignment model\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"Revisiting weakly supervised pre-training of visual perception models\",\n        \"Receptive fields of single neurones in the cat’s striate cortex\",\n        \"Backpropagation and the brain\",\n        \"Atoms of recognition in human and computer vision\",\n        \"Vision: A computational investigation into the human representation and processing of visual information\",\n        \"The ecological approach to visual perception: classic edition\",\n        \"Human-level concept learning through probabilistic program induction\",\n        \"Ten lessons we have learned in the new” sparseland”: A short handbook for sparse neural network researchers\",\n        \"A Modern Introduction to Probability and Statistics: Understanding why and how\",\n        \"Towards a theoretical framework of out-of-distribution generalization\",\n        \"Who is closer: A computational method for domain gap evaluation\",\n        \"Bridging theory and algorithm for domain adaptation\",\n        \"On the theory of transfer learning: The importance of task diversity\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"2017 international conference on engineering and technology (ICET), 1–6 (Ieee, 2017)\",\n        \"A survey of convolutional neural networks: analysis, applications, and prospects\",\n        \"K. et al. Incorporating convolution designs into visual transformers\",\n        \"Mmnet: A model-based multimodal network for human action recognition in rgb-d videos\",\n        \"M. Learning spatiotemporal features with 3d convolutional networks\",\n        \"action recognition? a new model and the kinetics dataset\",\n        \"K. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification\",\n        \"C. X3d: Expanding architectures for efficient video recognition\",\n        \"Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, 47–54 (Springer, 2016)\",\n        \"Temporal convolutional networks for action segmentation and detection\",\n        \"M. 3d human pose estimation in video with temporal convolutions and semi-supervised training\",\n        \"R. et al. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction\",\n        \"International Conference on Learning Representations (2021)\",\n        \"International conference on machine learning, 10347–10357 (PMLR, 2021)\",\n        \"Transformer in transformer\",\n        \"L. et al. Tokens-to-token vit: Training vision transformers from scratch on imagenet\",\n        \"W. et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions\",\n        \"Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows\",\n        \"Z. et al. Video swin transformer\",\n        \"Conditional positional encodings for vision transformers\",\n        \"Twins: Revisiting the design of spatial attention in vision transformers\",\n        \"Shuffle transformer: Rethinking spatial shuffle for vision transformer\",\n        \"J. et al. Cmt: Convolutional neural networks meet vision transformers\",\n        \"Volo: Vision outlooker for visual recognition\",\n        \"A. Revisiting unreasonable effectiveness of data in deep learning era\",\n        \"K. et al. Masked autoencoders are scalable vision learners\",\n        \"Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training\",\n        \"Florence: A new foundation model for computer vision\",\n        \"Image as a foreign language: Beit pretraining for all vision and vision-language tasks\",\n        \"Climax: A foundation model for weather and climate\",\n        \"Language models generalize beyond natural proteins\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, 709–727 (Springer, 2022)\",\n        \"S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning\",\n        \"Visual prompt tuning for test-time domain adaptation\",\n        \"Zegclip: Towards adapting clip for zero-shot semantic segmentation\",\n        \"Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?\",\n        \"Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data\",\n        \"Understanding zero-shot adversarial robustness for large-scale models\",\n        \"Unleashing the power of visual prompting at the pixel level\",\n        \"Prosfda: Prompt learning based source-free domain adaptation for medical image segmentation\",\n        \"\\\\\\\\\\\\backslasha-la-carte prompt tuning (apt): Combining distinct data via composable prompting\",\n        \"Prompting and tuning: A two-stage unsupervised domain adaptive person re-identification method on vision transformer backbone\",\n        \"Lpt: Long-tailed prompt tuning for image classification\",\n        \"R. et al. Pointclip: Point cloud understanding by clip\",\n        \"P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting\",\n        \"Advances in Neural Information Processing Systems (2022)\",\n        \"Neural prompt search\",\n        \"Prompt generation networks for efficient adaptation of frozen vision transformers\",\n        \"Feature-proxy transformer for few-shot segmentation\",\n        \"Fine-grained retrieval prompt tuning\",\n        \"Compositional prompt tuning with motion cues for open-vocabulary video relation detection\",\n        \"Open-vocabulary object detection via vision and language knowledge distillation\",\n        \"Lion: Implicit vision prompt tuning\",\n        \"Learning to prompt for vision-language models\",\n        \"Understanding and mitigating overfitting in prompt tuning for vision-language models\",\n        \"Multi-prompt alignment for multi-source unsupervised domain adaptation\",\n        \"Zegot: Zero-shot segmentation through optimal transport of text prompts\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IV, 1–18 (Springer, 2022)\",\n        \"Prompt-aligned gradient for prompt tuning\",\n        \"A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning\",\n        \"Prompting through prototype: A prototype-based prompt learning on pretrained vision-language models\",\n        \"Lanit: Language-driven image-to-image translation for unlabeled data\",\n        \"Sgva-clip: Semantic-guided visual adapting of vision-language models for few-shot image classification\",\n        \"Language-aware soft prompting for vision & language foundation models\",\n        \"Dualcoop: Fast adaptation to multi-label recognition with limited annotations\",\n        \"Prompt learning with optimal transport for vision-language models\",\n        \"Cpl: Counterfactual prompt learning for vision and language models\",\n        \"Learning to decompose visual features with latent textual prompts\",\n        \"Galip: Generative adversarial clips for text-to-image synthesis\",\n        \"Z. Conditional prompt learning for vision-language models\",\n        \"Pointclip v2: Adapting clip for powerful 3d open-world learning\",\n        \"Unified vision and language prompt learning\",\n        \"Class-aware visual prompt tuning for vision-language pre-trained model\",\n        \"Maple: Multi-modal prompt learning\",\n        \"Multitask vision-language prompt tuning\",\n        \"Learning domain invariant prompt for vision-language models\",\n        \"Test-time prompt tuning for zero-shot generalization in vision-language models\",\n        \"Advances in neural information processing systems (2017)\",\n        \"A. Efficient parametrization of multi-domain deep neural networks\",\n        \"Incremental learning through deep adaptation\",\n        \"Conv-adapter: Exploring parameter efficient transfer learning for convnets\",\n        \"Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks\",\n        \"Pro-tuning: Unified prompt tuning for vision tasks\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI, 50–67 (Springer, 2022)\",\n        \"The Eleventh International Conference on Learning Representations (2023)\",\n        \"Tiny adapters for vision transformers (2023)\",\n        \"Towards efficient visual adaption via structural re-parameterization\",\n        \"Benchmarking detection transfer learning with vision transformers\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IX, 280–296 (Springer, 2022)\",\n        \"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, 446–462 (Springer, 2020)\",\n        \"Collaboration of pre-trained models makes better few-shot learner\",\n        \"C. Continual learning with transformers for image classification\",\n        \"International Conference on Learning Representations (2023)\",\n        \"K. Universal deep image compression via content-adaptive optimization with adapters\",\n        \"How to adapt your large-scale vision-and-language model\",\n        \"Clip-adapter: Better vision-language models with feature adapters\",\n        \"Tip-adapter: Training-free clip-adapter for better vision-language modeling\",\n        \"A simple long-tailed recognition baseline via vision-language model\",\n        \"Magma–multimodal augmentation of generative models through adapter-based finetuning\",\n        \"Hierarchical3d adapters for long video-to-text summarization\",\n        \"Hyperpelt: Unified parameter-efficient language model tuning for both language and vision-and-language tasks\",\n        \"Svl-adapter: Self-supervised adapter for vision-language pretrained models\",\n        \"Vision transformers are parameter-efficient audio-visual learners\",\n        \"Cross-modal adapter for text-video retrieval\",\n        \"Multimodal video adapter for parameter efficient video text retrieval\",\n        \"Vision transformer adapter for dense predictions\",\n        \"Parameter-efficient and student-friendly knowledge distillation\",\n        \"Adaptformer: Adapting vision transformers for scalable visual recognition\",\n        \"Convolutional bypasses are better vision transformer adapters\",\n        \"Rethinking vision transformer and masked autoencoder in multimodal face anti-spoofing\",\n        \"Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling\",\n        \"Exploring efficient few-shot adaptation for vision transformers\",\n        \"Parameter-efficient fine-tuning for vision transformers\",\n        \"Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation\",\n        \"Towards parameter-efficient integration of pre-trained language models in temporal video grounding\",\n        \"Visual query tuning: Towards effective usage of intermediate representations for parameter and memory efficient transfer learning\",\n        \"S. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models\",\n        \"Side adapter network for open-vocabulary semantic segmentation\",\n        \"Adapterbias: Parameter-efficient token-dependent representation shift for adapters in nlp tasks\",\n        \"Differentially private bias-term only fine-tuning of foundation models\",\n        \"International Conference on Learning Representations (2022)\",\n        \"Motion style transfer: Modular low-rank adaptation for deep motion forecasting\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XX, 239–256 (Springer, 2022)\",\n        \"Compacter: Efficient low-rank hypercomplex adapter layers\",\n        \"Phnns: Lightweight neural networks via parameterized hypercomplex convolutions\",\n        \"Meta-learning the difference: Preparing large language models for efficient adaptation\",\n        \"Fact: Factor-tuning for lightweight adaptation on vision transformer\",\n        \"Krona: Parameter efficient tuning with kronecker adapter\",\n        \"Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces\",\n        \"Distilling the knowledge in a neural network\",\n        \"International Conference on Learning Representations (2015)\",\n        \"Learning student networks via feature embedding\",\n        \"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, 469–484 (Springer, 2020)\",\n        \"Knowledge distillation via adaptive instance normalization\",\n        \"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16, 664–680 (Springer, 2020)\",\n        \"A. Heterogeneous knowledge distillation using information flow modeling\",\n        \"Paraphrasing complex network: Network compression via factor transfer\",\n        \"M. Relational knowledge distillation\",\n        \"Y. et al. Search to distill: Pearls are everywhere but not the eyes\",\n        \"D. et al. Cross-layer distillation with semantic calibration\",\n        \"S. et al. Distilling holistic knowledge with graph neural networks\",\n        \"J. Distilling knowledge via knowledge review\",\n        \"J. Decoupled knowledge distillation\",\n        \"International Conference on Learning Representations (2016)\",\n        \"J. Efficient architecture search by network transformation\",\n        \"International Conference on Learning Representations (2018)\",\n        \"International Conference on Learning Representations, Workshop Track Proceedings (2018)\",\n        \"International Conference on Machine Learning, 678–687 (PMLR, 2018)\",\n        \"International Conference on Learning Representations (2020)\",\n        \"Fna++: Fast network adaptation via parameter remapping and architecture search\",\n        \"International Conference on Learning Representations (2019)\",\n        \"Advances in neural information processing systems, 874–884 (2019)\",\n        \"DATA: differentiable architecture approximation with distribution guided sampling\",\n        \"Q. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation\",\n        \"Darts+: Improved differentiable architecture search with early stopping\",\n        \"G. et al. Sgas: Sequential greedy architecture search\",\n        \"T. Milenas: Efficient neural architecture search via mixed-level reformulation\",\n        \"St-adapter: Parameter-efficient image-to-video transfer learning for action recognition\",\n        \"International conference on machine learning, 1691–1703 (PMLR, 2020)\",\n        \"Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–755 (Springer, 2014)\",\n        \"Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding\",\n        \"V. & Sminchisescu\",\n        \"T. et al. Else-net: Elastic semantic network for continual action recognition from skeleton data\",\n        \"Visual prompt tuning for generative transfer learning\",\n        \"Prompt tuning of deep neural networks for speaker-adaptive visual speech recognition\",\n        \"Decorate the newcomers: Visual domain prompt for continual test time adaptation\",\n        \"J. et al. Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training\",\n        \"Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV, 105–124 (Springer, 2022)\",\n        \"Prefix conditioning unifies language and label supervision\",\n        \"International Conference on Machine Learning, 2790–2799 (PMLR, 2019)\",\n        \"L. et al. Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting\",\n        \"A survey on deep hashing methods\",\n        \"Prompt-matched semantic segmentation\",\n        \"Towards a unified view on visual parameter-efficient transfer learning\",\n        \"Pruning adapters with lottery ticket\",\n        \"Parameter-efficient fine-tuning of large-scale pre-trained language models\",\n        \"Role of bias terms in dot-product attention\",\n        \"S.-W. Attribute injection for pretrained language models: A new benchmark and an efficient method\",\n        \"A multilinear singular value decomposition\",\n        \"Towards lightweight transformer via group-wise transformation for vision-and-language tasks\",\n        \"Unipelt: A unified framework for parameter-efficient language model tuning\",\n        \"Low-rank winograd transformation for 3d convolutional neural networks\",\n        \"S. Fast algorithms for convolutional neural networks\",\n        \"Compressing deep convolutional networks using vector quantization\",\n        \"Learning structured sparsity in deep neural networks\",\n        \"ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4783–4787 (IEEE, 2022)\",\n        \"R. & Niculescu-Mizil\",\n        \"Do deep nets really need to be deep?\",\n        \"Representation learning: A review and new perspectives\",\n        \"Y. et al. Knowledge distillation via instance relationship graph\",\n        \"B. et al. Correlation congruence for knowledge distillation\",\n        \"J. Mimicking very efficient network for object detection\",\n        \"J. et al. Distilling object detectors via decoupled features\",\n        \"Z. et al. Localization distillation for dense object detection\",\n        \"Y. et al. Structured knowledge distillation for semantic segmentation\",\n        \"Structured knowledge distillation for dense prediction\",\n        \"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16, 346–362 (Springer, 2020)\",\n        \"C. et al. Cross-image relational knowledge distillation for semantic segmentation\",\n        \"Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X 16, 93–110 (Springer, 2020)\",\n        \"2022 IEEE International Conference on Image Processing (ICIP), 3853–3557 (IEEE, 2022)\",\n        \"Y. et al. Data-free knowledge distillation for image super-resolution\",\n        \"Generative adversarial super-resolution at the edge with knowledge distillation\",\n        \"Boosting light-weight depth estimation via knowledge distillation\",\n        \"Z. Knowledge distillation for fast and accurate monocular depth estimation on mobile devices\",\n        \"L. et al. Efficient crowd counting via structured knowledge transfer\",\n        \"Bidirectional recurrent neural networks\",\n        \"L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks\",\n        \"International conference on machine learning, 4095–4104 (PMLR, 2018)\",\n        \"R. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search\",\n        \"A survey on multi-task learning\",\n        \"Nerf: Representing scenes as neural radiance fields for view synthesis\",\n        \"B. High-resolution image synthesis with latent diffusion models\",\n        \"Z. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy\",\n        \"Scaling and assessment of data quality\",\n        \"H. Data security and privacy protection issues in cloud computing\",\n        \"Data security and privacy in cloud computing\",\n        \"Secure, privacy-preserving and federated machine learning in medical imaging\",\n        \"Recent advances of continual learning in computer vision: An overview\",\n        \"Adversarial interference and its mitigations in privacy-preserving collaborative machine learning\",\n        \"Nvidia hopper h100 gpu: Scaling performance\",\n        \"Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges\",\n        \"Weight-sharing neural architecture search: A battle to shrink the optimization gap\",\n        \"Dynamic neural networks: A survey\",\n        \"Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction\",\n        \"Exploring a large-scale multi-modal transportation recommendation system\",\n        \"Images speak in images: A generalist painter for in-context visual learning\",\n        \"What makes good examples for visual in-context learning?\",\n        \"Chatgpt for robotics: Design principles and model abilities\"\n    ],\n    \"reasoning\": \"Selected these 278 references because they represent foundational work and recent advances in Visual Tuning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}