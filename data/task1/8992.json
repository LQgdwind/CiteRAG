{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Memory-Centric Computing\n\nPaper Abstract: Abstract Modern computing systems are processor-centric. Data processing (i.e., computation) happens only in the processor (e.g., a CPU, GPU, FPGA, ASIC). As such, data needs to be moved from where it is generated/captured (e.g., sensors) and stored (e.g., storage and memory devices) to the processor before it can be processed. The processor-centric design paradigm greatly limits the performance & energy-efficiency, as well as scalability & sustainability, of modern computing systems. Many studies show that even the most powerful processors and accelerators waste a large fraction (e.g., >60%) of their time simply waiting for data and energy on moving data between storage/memory units to the processor. This is so even though most of the hardware real estate of such systems is dedicated to data storage and communication (e.g., many levels of caches, DRAM chips, storage systems, and interconnects). Memory-centric computing aims to enable computation capability in and near all places where data is generated and stored. As such, it can greatly reduce the large negative performance and energy impact of data access and data movement, by fundamentally avoiding data movement and reducing data access latency & energy. Many recent studies show that memory-centric computing can greatly improve system performance and energy efficiency. Major industrial vendors and startup companies have also recently introduced memory chips that have sophisticated computation capabilities. This talk describes promising ongoing research and development efforts in memory-centric computing. We classify such efforts into two major fundamental categories: 1) processing using memory, which exploits analog operational properties of memory structures to perform massively-parallel operations in memory, and 2) processing near memory, which integrates processing capability in memory controllers, the logic layer of 3D-stacked memory technologies, or memory chips to enable high-bandwidth and low-latency memory access to near-memory logic. We show both types of architectures (and their combination) can enable orders of magnitude improvements in performance and energy consumption of many important workloads, such as graph analytics, databases, machine learning, video processing, climate modeling, genome analysis. We discuss adoption challenges for the memory-centric computing paradigm and conclude with some research & development opportunities.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A Modern Primer on Processing in Memory\",\n        \"Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks\",\n        \"In-DRAM Bulk Bitwise Execution Engine\",\n        \"Processing Data Where It Makes Sense: Enabling In-Memory Computation\",\n        \"RowClone: Fast and Energy-Efficient In-DRAM Bulk Data Copy and Initialization\",\n        \"Fast Bulk Bitwise AND and OR in DRAM\",\n        \"Buddy-RAM: Improving the Performance and Efficiency of Bulk Bitwise Operations Using DRAM\",\n        \"Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology\",\n        \"SIMDRAM: A Framework for Bit-Serial SIMD Processing Using DRAM\",\n        \"pLUTo: Enabling Massively Parallel Computation in DRAM via Lookup Tables\",\n        \"Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories\",\n        \"ISAAC: A Convolutional Neural Network Accelerator with In-situ Analog Arithmetic in Crossbars\",\n        \"PRIME: A Novel Processing-In-Memory Architecture for Neural Network Computation In ReRAM-Based Main Memory\",\n        \"Flash-Cosmos: In-Flash Bulk Bitwise Operations Using Inherent Computation Capability of NAND Flash Memory\",\n        \"ParaBit: Processing Parallel Bitwise Operations in NAND Flash Memory based SSDs\",\n        \"An Energy-Efficient VLSI Architecture for Pattern Recognition via Deep Embedding of Computation in SRAM\",\n        \"Compute Caches\",\n        \"ComputeDRAM: In-Memory Compute Using Off-the-Shelf DRAMs\",\n        \"PiDRAM: A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM\",\n        \"D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers with Low Latency and High Throughput\",\n        \"QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple Row Activation in Commodity DRAMs\",\n        \"A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing\",\n        \"SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems\",\n        \"NATSA: A Near-Data Processing Accelerator for Time Series Analysis\",\n        \"GenStore: A High-Performance and Energy-Efficient In-Storage Computing System for Genome Sequence Analysis\",\n        \"Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems\",\n        \"Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks\",\n        \"PIM-Enabled Instructions: A Low-Overhead, Locality-Aware Processing-in-Memory Architecture\",\n        \"Accelerating Pointer Chasing in 3D-Stacked Memory: Challenges, Mechanisms, Evaluation\",\n        \"Polynesia: Enabling Effective Hybrid Transactional Analytical Databases with Specialized Hardware Software Co-Design\",\n        \"The True Processing In Memory Accelerator\",\n        \"Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware\",\n        \"Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System\",\n        \"Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-in-Memory Architectures\",\n        \"Evaluating Machine Learning Workloads on Memory-Centric Computing Systems\",\n        \"A Framework for High-throughput Sequence Alignment using Real Processing-in-Memory Systems\",\n        \"25.4 A 20nm 6GB Function-In-Memory DRAM, Based on HBM2 with a 1.2 TFLOPS Programmable Computing Unit Using Bank-Level Parallelism, for Machine Learning Applications\",\n        \"A 1ynm 1.25V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-Memory supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning Applications\",\n        \"184QPS/W 64Mb/mm2 3D Logic-to-DRAM Hybrid Bonding with Process-Near-Memory Engine for Recommendation System\",\n        \"Near-Memory Processing in Action: Accelerating Personalized Recommendation with AxDIMM\",\n        \"FPGA-based Near-Memory Acceleration of Modern Data-Intensive Applications\",\n        \"NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling\",\n        \"Processing-in-Memory: A Workload-Driven Perspective\",\n        \"CoNDA: Efficient Cache Coherence Support for near-Data Accelerators\",\n        \"LazyPIM: An Efficient Cache Coherence Mechanism for Processing-in-Memory\",\n        \"SynCron: Efficient Synchronization Support for Near-Data-Processing Architectures\",\n        \"DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks\",\n        \"Ramulator: A Fast and Extensible DRAM Simulator\",\n        \"NAPEL: Near-memory Computing Application Performance Prediction via Ensemble Learning\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Memory-Centric Computing, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}