{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 65 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Nonnegative Convolution Is Equivalent to Dense Nonnegative ConvolutionThis work is part of the project TIPEA that has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 850979).\n\nPaper Abstract: Abstract Computing the convolution A⋆B⋆𝐴𝐵A\\star B of two length-n𝑛n vectors A,B𝐴𝐵A,B is an ubiquitous computational primitive, with applications in a variety of disciplines.Within theoretical computer science, applications range from string problems to Knapsack-type problems, and from 3SUM to All-Pairs Shortest Paths. These applications often come in the form of nonnegative convolution, where the entries of A,B𝐴𝐵A,B are nonnegative integers. The classical algorithm to compute A⋆B⋆𝐴𝐵A\\star B uses the Fast Fourier Transform (FFT) and runs in time O​(n​log⁡n)𝑂𝑛𝑛O(n\\log n).However, in many cases A𝐴A and B𝐵B might satisfy sparsity conditions, and hence one could hope for significant gains compared to the standard FFT algorithm. The ideal goal would be an O​(k​log⁡k)𝑂𝑘𝑘O(k\\log k)-time algorithm, where k𝑘k is the number of non-zero elements in the output, i.e., the size of the support of A⋆B⋆𝐴𝐵A\\star B. This problem is referred to as sparse nonnegative convolution, and has received a considerable amount of attention in the literature; the fastest algorithms to date run in time O​(k​log2⁡n)𝑂𝑘superscript2𝑛O(k\\log^{2}n).The main result of this paper is the first O​(k​log⁡k)𝑂𝑘𝑘O(k\\log k)-time algorithm for sparse nonnegative convolution. Our algorithm is randomized and assumes that the length n𝑛n and the largest entry of A𝐴A and B𝐵B are subexponential in k𝑘k. Surprisingly, we can phrase our algorithm as a reduction from the sparse case to the dense case of nonnegative convolution, showing that, under some mild assumptions, sparse nonnegative convolution is equivalent to dense nonnegative convolution for constant-error randomized algorithms. Specifically, if D​(n)𝐷𝑛D(n) is the time to convolve two nonnegative length-n𝑛n vectors with success probability 2/3232/3, and S​(k)𝑆𝑘S(k) is the time to convolve two nonnegative vectors with output size k𝑘k with success probability 2/3232/3, then S​(k)=O​(D​(k)+k​(log⁡log⁡k)2)𝑆𝑘𝑂𝐷𝑘𝑘superscript𝑘2S(k)=O(D(k)+k(\\log\\log k)^{2}).Our approach uses a variety of new techniques in combination with some old machinery from linear sketching and structured linear algebra, as well as new insights on linear hashing, the most classical hash function.\n\n{retrieved_text}\n\nGenerate exactly 65 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 65\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Gaussian smoothing\",\n        \"Generalized string matching\",\n        \"Lower bounds for multiplication via network coding\",\n        \"On the relationship between histogram indexing and block-massindexing\",\n        \"Deterministic length reduction: Fast convolution in sparse data andapplications\",\n        \"Faster algorithms for string matching with k𝑘k mismatches\",\n        \"Output-sensitive algorithms for sumset and sparse polynomialmultiplication\",\n        \"Fast algorithms for knapsack via convolution and prediction\",\n        \"A deterministic algorithm for sparse multivariate polynomialinterpolation\",\n        \"A linear filtering approach to the computation of discrete Fouriertransform\",\n        \"A near-linear pseudopolynomial time algorithm for subset sum\",\n        \"Top-k𝑘k-convolution and the quest for near-linear output-sensitivesubset sum\",\n        \"Pattern matching for spatial point sets\",\n        \"Reducing 3SUM to convolution-3SUM\",\n        \"Clustered integer 3SUM via additive combinatorics\",\n        \"Verifying candidate matches in sparse and wildcard matching\",\n        \"Fast Fourier transforms for nonequispaced data\",\n        \"String matching and other products\",\n        \"Near-optimal sparse Fourier representations via sampling\",\n        \"Approximate sparse recovery: Optimizing time and measurements\",\n        \"Improved time bounds for near-optimal space Fourierrepresentations\",\n        \"L2/L2subscript𝐿2subscript𝐿2L_{2}/L_{2}-foreach sparse recovery with low risk\",\n        \"Essentially optimal sparse polynomial multiplication\",\n        \"Digital processing of signals\",\n        \"Nearly optimal sparse Fourier transform\",\n        \"The restricted isometry property of subsampled Fourier matrices\",\n        \"Sparse polynomial interpolation over fields with large or zerocharacteristic\",\n        \"Faster algorithms for string matching problems: matching theconvolution bound\",\n        \"Sample-optimal Fourier sampling in any constant dimension\",\n        \"(Nearly) sample-optimal sparse Fourier transform\",\n        \"On the power of adaptivity in sparse recovery\",\n        \"On integer programming and convolution\",\n        \"Tail bounds for sums of geometric and exponential variables\",\n        \"Improved sparse multivariate polynomial interpolation algorithms\",\n        \"Sparse Fourier transform in any constant dimension withnearly-optimal sample complexity in sublinear time\",\n        \"Sample efficient estimation and recovery in sparse FFT viaisolation on average\",\n        \"Dimension-independent sparse Fourier transform\",\n        \"Fast algorithms for approximately counting mismatches\",\n        \"Linear hashing is awesome\",\n        \"Faster pseudopolynomial time algorithms for subset sum\",\n        \"A simple algorithm for approximating the text-to-pattern hammingdistance\",\n        \"On the arithmetic operational complexity for solving Vandermondelinear equations\",\n        \"Parallel sparse polynomial multiplication using heaps\",\n        \"POLY: A new polynomial data structure for Maple 17\",\n        \"The design of Maple’s sum-of-products and POLY data structuresfor representing mathematical objects\",\n        \"A subquadratic approximation scheme for partition\",\n        \"New results and open problems related to non-standard stringology\",\n        \"Nearly optimal sparse polynomial multiplication\",\n        \"(Nearly) sample-optimal sparse Fourier transform in anydimension; RIPless and filterless\",\n        \"Structured matrices and polynomials: Unified superfastalgorithms\",\n        \"Towards polynomial lower bounds for dynamic problems\",\n        \"On the evaluation of powers and monomials\",\n        \"A robust sparse Fourier transform in the continuous setting\",\n        \"Applications of the Shannon-Hartley theorem to data streams andrecovery\",\n        \"Adaptive polynomial multiplication\",\n        \"Chunky and equal-spaced polynomial multiplication\",\n        \"What can (and can’t) we do with sparse polynomials?\",\n        \"Approximate formulas for some functions of prime numbers\",\n        \"(Dense structured) matrix vector multiplication, lecture notes\",\n        \"Multivariate polynomial rings\",\n        \"On the complexity of multivariate blockwise polynomialmultiplication\",\n        \"On the bit-complexity of sparse polynomial and series multiplication\",\n        \"Modern Computer Algebra\",\n        \"Decoding of Bose-Chaudhuri-Hocquenghem codes and Prony’smethod for curve fitting\",\n        \"On the evaluation of powers\"\n    ],\n    \"reasoning\": \"Selected these 65 references because they represent foundational work and recent advances in Sparse Nonnegative Convolution Is Equivalent to Dense Nonnegative ConvolutionThis work is part of the project TIPEA that has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 850979)., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}