{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 403 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Datasets for Large Language Models: A Comprehensive Survey\n\nPaper Abstract: Abstract This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.\n\n{retrieved_text}\n\nGenerate exactly 403 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 403\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Towards a cleaner document-oriented multilingual crawled corpus\",\n        \"GPT-4 technical report\",\n        \"Explanations for CommonsenseQA: New dataset and models\",\n        \"Can we trust the evaluation on ChatGPT?\",\n        \"RAFT: A real-world few-shot text classification benchmark\",\n        \"Koncel-Kedziorski R\",\n        \"et al (2023) L-Eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088\",\n        \"et al (2023) GPT4All: An ecosystem of open source compressed language models. arXiv preprint arXiv:2311.04931\",\n        \"Anonymous (2024) ToolLLM: Facilitating large language models to master 16000+ real-world APIs\",\n        \"Kann K (2021) PROST: Physical reasoning about objects through space and time. In: Zong C\",\n        \"et al (2021) A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861\",\n        \"et al (2021) Program synthesis with large language models. arXiv preprint arXiv:2108.07732\",\n        \"et al (2023) Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.06786\",\n        \"et al (2022) PromptSource: An integrated development environment and repository for natural language prompts. In: Basile V\",\n        \"et al (2023a) QWen technical report. arXiv preprint arXiv:2309.16609\",\n        \"et al (2022) Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862\",\n        \"et al (2023b) Longbench: A bilingual\",\n        \"et al (2023c) Benchmarking foundation models with language-model-as-an-examiner\",\n        \"Vincent N (2021) Addressing “documentation debt” in machine learning: A retrospective datasheet for bookcorpus\",\n        \"et al (2020) ParaCrawl: Web-scale acquisition of parallel corpora. In: Jurafsky D\",\n        \"et al (2023) DISC-MedLLM: Bridging general large language models and real-world medical consultation. arXiv preprint arXiv:2308.14346\",\n        \"et al (2006) The second PASCAL recognising textual entailment challenge. URL https://api.semanticscholar.org/CorpusID:13385138\",\n        \"Stenetorp P (2020) Beat the AI: Investigating adversarial human annotation for reading comprehension\",\n        \"Blackburn J (2020) The Pushshift reddit dataset. In: Proceedings of the international AAAI conference on web and social media\",\n        \"BELLEGroup (2023) BELLE: Be everyone’s large language model engine\",\n        \"Giampiccolo D (2009) The fifth PASCAL recognizing textual entailment challenge. TAC 7:8\",\n        \"Choi Y (2020) PIQA: Reasoning about physical commonsense in natural language\",\n        \"Manning CD (2015) A large annotated corpus for learning natural language inference. In: Màrquez L\",\n        \"et al (2020) Language models are few-shot learners\",\n        \"Sutskever I (2016) Description2Code dataset. https://github.com/ethancaballero/description2code\",\n        \"Specia L (2017) SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In: Bethard S\",\n        \"et al (2017) Overview of the IWSLT 2017 evaluation campaign. In: Proceedings of the 14th International Workshop on Spoken Language Translation\",\n        \"Androutsopoulos I\",\n        \"et al (2023) A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109\",\n        \"Chaudhary S (2023) Code Alpaca: An instruction-following LLaMA model for code generation\",\n        \"Tang B (2018) The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification. In: Riloff E\",\n        \"et al (2021) Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374\",\n        \"et al (2023a) DISC-FinLLM: A Chinese financial large language model based on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205\",\n        \"et al (2023b) TheoremQA: A theorem-driven question answering dataset. In: Bouamor H\",\n        \"Fu C (2023c) TigerBot: An open multilingual multitask LLM. arXiv preprint arXiv:2312.08688\",\n        \"et al (2023d) Phoenix: Democratizing ChatGPT across languages. arXiv preprint arXiv:2304.10453\",\n        \"et al (2023) Evaluating hallucinations in Chinese large language models. arXiv preprint arXiv:2310.03368\",\n        \"et al (2018) QuAC: Question answering in context. In: Riloff E\",\n        \"Jurgens D (2023) Do LLMs understand social knowledge? Evaluating the sociability of large language models with SocKET benchmark. In: Bouamor H\",\n        \"et al (2023) MCTS: A multi-reference Chinese text simplification dataset. arXiv preprint arXiv:2306.02796\",\n        \"Amodei D (2017) Deep reinforcement learning from human preferences. In: Guyon I\",\n        \"Toutanova K (2019) BoolQ: Exploring the surprising difficulty of natural yes/no questions. In: Burstein J\",\n        \"et al (2020) TyDiQA: A benchmark for information-seeking question answering in typologically diverse languages\",\n        \"et al (2018) Think you have solved question answering? Try ARC\",\n        \"et al (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168\",\n        \"Kiela D (2018) SentEval: An evaluation toolkit for universal sentence representations. In: Calzolari N\",\n        \"et al (2018) XNLI: Evaluating cross-lingual sentence representations. In: Riloff E\",\n        \"et al (2020) Unsupervised cross-lingual representation learning at scale\",\n        \"et al (2023) Free Dolly: Introducing the world’s first truly open instruction-tuned LLM\",\n        \"Contributors O (2023) OpenCompass: A universal evaluation platform for foundation models\",\n        \"et al (2022) No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672\",\n        \"et al (2023) Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377\",\n        \"et al (2019) A span-extraction dataset for Chinese machine reading comprehension. In: Inui K\",\n        \"et al (2020) A sentence cloze dataset for Chinese machine reading comprehension. In: Scott D\",\n        \"Magnini B (2006) The PASCAL recognising textual entailment challenge. In: Quiñonero-Candela J\",\n        \"et al (2023) LAiW: A Chinese legal large language models benchmark (a technical report). arXiv preprint arXiv:2310.05620\",\n        \"et al (2023) EduChat: A large-scale language model-based chatbot system for intelligent education. arXiv preprint arXiv:2308.02773\",\n        \"Gardner M (2019) Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In: Inui K\",\n        \"Gardner M (2021) A dataset of information-seeking questions and answers anchored in research papers. In: Toutanova K\",\n        \"Tonhauser J (2019) The CommitmentBank: Investigating projection in naturally occurring discourse. In: proceedings of Sinn und Bedeutung\",\n        \"et al (2023) K2: A foundation language model for geoscience knowledge understanding and utilization. arXiv preprint arXiv:2306.05064\",\n        \"Limsopatham N (2017) Results of the WNUT2017 shared task on novel and emerging entity recognition. In: Derczynski L\",\n        \"In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n        \"et al (2021) Few-NERD: A few-shot named entity recognition dataset. In: Zong C\",\n        \"et al (2023) Enhancing chat language models by scaling high-quality instructional conversations. In: Bouamor H\",\n        \"Brockett C (2005) Automatically constructing a corpus of sentential paraphrases\",\n        \"et al (2022) GLM: General language model pretraining with autoregressive blank infilling\",\n        \"Gardner M (2019) DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In: Burstein J\",\n        \"et al (2023) AlpacaFarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387\",\n        \"Levy O (2023) LMentry: A language model benchmark of elementary language tasks. In: Rogers A\",\n        \"Chen Y (2010) MultiUN: A multilingual corpus from United Nation documents. In: Calzolari N\",\n        \"Swayamdipta S (2022) Understanding dataset difficulty with 𝒱𝒱\\\\mathcal{V}-usable information. In: Chaudhuri K\",\n        \"Radev D (2019) Multi-News: A large-scale multi-document summarization dataset and abstractive hierarchical model. In: Korhonen A\",\n        \"et al (2023) Mol-Instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018\",\n        \"et al (2023) LawBench: Benchmarking legal knowledge of large language models. arXiv preprint arXiv:2309.16289\",\n        \"Khot T (2023) Chain-of-Thought Hub: A continuous effort to measure large language models’ reasoning performance. arXiv preprint arXiv:2305.17306\",\n        \"et al (2020) The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027\",\n        \"Perez-Beltrachini L (2017) Creating training corpora for NLG micro-planners. In: Barzilay R\",\n        \"Berant J (2021) Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies\",\n        \"Poria S (2023) Flacuna: Unleashing the problem solving power of Vicuna using FLAN fine-tuning. arXiv preprint arXiv:2307.02053\",\n        \"Dolan B (2007) The third PASCAL recognizing textual entailment challenge. In: Sekine S\",\n        \"Wawer A (2019) SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In: Wang L\",\n        \"Huang L (2009) Twitter sentiment classification using distant supervision. CS224N project report\",\n        \"Cohen V (2019) OpenWebText corpus. http://Skylion007.github.io/OpenWebTextCorpus\",\n        \"Sundheim B (1996) Message Understanding Conference-6: A brief history. In: Proceedings of the 16th Conference on Computational Linguistics\",\n        \"Artzi Y (2018) Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In: Walker M\",\n        \"et al (2023) Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. arXiv preprint arXiv:2306.05783\",\n        \"et al (2023) LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models\",\n        \"et al (2023) Textbooks are all you need. arXiv preprint arXiv:2306.11644\",\n        \"et al (2023a) How close is ChatGPT to human experts? Comparison corpus\",\n        \"et al (2023b) OWL: A large language model for it operations. arXiv preprint arXiv:2309.09298\",\n        \"et al (2023c) Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736\",\n        \"Bigham J (2022) InstructDial: Improving zero and few-shot generalization in dialogue through instruction tuning. In: Goldberg Y\",\n        \"et al (2023) MedAlpaca–An open-source collection of medical conversational AI models and training data. arXiv preprint arXiv:2304.08247\",\n        \"et al (2018) FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In: Riloff E\",\n        \"et al (2021) XL-Sum: Large-scale multilingual abstractive summarization for 44 languages. In: Zong C\",\n        \"et al (2023a) WanJuan: A comprehensive multimodal dataset for advancing English and Chinese large models. arXiv preprint arXiv:2308.10755\",\n        \"Tu M (2019) Applying deep matching networks to Chinese medical question answering: A study and a dataset\",\n        \"et al (2023b) Can large language models understand real-world complex instructions? arXiv preprint arXiv:2309.09150\",\n        \"et al (2018) DuReader: A Chinese machine reading comprehension dataset from real-world applications. In: Choi E\",\n        \"et al (2023c) HanFei-1.0. https://github.com/siat-nlp/HanFei\",\n        \"et al (2021a) Measuring coding challenge competence with APPS. In: Vanschoren J\",\n        \"et al (2021b) Measuring massive multitask language understanding. In: International Conference on Learning Representations\",\n        \"In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)\",\n        \"In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)\",\n        \"Schmidhuber J (1997) Long short-term memory. Neural Computation 9(8):1735–1780\",\n        \"Schick T (2023) Unnatural Instructions: Tuning language models with (almost) no human labor. In: Rogers A\",\n        \"Callison-Burch C (2023) Choice-75: A dataset on decision branching in script learning. arXiv preprint arXiv:2309.11737\",\n        \"Zhu F (2015) LCSTS: A large scale Chinese short text summarization dataset. In: Màrquez L\",\n        \"et al (2022a) LoRA: Low-rank adaptation of large language models. In: International Conference on Learning Representations\",\n        \"Moss L (2020) OCNLI: Original Chinese natural language inference. In: Cohn T\",\n        \"Zhou J (2022b) CSCD-IME: Correcting spelling errors generated by Pinyin IME. arXiv preprint arXiv:2211.08788\",\n        \"et al (2023a) Emotionally numb or empathetic? Evaluating how LLMs feel using emotionbench. arXiv preprint arXiv:2308.03656\",\n        \"Choi Y (2019) Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In: Inui K\",\n        \"et al (2023b) Lawyer LLaMA technical report. arXiv preprint arXiv:2305.15062\",\n        \"et al (2023c) C-Eval: A multi-level multi-discipline Chinese evaluation suite for foundation models\",\n        \"et al (2023d) TrustGPT: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507\",\n        \"et al (2022) OPT-IML: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017\",\n        \"et al (2023) Phi-2: The surprising power of small language models\",\n        \"et al (2023a) BeaverTails: Towards improved safety alignment of LLM via a human-preference dataset. arXiv preprint arXiv:2307.04657\",\n        \"et al (2023b) Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases\",\n        \"et al (2024) Mixtral of experts. arXiv preprint arXiv:2401.04088\",\n        \"Li L (2019) Better modeling of incomplete annotations for named entity recognition. In: Burstein J\",\n        \"Lu X (2019) PubMedQA: A dataset for biomedical research question answering. In: Inui K\",\n        \"Zettlemoyer L (2017) TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In: Barzilay R\",\n        \"Smith NA (2020) The multilingual Amazon reviews corpus. In: Webber B\",\n        \"Roth D (2018) Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In: Walker M\",\n        \"Sabharwal A (2020) QASC: A dataset for question answering via sentence composition\",\n        \"et al (2022) ProsocialDialog: A prosocial backbone for conversational agents. arXiv preprint arXiv:2205.12688\",\n        \"et al (2023) The Stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research pp 1–28\",\n        \"Schütze H (2023) Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460\",\n        \"Wang WY (2018) Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305\",\n        \"et al (2019) Natural Questions: A benchmark for question answering research\",\n        \"McKeown K (2020) WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In: Cohn T\",\n        \"Hovy E (2017) RACE: Large-scale reading comprehension dataset from examinations. In: Palmer M\",\n        \"et al (2023) DS-1000: A natural and reliable benchmark for data science code generation. In: Krause A\",\n        \"et al (2021) QED: A framework and dataset for explanations in question answering\",\n        \"et al (2022) MWPToolkit: An open-source framework for deep learning-based math word problem solvers\",\n        \"In: Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track\",\n        \"Koyejo S (2023a) Beyond scale: The diversity coefficient as a data quality metric demonstrates LLMs are pre-trained on formally diverse data\",\n        \"Ruiz N (2023b) Platypus: Quick\",\n        \"et al (2023c) RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267\",\n        \"et al (2022) Factuality enhanced language models for open-ended text generation. In: Koyejo S\",\n        \"In: Thirteenth international conference on the principles of knowledge representation and reasoning\",\n        \"Kwong OO (eds) Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. ACL\",\n        \"Schwenk H (2020) MLQA: Evaluating cross-lingual extractive question answering. In: Jurafsky D\",\n        \"et al (2023a) How long can open-source LLMs truly promise on context length? https://lmsys.org/blog/2023-06-29-longchat\",\n        \"Ghanem B (2023b) CAMEL: Communicative agents for “mind” exploration of large language model society\",\n        \"Baldwin T (2023c) Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011\",\n        \"et al (2023d) CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212\",\n        \"Wen JR (2023e) HaluEval: A large-scale hallucination evaluation benchmark for large language models. In: Bouamor H\",\n        \"In: Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track\",\n        \"Zhang M (2023g) LooGLE: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939\",\n        \"et al (2023h) Huatuo-26M\",\n        \"et al (2023i) API-Bank: A comprehensive benchmark for tool-augmented LLMs. In: Bouamor H\",\n        \"et al (2023j) StarCoder: May the source be with you! Transactions on Machine Learning Research pp 1–43\",\n        \"et al (2022a) Competition-level code generation with AlphaCode\",\n        \"et al (2022b) CSL: A large-scale Chinese scientific literature dataset. In: Calzolari N\",\n        \"Lee YT (2023k) Textbooks are all you need II: Phi-1.5 technical report. arXiv preprint arXiv:2309.05463\",\n        \"Zhang Y (2023l) ChatDoctor: A medical chat model fine-tuned on a large language model Meta-AI (LLaMA) using medical domain knowledge. Cureus 15(6)\",\n        \"et al (2023m) EcomGPT: Instruction-tuning large language model with chain-of-task tasks for E-commerce. arXiv preprint arXiv:2308.06966\",\n        \"et al (2023n) CLEVA: Chinese language models evaluation platform. In: Feng Y\",\n        \"et al (2023) Holistic evaluation of language models. Transactions on Machine Learning Research pp 1–162. Featured Certification\",\n        \"Li C (2023) Luotuo-QA: Better conversational question answering model with answer completion. https://github.com/LC1332/Luotuo-QA\",\n        \"et al (2023) Let’s verify step by step. arXiv preprint arXiv:2305.20050\",\n        \"et al (2020) CommonGen: A constrained text generation challenge for generative commonsense reasoning. In: Cohn T\",\n        \"Lin CY (2004) ROUGE: A package for automatic evaluation of summaries\",\n        \"Gardner M (2019) Reasoning over paragraph effects in situations. In: Fisch A\",\n        \"Evans O (2022) TruthfulQA: Measuring how models mimic human falsehoods. In: Muresan S\",\n        \"Blunsom P (2017) Program induction by rationale generation: Learning to solve and explain algebraic word problems. In: Barzilay R\",\n        \"Choi Y (2022) WANLI: Worker and AI collaboration for natural language inference dataset creation. In: Goldberg Y\",\n        \"et al (2023a) M3KE: A massive multi-level multi-subject knowledge evaluation benchmark for Chinese large language models\",\n        \"Wang Y (2023b) LawGpt: Chinese legal dialogue language model. https://github.com/LiuHC0428/LAW_GPT\",\n        \"Zhang Y (2023c) LogiCoT: Logical chain-of-thought instruction tuning. In: Bouamor H\",\n        \"Zhang Y (2021) LogiQA: A challenge dataset for machine reading comprehension with logical reasoning\",\n        \"et al (2023d) Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960\",\n        \"Low BKH (2023) Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201\",\n        \"et al (2018) LCQMC: A large-scale Chinese question matching corpus. In: Bender EM\",\n        \"et al (2023e) WebGLM: Towards an efficient web-enhanced question answering system with human preferences\",\n        \"et al (2023f) AgentBench: Evaluating LLMs as agents. arXiv preprint arXiv:2308.03688\",\n        \"In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n        \"et al (2023a) The FLAN collection: Designing data and methods for effective instruction tuning\",\n        \"et al (2023b) The data provenance initiative: A large scale audit of dataset licensing & attribution in AI. arXiv preprint arXiv:2310.16787\",\n        \"et al (2023c) A pretrainer’s guide to training data: Measuring the effects of data age\",\n        \"et al (2023a) BBT-Fin: Comprehensive construction of Chinese financial domain pre-trained language model\",\n        \"et al (2022) Learn to explain: Multimodal reasoning via thought chains for science question answering. In: Koyejo S\",\n        \"et al (2023b) Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning\",\n        \"et al (2021) CodeXGLUE: A machine learning benchmark dataset for code understanding and generation\",\n        \"Potts C (2011) Learning word vectors for sentiment analysis. In: Lin D\",\n        \"Socher R (2018) The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730\",\n        \"Su KY (2021) A diverse corpus for evaluating and developing English math word problem solvers. arXiv preprint arXiv:2106.15772\",\n        \"Sabharwal A (2018) Can a suit of armor conduct electricity? A new dataset for open book question answering. In: Riloff E\",\n        \"Zhang Y (2019) A pilot study for Chinese SQL semantic parsing. In: Inui K\",\n        \"et al (2023) FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In: Bouamor H\",\n        \"et al (2024) Large language models: A survey. arXiv preprint arXiv:2402.06196\",\n        \"et al (2022a) LILA: A unified benchmark for mathematical reasoning. In: Goldberg Y\",\n        \"Hajishirzi H (2022b) Cross-task generalization via natural language crowdsourcing instructions. In: Muresan S\",\n        \"MNBVC Team (2023) MNBVC: Massive never-ending BT vast Chinese corpus. https://github.com/esbatmop/MNBVC\",\n        \"et al (2016) A corpus and cloze evaluation for deeper understanding of commonsense stories. In: Knight K\",\n        \"et al (2023a) OctoPack: Instruction tuning code large language models. In: NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following\",\n        \"et al (2023b) Crosslingual generalization through multitask finetuning. In: Rogers A\",\n        \"et al (2023) Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908\",\n        \"Awadallah A (2023) Orca: Progressive learning from complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707\",\n        \"et al (2021) WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332\",\n        \"et al (2021) DART: Open-domain structured data record to text generation. In: Toutanova K\",\n        \"Bowman SR (2020) CrowS-Pairs: A challenge dataset for measuring social biases in masked language models. In: Webber B\",\n        \"Lapata M (2018) Don’t give me the details\",\n        \"et al (2016) MS MARCO: A human generated machine reading comprehension dataset. choice 2640:660\",\n        \"et al (2023) CulturaX: A cleaned\",\n        \"et al (2023) Instruction in the wild: A user-based instruction dataset. https://github.com/XueFuzhao/InstructionWild\",\n        \"Kiela D (2020) Adversarial NLI: A new benchmark for natural language understanding. In: Jurafsky D\",\n        \"et al (2023) CodeGen: An open large language model for code with multi-turn program synthesis\",\n        \"Chalkidis I (2023) LEXTREME: A multi-lingual and multi-task benchmark for the legal domain. In: Bouamor H\",\n        \"Rieser V (2017) The E2E dataset: New challenges for end-to-end generation. In: Jokinen K\",\n        \"Durrett G (2021) CREAK: A dataset for commonsense reasoning over entity knowledge. arXiv preprint arXiv:2109.01653\",\n        \"OpenAI (2022) Introducing ChatGPT\",\n        \"5 Turbo fine-tuning and API updates\",\n        \"et al (2022) Training language models to follow instructions with human feedback. In: Koyejo S\",\n        \"Paek T (2006) Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment\",\n        \"et al (2016) The LAMBADA dataset: Word prediction requiring a broad discourse context. In: Erk K\",\n        \"Zhu WJ (2002) BLEU: A method for automatic evaluation of machine translation. In: Isabelle P\",\n        \"Goyal N (2021) Are NLP models really able to solve simple math word problems? In: Toutanova K\",\n        \"Gonzalez JE (2023) Gorilla: Large language model connected with massive APIs. arXiv preprint arXiv:2305.15334\",\n        \"Gao J (2023) Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277\",\n        \"Dredze M (2015) Named entity recognition for Chinese social media with jointly trained embeddings. In: Màrquez L\",\n        \"et al (2022) Red teaming language models with language models. In: Goldberg Y\",\n        \"Camacho-Collados J (2019) WiC: The word-in-context dataset for evaluating context-sensitive meaning representations. In: Burstein J\",\n        \"et al (2021) CodeNet: A large-scale AI for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655\",\n        \"Lan Z (2023) SMILE: Single-turn to multi-turn inclusive language expansion via ChatGPT for mental health support. arXiv preprint arXiv:2305.00450\",\n        \"et al (2019) Language models are unsupervised multitask learners. OpenAI blog 1(8):1–24\",\n        \"Lillicrap TP (2020) Compressive Transformers for long-range sequence modelling. In: International Conference on Learning Representations\",\n        \"et al (2021) Scaling language models: Methods\",\n        \"et al (2020) Exploring the limits of transfer learning with a unified text-to-text Transformer. J Mach Learn Res 21(1):5485–5551\",\n        \"Ng V (2012) Resolving complex cases of definite pronouns: The winograd schema challenge. In: Tsujii J\",\n        \"000+ questions for machine comprehension of text. In: Su J\",\n        \"Liang P (2018) Know what you don’t know: Unanswerable questions for SQuAD. In: Gurevych I\",\n        \"et al (2023) SCALE: Scaling up the complexity for advanced language model evaluation. arXiv preprint arXiv:2306.09237\",\n        \"Marasovic A (2022) CONDAQA: A contrastive reading comprehension dataset for reasoning about negation. In: Goldberg Y\",\n        \"Transactions of the Association for Computational Linguistics 7:249–266\",\n        \"Renshaw E (2013) MCTest: A challenge dataset for the open-domain machine comprehension of text. In: Yarowsky D\",\n        \"Gordon AS (2011) Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In: 2011 AAAI Spring Symposium Series\",\n        \"Rumshisky A (2020) Getting closer to AI complete question answering: A set of prerequisite real tasks\",\n        \"Shivade C (2018) Lessons from natural language inference in the clinical domain. In: Riloff E\",\n        \"Weston J (2015) A neural attention model for abstractive sentence summarization. In: Màrquez L\",\n        \"Sankaranarayanan K (2018) DuoRC: Towards complex language understanding with paraphrased reading comprehension. In: Gurevych I\",\n        \"Choi Y (2021) WinoGrande: An adversarial winograd schema challenge at scale. Commun ACM 64(9):99–106. 10.1145/3474381\",\n        \"Choi Y (2019) Social IQa: Commonsense reasoning about social interactions. In: Inui K\",\n        \"et al (2023) ARB: Advanced reasoning benchmark for large language models. arXiv preprint arXiv:2307.13692\",\n        \"Klimov O (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\",\n        \"Manning CD (2017) Get to the point: Summarization with pointer-generator networks. In: Barzilay R\",\n        \"et al (2022) When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain. In: Goldberg Y\",\n        \"Levy O (2023) ZeroSCROLLS: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196\",\n        \"Ruder S (2020) XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv preprint arXiv:2003.11080\",\n        \"et al (2024) Aya Dataset: An open-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619\",\n        \"et al (2023) Large language models encode clinical knowledge. Nature 620(7972):172–180\",\n        \"et al (2013) Recursive deep models for semantic compositionality over a sentiment treebank. In: Yarowsky D\",\n        \"et al (2024) Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159\",\n        \"et al (2023) Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\",\n        \"et al (2020) Learning to summarize with human feedback. In: Larochelle H\",\n        \"Huang M (2021a) PsyQA: A Chinese dataset for generating long counseling text for mental health support. In: Zong C\",\n        \"Huang M (2023a) Safety assessment of Chinese large language models. arXiv preprint arXiv:2304.10436\",\n        \"Cardie C (2019) DREAM: A challenge data set and models for dialogue-based reading comprehension\",\n        \"Cardie C (2020) Investigating prior knowledge for challenging Chinese machine reading comprehension\",\n        \"et al (2023b) MOSS: Training conversational language models from synthetic data. https://github.com/OpenLMLab/MOSS\",\n        \"et al (2021b) ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137\",\n        \"et al (2023) Challenging BIG-bench tasks and whether chain-of-thought can solve them. In: Rogers A\",\n        \"Sabharwal A (2019a) QuaRel: A dataset and models for answering questions about qualitative relationships\",\n        \"Clark P (2019b) QuaRTz: An open-domain dataset of qualitative relationship questions. In: Inui K\",\n        \"Berant J (2019) CommonsenseQA: A question answering challenge targeting commonsense knowledge. In: Burstein J\",\n        \"Bosselut A (2019) WIQA: A dataset for “what if…” reasoning over procedural text. In: Inui K\",\n        \"Wang H (2021) DuReader_robust: A Chinese dataset towards evaluating robustness and generalization of machine reading comprehension in real-world applications\",\n        \"Sun L (2023) ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301\",\n        \"et al (2023) Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca\",\n        \"et al (2022) Galactica: A large language model for science. arXiv preprint arXiv:2211.09085\",\n        \"De Meulder F (2003) Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition\",\n        \"Together (2023) RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models\",\n        \"Gitman I (2024) OpenMathInstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176\",\n        \"et al (2023a) LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971\",\n        \"et al (2023b) LLaMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288\",\n        \"Le QV (2018) A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847\",\n        \"Chen HH (2015) Introduction to SIGHAN 2015 bake-off for Chinese spelling check. In: Yu LC\",\n        \"et al (2022) Multitask prompted training enables zero-shot task generalization. In: International Conference on Learning Representations\",\n        \"Gómez-Rodríguez C (2019) HEAD-QA: A healthcare dataset for complex reasoning. In: Korhonen A\",\n        \"et al (2023) FreshLLMs: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214\",\n        \"Bowman S (2018) GLUE: A multi-task benchmark and analysis platform for natural language understanding. In: Linzen T\",\n        \"et al (2019) SuperGLUE: A stickier benchmark for general-purpose language understanding systems\",\n        \"Li L (2021a) CNewSum: A large-scale summarization dataset with human-annotated adequacy and deducibility level\",\n        \"Liu Y (2023a) OpenChat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235\",\n        \"Ling W (2016) Neural network-based abstract generation for opinions and arguments. In: Knight K\",\n        \"et al (2020a) DuSQL: A large-scale and pragmatic Chinese text-to-SQL dataset. In: Webber B\",\n        \"et al (2023b) Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926\",\n        \"et al (2023c) CMB: A comprehensive medical benchmark in Chinese. arXiv preprint arXiv:2308.08833\",\n        \"et al (2023d) SCIBENCH: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635\",\n        \"et al (2023e) MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691\",\n        \"Shi S (2017) Deep neural solver for math word problems. In: Palmer M\",\n        \"et al (2020b) A large-scale Chinese short-text conversation dataset. In: Zhu X\",\n        \"et al (2021b) YACLC: A Chinese learner corpus with multidimensional annotation. arXiv preprint arXiv:2112.15043\",\n        \"Alipoormolabashi P\",\n        \"et al (2023f) Self-Instruct: Aligning language model with self generated instructions\",\n        \"et al (2023g) PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. arXiv preprint arXiv:2306.05087\",\n        \"Neubig G (2023h) Execution-based evaluation for open-domain code generation. In: Bouamor H\",\n        \"Bowman SR (2019) Neural network acceptability judgments\",\n        \"et al (2022) Finetuned language models are zero-shot learners. In: International Conference on Learning Representations\",\n        \"Wang X (2023) ShenNong-TCM: A traditional Chinese medicine large language model. https://github.com/michael-wzhu/ShenNong-TCM-LLM\",\n        \"et al (2012) OntoNotes release 5.0 with OntoNotes DB tool v0.999 beta. Linguistic Data Consortium pp 1–53\",\n        \"Gardner M (2017) Crowdsourcing multiple choice science questions. In: Derczynski L\",\n        \"Bowman S (2018) A broad-coverage challenge corpus for sentence understanding through inference. In: Walker M\",\n        \"et al (2020) Transformers: State-of-the-art natural language processing. In: Liu Q\",\n        \"Aji AF (2023) LaMini-LM: A diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402\",\n        \"Lee LH (2013) Chinese spelling check evaluation at SIGHAN bake-off 2013. In: Yu LC\",\n        \"Hovy E (2018) Large-scale cloze test dataset created by teachers. In: Riloff E\",\n        \"et al (2024) The FinBen: An holistic financial benchmark for large language models. arXiv preprint arXiv:2402.12659\",\n        \"et al (2022) UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In: Goldberg Y\",\n        \"et al (2017) CN-DBpedia: A never-ending Chinese knowledge extraction system. In: Benferhat S\",\n        \"McAuley J (2023a) Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In: Bouamor H\",\n        \"et al (2023b) WizardLM: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244\",\n        \"Cambria E (2023c) Are large language models really good logical reasoners? A comprehensive evaluation from deductive\",\n        \"et al (2023d) CValues: Measuring the values of Chinese large language models from safety to responsibility. arXiv preprint arXiv:2307.09705\",\n        \"et al (2020a) CLUENER2020: Fine-grained named entity recognition dataset and benchmark for Chinese. arXiv preprint arXiv:2001.04351\",\n        \"et al (2020b) CLUE: A Chinese language understanding evaluation benchmark. In: Scott D\",\n        \"Dong Q (2020c) CLUECorpus2020: A large-scale Chinese corpus for pre-training language model. arXiv preprint arXiv:2003.01355\",\n        \"et al (2021) FewCLUE: A Chinese few-shot learning evaluation benchmark. arXiv preprint arXiv:2107.07498\",\n        \"et al (2023e) SuperCLUE: A comprehensive Chinese large language model benchmark. arXiv preprint arXiv:2307.15020\",\n        \"Xu M (2023) MedicalGPT: Training medical GPT model\",\n        \"Zhang J (2023f) On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504\",\n        \"In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n        \"Li Z (2017) Building task-oriented dialogue systems for online shopping\",\n        \"et al (2023a) Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305\",\n        \"et al (2023b) RedGPT: Reference-enlightened-dialogue by GPT and for GPT. https://github.com/ziliwangnlp/RedGPT\",\n        \"Yang J (2023) Firefly: Chinese conversational large language models\",\n        \"et al (2023c) GLUE-X: Evaluating natural language understanding models from an out-of-distribution generalization perspective. In: Rogers A\",\n        \"et al (2023d) Zhongjing: Enhancing the Chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue\",\n        \"Meek C (2015) WikiQA: A challenge dataset for open-domain question answering. In: Màrquez L\",\n        \"Baldridge J (2019) PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In: Inui K\",\n        \"et al (2018) HotpotQA: A dataset for diverse\",\n        \"et al (2019) DocRED: A large-scale document-level relation extraction dataset. In: Korhonen A\",\n        \"et al (2021) CUGE: A Chinese language understanding and generation evaluation benchmark. arXiv preprint arXiv:2112.13610\",\n        \"Ren X (2021) CrossFit: A few-shot learning challenge for cross-task generalization in NLP. In: Moens MF\",\n        \"In: The 2023 Conference on Empirical Methods in Natural Language Processing\",\n        \"Wan X (2023b) ALCUNA: Large language models meet new knowledge. In: Bouamor H\",\n        \"Yu D (2020a) Dialogue-based relation extraction. In: Jurafsky D\",\n        \"et al (2023a) KoLA: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296\",\n        \"et al (2023b) Taoli LLaMA. https://github.com/blcuicall/taoli\",\n        \"Chen HH (2014) Overview of SIGHAN 2014 bake-off for Chinese spelling check. In: Sun L\",\n        \"et al (2018) Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In: Riloff E\",\n        \"Feng J (2020b) ReClor: A reading comprehension dataset requiring logical reasoning. In: International Conference on Learning Representations\",\n        \"et al (2023) Revisiting out-of-distribution robustness in NLP: Benchmarks\",\n        \"et al (2021) WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models\",\n        \"et al (2023) DISC-LawLLM: Fine-tuning large language models for intelligent legal services. arXiv preprint arXiv:2309.11325\",\n        \"Choi Y (2019a) HellaSwag: Can a machine really finish your sentence? In: Korhonen A\",\n        \"et al (2019b) Defending against neural fake news\",\n        \"et al (2023a) GLM-130B: An open bilingual pre-trained model. In: The Eleventh International Conference on Learning Representations\",\n        \"et al (2020) MedDialog: Large-scale medical dialogue datasets. In: Webber B\",\n        \"Zeng H (2023) Measuring massive multitask Chinese understanding\",\n        \"Zhang N (2023b) Evaluating the generation capabilities of large Chinese language models. arXiv preprint arXiv:2308.04823\",\n        \"et al (2023a) Chinese open instruction generalist: A preliminary release. arXiv preprint arXiv:2304.07987\",\n        \"Song Y (2020a) WinoWhy: A deep diagnosis of essential commonsense knowledge for answering winograd schema challenge. In: Jurafsky D\",\n        \"et al (2023b) HuatuoGPT\",\n        \"et al (2023c) DialogStudio: Towards richest and most diverse unified dataset collection for conversational AI. arXiv preprint arXiv:2307.10172\",\n        \"et al (2023d) FinEval: A Chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint arXiv:2308.09975\",\n        \"et al (2023e) LLMEval-2. https://github.com/llmeval/llmeval-2\",\n        \"et al (2023f) LLMEval-1. https://github.com/llmeval/llmeval-1\",\n        \"et al (2022) CBLUE: A Chinese biomedical language understanding evaluation benchmark. In: Muresan S\",\n        \"Tetreault J (2019) This email could save your life: Introducing the task of email subject line generation. In: Korhonen A\",\n        \"Van Durme B (2018a) ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885\",\n        \"Liu S (2018b) Multi-scale attentive interaction networks for Chinese medical question answer selection\",\n        \"et al (2023g) Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792\",\n        \"et al (2023h) BayLing: Bridging cross-lingual alignment and instruction following through interactive translation for large language models\",\n        \"Bing L (2023i) M3Exam: A multilingual\",\n        \"0: A large Chinese financial chat model with hundreds of billions parameters\",\n        \"LeCun Y (2015) Character-level convolutional networks for text classification. In: Cortes C\",\n        \"et al (2023j) InfiniteBench: 128k long-context benchmark for language models. https://github.com/OpenBMB/InfiniteBench?tab=readme-ov-file\",\n        \"Qiu X (2023k) Evaluating the performance of large language models on GAOKAO benchmark. arXiv preprint arXiv:2305.12474\",\n        \"et al (2023l) Wider and deeper LLM networks are fairer LLM evaluators. arXiv preprint arXiv:2308.01862\",\n        \"Yang J (2018) Chinese NER using lattice LSTM. In: Gurevych I\",\n        \"Manning CD (2017) Position-aware attention and supervised data improve slot filling. In: Palmer M\",\n        \"He L (2019) PAWS: Paraphrase adversaries from word scrambling. In: Burstein J\",\n        \"et al (2020b) DIALOGPT: Large-scale generative pre-training for conversational response generation. In: Celikyilmaz A\",\n        \"et al (2023m) Siren’s song in the AI ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219\",\n        \"et al (2023n) SafetyBench: Evaluating the safety of large language models with multiple choice questions. arXiv preprint arXiv:2309.07045\",\n        \"Liu J (2020) Ape210K: A large-scale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506\",\n        \"et al (2023) A survey of large language models. arXiv preprint arXiv:2303.18223\",\n        \"Sun A (2019) ChID: A large-scale Chinese idiom dataset for cloze test. In: Korhonen A\",\n        \"Polu S (2022) MiniF2F: A cross-system benchmark for formal Olympiad-level mathematics. In: International Conference on Learning Representations\",\n        \"et al (2023a) LMSYS-Chat-1M: A large-scale real-world LLM conversation dataset. arXiv preprint arXiv:2309.11998\",\n        \"Sun M (2020) JEC-QA: A legal-domain question answering dataset\",\n        \"et al (2023) AGIEval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364\",\n        \"et al (2023a) LIMA: Less is more for alignment. arXiv preprint arXiv:2305.11206\",\n        \"et al (2023b) Don’t make your LLM an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964\",\n        \"et al (2022) METS-CoV: A dataset of medical entity and targeted sentiment on COVID-19 related tweets. In: Koyejo S\",\n        \"Zeng M (2021) MediaSum: A large-scale media interview dataset for dialogue summarization. In: Toutanova K\",\n        \"et al (2022) DuQM: A Chinese dataset of linguistically perturbed natural questions for evaluating the robustness of question matching models\",\n        \"et al (2023) PromptBench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528\",\n        \"Wang X (2023) ChatMed: A Chinese medical large language model. https://github.com/michael-wzhu/ChatMed\",\n        \"et al (2015) Aligning books and movies: Towards story-like visual explanations by watching movies and reading books\",\n        \"Li C (2023) Luotuo: An instruction-following Chinese language model\",\n        \"et al (2019) Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593\",\n        \"Junczys-Dowmunt M\"\n    ],\n    \"reasoning\": \"Selected these 403 references because they represent foundational work and recent advances in Datasets for Large Language Models: A Comprehensive Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}