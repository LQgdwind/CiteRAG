{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: On Equivalent Optimization of Machine Learning Methods\n\nPaper Abstract: Abstract At the core of many machine learning methods resides an iterative optimization algorithm for their training. Such optimization algorithms often come with a plethora of choices regarding their implementation. In the case of deep neural networks, choices of optimizer, learning rate, batch size, etc. must be made. Despite the fundamental way in which these choices impact the training of deep neural networks, there exists no general method for identifying when they lead to equivalent, or non-equivalent, optimization trajectories. By viewing iterative optimization as a discrete-time dynamical system, we are able to leverage Koopman operator theory, where it is known that conjugate dynamics can have identical spectral objects. We find highly overlapping Koopman spectra associated with the application of online mirror and gradient descent to specific problems, illustrating that such a data-driven approach can corroborate the recently discovered analytical equivalence between the two optimizers. We extend our analysis to feedforward, fully connected neural networks, providing the first general characterization of when choices of learning rate, batch size, layer width, data set, and activation function lead to equivalent, and non-equivalent, evolution of network parameters during training. Among our main results, we find that learning rate to batch size ratio, layer width, nature of data set (handwritten vs. synthetic), and activation function affect the nature of conjugacy. Our data-driven approach is general and can be utilized broadly to compare the optimization of machine learning methods.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Hamiltonian systems and transformation in hilbert space\",\n        \"Dynamical systems of continuous spectra\",\n        \"Spectral properties of dynamical systems, model reduction and decompositions\",\n        \"Applied koopmanism\",\n        \"Modern koopman theory for dynamical systems\",\n        \"Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the koopman operator\",\n        \"Isostables, isochrons, and koopman spectrum for the action–angle representation of stable fixed point dynamics\",\n        \"Spectrum of the koopman operator, spectral expansions in functional spaces, and state-space geometry\",\n        \"Optimizing neural networks via koopman operator theory\",\n        \"Accelerating training in artificial neural networks with dynamic mode decomposition\",\n        \"Applications of koopman mode analysis to neural networks\",\n        \"A koopman approach to understanding sequence neural models\",\n        \"Predicting the critical number of layers for hierarchical support vector regression\",\n        \"An operator theoretic perspective on pruning deep neural networks\",\n        \"Learning to optimize with dynamic mode decomposition\",\n        \"Koopman operator learning for accelerating quantum optimization and machine learning\",\n        \"Credit assignment for trained neural networks based on koopman operator theory\",\n        \"On the koopman operator of algorithms\",\n        \"Algorithmic (semi-) conjugacy via koopman operator theory\",\n        \"Reparameterizing mirror descent as gradient descent\",\n        \"Winnowing with gradient descent\",\n        \"Non-convex online learning via algorithmic equivalence\",\n        \"On comparison of dynamics of dissipative and finite-time systems using koopman operator methods**the funding provided by aro grant w911nf-11-1-0511\",\n        \"Linearization in the large of nonlinear systems and koopman operator spectrum\",\n        \"Data-driven spectral analysis of the koopman operator\",\n        \"Rigorous data-driven computation of spectral properties of koopman operators for dynamical systems\",\n        \"Existence and uniqueness of global koopman eigenfunctions for stable fixed points and periodic orbits\",\n        \"On numerical approximations of the koopman operator\",\n        \"Dynamic mode decomposition of numerical and experimental data\",\n        \"Spectral analysis of nonlinear flows\",\n        \"On dynamic mode decomposition: Theory and applications\",\n        \"Variants of dynamic mode decomposition: Boundary condition, koopman, and fourier analyses\",\n        \"A data–driven approximation of the koopman operator: Extending dynamic mode decomposition\",\n        \"Data driven modal decompositions: analysis and enhancements\",\n        \"Analysis of fluid flows via spectral properties of the koopman operator\",\n        \"Delay-coordinate maps and the spectra of koopman operators\",\n        \"Practical recommendations for gradient-based training of deep architectures\",\n        \"Turbulence and the dynamics of coherent structures. i. coherent structures\",\n        \"Koopman operator family spectrum for nonautonomous systems\",\n        \"Koopman operator spectrum for random dynamical systems\",\n        \"Robust approximation of the stochastic koopman operator\",\n        \"Three factors influencing minima in sgd\",\n        \"Dynamical Systems and Numerical ANalysis\",\n        \"Linear algebra algorithms as dynamical systems\",\n        \"Analysis and design of optimization algorithms via integral quadratic constraints\",\n        \"An automatic system to detect equivalence between iterative algorithms\",\n        \"On the algebraic structure of feedforward network weight spaces\",\n        \"The role of permutation invariance in linear mode connectivity of neural networks\",\n        \"Git re-basin: Merging models modulo permutation symmetries\",\n        \"Transformations between deep neural networks\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in On Equivalent Optimization of Machine Learning Methods, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}