{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 174 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Pruning and Quantization for Deep Neural Network Acceleration: A Survey\n\nPaper Abstract: Abstract Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.\n\n{retrieved_text}\n\nGenerate exactly 174 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 174\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\n        \"Convolutional Neural Networks for Speech Recognition\",\n        \"Accelerating CNN inference on FPGAs: A Survey\",\n        \"FPGAs Enable the Next Generation of Communication and Networking Solutions\",\n        \"convnet-burden\",\n        \"Ternary neural networks for resource-efficient AI applications\",\n        \"Radeon Instinct™ MI25 Accelerator\",\n        \"ARM Architecture Reference Manual ARMv8, for ARMv8-A architecture profile\",\n        \"Arm Cortex-M Processor Comparison Table\",\n        \"MALI-G76 High-Performance GPU for Complex Graphics Features and Bene ts High Performance for Mixed Realities\",\n        \"Neon technology introduction\",\n        \"Pruning algorithms of neural networks - A comparative study\",\n        \"PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice\",\n        \"Weight quantization in Boltzmann machines\",\n        \"Scalable methods for 8-bit training of neural networks\",\n        \"Post training 4-bit quantization of convolutional networks for rapid-deployment\",\n        \"Sparse Convolutional Neural Networks\",\n        \"UNIQ: Uniform Noise Injection for Non-Uniform Quantization of Neural Networks\",\n        \"Conditional Computation in Neural Networks for faster models\",\n        \"Estimating or Propagating Gradients Through Stochastic Neurons\",\n        \"MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?\",\n        \"BinaryDenseNet: Developing an architecture for binary neural networks\",\n        \"Benchmark analysis of representative deep neural network architectures\",\n        \"What is the State of Neural Network Pruning?\",\n        \"Adaptive Neural Networks for Efficient Inference\",\n        \"Language Models are Few-Shot Learners\",\n        \"BUG1989/caffe-int8-convert-tools: Generate a quantization parameter file for ncnn framework int8 inference\",\n        \"Once-for-All: Train One Network and Specialize it for Efficient Deployment\",\n        \"SeerNet : Predicting Convolutional Neural Network Feature-Map Sparsity through Low-Bit Quantization\",\n        \"High Performance Convolutional Neural Networks for Document Processing, in: Tenth International Workshop on Frontiers in Handwriting Recognition\",\n        \"DianNao family: Energy-Efficient Hardware Accelerators for Machine Learning\",\n        \"Recent advances in efficient computation of deep convolutional neural networks\",\n        \"A Survey of Model Compression and Acceleration for Deep Neural Networks\",\n        \"Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation\",\n        \"Solving local minima problem with large number of hidden nodes on two-layered feed-forward artificial neural networks\",\n        \"PACT: Parameterized Clipping Activation for Quantized Neural Networks\",\n        \"Towards the Limit of Network Quantization, in: International Conference on Learning Representations(ICLR), IEEE\",\n        \"Energy-Efficient Design of Processing Element for Convolutional Neural Network\",\n        \"A comprehensive survey on model compression and acceleration\",\n        \"Intel ® AVX-512 Instructions and Their Use in the Implementation of Math Functions\",\n        \"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\",\n        \"Feature selection for classification\",\n        \"8-Bit Approximations for Parallelism in Deep Learning, in: International Conference on Learning Representations(ICLR)\",\n        \"More is less: A more complicated network with less inference complexity\",\n        \"A set of level 3 basic linear algebra subprograms\",\n        \"QNNPACK: Open source library for optimized mobile deep learning - Facebook Engineering\",\n        \"DeepShift: Towards Multiplication-Less Neural Networks\",\n        \"Neural Architecture Search\",\n        \"A new pruning heuristic based on variance analysis of sensitivity information\",\n        \"Convolutional networks for fast, energy-efficient neuromorphic computing\",\n        \"SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\n        \"Weight discretization paradigm for optical neural networks\",\n        \"Intel® FPGA Development Tools - Intel FPGA\",\n        \"The lottery ticket hypothesis: Finding sparse, trainable neural networks, in: International Conference on Learning Representations(ICLR)\",\n        \"Neocognitron: A hierarchical neural network capable of visual pattern recognition\",\n        \"The State of Sparsity in Deep Neural Networks\",\n        \"HSA-enabled DSPs and accelerators\",\n        \"Compressing Deep Convolutional Networks using Vector Quantization, in: International Conference on Learning Representations(ICLR)\",\n        \"Hosted models | TensorFlow Lite\",\n        \"google/gemmlowp: Low-precision matrix multiplication\",\n        \"Knowledge Distillation: A Survey\",\n        \"Low-Precision Batch-Normalized Activations\",\n        \"Adaptive Computation Time for Recurrent Neural Networks\",\n        \"ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks\",\n        \"Angel-Eye: A complete design flow for mapping CNN onto embedded FPGA\",\n        \"A Survey of FPGA-Based Neural Network Accelerator\",\n        \"A Survey on Methods and Theories of Quantized Neural Networks\",\n        \"Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks\",\n        \"DSD: Dense-Sparse-Dense Training for Deep Neural Networks, in: International Conference on Learning Representations(ICLR)\",\n        \"Deep Speech: Scaling up end-to-end speech recognition\",\n        \"Optimal brain surgeon and general network pruning\",\n        \"Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration\",\n        \"Neural networks for machine learning\",\n        \"Improving neural networks by preventing co-adaptation of feature detectors\",\n        \"Loss-aware Binarization of Deep Networks, in: International Conference on Learning Representations(ICLR)\",\n        \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\",\n        \"Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures\",\n        \"Multi-scale dense networks for resource efficient image classification, in: International Conference on Learning Representations(ICLR)\",\n        \"Labeled faces in the wild: Updates and new reporting procedures\",\n        \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\",\n        \"AI benchmark: All about deep learning on smartphones in 2019\",\n        \"PowerVR - embedded graphics processors powering iconic products\",\n        \"OpenVINO™ Toolkit\",\n        \"Dissecting the graphcore IPU architecture via microbenchmarking\",\n        \"ImageNet: A large-scale hierarchical image database\",\n        \"In-Datacenter Performance Analysis of a Tensor Processing Unit\",\n        \"Cnvlutin2: Ineffectual-Activation-and-Weight-Free Deep Neural Network Computing\",\n        \"Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss\",\n        \"CMSIS NN Software Library\",\n        \"Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks\",\n        \"Quantizing deep convolutional networks for efficient inference: A whitepaper\",\n        \"Learning Multiple Layers of Features from Tiny Images\",\n        \"MLIR: A Compiler Infrastructure for the End of Moore’s Law\",\n        \"Speeding-up convolutional neural networks: A survey\",\n        \"Deep learning\",\n        \"Gradient-based learning applied to document recognition\",\n        \"SnIP: Single-shot network pruning based on connection sensitivity, in: International Conference on Learning Representations(ICLR)\",\n        \"Survey of Deep Neural Network Model Compression\",\n        \"Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM\",\n        \"The cascading neural network: building the Internet of Smart Things\",\n        \"Ternary Weight Networks, in: Advances in Neural Information Processing Systems (NIPS)\",\n        \"Pruning Filters for Efficient ConvNets, in: International Conference on Learning Representations (ICLR)\",\n        \"The Deep Learning Compiler: A Comprehensive Survey\",\n        \"A survey of neural network accelerators\",\n        \"A survey of FPGA design for AI era\",\n        \"Neural Networks with Few Multiplications, in: International Conference on Learning Representations(ICLR)\",\n        \"Tensor Completion for Estimating Missing Values in Visual Data\",\n        \"MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning, in: IEEE International Conference on Computer Vision\",\n        \"Bi-Real Net: Enhancing the performance of 1-bit CNNs with improved representational capability and advanced training algorithm\",\n        \"AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference\",\n        \"ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression\",\n        \"Scalable and modularized RTL compilation of Convolutional Neural Networks onto FPGA\",\n        \"Coincidence Approach To Stochastic Point Process\",\n        \"Fast Training of Convolutional Networks through FFTs\",\n        \"Habana Labs presentation\",\n        \"Ternary Neural Networks with Fine-Grained Quantization\",\n        \"Deep neural networks are robust to weight binarization and other non-linear distortions\",\n        \"Mixed Precision Training, in: International Conference on Learning Representations(ICLR)\",\n        \"8-bit inference with TensorRT\",\n        \"Convolutional Neural Networks using Logarithmic Data Representation\",\n        \"Rounding Methods for Neural Networks with Low Resolution Synaptic Weights\",\n        \"An Overview of Neural Network Compression\",\n        \"NVIDIA GeForce GTX 980 Featuring Maxwell, The Most Advanced GPU Ever Made\",\n        \"NVIDIA DGX-1 With Tesla V100 System Architecture\",\n        \"NVIDIA Tesla V100 GPU Volta Architecture\",\n        \"NVIDIA Turing GPU Architecture\",\n        \"onnx/onnx: Open standard for machine learning interoperability\",\n        \"PyTorch : An Imperative Style , High-Performance Deep Learning Library\",\n        \"Channel-level acceleration of deep face representations\",\n        \"Binary neural networks: A survey\",\n        \"Pruning Algorithms - A Survey\",\n        \"Introducing the Intel® Vision Accelerator Design with Intel® Arria® 10 FPGA\",\n        \"Lower Numerical Precision Deep Learning Inference and Training\",\n        \"Glow: Graph lowering compiler techniques for neural networks\",\n        \"The State of Knowledge Distillation for Classification\",\n        \"ImageNet Large Scale Visual Recognition Challenge\",\n        \"Training Feed Forward Nets with Binary Weights Via a Modified CHIR Algorithm\",\n        \"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, in: International Conference on Learning Representations(ICLR)\",\n        \"Quantizing Convolutional Neural Networks for Low-Power High-Throughput Inference Engines\",\n        \"Searching for accurate binary neural architectures\",\n        \"A Quantization-Friendly Separable Convolution for MobileNets\",\n        \"A review of binarized neural networks\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"Categorizing Malware via A Word2Vec-based Temporal Convolutional Network Scheme\",\n        \"Learning Pooling for Convolutional Neural Network\",\n        \"Efficient Processing of Deep Neural Networks: A Tutorial and Survey\",\n        \"Fixed Point Quantization\",\n        \"Snapdragon Neural Processing Engine SDK\",\n        \"NCNN is a high-performance neural network inference framework optimized for the mobile platform\",\n        \"Regression shrinkage and selection via the Lasso\",\n        \"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference\",\n        \"Efficient Inference with TensorRT\",\n        \"Toolflows for Mapping Convolutional Neural Networks on FPGAs\",\n        \"Integer quantization for deep learning inference: Principles and empirical evaluation\",\n        \"Training and Inference with Integers in Deep Neural Networks, in: International Conference on Learning Representations (ICLR)\",\n        \"L1-Norm Batch Normalization for Efficient Training of Deep Neural Networks\",\n        \"MACE is a deep learning inference framework optimized for mobile heterogeneous computing platforms\",\n        \"Accelerating DNNs with Xilinx Alveo Accelerator Cards (WP504)\",\n        \"Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks, in: International Conference on Learning Representations (ICLR) - Workshop\",\n        \"Training high-performance and large-scale deep neural networks with full 8-bit integers\",\n        \"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers\",\n        \"Blended coarse gradient descent for full quantization of deep neural networks\",\n        \"Scalpel: Customizing DNN pruning to the underlying hardware parallelism\",\n        \"Model selection and estimation in regression with grouped variables\",\n        \"A dual-attention recurrent neural network method for deep cone thickener underflow concentration prediction\",\n        \"Recent Advances in Convolutional Neural Network Acceleration\",\n        \"Exploiting Channel Similarity for Accelerating Deep Convolutional Neural Networks\",\n        \"Towards Lower Bit Multiplication for Convolutional Neural Network Training\",\n        \"Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights, in: International Conference on Learning Representations(ICLR)\",\n        \"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\",\n        \"Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks\",\n        \"Towards Unified INT8 Training for Convolutional Neural Network, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\n        \"Structured binary neural networks for accurate image classification and semantic segmentation\",\n        \"Learning Transferable Architectures for Scalable Image Recognition\"\n    ],\n    \"reasoning\": \"Selected these 174 references because they represent foundational work and recent advances in Pruning and Quantization for Deep Neural Network Acceleration: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}