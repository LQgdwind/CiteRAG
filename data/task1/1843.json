{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 128 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Enabling High-Capacity, Latency-Tolerant, and Highly-Concurrent GPU Register Files via Software/Hardware Cooperation\n\nPaper Abstract: Abstract. Graphics Processing Units (GPUs) employ large register files to accommodate all active threads and accelerate context switching. Unfortunately, register files are a scalability bottleneck for future GPUs due to long access latency, high power consumption, and large silicon area provisioning. Prior work proposes hierarchical register file to reduce the register file power consumption by caching registers in a smaller register file cache. Unfortunately, this approach does not improve register access latency due to the low hit rate in the register file cache. In this paper, we propose the Latency-Tolerant Register File (LTRF) architecture to achieve low latency in a two-level hierarchical structure while keeping power consumption low. We observe that compile-time interval analysis enables us to divide GPU program execution into intervals with an accurate estimate of a warp’s aggregate register working-set within each interval. The key idea of LTRF is to prefetch the estimated register working-set from the main register file to the register file cache under software control, at the beginning of each interval, and overlap the prefetch latency with the execution of other warps. We observe that register bank conflicts while prefetching the registers could greatly reduce the effectiveness of LTRF. Therefore, we devise a compile-time register renumbering technique to reduce the likelihood of register bank conflicts. Our experimental results show that LTRF enables high-capacity yet long-latency main GPU register files, paving the way for various optimizations. As an example optimization, we implement the main register file with emerging high-density high-latency memory technologies, enabling 8×\\times larger capacity and improving overall GPU performance by 34%.\n\n{retrieved_text}\n\nGenerate exactly 128 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 128\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Warped register file: A power efficient register file for GPGPUs\",\n        \"Pilot Register File: Energy efficient partitioned register file for GPUs\",\n        \"A scalable processing-in-memory accelerator for parallel graph processing\",\n        \"Compilers: principles, techniques and tools (for Anna University), 2/e\",\n        \"Data prefetching by dependence graph precomputation\",\n        \"Racetrack memory cell array with integrated magnetic tunnel junction readout\",\n        \"BOW: Breathing operand windows to exploit bypassing in GPUs\",\n        \"Self-contained, accurate precomputation prefetching\",\n        \"Numerical analysis of domain wall propagation for dense memory arrays\",\n        \"Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance\",\n        \"Prefetching with adaptive cache culling for striped disk arrays\",\n        \"On-chip network design considerations for compute accelerators\",\n        \"Throughput-effective on-chip networks for manycore accelerators\",\n        \"Designing on-chip networks for throughput accelerators\",\n        \"Analyzing CUDA workloads using a detailed GPU simulator\",\n        \"Fast Data Delivery for Many-Core Processors\",\n        \"An efficient temporal data prefetcher for L1 caches\",\n        \"Domino temporal data prefetcher\",\n        \"Bingo Spatial Data Prefetcher\",\n        \"Evaluation of hardware data prefetchers on server processors\",\n        \"Reducing the complexity of the register file in dynamic superscalar processors\",\n        \"Dspatch: Dual spatial pattern prefetcher\",\n        \"Vertical tunnel field-effect transistor\",\n        \"Loose loops sink chips\",\n        \"Register allocation via graph coloring\",\n        \"Speculative precomputation on chip multiprocessors\",\n        \"Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling\",\n        \"Disk Prefetching Mechanisms for Increasing HTTP Streaming Video Server Throughput\",\n        \"Register allocation via coloring\",\n        \"Simultaneous subordinate microthreading (SSMT)\",\n        \"Rodinia: A benchmark suite for heterogeneous computing\",\n        \"Effective hardware-based data prefetching for high-performance processors\",\n        \"Application-aware prefetch prioritization in on-chip networks\",\n        \"Dynamic hot data stream prefetching for general-purpose programs\",\n        \"Dynamic speculative precomputation\",\n        \"Speculative precomputation: Long-range prefetching of delinquent loads\",\n        \"Compiler-controlled memory\",\n        \"Introduction to algorithms\",\n        \"Multiple-banked register file architectures\",\n        \"DiskSeen: Exploiting disk layout and access history to enhance I/O prefetch.\",\n        \"NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory\",\n        \"Prefetch-aware shared resource management for multi-core systems\",\n        \"Coordinated control of multiple prefetchers in multi-core systems\",\n        \"Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems\",\n        \"CORF: Coalescing operand register file for GPUs.\",\n        \"Proactive instruction fetch\",\n        \"Loop-aware memory prefetching using code block working sets\",\n        \"Low-current perpendicular domain wall motion cell for scalable high-speed MRAM\",\n        \"Energy-efficient mechanisms for managing thread context in throughput processors\",\n        \"A compile-time managed multi-level register file hierarchy\",\n        \"Unifying primary cache, scratch, and register file memories in a throughput processor\",\n        \"Stephen W Keckler\",\n        \"James K Archibald\",\n        \"LTRF Register-Interval-Algorithm\",\n        \"Jayanth Gummaraju and Mendel Rosenblum\",\n        \"Milad Hashemi and Yale N Patt\",\n        \"Flow analysis of computer programs\",\n        \"Akanksha Jain and Calvin Lin\",\n        \"Gokul Subramanian Ravi\",\n        \"George Michelogiannakis\",\n        \"Shrikanth Ganapathy\",\n        \"Nachiappan Chidambaram Nachiappan\",\n        \"Mahmut T Kandemir\",\n        \"Antonio González\",\n        \"Mahmut Taylan Kandemir\",\n        \"Ashutosh Pattnaik\",\n        \"H. Asghari Esfeden\",\n        \"Hodjat Asghari Esfeden\",\n        \"Juan Del Cuvillo\",\n        \"Dongkeun Kim and Donald Yeung\",\n        \"Algorithm design\",\n        \"Jonathan Beaumont\",\n        \"Anand Sivasubramaniam\",\n        \"Junjie Lai and André Seznec\",\n        \"Chris Lattner and Vikram Adve\",\n        \"Nagesh B Lakshminarayana\",\n        \"Tayler Hetherington\",\n        \"A Fernandez-Pacheco\",\n        \"Rachata Ausavarungnirun\",\n        \"Shuaiwen Leon Song\",\n        \"Gerolf Hoflehner\",\n        \"Simulating multiported memories using lower port count memories\",\n        \"William J Schmidt\",\n        \"Chi-Keung Luk and Todd C Mowry\",\n        \"Mohammad Sadrosadati\",\n        \"A. Fakhrzadehgan\",\n        \"Akshitha Sriraman\",\n        \"Saurabh Mookerjea and Suman Datta\",\n        \"Rajeev Balasubramonian\",\n        \"David N Armstrong\",\n        \"Michael Shebanow\",\n        \"White paper: NVIDIA GeForce GTX 980\",\n        \"White paper: NVIDIA Tesla P100\",\n        \"Masamitsu Hayashi\",\n        \"Conflict-free register allocation using a multi-bank register file with input operand alignment\",\n        \"Informed prefetching and caching\",\n        \"Nandita Vijaykumar\",\n        \"Massimiliano Poletto and Vivek Sarkar\",\n        \"and Rajeev Balasubramonian\",\n        \"Vijayalakshmi Srinivasan\",\n        \"Oliverio J Santana\",\n        \"William M Reddick and Gehan AJ Amaratunga\",\n        \"Amir Roth and Gurindar S Sohi\",\n        \"Seyed Borna Ehsani\",\n        \"Amirhossein Mirhosseini\",\n        \"Hamed Abbasitabar\",\n        \"Mohammad Arjomand\",\n        \"Ankit Sethia and Scott Mahlke\",\n        \"Rangharajan Venkatesan\",\n        \"Ahmad Sharif and Hsien-Hsin S Lee\",\n        \"Masahiro Goshima\",\n        \"Krishnan Ramakrishnan\",\n        \"Thomas F Wenisch\",\n        \"Seung Woo Son and Mahmut Kandemir\",\n        \"Parboil: A revised benchmark suite for scientific and commercial throughput computing\",\n        \"Enabling efficient RDMA-based synchronous mirroring of persistent memory transactions\",\n        \"Christopher Frost\",\n        \"Shankar Ganesh Ramasubramanian\",\n        \"Phillip B Gibbons\",\n        \"Diptesh Majumdar\",\n        \"Gennady Pekhimenko\",\n        \"Complementary tunneling-FETs (CTFET) in CMOS technology\",\n        \"Kathryn S McKinley\",\n        \"Anastasia Ailamaki\",\n        \"Christopher J Hughes\",\n        \"Xiaotong Zhuang and Santosh Pande\",\n        \"Craig Zilles and Gurindar Sohi\",\n        \"Controller for a synchronous DRAM that maximizes throughput by allowing memory requests and commands to be issued out of order\"\n    ],\n    \"reasoning\": \"Selected these 128 references because they represent foundational work and recent advances in Enabling High-Capacity, Latency-Tolerant, and Highly-Concurrent GPU Register Files via Software/Hardware Cooperation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}