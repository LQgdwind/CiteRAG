{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Intercategorical Label Interpolation for Emotional Face Generation with Conditional Generative Adversarial Networks\n\nPaper Abstract: Abstract Generative adversarial networks offer the possibility to generate deceptively real images that are almost indistinguishable from actual photographs. Such systems however rely on the presence of large datasets to realistically replicate the corresponding domain. This is especially a problem if not only random new images are to be generated, but specific (continuous) features are to be co-modeled. A particularly important use case in Human-Computer Interaction (HCI) research is the generation of emotional images of human faces, which can be used for various use cases, such as the automatic generation of avatars. The problem hereby lies in the availability of training data. Most suitable datasets for this task rely on categorical emotion models and therefore feature only discrete annotation labels. This greatly hinders the learning and modeling of smooth transitions between displayed affective states. To overcome this challenge, we explore the potential of label interpolation to enhance networks trained on categorical datasets with the ability to generate images conditioned on continuous features.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Towards principled methods for training generative adversarial networks\",\n        \"Wasserstein generative adversarial networks\",\n        \"The montreal set of facial displays of emotion\",\n        \"Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation\",\n        \"Stargan: Unified generative adversarial networks for multi-domain image-to-image translation\",\n        \"Exprgan: Facial expression editing with controllable expression intensity\",\n        \"Faces—a database of facial expressions in young, middle-aged, and older women and men: Development and validation\",\n        \"Synthetic data augmentation using gan for improved liver lesion classification\",\n        \"Conditional generative adversarial nets for convolutional face generation\",\n        \"When a talking-face computer agent is half-human and half-humanoid: Human identity and consistency preference\",\n        \"Generative adversarial networks\",\n        \"Improved training of wasserstein gans\",\n        \"Attgan: Facial attribute editing by only changing what you want\",\n        \"Progressive growing of gans for improved quality, stability, and variation\",\n        \"Socially-aware personality adaptation\",\n        \"Afew-va database for valence and arousal estimation in-the-wild\",\n        \"Learning multiple layers of features from tiny images (technical report)\",\n        \"Motivated attention: Affect, activation, and action\",\n        \"International affective picture system (iaps): Technical manual and affective ratings\",\n        \"Conditional image-to-image translation\",\n        \"Unsupervised image-to-image translation networks\",\n        \"The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression\",\n        \"Bagan: Data augmentation with balancing gan\",\n        \"Japanese and Caucasian facial expressions of emotion (JACFEE)\",\n        \"Framework for a comprehensive description and measurement of emotional states\",\n        \"An evolutionary-based generative approach for audio data augmentation\",\n        \"On the potential of modular voice conversion for virtual agents\",\n        \"Continuous emotions: Exploring label interpolation in conditional generative adversarial networks for face generation\",\n        \"Data augmentation for semantic segmentation in the context of carbon fiber defect detection using adversarial learning\",\n        \"Conditional generative adversarial nets\",\n        \"Affectnet: A database for facial expression, valence, and arousal computing in the wild\",\n        \"Unsupervised representation learning with deep convolutional generative adversarial networks\",\n        \"Personalized synthesis of intentional and emotional non-verbal sounds for social robots\",\n        \"Xgan: Unsupervised image-to-image translation for many-to-many mappings\",\n        \"Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant\",\n        \"Mobilenetv2: Inverted residuals and linear bottlenecks\",\n        \"Unsupervised domain extension for nighttime semantic segmentation in urban scenes\",\n        \"Taming the chaos: exploring graphical input vector manipulation user interfaces for gans in a musical context\",\n        \"Interactive evolution and exploration within latent level-design space of generative adversarial networks\",\n        \"Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"Macbrain face stimulus set\",\n        \"Let’s get dirty: Gan based data augmentation for camera lens soiling detection in autonomous driving\",\n        \"Validation of the amsterdam dynamic facial expression set (adfes)\",\n        \"Exploring Emotional Prototypes in a High Dimensional TTS Latent Space\",\n        \"Covidgan: data augmentation using auxiliary classifier gan for improved covid-19 detection\",\n        \"From attributes to faces: a conditional generative network for face generation\",\n        \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\",\n        \"Data augmentation using conditional gans for facial emotion recognition\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Intercategorical Label Interpolation for Emotional Face Generation with Conditional Generative Adversarial Networks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}