{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 63 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Auto-Split: A General Framework of Collaborative Edge-Cloud AI\n\nPaper Abstract: Abstract. In many industry scale applications, large and resource consuming machine learning models reside in powerful cloud servers. At the same time, large amounts of input data are collected at the edge of cloud. The inference results are also communicated to users or passed to downstream tasks at the edge. The edge often consists of a large number of low-power devices. It is a big challenge to design industry products to support sophisticated deep model deployment and conduct model inference in an efficient manner so that the model accuracy remains high and the end-to-end latency is kept low. This paper describes the techniques and engineering practice behind Auto-Split, an edge-cloud collaborative prototype of Huawei Cloud. This patented technology is already validated on selected applications, is on its way for broader systematic edge-cloud application integration, and is being made available for public use as an automated pipeline service for end-to-end cloud-edge collaborative intelligence deployment. To the best of our knowledge, there is no existing industry product that provides the capability of Deep Neural Network (DNN) splitting. 111Code and demo are available at: https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=5fad1eb4-50b2-4ac9-bcb0-a1f744cf85c7\n\n{retrieved_text}\n\nGenerate exactly 63 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 63\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"ModelArts: Deploying a Model as an Edge Service\",\n        \"Google Anthos\",\n        \"Alibaba Edge Computing APIs\",\n        \"ACIQ: Analytical Clipping for Integer Quantization of neural networks\",\n        \"Post training 4-bit quantization of convolutional networks for rapid-deployment\",\n        \"Language models are few-shot learners\",\n        \"ZeroQ: A Novel Zero Shot Quantization Framework\",\n        \"Software-Defined Camera\",\n        \"Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks\",\n        \"A survey of model compression and acceleration for deep neural networks\",\n        \"PACT: Parameterized Clipping Activation for Quantized Neural Networks\",\n        \"Prema: A predictive multi-task scheduling algorithm for preemptible neural processing units\",\n        \"Low-bit Quantization of Neural Networks for Efficient Inference\",\n        \"Alibaba Edge Computing\",\n        \"ChamNet: Towards Efficient Network Design Through Platform-Aware Model Adaptation\",\n        \"ModelArts Pro Model Deployment\",\n        \"HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision\",\n        \"MindX Edge\",\n        \"Learned step size quantization\",\n        \"Astraea: Deploy AI Services at the Edge in Elegant Ways\",\n        \"Rethinking Pruning for Accelerating Deep Inference At the Edge\",\n        \"PULP-NN: accelerating quantized neural networks on parallel ultra-low-power RISC-V processors\",\n        \"Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding\",\n        \"Channel Pruning for Accelerating Very Deep Neural Networks\",\n        \"Benchmarking neural network robustness to common corruptions and perturbations\",\n        \"Distilling the Knowledge in a Neural Network\",\n        \"Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge\",\n        \"Multi-Scale Dense Networks for Resource Efficient Image Classification\",\n        \"Intel Nervana. Nervana’s Early Exit Inference\",\n        \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\",\n        \"Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge\",\n        \"Quantizing deep convolutional networks for efficient inference: A whitepaper\",\n        \"SPINN: synergistic progressive inference of neural networks over device and cloud\",\n        \"Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing\",\n        \"Benchmarking robustness in object detection: Autonomous driving when winter is coming\",\n        \"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy\",\n        \"Loss Aware Post-training Quantization\",\n        \"Amazon SageMaker Neo\",\n        \"Baidu’s PaddlePaddle\",\n        \"Timeloop: A systematic approach to dnn accelerator evaluation\",\n        \"HiLens Platform and Edge Device\",\n        \"Tencent PocketFlow\",\n        \"Model compression via distillation and quantization\",\n        \"Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers\",\n        \"SCALE-Sim: Systolic CNN Accelerator\",\n        \"Efficient bit allocation for an arbitrary set of quantizers\",\n        \"Platform-Aware Neural Architecture Search for Mobile\",\n        \"Branchynet: Fast inference via early exiting from deep neural networks\",\n        \"Tenstorrent’s Grayskull AI Chip\",\n        \"Not just privacy: Improving performance of private deep learning in mobile cloud\",\n        \"Haq: Hardware-aware automated quantization with mixed precision\",\n        \"Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search\",\n        \"Accelergy: An architecture-level energy estimation methodology for accelerator designs\",\n        \"Energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking\",\n        \"NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\",\n        \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks\",\n        \"SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models\",\n        \"Towards Real-time Cooperative Deep Inference over the Cloud and Edge End Devices\",\n        \"Improving Neural Network Quantization without Retraining using Outlier Channel Splitting\",\n        \"Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression\",\n        \"Adaptive feeding: Achieving fast and accurate detections by adaptively combining object detectors\",\n        \"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\",\n        \"Neural Network Distiller: A Python Package For DNN Compression Research\"\n    ],\n    \"reasoning\": \"Selected these 63 references because they represent foundational work and recent advances in Auto-Split: A General Framework of Collaborative Edge-Cloud AI, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}