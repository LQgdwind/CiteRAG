{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Melody transcription via generative pre-training\n\nPaper Abstract: Abstract Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in melody transcription is building methods which can handle broad audio containing any number of instrument ensembles and musical styles—existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox [1], a generative model of broad music audio, thereby improving performance on melody transcription by 202020% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data—we derive a new dataset containing 505050 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77%percent7777\\% stronger performance on melody transcription relative to the strongest available baseline.111Examples: https://chrisdonahue.com/sheetsage Code: https://github.com/chrisdonahue/sheetsage By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build Sheet Sage, a system capable of transcribing human-readable lead sheets directly from music audio.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Jukebox: A generative model for music\",\n        \"Accompaniment separation and karaoke application based on automatic melody transcription\",\n        \"Music preference and music education: A review of literature\",\n        \"Towards a digital library of popular music\",\n        \"Query by humming: Musical information retrieval in an audio database\",\n        \"Score-informed source separation for musical audio recordings: An overview\",\n        \"Enabling factorized piano music modeling and generation with the MAESTRO dataset\",\n        \"A real-time music scene description system: Detecting melody and bass lines in audio signals\",\n        \"A real-time music-scene-description system: Predominant-F0 estimation for detecting melody and bass lines in real-world audio signals\",\n        \"Melody extraction from polyphonic music signals: Approaches, applications, and challenges\",\n        \"Melody extraction from polyphonic music by deep learning approaches: A review\",\n        \"Joint beat and downbeat tracking with recurrent neural networks\",\n        \"madmom: A new python audio and music signal processing library\",\n        \"Attention is all you need\",\n        \"An end-to-end neural network for polyphonic piano music transcription\",\n        \"Onsets and frames: Dual-objective piano transcription\",\n        \"Codified audio language modeling learns useful representations for music information retrieval\",\n        \"Ten years of MIREX: reflections, challenges and opportunities\",\n        \"audio_to_midi_melodia\",\n        \"Musical note estimation for F0 trajectories of singing voices based on a Bayesian semi-beat-synchronous HMM\",\n        \"Scale-and rhythm-aware musical note estimation for vocal F0 trajectories based on a semi-tatum-synchronous hierarchical hidden semi-markov model\",\n        \"An auditory model based approach for melody detection in polyphonic musical recordings\",\n        \"On the detection of melody notes in polyphonic audio\",\n        \"Automatic transcription of melody, bass line, and chords in polyphonic music\",\n        \"Automatic generation of lead sheets from polyphonic music signals\",\n        \"Automatic melody transcription based on chord transcription\",\n        \"SiPTH: Singing transcription based on hysteresis defined on the pitch-time curve\",\n        \"Computer-aided melody note transcription using the Tony software: Accuracy and efficiency\",\n        \"Bayesian singing transcription based on a hierarchical generative model of keys, musical notes, and F0 trajectories\",\n        \"Audio-to-score singing transcription based on a CRNN-HSMM hybrid model\",\n        \"Spleeter: a fast and efficient music source separation tool with pre-trained models\",\n        \"Automatic music transcription: challenges and future directions\",\n        \"Learning features of music from scratch\",\n        \"Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity\",\n        \"MT3: Multi-task multitrack music transcription\",\n        \"Investigating the perceptual validity of evaluation metrics for automatic piano music transcription\",\n        \"mir_eval: A transparent implementation of common MIR metrics\",\n        \"RWC Music Database: Popular, classical and jazz music databases\",\n        \"RWC Music Database: Music genre database and musical instrument sound database\",\n        \"Development of the RWC Music Database\",\n        \"SurpriseNet: Melody harmonization conditioning on user-controlled surprise contours\",\n        \"Automatic melody harmonization with triad chords: A comparative study\",\n        \"MIREX 2019 submission: Crowd annotation for audio key estimation\",\n        \"TransformerVAE: A hierarchical model for structure-aware and interpretable music representation learning\",\n        \"Sequence-to-sequence piano transcription with transformers\",\n        \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\",\n        \"Analyzing song structure with spectral clustering\",\n        \"Melody extraction from polyphonic music signals using pitch contour characteristics\",\n        \"Hooktheory TheoryTab DB\",\n        \"Chordify: Chord transcription for the masses\",\n        \"Cognitive foundations of musical pitch\",\n        \"What’s key for key? The Krumhansl-Schmuckler key-finding algorithm reconsidered\",\n        \"LilyPond, a system for automated music engraving\",\n        \"AI Song Contest: Human-AI co-creation in songwriting\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Melody transcription via generative pre-training, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}