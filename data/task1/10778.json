{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 56 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: MultiLS: An End-to-End Lexical Simplification Framework\n\nPaper Abstract: Abstract. Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence’s original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portuguese. Model performances are reported, ranging from transformer-based models to more recent large language models (LLMs).\n\n{retrieved_text}\n\nGenerate exactly 56 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 56\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Collection and evaluation of lexical complexity data for Russian language using crowdsourcing\",\n        \"Lexical Simplification System to Improve Web Accessibility\",\n        \"Fostering Digital Inclusion and Accessibility: The PorSimples Project for Simplification of Portuguese Texts\",\n        \"Findings of the WMT 2019 Biomedical Translation Shared Task: Evaluation for MEDLINE Abstracts and Biomedical Terminologies\",\n        \"ReSyf: a French lexicon with ranked synonyms\",\n        \"Test-based age-of-acquisition norms for 44 thousand English word meanings\",\n        \"Word prevalence norms for 62,000 English lemmas\",\n        \"Creating a Silver Standard for Patent Simplification\",\n        \"Unsupervised Cross-lingual Representation Learning at Scale\",\n        \"LCP-RIT at SemEval-2021 Task 1: Exploring Linguistic Features for Lexical Complexity Prediction\",\n        \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n        \"Ethnologue: Languages of the World (twenty-sixth ed.)\",\n        \"ALEXSIS: A Dataset for Lexical Simplification in Spanish\",\n        \"Adaptação Lexical Automática em Textos Informativos do Português Brasileiro para o Ensino Fundamental\",\n        \"Learning a Lexical Simplifier Using Wikipedia\",\n        \"Literacy in Brazil: From Rights to REality\",\n        \"Mistral 7B\",\n        \"Evaluation Dataset and System for Japanese Lexical Simplification\",\n        \"A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese\",\n        \"Automatic prediction of vocabulary knowledge for learners of Chinese as a foreign language\",\n        \"A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification\",\n        \"SemEval-2007 Task 10: English Lexical Substitution Task\",\n        \"Creación de un corpus de textos universitarios en español para la identificación de palabras complejas en el área de la simplificación léxica\",\n        \"ALEXSIS+: Improving Substitute Generation and Selection for Lexical Simplification with Information Retrieval\",\n        \"GMU-WLV at TSAR-2022 Shared Task: Evaluating Lexical Simplification Models\",\n        \"Deep Learning Approaches to Lexical Simplification: A Survey\",\n        \"ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification\",\n        \"An Evaluation of Binary Comparative Lexical Complexity Models\",\n        \"Lexical Complexity Prediction: An Overview\",\n        \"Features of lexical complexity: insights from L1 and L2 speakers\",\n        \"PLUMBErr: An Automatic Error Identification Framework for Lexical Simplification\",\n        \"SemEval 2016 Task 11: Complex Word Identification\",\n        \"LEXenstein: A Framework for Lexical Simplification\",\n        \"Benchmarking Lexical Simplification Systems\",\n        \"Unsupervised lexical simplification for non-native speakers\",\n        \"A Survey on Lexical Simplification\",\n        \"Leveraging Social Media for Medical Text Simplification\",\n        \"Lexical Simplification with Pretrained Encoders\",\n        \"Chinese Lexical Simplification\",\n        \"Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*\",\n        \"Findings of the TSAR-2022 Shared Task on Multilingual Lexical Simplification\",\n        \"The CW Corpus: A New Resource for Evaluating the Identification of Complex Words\",\n        \"CompLex — A New Corpus for Lexical Complexity Prediction from Likert Scale Data\",\n        \"SemEval-2021 Task 1: Lexical Complexity Prediction\",\n        \"Predicting Lexical Complexity in English Texts\",\n        \"Predicting Lexical Complexity in English Texts: The Complex 2.0 Dataset\",\n        \"Complex Word Identification for Swedish\",\n        \"BERTimbau: pretrained BERT models for Brazilian Portuguese\",\n        \"SemEval - 2012 Task 1: English Lexical Simplification\",\n        \"Lexical Simplification Benchmarks for English, Portuguese, and Spanish\",\n        \"Evaluating Lexical Simplification and Vocabulary Knowledge for Learners of French: Possibilities of Using the FLELex Resource\",\n        \"Llama 2: Open Foundation and Fine-Tuned Chat Models\",\n        \"A Report on the Complex Word Identification Shared Task 2018\",\n        \"Overview of ALexS 2020: First Workshop on Lexical Analysis at SEPLN\",\n        \"Complex Word Identification: Challenges in Data Annotation and System Performance\",\n        \"A Robustly Optimized BERT Pre-training Approach with Post-training\"\n    ],\n    \"reasoning\": \"Selected these 56 references because they represent foundational work and recent advances in MultiLS: An End-to-End Lexical Simplification Framework, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}