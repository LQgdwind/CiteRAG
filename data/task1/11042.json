{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 214 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Knowledge Conflicts for LLMs: A Survey\n\nPaper Abstract: Abstract This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.\n\n{retrieved_text}\n\nGenerate exactly 214 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 214\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Gpt-4 technical report\",\n        \"Explanations for CommonsenseQA: New Dataset and Models\",\n        \"Do language models know when they’re hallucinating references?\",\n        \"Towards tracing knowledge in language models back to the training data\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"The looming threat of fake and llm-generated linkedin profiles: Challenges and opportunities for detection and prevention\",\n        \"Identifying and mitigating the security risks of generative ai\",\n        \"Self-consistency of large language models under ambiguity\",\n        \"On the dangers of stochastic parrots: Can language models be too big?\",\n        \"Managing ai risks in an era of rapid progress\",\n        \"SubjQA: A Dataset for Subjectivity and Review Comprehension\",\n        \"Improving language models by retrieving from trillions of tokens\",\n        \"Audiolm: a language modeling approach to audio generation\",\n        \"Language models are few-shot learners\",\n        \"On the application of large language models for language teaching and assessment technology\",\n        \"Poisoning web-scale training datasets is practical\",\n        \"Language model behavior: A comprehensive survey\",\n        \"Can lms generalize to future data? an empirical analysis on text summarization\",\n        \"Can llm-generated misinformation be detected?\",\n        \"Combating misinformation in the age of llms: Opportunities and challenges\",\n        \"Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence\",\n        \"Say what you mean! large language models speak too positively about negative commonsense knowledge\",\n        \"Benchmarking large language models in retrieval-augmented generation\",\n        \"Beyond factuality: A comprehensive evaluation of large language models as knowledge generators\",\n        \"A dataset for answering time-sensitive questions\",\n        \"Factool: Factuality detection in generative ai–a tool augmented framework for multi-task and multi-domain scenarios\",\n        \"Factllama: Optimizing instruction-following language models with external knowledge for automated fact-checking\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Dola: Decoding by contrasting layers improves factuality in large language models\",\n        \"Summing up the facts: Additive mechanisms behind factual recall in llms\",\n        \"BoolQ: Exploring the surprising difficulty of natural yes/no questions\",\n        \"Does bert solve commonsense task via commonsense knowledge\",\n        \"Editing factual knowledge in language models\",\n        \"A continual learning survey: Defying forgetting in classification tasks\",\n        \"Cross-lingual evidence improves monolingual fake news detection\",\n        \"Time-aware language models as temporal knowledge bases\",\n        \"Chain-of-verification reduces hallucination in large language models\",\n        \"Statistical knowledge assessment for large language models\",\n        \"e-care: a new dataset for exploring explainable causal reasoning\",\n        \"Synthetic disinformation attacks on automated fact verification systems\",\n        \"Neural path hunter: Reducing hallucination in dialogue systems via path grounding\",\n        \"Measuring causal effects of data statistics on language model’sfactual’predictions\",\n        \"Measuring and improving consistency in pretrained language models\",\n        \"T-REx: A large scale alignment of natural language with knowledge base triples\",\n        \"Hierarchical neural story generation\",\n        \"Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications\",\n        \"Genai against humanity: Nefarious applications of generative artificial intelligence and large language models\",\n        \"Ai as agency without intelligence: on chatgpt, large language models, and other generative models\",\n        \"The battlefront of combating misinformation and coping with media bias\",\n        \"Rarr: Researching and revising what language models say, using language models\",\n        \"Retrieval-augmented generation for large language models: A survey\",\n        \"Trueteacher: Learning factual consistency evaluation with large language models\",\n        \"Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies\",\n        \"Generative language models and automated influence operations: Emerging threats and potential mitigations\",\n        \"More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models\",\n        \"Studying large language model generalization with influence functions\",\n        \"A survey on automated fact-checking\",\n        \"Retrieval augmented language model pre-training\",\n        \"Methods for measuring, updating, and visualizing factual beliefs in language models\",\n        \"Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models\",\n        \"Teaching machines to read and comprehend\",\n        \"Training compute-optimal large language models\",\n        \"The curious case of neural text degeneration\",\n        \"Discern and answer: Mitigating the impact of misinformation in retrieval-augmented models with discriminators\",\n        \"Wikicontradiction: Detecting self-contradiction articles on wikipedia\",\n        \"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions\",\n        \"Editing models with task arithmetic\",\n        \"Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models\",\n        \"Towards continual knowledge learning of language models\",\n        \"Improving language models meaning understanding and consistency by learning conceptual roles from dictionary\",\n        \"Automatic detection of machine generated text: A critical survey\",\n        \"What does BERT learn about the structure of language?\",\n        \"Survey of hallucination in natural language generation\",\n        \"Disinformation detection: An evolving challenge in the age of llms\",\n        \"Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models\",\n        \"Cutting off the head ends the conflict: A mechanism for interpreting and mitigating knowledge conflicts in language models\",\n        \"TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension\",\n        \"Prompting visual-language models for efficient video understanding\",\n        \"Challenges and applications of large language models\",\n        \"Large language models struggle to learn long-tail knowledge\",\n        \"Impact of co-occurrence on factual knowledge of large language models\",\n        \"Dense passage retrieval for open-domain question answering\",\n        \"Realtime qa: What’s the answer right now?\",\n        \"How ai can distort human beliefs\",\n        \"Claimdiff: Comparing and contrasting claims on contentious issues\",\n        \"The NarrativeQA reading comprehension challenge\",\n        \"False information on web and social media: A survey\",\n        \"Natural questions: A benchmark for question answering research\",\n        \"Analyzing the use of influence functions for instance-specific data filtering in neural machine translation\",\n        \"Internet-augmented language models through few-shot prompting for open-domain question answering\",\n        \"Mind the gap: Assessing temporal generalization in neural language models\",\n        \"Plug-and-play adaptation for continuously-updated qa\",\n        \"Factuality enhanced language models for open-ended text generation\",\n        \"Detecting misinformation with llm-predicted credibility signals and weak supervision\",\n        \"Retrieval-augmented generation for knowledge-intensive NLP tasks\",\n        \"Large language models with controllable working memory\",\n        \"Contradoc: Understanding self-contradictions in documents with large language models\",\n        \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\",\n        \"Inference-time intervention: Eliciting truthful answers from a language model\",\n        \"How pre-trained language models capture factual knowledge? a causal-inspired analysis\",\n        \"Contrastive decoding: Open-ended text generation as optimization\",\n        \"Benchmarking and improving generator-validator consistency of language models\",\n        \"Large language models in finance: A survey\",\n        \"Unveiling the pitfalls of knowledge editing for large language models\",\n        \"Truthfulqa: Measuring how models mimic human falsehoods\",\n        \"Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models\",\n        \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n        \"Prompt injection attack against llm-integrated applications\",\n        \"Entity-based knowledge conflicts in question answering\",\n        \"Time waits for no one! analysis and challenges of temporal misalignment\",\n        \"When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories\",\n        \"When not to trust language models: Investigating effectiveness of parametric and non-parametric memories\",\n        \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\",\n        \"Dynamic benchmarking of masked language models on temporal concept drift with multiple views\",\n        \"Better call gpt, comparing large language models against lawyers\",\n        \"How decoding strategies affect the verifiability of generated text\",\n        \"Fake news, rumor, information pollution in social media and web: A contemporary survey of state-of-the-arts, challenges and opportunities\",\n        \"Addressing the harms of ai-generated inauthentic content\",\n        \"Locating and editing factual associations in gpt\",\n        \"Pointer sentinel mixture models\",\n        \"Large language models challenge the future of higher education\",\n        \"Fast model editing at scale\",\n        \"Enhancing self-consistency and performance of pre-trained language models through natural language inference\",\n        \"Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation\",\n        \"A comprehensive overview of large language models\",\n        \"Disentqa: Disentangling parametric and contextual knowledge with counterfactual question answering\",\n        \"Confirmation bias: A ubiquitous phenomenon in many guises\",\n        \"Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses\",\n        \"Can lms learn new entities from descriptions? challenges in propagating injected knowledge\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Attacking open-domain question answering by injecting misinformation\",\n        \"Knowledge-in-context: Towards knowledgeable semi-parametric language models\",\n        \"On the risk of misinformation pollution with large language models\",\n        \"Check your facts and try again: Improving large language models with external knowledge and automated feedback\",\n        \"Discovering language model behaviors with model-written evaluations\",\n        \"Language models as knowledge bases?\",\n        \"A linguistic investigation of machine learning based contradiction detection models: an empirical analysis and future perspectives\",\n        \"Emptying the ocean with a spoon: Should we edit models?\",\n        \"Cross-lingual consistency of factual knowledge in multilingual language models\",\n        \"merge conflicts!\\\" exploring the impacts of external distractors to parametric knowledge graphs\",\n        \"Predicting question-answering performance of large language models through semantic consistency\",\n        \"Semantic consistency for assuring reliability of large language models\",\n        \"Measuring reliability of large language models through semantic consistency\",\n        \"Know what you don’t know: Unanswerable questions for SQuAD\",\n        \"SQuAD: 100,000+ questions for machine comprehension of text\",\n        \"How much knowledge can you pack into the parameters of a language model?\",\n        \"A primer in BERTology: What we know about how BERT works\",\n        \"Toolformer: Language models can teach themselves to use tools\",\n        \"The cost of training nlp models: A concise overview\",\n        \"Towards understanding sycophancy in language models\",\n        \"Trusting your evidence: Hallucinate less with context-aware decoding\",\n        \"In-context pretraining: Language modeling beyond document boundaries\",\n        \"Replug: Retrieval-augmented black-box language models\",\n        \"Fake news detection on social media: A data mining perspective\",\n        \"A comprehensive evaluation of large language models on legal judgment prediction\",\n        \"Retrieval augmentation reduces hallucination in conversation\",\n        \"Large language models encode clinical knowledge\",\n        \"Editable neural networks\",\n        \"What large models cost you – there is no free ai lunch\",\n        \"Evaluating the social impact of generative ai systems in systems and society\",\n        \"Ai model gpt-3 (dis) informs us better than humans\",\n        \"Blinded by generated contexts: How language models merge generated and retrieved contexts for open-domain qa?\",\n        \"The science of detecting llm-generated texts\",\n        \"BERT rediscovers the classical NLP pipeline\",\n        \"Large language models in medicine\",\n        \"Llama 2: Open foundation and fine-tuned chat models\",\n        \"Musique: Multihop questions via single-hop question composition\",\n        \"Language models don’t always say what they think: Unfaithful explanations in chain-of-thought prompting\",\n        \"The epistemology of fact checking\",\n        \"Comparing gpt-4 and open-source language models in misinformation mitigation\",\n        \"Freshllms: Refreshing large language models with search engine augmentation\",\n        \"What evidence do language models find convincing?\",\n        \"Does it make sense? and why? a pilot study for sense making and explanation\",\n        \"Survey on factuality in large language models: Knowledge, retrieval and domain-specificity\",\n        \"Exploiting Abstract Meaning Representation for open-domain question answering\",\n        \"RFiD: Towards rational fusion-in-decoder for open-domain question answering\",\n        \"A causal view of entity bias in (large) language models\",\n        \"Cross-lingual knowledge editing in large language models\",\n        \"Incorporating neuro-inspired adaptability for continual learning in artificial intelligence\",\n        \"Resolving knowledge conflicts in large language models\",\n        \"Simple synthetic data reduces sycophancy in large language models\",\n        \"Ethical and social risks of harm from language models\",\n        \"Sociotechnical safety evaluation of generative ai systems\",\n        \"Defending against misinformation attacks in open-domain question answering\",\n        \"A broad-coverage challenge corpus for sentence understanding through inference\",\n        \"Topological analysis of contradictions in text\",\n        \"Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation\",\n        \"Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts\",\n        \"Does your model classify entities reasonably? diagnosing and mitigating spurious correlations in entity typing\",\n        \"The earth is flat because…: Investigating llms’ belief towards misinformation via persuasive conversation\",\n        \"Improving factual consistency for knowledge-grounded dialogue systems via knowledge enhancement and alignment\",\n        \"Editing large language models: Problems, methods, and opportunities\",\n        \"Benchmarking and defending against indirect prompt injection attacks on large language models\",\n        \"Intuitive or dependent? investigating llms’ robustness to conflicting prompts\",\n        \"Generate rather than retrieve: Large language models are strong context generators\",\n        \"Glm-130b: An open bilingual pre-trained model\",\n        \"Enhancing financial sentiment analysis via retrieval augmented large language models\",\n        \"Video-llama: An instruction-tuned audio-visual language model for video understanding\",\n        \"Sac3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency\",\n        \"Situatedqa: Incorporating extra-linguistic contexts into qa\",\n        \"Mitigating temporal misalignment by discarding outdated facts\",\n        \"DIALOGPT : Large-scale generative pre-training for conversational response generation\",\n        \"Siren’s song in the ai ocean: A survey on hallucination in large language models\",\n        \"Merging generated and retrieved knowledge for open-domain qa\",\n        \"Explainability for large language models: A survey\",\n        \"Knowing what llms do not know: A simple yet effective self-detection method\",\n        \"Cdconv: A benchmark for contradiction detection in chinese conversations\",\n        \"Mquake: Assessing knowledge editing in language models via multi-hop questions\",\n        \"Lima: Less is more for alignment\",\n        \"A survey of large language models in medicine: Progress, application, and challenge\",\n        \"Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions\",\n        \"Context-faithful prompting for large language models\",\n        \"Toolqa: A dataset for llm question answering with external tools\",\n        \"Detection and resolution of rumours in social media: A survey\"\n    ],\n    \"reasoning\": \"Selected these 214 references because they represent foundational work and recent advances in Knowledge Conflicts for LLMs: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}