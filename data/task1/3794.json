{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors\n\nPaper Abstract: Abstract Attractor-based end-to-end diarization is achieving comparable accuracy to the carefully tuned conventional clustering-based methods on challenging datasets. However, the main drawback is that it cannot deal with the case where the number of speakers is larger than the one observed during training. This is because its speaker counting relies on supervised learning. In this work, we introduce an unsupervised clustering process embedded in the attractor-based end-to-end diarization. We first split a sequence of frame-wise embeddings into short subsequences and then perform attractor-based diarization for each subsequence. Given subsequence-wise diarization results, inter-subsequence speaker correspondence is obtained by unsupervised clustering of the vectors computed from the attractors from all the subsequences. This makes it possible to produce diarization results of a large number of speakers for the whole recording even if the number of output speakers for each subsequence is limited. Experimental results showed that our method could produce accurate diarization results of an unseen number of speakers. Our method achieved 11.84 %times11.84percent11.84\\text{\\,}\\mathrm{\\char 37\\relax}, 28.33 %times28.33percent28.33\\text{\\,}\\mathrm{\\char 37\\relax}, and 19.49 %times19.49percent19.49\\text{\\,}\\mathrm{\\char 37\\relax} on the CALLHOME, DIHARD II, and DIHARD III datasets, respectively, each of which is better than the conventional end-to-end diarization methods.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A review of speaker diarization: Recent advances with deep learning\",\n        \"Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: Theory, implementation and analysis on standard tasks\",\n        \"The third DIHARD diarization challenge\",\n        \"End-to-end neural speaker diarization with self-attention\",\n        \"Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario\",\n        \"End-to-end neural diarization: From transformer to conformer\",\n        \"2000 NIST Speaker Recognition Evaluation\",\n        \"Joint speech recognition and speaker diarization via sequence transduction\",\n        \"CHiME-6 Challenge: Tackling multispeaker speech recognition for unsegmented recordings\",\n        \"End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors\",\n        \"End-to-end speaker diarization conditioned on speech activity and overlap detection\",\n        \"End-to-end diarization for variable number of speakers with local-global networks and discriminative speaker embeddings\",\n        \"BW-EDA-EEND: Streaming end-to-end neural speaker diarization for a variable number of speakers\",\n        \"Encoder-decoder based attractor calculation for end-to-end neural diarization\",\n        \"Advances in online audio-visual meeting transcription\",\n        \"Continuous speech separation: Dataset and analysis\",\n        \"The Second DIHARD Diarization Challenge: Dataset, task, and baselines\",\n        \"Deep clustering: Discriminative embeddings for segmentation and separation\",\n        \"Permutation invariant training of deep models for speaker-independent multi-talker speech separation\",\n        \"Optimal mapping loss: A faster loss for end-to-end speaker diarization\",\n        \"Semantic instance segmentation via deep metric learning\",\n        \"Recurrent pixel embedding for instance grouping\",\n        \"Speaker diarization with LSTM\",\n        \"Auto-tuning spectral clustering for speaker diarization using normalized maximum eigengap\",\n        \"Constrained k-means clustering with background knowledge\",\n        \"Integrating end-to-end neural and clustering-based diarization: Getting the best of both worlds\",\n        \"Advances in integration of end-to-end neural and clustering-based diarization for real conversational speech\",\n        \"An improved Cop-Kmeans clustering for solving constraint violation based on MapReduce framework\",\n        \"ResNeXt and Res2Net structures for speaker verification\",\n        \"Fully supervised speaker diarization\",\n        \"Discriminative neural clustering for speaker diarisation\",\n        \"Overlap-aware diarization: Resegmentation using neural end-to-end overlapped speech detection\",\n        \"Personal VAD: Speaker-conditioned voice activity detection\",\n        \"VoiceFilter-Lite: Streaming targeted voice separation for on-device speech recognition\",\n        \"Tackling real noisy reverberant meetings with all-neural source separation, counting, and diarization system\",\n        \"End-to-end speaker diarization as post-processing\",\n        \"LibriSpeech: An ASR corpus based on public domain audio books\",\n        \"VoxCeleb: Large-scale speaker verification in the wild\",\n        \"Alternative objective functions for deep clustering\",\n        \"TasNet: time-domain audio separation network for real-time, single-channel speech separation\",\n        \"Conv-TasNet: Surpassing ideal timeâ€“frequency magnitude masking for speech separation\",\n        \"Voice separation with an unknown number of multiple speakers\",\n        \"Deep attractor network for single-microphone speaker separation\",\n        \"Wavesplit: End-to-end speech separation by speaker clustering\",\n        \"Recursive speech separation for unknown number of speakers\",\n        \"Adam: A method for stochastic optimization\",\n        \"Attention is all you need\",\n        \"Neural speaker diarization with speaker-wise chain rule\",\n        \"BUT system for the Second DIHARD Speech Diarization Challenge\",\n        \"The Hitachi-JHU DIHARD III system: Competitive end-to-end neural diarization and x-vector clustering systems combined by DOVER-Lap\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}