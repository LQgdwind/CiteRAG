{
  "paper_id": "2203.00048v3",
  "title": "Multi-modal Alignment using Representation Codebook",
  "sections": {
    "evaluation on image-text retrieval": "For the image-text retrieval tasks, we conduct two different scenarios for evaluation: “zero-shot” retrieval task and “after-finetuning” retrieval task, following the setting in [ref]25  [ref]28. We compare with both early-fusion methods such as  [ref]28  and late-fusion methods such as  [ref]21. ALBEF [ref]25 is an hybrid approach that also performs feature alignment along with fusion. Results in Table 1 and 2 show consistent improvements of our approach against prior state-of-the-arts. “Zero-shot”: As shown from Table 1, CODIS outperforms existing baselines with a clear margin across the two datasets, for both image and text retrieval tasks, especially at R@1. Compared to the best-performing early-fusion approach , we obtain a margin of 11.0%percent11.011.0\\%/13.5%percent13.513.5\\% TR/IR in terms of R@1 on Flickr30K. When compared to highest late-fusion approach [ref]21, there’s an increase of 12.9%percent12.912.9\\%/8.3%percent8.38.3\\% TR/IR in R@1 on MSCOCO and a boost of 3.1%percent3.13.1\\%/4.0%percent4.04.0\\% TR/IR in R@1 on Flickr30K, despite the fact that ALIGN [ref]21 uses 1.8B data in training (approx. 360×\\times more image-text pairs than our model). Our approach also outperforms ALBEF 4M [ref]25 with a clear margin of 2.9%/3.8% R@1 for TR/IR on MSCOCO and 1.2%/2.9% in terms of R@1 for TR/IR on Flickr30K, revealing that our model can further benefit from codebook representation learning. “After-finetuning”: This task showcases the ability of V&L pretraining via transfer learning. For small datasets such as Flickr30K, performance gap tends to reduce as the model converges. However, our approach still achieves the best result in most of the metrics and the largest margins occur for R@1, especially on MSCOCO. Compared against the closest performing method ALBEF [ref]25, CODIS obtains an improvement of 2.2%/1.9%percent2.2percent1.92.2\\%/1.9\\% TR/IR in R@1 on MSCOCO, providing evidence to the effectiveness of CODIS for transfer learning."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Gradient flows: in metric spaces and in the space of probability measures",
      "abstract": "",
      "year": "2008",
      "venue": "Springer Science & Business Media",
      "authors": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré"
    },
    {
      "index": 1,
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.08254",
      "authors": "Hangbo Bao, Li Dong, and Furu Wei",
      "orig_title": "Beit: Bert pre-training of image transformers",
      "paper_id": "2106.08254v2"
    },
    {
      "index": 2,
      "title": "Deep clustering for unsupervised learning of visual features",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze"
    },
    {
      "index": 3,
      "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.09882",
      "authors": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin",
      "orig_title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "paper_id": "2006.09882v5"
    },
    {
      "index": 4,
      "title": "Emerging properties in self-supervised vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.14294",
      "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin"
    },
    {
      "index": 5,
      "title": "Graph Optimal Transport for Cross-Domain Alignment",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu",
      "orig_title": "Graph optimal transport for cross-domain alignment",
      "paper_id": "2006.14744v3"
    },
    {
      "index": 6,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 7,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu",
      "orig_title": "Uniter: Universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 8,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le"
    },
    {
      "index": 9,
      "title": "An optimal transport approach to robust reconstruction and simplification of 2d shapes",
      "abstract": "",
      "year": "2011",
      "venue": "Computer Graphics Forum",
      "authors": "Fernando De Goes et al."
    },
    {
      "index": 10,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 11,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.11929",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 12,
      "title": "SLADE: A Self-Training Framework For Distance Metric Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiali Duan, Yen-Liang Lin, Son Tran, Larry S Davis, and C-C Jay Kuo",
      "orig_title": "Slade: A self-training framework for distance metric learning",
      "paper_id": "2011.10269v2"
    },
    {
      "index": 13,
      "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.06195",
      "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu",
      "orig_title": "Large-scale adversarial training for vision-and-language representation learning",
      "paper_id": "2006.06195v2"
    },
    {
      "index": 14,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 15,
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.07733",
      "authors": "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al.",
      "orig_title": "Bootstrap your own latent: A new approach to self-supervised learning",
      "paper_id": "2006.07733v3"
    },
    {
      "index": 16,
      "title": "Dimensionality reduction by learning an invariant mapping",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)",
      "authors": "Raia Hadsell, Sumit Chopra, and Yann LeCun"
    },
    {
      "index": 17,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 18,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 19,
      "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu",
      "orig_title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
      "paper_id": "2104.03135v2"
    },
    {
      "index": 20,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.05918",
      "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig"
    },
    {
      "index": 21,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03334",
      "authors": "Wonjae Kim, Bokyung Son, and Ildoo Kim",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 22,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International Journal of Computer Vision",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al."
    },
    {
      "index": 23,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 24,
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.07651",
      "authors": "Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi",
      "orig_title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "paper_id": "2107.07651v2"
    },
    {
      "index": 25,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 26,
      "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.15409",
      "authors": "Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang"
    },
    {
      "index": 27,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al.",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 28,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "European Conference on Computer Vision",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 29,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.05101",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 30,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.02265",
      "authors": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee"
    },
    {
      "index": 31,
      "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee",
      "orig_title": "12-in-1: Multi-task vision and language representation learning",
      "paper_id": "1912.02315v2"
    },
    {
      "index": 32,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.03748",
      "authors": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 33,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg"
    },
    {
      "index": 34,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 35,
      "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.07966",
      "authors": "Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti",
      "orig_title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data",
      "paper_id": "2001.07966v2"
    },
    {
      "index": 36,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.00020",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 37,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever"
    },
    {
      "index": 38,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 39,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut"
    },
    {
      "index": 40,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.08530",
      "authors": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai"
    },
    {
      "index": 41,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.00491",
      "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 42,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.07490",
      "authors": "Hao Tan and Mohit Bansal",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 43,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 44,
      "title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin",
      "orig_title": "Unsupervised feature learning via non-parametric instance discrimination",
      "paper_id": "1805.01978v1"
    },
    {
      "index": 45,
      "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.06706",
      "authors": "Ning Xie, Farley Lai, Derek Doran, and Asim Kadav",
      "orig_title": "Visual entailment: A novel task for fine-grained image understanding",
      "paper_id": "1901.06706v1"
    },
    {
      "index": 46,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le"
    },
    {
      "index": 47,
      "title": "Vision-Language Pre-Training with Triple Contrastive Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang",
      "orig_title": "Vision-language pre-training with triple contrastive learning",
      "paper_id": "2202.10401v4"
    },
    {
      "index": 48,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao",
      "orig_title": "Vinvl: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    }
  ]
}