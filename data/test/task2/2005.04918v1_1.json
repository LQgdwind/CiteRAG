{
  "paper_id": "2005.04918v1",
  "title": "On Radial Isotropic Position: Theory and Algorithms Work on this paper by Shiri Artstein was supported in part by the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No. 770127), and in part by ISF grant 665/15. Work by Haim Kaplan was supported in part by by ISF grant 1595/19 and grant 1367/2016 from the German-Israeli Science Foundation (GIF). Work by Micha Sharir was supported in part by Grant 260/18 from the Israel Science Foundation, Grant G-1367-407.6/2016 from the German-Israeli Foundation for Scientific Research and Development, by the Blavatnik Research Fund in Computer Science at Tel Aviv University, and by the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).",
  "sections": {
    "introduction": "A set X={xi}i=1nğ‘‹superscriptsubscriptsubscriptğ‘¥ğ‘–ğ‘–1ğ‘›X=\\{x_{i}\\}_{i=1}^{n} of nâ‰¥dğ‘›ğ‘‘n\\geq d (column) vectors in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}\nis in isotropic position\nif one (and thus all) of the following equivalent conditions holds. Here we denote by xâŠ—xtensor-productğ‘¥ğ‘¥x\\otimes x the rank-one operator (xâŠ—x)â€‹y=âŸ¨x,yâŸ©â€‹xtensor-productğ‘¥ğ‘¥ğ‘¦ğ‘¥ğ‘¦ğ‘¥(x\\otimes x)y=\\langle x,y\\rangle x\n(which is xâ€‹xTğ‘¥superscriptğ‘¥ğ‘‡xx^{T} in matrix form), and Idsubscriptğ¼ğ‘‘I_{d} is the dÃ—dğ‘‘ğ‘‘d\\times d identity matrix.\nIn words, the third condition asserts that the sum of the squares of the projections of our vectors\nin any direction has the same value n/dğ‘›ğ‘‘n/d. If the xisubscriptğ‘¥ğ‘–x_{i}â€™s are unit vectors then the third\ncondition also says that the sum of the squares of the projections of any unit vector on the xisubscriptğ‘¥ğ‘–x_{i}â€™s is n/dğ‘›ğ‘‘n/d. As an easy example, the standard basis {ei}i=1dsuperscriptsubscriptsubscriptğ‘’ğ‘–ğ‘–1ğ‘‘\\{e_{i}\\}_{i=1}^{d} is a set in isotropic position.\nAs a slightly less obvious example, the\nvertices of a regular simplex, so that its center is at the origin and the vertices lie on ğ•Šdâˆ’1superscriptğ•Šğ‘‘1\\mathbb{S}^{d-1},\nform a set of d+1ğ‘‘1d+1 points in isotropic position. In more generality,\nconsider a subspace Eğ¸E of â„nsuperscriptâ„ğ‘›\\mathbb{R}^{n} of dimension dğ‘‘d.\nLet {ei}i=1nsuperscriptsubscriptsubscriptğ‘’ğ‘–ğ‘–1ğ‘›\\{e_{i}\\}_{i=1}^{n} be an orthonormal basis of â„nsuperscriptâ„ğ‘›\\mathbb{R}^{n},\nand let yi=PEâ€‹eisubscriptğ‘¦ğ‘–subscriptğ‘ƒğ¸subscriptğ‘’ğ‘–y_{i}=P_{E}e_{i} be their orthogonal projections onto Eğ¸E.\nWe have âˆ‘i=1neiâŠ—ei=Insuperscriptsubscriptğ‘–1ğ‘›tensor-productsubscriptğ‘’ğ‘–subscriptğ‘’ğ‘–subscriptğ¼ğ‘›\\sum_{i=1}^{n}e_{i}\\otimes e_{i}=I_{n}, and therefore\nâˆ‘i=1nyiâŠ—yi=Idsuperscriptsubscriptğ‘–1ğ‘›tensor-productsubscriptğ‘¦ğ‘–subscriptğ‘¦ğ‘–subscriptğ¼ğ‘‘\\sum_{i=1}^{n}y_{i}\\otimes y_{i}=I_{d}, as is easily verified\n(note that PEâ€‹PET=Idsubscriptğ‘ƒğ¸superscriptsubscriptğ‘ƒğ¸ğ‘‡subscriptğ¼ğ‘‘P_{E}P_{E}^{T}=I_{d}). This means that by rewriting\nxi=yi/|yi|subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘¦ğ‘–x_{i}=y_{i}/|y_{i}| (assuming that all the yisubscriptğ‘¦ğ‘–y_{i}â€™s are non-zero),\nand letting ci=|yi|2â‰¥0subscriptğ‘ğ‘–superscriptsubscriptğ‘¦ğ‘–20c_{i}=|y_{i}|^{2}\\geq 0, we get that\nId=âˆ‘i=1nciâ€‹xiâŠ—xi.subscriptğ¼ğ‘‘superscriptsubscriptğ‘–1ğ‘›tensor-productsubscriptğ‘ğ‘–subscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘–I_{d}=\\sum_{i=1}^{n}c_{i}x_{i}\\otimes x_{i}.\nIn the case of a simplex, one can present the vertices of the regular simplex\nas the projections of an orthonormal basis in â„d+1superscriptâ„ğ‘‘1\\mathbb{R}^{d+1} such that\nall projections have equal length, which is why all the\ncisubscriptğ‘ğ‘–c_{i}â€™s are equal (to d/(d+1)ğ‘‘ğ‘‘1d/(d+1)) and we get isotropic position.\nThis naturally brings us to the following generalizations of isotropy. cğ‘c-isotropy.\nLet c=(c1,â€¦,cn)ğ‘subscriptğ‘1â€¦subscriptğ‘ğ‘›c=(c_{1},\\ldots,c_{n}) be an nğ‘›n-sequence of positive real weights.\nA set X={xi}i=1nğ‘‹superscriptsubscriptsubscriptğ‘¥ğ‘–ğ‘–1ğ‘›X=\\{x_{i}\\}_{i=1}^{n} of nâ‰¥dğ‘›ğ‘‘n\\geq d (column) vectors in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}\nis in cğ‘c-isotropic position if\nthe following equivalent properties hold. Isotropy is the special case of cğ‘c-isotropy when ci=d/nsubscriptğ‘ğ‘–ğ‘‘ğ‘›c_{i}=d/n for all iğ‘–i. It is a classical fact that any set of nğ‘›n (not necessarily unit) vectors {yi}i=1nsuperscriptsubscriptsubscriptğ‘¦ğ‘–ğ‘–1ğ‘›\\{y_{i}\\}_{i=1}^{n}\nthat span â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d} can be put in cğ‘c-isotropic position, for any positive weight sequence cğ‘c, using the\nlinear transformation A=(âˆ‘iciâ€‹yiâŠ—yi)âˆ’1/2ğ´superscriptsubscriptğ‘–tensor-productsubscriptğ‘ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘¦ğ‘–12A=\\Bigl{(}\\sum_{i}c_{i}y_{i}\\otimes y_{i}\\Bigr{)}^{-1/2} (which is the unique\npositive definite matrix A=ATğ´superscriptğ´ğ‘‡A=A^{T} for which A2=(âˆ‘iciâ€‹yiâŠ—yi)âˆ’1superscriptğ´2superscriptsubscriptğ‘–tensor-productsubscriptğ‘ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘¦ğ‘–1A^{2}=\\Bigl{(}\\sum_{i}c_{i}y_{i}\\otimes y_{i}\\Bigr{)}^{-1},\nand which exists as âˆ‘ciâ€‹yiâŠ—yitensor-productsubscriptğ‘ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘¦ğ‘–\\sum c_{i}y_{i}\\otimes y_{i} is positive definite), because In this paper we are specifically interested in the notion of radial isotropy, defined as follows. Radial cğ‘c-isotropy.\nA set X={xi}i=1nğ‘‹superscriptsubscriptsubscriptğ‘¥ğ‘–ğ‘–1ğ‘›X=\\{x_{i}\\}_{i=1}^{n} of nâ‰¥dğ‘›ğ‘‘n\\geq d nonzero (column) vectors in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}\nis in radial cğ‘c-isotropic position (for a vector cğ‘c as in DefinitionÂ 1.1)\nif the normalized vectors xi/|xi|subscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘–x_{i}/|x_{i}|, for i=1,â€¦,nğ‘–1â€¦ğ‘›i=1,\\ldots,n,\nare in cğ‘c-isotropic position. We say that the vectors are in (standard, or uniform)\nradial isotropic position when the above property holds with ci=d/nsubscriptğ‘ğ‘–ğ‘‘ğ‘›c_{i}=d/n for all iğ‘–i. It is easily checked that any weights cisubscriptğ‘ğ‘–c_{i} that admit a point set in\nradial cğ‘c-isotropic position must satisfy âˆ‘i=1nci=dsuperscriptsubscriptğ‘–1ğ‘›subscriptğ‘ğ‘–ğ‘‘\\sum_{i=1}^{n}c_{i}=d. We are interested in the existence and the computation of a linear transformation that maps\na given set of vectors into radial cğ‘c-isotropic position, for some prescribed sequence cğ‘c\nof positive weights. Formally, We say that a nonsingular linear transformation T:â„dâ†’â„d:ğ‘‡â†’superscriptâ„ğ‘‘superscriptâ„ğ‘‘T:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\nputs a set Xğ‘‹X of nğ‘›n nonzero vectors x1,â€¦,xnâˆˆâ„dsubscriptğ‘¥1â€¦subscriptğ‘¥ğ‘›superscriptâ„ğ‘‘x_{1},\\dots,x_{n}\\in\\mathbb{R}^{d} in radial cğ‘c-isotropic position,\nfor a coefficient vector câˆˆ(â„+)nğ‘superscriptsuperscriptâ„ğ‘›c\\in(\\mathbb{R}^{+})^{n} of â„“1subscriptâ„“1\\ell_{1}-norm dğ‘‘d, if The existence of such a linear transformation, even if we consider only the simpler form of\nradial isotropy (ci=d/nsubscriptğ‘ğ‘–ğ‘‘ğ‘›c_{i}=d/n for each iğ‘–i),\nis more intricate than the standard, non-radial setup (and in general it may fail to exist).\nForster 2 proved that when the vectors of Xğ‘‹X are in general position\n(i.e., every dğ‘‘d vectors among them are linearly independent),\nsuch a transformation exists. In fact, earlier, Barthe  proved\nthat the vectors xisubscriptğ‘¥ğ‘–x_{i}, i=1,â€¦,nğ‘–1â€¦ğ‘›i=1,\\ldots,n, can be put in a radial cğ‘c-isotropic position if and only if\ncğ‘c is in the relative interior of the so called basis polytope (see below) defined by the xisubscriptğ‘¥ğ‘–x_{i}â€™s.\nWe give a complete, and somewhat enhanced version of this theory in Section 2.\nSince this theory is well known,111It is well known to the experts. The details that we provide in the appendix are spelled\nout for the convenience of the non-expert reader.\nexcept for the enhancements that we derive and add to it, SectionÂ 2 contains\nonly the highlights of the theory, and most details, including proofs, are presented in\nAppendix A.\nIn a later study, Carlen, Lieb and LossÂ  gave an equivalent characterization\nof the basis polytope, which we use extensively in our study (we prove it too, for completeness,\nin Appendix A). Radial isotropy arises when we have a set of nğ‘›n subspaces, say lines, and we want to linearly transform these\nsubspaces so that the sum of the squared projections of any unit vector xğ‘¥x on their transformed copies is n/dğ‘›ğ‘‘n/d.\nA similar interpretation can be given for general weights cisubscriptğ‘ğ‘–c_{i}.222Radial isotropy corresponds to the case of lines; higher-dimensional subspaces require further extension of the notion.\nFurthermore, as suggested by Hardt and Moitra 7,\nradial isotropic position can also be thought of as a stable analogue of isotropic position.\nThat is, while isotropic position has important applications both in algorithms and in exploratory data analysis,\nit is rather sensitive to even a small number of outliers.\nRadial isotropic position is more robust in the presence of outliers. Specific Applications.\nThe problem arises in several, rather diverse applications, and several other studies\naddress problems of very similar or more general nature. Among these applications and related work we mention (i)\nthe derivation of a linear lower bound on the unbounded error probabilistic communication complexityÂ 2, (ii)\nrobust subspace recovery in the presence of outliers in machine learningÂ 7, (iii)\nalgorithmic and optimization aspects of Brascamp-Lieb inequalitiesÂ 3, (iv)\na superquadratic lower bound for 3-query locally correctable codes over the realsÂ ,\nand (somewhat more remote but still related) (v)\na deterministic polynomial-time algorithm for\napproximating mixed discriminants and mixed volumesÂ 6. See below for some additional details. Another context in which radial isotropy arises (often as a special case) is in\nentropy maximization; see the works of Carlen et al.Â , Singh and VishnoiÂ \nand Straszak and VishnoiÂ , and also Leeâ€™s blogÂ  and\nsee below for an additional discussion. It also related to so-called finite tight frames\n(seeÂ  for a recent monograph on this topic). Last, but not least, is\na recent application in Kane et al.Â , who use radial isotropic position for\ncomparison-based algorithms for point location in a high-dimensional arrangement of hyperplanes.\nThis work, which served as a starting motivation for our work on this paper,\nfollowed an earlier breakthrough result of Kane et al.Â 9,\nin which they showed that one can solve the 3-SUM problem (decide whether any three\nout of nğ‘›n given real numbers sum to 00) using only Oâ€‹(nâ€‹log2â¡n)ğ‘‚ğ‘›superscript2ğ‘›O(n\\log^{2}n) simple linear queries on the input.\nAs a matter of fact, Kane et al.Â established in 9 a more general result,\nnamely that one can answer point location queries\nin an arrangement of a set Hğ»H of hyperplanes in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}, each of which has\ninteger coefficients with a small â„“1subscriptâ„“1\\ell_{1}-norm, using only\nOâ€‹(dâ€‹logâ¡dâ€‹logâ¡|H|)ğ‘‚ğ‘‘ğ‘‘ğ»O(d\\log d\\log|H|) simple linear comparisons involving the query point xğ‘¥x.\nHere dğ‘‘d is the dimension of the ambient space. (In the 3-SUM application we have\nd=nğ‘‘ğ‘›d=n and |H|=(n3)ğ»binomialğ‘›3|H|={n\\choose 3}.) In the follow-up studyÂ , Kane et al.Â extended their technique\nto sets of general hyperplanes, with arbitrary coefficients, allowing only\ntwo types of comparisons involving the input xğ‘¥x: sign tests\n(determining on which side of a hyperplane xğ‘¥x lies), and\ngeneralized comparison queries, in which one asks\nfor the sign (positive, negative, or zero) of expressions\nof the form Î±â€‹h1â€‹(x)+Î²â€‹h2â€‹(x)ğ›¼subscriptâ„1ğ‘¥ğ›½subscriptâ„2ğ‘¥\\alpha h_{1}(x)+\\beta h_{2}(x), where Î±ğ›¼\\alpha and Î²ğ›½\\beta\nare arbitrary real parameters. Kane et al.Â showed that Oâ€‹(d3â€‹logâ¡dâ€‹logâ¡|H|)ğ‘‚superscriptğ‘‘3ğ‘‘ğ»O(d^{3}\\log d\\log|H|)\nsign tests and generalized comparisons suffice for point location.333This performance is worse than the best known recent bound of\nEzra and SharirÂ 0, but the queries that their algorithm\nperforms, and the space decomposition that is induced by the algorithm, are\nmuch simpler than those in 0. A crucial step that the algorithm of  performs is transforming the\nnormals of the input hyperplanes into a set in radial (uniform)\nisotropic position by a linear transformation Tğ‘‡T followed by a normalization.\nThey then locate Tâˆ’1â€‹xsuperscriptğ‘‡1ğ‘¥T^{-1}x in the arrangement of the transformed hyperplanes\n(via standard comparisons, which are equivalent to generalized comparisons in\nthe original space). After the transformation they have the property that\nthe sum of the suitably defined squared â€˜scalar productsâ€™ of the hyperplanes\nwith any normalized query point xğ‘¥x is the same, and sufficiently large,\na fact which is crucial for their analysis. Since the latter property is\nall that they need, a relaxed, approximate notion of radial isotropy certainly\nsuffices for their needs. In their work, since they only measure the number of\nsign and (generalized) comparison queries, they do not care about\nthe cost of computing the transformation that brings the hyperplanes\ninto (approximate) radial isotropic position. In another recent study\nby Ezra et al.Â 1, a full implementation of the first technique\nof Kane et al.Â 9 is presented. Having an efficient procedure\nfor transforming the input hyperplanes into approximate radial isotropic\nposition, like the one presented in this paper, facilitates a\nstraightforward adaptaion of the machinery in 1 to\nobtain a full implementation of the second, isotropy-based technique\nof Kane et al.Â  for point location in arrangements of arbitrary\nhyperplanes. A second interesting application, due to\nHardt and Moitra 7, studies the problem of robust subspace recovery.\nIn this problem we are given a set Xğ‘‹X of nğ‘›n vectors in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d} and we\nwant to determine whether there exists some subspace of some dimension\nâ„“â„“\\ell that contains more than (â„“/d)â€‹nâ„“ğ‘‘ğ‘›(\\ell/d)n of these vectors.\nThe motivation is to detect whether a\ndataset Xğ‘‹X in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d} does in fact reside in a lower-dimensional\nspace if we remove a relatively small subset of outliers. Hardt\nand Moitra gave a Las Vegas algorithm that makes Oâ€‹(d2â€‹n)ğ‘‚superscriptğ‘‘2ğ‘›O(d^{2}n) iterations\non average, where in each iteration it draws dğ‘‘d vectors from Xğ‘‹X,\nand when it finds dğ‘‘d linearly dependent vectors, it identifies\nthe subspace that they span as a candidate rich subspace. Hardt and Moitra also argue that when such a subspace exists, the\nvector dnâ€‹ğŸğ‘‘ğ‘›1\\frac{d}{n}{\\bf 1} must be outside the basis polytope of Xğ‘‹X,\nand use this fact, together with a polynomial-time algorithm for\ndetecting membership in the basis polytope, to derandomize their algorithm. As follows from the theory of radial isotropic position (that we will review\nin SectionÂ 2 and AppendixÂ A),\nthe condition that dnâ€‹ğŸğ‘‘ğ‘›1\\frac{d}{n}{\\bf 1} is not in the basis polytope of Xğ‘‹X\nis equivalent to the condition that we cannot put Xğ‘‹X into radial isotropic\nposition with respect to the vector dnâ€‹ğŸğ‘‘ğ‘›1\\frac{d}{n}{\\bf 1}. Saying it the\nother way around, it follows that putting Xğ‘‹X into radial isotropic position\nwith respect to dnâ€‹ğŸğ‘‘ğ‘›1\\frac{d}{n}{\\bf 1} is a proof that there is no\nâ„“â„“\\ell-dimensional subspace that contains more than (â„“/d)â€‹nâ„“ğ‘‘ğ‘›(\\ell/d)n vectors of Xğ‘‹X,\nfor any â„“â„“\\ell. Motivated by this observation, Hardt and Moitra applied Bartheâ€™s\ncharacterization of attaining radial cğ‘c-isotropic position, in order\nto derive the aforementioned polynomial-time ellipsoid-based algorithm that finds such a\ncertificate (i.e., puts the vectors into radial isotropic position). As mentioned, our problem can be viewed as a special case of a more general problem considered\nby Singh and Vishnoi  and Straszak and Vishnoi  on\nentropy maximization. They consider the problem of finding a distribution qğ‘q of\nmaximum entropy over a (possibly large) collection â„±â„±{\\cal F} of subsets Fğ¹F of\n[n]={1,2,â€¦,n}delimited-[]ğ‘›12â€¦ğ‘›[n]=\\{1,2,\\ldots,n\\}, among all distributions with a given vector cğ‘c of marginals. A somewhat more distantly related, and more general, line of research is on\noperator scaling and its connection to Brascamp-Lieb constants (see for example\n3 6). The most recent and closely related among these works is\nby Garg et al.Â 3, who give an algorithm to compute the\nBrascamp-Lieb constant for a particular Brascamp-Lieb â€œdatumâ€\n(a set of linear transformations and a vector of exponents).\nThe algorithm uses an alternative minimization technique (as the problem is not convex) to\nbring the instance into a so called â€œgeometric positionâ€,\nfrom which it can deduce the desired constant. In this terminology our problem\nis to find a transformation that brings a â€œBrascamp-Lieb rank-one datumâ€\ninto isotropic position. For rank one this problem is convex and thereby easier.\nIt was treated before by Hardt and Moitra 7 and in a somewhat different\nsettings also by Gurvitz and Samorodnitsky 6 who applied the ellipsoid\nalgorithm to solve it. See the end of the introduction for a summary of highlights of the novel contributions\nof our work. Given a set X={x1,â€¦,xn}ğ‘‹subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘›X=\\{x_{1},\\ldots,x_{n}\\} of nğ‘›n nonzero vectors in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}, and a weight vector\nc=(c1,â€¦,cn)âˆˆ(â„+)nğ‘subscriptğ‘1â€¦subscriptğ‘ğ‘›superscriptsuperscriptâ„ğ‘›c=(c_{1},\\ldots,c_{n})\\in\\left(\\mathbb{R}^{+}\\right)^{n},\nsuch that there exists a linear transformation sending the xisubscriptğ‘¥ğ‘–x_{i}â€™s into radial cğ‘c-isotropic position, we develop\nalgorithms to find such a transformation. We first consider algorithms that find an exact solution.\nWe show how to do it either by solving a system of nğ‘›n polynomial equations,\nin nğ‘›n variables, each of degree dğ‘‘d, or a different system of d2superscriptğ‘‘2d^{2} equations\nin d2superscriptğ‘‘2d^{2} variables, each of degree 2â€‹n2ğ‘›2n. The running times are dOâ€‹(n)superscriptğ‘‘ğ‘‚ğ‘›d^{O(n)} or\nnOâ€‹(d2)superscriptğ‘›ğ‘‚superscriptğ‘‘2n^{O(d^{2})}, respectively, using well known techniques from symbolic algebra\n(or computational real algebraic geometryÂ ). Our main focus, however, is on computing such a transformation approximately and\nconsiderably more efficiently. That is, we want to compute a linear transformation\nthat puts the xisubscriptğ‘¥ğ‘–x_{i}â€™s into radial capxsubscriptğ‘apxc_{\\rm apx}-isotropic position, for some vector capxsubscriptğ‘apxc_{\\rm apx} that\nsatisfies |capxâˆ’c|â‰¤Îµsubscriptğ‘apxğ‘ğœ€|c_{\\rm apx}-c|\\leq{\\varepsilon}, say in the â„“2subscriptâ„“2\\ell_{2}-norm, for some prespecified accuracy parameter Îµğœ€{\\varepsilon}. We assume that XâŠ‚ğ•Šdâˆ’1ğ‘‹superscriptğ•Šğ‘‘1X\\subset\\mathbb{S}^{d-1}; this is a natural assumption for radial isotropy,\nit occurs in the applications we are aware of, and it involves no loss of generality. As follows from Bartheâ€™s results, detecting whether the xisubscriptğ‘¥ğ‘–x_{i}â€™s can be put in radial cğ‘c-isotropic position\nis equivalent to testing whether the vector cğ‘c is in the relative interior of\nthe basis polytope associated with Xğ‘‹X, given by where ğŸSsubscript1ğ‘†{\\bf 1}_{S} is the nğ‘›n-dimensional indicator vector of the set Sğ‘†S,\nor, equivalently (as proved inÂ ), by These two expressions are dual, in a sense: In (1), KXsubscriptğ¾ğ‘‹K_{X} is defined\nas the convex hull of a set of point, whereas in (2) it is defined as the intersection of halfspaces. The basis polytope plays a central role in matroid theory and submodular optimization;\nsee, e.g., , and there are efficient algorithms for detecting membership in\nthe basis polytope   5 8 4. Furthermore, in some common cases, membership\nof cğ‘c in the basis polytope is obvious, as, for example, in the case studied by Forster 2,\nwhere ci=d/nsubscriptğ‘ğ‘–ğ‘‘ğ‘›c_{i}=d/n for each iğ‘–i and the xisubscriptğ‘¥ğ‘–x_{i}â€™s are in general position. Bartheâ€™s characterization reduces the problem of finding a transformation that puts\nthe input vectors in radial cğ‘c-isotropic position to a problem of finding a point\ntâˆ—âˆˆâ„nsuperscriptğ‘¡superscriptâ„ğ‘›t^{*}\\in\\mathbb{R}^{n} that attains the minimum of a specific convex function fâ€‹(t)ğ‘“ğ‘¡f(t),\ndefined in terms of Xğ‘‹X and cğ‘c. Concretely, It follows that a most natural (and simple) approach, which is the one proposed\nin this paper, to finding the transformation that\nputs the vectors in radial cğ‘c-isotropic position is to find the minimizing vector tâˆ—superscriptğ‘¡t^{*} using\nan appropriate variant of the gradient descent technique; see BubeckÂ  for details.\nOur main set of results is an analysis of gradient descent applied to fğ‘“f, and of various parameters\nthat affect its efficiency. We introduce a new concept promising that the vector cğ‘c lies â€œdeeply insideâ€ KXsubscriptğ¾ğ‘‹K_{X}, which is\nrelated to the representation (2). This notion is different and more robust than the ones\nin previous works, and we discuss it in more detail and compare it to other parameters\nconsidered in the literature, in Section 4.  Let X={xi}i=1nâŠ‚Sdâˆ’1âŠ‚â„dğ‘‹superscriptsubscriptsubscriptğ‘¥ğ‘–ğ‘–1ğ‘›superscriptğ‘†ğ‘‘1superscriptâ„ğ‘‘X=\\{x_{i}\\}_{i=1}^{n}\\subset S^{d-1}\\subset\\mathbb{R}^{d} and let KXsubscriptğ¾ğ‘‹K_{X} be given\nby (1) or (2). We say, for Î·,Î´>0ğœ‚ğ›¿0\\eta,\\delta>0, that\na vector cğ‘c lies (Î·,Î´ğœ‚ğ›¿\\eta,\\delta)-deep inside KXsubscriptğ¾ğ‘‹K_{X} if,\nfor any subspace Eğ¸E with dim(E)=kâˆˆ{1,â€¦,dâˆ’1}dimensionğ¸ğ‘˜1â€¦ğ‘‘1\\dim(E)=k\\in\\{1,\\ldots,d-1\\}, we have that and the distance dâ€‹(x,E)ğ‘‘ğ‘¥ğ¸d(x,E) is the Euclidean distance. Note that for Î·=Î´=0ğœ‚ğ›¿0\\eta=\\delta=0 this condition just says that cğ‘c is in the basis polytope, by (2).\nIn general this is a stronger constraint, as we also include in EÎ´subscriptğ¸ğ›¿E_{\\delta} vectors that lie close\nto Eğ¸E, and impose a stricter inequality on the corresponding cjsubscriptğ‘ğ‘—c_{j}â€™s.\nOur algorithm does not need to compute or know Î·ğœ‚\\eta and Î´ğ›¿\\delta. These parameters are used only for analysis. We establish, in Section 4, the following results, after discussing, in some detail,\na few variants of the gradient descent method, and investigating in depth the associated parameters\nthat control their efficiency. The first set of variants that we use are the projected gradient descent for smooth functions [5, Section 3.2]\nand Nesterovâ€™s accelerated version [5, Section 3.7]; see SectionÂ 4 for more details. For a vector cğ‘c that is (Î·,Î´)ğœ‚ğ›¿(\\eta,\\delta)-deep inside KXsubscriptğ¾ğ‘‹K_{X}, we can construct\na transformation that brings Xğ‘‹X into radial capxsubscriptğ‘apxc_{\\rm apx}-isotropic position,\nfor some vector capxsubscriptğ‘apxc_{\\rm apx} that satisfies |capxâˆ’c|â‰¤Îµsubscriptğ‘apxğ‘ğœ€|c_{\\rm apx}-c|\\leq\\sqrt{{\\varepsilon}},\nin Oâ€‹(|tâˆ—|2/Îµ)=Oâ€‹(nâ€‹|tâˆ—|âˆ2/Îµ)ğ‘‚superscriptsuperscriptğ‘¡2ğœ€ğ‘‚ğ‘›superscriptsubscriptsuperscriptğ‘¡2ğœ€O(|t^{*}|^{2}/{\\varepsilon})=O(n|t^{*}|_{\\infty}^{2}/{\\varepsilon}) iterations of gradient descent\n(or Oâ€‹(nâ€‹|tâˆ—|âˆ/Îµ)ğ‘‚ğ‘›subscriptsuperscriptğ‘¡ğœ€O(\\sqrt{n}|t^{*}|_{\\infty}/\\sqrt{{\\varepsilon}}) iterations of accelerated gradient descent),\nwhere tâˆ—superscriptğ‘¡t^{*} is the (unique) extremizing vector of fğ‘“f with miniâ¡tiâˆ—=0subscriptğ‘–subscriptsuperscriptğ‘¡ğ‘–0\\min_{i}t^{*}_{i}=0.\nEach iteration takes\nOâ€‹(nâ€‹d2â€‹logâ¡(logâ¡n+|tâˆ—|âˆ+logâ¡1Îµ))ğ‘‚ğ‘›superscriptğ‘‘2ğ‘›subscriptsuperscriptğ‘¡1ğœ€O\\left(nd^{2}\\log\\left(\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}}\\right)\\right)\narithmetic operations on words of logâ¡n+|tâˆ—|âˆ+logâ¡1Îµğ‘›subscriptsuperscriptğ‘¡1ğœ€\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}} bits.\nMoreover, putting cmin:=miniâ¡ciassignsubscriptğ‘minsubscriptğ‘–subscriptğ‘ğ‘–c_{\\rm min}:=\\min_{i}c_{i}, we have Theorem 1.5 implies that, for\nmoderate values of Îµğœ€{\\varepsilon} (such as Oâ€‹(d/n)ğ‘‚ğ‘‘ğ‘›O(d/n), which suffices for the applications\nwe are aware of), gradient descent should be reasonably fast, especially when\ncminsubscriptğ‘minc_{\\rm min} is not too close to 00. Verifying this experimentally, though, and\ncomparing its performance in practice with the other approaches (such as\nellipsoid-based techniques), is left for future research. The two main steps in our proof are as follows. (1) In Section 4.3 we show that the â„“1subscriptâ„“1\\ell_{1}-norm of the\ngradient of fğ‘“f is bounded by 2â€‹d2ğ‘‘2d (and the â„“2subscriptâ„“2\\ell_{2}-norm by 2â€‹d2ğ‘‘\\sqrt{2}d), for all tğ‘¡t,\nand the computation of âˆ‡fâˆ‡ğ‘“\\nabla f amounts to computing the singular value decomposition (SVD)\nof the vector set {eti/2â€‹xi}i=1nsuperscriptsubscriptsuperscriptğ‘’subscriptğ‘¡ğ‘–2subscriptğ‘¥ğ‘–ğ‘–1ğ‘›\\{e^{t_{i}/2}x_{i}\\}_{i=1}^{n},\nwhere t=(t1,â€¦,tn)ğ‘¡subscriptğ‘¡1â€¦subscriptğ‘¡ğ‘›t=(t_{1},\\ldots,t_{n}) is the current approximation maintained by the\ngradient descent. See SectionÂ 3 for details.\nSince SVD is used in numerous applications and is available\nin many scientific and statistical packages, the application of gradient descent\nto fğ‘“f is particularly simple to implement. (2)\nIn Section 4.5 we show that the largest eigenvalue of the Hessian of\nfğ‘“f is â‰¤1/2absent12\\leq 1/2 (for all tğ‘¡t). This justifies using a variant of gradient descent\nfor smooth functions (given in BubeckÂ [5, Section 3.2]).\nSuch a variant finds a point tâ€²superscriptğ‘¡â€²t^{\\prime} such that |fâ€‹(tâ€²)âˆ’fâ€‹(tâˆ—)|â‰¤Îµğ‘“superscriptğ‘¡â€²ğ‘“superscriptğ‘¡ğœ€|f(t^{\\prime})-f(t^{*})|\\leq{\\varepsilon}\nin Oâ€‹(|tâˆ—|2/Îµ)ğ‘‚superscriptsuperscriptğ‘¡2ğœ€O(|t^{*}|^{2}/{\\varepsilon}) steps (or Oâ€‹(|tâˆ—|/Îµ)ğ‘‚superscriptğ‘¡ğœ€O(|t^{*}|/\\sqrt{{\\varepsilon}}) steps if we\nuse Nesterovâ€™s accelerated gradient descentÂ [5, Section 3.7]),\nwhen we start the descent from the origin t=0ğ‘¡0t=0.\nWe also show (Section B.3), using again our bound\non the eigenvalues of the Hessian, that |âˆ‡fâ€‹(tâ€²)âˆ’âˆ‡fâ€‹(tâˆ—)|â‰¤Îµâˆ‡ğ‘“superscriptğ‘¡â€²âˆ‡ğ‘“superscriptğ‘¡ğœ€|{\\nabla}f(t^{\\prime})-{\\nabla}f(t^{*})|\\leq\\sqrt{{\\varepsilon}},\nwhich implies in our setting that tâ€²superscriptğ‘¡â€²t^{\\prime} yields a transformation that maps Xğ‘‹X\nto radial capxsubscriptğ‘apxc_{\\rm apx}-isotropic position, for some capxsubscriptğ‘apxc_{\\rm apx}\nsatisfying |capxâˆ’c|2â‰¤Îµsubscriptsubscriptğ‘apxğ‘2ğœ€|c_{\\rm apx}-c|_{2}\\leq\\sqrt{{\\varepsilon}}. To complete this part of the analysis, we establish an upper bound on |tâˆ—|superscriptğ‘¡|t^{*}|,\nin Section 4.4.\nOur bound, stated in TheoremÂ 1.5,\nis logarithmic in the paranmeters Î·ğœ‚\\eta and Î´ğ›¿\\delta of Definition 1.4 (see (4)).\nWe note that Hardt and Moitra 7, as well as Singh and Vishnoi , use\na different set of parameters for bounding |tâˆ—|âˆsubscriptsuperscriptğ‘¡|t^{*}|_{\\infty} (Singh and Vishnoi\ndo this in their more general setting). We use our technique to significantly\nstrengthen the bound of Hardt and Moitra 7, and finally to deduce Theorem 1.5. We say that a function fğ‘“f is Î±ğ›¼\\alpha-strongly convex if it satisfies for any xğ‘¥x and yğ‘¦y. For smooth and strongly convex functions,\ngradient descent converges faster, in Oâ€‹(Îºâ€‹logâ¡|tâˆ—|Îµ)ğ‘‚ğœ…superscriptğ‘¡ğœ€O\\left(\\kappa\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) steps\n(or in Oâ€‹(Îºâ€‹logâ¡|tâˆ—|Îµ)ğ‘‚ğœ…superscriptğ‘¡ğœ€O\\left(\\sqrt{\\kappa}\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) steps with Nesterovâ€™s\nacceleration), where Îºğœ…\\kappa is the ratio between the largest and smallest positive\neigenvalues of the Hessian; see SectionÂ 4 for more details.\nUnfortunately, our function fğ‘“f is not strongly convex, as there are directions,\nsuch as the all-1 vector ğŸ1{\\bf 1}, in which fğ‘“f is constant.\nNevertheless, we show that, under certain irreducibility assumptions\n(which automatically hold if cğ‘c is deep inside the basis polytope, in the sense of Definition 1.4),\nand under a suitable definition of the optimization domain, fğ‘“f is strongly convex in that domain.\nThis gives the bound stated in the following theorem, which depends only logarithmically\non Îµğœ€{\\varepsilon}, but with considerably worse dependence on the other parameters of the problem. For a vector cğ‘c that is (Î·,Î´)ğœ‚ğ›¿(\\eta,\\delta)-deep inside KXsubscriptğ¾ğ‘‹K_{X}, we can construct\nthe transformation that brings Xğ‘‹X into radial capxsubscriptğ‘apxc_{\\rm apx}-isotropic position,\nfor some capxsubscriptğ‘apxc_{\\rm apx} satisfying |capxâˆ’c|2â‰¤Îµsubscriptsubscriptğ‘apxğ‘2ğœ€|c_{\\rm apx}-c|_{2}\\leq{\\varepsilon},\nin Oâ€‹(Îºâ€‹logâ¡|tâˆ—|Îµ)ğ‘‚ğœ…superscriptğ‘¡ğœ€O\\left(\\kappa\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) iterations of gradient descent\n(or Oâ€‹(Îºâ€‹logâ¡|tâˆ—|Îµ)ğ‘‚ğœ…superscriptğ‘¡ğœ€O\\left(\\sqrt{\\kappa}\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) iterations of accelerated gradient descent).\nEach iteration takes Oâ€‹(nâ€‹d2â€‹logâ¡(logâ¡n+|tâˆ—|âˆ+logâ¡1Îµ))ğ‘‚ğ‘›superscriptğ‘‘2ğ‘›subscriptsuperscriptğ‘¡1ğœ€O\\left(nd^{2}\\log\\left(\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}}\\right)\\right)\narithmetic operations on words of logâ¡n+|tâˆ—|âˆ+logâ¡1Îµğ‘›subscriptsuperscriptğ‘¡1ğœ€\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}} bits, with\nthe same upper bound on |tâˆ—|âˆsubscriptsuperscriptğ‘¡|t^{*}|_{\\infty} as in TheoremÂ 1.5 and with If Xğ‘‹X is in general position then we also have Here Î”Smâ€‹iâ€‹nsuperscriptsubscriptÎ”ğ‘†ğ‘šğ‘–ğ‘›\\Delta_{S}^{min} is the minimal square determinant of a dğ‘‘d-tuple of vectors from Xğ‘‹X. The latter result is obtained by a careful analysis of the Hessian of the function fğ‘“f,\ngiven in two completely different forms. The first form works for general Xğ‘‹X and a\nvector cğ‘c that is deeply inside the basic polytope, whereas the second bound,\nwith better dependence on nğ‘›n and dğ‘‘d, depends on Xğ‘‹X being in general position, with a bound that depends\non the minimum square determinant of any dğ‘‘d-tuple Sğ‘†S from Xğ‘‹X (as do earlier studies, such as 7).\nFinally, since for implementing gradient descent one\npretends to have access to an exact gradient, whereas in practice we compute the gradient only\napproximately using SVD, we show in Section B.2, that we may account\nfor these errors within the same asymptotic bounds as in the theorems above. Other algorithmic approaches.\nThe aforementioned related work uses two other algorithmic approaches to our problem.\nThe first approach is already implicit in the proof of ForsterÂ 2.\nAs mentioned, Forster considers radial isotropy\n(with ci=d/nsubscriptğ‘ğ‘–ğ‘‘ğ‘›c_{i}=d/n for each iğ‘–i and for vectors in general position) and proposes to\ntransform the vectors by the mapping zi=Bâ€‹xi/|Bâ€‹xi|subscriptğ‘§ğ‘–ğµsubscriptğ‘¥ğ‘–ğµsubscriptğ‘¥ğ‘–{z}_{i}=B{x}_{i}/|B{x}_{i}|,\nwhere B=Î£âˆ’1â€‹VTğµsuperscriptÎ£1superscriptğ‘‰ğ‘‡B=\\Sigma^{-1}V^{T}, and where Vğ‘‰V and Î£Î£\\Sigma are\ntwo of the matrices that are produced as part of the SVD\nXT=Uâ€‹Î£â€‹VTsuperscriptğ‘‹ğ‘‡ğ‘ˆÎ£superscriptğ‘‰ğ‘‡X^{T}=U\\Sigma V^{T} of XTsuperscriptğ‘‹ğ‘‡X^{T} (see SectionÂ 3 for details).\nForster proves that the smallest\neigenvalue of the linear operator âˆ‘i=1nziâŠ—zisuperscriptsubscriptğ‘–1ğ‘›tensor-productsubscriptğ‘§ğ‘–subscriptğ‘§ğ‘–\\sum_{i=1}^{n}z_{i}\\otimes z_{i} is\neither greater than the smallest eigenvalue of âˆ‘i=1nxiâŠ—xisuperscriptsubscriptğ‘–1ğ‘›tensor-productsubscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘–\\sum_{i=1}^{n}x_{i}\\otimes x_{i},\nor is the same but with a strictly smaller multiplicity.\nUsing this fact, Forster shows that if we iterate this step it converges\nto a set of vectors in radial isotropic position.\nHowever, no guarantee about the rate of convergence is given in 2.\nThe transformation that brings Xğ‘‹X to (approximate)\nradial isotropic position is obtained by composing the transformations\nused in each iterative step. Garg et al.Â 3 also use a similar algorithm for the more\ngeneral problem of bringing a higher-rank Brascamp-Lieb datum into\ngeometric position. They bound the running time of this approach\nby a polynomial in the bit length of the input, the common denominator of the\nentries in the vector cğ‘c (which they assume are rational whose common denominator\nis not too large), and in 1/Îµ1ğœ€1/{\\varepsilon}, where Îµğœ€{\\varepsilon} is the approximation parameter. The second approach, due to Hardt and MoitraÂ 7 (and also used by\nGurvits and Samorodnitsky 6, Singh and Vishnoi ,\nand Straszak and Vishnoi , in other related settings),\napplies an ellipsoid-based procedure for (roughly) halving the region\ncontaining the minimizing vector tâˆ—superscriptğ‘¡t^{*}.\nFor this, they bound the region in which tâˆ—superscriptğ‘¡t^{*} lies, and quantify the strong convexity of fğ‘“f.\nThe resulting algorithm is polynomial in logâ¡1/Îµ1ğœ€\\log 1/{\\varepsilon}, in 1/Î³1ğ›¾1/\\gamma\n(where Î³ğ›¾\\gamma is another parameter that also measures (roughly) how\ndeep is cğ‘c inside the basis polytope), in Lğ¿L (the bit complexity\nof the input vectors and of cğ‘c), and inversely in the minimum\nsquare determinant Î”SminsuperscriptsubscriptÎ”ğ‘†min\\Delta_{S}^{\\rm min} of any dğ‘‘d-tuple Sğ‘†S of linearly\nindependent xisubscriptğ‘¥ğ‘–x_{i}â€™s (as in the second bound in TheoremÂ 1.6).\nOur results here, when plugged into Hardt and Moitraâ€™s analysis,\nimprove their bounds considerably. Hardt and Moitraâ€™s algorithm is\ncertainly harder to implement than those that use SVD-based gradient descent,\nlike our algorithms. Straszak and Vishnoi  prove (in their more general setting,\nof which ours is a special case) that there always exists\na vector tâ€²superscriptğ‘¡â€²t^{\\prime} such that |fâ€‹(tâ€²)âˆ’fâ€‹(tâˆ—)|â‰¤Îµğ‘“superscriptğ‘¡â€²ğ‘“superscriptğ‘¡ğœ€|f(t^{\\prime})-f(t^{*})|\\leq{\\varepsilon}\nand |tâ€²|2subscriptsuperscriptğ‘¡â€²2|t^{\\prime}|_{2} is polynomial in logâ¡(1/Îµ)1ğœ€\\log(1/{\\varepsilon}). This independence of\nother parameters allows us to actually strengthen our\nTheorems 1.5 and 1.6, by replacing\nthe upper bound on |tâˆ—|âˆsubscriptsuperscriptğ‘¡|t^{*}|_{\\infty} by the minimum between the actual upper bound\nthat we derive here and polyâ€‹(logâ¡(1/Îµ))poly1ğœ€{\\rm poly}(\\log(1/{\\varepsilon})) (and\nmodify the optimization region accordingly; see Equation (12)). (1) We use gradient descent for computing the minimizer tâˆ—superscriptğ‘¡t^{*},\ninstead of the other techniques proposed so far in the literature\n(and reviewed above). We believe\nit to be a superior technique, which is easy to implement and which\nshould run much faster in practice than the other approaches. \n (2) The connection between radial isotropy and SVD, although\nalready noted by ForsterÂ 2, is explored here in a deeper and more extended\ncontext, and is shown to be very beneficial both for the algorithms themselves and\nfor their analysis. \n (3) We offer detailed (and fairly nontrivial) analysis of several important\nparameters of the problem, such as |tâˆ—|âˆsubscriptsuperscriptğ‘¡|t^{*}|_{\\infty}, the smoothness parameter\nÎ²ğ›½\\beta, the parameter Î±ğ›¼\\alpha of strong convexity, and more. \n (4)\nWe introduce a new notion of being â€œdeep insideâ€ the basis polytope, using the parameters Î·ğœ‚\\eta and Î´ğ›¿\\delta\nof Definition 1.4. We use these parameters as an alternative to\nthe minimum square determinant\nÎ”SminsuperscriptsubscriptÎ”ğ‘†min\\Delta_{S}^{\\rm min} (which was used in previous studies, and which is very\nsensitive to even a single â€˜nearly dependentâ€™ dğ‘‘d-tuple in Xğ‘‹X),\nwhich makes our approach more stable with respect to any small perturbation\nof the input, and allows us to obtain better bounds for the parameters\nmentioned in (3), and thereby for the performance of gradient descent.\n (5) Last, but perhaps not least, we offer a comprehensive\ntreatment of this fascinating topic, which we believe to be helpful, given the scattered\nnature of the existing relevant literature, where the problem is discussed in widely\ndifferent contexts, using different styles of terminology, often addressed only as\na subproblem of other problems, and often receiving rather sketchy treatments,\nand suboptimal analysis of its parameters."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "On a reverse form of the Brascamp-Lieb inequality",
      "abstract": "",
      "year": "1998",
      "venue": "Inventiones Mathematicae",
      "authors": "F. Barthe"
    },
    {
      "index": 1,
      "title": "Algorithms in Real Algebraic Geometry",
      "abstract": "",
      "year": "2006",
      "venue": "Springer-Verlag",
      "authors": "S. Basu, R. Pollack, and M.-F. Roy"
    },
    {
      "index": 2,
      "title": "Foundations of Data Science",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "A. Blum, J. Hopcroft and R. Kannan"
    },
    {
      "index": 3,
      "title": "A Comprehensive Introduction to Linear Algebra",
      "abstract": "",
      "year": "1989",
      "venue": "Addison Wesley",
      "authors": "J.G. Broida and S.G. Williamson"
    },
    {
      "index": 4,
      "title": "Convex Optimization: Algorithms and Complexity",
      "abstract": "",
      "year": "2015",
      "venue": "Foundations and Trends in Machine Learning",
      "authors": "S. Bubeck",
      "orig_title": "Convex Optimization: Algorithms and Complexity",
      "paper_id": "1405.4980v2"
    },
    {
      "index": 5,
      "title": "A sharp analog of Youngâ€™s inequality on SNsuperscriptğ‘†ğ‘S^{N} and related entropy inequalities",
      "abstract": "",
      "year": "2004",
      "venue": "J. Geometric Analysis",
      "authors": "E. Carlen, E. Lieb and M. Loss"
    },
    {
      "index": 6,
      "title": "Testing membership in matroid polyhedra.",
      "abstract": "",
      "year": "1984",
      "venue": "J. Combinat. Theory Ser. B",
      "authors": "W. Cunningham"
    },
    {
      "index": 7,
      "title": "Superquadratic lower bound for 3-query locally correctable codes over the reals",
      "abstract": "",
      "year": "2017",
      "venue": "Theory of Computing",
      "authors": "Z. Dvir, S. Saraf and A. Wigderson"
    },
    {
      "index": 8,
      "title": "Submodular functions, matroids, and certain polyhedra",
      "abstract": "",
      "year": "1970",
      "venue": "Combinatorial Structures",
      "authors": "J. Edmonds"
    },
    {
      "index": 9,
      "title": "A nearly quadratic bound for point location in hyperplane arrangements, in the linear decision tree model",
      "abstract": "",
      "year": "2018",
      "venue": "Discrete Comput. Geom.",
      "authors": "E. Ezra and M. Sharir"
    },
    {
      "index": 10,
      "title": "Decomposing arrangements of hyperplanes: VC-dimension, combinatorial dimension, and point location",
      "abstract": "",
      "year": "",
      "venue": "Discrete Comput. Geom.",
      "authors": "E. Ezra, S. Har-Peled, H. Kaplan and M. Sharir"
    },
    {
      "index": 11,
      "title": "A linear lower bound on the unbounded error probabilistic communication complexity",
      "abstract": "",
      "year": "2002",
      "venue": "J. Comput. Systems Sci.",
      "authors": "J. Forster"
    },
    {
      "index": 12,
      "title": "Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via Operator Scaling",
      "abstract": "",
      "year": "2018",
      "venue": "Geom. Funct. Anal.",
      "authors": "A. Garg, L. Gurvitz, R. Oliveira, and A. Wigderson",
      "orig_title": "Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via operator scaling",
      "paper_id": "1607.06711v4"
    },
    {
      "index": 13,
      "title": "Matrix Computations, 4th Edition",
      "abstract": "",
      "year": "2013",
      "venue": "Johns Hopkins University Press",
      "authors": "G. H. Golub and C. F. Van Loan"
    },
    {
      "index": 14,
      "title": "The ellipsoid method and its consequences in combinatorial optimization",
      "abstract": "",
      "year": "1981",
      "venue": "Combinatorica",
      "authors": "M. GrÃ¶tschel, L. LovÃ¡sz, and A. Schrijver"
    },
    {
      "index": 15,
      "title": "A deterministic polynomial-time algorithm for approximating mixed discriminant and mixed volume",
      "abstract": "",
      "year": "2000",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "L. Gurvits and A. Samorodnitsky"
    },
    {
      "index": 16,
      "title": "Algorithms and hardness for robust subspace recovery",
      "abstract": "",
      "year": "2013",
      "venue": "JMLR",
      "authors": "M. Hardt and A. Moitra"
    },
    {
      "index": 17,
      "title": "A combinatorial strongly polynomial time algorithm for minimizing submodular functions",
      "abstract": "",
      "year": "2001",
      "venue": "J. ACM",
      "authors": "S. Iwata, L. Fleischer, and S. Fujishige"
    },
    {
      "index": 18,
      "title": "Near-optimal linear decision trees for k-SUM and related problems",
      "abstract": "",
      "year": "2018",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "D. Kane, S. Lovett and S. Moran",
      "orig_title": "Near-optimal linear decision trees for kğ‘˜k-SUM and related problems",
      "paper_id": "1705.01720v1"
    },
    {
      "index": 19,
      "title": "Generalized comparison trees for point-location problems",
      "abstract": "",
      "year": "2018",
      "venue": "Internat. Colloq. Automata Languages and Programming",
      "authors": "D. Kane, S. Lovett and S. Moran",
      "orig_title": "Generalized comparison trees for point-location problems",
      "paper_id": "1804.08237v1"
    },
    {
      "index": 20,
      "title": "On nearly radial marginals of high-dimensional probability measures.",
      "abstract": "",
      "year": "2010",
      "venue": "J. Eur. Math. Soc",
      "authors": "B. Klartag"
    },
    {
      "index": 21,
      "title": "tcsmath.wordpress.com/2015/06/19/entropy-optimality-forsters-isotropy.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "J. R. Lee"
    },
    {
      "index": 22,
      "title": "Convex Analysis",
      "abstract": "",
      "year": "1970",
      "venue": "Princeton University Press",
      "authors": "R. T. Rockafellar"
    },
    {
      "index": 23,
      "title": "A combinatorial algorithm for minimizing submodular functions in strongly polynomial time",
      "abstract": "",
      "year": "2000",
      "venue": "J. Combinat. Theory Ser. B",
      "authors": "A. Schrijver"
    },
    {
      "index": 24,
      "title": "Entropy, optimization and counting",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "D. Singh and N. K. Vishnoi"
    },
    {
      "index": 25,
      "title": "Matrix Algorithms. Volume II: Eigensystems",
      "abstract": "",
      "year": "2001",
      "venue": "SIAM",
      "authors": "G. W. Stewart"
    },
    {
      "index": 26,
      "title": "Computing maximum entropy distributions everywhere",
      "abstract": "",
      "year": "",
      "venue": "arXiv",
      "authors": "D. Straszak and N. K. Vishnoi"
    },
    {
      "index": 27,
      "title": "An Introduction to Finite Tight Frames",
      "abstract": "",
      "year": "2018",
      "venue": "BirkhÃ¤user (Springer)",
      "authors": "S. Waldron"
    }
  ]
}