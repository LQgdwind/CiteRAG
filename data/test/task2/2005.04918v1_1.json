{
  "paper_id": "2005.04918v1",
  "title": "On Radial Isotropic Position: Theory and Algorithms Work on this paper by Shiri Artstein was supported in part by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 770127), and in part by ISF grant 665/15. Work by Haim Kaplan was supported in part by by ISF grant 1595/19 and grant 1367/2016 from the German-Israeli Science Foundation (GIF). Work by Micha Sharir was supported in part by Grant 260/18 from the Israel Science Foundation, Grant G-1367-407.6/2016 from the German-Israeli Foundation for Scientific Research and Development, by the Blavatnik Research Fund in Computer Science at Tel Aviv University, and by the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).",
  "sections": {
    "introduction": "A set X={xi}i=1n𝑋superscriptsubscriptsubscript𝑥𝑖𝑖1𝑛X=\\{x_{i}\\}_{i=1}^{n} of n≥d𝑛𝑑n\\geq d (column) vectors in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}\nis in isotropic position\nif one (and thus all) of the following equivalent conditions holds. Here we denote by x⊗xtensor-product𝑥𝑥x\\otimes x the rank-one operator (x⊗x)​y=⟨x,y⟩​xtensor-product𝑥𝑥𝑦𝑥𝑦𝑥(x\\otimes x)y=\\langle x,y\\rangle x\n(which is x​xT𝑥superscript𝑥𝑇xx^{T} in matrix form), and Idsubscript𝐼𝑑I_{d} is the d×d𝑑𝑑d\\times d identity matrix.\nIn words, the third condition asserts that the sum of the squares of the projections of our vectors\nin any direction has the same value n/d𝑛𝑑n/d. If the xisubscript𝑥𝑖x_{i}’s are unit vectors then the third\ncondition also says that the sum of the squares of the projections of any unit vector on the xisubscript𝑥𝑖x_{i}’s is n/d𝑛𝑑n/d. As an easy example, the standard basis {ei}i=1dsuperscriptsubscriptsubscript𝑒𝑖𝑖1𝑑\\{e_{i}\\}_{i=1}^{d} is a set in isotropic position.\nAs a slightly less obvious example, the\nvertices of a regular simplex, so that its center is at the origin and the vertices lie on 𝕊d−1superscript𝕊𝑑1\\mathbb{S}^{d-1},\nform a set of d+1𝑑1d+1 points in isotropic position. In more generality,\nconsider a subspace E𝐸E of ℝnsuperscriptℝ𝑛\\mathbb{R}^{n} of dimension d𝑑d.\nLet {ei}i=1nsuperscriptsubscriptsubscript𝑒𝑖𝑖1𝑛\\{e_{i}\\}_{i=1}^{n} be an orthonormal basis of ℝnsuperscriptℝ𝑛\\mathbb{R}^{n},\nand let yi=PE​eisubscript𝑦𝑖subscript𝑃𝐸subscript𝑒𝑖y_{i}=P_{E}e_{i} be their orthogonal projections onto E𝐸E.\nWe have ∑i=1nei⊗ei=Insuperscriptsubscript𝑖1𝑛tensor-productsubscript𝑒𝑖subscript𝑒𝑖subscript𝐼𝑛\\sum_{i=1}^{n}e_{i}\\otimes e_{i}=I_{n}, and therefore\n∑i=1nyi⊗yi=Idsuperscriptsubscript𝑖1𝑛tensor-productsubscript𝑦𝑖subscript𝑦𝑖subscript𝐼𝑑\\sum_{i=1}^{n}y_{i}\\otimes y_{i}=I_{d}, as is easily verified\n(note that PE​PET=Idsubscript𝑃𝐸superscriptsubscript𝑃𝐸𝑇subscript𝐼𝑑P_{E}P_{E}^{T}=I_{d}). This means that by rewriting\nxi=yi/|yi|subscript𝑥𝑖subscript𝑦𝑖subscript𝑦𝑖x_{i}=y_{i}/|y_{i}| (assuming that all the yisubscript𝑦𝑖y_{i}’s are non-zero),\nand letting ci=|yi|2≥0subscript𝑐𝑖superscriptsubscript𝑦𝑖20c_{i}=|y_{i}|^{2}\\geq 0, we get that\nId=∑i=1nci​xi⊗xi.subscript𝐼𝑑superscriptsubscript𝑖1𝑛tensor-productsubscript𝑐𝑖subscript𝑥𝑖subscript𝑥𝑖I_{d}=\\sum_{i=1}^{n}c_{i}x_{i}\\otimes x_{i}.\nIn the case of a simplex, one can present the vertices of the regular simplex\nas the projections of an orthonormal basis in ℝd+1superscriptℝ𝑑1\\mathbb{R}^{d+1} such that\nall projections have equal length, which is why all the\ncisubscript𝑐𝑖c_{i}’s are equal (to d/(d+1)𝑑𝑑1d/(d+1)) and we get isotropic position.\nThis naturally brings us to the following generalizations of isotropy. c𝑐c-isotropy.\nLet c=(c1,…,cn)𝑐subscript𝑐1…subscript𝑐𝑛c=(c_{1},\\ldots,c_{n}) be an n𝑛n-sequence of positive real weights.\nA set X={xi}i=1n𝑋superscriptsubscriptsubscript𝑥𝑖𝑖1𝑛X=\\{x_{i}\\}_{i=1}^{n} of n≥d𝑛𝑑n\\geq d (column) vectors in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}\nis in c𝑐c-isotropic position if\nthe following equivalent properties hold. Isotropy is the special case of c𝑐c-isotropy when ci=d/nsubscript𝑐𝑖𝑑𝑛c_{i}=d/n for all i𝑖i. It is a classical fact that any set of n𝑛n (not necessarily unit) vectors {yi}i=1nsuperscriptsubscriptsubscript𝑦𝑖𝑖1𝑛\\{y_{i}\\}_{i=1}^{n}\nthat span ℝdsuperscriptℝ𝑑\\mathbb{R}^{d} can be put in c𝑐c-isotropic position, for any positive weight sequence c𝑐c, using the\nlinear transformation A=(∑ici​yi⊗yi)−1/2𝐴superscriptsubscript𝑖tensor-productsubscript𝑐𝑖subscript𝑦𝑖subscript𝑦𝑖12A=\\Bigl{(}\\sum_{i}c_{i}y_{i}\\otimes y_{i}\\Bigr{)}^{-1/2} (which is the unique\npositive definite matrix A=AT𝐴superscript𝐴𝑇A=A^{T} for which A2=(∑ici​yi⊗yi)−1superscript𝐴2superscriptsubscript𝑖tensor-productsubscript𝑐𝑖subscript𝑦𝑖subscript𝑦𝑖1A^{2}=\\Bigl{(}\\sum_{i}c_{i}y_{i}\\otimes y_{i}\\Bigr{)}^{-1},\nand which exists as ∑ci​yi⊗yitensor-productsubscript𝑐𝑖subscript𝑦𝑖subscript𝑦𝑖\\sum c_{i}y_{i}\\otimes y_{i} is positive definite), because In this paper we are specifically interested in the notion of radial isotropy, defined as follows. Radial c𝑐c-isotropy.\nA set X={xi}i=1n𝑋superscriptsubscriptsubscript𝑥𝑖𝑖1𝑛X=\\{x_{i}\\}_{i=1}^{n} of n≥d𝑛𝑑n\\geq d nonzero (column) vectors in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}\nis in radial c𝑐c-isotropic position (for a vector c𝑐c as in Definition 1.1)\nif the normalized vectors xi/|xi|subscript𝑥𝑖subscript𝑥𝑖x_{i}/|x_{i}|, for i=1,…,n𝑖1…𝑛i=1,\\ldots,n,\nare in c𝑐c-isotropic position. We say that the vectors are in (standard, or uniform)\nradial isotropic position when the above property holds with ci=d/nsubscript𝑐𝑖𝑑𝑛c_{i}=d/n for all i𝑖i. It is easily checked that any weights cisubscript𝑐𝑖c_{i} that admit a point set in\nradial c𝑐c-isotropic position must satisfy ∑i=1nci=dsuperscriptsubscript𝑖1𝑛subscript𝑐𝑖𝑑\\sum_{i=1}^{n}c_{i}=d. We are interested in the existence and the computation of a linear transformation that maps\na given set of vectors into radial c𝑐c-isotropic position, for some prescribed sequence c𝑐c\nof positive weights. Formally, We say that a nonsingular linear transformation T:ℝd→ℝd:𝑇→superscriptℝ𝑑superscriptℝ𝑑T:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}\nputs a set X𝑋X of n𝑛n nonzero vectors x1,…,xn∈ℝdsubscript𝑥1…subscript𝑥𝑛superscriptℝ𝑑x_{1},\\dots,x_{n}\\in\\mathbb{R}^{d} in radial c𝑐c-isotropic position,\nfor a coefficient vector c∈(ℝ+)n𝑐superscriptsuperscriptℝ𝑛c\\in(\\mathbb{R}^{+})^{n} of ℓ1subscriptℓ1\\ell_{1}-norm d𝑑d, if The existence of such a linear transformation, even if we consider only the simpler form of\nradial isotropy (ci=d/nsubscript𝑐𝑖𝑑𝑛c_{i}=d/n for each i𝑖i),\nis more intricate than the standard, non-radial setup (and in general it may fail to exist).\nForster 2 proved that when the vectors of X𝑋X are in general position\n(i.e., every d𝑑d vectors among them are linearly independent),\nsuch a transformation exists. In fact, earlier, Barthe  proved\nthat the vectors xisubscript𝑥𝑖x_{i}, i=1,…,n𝑖1…𝑛i=1,\\ldots,n, can be put in a radial c𝑐c-isotropic position if and only if\nc𝑐c is in the relative interior of the so called basis polytope (see below) defined by the xisubscript𝑥𝑖x_{i}’s.\nWe give a complete, and somewhat enhanced version of this theory in Section 2.\nSince this theory is well known,111It is well known to the experts. The details that we provide in the appendix are spelled\nout for the convenience of the non-expert reader.\nexcept for the enhancements that we derive and add to it, Section 2 contains\nonly the highlights of the theory, and most details, including proofs, are presented in\nAppendix A.\nIn a later study, Carlen, Lieb and Loss  gave an equivalent characterization\nof the basis polytope, which we use extensively in our study (we prove it too, for completeness,\nin Appendix A). Radial isotropy arises when we have a set of n𝑛n subspaces, say lines, and we want to linearly transform these\nsubspaces so that the sum of the squared projections of any unit vector x𝑥x on their transformed copies is n/d𝑛𝑑n/d.\nA similar interpretation can be given for general weights cisubscript𝑐𝑖c_{i}.222Radial isotropy corresponds to the case of lines; higher-dimensional subspaces require further extension of the notion.\nFurthermore, as suggested by Hardt and Moitra 7,\nradial isotropic position can also be thought of as a stable analogue of isotropic position.\nThat is, while isotropic position has important applications both in algorithms and in exploratory data analysis,\nit is rather sensitive to even a small number of outliers.\nRadial isotropic position is more robust in the presence of outliers. Specific Applications.\nThe problem arises in several, rather diverse applications, and several other studies\naddress problems of very similar or more general nature. Among these applications and related work we mention (i)\nthe derivation of a linear lower bound on the unbounded error probabilistic communication complexity 2, (ii)\nrobust subspace recovery in the presence of outliers in machine learning 7, (iii)\nalgorithmic and optimization aspects of Brascamp-Lieb inequalities 3, (iv)\na superquadratic lower bound for 3-query locally correctable codes over the reals ,\nand (somewhat more remote but still related) (v)\na deterministic polynomial-time algorithm for\napproximating mixed discriminants and mixed volumes 6. See below for some additional details. Another context in which radial isotropy arises (often as a special case) is in\nentropy maximization; see the works of Carlen et al. , Singh and Vishnoi \nand Straszak and Vishnoi , and also Lee’s blog  and\nsee below for an additional discussion. It also related to so-called finite tight frames\n(see  for a recent monograph on this topic). Last, but not least, is\na recent application in Kane et al. , who use radial isotropic position for\ncomparison-based algorithms for point location in a high-dimensional arrangement of hyperplanes.\nThis work, which served as a starting motivation for our work on this paper,\nfollowed an earlier breakthrough result of Kane et al. 9,\nin which they showed that one can solve the 3-SUM problem (decide whether any three\nout of n𝑛n given real numbers sum to 00) using only O​(n​log2⁡n)𝑂𝑛superscript2𝑛O(n\\log^{2}n) simple linear queries on the input.\nAs a matter of fact, Kane et al. established in 9 a more general result,\nnamely that one can answer point location queries\nin an arrangement of a set H𝐻H of hyperplanes in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}, each of which has\ninteger coefficients with a small ℓ1subscriptℓ1\\ell_{1}-norm, using only\nO​(d​log⁡d​log⁡|H|)𝑂𝑑𝑑𝐻O(d\\log d\\log|H|) simple linear comparisons involving the query point x𝑥x.\nHere d𝑑d is the dimension of the ambient space. (In the 3-SUM application we have\nd=n𝑑𝑛d=n and |H|=(n3)𝐻binomial𝑛3|H|={n\\choose 3}.) In the follow-up study , Kane et al. extended their technique\nto sets of general hyperplanes, with arbitrary coefficients, allowing only\ntwo types of comparisons involving the input x𝑥x: sign tests\n(determining on which side of a hyperplane x𝑥x lies), and\ngeneralized comparison queries, in which one asks\nfor the sign (positive, negative, or zero) of expressions\nof the form α​h1​(x)+β​h2​(x)𝛼subscriptℎ1𝑥𝛽subscriptℎ2𝑥\\alpha h_{1}(x)+\\beta h_{2}(x), where α𝛼\\alpha and β𝛽\\beta\nare arbitrary real parameters. Kane et al. showed that O​(d3​log⁡d​log⁡|H|)𝑂superscript𝑑3𝑑𝐻O(d^{3}\\log d\\log|H|)\nsign tests and generalized comparisons suffice for point location.333This performance is worse than the best known recent bound of\nEzra and Sharir 0, but the queries that their algorithm\nperforms, and the space decomposition that is induced by the algorithm, are\nmuch simpler than those in 0. A crucial step that the algorithm of  performs is transforming the\nnormals of the input hyperplanes into a set in radial (uniform)\nisotropic position by a linear transformation T𝑇T followed by a normalization.\nThey then locate T−1​xsuperscript𝑇1𝑥T^{-1}x in the arrangement of the transformed hyperplanes\n(via standard comparisons, which are equivalent to generalized comparisons in\nthe original space). After the transformation they have the property that\nthe sum of the suitably defined squared ‘scalar products’ of the hyperplanes\nwith any normalized query point x𝑥x is the same, and sufficiently large,\na fact which is crucial for their analysis. Since the latter property is\nall that they need, a relaxed, approximate notion of radial isotropy certainly\nsuffices for their needs. In their work, since they only measure the number of\nsign and (generalized) comparison queries, they do not care about\nthe cost of computing the transformation that brings the hyperplanes\ninto (approximate) radial isotropic position. In another recent study\nby Ezra et al. 1, a full implementation of the first technique\nof Kane et al. 9 is presented. Having an efficient procedure\nfor transforming the input hyperplanes into approximate radial isotropic\nposition, like the one presented in this paper, facilitates a\nstraightforward adaptaion of the machinery in 1 to\nobtain a full implementation of the second, isotropy-based technique\nof Kane et al.  for point location in arrangements of arbitrary\nhyperplanes. A second interesting application, due to\nHardt and Moitra 7, studies the problem of robust subspace recovery.\nIn this problem we are given a set X𝑋X of n𝑛n vectors in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d} and we\nwant to determine whether there exists some subspace of some dimension\nℓℓ\\ell that contains more than (ℓ/d)​nℓ𝑑𝑛(\\ell/d)n of these vectors.\nThe motivation is to detect whether a\ndataset X𝑋X in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d} does in fact reside in a lower-dimensional\nspace if we remove a relatively small subset of outliers. Hardt\nand Moitra gave a Las Vegas algorithm that makes O​(d2​n)𝑂superscript𝑑2𝑛O(d^{2}n) iterations\non average, where in each iteration it draws d𝑑d vectors from X𝑋X,\nand when it finds d𝑑d linearly dependent vectors, it identifies\nthe subspace that they span as a candidate rich subspace. Hardt and Moitra also argue that when such a subspace exists, the\nvector dn​𝟏𝑑𝑛1\\frac{d}{n}{\\bf 1} must be outside the basis polytope of X𝑋X,\nand use this fact, together with a polynomial-time algorithm for\ndetecting membership in the basis polytope, to derandomize their algorithm. As follows from the theory of radial isotropic position (that we will review\nin Section 2 and Appendix A),\nthe condition that dn​𝟏𝑑𝑛1\\frac{d}{n}{\\bf 1} is not in the basis polytope of X𝑋X\nis equivalent to the condition that we cannot put X𝑋X into radial isotropic\nposition with respect to the vector dn​𝟏𝑑𝑛1\\frac{d}{n}{\\bf 1}. Saying it the\nother way around, it follows that putting X𝑋X into radial isotropic position\nwith respect to dn​𝟏𝑑𝑛1\\frac{d}{n}{\\bf 1} is a proof that there is no\nℓℓ\\ell-dimensional subspace that contains more than (ℓ/d)​nℓ𝑑𝑛(\\ell/d)n vectors of X𝑋X,\nfor any ℓℓ\\ell. Motivated by this observation, Hardt and Moitra applied Barthe’s\ncharacterization of attaining radial c𝑐c-isotropic position, in order\nto derive the aforementioned polynomial-time ellipsoid-based algorithm that finds such a\ncertificate (i.e., puts the vectors into radial isotropic position). As mentioned, our problem can be viewed as a special case of a more general problem considered\nby Singh and Vishnoi  and Straszak and Vishnoi  on\nentropy maximization. They consider the problem of finding a distribution q𝑞q of\nmaximum entropy over a (possibly large) collection ℱℱ{\\cal F} of subsets F𝐹F of\n[n]={1,2,…,n}delimited-[]𝑛12…𝑛[n]=\\{1,2,\\ldots,n\\}, among all distributions with a given vector c𝑐c of marginals. A somewhat more distantly related, and more general, line of research is on\noperator scaling and its connection to Brascamp-Lieb constants (see for example\n3 6). The most recent and closely related among these works is\nby Garg et al. 3, who give an algorithm to compute the\nBrascamp-Lieb constant for a particular Brascamp-Lieb “datum”\n(a set of linear transformations and a vector of exponents).\nThe algorithm uses an alternative minimization technique (as the problem is not convex) to\nbring the instance into a so called “geometric position”,\nfrom which it can deduce the desired constant. In this terminology our problem\nis to find a transformation that brings a “Brascamp-Lieb rank-one datum”\ninto isotropic position. For rank one this problem is convex and thereby easier.\nIt was treated before by Hardt and Moitra 7 and in a somewhat different\nsettings also by Gurvitz and Samorodnitsky 6 who applied the ellipsoid\nalgorithm to solve it. See the end of the introduction for a summary of highlights of the novel contributions\nof our work. Given a set X={x1,…,xn}𝑋subscript𝑥1…subscript𝑥𝑛X=\\{x_{1},\\ldots,x_{n}\\} of n𝑛n nonzero vectors in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}, and a weight vector\nc=(c1,…,cn)∈(ℝ+)n𝑐subscript𝑐1…subscript𝑐𝑛superscriptsuperscriptℝ𝑛c=(c_{1},\\ldots,c_{n})\\in\\left(\\mathbb{R}^{+}\\right)^{n},\nsuch that there exists a linear transformation sending the xisubscript𝑥𝑖x_{i}’s into radial c𝑐c-isotropic position, we develop\nalgorithms to find such a transformation. We first consider algorithms that find an exact solution.\nWe show how to do it either by solving a system of n𝑛n polynomial equations,\nin n𝑛n variables, each of degree d𝑑d, or a different system of d2superscript𝑑2d^{2} equations\nin d2superscript𝑑2d^{2} variables, each of degree 2​n2𝑛2n. The running times are dO​(n)superscript𝑑𝑂𝑛d^{O(n)} or\nnO​(d2)superscript𝑛𝑂superscript𝑑2n^{O(d^{2})}, respectively, using well known techniques from symbolic algebra\n(or computational real algebraic geometry ). Our main focus, however, is on computing such a transformation approximately and\nconsiderably more efficiently. That is, we want to compute a linear transformation\nthat puts the xisubscript𝑥𝑖x_{i}’s into radial capxsubscript𝑐apxc_{\\rm apx}-isotropic position, for some vector capxsubscript𝑐apxc_{\\rm apx} that\nsatisfies |capx−c|≤εsubscript𝑐apx𝑐𝜀|c_{\\rm apx}-c|\\leq{\\varepsilon}, say in the ℓ2subscriptℓ2\\ell_{2}-norm, for some prespecified accuracy parameter ε𝜀{\\varepsilon}. We assume that X⊂𝕊d−1𝑋superscript𝕊𝑑1X\\subset\\mathbb{S}^{d-1}; this is a natural assumption for radial isotropy,\nit occurs in the applications we are aware of, and it involves no loss of generality. As follows from Barthe’s results, detecting whether the xisubscript𝑥𝑖x_{i}’s can be put in radial c𝑐c-isotropic position\nis equivalent to testing whether the vector c𝑐c is in the relative interior of\nthe basis polytope associated with X𝑋X, given by where 𝟏Ssubscript1𝑆{\\bf 1}_{S} is the n𝑛n-dimensional indicator vector of the set S𝑆S,\nor, equivalently (as proved in ), by These two expressions are dual, in a sense: In (1), KXsubscript𝐾𝑋K_{X} is defined\nas the convex hull of a set of point, whereas in (2) it is defined as the intersection of halfspaces. The basis polytope plays a central role in matroid theory and submodular optimization;\nsee, e.g., , and there are efficient algorithms for detecting membership in\nthe basis polytope   5 8 4. Furthermore, in some common cases, membership\nof c𝑐c in the basis polytope is obvious, as, for example, in the case studied by Forster 2,\nwhere ci=d/nsubscript𝑐𝑖𝑑𝑛c_{i}=d/n for each i𝑖i and the xisubscript𝑥𝑖x_{i}’s are in general position. Barthe’s characterization reduces the problem of finding a transformation that puts\nthe input vectors in radial c𝑐c-isotropic position to a problem of finding a point\nt∗∈ℝnsuperscript𝑡superscriptℝ𝑛t^{*}\\in\\mathbb{R}^{n} that attains the minimum of a specific convex function f​(t)𝑓𝑡f(t),\ndefined in terms of X𝑋X and c𝑐c. Concretely, It follows that a most natural (and simple) approach, which is the one proposed\nin this paper, to finding the transformation that\nputs the vectors in radial c𝑐c-isotropic position is to find the minimizing vector t∗superscript𝑡t^{*} using\nan appropriate variant of the gradient descent technique; see Bubeck  for details.\nOur main set of results is an analysis of gradient descent applied to f𝑓f, and of various parameters\nthat affect its efficiency. We introduce a new concept promising that the vector c𝑐c lies “deeply inside” KXsubscript𝐾𝑋K_{X}, which is\nrelated to the representation (2). This notion is different and more robust than the ones\nin previous works, and we discuss it in more detail and compare it to other parameters\nconsidered in the literature, in Section 4.  Let X={xi}i=1n⊂Sd−1⊂ℝd𝑋superscriptsubscriptsubscript𝑥𝑖𝑖1𝑛superscript𝑆𝑑1superscriptℝ𝑑X=\\{x_{i}\\}_{i=1}^{n}\\subset S^{d-1}\\subset\\mathbb{R}^{d} and let KXsubscript𝐾𝑋K_{X} be given\nby (1) or (2). We say, for η,δ>0𝜂𝛿0\\eta,\\delta>0, that\na vector c𝑐c lies (η,δ𝜂𝛿\\eta,\\delta)-deep inside KXsubscript𝐾𝑋K_{X} if,\nfor any subspace E𝐸E with dim(E)=k∈{1,…,d−1}dimension𝐸𝑘1…𝑑1\\dim(E)=k\\in\\{1,\\ldots,d-1\\}, we have that and the distance d​(x,E)𝑑𝑥𝐸d(x,E) is the Euclidean distance. Note that for η=δ=0𝜂𝛿0\\eta=\\delta=0 this condition just says that c𝑐c is in the basis polytope, by (2).\nIn general this is a stronger constraint, as we also include in Eδsubscript𝐸𝛿E_{\\delta} vectors that lie close\nto E𝐸E, and impose a stricter inequality on the corresponding cjsubscript𝑐𝑗c_{j}’s.\nOur algorithm does not need to compute or know η𝜂\\eta and δ𝛿\\delta. These parameters are used only for analysis. We establish, in Section 4, the following results, after discussing, in some detail,\na few variants of the gradient descent method, and investigating in depth the associated parameters\nthat control their efficiency. The first set of variants that we use are the projected gradient descent for smooth functions [5, Section 3.2]\nand Nesterov’s accelerated version [5, Section 3.7]; see Section 4 for more details. For a vector c𝑐c that is (η,δ)𝜂𝛿(\\eta,\\delta)-deep inside KXsubscript𝐾𝑋K_{X}, we can construct\na transformation that brings X𝑋X into radial capxsubscript𝑐apxc_{\\rm apx}-isotropic position,\nfor some vector capxsubscript𝑐apxc_{\\rm apx} that satisfies |capx−c|≤εsubscript𝑐apx𝑐𝜀|c_{\\rm apx}-c|\\leq\\sqrt{{\\varepsilon}},\nin O​(|t∗|2/ε)=O​(n​|t∗|∞2/ε)𝑂superscriptsuperscript𝑡2𝜀𝑂𝑛superscriptsubscriptsuperscript𝑡2𝜀O(|t^{*}|^{2}/{\\varepsilon})=O(n|t^{*}|_{\\infty}^{2}/{\\varepsilon}) iterations of gradient descent\n(or O​(n​|t∗|∞/ε)𝑂𝑛subscriptsuperscript𝑡𝜀O(\\sqrt{n}|t^{*}|_{\\infty}/\\sqrt{{\\varepsilon}}) iterations of accelerated gradient descent),\nwhere t∗superscript𝑡t^{*} is the (unique) extremizing vector of f𝑓f with mini⁡ti∗=0subscript𝑖subscriptsuperscript𝑡𝑖0\\min_{i}t^{*}_{i}=0.\nEach iteration takes\nO​(n​d2​log⁡(log⁡n+|t∗|∞+log⁡1ε))𝑂𝑛superscript𝑑2𝑛subscriptsuperscript𝑡1𝜀O\\left(nd^{2}\\log\\left(\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}}\\right)\\right)\narithmetic operations on words of log⁡n+|t∗|∞+log⁡1ε𝑛subscriptsuperscript𝑡1𝜀\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}} bits.\nMoreover, putting cmin:=mini⁡ciassignsubscript𝑐minsubscript𝑖subscript𝑐𝑖c_{\\rm min}:=\\min_{i}c_{i}, we have Theorem 1.5 implies that, for\nmoderate values of ε𝜀{\\varepsilon} (such as O​(d/n)𝑂𝑑𝑛O(d/n), which suffices for the applications\nwe are aware of), gradient descent should be reasonably fast, especially when\ncminsubscript𝑐minc_{\\rm min} is not too close to 00. Verifying this experimentally, though, and\ncomparing its performance in practice with the other approaches (such as\nellipsoid-based techniques), is left for future research. The two main steps in our proof are as follows. (1) In Section 4.3 we show that the ℓ1subscriptℓ1\\ell_{1}-norm of the\ngradient of f𝑓f is bounded by 2​d2𝑑2d (and the ℓ2subscriptℓ2\\ell_{2}-norm by 2​d2𝑑\\sqrt{2}d), for all t𝑡t,\nand the computation of ∇f∇𝑓\\nabla f amounts to computing the singular value decomposition (SVD)\nof the vector set {eti/2​xi}i=1nsuperscriptsubscriptsuperscript𝑒subscript𝑡𝑖2subscript𝑥𝑖𝑖1𝑛\\{e^{t_{i}/2}x_{i}\\}_{i=1}^{n},\nwhere t=(t1,…,tn)𝑡subscript𝑡1…subscript𝑡𝑛t=(t_{1},\\ldots,t_{n}) is the current approximation maintained by the\ngradient descent. See Section 3 for details.\nSince SVD is used in numerous applications and is available\nin many scientific and statistical packages, the application of gradient descent\nto f𝑓f is particularly simple to implement. (2)\nIn Section 4.5 we show that the largest eigenvalue of the Hessian of\nf𝑓f is ≤1/2absent12\\leq 1/2 (for all t𝑡t). This justifies using a variant of gradient descent\nfor smooth functions (given in Bubeck [5, Section 3.2]).\nSuch a variant finds a point t′superscript𝑡′t^{\\prime} such that |f​(t′)−f​(t∗)|≤ε𝑓superscript𝑡′𝑓superscript𝑡𝜀|f(t^{\\prime})-f(t^{*})|\\leq{\\varepsilon}\nin O​(|t∗|2/ε)𝑂superscriptsuperscript𝑡2𝜀O(|t^{*}|^{2}/{\\varepsilon}) steps (or O​(|t∗|/ε)𝑂superscript𝑡𝜀O(|t^{*}|/\\sqrt{{\\varepsilon}}) steps if we\nuse Nesterov’s accelerated gradient descent [5, Section 3.7]),\nwhen we start the descent from the origin t=0𝑡0t=0.\nWe also show (Section B.3), using again our bound\non the eigenvalues of the Hessian, that |∇f​(t′)−∇f​(t∗)|≤ε∇𝑓superscript𝑡′∇𝑓superscript𝑡𝜀|{\\nabla}f(t^{\\prime})-{\\nabla}f(t^{*})|\\leq\\sqrt{{\\varepsilon}},\nwhich implies in our setting that t′superscript𝑡′t^{\\prime} yields a transformation that maps X𝑋X\nto radial capxsubscript𝑐apxc_{\\rm apx}-isotropic position, for some capxsubscript𝑐apxc_{\\rm apx}\nsatisfying |capx−c|2≤εsubscriptsubscript𝑐apx𝑐2𝜀|c_{\\rm apx}-c|_{2}\\leq\\sqrt{{\\varepsilon}}. To complete this part of the analysis, we establish an upper bound on |t∗|superscript𝑡|t^{*}|,\nin Section 4.4.\nOur bound, stated in Theorem 1.5,\nis logarithmic in the paranmeters η𝜂\\eta and δ𝛿\\delta of Definition 1.4 (see (4)).\nWe note that Hardt and Moitra 7, as well as Singh and Vishnoi , use\na different set of parameters for bounding |t∗|∞subscriptsuperscript𝑡|t^{*}|_{\\infty} (Singh and Vishnoi\ndo this in their more general setting). We use our technique to significantly\nstrengthen the bound of Hardt and Moitra 7, and finally to deduce Theorem 1.5. We say that a function f𝑓f is α𝛼\\alpha-strongly convex if it satisfies for any x𝑥x and y𝑦y. For smooth and strongly convex functions,\ngradient descent converges faster, in O​(κ​log⁡|t∗|ε)𝑂𝜅superscript𝑡𝜀O\\left(\\kappa\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) steps\n(or in O​(κ​log⁡|t∗|ε)𝑂𝜅superscript𝑡𝜀O\\left(\\sqrt{\\kappa}\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) steps with Nesterov’s\nacceleration), where κ𝜅\\kappa is the ratio between the largest and smallest positive\neigenvalues of the Hessian; see Section 4 for more details.\nUnfortunately, our function f𝑓f is not strongly convex, as there are directions,\nsuch as the all-1 vector 𝟏1{\\bf 1}, in which f𝑓f is constant.\nNevertheless, we show that, under certain irreducibility assumptions\n(which automatically hold if c𝑐c is deep inside the basis polytope, in the sense of Definition 1.4),\nand under a suitable definition of the optimization domain, f𝑓f is strongly convex in that domain.\nThis gives the bound stated in the following theorem, which depends only logarithmically\non ε𝜀{\\varepsilon}, but with considerably worse dependence on the other parameters of the problem. For a vector c𝑐c that is (η,δ)𝜂𝛿(\\eta,\\delta)-deep inside KXsubscript𝐾𝑋K_{X}, we can construct\nthe transformation that brings X𝑋X into radial capxsubscript𝑐apxc_{\\rm apx}-isotropic position,\nfor some capxsubscript𝑐apxc_{\\rm apx} satisfying |capx−c|2≤εsubscriptsubscript𝑐apx𝑐2𝜀|c_{\\rm apx}-c|_{2}\\leq{\\varepsilon},\nin O​(κ​log⁡|t∗|ε)𝑂𝜅superscript𝑡𝜀O\\left(\\kappa\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) iterations of gradient descent\n(or O​(κ​log⁡|t∗|ε)𝑂𝜅superscript𝑡𝜀O\\left(\\sqrt{\\kappa}\\log\\frac{|t^{*}|}{{\\varepsilon}}\\right) iterations of accelerated gradient descent).\nEach iteration takes O​(n​d2​log⁡(log⁡n+|t∗|∞+log⁡1ε))𝑂𝑛superscript𝑑2𝑛subscriptsuperscript𝑡1𝜀O\\left(nd^{2}\\log\\left(\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}}\\right)\\right)\narithmetic operations on words of log⁡n+|t∗|∞+log⁡1ε𝑛subscriptsuperscript𝑡1𝜀\\log n+|t^{*}|_{\\infty}+\\log\\frac{1}{{\\varepsilon}} bits, with\nthe same upper bound on |t∗|∞subscriptsuperscript𝑡|t^{*}|_{\\infty} as in Theorem 1.5 and with If X𝑋X is in general position then we also have Here ΔSm​i​nsuperscriptsubscriptΔ𝑆𝑚𝑖𝑛\\Delta_{S}^{min} is the minimal square determinant of a d𝑑d-tuple of vectors from X𝑋X. The latter result is obtained by a careful analysis of the Hessian of the function f𝑓f,\ngiven in two completely different forms. The first form works for general X𝑋X and a\nvector c𝑐c that is deeply inside the basic polytope, whereas the second bound,\nwith better dependence on n𝑛n and d𝑑d, depends on X𝑋X being in general position, with a bound that depends\non the minimum square determinant of any d𝑑d-tuple S𝑆S from X𝑋X (as do earlier studies, such as 7).\nFinally, since for implementing gradient descent one\npretends to have access to an exact gradient, whereas in practice we compute the gradient only\napproximately using SVD, we show in Section B.2, that we may account\nfor these errors within the same asymptotic bounds as in the theorems above. Other algorithmic approaches.\nThe aforementioned related work uses two other algorithmic approaches to our problem.\nThe first approach is already implicit in the proof of Forster 2.\nAs mentioned, Forster considers radial isotropy\n(with ci=d/nsubscript𝑐𝑖𝑑𝑛c_{i}=d/n for each i𝑖i and for vectors in general position) and proposes to\ntransform the vectors by the mapping zi=B​xi/|B​xi|subscript𝑧𝑖𝐵subscript𝑥𝑖𝐵subscript𝑥𝑖{z}_{i}=B{x}_{i}/|B{x}_{i}|,\nwhere B=Σ−1​VT𝐵superscriptΣ1superscript𝑉𝑇B=\\Sigma^{-1}V^{T}, and where V𝑉V and ΣΣ\\Sigma are\ntwo of the matrices that are produced as part of the SVD\nXT=U​Σ​VTsuperscript𝑋𝑇𝑈Σsuperscript𝑉𝑇X^{T}=U\\Sigma V^{T} of XTsuperscript𝑋𝑇X^{T} (see Section 3 for details).\nForster proves that the smallest\neigenvalue of the linear operator ∑i=1nzi⊗zisuperscriptsubscript𝑖1𝑛tensor-productsubscript𝑧𝑖subscript𝑧𝑖\\sum_{i=1}^{n}z_{i}\\otimes z_{i} is\neither greater than the smallest eigenvalue of ∑i=1nxi⊗xisuperscriptsubscript𝑖1𝑛tensor-productsubscript𝑥𝑖subscript𝑥𝑖\\sum_{i=1}^{n}x_{i}\\otimes x_{i},\nor is the same but with a strictly smaller multiplicity.\nUsing this fact, Forster shows that if we iterate this step it converges\nto a set of vectors in radial isotropic position.\nHowever, no guarantee about the rate of convergence is given in 2.\nThe transformation that brings X𝑋X to (approximate)\nradial isotropic position is obtained by composing the transformations\nused in each iterative step. Garg et al. 3 also use a similar algorithm for the more\ngeneral problem of bringing a higher-rank Brascamp-Lieb datum into\ngeometric position. They bound the running time of this approach\nby a polynomial in the bit length of the input, the common denominator of the\nentries in the vector c𝑐c (which they assume are rational whose common denominator\nis not too large), and in 1/ε1𝜀1/{\\varepsilon}, where ε𝜀{\\varepsilon} is the approximation parameter. The second approach, due to Hardt and Moitra 7 (and also used by\nGurvits and Samorodnitsky 6, Singh and Vishnoi ,\nand Straszak and Vishnoi , in other related settings),\napplies an ellipsoid-based procedure for (roughly) halving the region\ncontaining the minimizing vector t∗superscript𝑡t^{*}.\nFor this, they bound the region in which t∗superscript𝑡t^{*} lies, and quantify the strong convexity of f𝑓f.\nThe resulting algorithm is polynomial in log⁡1/ε1𝜀\\log 1/{\\varepsilon}, in 1/γ1𝛾1/\\gamma\n(where γ𝛾\\gamma is another parameter that also measures (roughly) how\ndeep is c𝑐c inside the basis polytope), in L𝐿L (the bit complexity\nof the input vectors and of c𝑐c), and inversely in the minimum\nsquare determinant ΔSminsuperscriptsubscriptΔ𝑆min\\Delta_{S}^{\\rm min} of any d𝑑d-tuple S𝑆S of linearly\nindependent xisubscript𝑥𝑖x_{i}’s (as in the second bound in Theorem 1.6).\nOur results here, when plugged into Hardt and Moitra’s analysis,\nimprove their bounds considerably. Hardt and Moitra’s algorithm is\ncertainly harder to implement than those that use SVD-based gradient descent,\nlike our algorithms. Straszak and Vishnoi  prove (in their more general setting,\nof which ours is a special case) that there always exists\na vector t′superscript𝑡′t^{\\prime} such that |f​(t′)−f​(t∗)|≤ε𝑓superscript𝑡′𝑓superscript𝑡𝜀|f(t^{\\prime})-f(t^{*})|\\leq{\\varepsilon}\nand |t′|2subscriptsuperscript𝑡′2|t^{\\prime}|_{2} is polynomial in log⁡(1/ε)1𝜀\\log(1/{\\varepsilon}). This independence of\nother parameters allows us to actually strengthen our\nTheorems 1.5 and 1.6, by replacing\nthe upper bound on |t∗|∞subscriptsuperscript𝑡|t^{*}|_{\\infty} by the minimum between the actual upper bound\nthat we derive here and poly​(log⁡(1/ε))poly1𝜀{\\rm poly}(\\log(1/{\\varepsilon})) (and\nmodify the optimization region accordingly; see Equation (12)). (1) We use gradient descent for computing the minimizer t∗superscript𝑡t^{*},\ninstead of the other techniques proposed so far in the literature\n(and reviewed above). We believe\nit to be a superior technique, which is easy to implement and which\nshould run much faster in practice than the other approaches. \n (2) The connection between radial isotropy and SVD, although\nalready noted by Forster 2, is explored here in a deeper and more extended\ncontext, and is shown to be very beneficial both for the algorithms themselves and\nfor their analysis. \n (3) We offer detailed (and fairly nontrivial) analysis of several important\nparameters of the problem, such as |t∗|∞subscriptsuperscript𝑡|t^{*}|_{\\infty}, the smoothness parameter\nβ𝛽\\beta, the parameter α𝛼\\alpha of strong convexity, and more. \n (4)\nWe introduce a new notion of being “deep inside” the basis polytope, using the parameters η𝜂\\eta and δ𝛿\\delta\nof Definition 1.4. We use these parameters as an alternative to\nthe minimum square determinant\nΔSminsuperscriptsubscriptΔ𝑆min\\Delta_{S}^{\\rm min} (which was used in previous studies, and which is very\nsensitive to even a single ‘nearly dependent’ d𝑑d-tuple in X𝑋X),\nwhich makes our approach more stable with respect to any small perturbation\nof the input, and allows us to obtain better bounds for the parameters\nmentioned in (3), and thereby for the performance of gradient descent.\n (5) Last, but perhaps not least, we offer a comprehensive\ntreatment of this fascinating topic, which we believe to be helpful, given the scattered\nnature of the existing relevant literature, where the problem is discussed in widely\ndifferent contexts, using different styles of terminology, often addressed only as\na subproblem of other problems, and often receiving rather sketchy treatments,\nand suboptimal analysis of its parameters."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "On a reverse form of the Brascamp-Lieb inequality",
      "abstract": "",
      "year": "1998",
      "venue": "Inventiones Mathematicae",
      "authors": "F. Barthe"
    },
    {
      "index": 1,
      "title": "Algorithms in Real Algebraic Geometry",
      "abstract": "",
      "year": "2006",
      "venue": "Springer-Verlag",
      "authors": "S. Basu, R. Pollack, and M.-F. Roy"
    },
    {
      "index": 2,
      "title": "Foundations of Data Science",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "A. Blum, J. Hopcroft and R. Kannan"
    },
    {
      "index": 3,
      "title": "A Comprehensive Introduction to Linear Algebra",
      "abstract": "",
      "year": "1989",
      "venue": "Addison Wesley",
      "authors": "J.G. Broida and S.G. Williamson"
    },
    {
      "index": 4,
      "title": "Convex Optimization: Algorithms and Complexity",
      "abstract": "",
      "year": "2015",
      "venue": "Foundations and Trends in Machine Learning",
      "authors": "S. Bubeck",
      "orig_title": "Convex Optimization: Algorithms and Complexity",
      "paper_id": "1405.4980v2"
    },
    {
      "index": 5,
      "title": "A sharp analog of Young’s inequality on SNsuperscript𝑆𝑁S^{N} and related entropy inequalities",
      "abstract": "",
      "year": "2004",
      "venue": "J. Geometric Analysis",
      "authors": "E. Carlen, E. Lieb and M. Loss"
    },
    {
      "index": 6,
      "title": "Testing membership in matroid polyhedra.",
      "abstract": "",
      "year": "1984",
      "venue": "J. Combinat. Theory Ser. B",
      "authors": "W. Cunningham"
    },
    {
      "index": 7,
      "title": "Superquadratic lower bound for 3-query locally correctable codes over the reals",
      "abstract": "",
      "year": "2017",
      "venue": "Theory of Computing",
      "authors": "Z. Dvir, S. Saraf and A. Wigderson"
    },
    {
      "index": 8,
      "title": "Submodular functions, matroids, and certain polyhedra",
      "abstract": "",
      "year": "1970",
      "venue": "Combinatorial Structures",
      "authors": "J. Edmonds"
    },
    {
      "index": 9,
      "title": "A nearly quadratic bound for point location in hyperplane arrangements, in the linear decision tree model",
      "abstract": "",
      "year": "2018",
      "venue": "Discrete Comput. Geom.",
      "authors": "E. Ezra and M. Sharir"
    },
    {
      "index": 10,
      "title": "Decomposing arrangements of hyperplanes: VC-dimension, combinatorial dimension, and point location",
      "abstract": "",
      "year": "",
      "venue": "Discrete Comput. Geom.",
      "authors": "E. Ezra, S. Har-Peled, H. Kaplan and M. Sharir"
    },
    {
      "index": 11,
      "title": "A linear lower bound on the unbounded error probabilistic communication complexity",
      "abstract": "",
      "year": "2002",
      "venue": "J. Comput. Systems Sci.",
      "authors": "J. Forster"
    },
    {
      "index": 12,
      "title": "Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via Operator Scaling",
      "abstract": "",
      "year": "2018",
      "venue": "Geom. Funct. Anal.",
      "authors": "A. Garg, L. Gurvitz, R. Oliveira, and A. Wigderson",
      "orig_title": "Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via operator scaling",
      "paper_id": "1607.06711v4"
    },
    {
      "index": 13,
      "title": "Matrix Computations, 4th Edition",
      "abstract": "",
      "year": "2013",
      "venue": "Johns Hopkins University Press",
      "authors": "G. H. Golub and C. F. Van Loan"
    },
    {
      "index": 14,
      "title": "The ellipsoid method and its consequences in combinatorial optimization",
      "abstract": "",
      "year": "1981",
      "venue": "Combinatorica",
      "authors": "M. Grötschel, L. Lovász, and A. Schrijver"
    },
    {
      "index": 15,
      "title": "A deterministic polynomial-time algorithm for approximating mixed discriminant and mixed volume",
      "abstract": "",
      "year": "2000",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "L. Gurvits and A. Samorodnitsky"
    },
    {
      "index": 16,
      "title": "Algorithms and hardness for robust subspace recovery",
      "abstract": "",
      "year": "2013",
      "venue": "JMLR",
      "authors": "M. Hardt and A. Moitra"
    },
    {
      "index": 17,
      "title": "A combinatorial strongly polynomial time algorithm for minimizing submodular functions",
      "abstract": "",
      "year": "2001",
      "venue": "J. ACM",
      "authors": "S. Iwata, L. Fleischer, and S. Fujishige"
    },
    {
      "index": 18,
      "title": "Near-optimal linear decision trees for k-SUM and related problems",
      "abstract": "",
      "year": "2018",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "D. Kane, S. Lovett and S. Moran",
      "orig_title": "Near-optimal linear decision trees for k𝑘k-SUM and related problems",
      "paper_id": "1705.01720v1"
    },
    {
      "index": 19,
      "title": "Generalized comparison trees for point-location problems",
      "abstract": "",
      "year": "2018",
      "venue": "Internat. Colloq. Automata Languages and Programming",
      "authors": "D. Kane, S. Lovett and S. Moran",
      "orig_title": "Generalized comparison trees for point-location problems",
      "paper_id": "1804.08237v1"
    },
    {
      "index": 20,
      "title": "On nearly radial marginals of high-dimensional probability measures.",
      "abstract": "",
      "year": "2010",
      "venue": "J. Eur. Math. Soc",
      "authors": "B. Klartag"
    },
    {
      "index": 21,
      "title": "tcsmath.wordpress.com/2015/06/19/entropy-optimality-forsters-isotropy.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "J. R. Lee"
    },
    {
      "index": 22,
      "title": "Convex Analysis",
      "abstract": "",
      "year": "1970",
      "venue": "Princeton University Press",
      "authors": "R. T. Rockafellar"
    },
    {
      "index": 23,
      "title": "A combinatorial algorithm for minimizing submodular functions in strongly polynomial time",
      "abstract": "",
      "year": "2000",
      "venue": "J. Combinat. Theory Ser. B",
      "authors": "A. Schrijver"
    },
    {
      "index": 24,
      "title": "Entropy, optimization and counting",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Sympos. on Theory of Computing",
      "authors": "D. Singh and N. K. Vishnoi"
    },
    {
      "index": 25,
      "title": "Matrix Algorithms. Volume II: Eigensystems",
      "abstract": "",
      "year": "2001",
      "venue": "SIAM",
      "authors": "G. W. Stewart"
    },
    {
      "index": 26,
      "title": "Computing maximum entropy distributions everywhere",
      "abstract": "",
      "year": "",
      "venue": "arXiv",
      "authors": "D. Straszak and N. K. Vishnoi"
    },
    {
      "index": 27,
      "title": "An Introduction to Finite Tight Frames",
      "abstract": "",
      "year": "2018",
      "venue": "Birkhäuser (Springer)",
      "authors": "S. Waldron"
    }
  ]
}