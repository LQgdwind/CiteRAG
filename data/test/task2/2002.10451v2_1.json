{
  "paper_id": "2002.10451v2",
  "title": "Neural Lyapunov Model Predictive Control: Learning Safe Global Controllers from Sub-optimal Examples",
  "sections": {
    "proof of theorem 2": "We prove the following, extended version of the theorem. Stability and robustness\nAssume that Vâ€‹(x)ğ‘‰ğ‘¥V(x) satisfies (5), with Î»âˆˆ[0,1)ğœ†01\\lambda\\in[0,1), ğ•_â€‹T={0}subscriptğ•_ğ‘‡0\\mathbb{X}_{\\_}T=\\{0\\}. Then, for any horizon length Nâ‰¥1ğ‘1N\\geq 1 there exist a constant Î±Â¯â‰¥0Â¯ğ›¼0\\bar{\\alpha}\\geq 0, a minimum discount factor Î³Â¯âˆˆ(0,1]Â¯ğ›¾01\\bar{\\gamma}\\in(0,1], and a model error bound Î¼Â¯Â¯ğœ‡\\bar{\\mu} such that, if Î±â‰¥Î±Â¯ğ›¼Â¯ğ›¼\\alpha\\geq\\bar{\\alpha}, Î¼â‰¤Î¼Â¯ğœ‡Â¯ğœ‡\\mu\\leq\\bar{\\mu} and Î³â‰¥Î³Â¯ğ›¾Â¯ğ›¾\\gamma\\geq\\bar{\\gamma}, then, âˆ€xâ€‹(0)âˆˆğ’â€‹(ğ•_â€‹s)for-allğ‘¥0ğ’subscriptğ•_ğ‘ \\forall x(0)\\in\\mathcal{C}(\\mathbb{X}_{\\_}s) If N=1ğ‘1N=1, Î¼=0ğœ‡0\\mu=0, then the system is asymptotically stable for any Î³>0ğ›¾0\\gamma>0, âˆ€xâ€‹(0)âˆˆÎ¥_â€‹N,Î³,Î±for-allğ‘¥0subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If N>1ğ‘1N>1, Î¼=0ğœ‡0\\mu=0, then the system reaches a set ğ”¹_â€‹Î³subscriptğ”¹_ğ›¾\\mathbb{B}_{\\_}{\\gamma} that is included in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s. This set increases monotonically with decreasing discount factors, Î³ğ›¾\\gamma, âˆ€xâ€‹(0)âˆˆÎ¥_â€‹N,Î³,Î±for-allğ‘¥0subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. Î³=1â‡’ğ”¹_â€‹Î³={0}ğ›¾1â‡’subscriptğ”¹_ğ›¾0\\gamma=1\\Rightarrow\\mathbb{B}_{\\_}{\\gamma}=\\{0\\}. If N>1ğ‘1N>1, Î¼=0ğœ‡0\\mu=0, and once in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s we switch to the expert policy, then the system is asymptotically stable, âˆ€xâ€‹(0)âˆˆÎ¥_â€‹N,Î³,Î±for-allğ‘¥0subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) is the optimal value function for the discounted problem, Î¼=0ğœ‡0\\mu=0, and if ğ’â€‹(ğ•_â€‹s)=ğ•_â€‹sğ’subscriptğ•_ğ‘ subscriptğ•_ğ‘ \\mathcal{C}(\\mathbb{X}_{\\_}s)=\\mathbb{X}_{\\_}s, then the system is asymptotically stable, âˆ€xâ€‹(0)âˆˆÎ¥_â€‹N,Î³,Î±for-allğ‘¥0subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) is the optimal value function in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s for the problem, Î¼=0ğœ‡0\\mu=0, and if ğ’â€‹(ğ•_â€‹s)â‰ ğ•_â€‹sğ’subscriptğ•_ğ‘ subscriptğ•_ğ‘ \\mathcal{C}(\\mathbb{X}_{\\_}s)\\neq\\mathbb{X}_{\\_}s, then the system is asymptotically stable, âˆ€xâ€‹(0)âˆˆÎ¥_â€‹N,Î³,Î±for-allğ‘¥0subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. The MPC has a stability margin. If the MPC uses a surrogate model satisfying (8), with one-step error bound â€–wâ€–_â€‹22<Î¼Â¯2=1âˆ’Î»L_â€‹Vâ€‹L_â€‹f^â€‹x2â€‹Nâ€‹l_â€‹ssubscriptnormğ‘¤_superscript22superscriptÂ¯ğœ‡21ğœ†subscriptğ¿_ğ‘‰subscriptğ¿_^ğ‘“superscriptğ‘¥2ğ‘subscriptğ‘™_ğ‘ \\|w\\|_{\\_}2^{2}<\\bar{\\mu}^{2}=\\frac{1-\\lambda}{L_{\\_}VL_{\\_}{\\hat{f}x}^{2N}}l_{\\_}s, then the system is Input-to-State (practically) Stable (ISpS) and there exists a set ğ”¹_â€‹N,Î³,Î¼:xâ€‹(t)â†’ğ”¹_â€‹N,Î³,Î¼:subscriptğ”¹_ğ‘ğ›¾ğœ‡â†’ğ‘¥ğ‘¡subscriptğ”¹_ğ‘ğ›¾ğœ‡\\mathbb{B}_{\\_}{N,{\\gamma},{\\mu}}:\\ x(t)\\rightarrow\\mathbb{B}_{\\_}{N,{\\gamma},{\\mu}}, âˆ€xâ€‹(0)âˆˆÎ²â€‹Î¥_â€‹N,Î³,Î±for-allğ‘¥0ğ›½subscriptÎ¥_ğ‘ğ›¾ğ›¼\\forall x(0)\\in\\beta\\Upsilon_{\\_}{N,\\gamma,\\alpha}, Î²â‰¤1ğ›½1\\beta\\leq 1. If Î¼=0ğœ‡0\\mu=0, then Î±â‰¥Î±Â¯ğ›¼Â¯ğ›¼\\alpha\\geq\\bar{\\alpha} implies that Î±â€‹Vâ€‹(x)â‰¥Vâ‹†â€‹(x),âˆ€xâˆˆğ•_â€‹sformulae-sequenceğ›¼ğ‘‰ğ‘¥superscriptğ‘‰â‹†ğ‘¥for-allğ‘¥subscriptğ•_ğ‘ \\alpha V(x)\\geq V^{\\star}(x),\\forall x\\in\\mathbb{X}_{\\_}s, where Vâ‹†superscriptğ‘‰â‹†V^{\\star} is the optimal value function for the infinite horizon problem with cost (3) and subject to (2). In order to prove point 1 in the theorem, we first use the standard arguments for the MPC without terminal constraint  [ref]20 in the undiscounted case. We then extend the results to the discounted case. First, when an invariant set terminal constraint is used, which in our case corresponds to the condition Vâ€‹(xâ€‹(N))â‰¤l_â€‹sğ‘‰ğ‘¥ğ‘subscriptğ‘™_ğ‘ V(x(N))\\leq l_{\\_}s with ğ•_â€‹sâŠ†ğ•subscriptğ•_ğ‘ ğ•\\mathbb{X}_{\\_}s\\subseteq\\mathbb{X}, then  have provided conditions to prove stability by demonstrating that J_â‹†â€‹MPCâ€‹(x)subscriptsuperscriptğ½â‹†_MPCğ‘¥J^{\\star}_{\\_}{\\text{MPC}}(x) is a Lyapunov function. These require the terminal cost to be a Lyapunov function that satisfiesÂ EquationÂ 5. Hence, we start by looking for values of Î±ğ›¼\\alpha such that Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) satisfiesÂ EquationÂ 5. In other words, we wish to find an Î±Â¯_â€‹1â‰¥1subscriptÂ¯ğ›¼_11\\bar{\\alpha}_{\\_}1\\geq 1 such that, for all Î±â‰¥Î±Â¯_â€‹1ğ›¼subscriptÂ¯ğ›¼_1\\alpha\\geq\\bar{\\alpha}_{\\_}1 and for some policy K_â€‹0subscriptğ¾_0K_{\\_}0 (in our case, the demonstrator for Vğ‘‰V), the following condition holds: Let us denote x+=fâ€‹(x,K_â€‹0â€‹(x))superscriptğ‘¥ğ‘“ğ‘¥subscriptğ¾_0ğ‘¥x^{+}=f(x,K_{\\_}0(x)) for brevity. We have, by assumption, that: This implies that: Recall that the loss function satisfies l_â€‹â„“â€‹â€–xâ€–_â€‹22â‰¤â„“â€‹(x,u)subscriptğ‘™_â„“subscriptnormğ‘¥_superscript22â„“ğ‘¥ğ‘¢l_{\\_}\\ell\\|x\\|_{\\_}2^{2}\\leq\\ell(x,u). Since the MPC is solved using a sequence of convex quadratic programs, it is also LipschitzÂ . Similarly, if K_â€‹0subscriptğ¾_0K_{\\_}0 is Lipschitz or (uniformly) continuous over the closed and bounded set ğ•Œğ•Œ\\mathbb{U}, then since ğ•ğ•\\mathbb{X} is also closed and bounded, there also exists a local upper bound for the loss function on this policy, namely, â„“â€‹(x,K_â€‹0â€‹(x))â‰¤L_â€‹â„“â€‹â€–xâ€–_â€‹22â„“ğ‘¥subscriptğ¾_0ğ‘¥subscriptğ¿_â„“subscriptnormğ‘¥_superscript22\\ell(x,K_{\\_}0(x))\\leq L_{\\_}\\ell\\|x\\|_{\\_}2^{2}. Further, recall fromÂ EquationÂ 4 that l_â€‹â„“â€‹â€–xâ€–_â€‹22â‰¤Vâ€‹(x)subscriptğ‘™_â„“subscriptnormğ‘¥_superscript22ğ‘‰ğ‘¥l_{\\_}\\ell\\|x\\|_{\\_}2^{2}\\leq V(x). Using the above notions, we have: To satisfy the above condition, solving for a Î²â‰¥1ğ›½1\\beta\\geq 1 is sufficient. From EquationsÂ (40) andÂ (41), it implies that: Now, the function Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) satisfies all the sufficient conditions stated byÂ  for the stability of an MPC under the terminal constraint x^â€‹(N)âˆˆğ•_â€‹s^ğ‘¥ğ‘subscriptğ•_ğ‘ \\hat{x}(N)\\in\\mathbb{X}_{\\_}s which is equivalent to Vâ€‹(x^â€‹(N))â‰¤l_â€‹sğ‘‰^ğ‘¥ğ‘subscriptğ‘™_ğ‘ V(\\hat{x}(N))\\leq l_{\\_}s, without discount (with Î³=1ğ›¾1\\gamma=1). Since we do not wish to have such a terminal constraint, we wish for another lower bound Î±^_â€‹2â‰¥1subscript^ğ›¼_21\\hat{\\alpha}_{\\_}2\\geq 1 such that, if Î±â‰¥Î±Â¯_â€‹2ğ›¼subscriptÂ¯ğ›¼_2\\alpha\\geq\\bar{\\alpha}_{\\_}2, then Vâ€‹(x^â€‹(N))â‰¤l_â€‹sğ‘‰^ğ‘¥ğ‘subscriptğ‘™_ğ‘ V(\\hat{x}(N))\\leq l_{\\_}s at the optimal solution. The computation of this Î±Â¯_â€‹2subscriptÂ¯ğ›¼_2\\bar{\\alpha}_{\\_}2 has been outlined byÂ  for the undiscounted case. Since our constraints are closed, bounded and they contain the origin, our model and the MPC control law are both Lipschitz, we directly use the result from  to compute Î±Â¯_â€‹2subscriptÂ¯ğ›¼_2\\bar{\\alpha}_{\\_}2: where x~â€‹(i),u~â€‹(i)~ğ‘¥ğ‘–~ğ‘¢ğ‘–\\tilde{x}(i),\\tilde{u}(i) represent a sub-optimal state-action sequence for which Vâ€‹(x~â€‹(N))â‰¤Ïâ€‹l_â€‹sğ‘‰~ğ‘¥ğ‘ğœŒsubscriptğ‘™_ğ‘ V(\\tilde{x}(N))\\leq\\rho l_{\\_}s with Ïâˆˆ[0,1)ğœŒ01\\rho\\in[0,1), and dğ‘‘d is a lower bound for the stage loss â„“â„“\\ell for all x outside ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s and all uğ‘¢u in ğ•Œğ•Œ\\mathbb{U}. Then, one can take: to guarantee stability when Î³=1ğ›¾1\\gamma=1. When the discount factor (Î³<1ğ›¾1\\gamma<1) is used, conditionÂ (38) is still respected by the same range of Î±ğ›¼\\alpha since However, from the discussion in , for infinite horizon optimal control, it appears thatÂ EquationÂ 38 is not sufficient for J_â‹†â€‹MPCâ€‹(x)subscriptsuperscriptğ½â‹†_MPCğ‘¥J^{\\star}_{\\_}{\\text{MPC}}(x) to be a Lyapunov function, even when a terminal constraint is used. We wish to find a lower-bound Î³Â¯Â¯ğ›¾\\bar{\\gamma} such that, given Î±ğ›¼\\alpha satisfying EquationÂ 44, the MPC is stable for Î³â‰¥Î³Â¯ğ›¾Â¯ğ›¾\\gamma\\geq\\bar{\\gamma}.\nFor infinite-horizon optimal control, this was done by . First, recall that: In , it shown that 1â‰¤C<1/(1âˆ’Î³)1ğ¶11ğ›¾1\\leq C<1/(1-\\gamma) is sufficient for stability of an infinite-horizon discounted optimal control problem, when Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) is its value function. This means that: which implies that: For MPC, we will instead present an additional condition to the above one that leads to at least convergence to the safe set. This results in a bounded and safe solution. Exact convergence to the origin will be then confirmed when Vğ‘‰V is the actual value function, as in , or if we switch to the demonstrating policy, K_â€‹0subscriptğ¾_0K_{\\_}0, once in the terminal set. Finally, we will remove the terminal constraint as done for the undiscounted case with a final bound on Î±ğ›¼\\alpha and Î³ğ›¾\\gamma. Recall that condition (38) applies.\nIf the terminal constraint was met at time tğ‘¡t, namely, if Vâ€‹(xâ‹†â€‹(N))â‰¤l_â€‹sğ‘‰superscriptğ‘¥â‹†ğ‘subscriptğ‘™_ğ‘ V(x^{\\star}(N))\\leq l_{\\_}s, then at the next time step, t+1ğ‘¡1t+1 we have that uâ€‹(N+1)=K_â€‹0â€‹(xâ‹†â€‹(N))ğ‘¢ğ‘1subscriptğ¾_0superscriptğ‘¥â‹†ğ‘u(N+1)=K_{\\_}0(x^{\\star}(N)) is feasible. Hence, the optimal MPC solution can be upper-bounded by the shifted solution at the previous time tğ‘¡t, with the K_â€‹0subscriptğ¾_0K_{\\_}0 policy appended at the end of the horizon . Denote this policy as uÂ¯~~Â¯ğ‘¢\\tilde{\\underline{u}} and x~~ğ‘¥\\tilde{x} as the predictions. We have that: Hence, where\nL_â€‹Nâˆ’1â€‹(x)=âˆ‘_i=1Nâˆ’1â€‹Î³iâˆ’1â€‹â„“â€‹(x~â€‹(i),u~â€‹(i))subscriptğ¿_ğ‘1ğ‘¥subscript_ğ‘–superscript1ğ‘1superscriptğ›¾ğ‘–1â„“~ğ‘¥ğ‘–~ğ‘¢ğ‘–L_{\\_}{N-1}(x)=\\sum_{\\_}{i=1}^{N-1}\\gamma^{i-1}\\ell(\\tilde{x}(i),\\tilde{u}(i)) and we have taken Î±ğ›¼\\alpha such that Î³â€‹Î±â‰¥Î±Â¯_â€‹1ğ›¾ğ›¼subscriptÂ¯ğ›¼_1\\gamma\\alpha\\geq\\bar{\\alpha}_{\\_}1. Now, for Î³=1ğ›¾1\\gamma=1, the effect of L_â€‹Nâˆ’1subscriptğ¿_ğ‘1L_{\\_}{N-1} disappears and the MPC optimal cost is a Lyapunov function as in the standard MPC stability result from . By inspection of L_â€‹Nâˆ’1subscriptğ¿_ğ‘1L_{\\_}{N-1}, since the cost is bounded over bounded sets, also a small enough Î³ğ›¾\\gamma could be found such that L_â€‹Nâˆ’1â€‹(x)<â„“â€‹(x,u~â€‹(0))subscriptğ¿_ğ‘1ğ‘¥â„“ğ‘¥~ğ‘¢0L_{\\_}{N-1}(x)<\\ell(x,\\tilde{u}(0)). This Î³ğ›¾\\gamma, however, depends on xğ‘¥x. Consider xâˆ‰ğ•_â€‹sğ‘¥subscriptğ•_ğ‘ x\\not\\in\\mathbb{X}_{\\_}s, for which there exist a feasible solution, namely a solution providing x~â€‹(N)âˆˆğ•_â€‹s~ğ‘¥ğ‘subscriptğ•_ğ‘ \\tilde{x}(N)\\in\\mathbb{X}_{\\_}s. Then, since â„“â„“\\ell is strictly increasing, â„“â€‹(0,0)=0â„“000\\ell(0,0)=0, ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s contains the origin and the constraints are bounded, we have that there exist a Ï…â‰¥1ğœ1\\upsilon\\geq 1 such that for any feasbile xğ‘¥x: is an upper bound for L_â€‹Nâˆ’1â€‹(x)subscriptğ¿_ğ‘1ğ‘¥L_{\\_}{N-1}(x). For instance, is sufficient for any closed set of initial conditions xâ€‹(0)âˆˆÏµâ€‹ğ•âŠƒğ•_â€‹sğ‘¥0italic-Ïµğ•superset-ofsubscriptğ•_ğ‘ x(0)\\in\\epsilon\\mathbb{X}\\supset\\mathbb{X}_{\\_}s, with Ïµ>0italic-Ïµ0\\epsilon>0.\nIn order to have stability, it suffices to have (1âˆ’Î³)â€‹LÂ¯_â€‹Nâˆ’1âˆ’â„“â€‹(x,u~â€‹(0))â‰¤01ğ›¾subscriptÂ¯ğ¿_ğ‘1â„“ğ‘¥~ğ‘¢00(1-\\gamma)\\bar{L}_{\\_}{N-1}-\\ell(x,\\tilde{u}(0))\\leq 0 which requires: In the above condition Î³Â¯â€‹(x)Â¯ğ›¾ğ‘¥\\bar{\\gamma}(x) can be less than 111 only outside a neighborhood of origin. Consider again Then taking provides that the system trajectory will enter the safe set ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s, hence ğ”¹_â€‹Î³âŠ†ğ•_â€‹ssubscriptğ”¹_ğ›¾subscriptğ•_ğ‘ \\mathbb{B}_{\\_}\\gamma\\subseteq\\mathbb{X}_{\\_}s.\nFinally, once xâˆˆğ•_â€‹sğ‘¥subscriptğ•_ğ‘ x\\in\\mathbb{X}_{\\_}s, we that\nthe policy K_â€‹0â€‹(x)subscriptğ¾_0ğ‘¥K_{\\_}0(x) is feasible and: Hence, we can use this policy to upper bound the MPC cost: If the above is true with equality, then we can proceed as in Theorem 3.1 of , with Î³>Î³Â¯_â€‹1ğ›¾subscriptÂ¯ğ›¾_1\\gamma>\\bar{\\gamma}_{\\_}1. This would require Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) to be also a value function for the discounted problem. From the above considerations, we can conclude that that: If N=1ğ‘1N=1, then LÂ¯_â€‹Nâˆ’1=0subscriptÂ¯ğ¿_ğ‘10\\bar{L}_{\\_}{N-1}=0 and the system is asymptotically stable for any Î³>0ğ›¾0\\gamma>0. If N>1ğ‘1N>1, Î³â‰¥Î³_â€‹2Â¯ğ›¾Â¯subscriptğ›¾_2\\gamma\\geq\\bar{\\gamma_{\\_}2}, then the system reaches an bound ğ”¹_â€‹Î³subscriptğ”¹_ğ›¾\\mathbb{B}_{\\_}\\gamma that is included in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s. If N>1ğ‘1N>1 Î³â‰¥Î³_â€‹2Â¯ğ›¾Â¯subscriptğ›¾_2\\gamma\\geq\\bar{\\gamma_{\\_}2} and once in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s we switch to the policy K_â€‹0â€‹(x)subscriptğ¾_0ğ‘¥K_{\\_}0(x) then the system is asymptotically stable. If Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) is the global value function for the discounted problem and if â„›â€‹(ğ•_â€‹s)=ğ•_â€‹sâ„›subscriptğ•_ğ‘ subscriptğ•_ğ‘ \\mathcal{R}(\\mathbb{X}_{\\_}s)=\\mathbb{X}_{\\_}s, then Î³>Î³Â¯_â€‹1ğ›¾subscriptÂ¯ğ›¾_1\\gamma>\\bar{\\gamma}_{\\_}1 provides that the system is Asymptotically stable. If Î±â€‹Vâ€‹(x)ğ›¼ğ‘‰ğ‘¥\\alpha V(x) is only the value function in ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s for the discounted problem, and if â„›â€‹(ğ•_â€‹s)â‰ ğ•_â€‹sâ„›subscriptğ•_ğ‘ subscriptğ•_ğ‘ \\mathcal{R}(\\mathbb{X}_{\\_}s)\\neq\\mathbb{X}_{\\_}s, then Î³>maxâ¡(Î³Â¯_â€‹1,Î³_â€‹2Â¯)ğ›¾subscriptÂ¯ğ›¾_1Â¯subscriptğ›¾_2\\gamma>\\max(\\bar{\\gamma}_{\\_}1,\\bar{\\gamma_{\\_}2}) provides that the system is Asymptotically stable. Finally, following Theorem 3 from [ref]20, the terminal constraint can be removed for all points xâˆˆâ„›Nâ€‹(Ïâ€‹ğ•_â€‹s)ğ‘¥superscriptâ„›ğ‘ğœŒsubscriptğ•_ğ‘ x\\in\\mathcal{R}^{N}(\\rho\\mathbb{X}_{\\_}s), with Ïâˆˆ[0,1)ğœŒ01\\rho\\in[0,1), by setting: In fact, by the same argument of [ref]20, for any states for which it exists a feasible sequence uÂ¯~~Â¯ğ‘¢\\tilde{\\underline{u}} taking x^â€‹(N)^ğ‘¥ğ‘\\hat{x}(N) to Ïâ€‹ğ•_â€‹sğœŒsubscriptğ•_ğ‘ \\rho\\mathbb{X}_{\\_}s we have that: If Î±ğ›¼\\alpha satisfies (52), then we also have that: from which we can directly verify that the set defined in (III-B) is a ROA (for either asymptotic or practical stability): For point 6, the stability margins of nominal MPC have been studied in . In particular, in a setup without terminal constraint, under nominal stabilising conditions, with a uniformly continuous model (in our case even Lipschitz), cost functions being also uniformly continuous, then the optimal MPC cost is also uniformly continuous [24, Proposition 1]. In other words, from [24, Theorem 1], there is a ğ’¦_â€‹âˆsubscriptğ’¦_\\mathcal{K}_{\\_}\\infty-function, Ïƒğœ\\sigma, such that at the optimal solution, uÂ¯â‹†superscriptÂ¯ğ‘¢â‹†\\underline{u}^{\\star}, we have: Using the above identity, one wish to bound the increase in the MPC cost due to uncertainty. At the same time, we wish the MPC to remain feasible and perform a contraction, namely, to have a stability margin. Since we are using soft constraints, then the MPC remains always feasible, however, we need the predictions at the end of the horizon to be in an invariant set ğ•_â€‹ssubscriptğ•_ğ‘ \\mathbb{X}_{\\_}s even under the effect of uncertainty. In particular, we will use Vâ€‹(x)ğ‘‰ğ‘¥V(x) and its contraction factor Î»ğœ†\\lambda to compute a smaller level set Î¶â€‹ğ•_â€‹sğœsubscriptğ•_ğ‘ \\zeta\\mathbb{X}_{\\_}s, for some Î¶âˆˆ(0,1)ğœ01\\zeta\\in(0,1) which is invariant under the uncertainty. Once this is found then we can compute a new Î±ğ›¼\\alpha for this set according to (52).\nIn particular, under the policy K_â€‹0subscriptğ¾_0K_{\\_}0, we have that: We wish this quantity to be non-positive for xâˆ‰Î¶â€‹ğ•_â€‹sğ‘¥ğœsubscriptğ•_ğ‘ x\\not\\in\\zeta\\mathbb{X}_{\\_}s, which means that Vâ€‹(x)â‰¥Î¶â€‹l_â€‹sğ‘‰ğ‘¥ğœsubscriptğ‘™_ğ‘ V(x)\\geq\\zeta l_{\\_}s. For this it is sufficient to have: Since we apply the function Vğ‘‰V to the prediction at time Nğ‘N, and at the next step the measured state (prediction at index 00) differs from the previous MPC prediction of a quantity wğ‘¤w, for the MPC this condition translates into: Therefore, given the model error wğ‘¤w, if the MPC remains feasible and if Î±ğ›¼\\alpha and Î³ğ›¾\\gamma exceed their lower bounds given the restricted set Î¶â€‹ğ•_â€‹sğœsubscriptğ•_ğ‘ \\zeta\\mathbb{X}_{\\_}s, we have that â€–wâ€–_â€‹2<Î¼Â¯subscriptnormğ‘¤_2Â¯ğœ‡\\|w\\|_{\\_}2<\\bar{\\mu} implies that: which is the definition of Input-to-State practical Stability  , where we have defined dÂ¯â€‹(N)=(1âˆ’Î³)â€‹LÂ¯_â€‹Nâˆ’1Â¯ğ‘‘ğ‘1ğ›¾subscriptÂ¯ğ¿_ğ‘1\\bar{d}(N)=(1-\\gamma)\\bar{L}_{\\_}{N-1}. The trajectory of the system is therefore bounded by the level-set of J_â‹†â€‹Mâ€‹Pâ€‹Câ€‹(x)subscriptsuperscriptğ½â‹†_ğ‘€ğ‘ƒğ¶ğ‘¥J^{\\star}_{\\_}{MPC}(x) outside which Ïƒâ€‹(Î¼)+dÂ¯â€‹(N)â‰¤l_â€‹â„“â€‹â€–xâ€–_â€‹22.ğœğœ‡Â¯ğ‘‘ğ‘subscriptğ‘™_â„“subscriptnormğ‘¥_superscript22\\sigma(\\mu)+\\bar{d}(N)\\leq l_{\\_}\\ell\\|x\\|_{\\_}2^{2}.\nSince Ïƒğœ\\sigma is strictly increasing and dÂ¯Â¯ğ‘‘\\bar{d} is strictly decreasing, we can also conclude that the size of this bound increases with increasing model error Î¼ğœ‡\\mu and with the horizon length Nğ‘N. Note that the term dÂ¯Â¯ğ‘‘\\bar{d} vanishes if Î³=1ğ›¾1\\gamma=1 but the term Ïƒğœ\\sigma will also increase with Nğ‘N if LÂ¯_â€‹f>1subscriptÂ¯ğ¿_ğ‘“1\\bar{L}_{\\_}f>1. From the restriction of the terminal set, it also follows that the ROA defined in EquationÂ III-B will also be restricted unless we recompute a larger Î±ğ›¼\\alpha for this new set. Denote V^â€‹(x)=Î±â€‹Vâ€‹(x)^ğ‘‰ğ‘¥ğ›¼ğ‘‰ğ‘¥\\hat{V}(x)=\\alpha V(x). Recall that, if Î±â‰¥Î±_â€‹1ğ›¼subscriptğ›¼_1\\alpha\\geq\\alpha_{\\_}1, as defined in EquationÂ 42, and if â€–wâ€‹(t)â€–=0â€‹âˆ€tnormğ‘¤ğ‘¡0for-allğ‘¡\\|w(t)\\|=0\\ \\forall t, then we have that, âˆ€xâ€‹(t)âˆˆğ•_â€‹sfor-allğ‘¥ğ‘¡subscriptğ•_ğ‘ \\forall x(t)\\in\\mathbb{X}_{\\_}s: which in turn implies that, by means of Î³â‰¤1ğ›¾1\\gamma\\leq 1 and of induction: which is the value of the policy K_â€‹0subscriptğ¾_0K_{\\_}0. This is, by definition, an upper bound of the optimal value function, Vâ‹†â€‹(x)superscriptğ‘‰â‹†ğ‘¥V^{\\star}(x). Hence Î±â€‹Vâ€‹(x)â‰¥Vâ‹†â€‹(x),âˆ€xâˆˆğ•_â€‹sformulae-sequenceğ›¼ğ‘‰ğ‘¥superscriptğ‘‰â‹†ğ‘¥for-allğ‘¥subscriptğ•_ğ‘ \\alpha V(x)\\geq V^{\\star}(x),\\forall x\\in\\mathbb{X}_{\\_}s. âˆ In practice, the ISS bound Ïƒâ€‹(Î¼)ğœğœ‡\\sigma(\\mu) from Theorem 2 has a form similar to the one discussed for the constraint penalty in the proof of Theorem 3, seeÂ EquationÂ 37. Its explicit computation is omitted for brevity; however, in general, we can expect the bound to become worse for systems that are open-loop unstable as the horizon length increases."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.06565",
      "authors": "D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. ManÃ©",
      "orig_title": "Concrete problems in ai safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 1,
      "title": "Model-based reinforcement learning: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.16712",
      "authors": "T. M. Moerland, J. Broekens, and C. M. Jonker"
    },
    {
      "index": 2,
      "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.01848",
      "authors": "K. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch",
      "orig_title": "Plan online, learn offline: Efficient learning and exploration via model-based control",
      "paper_id": "1811.01848v3"
    },
    {
      "index": 3,
      "title": "Nonlinear Control",
      "abstract": "",
      "year": "2014",
      "venue": "Pearson",
      "authors": "H. K. Khalil"
    },
    {
      "index": 4,
      "title": "Sampling driven stability domains computation and predictive control of constrained nonlinear systems",
      "abstract": "",
      "year": "2017",
      "venue": "Ph.D. dissertation",
      "authors": "R. V. Bobiti"
    },
    {
      "index": 5,
      "title": "Samplingâ€“based verification of Lyapunovâ€™s inequality for piecewise continuous nonlinear systems",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.00302",
      "authors": "R. Bobiti and M. Lazar",
      "orig_title": "Sampling-based verification of lyapunovâ€™s inequality for piecewise continuous nonlinear systems",
      "paper_id": "1609.00302v1"
    },
    {
      "index": 6,
      "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.08551",
      "authors": "F. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause",
      "orig_title": "Safe model-based reinforcement learning with stability guarantees",
      "paper_id": "1705.08551v3"
    },
    {
      "index": 7,
      "title": "Safe Interactive Model-Based Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.06556",
      "authors": "M. Gallieri, S. S. M. Salehian, N. E. Toklu, A. Quaglino, J. Masci, J. KoutnÃ­k, and F. Gomez",
      "orig_title": "Safe interactive model-based learning",
      "paper_id": "1911.06556v2"
    },
    {
      "index": 8,
      "title": "Neural Lyapunov Control",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00611",
      "authors": "Y.-C. Chang, N. Roohi, and S. Gao",
      "orig_title": "Neural lyapunov control",
      "paper_id": "2005.00611v4"
    },
    {
      "index": 9,
      "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.00177",
      "authors": "X. B. Peng, A. Kumar, G. Zhang, and S. Levine",
      "orig_title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
      "paper_id": "1910.00177v3"
    },
    {
      "index": 10,
      "title": "Set-Theoretic Methods in Control (Systems & Control: Foundations & Applications)",
      "abstract": "",
      "year": "2007",
      "venue": "BirkhÃ¤user",
      "authors": "F. Blanchini and S. Miani"
    },
    {
      "index": 11,
      "title": "Robust constraint satisfaction: Invariant sets and predictive control",
      "abstract": "",
      "year": "2000",
      "venue": "Ph.D. dissertation",
      "authors": "E. Kerrigan"
    },
    {
      "index": 12,
      "title": "Formal Synthesis of Lyapunov Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Control Systems Letters",
      "authors": "A. Abate, D. Ahmed, M. Giacobbe, and A. Peruffo",
      "orig_title": "Formal synthesis of lyapunov neural networks",
      "paper_id": "2003.08910v2"
    },
    {
      "index": 13,
      "title": "dreach: Î´ğ›¿\\delta-reachability analysis for hybrid systems",
      "abstract": "",
      "year": "2015",
      "venue": "Tools and Algorithms for the Construction and Analysis of Systems",
      "authors": "S. Kong, S. Gao, W. Chen, and E. Clarke"
    },
    {
      "index": 14,
      "title": "Echo State Networks: analysis, training and predictive control",
      "abstract": "",
      "year": "2019",
      "venue": "2019 18th European Control Conference (ECC)",
      "authors": "L. B. Armenio, E. Terzi, M. Farina, and R. Scattolini",
      "orig_title": "Echo state networks: analysis, training and predictive control",
      "paper_id": "1902.01618v1"
    },
    {
      "index": 15,
      "title": "Physics-informed echo state networks for chaotic systems forecasting",
      "abstract": "",
      "year": "2019",
      "venue": "Lecture Notes in Computer Science",
      "authors": "N. A. K. Doan, W. Polifke, and L. Magri"
    },
    {
      "index": 16,
      "title": "Using machine learning to replicate chaotic attractors and calculate lyapunov exponents from data",
      "abstract": "",
      "year": "2017",
      "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science",
      "authors": "J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott"
    },
    {
      "index": 17,
      "title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1804.07209 [cs, stat]",
      "authors": "M. Ciccone, M. Gallieri, J. Masci, C. Osendorfer, and F. Gomez",
      "orig_title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations",
      "paper_id": "1804.07209v4"
    },
    {
      "index": 18,
      "title": "Constrained model predictive control: Stability and optimality",
      "abstract": "",
      "year": "2000",
      "venue": "Automatica",
      "authors": "D. Mayne, J. Rawlings, C. Rao, and P. Scokaert"
    },
    {
      "index": 19,
      "title": "Stable constrained MPC without terminal constraint",
      "abstract": "",
      "year": "2003",
      "venue": "American Control Conference",
      "authors": "D. Limon, T. Alamo, and E. Camacho"
    },
    {
      "index": 20,
      "title": "The Lyapunov Neural Network: Adaptive Stability Certification for Safe Learning of Dynamical Systems",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Robot Learning",
      "authors": "S. M. Richards, F. Berkenkamp, and A. Krause",
      "orig_title": "The lyapunov neural network: Adaptive stability certification for safe learning of dynamical systems",
      "paper_id": "1808.00924v2"
    },
    {
      "index": 21,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 22,
      "title": "Soft constraints and exact penalty functions in model predictive control",
      "abstract": "",
      "year": "2000",
      "venue": "UKACC International Conference",
      "authors": "E. C. Kerrigan and J. M. Maciejowski"
    },
    {
      "index": 23,
      "title": "Input-to-State Stability: A Unifying Framework for Robust Model Predictive Control",
      "abstract": "",
      "year": "2009",
      "venue": "Nonlinear Model Predictive Control",
      "authors": "D. Limon, T. Alamo, D. M. Raimondo, D. M. de la PeÃ±a, J. M. Bravo, A. Ferramosca, and E. F. Camacho"
    },
    {
      "index": 24,
      "title": "LASSO-MPC â€“ Predictive Control with â„“_â€‹1subscriptâ„“_1\\ell_{\\_}1-Regularised Least Squares",
      "abstract": "",
      "year": "2016",
      "venue": "Springer-Verlag",
      "authors": "M. Gallieri"
    },
    {
      "index": 25,
      "title": "Lipschitz Continuity in Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "K. Asadi, D. Misra, and M. Littman",
      "orig_title": "Lipschitz continuity in model-based reinforcement learning",
      "paper_id": "1804.07193v3"
    },
    {
      "index": 26,
      "title": "Differentiable convex optimization layers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.12430",
      "authors": "A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter"
    },
    {
      "index": 27,
      "title": "Differentiable MPC for End-to-end Planning and Control",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.13400",
      "authors": "B. Amos, I. D. J Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter",
      "orig_title": "Differentiable mpc for end-to-end planning and control",
      "paper_id": "1810.13400v3"
    },
    {
      "index": 28,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 29,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.10528",
      "authors": "J. Achiam, D. Held, A. Tamar, and P. Abbeel",
      "orig_title": "Constrained policy optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 30,
      "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "A. Ray, J. Achiam, and D. Amodei"
    },
    {
      "index": 31,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 32,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.01290",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 33,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Janner, J. Fu, M. Zhang, and S. Levine"
    },
    {
      "index": 34,
      "title": "Model Predictive Control Theory and Design",
      "abstract": "",
      "year": "2009",
      "venue": "Nob Hill Pub, Llc",
      "authors": "J. B. Rawlings and D. Q. Mayne"
    },
    {
      "index": 35,
      "title": "Homothetic tube model predictive control",
      "abstract": "",
      "year": "2012",
      "venue": "Automatica",
      "authors": "S. V. RakoviÄ‡, B. Kouvaritakis, R. Findeisen, and M. Cannon"
    },
    {
      "index": 36,
      "title": "Stabilization with discounted optimal control",
      "abstract": "",
      "year": "2015",
      "venue": "Systems & Control Letters",
      "authors": "V. Gaitsgory, L. GrÃ¼ne, and N. Thatcher"
    },
    {
      "index": 37,
      "title": "Synthesis and stabilization of complex behaviors through online trajectory optimization",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Y. Tassa, T. Erez, and E. Todorov"
    },
    {
      "index": 38,
      "title": "Numerical optimization",
      "abstract": "",
      "year": "2006",
      "venue": "Springer Science & Business Media",
      "authors": "J. Nocedal and S. Wright"
    },
    {
      "index": 39,
      "title": "SNODE: Spectral Discretization of Neural ODEs for System Identification",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:1906.07038 [cs]",
      "authors": "A. Quaglino, M. Gallieri, J. Masci, and J. KoutnÃ­k"
    },
    {
      "index": 40,
      "title": "ODE2VAE: Deep generative second order ODEs with Bayesian neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1905.10994 [cs, stat]",
      "authors": "C. YÄ±ldÄ±z, M. Heinonen, and H. LÃ¤hdesmÃ¤ki"
    },
    {
      "index": 41,
      "title": "Tustin neural networks: a class of recurrent nets for adaptive MPC of mechanical systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "S. Pozzoli, M. Gallieri, and R. Scattolini",
      "orig_title": "Tustin neural networks: a class of recurrent nets for adaptive MPC of mechanical systems",
      "paper_id": "1911.01310v1"
    },
    {
      "index": 42,
      "title": "Lvis: learning from value function intervals for contact-aware robot controllers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "R. Deits, T. Koolen, and R. Tedrake"
    },
    {
      "index": 43,
      "title": "Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.01675",
      "authors": "J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee",
      "orig_title": "Sample-efficient reinforcement learning with stochastic ensemble value expansion",
      "paper_id": "1807.01675v2"
    },
    {
      "index": 44,
      "title": "On the infinite horizon performance of receding horizon controllers",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "L. Grune and A. Rantzer"
    },
    {
      "index": 45,
      "title": "Learning-based Model Predictive Control for Safe Exploration",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE Conference on Decision and Control (CDC)",
      "authors": "T. Koller, F. Berkenkamp, M. Turchetta, and A. Krause",
      "orig_title": "Learning-based model predictive control for safe exploration",
      "paper_id": "1803.08287v3"
    },
    {
      "index": 46,
      "title": "Learning-based model predictive control: Toward safe learning in control",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Review of Control, Robotics, and Autonomous Systems",
      "authors": "L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger"
    },
    {
      "index": 47,
      "title": "Lyapunov Criterion for Stochastic Systems and Its Applications in Distributed Computation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Y. Qin, M. Cao, and B. D. Anderson",
      "orig_title": "Lyapunov criterion for stochastic systems and its applications in distributed computation",
      "paper_id": "1902.04332v1"
    },
    {
      "index": 48,
      "title": "The explicit solution of model predictive control via multiparametric quadratic programming",
      "abstract": "",
      "year": "2000",
      "venue": "American Control Conference",
      "authors": "A. Bemporad, M. Morari, V. Dua, and E. Pistikopoulos"
    },
    {
      "index": 49,
      "title": "Sequential quadratic programming for task plan optimization",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "D. Hadfield-Menell, C. Lin, R. Chitnis, S. Russell, and P. Abbeel"
    },
    {
      "index": 50,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01703",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 51,
      "title": "Handbook of marine craft hydrodynamics and motion control",
      "abstract": "",
      "year": "2011",
      "venue": "John Wiley & Sons",
      "authors": "T. I. Fossen"
    },
    {
      "index": 52,
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS)",
      "authors": "X. Glorot and Y. Bengio"
    }
  ]
}