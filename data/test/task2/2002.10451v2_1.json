{
  "paper_id": "2002.10451v2",
  "title": "Neural Lyapunov Model Predictive Control: Learning Safe Global Controllers from Sub-optimal Examples",
  "sections": {
    "proof of theorem 2": "We prove the following, extended version of the theorem. Stability and robustness\nAssume that V​(x)𝑉𝑥V(x) satisfies (5), with λ∈[0,1)𝜆01\\lambda\\in[0,1), 𝕏_​T={0}subscript𝕏_𝑇0\\mathbb{X}_{\\_}T=\\{0\\}. Then, for any horizon length N≥1𝑁1N\\geq 1 there exist a constant α¯≥0¯𝛼0\\bar{\\alpha}\\geq 0, a minimum discount factor γ¯∈(0,1]¯𝛾01\\bar{\\gamma}\\in(0,1], and a model error bound μ¯¯𝜇\\bar{\\mu} such that, if α≥α¯𝛼¯𝛼\\alpha\\geq\\bar{\\alpha}, μ≤μ¯𝜇¯𝜇\\mu\\leq\\bar{\\mu} and γ≥γ¯𝛾¯𝛾\\gamma\\geq\\bar{\\gamma}, then, ∀x​(0)∈𝒞​(𝕏_​s)for-all𝑥0𝒞subscript𝕏_𝑠\\forall x(0)\\in\\mathcal{C}(\\mathbb{X}_{\\_}s) If N=1𝑁1N=1, μ=0𝜇0\\mu=0, then the system is asymptotically stable for any γ>0𝛾0\\gamma>0, ∀x​(0)∈Υ_​N,γ,αfor-all𝑥0subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If N>1𝑁1N>1, μ=0𝜇0\\mu=0, then the system reaches a set 𝔹_​γsubscript𝔹_𝛾\\mathbb{B}_{\\_}{\\gamma} that is included in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s. This set increases monotonically with decreasing discount factors, γ𝛾\\gamma, ∀x​(0)∈Υ_​N,γ,αfor-all𝑥0subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. γ=1⇒𝔹_​γ={0}𝛾1⇒subscript𝔹_𝛾0\\gamma=1\\Rightarrow\\mathbb{B}_{\\_}{\\gamma}=\\{0\\}. If N>1𝑁1N>1, μ=0𝜇0\\mu=0, and once in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s we switch to the expert policy, then the system is asymptotically stable, ∀x​(0)∈Υ_​N,γ,αfor-all𝑥0subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If α​V​(x)𝛼𝑉𝑥\\alpha V(x) is the optimal value function for the discounted problem, μ=0𝜇0\\mu=0, and if 𝒞​(𝕏_​s)=𝕏_​s𝒞subscript𝕏_𝑠subscript𝕏_𝑠\\mathcal{C}(\\mathbb{X}_{\\_}s)=\\mathbb{X}_{\\_}s, then the system is asymptotically stable, ∀x​(0)∈Υ_​N,γ,αfor-all𝑥0subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. If α​V​(x)𝛼𝑉𝑥\\alpha V(x) is the optimal value function in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s for the problem, μ=0𝜇0\\mu=0, and if 𝒞​(𝕏_​s)≠𝕏_​s𝒞subscript𝕏_𝑠subscript𝕏_𝑠\\mathcal{C}(\\mathbb{X}_{\\_}s)\\neq\\mathbb{X}_{\\_}s, then the system is asymptotically stable, ∀x​(0)∈Υ_​N,γ,αfor-all𝑥0subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\Upsilon_{\\_}{N,\\gamma,\\alpha}. The MPC has a stability margin. If the MPC uses a surrogate model satisfying (8), with one-step error bound ‖w‖_​22<μ¯2=1−λL_​V​L_​f^​x2​N​l_​ssubscriptnorm𝑤_superscript22superscript¯𝜇21𝜆subscript𝐿_𝑉subscript𝐿_^𝑓superscript𝑥2𝑁subscript𝑙_𝑠\\|w\\|_{\\_}2^{2}<\\bar{\\mu}^{2}=\\frac{1-\\lambda}{L_{\\_}VL_{\\_}{\\hat{f}x}^{2N}}l_{\\_}s, then the system is Input-to-State (practically) Stable (ISpS) and there exists a set 𝔹_​N,γ,μ:x​(t)→𝔹_​N,γ,μ:subscript𝔹_𝑁𝛾𝜇→𝑥𝑡subscript𝔹_𝑁𝛾𝜇\\mathbb{B}_{\\_}{N,{\\gamma},{\\mu}}:\\ x(t)\\rightarrow\\mathbb{B}_{\\_}{N,{\\gamma},{\\mu}}, ∀x​(0)∈β​Υ_​N,γ,αfor-all𝑥0𝛽subscriptΥ_𝑁𝛾𝛼\\forall x(0)\\in\\beta\\Upsilon_{\\_}{N,\\gamma,\\alpha}, β≤1𝛽1\\beta\\leq 1. If μ=0𝜇0\\mu=0, then α≥α¯𝛼¯𝛼\\alpha\\geq\\bar{\\alpha} implies that α​V​(x)≥V⋆​(x),∀x∈𝕏_​sformulae-sequence𝛼𝑉𝑥superscript𝑉⋆𝑥for-all𝑥subscript𝕏_𝑠\\alpha V(x)\\geq V^{\\star}(x),\\forall x\\in\\mathbb{X}_{\\_}s, where V⋆superscript𝑉⋆V^{\\star} is the optimal value function for the infinite horizon problem with cost (3) and subject to (2). In order to prove point 1 in the theorem, we first use the standard arguments for the MPC without terminal constraint  [ref]20 in the undiscounted case. We then extend the results to the discounted case. First, when an invariant set terminal constraint is used, which in our case corresponds to the condition V​(x​(N))≤l_​s𝑉𝑥𝑁subscript𝑙_𝑠V(x(N))\\leq l_{\\_}s with 𝕏_​s⊆𝕏subscript𝕏_𝑠𝕏\\mathbb{X}_{\\_}s\\subseteq\\mathbb{X}, then  have provided conditions to prove stability by demonstrating that J_⋆​MPC​(x)subscriptsuperscript𝐽⋆_MPC𝑥J^{\\star}_{\\_}{\\text{MPC}}(x) is a Lyapunov function. These require the terminal cost to be a Lyapunov function that satisfies Equation 5. Hence, we start by looking for values of α𝛼\\alpha such that α​V​(x)𝛼𝑉𝑥\\alpha V(x) satisfies Equation 5. In other words, we wish to find an α¯_​1≥1subscript¯𝛼_11\\bar{\\alpha}_{\\_}1\\geq 1 such that, for all α≥α¯_​1𝛼subscript¯𝛼_1\\alpha\\geq\\bar{\\alpha}_{\\_}1 and for some policy K_​0subscript𝐾_0K_{\\_}0 (in our case, the demonstrator for V𝑉V), the following condition holds: Let us denote x+=f​(x,K_​0​(x))superscript𝑥𝑓𝑥subscript𝐾_0𝑥x^{+}=f(x,K_{\\_}0(x)) for brevity. We have, by assumption, that: This implies that: Recall that the loss function satisfies l_​ℓ​‖x‖_​22≤ℓ​(x,u)subscript𝑙_ℓsubscriptnorm𝑥_superscript22ℓ𝑥𝑢l_{\\_}\\ell\\|x\\|_{\\_}2^{2}\\leq\\ell(x,u). Since the MPC is solved using a sequence of convex quadratic programs, it is also Lipschitz . Similarly, if K_​0subscript𝐾_0K_{\\_}0 is Lipschitz or (uniformly) continuous over the closed and bounded set 𝕌𝕌\\mathbb{U}, then since 𝕏𝕏\\mathbb{X} is also closed and bounded, there also exists a local upper bound for the loss function on this policy, namely, ℓ​(x,K_​0​(x))≤L_​ℓ​‖x‖_​22ℓ𝑥subscript𝐾_0𝑥subscript𝐿_ℓsubscriptnorm𝑥_superscript22\\ell(x,K_{\\_}0(x))\\leq L_{\\_}\\ell\\|x\\|_{\\_}2^{2}. Further, recall from Equation 4 that l_​ℓ​‖x‖_​22≤V​(x)subscript𝑙_ℓsubscriptnorm𝑥_superscript22𝑉𝑥l_{\\_}\\ell\\|x\\|_{\\_}2^{2}\\leq V(x). Using the above notions, we have: To satisfy the above condition, solving for a β≥1𝛽1\\beta\\geq 1 is sufficient. From Equations (40) and (41), it implies that: Now, the function α​V​(x)𝛼𝑉𝑥\\alpha V(x) satisfies all the sufficient conditions stated by  for the stability of an MPC under the terminal constraint x^​(N)∈𝕏_​s^𝑥𝑁subscript𝕏_𝑠\\hat{x}(N)\\in\\mathbb{X}_{\\_}s which is equivalent to V​(x^​(N))≤l_​s𝑉^𝑥𝑁subscript𝑙_𝑠V(\\hat{x}(N))\\leq l_{\\_}s, without discount (with γ=1𝛾1\\gamma=1). Since we do not wish to have such a terminal constraint, we wish for another lower bound α^_​2≥1subscript^𝛼_21\\hat{\\alpha}_{\\_}2\\geq 1 such that, if α≥α¯_​2𝛼subscript¯𝛼_2\\alpha\\geq\\bar{\\alpha}_{\\_}2, then V​(x^​(N))≤l_​s𝑉^𝑥𝑁subscript𝑙_𝑠V(\\hat{x}(N))\\leq l_{\\_}s at the optimal solution. The computation of this α¯_​2subscript¯𝛼_2\\bar{\\alpha}_{\\_}2 has been outlined by  for the undiscounted case. Since our constraints are closed, bounded and they contain the origin, our model and the MPC control law are both Lipschitz, we directly use the result from  to compute α¯_​2subscript¯𝛼_2\\bar{\\alpha}_{\\_}2: where x~​(i),u~​(i)~𝑥𝑖~𝑢𝑖\\tilde{x}(i),\\tilde{u}(i) represent a sub-optimal state-action sequence for which V​(x~​(N))≤ρ​l_​s𝑉~𝑥𝑁𝜌subscript𝑙_𝑠V(\\tilde{x}(N))\\leq\\rho l_{\\_}s with ρ∈[0,1)𝜌01\\rho\\in[0,1), and d𝑑d is a lower bound for the stage loss ℓℓ\\ell for all x outside 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s and all u𝑢u in 𝕌𝕌\\mathbb{U}. Then, one can take: to guarantee stability when γ=1𝛾1\\gamma=1. When the discount factor (γ<1𝛾1\\gamma<1) is used, condition (38) is still respected by the same range of α𝛼\\alpha since However, from the discussion in , for infinite horizon optimal control, it appears that Equation 38 is not sufficient for J_⋆​MPC​(x)subscriptsuperscript𝐽⋆_MPC𝑥J^{\\star}_{\\_}{\\text{MPC}}(x) to be a Lyapunov function, even when a terminal constraint is used. We wish to find a lower-bound γ¯¯𝛾\\bar{\\gamma} such that, given α𝛼\\alpha satisfying Equation 44, the MPC is stable for γ≥γ¯𝛾¯𝛾\\gamma\\geq\\bar{\\gamma}.\nFor infinite-horizon optimal control, this was done by . First, recall that: In , it shown that 1≤C<1/(1−γ)1𝐶11𝛾1\\leq C<1/(1-\\gamma) is sufficient for stability of an infinite-horizon discounted optimal control problem, when α​V​(x)𝛼𝑉𝑥\\alpha V(x) is its value function. This means that: which implies that: For MPC, we will instead present an additional condition to the above one that leads to at least convergence to the safe set. This results in a bounded and safe solution. Exact convergence to the origin will be then confirmed when V𝑉V is the actual value function, as in , or if we switch to the demonstrating policy, K_​0subscript𝐾_0K_{\\_}0, once in the terminal set. Finally, we will remove the terminal constraint as done for the undiscounted case with a final bound on α𝛼\\alpha and γ𝛾\\gamma. Recall that condition (38) applies.\nIf the terminal constraint was met at time t𝑡t, namely, if V​(x⋆​(N))≤l_​s𝑉superscript𝑥⋆𝑁subscript𝑙_𝑠V(x^{\\star}(N))\\leq l_{\\_}s, then at the next time step, t+1𝑡1t+1 we have that u​(N+1)=K_​0​(x⋆​(N))𝑢𝑁1subscript𝐾_0superscript𝑥⋆𝑁u(N+1)=K_{\\_}0(x^{\\star}(N)) is feasible. Hence, the optimal MPC solution can be upper-bounded by the shifted solution at the previous time t𝑡t, with the K_​0subscript𝐾_0K_{\\_}0 policy appended at the end of the horizon . Denote this policy as u¯~~¯𝑢\\tilde{\\underline{u}} and x~~𝑥\\tilde{x} as the predictions. We have that: Hence, where\nL_​N−1​(x)=∑_i=1N−1​γi−1​ℓ​(x~​(i),u~​(i))subscript𝐿_𝑁1𝑥subscript_𝑖superscript1𝑁1superscript𝛾𝑖1ℓ~𝑥𝑖~𝑢𝑖L_{\\_}{N-1}(x)=\\sum_{\\_}{i=1}^{N-1}\\gamma^{i-1}\\ell(\\tilde{x}(i),\\tilde{u}(i)) and we have taken α𝛼\\alpha such that γ​α≥α¯_​1𝛾𝛼subscript¯𝛼_1\\gamma\\alpha\\geq\\bar{\\alpha}_{\\_}1. Now, for γ=1𝛾1\\gamma=1, the effect of L_​N−1subscript𝐿_𝑁1L_{\\_}{N-1} disappears and the MPC optimal cost is a Lyapunov function as in the standard MPC stability result from . By inspection of L_​N−1subscript𝐿_𝑁1L_{\\_}{N-1}, since the cost is bounded over bounded sets, also a small enough γ𝛾\\gamma could be found such that L_​N−1​(x)<ℓ​(x,u~​(0))subscript𝐿_𝑁1𝑥ℓ𝑥~𝑢0L_{\\_}{N-1}(x)<\\ell(x,\\tilde{u}(0)). This γ𝛾\\gamma, however, depends on x𝑥x. Consider x∉𝕏_​s𝑥subscript𝕏_𝑠x\\not\\in\\mathbb{X}_{\\_}s, for which there exist a feasible solution, namely a solution providing x~​(N)∈𝕏_​s~𝑥𝑁subscript𝕏_𝑠\\tilde{x}(N)\\in\\mathbb{X}_{\\_}s. Then, since ℓℓ\\ell is strictly increasing, ℓ​(0,0)=0ℓ000\\ell(0,0)=0, 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s contains the origin and the constraints are bounded, we have that there exist a υ≥1𝜐1\\upsilon\\geq 1 such that for any feasbile x𝑥x: is an upper bound for L_​N−1​(x)subscript𝐿_𝑁1𝑥L_{\\_}{N-1}(x). For instance, is sufficient for any closed set of initial conditions x​(0)∈ϵ​𝕏⊃𝕏_​s𝑥0italic-ϵ𝕏superset-ofsubscript𝕏_𝑠x(0)\\in\\epsilon\\mathbb{X}\\supset\\mathbb{X}_{\\_}s, with ϵ>0italic-ϵ0\\epsilon>0.\nIn order to have stability, it suffices to have (1−γ)​L¯_​N−1−ℓ​(x,u~​(0))≤01𝛾subscript¯𝐿_𝑁1ℓ𝑥~𝑢00(1-\\gamma)\\bar{L}_{\\_}{N-1}-\\ell(x,\\tilde{u}(0))\\leq 0 which requires: In the above condition γ¯​(x)¯𝛾𝑥\\bar{\\gamma}(x) can be less than 111 only outside a neighborhood of origin. Consider again Then taking provides that the system trajectory will enter the safe set 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s, hence 𝔹_​γ⊆𝕏_​ssubscript𝔹_𝛾subscript𝕏_𝑠\\mathbb{B}_{\\_}\\gamma\\subseteq\\mathbb{X}_{\\_}s.\nFinally, once x∈𝕏_​s𝑥subscript𝕏_𝑠x\\in\\mathbb{X}_{\\_}s, we that\nthe policy K_​0​(x)subscript𝐾_0𝑥K_{\\_}0(x) is feasible and: Hence, we can use this policy to upper bound the MPC cost: If the above is true with equality, then we can proceed as in Theorem 3.1 of , with γ>γ¯_​1𝛾subscript¯𝛾_1\\gamma>\\bar{\\gamma}_{\\_}1. This would require α​V​(x)𝛼𝑉𝑥\\alpha V(x) to be also a value function for the discounted problem. From the above considerations, we can conclude that that: If N=1𝑁1N=1, then L¯_​N−1=0subscript¯𝐿_𝑁10\\bar{L}_{\\_}{N-1}=0 and the system is asymptotically stable for any γ>0𝛾0\\gamma>0. If N>1𝑁1N>1, γ≥γ_​2¯𝛾¯subscript𝛾_2\\gamma\\geq\\bar{\\gamma_{\\_}2}, then the system reaches an bound 𝔹_​γsubscript𝔹_𝛾\\mathbb{B}_{\\_}\\gamma that is included in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s. If N>1𝑁1N>1 γ≥γ_​2¯𝛾¯subscript𝛾_2\\gamma\\geq\\bar{\\gamma_{\\_}2} and once in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s we switch to the policy K_​0​(x)subscript𝐾_0𝑥K_{\\_}0(x) then the system is asymptotically stable. If α​V​(x)𝛼𝑉𝑥\\alpha V(x) is the global value function for the discounted problem and if ℛ​(𝕏_​s)=𝕏_​sℛsubscript𝕏_𝑠subscript𝕏_𝑠\\mathcal{R}(\\mathbb{X}_{\\_}s)=\\mathbb{X}_{\\_}s, then γ>γ¯_​1𝛾subscript¯𝛾_1\\gamma>\\bar{\\gamma}_{\\_}1 provides that the system is Asymptotically stable. If α​V​(x)𝛼𝑉𝑥\\alpha V(x) is only the value function in 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s for the discounted problem, and if ℛ​(𝕏_​s)≠𝕏_​sℛsubscript𝕏_𝑠subscript𝕏_𝑠\\mathcal{R}(\\mathbb{X}_{\\_}s)\\neq\\mathbb{X}_{\\_}s, then γ>max⁡(γ¯_​1,γ_​2¯)𝛾subscript¯𝛾_1¯subscript𝛾_2\\gamma>\\max(\\bar{\\gamma}_{\\_}1,\\bar{\\gamma_{\\_}2}) provides that the system is Asymptotically stable. Finally, following Theorem 3 from [ref]20, the terminal constraint can be removed for all points x∈ℛN​(ρ​𝕏_​s)𝑥superscriptℛ𝑁𝜌subscript𝕏_𝑠x\\in\\mathcal{R}^{N}(\\rho\\mathbb{X}_{\\_}s), with ρ∈[0,1)𝜌01\\rho\\in[0,1), by setting: In fact, by the same argument of [ref]20, for any states for which it exists a feasible sequence u¯~~¯𝑢\\tilde{\\underline{u}} taking x^​(N)^𝑥𝑁\\hat{x}(N) to ρ​𝕏_​s𝜌subscript𝕏_𝑠\\rho\\mathbb{X}_{\\_}s we have that: If α𝛼\\alpha satisfies (52), then we also have that: from which we can directly verify that the set defined in (III-B) is a ROA (for either asymptotic or practical stability): For point 6, the stability margins of nominal MPC have been studied in . In particular, in a setup without terminal constraint, under nominal stabilising conditions, with a uniformly continuous model (in our case even Lipschitz), cost functions being also uniformly continuous, then the optimal MPC cost is also uniformly continuous [24, Proposition 1]. In other words, from [24, Theorem 1], there is a 𝒦_​∞subscript𝒦_\\mathcal{K}_{\\_}\\infty-function, σ𝜎\\sigma, such that at the optimal solution, u¯⋆superscript¯𝑢⋆\\underline{u}^{\\star}, we have: Using the above identity, one wish to bound the increase in the MPC cost due to uncertainty. At the same time, we wish the MPC to remain feasible and perform a contraction, namely, to have a stability margin. Since we are using soft constraints, then the MPC remains always feasible, however, we need the predictions at the end of the horizon to be in an invariant set 𝕏_​ssubscript𝕏_𝑠\\mathbb{X}_{\\_}s even under the effect of uncertainty. In particular, we will use V​(x)𝑉𝑥V(x) and its contraction factor λ𝜆\\lambda to compute a smaller level set ζ​𝕏_​s𝜁subscript𝕏_𝑠\\zeta\\mathbb{X}_{\\_}s, for some ζ∈(0,1)𝜁01\\zeta\\in(0,1) which is invariant under the uncertainty. Once this is found then we can compute a new α𝛼\\alpha for this set according to (52).\nIn particular, under the policy K_​0subscript𝐾_0K_{\\_}0, we have that: We wish this quantity to be non-positive for x∉ζ​𝕏_​s𝑥𝜁subscript𝕏_𝑠x\\not\\in\\zeta\\mathbb{X}_{\\_}s, which means that V​(x)≥ζ​l_​s𝑉𝑥𝜁subscript𝑙_𝑠V(x)\\geq\\zeta l_{\\_}s. For this it is sufficient to have: Since we apply the function V𝑉V to the prediction at time N𝑁N, and at the next step the measured state (prediction at index 00) differs from the previous MPC prediction of a quantity w𝑤w, for the MPC this condition translates into: Therefore, given the model error w𝑤w, if the MPC remains feasible and if α𝛼\\alpha and γ𝛾\\gamma exceed their lower bounds given the restricted set ζ​𝕏_​s𝜁subscript𝕏_𝑠\\zeta\\mathbb{X}_{\\_}s, we have that ‖w‖_​2<μ¯subscriptnorm𝑤_2¯𝜇\\|w\\|_{\\_}2<\\bar{\\mu} implies that: which is the definition of Input-to-State practical Stability  , where we have defined d¯​(N)=(1−γ)​L¯_​N−1¯𝑑𝑁1𝛾subscript¯𝐿_𝑁1\\bar{d}(N)=(1-\\gamma)\\bar{L}_{\\_}{N-1}. The trajectory of the system is therefore bounded by the level-set of J_⋆​M​P​C​(x)subscriptsuperscript𝐽⋆_𝑀𝑃𝐶𝑥J^{\\star}_{\\_}{MPC}(x) outside which σ​(μ)+d¯​(N)≤l_​ℓ​‖x‖_​22.𝜎𝜇¯𝑑𝑁subscript𝑙_ℓsubscriptnorm𝑥_superscript22\\sigma(\\mu)+\\bar{d}(N)\\leq l_{\\_}\\ell\\|x\\|_{\\_}2^{2}.\nSince σ𝜎\\sigma is strictly increasing and d¯¯𝑑\\bar{d} is strictly decreasing, we can also conclude that the size of this bound increases with increasing model error μ𝜇\\mu and with the horizon length N𝑁N. Note that the term d¯¯𝑑\\bar{d} vanishes if γ=1𝛾1\\gamma=1 but the term σ𝜎\\sigma will also increase with N𝑁N if L¯_​f>1subscript¯𝐿_𝑓1\\bar{L}_{\\_}f>1. From the restriction of the terminal set, it also follows that the ROA defined in Equation III-B will also be restricted unless we recompute a larger α𝛼\\alpha for this new set. Denote V^​(x)=α​V​(x)^𝑉𝑥𝛼𝑉𝑥\\hat{V}(x)=\\alpha V(x). Recall that, if α≥α_​1𝛼subscript𝛼_1\\alpha\\geq\\alpha_{\\_}1, as defined in Equation 42, and if ‖w​(t)‖=0​∀tnorm𝑤𝑡0for-all𝑡\\|w(t)\\|=0\\ \\forall t, then we have that, ∀x​(t)∈𝕏_​sfor-all𝑥𝑡subscript𝕏_𝑠\\forall x(t)\\in\\mathbb{X}_{\\_}s: which in turn implies that, by means of γ≤1𝛾1\\gamma\\leq 1 and of induction: which is the value of the policy K_​0subscript𝐾_0K_{\\_}0. This is, by definition, an upper bound of the optimal value function, V⋆​(x)superscript𝑉⋆𝑥V^{\\star}(x). Hence α​V​(x)≥V⋆​(x),∀x∈𝕏_​sformulae-sequence𝛼𝑉𝑥superscript𝑉⋆𝑥for-all𝑥subscript𝕏_𝑠\\alpha V(x)\\geq V^{\\star}(x),\\forall x\\in\\mathbb{X}_{\\_}s. ∎ In practice, the ISS bound σ​(μ)𝜎𝜇\\sigma(\\mu) from Theorem 2 has a form similar to the one discussed for the constraint penalty in the proof of Theorem 3, see Equation 37. Its explicit computation is omitted for brevity; however, in general, we can expect the bound to become worse for systems that are open-loop unstable as the horizon length increases."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.06565",
      "authors": "D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané",
      "orig_title": "Concrete problems in ai safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 1,
      "title": "Model-based reinforcement learning: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.16712",
      "authors": "T. M. Moerland, J. Broekens, and C. M. Jonker"
    },
    {
      "index": 2,
      "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.01848",
      "authors": "K. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch",
      "orig_title": "Plan online, learn offline: Efficient learning and exploration via model-based control",
      "paper_id": "1811.01848v3"
    },
    {
      "index": 3,
      "title": "Nonlinear Control",
      "abstract": "",
      "year": "2014",
      "venue": "Pearson",
      "authors": "H. K. Khalil"
    },
    {
      "index": 4,
      "title": "Sampling driven stability domains computation and predictive control of constrained nonlinear systems",
      "abstract": "",
      "year": "2017",
      "venue": "Ph.D. dissertation",
      "authors": "R. V. Bobiti"
    },
    {
      "index": 5,
      "title": "Sampling–based verification of Lyapunov’s inequality for piecewise continuous nonlinear systems",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.00302",
      "authors": "R. Bobiti and M. Lazar",
      "orig_title": "Sampling-based verification of lyapunov’s inequality for piecewise continuous nonlinear systems",
      "paper_id": "1609.00302v1"
    },
    {
      "index": 6,
      "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.08551",
      "authors": "F. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause",
      "orig_title": "Safe model-based reinforcement learning with stability guarantees",
      "paper_id": "1705.08551v3"
    },
    {
      "index": 7,
      "title": "Safe Interactive Model-Based Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.06556",
      "authors": "M. Gallieri, S. S. M. Salehian, N. E. Toklu, A. Quaglino, J. Masci, J. Koutník, and F. Gomez",
      "orig_title": "Safe interactive model-based learning",
      "paper_id": "1911.06556v2"
    },
    {
      "index": 8,
      "title": "Neural Lyapunov Control",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00611",
      "authors": "Y.-C. Chang, N. Roohi, and S. Gao",
      "orig_title": "Neural lyapunov control",
      "paper_id": "2005.00611v4"
    },
    {
      "index": 9,
      "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.00177",
      "authors": "X. B. Peng, A. Kumar, G. Zhang, and S. Levine",
      "orig_title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
      "paper_id": "1910.00177v3"
    },
    {
      "index": 10,
      "title": "Set-Theoretic Methods in Control (Systems & Control: Foundations & Applications)",
      "abstract": "",
      "year": "2007",
      "venue": "Birkhäuser",
      "authors": "F. Blanchini and S. Miani"
    },
    {
      "index": 11,
      "title": "Robust constraint satisfaction: Invariant sets and predictive control",
      "abstract": "",
      "year": "2000",
      "venue": "Ph.D. dissertation",
      "authors": "E. Kerrigan"
    },
    {
      "index": 12,
      "title": "Formal Synthesis of Lyapunov Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Control Systems Letters",
      "authors": "A. Abate, D. Ahmed, M. Giacobbe, and A. Peruffo",
      "orig_title": "Formal synthesis of lyapunov neural networks",
      "paper_id": "2003.08910v2"
    },
    {
      "index": 13,
      "title": "dreach: δ𝛿\\delta-reachability analysis for hybrid systems",
      "abstract": "",
      "year": "2015",
      "venue": "Tools and Algorithms for the Construction and Analysis of Systems",
      "authors": "S. Kong, S. Gao, W. Chen, and E. Clarke"
    },
    {
      "index": 14,
      "title": "Echo State Networks: analysis, training and predictive control",
      "abstract": "",
      "year": "2019",
      "venue": "2019 18th European Control Conference (ECC)",
      "authors": "L. B. Armenio, E. Terzi, M. Farina, and R. Scattolini",
      "orig_title": "Echo state networks: analysis, training and predictive control",
      "paper_id": "1902.01618v1"
    },
    {
      "index": 15,
      "title": "Physics-informed echo state networks for chaotic systems forecasting",
      "abstract": "",
      "year": "2019",
      "venue": "Lecture Notes in Computer Science",
      "authors": "N. A. K. Doan, W. Polifke, and L. Magri"
    },
    {
      "index": 16,
      "title": "Using machine learning to replicate chaotic attractors and calculate lyapunov exponents from data",
      "abstract": "",
      "year": "2017",
      "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science",
      "authors": "J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott"
    },
    {
      "index": 17,
      "title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1804.07209 [cs, stat]",
      "authors": "M. Ciccone, M. Gallieri, J. Masci, C. Osendorfer, and F. Gomez",
      "orig_title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations",
      "paper_id": "1804.07209v4"
    },
    {
      "index": 18,
      "title": "Constrained model predictive control: Stability and optimality",
      "abstract": "",
      "year": "2000",
      "venue": "Automatica",
      "authors": "D. Mayne, J. Rawlings, C. Rao, and P. Scokaert"
    },
    {
      "index": 19,
      "title": "Stable constrained MPC without terminal constraint",
      "abstract": "",
      "year": "2003",
      "venue": "American Control Conference",
      "authors": "D. Limon, T. Alamo, and E. Camacho"
    },
    {
      "index": 20,
      "title": "The Lyapunov Neural Network: Adaptive Stability Certification for Safe Learning of Dynamical Systems",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Robot Learning",
      "authors": "S. M. Richards, F. Berkenkamp, and A. Krause",
      "orig_title": "The lyapunov neural network: Adaptive stability certification for safe learning of dynamical systems",
      "paper_id": "1808.00924v2"
    },
    {
      "index": 21,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 22,
      "title": "Soft constraints and exact penalty functions in model predictive control",
      "abstract": "",
      "year": "2000",
      "venue": "UKACC International Conference",
      "authors": "E. C. Kerrigan and J. M. Maciejowski"
    },
    {
      "index": 23,
      "title": "Input-to-State Stability: A Unifying Framework for Robust Model Predictive Control",
      "abstract": "",
      "year": "2009",
      "venue": "Nonlinear Model Predictive Control",
      "authors": "D. Limon, T. Alamo, D. M. Raimondo, D. M. de la Peña, J. M. Bravo, A. Ferramosca, and E. F. Camacho"
    },
    {
      "index": 24,
      "title": "LASSO-MPC – Predictive Control with ℓ_​1subscriptℓ_1\\ell_{\\_}1-Regularised Least Squares",
      "abstract": "",
      "year": "2016",
      "venue": "Springer-Verlag",
      "authors": "M. Gallieri"
    },
    {
      "index": 25,
      "title": "Lipschitz Continuity in Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "K. Asadi, D. Misra, and M. Littman",
      "orig_title": "Lipschitz continuity in model-based reinforcement learning",
      "paper_id": "1804.07193v3"
    },
    {
      "index": 26,
      "title": "Differentiable convex optimization layers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.12430",
      "authors": "A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter"
    },
    {
      "index": 27,
      "title": "Differentiable MPC for End-to-end Planning and Control",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.13400",
      "authors": "B. Amos, I. D. J Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter",
      "orig_title": "Differentiable mpc for end-to-end planning and control",
      "paper_id": "1810.13400v3"
    },
    {
      "index": 28,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 29,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.10528",
      "authors": "J. Achiam, D. Held, A. Tamar, and P. Abbeel",
      "orig_title": "Constrained policy optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 30,
      "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "A. Ray, J. Achiam, and D. Amodei"
    },
    {
      "index": 31,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 32,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.01290",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 33,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Janner, J. Fu, M. Zhang, and S. Levine"
    },
    {
      "index": 34,
      "title": "Model Predictive Control Theory and Design",
      "abstract": "",
      "year": "2009",
      "venue": "Nob Hill Pub, Llc",
      "authors": "J. B. Rawlings and D. Q. Mayne"
    },
    {
      "index": 35,
      "title": "Homothetic tube model predictive control",
      "abstract": "",
      "year": "2012",
      "venue": "Automatica",
      "authors": "S. V. Raković, B. Kouvaritakis, R. Findeisen, and M. Cannon"
    },
    {
      "index": 36,
      "title": "Stabilization with discounted optimal control",
      "abstract": "",
      "year": "2015",
      "venue": "Systems & Control Letters",
      "authors": "V. Gaitsgory, L. Grüne, and N. Thatcher"
    },
    {
      "index": 37,
      "title": "Synthesis and stabilization of complex behaviors through online trajectory optimization",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Y. Tassa, T. Erez, and E. Todorov"
    },
    {
      "index": 38,
      "title": "Numerical optimization",
      "abstract": "",
      "year": "2006",
      "venue": "Springer Science & Business Media",
      "authors": "J. Nocedal and S. Wright"
    },
    {
      "index": 39,
      "title": "SNODE: Spectral Discretization of Neural ODEs for System Identification",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:1906.07038 [cs]",
      "authors": "A. Quaglino, M. Gallieri, J. Masci, and J. Koutník"
    },
    {
      "index": 40,
      "title": "ODE2VAE: Deep generative second order ODEs with Bayesian neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1905.10994 [cs, stat]",
      "authors": "C. Yıldız, M. Heinonen, and H. Lähdesmäki"
    },
    {
      "index": 41,
      "title": "Tustin neural networks: a class of recurrent nets for adaptive MPC of mechanical systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "S. Pozzoli, M. Gallieri, and R. Scattolini",
      "orig_title": "Tustin neural networks: a class of recurrent nets for adaptive MPC of mechanical systems",
      "paper_id": "1911.01310v1"
    },
    {
      "index": 42,
      "title": "Lvis: learning from value function intervals for contact-aware robot controllers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "R. Deits, T. Koolen, and R. Tedrake"
    },
    {
      "index": 43,
      "title": "Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.01675",
      "authors": "J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee",
      "orig_title": "Sample-efficient reinforcement learning with stochastic ensemble value expansion",
      "paper_id": "1807.01675v2"
    },
    {
      "index": 44,
      "title": "On the infinite horizon performance of receding horizon controllers",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "L. Grune and A. Rantzer"
    },
    {
      "index": 45,
      "title": "Learning-based Model Predictive Control for Safe Exploration",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE Conference on Decision and Control (CDC)",
      "authors": "T. Koller, F. Berkenkamp, M. Turchetta, and A. Krause",
      "orig_title": "Learning-based model predictive control for safe exploration",
      "paper_id": "1803.08287v3"
    },
    {
      "index": 46,
      "title": "Learning-based model predictive control: Toward safe learning in control",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Review of Control, Robotics, and Autonomous Systems",
      "authors": "L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger"
    },
    {
      "index": 47,
      "title": "Lyapunov Criterion for Stochastic Systems and Its Applications in Distributed Computation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Y. Qin, M. Cao, and B. D. Anderson",
      "orig_title": "Lyapunov criterion for stochastic systems and its applications in distributed computation",
      "paper_id": "1902.04332v1"
    },
    {
      "index": 48,
      "title": "The explicit solution of model predictive control via multiparametric quadratic programming",
      "abstract": "",
      "year": "2000",
      "venue": "American Control Conference",
      "authors": "A. Bemporad, M. Morari, V. Dua, and E. Pistikopoulos"
    },
    {
      "index": 49,
      "title": "Sequential quadratic programming for task plan optimization",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "D. Hadfield-Menell, C. Lin, R. Chitnis, S. Russell, and P. Abbeel"
    },
    {
      "index": 50,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01703",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 51,
      "title": "Handbook of marine craft hydrodynamics and motion control",
      "abstract": "",
      "year": "2011",
      "venue": "John Wiley & Sons",
      "authors": "T. I. Fossen"
    },
    {
      "index": 52,
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS)",
      "authors": "X. Glorot and Y. Bengio"
    }
  ]
}