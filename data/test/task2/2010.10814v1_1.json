{
  "paper_id": "2010.10814v1",
  "title": "Improving Generalization in Reinforcement Learning with Mixture Regularization",
  "sections": {
    "introduction": "Deep Reinforcement Learning (RL) has brought significant progress in learning policies to tackle various challenging tasks, such as board games like Go  , Chess and Shogi , video games like Atari   and StarCraft , and robotics control tasks 4. Despite its outstanding performance, deep RL agents tend to suffer poor generalization to unseen environments   [ref]29 [ref]3 [ref]2 . For example, in video games, agents trained with a small set of levels struggle to make progress in unseen levels of the same game [ref]2; in robotics control, agents trained in simulation environments of low diversity generalize poorly to the realistic environments . Such a generalization gap has become a major obstacle for deploying deep RL in real applications. One of the main causes for this generalization gap is the limited diversity of training environments [ref]29 [ref]3 [ref]2.\nMotivated by this, some works propose to improve RL agents’ generalizability by diversifying the training data via data augmentation techniques [ref]3 3 2 1.\nHowever, these approaches merely augment the observations individually with image processing techniques, such as random crop, patch cutout  and random convolutions 3.\nAs shown in Figure 1 (left), such techniques are performing local perturbation within the state feature space,\nwhich only incrementally increases the training data diversity and thus leads to limited generalization performance gain.\nThis is evidenced by our findings that these augmentation techniques fail to improve generalization performance of the RL agents when evaluated on a large-scale benchmark (see Section 4.1). In this work, we introduce mixreg that trains the RL agent on a mixture of observations collected from different training environments.\nInspired by the success of mixup  in supervised learning, per training step, mixreg generates augmented observations by convexly combining two observations randomly sampled from the collected batch, and trains the RL agent on them with their interpolated supervision signal (e.g. the associated rewards or state values).\nIn this way, the generated observations are widely distributed between the diverse observations and can effectively increase the training data diversity, as shown in Figure 1 (right).\nMoreover, mixreg imposes piece-wise linearity regularization to the learned policy and value functions w.r.t. the states.\nSuch regularization encourages the agent to learn a smoother policy with better generalization performance.\nNotably, mixreg is a general scheme and can be applied to both policy-based and value-based RL algorithms. We evaluate mixreg on the recently introduced Procgen Benchmark [ref]2.\nWe compare mixreg with three best-performing data augmentation techniques (i.e. cutout-color, random crop, random convolution) in 2, and two regularization techniques (i.e. batch normalization 0 and ℓ2subscriptℓ2\\ell_{2} regularization) adopted in previous works [ref]3 .\nWe find that mixreg boosts the generalization performance of the RL agent more significantly, surpassing the baselines by a large margin. Moreover, when combined with other methods such as ℓ2subscriptℓ2\\ell_{2} regularization, mixreg brings further improvement.\nWe also verify the effectiveness of mixreg for both policy-based and value-based algorithms.\nAdditionally, we conduct several analytical experiments to study and provide better understanding on its effectiveness. This work makes the following contributions. We are among the first to study how to effectively increase training data diversity to improve RL generalization. Different from data augmentation techniques as commonly adopted in recent works, we propose to look into mixing observations from different environments. We introduce mixreg, a simple and effective approach for improving RL generalization by learning smooth policy over mixed observations. Mixreg can be easily deployed for both policy and value-based RL algorithms. On the recent large-scale Procgen benchmark, mixreg outperforms many well-established baselines by large margins. It also serves as a strong baseline for future studies."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Agent57: Outperforming the Atari Human Benchmark",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.13350",
      "authors": "A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell",
      "orig_title": "Agent57: outperforming the atari human benchmark",
      "paper_id": "2003.13350v1"
    },
    {
      "index": 1,
      "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01588",
      "authors": "K. Cobbe, C. Hesse, J. Hilton, and J. Schulman",
      "orig_title": "Leveraging procedural generation to benchmark reinforcement learning",
      "paper_id": "1912.01588v2"
    },
    {
      "index": 2,
      "title": "Quantifying Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.02341",
      "authors": "K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman",
      "orig_title": "Quantifying generalization in reinforcement learning",
      "paper_id": "1812.02341v3"
    },
    {
      "index": 3,
      "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04552",
      "authors": "T. DeVries and G. W. Taylor",
      "orig_title": "Improved regularization of convolutional neural networks with cutout",
      "paper_id": "1708.04552v2"
    },
    {
      "index": 4,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.01561",
      "authors": "L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al.",
      "orig_title": "Impala: scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 5,
      "title": "Generalization and Regularization in DQN",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00123",
      "authors": "J. Farebrother, M. C. Machado, and M. Bowling",
      "orig_title": "Generalization and regularization in dqn",
      "paper_id": "1810.00123v3"
    },
    {
      "index": 6,
      "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07377",
      "authors": "S. Gamrian and Y. Goldberg",
      "orig_title": "Transfer learning for related reinforcement learning tasks via image-to-image translation",
      "paper_id": "1806.07377v6"
    },
    {
      "index": 7,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver",
      "orig_title": "Rainbow: combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 8,
      "title": "Generalization in reinforcement learning with selective noise injection and information bottleneck",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann"
    },
    {
      "index": 9,
      "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1502.03167",
      "authors": "S. Ioffe and C. Szegedy"
    },
    {
      "index": 10,
      "title": "Image augmentation is all you need: regularizing deep reinforcement learning from pixels",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.13649",
      "authors": "I. Kostrikov, D. Yarats, and R. Fergus"
    },
    {
      "index": 11,
      "title": "Reinforcement Learning with Augmented Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.14990",
      "authors": "M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas",
      "orig_title": "Reinforcement learning with augmented data",
      "paper_id": "2004.14990v5"
    },
    {
      "index": 12,
      "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "K. Lee, K. Lee, J. Shin, and H. Lee",
      "orig_title": "Network randomization: a simple technique for generalization in deep reinforcement learning",
      "paper_id": "1910.05396v3"
    },
    {
      "index": 13,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1509.02971",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra"
    },
    {
      "index": 14,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature 518 (7540)",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al."
    },
    {
      "index": 15,
      "title": "Assessing Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.12282",
      "authors": "C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song",
      "orig_title": "Assessing generalization in deep reinforcement learning",
      "paper_id": "1810.12282v2"
    },
    {
      "index": 16,
      "title": "Automatic data augmentation for generalization in deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12862",
      "authors": "R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus"
    },
    {
      "index": 17,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 18,
      "title": "Mastering the game of go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "nature 529 (7587)",
      "authors": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al."
    },
    {
      "index": 19,
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.01815",
      "authors": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al.",
      "orig_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
      "paper_id": "1712.01815v1"
    },
    {
      "index": 20,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature 550 (7676)",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al."
    },
    {
      "index": 21,
      "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.07854",
      "authors": "A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine",
      "orig_title": "End-to-end robotic reinforcement learning without reward engineering",
      "paper_id": "1904.07854v2"
    },
    {
      "index": 22,
      "title": "Observational Overfitting in Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.02975",
      "authors": "X. Song, Y. Jiang, Y. Du, and B. Neyshabur",
      "orig_title": "Observational overfitting in reinforcement learning",
      "paper_id": "1912.02975v2"
    },
    {
      "index": 23,
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "abstract": "",
      "year": "2014",
      "venue": "The journal of machine learning research 15 (1)",
      "authors": "N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov"
    },
    {
      "index": 24,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour"
    },
    {
      "index": 25,
      "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)",
      "authors": "J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel",
      "orig_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
      "paper_id": "1703.06907v1"
    },
    {
      "index": 26,
      "title": "StarCraft II: A New Challenge for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04782",
      "authors": "O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, et al.",
      "orig_title": "Starcraft ii: a new challenge for reinforcement learning",
      "paper_id": "1708.04782v1"
    },
    {
      "index": 27,
      "title": "Learning to reinforcement learn",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.05763",
      "authors": "J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick"
    },
    {
      "index": 28,
      "title": "A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07937",
      "authors": "A. Zhang, N. Ballas, and J. Pineau",
      "orig_title": "A dissection of overfitting and generalization in continuous reinforcement learning",
      "paper_id": "1806.07937v2"
    },
    {
      "index": 29,
      "title": "Natural Environment Benchmarks for Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.06032",
      "authors": "A. Zhang, Y. Wu, and J. Pineau",
      "orig_title": "Natural environment benchmarks for reinforcement learning",
      "paper_id": "1811.06032v1"
    },
    {
      "index": 30,
      "title": "A study on overfitting in deep reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.06893",
      "authors": "C. Zhang, O. Vinyals, R. Munos, and S. Bengio"
    },
    {
      "index": 31,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.09412",
      "authors": "H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz",
      "orig_title": "Mixup: beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    }
  ]
}