{
  "paper_id": "2203.00048v3",
  "title": "Multi-modal Alignment using Representation Codebook",
  "sections": {
    "related work": "Vision-Language Pre-training (V&L)\nV&L pretraining is an active research area with many recent works. We review here the works that are most relevant to ours.\nArchitecture wise, previous approaches can be broadly classified into two categories early fusion and late fusion. In early-fusion approaches     , image and text are transformed into sequences (tokenization) and passed to a single encoder (typically Transformer-based) for embedding generation. Thus multimodal signals are fused in the early stage. Whereas in late-fusion works   , separate encoders are used for image and text. Extracted features are typically fused during the later fine tuning stage. Our work is a hybrid between these two approaches, similar to 5 . The main difference is the codebook and various related contrastive losses. In vision language learning, codebook has been used in a number of recent works, mostly for image tokenization. BEiT  constructed a dictionary of visual words, then used it to form mask image modeling task in the same fashion as mask language modeling. SOHO 0 integrated visual dictionary to the main model and jointly trained both of them. Both works quantized the visual input space. In contrast, our codebook is used to quantize the joint output space\n, where multimodal views are aligned via optimal transport .\nOther concurrent works to ours include 7 5. They both align cross-modal instances using InfoNCE . In contrast, we enforce both unimodal and cross-modal alignment, both at the instance level and at the cluster level. Self-supervised Contrastive Learning\nThe goal of contrastive learning 7 is to attract positive sample pairs and repulse the negative sample pairs. Recently, it has been widely used in computer vision for unsupervised, semi-supervised 3 and self-supervised representation learning 8  . Contrastive reasoning is typically formed based on two augmented views of the same input image. One of the main challenge is feature collapsing, and in practice, a large number of negative samples are required, through either large batch size  or memory banks 8 , to alleviate this problem. Several recent works have shown that one can learn unsupervised features without discriminating instances. Deep clustering  and SwAV  incorporate online clustering into Siamese networks. In BYOL 6, features are trained by matching them to representations obtained by a momentum encoder. DINO  instantiates the momentum encoder with a vision-transformer and adopts a teacher-student distillation paradigm 9 7 3. Our alignment techniques and momentum update were inspired by these works and can be considered as extensions to the multimodal setting. ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Gradient flows: in metric spaces and in the space of probability measures",
      "abstract": "",
      "year": "2008",
      "venue": "Springer Science & Business Media",
      "authors": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré"
    },
    {
      "index": 1,
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.08254",
      "authors": "Hangbo Bao, Li Dong, and Furu Wei",
      "orig_title": "Beit: Bert pre-training of image transformers",
      "paper_id": "2106.08254v2"
    },
    {
      "index": 2,
      "title": "Deep clustering for unsupervised learning of visual features",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze"
    },
    {
      "index": 3,
      "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.09882",
      "authors": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin",
      "orig_title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "paper_id": "2006.09882v5"
    },
    {
      "index": 4,
      "title": "Emerging properties in self-supervised vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.14294",
      "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin"
    },
    {
      "index": 5,
      "title": "Graph Optimal Transport for Cross-Domain Alignment",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu",
      "orig_title": "Graph optimal transport for cross-domain alignment",
      "paper_id": "2006.14744v3"
    },
    {
      "index": 6,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 7,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu",
      "orig_title": "Uniter: Universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 8,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le"
    },
    {
      "index": 9,
      "title": "An optimal transport approach to robust reconstruction and simplification of 2d shapes",
      "abstract": "",
      "year": "2011",
      "venue": "Computer Graphics Forum",
      "authors": "Fernando De Goes et al."
    },
    {
      "index": 10,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 11,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.11929",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 12,
      "title": "SLADE: A Self-Training Framework For Distance Metric Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiali Duan, Yen-Liang Lin, Son Tran, Larry S Davis, and C-C Jay Kuo",
      "orig_title": "Slade: A self-training framework for distance metric learning",
      "paper_id": "2011.10269v2"
    },
    {
      "index": 13,
      "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.06195",
      "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu",
      "orig_title": "Large-scale adversarial training for vision-and-language representation learning",
      "paper_id": "2006.06195v2"
    },
    {
      "index": 14,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 15,
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.07733",
      "authors": "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al.",
      "orig_title": "Bootstrap your own latent: A new approach to self-supervised learning",
      "paper_id": "2006.07733v3"
    },
    {
      "index": 16,
      "title": "Dimensionality reduction by learning an invariant mapping",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)",
      "authors": "Raia Hadsell, Sumit Chopra, and Yann LeCun"
    },
    {
      "index": 17,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 18,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 19,
      "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu",
      "orig_title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
      "paper_id": "2104.03135v2"
    },
    {
      "index": 20,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.05918",
      "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig"
    },
    {
      "index": 21,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03334",
      "authors": "Wonjae Kim, Bokyung Son, and Ildoo Kim",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 22,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International Journal of Computer Vision",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al."
    },
    {
      "index": 23,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 24,
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.07651",
      "authors": "Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi",
      "orig_title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "paper_id": "2107.07651v2"
    },
    {
      "index": 25,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 26,
      "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.15409",
      "authors": "Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang"
    },
    {
      "index": 27,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al.",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 28,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "European Conference on Computer Vision",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 29,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.05101",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 30,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.02265",
      "authors": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee"
    },
    {
      "index": 31,
      "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee",
      "orig_title": "12-in-1: Multi-task vision and language representation learning",
      "paper_id": "1912.02315v2"
    },
    {
      "index": 32,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.03748",
      "authors": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 33,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg"
    },
    {
      "index": 34,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 35,
      "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.07966",
      "authors": "Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti",
      "orig_title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data",
      "paper_id": "2001.07966v2"
    },
    {
      "index": 36,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.00020",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 37,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever"
    },
    {
      "index": 38,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 39,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut"
    },
    {
      "index": 40,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.08530",
      "authors": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai"
    },
    {
      "index": 41,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.00491",
      "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 42,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.07490",
      "authors": "Hao Tan and Mohit Bansal",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 43,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 44,
      "title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin",
      "orig_title": "Unsupervised feature learning via non-parametric instance discrimination",
      "paper_id": "1805.01978v1"
    },
    {
      "index": 45,
      "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.06706",
      "authors": "Ning Xie, Farley Lai, Derek Doran, and Asim Kadav",
      "orig_title": "Visual entailment: A novel task for fine-grained image understanding",
      "paper_id": "1901.06706v1"
    },
    {
      "index": 46,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le"
    },
    {
      "index": 47,
      "title": "Vision-Language Pre-Training with Triple Contrastive Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang",
      "orig_title": "Vision-language pre-training with triple contrastive learning",
      "paper_id": "2202.10401v4"
    },
    {
      "index": 48,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao",
      "orig_title": "Vinvl: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    }
  ]
}