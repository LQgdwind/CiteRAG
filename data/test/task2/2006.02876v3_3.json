{
  "paper_id": "2006.02876v3",
  "title": "Enhanced back-translation for low resource neural machine translation using self-trainingSupported by NITDEF PhD Scholarship Scheme 2018.",
  "sections": {
    "neural machine translation (nmt)": "This work is based on a unidirectional LSTM encoder-decoder architecture with Luong attention [ref]2. This is a recurrent neural network RNMT architecture and it is summarized below. Our approach can be applied to other architectures such as the convolutional neural network NMT (CNMT) 0  and the Transformer  2. Neural Machine Translation (NMT) is based on a sequence-to-sequence encoder-decoder system made of neural networks that models the conditional probability of a source sentence to a target sentence [ref]1 3 [ref]2. The encoder converts the input in the source language into a set of vectors while the decoder converts the set of vectors into the target language, word by word, through an attention mechanism – introduced to keep track of context in longer sentences [ref]1. The NMT model produces the translated sentence by generating one target word at every time step. Given the “right” amount of qualitative parallel data only, the NMT model can learn the probability of mapping sentences in the source language to their equivalents in another language – the target language – word by word . Given an input sequence X=(x1,…,xTx)𝑋subscript𝑥1…subscript𝑥subscript𝑇𝑥X=(x_{1},...,x_{T_{x}}), the encoder – made up of a bidirectional or unidirectional neural network with Long Short-Term Memory (LSTM) 4 or gated recurrent units (GRU) 5 – computes the annotation vector hjsubscriptℎ𝑗h_{j}, which is a concatenation of the forward and backward hidden states hj→→subscriptℎ𝑗\\overrightarrow{h_{j}} and hj←←subscriptℎ𝑗\\overleftarrow{h_{j}} respectively. The decoder is made up of a recurrent neural network that takes a recurrent hidden state sisubscript𝑠𝑖s_{i}, the previously translated words (y1,…,yi−1)subscript𝑦1…subscript𝑦𝑖1(y_{1},...,y_{i-1}) and a context vector cisubscript𝑐𝑖c_{i} to predict the probability of the next word yisubscript𝑦𝑖y_{i} as the weighted summation of the annotations hjsubscriptℎ𝑗h_{j}. An alignment model – a single layer feed-forward network which is learned jointly with the rest of the network through back-propagation – which models the probability that yisubscript𝑦𝑖y_{i} is aligned to xisubscript𝑥𝑖x_{i} is used to compute the weight of each annotation hjsubscriptℎ𝑗h_{j}. All of the parameters in the NMT model, θ𝜃\\theta, are optimized to maximize the following conditional log-likelihood of the M sentence aligned bilingual samples ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Bahdanau D, Cho K, Bengio Y.",
      "orig_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 1,
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "Luong MT, Pham H, Manning CD.",
      "orig_title": "Effective Approaches to Attention-based Neural Machine Translation",
      "paper_id": "1508.04025v5"
    },
    {
      "index": 2,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "31st Conference on Neural Information Processing Systems",
      "authors": "Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.",
      "orig_title": "Attention Is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 3,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Edunov S, Ott M, Auli M, Grangier D.",
      "orig_title": "Understanding Back-Translation at Scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 4,
      "title": "Scaling Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Ott M, Edunov S, Grangier D, Auli M.",
      "orig_title": "Scaling Neural Machine Translation",
      "paper_id": "1806.00187v3"
    },
    {
      "index": 5,
      "title": "Statistical Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Koehn P."
    },
    {
      "index": 6,
      "title": "Transfer Learning for Low-Resource Neural Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Zoph B, Yuret D, May J, Knight K.",
      "orig_title": "Transfer Learning for Low-Resource Neural Machine Translation",
      "paper_id": "1604.02201v1"
    },
    {
      "index": 7,
      "title": "Effectively training neural machine translation models with monolingual data",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "Yang Z, Chen W, Wang F, Xu B."
    },
    {
      "index": 8,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2018",
      "venue": "Machine Translation Quality Estimation: Applications and Future Perspectives",
      "authors": "Specia L, Shah K.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 9,
      "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Zhang J, Zong C."
    },
    {
      "index": 10,
      "title": "Improving Neural Machine Translation Models with Monolingual Data",
      "abstract": "",
      "year": "2016",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Sennrich R, Haddow B, Birch A"
    },
    {
      "index": 11,
      "title": "Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "2nd Workshop on Neural Machine Translation and Generation",
      "authors": "Imamura K, Fujita A, Sumita E."
    },
    {
      "index": 12,
      "title": "Tagged Back-Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Caswell I, Chelba C, Grangier D."
    },
    {
      "index": 13,
      "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
      "abstract": "",
      "year": "2002",
      "venue": "40th Annual Meeting on Association for Computational Linguistics",
      "authors": "Papineni K, Roukos S, Ward T, Zhu WJ."
    },
    {
      "index": 14,
      "title": "Dual Learning for Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "30th International Conference on Neural Information Processing Systems",
      "authors": "He D, Xia Y, Qin T, Wang L, Yu N, Liu TY, et al.",
      "orig_title": "Dual Learning for Machine Translation",
      "paper_id": "1611.00179v1"
    },
    {
      "index": 15,
      "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Eighth International Joint Conference on Natural Language Processing",
      "authors": "Nguyen TQ, Chiang D."
    },
    {
      "index": 16,
      "title": "NICT’s Supervised Neural Machine Translation Systems for the WMT19 News Translation Task",
      "abstract": "",
      "year": "2019",
      "venue": "Fourth Conference on Machine Translation (WMT)",
      "authors": "Dabre R, Chen K, Marie B, Wang R, Fujita A, Utiyama M, et al."
    },
    {
      "index": 17,
      "title": "CUNI Submission for Low-Resource Languages in WMT News 2019",
      "abstract": "",
      "year": "2019",
      "venue": "Fourth Conference on Machine Translation (WMT)",
      "authors": "Kocmi T, Bojar O."
    },
    {
      "index": 18,
      "title": "Cross-lingual Language Model Pretraining",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Lample G, Conneau A.",
      "orig_title": "Cross-lingual Language Model Pretraining",
      "paper_id": "1901.07291v1"
    },
    {
      "index": 19,
      "title": "Time-aware Large Kernel Convolutions",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Lioutas V, Guo Y.",
      "orig_title": "Time-aware Large Kernel Convolutions",
      "paper_id": "2002.03184v2"
    },
    {
      "index": 20,
      "title": "Iterative Back-Translation for Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "2nd Workshop on Neural Machine Translation and Generation",
      "authors": "Hoang VCD, Koehn P, Haffari G, Cohn T."
    },
    {
      "index": 21,
      "title": "Bi-Directional Neural Machine Translation with Synthetic Parallel Data",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Niu X, Denkowski M, Carpuat M.",
      "orig_title": "Bi-Directional Neural Machine Translation with Synthetic Parallel Data",
      "paper_id": "1805.11213v2"
    },
    {
      "index": 22,
      "title": "Joint Training for Neural Machine Translation Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Zhang Z, Liu S, Li M, Zhou M, Chen E."
    },
    {
      "index": 23,
      "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Burlot F, Yvon F.",
      "orig_title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
      "paper_id": "1903.11437v1"
    },
    {
      "index": 24,
      "title": "Generalizing Back-Translation in Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Graca M, Kim Y, Schamper J, Khadivi S, Ney H."
    },
    {
      "index": 25,
      "title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Fadaee M, Monz C.",
      "orig_title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
      "paper_id": "1808.09006v2"
    },
    {
      "index": 26,
      "title": "Investigating Backtranslation in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Poncelas A, Shterionov D, Way A, Maillette de Buy GW, Passban P.",
      "orig_title": "Investigating Backtranslation in Neural Machine Translation",
      "paper_id": "1804.06189v1"
    },
    {
      "index": 27,
      "title": "Using Monolingual Source-Language Data to Improve MT Performance",
      "abstract": "",
      "year": "2006",
      "venue": "International Workshop on Spoken Language Translation",
      "authors": "Ueffing N."
    },
    {
      "index": 28,
      "title": "QuEst - A translation quality estimation framework",
      "abstract": "",
      "year": "2013",
      "venue": "51st ACL: System Demonstrations",
      "authors": "Specia L, Shah K, de Souza JGC, Cohn T."
    },
    {
      "index": 29,
      "title": "Convolutional Sequence to Sequence Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Gehring J, Michael A, Grangier D, Yarats D, Dauphin YN."
    },
    {
      "index": 30,
      "title": "Pay less attention with Lightweight and Dynamic Convolutions",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Wu F, Fan A, Baevski A, Dauphin YN, Auli M.",
      "orig_title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
      "paper_id": "1901.10430v2"
    },
    {
      "index": 31,
      "title": "Universal Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser Ł.",
      "orig_title": "Universal Transformers",
      "paper_id": "1807.03819v3"
    },
    {
      "index": 32,
      "title": "Sequence to Sequence Learning with Neural Networks",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS",
      "authors": "Sutskever I, Vinyals O, Le QV."
    },
    {
      "index": 33,
      "title": "Long Short-Term Memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "Hochreiter S, Schmidhuber J."
    },
    {
      "index": 34,
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "Cho K, van Merrienboer B, Gülçehre Ç, Bougares F, Schwenk H, Bengio Y."
    },
    {
      "index": 35,
      "title": "On integrating a language model into neural machine translation",
      "abstract": "",
      "year": "2017",
      "venue": "Computer Speech & Language",
      "authors": "Gulcehre C, Firat O, Xu K, Cho K, Bengio Y."
    },
    {
      "index": 36,
      "title": "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Second Conference on Machine Translation",
      "authors": "Currey A, Miceli Barone AV, Heafield K."
    },
    {
      "index": 37,
      "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Sennrich R, Haddow B, Birch A."
    },
    {
      "index": 38,
      "title": "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Johnson M, Schuster M, Le QV, Krikun M, Wu Y, Chen Z, et al.",
      "orig_title": "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
      "paper_id": "1611.04558v2"
    },
    {
      "index": 39,
      "title": "Tag-less Back-Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Abdulmumin I, Galadanci BS, Garba A."
    },
    {
      "index": 40,
      "title": "Report on the 11th IWSLT Evaluation Campaign, IWSLT 2014",
      "abstract": "",
      "year": "2014",
      "venue": "11th Workshop on Spoken Language Translation",
      "authors": "Cettolo M, Niehues J, Stüker S, Bentivogli L, Federico M."
    },
    {
      "index": 41,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Ranzato M, Chopra S, Auli M, Zaremba W.",
      "orig_title": "Sequence Level Training with Recurrent Neural Networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 42,
      "title": "WIT3: Web Inventory of Transcribed and Translated Talks",
      "abstract": "",
      "year": "2012",
      "venue": "Conference of European Association for Machine Translation",
      "authors": "Cettolo M, Christian G, Federico M."
    },
    {
      "index": 43,
      "title": "Findings of the 2017 Conference on Machine Translation (WMT17)",
      "abstract": "",
      "year": "2017",
      "venue": "Second Conference on Machine Translation, Volume 2: Shared Task Papers",
      "authors": "Bojar O, Chatterjee R, Federmann C, Graham Y, Haddow B, Huang S, et al."
    },
    {
      "index": 44,
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Sennrich R, Haddow B, Birch A."
    },
    {
      "index": 45,
      "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Klein G, Kim Y, Deng Y, Senellart J, Rush AM.",
      "orig_title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
      "paper_id": "1709.03815v1"
    },
    {
      "index": 46,
      "title": "TensorFlow: A system for large-scale machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "12th USENIX Conference on Operating Systems Design and Implementation",
      "authors": "Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al.",
      "orig_title": "TensorFlow: A System for Large-scale Machine Learning",
      "paper_id": "1605.08695v2"
    },
    {
      "index": 47,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Kingma DP, Ba J."
    }
  ]
}