{
  "paper_id": "2010.09672v2",
  "title": "Multi-Stage Fusion for One-Click Segmentation",
  "sections": {
    "tap-and-shoot segmentation": "Following , we use MSRA10K 7 for training and partition the dataset into three non-overlapping subsets of 800080008000, 100010001000 and 100010001000 images as our training, validation and test set. We report the mIoU after training for 16K iterations and again after network convergence (at 43k iterations for us, vs. 260k iterations in ) in Table 1. During training, we resize the images to 512×512512512512\\times 512 pixels. This choice of resolution is driven primarily by matching the resolution to that of the training images for the ResNet-101 backbone [ref]21. The -baseline models are trained using only the 333-channel RGB image and the instance ground truth mask without any user click transformations. The -early models use Gaussian guidance maps ; the network input is 333-channel RGB image and Gaussian encoding of the user’s tap on the object of interest (Fig. 2(a)). The -multi models refer to the multi-stage fusion models with Gaussian encoding of user clicks. Note that we do not train a late-fusion model; standalone late-fusion models show inferior performance compared to their early-fusion counterparts [ref]7. From Table. 1, we observe that our trained network converges mostly within 16K iterations. For simplistic datasets such as MSRA10K and ECSSD, the vgg-baseline without user click transformation compares favourably with the approach of  at the same training resolution of 256×256256256256\\times 256. resnet-baseline models trained with 512×512512512512\\times 512 images significantly outperform  reporting absolute mIoU gains of till 7%percent77\\% across the datasets. Based on this result alone, we conclude that one-click (and standard) interactive segmentation approaches should be benchmarked on more challenging datasets. Examples include PASCAL VOC 2012 and MS COCO, which feature cluttered scenes, multiple objects, occlusions and challenging lighting conditions. (see Table 3). Furthermore, with only the Gaussian transformation and ResNet-101 backbone trained on 512×512512512512\\times 512, we are able to achieve mIoU increase in the range of 555-11%percent1111\\% across datasets at convergence w.r.t . Having the multi-stage fusion offers us absolute mIoU gains of 111-4%percent44\\% w.r.t the early fusion variant (resnet-early vs. resnet-multi when trained with 512×512512512512\\times 512 images). Additionally, our resnet models require significantly less memory; 195.8195.8195.8 MB (stored as 323232-bit/444-byte floating point numbers) instead of the 652.45652.45652.45 MB required for the segmentation network of ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Tap and shoot segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, and Long-Wen Chang"
    },
    {
      "index": 1,
      "title": "Deep interactive object selection",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang"
    },
    {
      "index": 2,
      "title": "Iteratively Trained Interactive Segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "BMVC",
      "authors": "Sabarinath Mahadevan, Paul Voigtlaender, and Bastian Leibe",
      "orig_title": "Iteratively trained interactive segmentation",
      "paper_id": "1805.04398v1"
    },
    {
      "index": 3,
      "title": "Content-aware multi-level guidance for interactive instance segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Soumajit Majumder and Angela Yao"
    },
    {
      "index": 4,
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "Yang Hu, Andrea Soltoggio, Russell Lock, and Steve Carter",
      "orig_title": "A fully convolutional two-stream fusion network for interactive image segmentation",
      "paper_id": "1807.02480v2"
    },
    {
      "index": 5,
      "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu",
      "orig_title": "Semantic image synthesis with spatially-adaptive normalization",
      "paper_id": "1903.07291v2"
    },
    {
      "index": 6,
      "title": "Few-Shot Segmentation Propagation with Guided Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07373",
      "authors": "Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A Efros, and Sergey Levine",
      "orig_title": "Few-shot segmentation propagation with guided networks",
      "paper_id": "1806.07373v1"
    },
    {
      "index": 7,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 8,
      "title": "Intelligent scissors for image composition",
      "abstract": "",
      "year": "1995",
      "venue": "SIGGRAPH",
      "authors": "Eric. N. Mortensen and William. A. Barrett"
    },
    {
      "index": 9,
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Jonathan Long, Evan Shelhamer, and Trevor Darrell",
      "orig_title": "Fully convolutional networks for semantic segmentation",
      "paper_id": "1605.06211v1"
    },
    {
      "index": 10,
      "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam",
      "orig_title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
      "paper_id": "1802.02611v3"
    },
    {
      "index": 11,
      "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
      "abstract": "",
      "year": "2018",
      "venue": "TPAMI",
      "authors": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille"
    },
    {
      "index": 12,
      "title": "Interactive graph cuts for optimal boundary & region segmentation of objects in nd images",
      "abstract": "",
      "year": "2001",
      "venue": "ICCV",
      "authors": "Yuri Y Boykov and M-P Jolly"
    },
    {
      "index": 13,
      "title": "Grabcut: Interactive foreground extraction using iterated graph cuts",
      "abstract": "",
      "year": "2004",
      "venue": "ACM transactions on graphics (TOG)",
      "authors": "Carsten Rother, Vladimir Kolmogorov, and Andrew Blake"
    },
    {
      "index": 14,
      "title": "Geodesic star convexity for interactive image segmentation",
      "abstract": "",
      "year": "2010",
      "venue": "CVPR",
      "authors": "Varun Gulshan, Carsten Rother, Antonio Criminisi, Andrew Blake, and Andrew Zisserman"
    },
    {
      "index": 15,
      "title": "Hierarchical image saliency detection on extended cssd",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE TPAMI",
      "authors": "Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia"
    },
    {
      "index": 16,
      "title": "Global contrast based salient region detection",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE TPAMI",
      "authors": "Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu"
    },
    {
      "index": 17,
      "title": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild",
      "abstract": "",
      "year": "2017",
      "venue": "19th ACM International Conference on Multimodal Interaction",
      "authors": "Valentin Vielzeuf, Stéphane Pateux, and Frédéric Jurie",
      "orig_title": "Temporal multimodal fusion for video emotion classification in the wild",
      "paper_id": "1709.07200v1"
    },
    {
      "index": 18,
      "title": "A late fusion cnn for digital matting",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu"
    },
    {
      "index": 19,
      "title": "Nuclei segmentation via a deep panoptic model with semantic feature fusion",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Dongnan Liu, Donghao Zhang, Yang Song, Chaoyi Zhang, Fan Zhang, Lauren O’Donnell, and Weidong Cai"
    },
    {
      "index": 20,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 21,
      "title": "Pyramid Scene Parsing Network",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia",
      "orig_title": "Pyramid scene parsing network",
      "paper_id": "1612.01105v2"
    },
    {
      "index": 22,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Jie Hu, Li Shen, and Gang Sun",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 23,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 24,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft COCO: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 25,
      "title": "Large-scale interactive object segmentation with human annotators",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari",
      "orig_title": "Large-scale interactive object segmentation with human annotators",
      "paper_id": "1903.10830v2"
    },
    {
      "index": 26,
      "title": "A comparative evaluation of interactive segmentation algorithms",
      "abstract": "",
      "year": "2010",
      "venue": "Pattern Recognition",
      "authors": "Kevin McGuinness and Noel E O’connor"
    },
    {
      "index": 27,
      "title": "The pascal visual object classes (voc) challenge",
      "abstract": "",
      "year": "2010",
      "venue": "IJCV",
      "authors": "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman"
    },
    {
      "index": 28,
      "title": "Semantic contours from inverse detectors",
      "abstract": "",
      "year": "2011",
      "venue": "ICCV",
      "authors": "Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik"
    },
    {
      "index": 29,
      "title": "Geodesic matting: A framework for fast interactive image and video segmentation and matting",
      "abstract": "",
      "year": "2009",
      "venue": "IJCV",
      "authors": "Xue Bai and Guillermo Sapiro"
    }
  ]
}