{
  "paper_id": "2206.06359v1",
  "title": "EnergyMatch: Energy-based Pseudo-Labeling for Semi-Supervised Learning",
  "sections": {
    "experiments": "We evaluate our approach on two SSL settings: (1) the imbalanced setting with long-tailed class distributions, which we believe reflects many real-world scenarios; and (2) the standard class-balanced setting, which assumes a balanced distribution of classes. Our evaluation follows standard protocols as in  0. We also conduct ablation studies to analyze important design choices of our method. Datasets. We evaluate on several SSL image classification benchmarks. For the imbalanced setting, we use CIFAR10-LT and CIFAR100-LT [ref]17, which are long-tailed variants of the original CIFAR datasets. We follow prior work in long-tail SSL  [ref]21, and use an exponential imbalance function  to create the long-tailed version of CIFAR10 and CIFAR100. We select 10% and 30% data from each class as the labeled set for CIFAR10-LT and CIFAR100-LT, respectively. We experiment with imbalance ratio from 50 to 200 for CIFAR10-LT and from 50 to 100 for CIFAR100-LT.\nDetails of constructing these datasets can be found in Appendix A.2. For the standard SSL evaluation, we use balanced CIFAR10/100 [ref]17, whose labeled and unlabeled sets are constructed in a balanced fashion, SVHN , and STL-10  using the standard train and test splits. Baselines. We compare to the latest methods developed for long-tailed SSL (DARP , CReST , and ABC [ref]21) and for balanced SSL (UDA , UPS , FixMatch , and FlexMatch 0). All methods except UPS use the strong-weak data augmentation paradigm and our implementation makes the data augmentation operation consistent across different methods. FixMatch and UDA use fixed confidence thresholding with hard pseudo-labels and soft pseudo-labels, respestively. FlexMatch dynamically adjusts thresholds for different classes to (roughly) balance the pseudo-labeled samples per class. UPS adopts an uncertainty metric through MC-Dropout  by running forward pass 10 times and computing the standard deviation of outputs. This practice is very time consuming in modern SSL frameworks as the pseudo-labels are produced on-the-fly at each iteration. Therefore, for the balanced SSL setting, we report the CIFAR results from the original paper. For the imbalanced setting, we incorporate UPS into FixMatch (denoted as FixMatch-UPS) and only evaluate it on CIFAR10-LT due to its high computation cost. For SSL-LT baselines, DARP refines pseudo-labels via convex optimization designed for the imbalance scenario. CReST tries to alleviate the imbalance via moving unlabeled samples to labeled sets with probabilities inversely proportional to class frequency and restart training. ABC, in contrast, implicitly balances the classifier by introducing an auxiliary classifier trained with balanced sampling.\nWe implement our energy-based pseudo-labeling in the framework of FixMatch for its simplicity and denote our method as EnergyMatch. Implementation Details. We implement our method in the open-source SSL codebase TorchSSL 0 and conduct each experiment with three runs using different random seeds. For a fair comparison, unless otherwise specified, the baseline results and our results are generated with the same codebase, same random seeds, same data splits, and same network architecture.\nFollowing prior work  0, we use Wide ResNet-28-2  with 1.5M parameters for Cifar10 and SVHN, WRN-28-8 for CIFAR-100, and WRN-37-2 for STL-10. All methods are trained with SGD with momentum of 0.9. For the balanced datasets, we use an initial learning rate of 0.03 and the total training iterations is set to 220superscript2202^{20} with a cosine learning rate schedule of 716716\\frac{7}{16} cycle. We use a constant learning rate for long-tailed datsets as we found using cosine decay leads to worse performance for all methods. To be consistent with our baselines  0 , we use a batch size of 64 for labeled data, 7x larger batch size for unlabeled data, exponential moving average with a momentum of 0.999 for inference, random horizontal flip for weak augmentation and RandAugment  and CutOut  for strong augmentation. Our code will be made publicly available. More training details for each experiment, hyper-parameter choices, and an anonymous link to our code base can be found in Appendix A.1."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "MixMatch: A holistic approach to semi-supervised learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel"
    },
    {
      "index": 1,
      "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel",
      "orig_title": "ReMixMatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
      "paper_id": "1911.09785v2"
    },
    {
      "index": 2,
      "title": "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "O. Chapelle, B. Scholkopf, and A. Zien"
    },
    {
      "index": 3,
      "title": "An analysis of single-layer networks in unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "fourteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings",
      "authors": "A. Coates, A. Ng, and H. Lee"
    },
    {
      "index": 4,
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le"
    },
    {
      "index": 5,
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie",
      "orig_title": "Class-balanced loss based on effective number of samples",
      "paper_id": "1901.05555v1"
    },
    {
      "index": 6,
      "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04552",
      "authors": "T. DeVries and G. W. Taylor",
      "orig_title": "Improved regularization of convolutional neural networks with cutout",
      "paper_id": "1708.04552v2"
    },
    {
      "index": 7,
      "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning, PMLR",
      "authors": "Y. Gal and Z. Ghahramani",
      "orig_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "paper_id": "1506.02142v6"
    },
    {
      "index": 8,
      "title": "Semi-supervised learning by entropy minimization",
      "abstract": "",
      "year": "2004",
      "venue": "Advances in neural information processing systems",
      "authors": "Y. Grandvalet and Y. Bengio"
    },
    {
      "index": 9,
      "title": "Your classifier is secretly an energy based model and you should treat it like one",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "W. Grathwohl, K.-C. Wang, J.-H. Jacobsen, D. Duvenaud, M. Norouzi, and K. Swersky",
      "orig_title": "Your classifier is secretly an energy based model and you should treat it like one",
      "paper_id": "1912.03263v3"
    },
    {
      "index": 10,
      "title": "On Calibration of Modern Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning, PMLR",
      "authors": "C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger",
      "orig_title": "On calibration of modern neural networks",
      "paper_id": "1706.04599v2"
    },
    {
      "index": 11,
      "title": "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "M. Hein, M. Andriushchenko, and J. Bitterwolf",
      "orig_title": "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
      "paper_id": "1812.05720v2"
    },
    {
      "index": 12,
      "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Hendrycks and K. Gimpel",
      "orig_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "paper_id": "1610.02136v3"
    },
    {
      "index": 13,
      "title": "Label Propagation for Deep Semi-supervised Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Iscen, G. Tolias, Y. Avrithis, and O. Chum",
      "orig_title": "Label propagation for deep semi-supervised learning",
      "paper_id": "1904.04717v1"
    },
    {
      "index": 14,
      "title": "Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Kim, Y. Hur, S. Park, E. Yang, S. J. Hwang, and J. Shin",
      "orig_title": "Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning",
      "paper_id": "2007.08844v2"
    },
    {
      "index": 15,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 16,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "Technical report, University of Toronto",
      "authors": "A. Krizhevsky and G. Hinton"
    },
    {
      "index": 17,
      "title": "Temporal Ensembling for Semi-Supervised Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "S. Laine and T. Aila",
      "orig_title": "Temporal ensembling for semi-supervised learning",
      "paper_id": "1610.02242v3"
    },
    {
      "index": 18,
      "title": "A tutorial on energy-based learning",
      "abstract": "",
      "year": "2006",
      "venue": "Predicting structured data",
      "authors": "Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang"
    },
    {
      "index": 19,
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Workshop on challenges in representation learning, ICML",
      "authors": "D.-H. Lee et al."
    },
    {
      "index": 20,
      "title": "ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "H. Lee, S. Shin, and H. Kim",
      "orig_title": "ABC: Auxiliary balanced classifier for class-imbalanced semi-supervised learning",
      "paper_id": "2110.10368v1"
    },
    {
      "index": 21,
      "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. Liang, Y. Li, and R. Srikant",
      "orig_title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
      "paper_id": "1706.02690v5"
    },
    {
      "index": 22,
      "title": "Energy-based Out-of-distribution Detection",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "W. Liu, X. Wang, J. Owens, and Y. Li",
      "orig_title": "Energy-based out-of-distribution detection",
      "paper_id": "2010.03759v4"
    },
    {
      "index": 23,
      "title": "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis",
      "abstract": "",
      "year": "1975",
      "venue": "Journal of the American Statistical Association",
      "authors": "G. J. McLachlan"
    },
    {
      "index": 24,
      "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii",
      "orig_title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
      "paper_id": "1704.03976v2"
    },
    {
      "index": 25,
      "title": "Reading digits in natural images with unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "Workshop on Deep Learning and Unsupervised Feature Learning, NIPS",
      "authors": "Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng"
    },
    {
      "index": 26,
      "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "A. Nguyen, J. Yosinski, and J. Clune",
      "orig_title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "paper_id": "1412.1897v4"
    },
    {
      "index": 27,
      "title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Goodfellow",
      "orig_title": "Realistic evaluation of deep semi-supervised learning algorithms",
      "paper_id": "1804.09170v4"
    },
    {
      "index": 28,
      "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "M. N. Rizve, K. Duarte, Y. S. Rawat, and M. Shah",
      "orig_title": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
      "paper_id": "2101.06329v3"
    },
    {
      "index": 29,
      "title": "Semi-supervised self-training of object detection models",
      "abstract": "",
      "year": "2005",
      "venue": "Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION’05)-Volume 1",
      "authors": "C. Rosenberg, M. Hebert, and H. Schneiderman"
    },
    {
      "index": 30,
      "title": "Probability of error of some adaptive pattern-recognition machines",
      "abstract": "",
      "year": "1965",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "H. Scudder"
    },
    {
      "index": 31,
      "title": "FixMatch: Simplifying semi-supervised learning with consistency and confidence",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li"
    },
    {
      "index": 32,
      "title": "Graph-based semi-supervised learning: A comprehensive review",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Z. Song, X. Yang, Z. Xu, and I. King"
    },
    {
      "index": 33,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus"
    },
    {
      "index": 34,
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Tarvainen and H. Valpola",
      "orig_title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "paper_id": "1703.01780v6"
    },
    {
      "index": 35,
      "title": "CReST: A class-rebalancing self-training framework for imbalanced semi-supervised learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "C. Wei, K. Sohn, C. Mellina, A. Yuille, and F. Yang"
    },
    {
      "index": 36,
      "title": "Unsupervised data augmentation for consistency training",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le"
    },
    {
      "index": 37,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le"
    },
    {
      "index": 38,
      "title": "Wide Residual Networks",
      "abstract": "",
      "year": "2016",
      "venue": "British Machine Vision Conference, British Machine Vision Association",
      "authors": "S. Zagoruyko and N. Komodakis",
      "orig_title": "Wide residual networks",
      "paper_id": "1605.07146v4"
    },
    {
      "index": 39,
      "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and T. Shinozaki",
      "orig_title": "FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "paper_id": "2110.08263v3"
    },
    {
      "index": 40,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 41,
      "title": "Learning from labeled and unlabeled data with label propagation",
      "abstract": "",
      "year": "2002",
      "venue": "Technical report, CMU-CALD-02-107, Carnegie Mellon University",
      "authors": "X. Zhu and Z. Ghahramani"
    },
    {
      "index": 42,
      "title": "Introduction to semi-supervised learning",
      "abstract": "",
      "year": "2009",
      "venue": "Synthesis lectures on artificial intelligence and machine learning",
      "authors": "X. Zhu and A. B. Goldberg"
    },
    {
      "index": 43,
      "title": "Semi-supervised learning literature survey",
      "abstract": "",
      "year": "2005",
      "venue": "Technical report, University of Wisconsin-Madison Department of Computer Sciences",
      "authors": "X. J. Zhu"
    }
  ]
}