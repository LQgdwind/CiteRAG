{
  "paper_id": "2411.08735v2",
  "title": "New advances in universal approximation with neural networks of minimal width",
  "sections": {
    "conclusion and outlook": "The minimal widths of FNNs with ReLU or leaky ReLU activations for universal approximation properties have been specified exactly by Park et al. , Cai [ref]5 and our 1. Note additionally that, due to the coding scheme construction and because the memorizer has dimension one, the universal approximation results of Park et al.  and ours have minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1. Therefore, while Cai [ref]5 first proved the minimal width for leaky ReLU FNNs, our alternative proof of the upper bound of the minimal width in 1 constructs the approximating sequences explicitly, and shows that there always exists an approximating sequence of minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1. Moreover, Cai [ref]5 use results, connected to the Theorem 2.5 (i) of Brenier and Gangbo , to infer the UAP in Lpsuperscript𝐿𝑝L^{p} with LReLU activations 1 from the UAP  with respect to flow maps for neuralODEs . We, in contrast, directly prove the UAP in Lpsuperscript𝐿𝑝L^{p} and thereby provide a constructive proof of Brenier and Gangbo’s result as a corollary of 4. For proving additional theoretical results regarding the approximation with FNNs with leaky ReLU activations, it might therefore be advantageous to consider the explicit construction obtained in the proof of our 1 and 4. For example, as we constructed the memorizer as a leaky ReLU FNN of input dimension 111, it is clear that, in general, a neural network in our approximating sequences for Lpsuperscript𝐿𝑝L^{p} integrable functions has at least one hidden layer with one neuron, corresponding to a minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1, independent of the input and output dimension dx,dy∈ℕsubscript𝑑𝑥subscript𝑑𝑦ℕd_{x},d_{y}\\in\\mathbb{N}, respectively. In particular, this allowed us to prove universal approximation for autoencoders with arbitrary interior feature dimension in Corollary 2.9. In contrast, Cai [ref]5 shows that for every Lpsuperscript𝐿𝑝L^{p} integrable function there exists an approximating sequence of LReLU FNNs of width max⁡{dx,dy,2}subscript𝑑𝑥subscript𝑑𝑦2\\max\\{d_{x},d_{y},2\\}, but more specifics of the FNNs, such as the minimal interior dimension they attain, are unclear. Moreover, note that Cai [ref]5 showed that the minimal width for universal approximation of FNNs with arbitrary activations is lower bounded by wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}, which means that FNNs with leaky ReLU activations achieve the minimal width over all possible activation functions if min⁡{dx,dy}≥2subscript𝑑𝑥subscript𝑑𝑦2\\min\\{d_{x},d_{y}\\}\\geq 2. Thus, the leaky ReLU activation achieves the minimal width for all but infinitely many choices of input and output dimensions except for the three special cases (dx,dy)∈{(1,1),(1,2),(2,1)}subscript𝑑𝑥subscript𝑑𝑦111221(d_{x},d_{y})\\in\\{(1,1),(1,2),(2,1)\\}. Moreover as the minimal width for universal approximation for FNNs with ReLU activations is given by max⁡{dx+1,dy}subscript𝑑𝑥1subscript𝑑𝑦\\max\\{d_{x}+1,d_{y}\\} by Park et al.  the ReLU fails to achieve this property in the infinitely many cases where dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}. An interesting open question lies in exactly specifying the minimal width for universal approximation of continuous functions on Euclidean spaces with respect to the supremum norm for FNNs with the commonly used activation functions, such as ReLU or leaky ReLU. Cai [ref]5 proved that wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} is the lowest minimal width that any activation function can achieve, both for uniform and Lpsuperscript𝐿𝑝L^{p} universal approximation. Furthermore, our 5 shows that dx+1subscript𝑑𝑥1d_{x}+1 is a lower bound for the minimal width for uniform universal approximation when dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}, for Lipschitz continuous monotone activations, thus including ReLU and leaky ReLU. Park et al.  showed that the minimal width for uniform universal approximation of functions in C0​( ,ℝ2)superscript𝐶001superscriptℝ2C^{0}( ,\\mathbb{R}^{2}) with ReLU FNNs is wmin=3subscript𝑤3w_{\\min}=3, hence the lower bound max⁡{dx,dy}=2subscript𝑑𝑥subscript𝑑𝑦2\\max\\{d_{x},d_{y}\\}=2 is not tight in this case. However, in general, even for ReLU and leaky ReLU activations, it is not known whether these bounds are tight, making it an important topic for further research to extract upper bounds for the universal approximation of continuous functions with respect to the supremum norm, especially for ReLU and leaky ReLU, but also for other activations of practical and theoretical interest. Furthermore, our 5 demonstrates that, in the case dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}, FNNs with leaky ReLU activations are unable to attain the smallest possible minimal width over all activation functions, given by wmin∗=max⁡{dx,dy}=dxsuperscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦subscript𝑑𝑥w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}=d_{x}, while they achieve it for Lpsuperscript𝐿𝑝L^{p} universal approximation of Lpsuperscript𝐿𝑝L^{p} integrable functions when dx≥2subscript𝑑𝑥2d_{x}\\geq 2 and dy≥2subscript𝑑𝑦2d_{y}\\geq 2, as shown in 1 and by Cai [ref]5. Moreover, it should be noted that Cai [ref]5 proved that the minimal width for uniform universal approximation of continuous functions with FNNs with ReLU+FLOOR activations is given by wmin=max⁡{2,dx,dy}subscript𝑤2subscript𝑑𝑥subscript𝑑𝑦w_{\\min}=\\max\\{2,d_{x},d_{y}\\}. Nevertheless, despite this result being optimal for approximation in theory, it is difficult for practical purposes, as using discontinuous activations in neural networks fundamentally breaks backpropagation and gradient descent. These methods are designed for smooth functions for computing gradients  9, and while there are workarounds, they might be inefficient and impractical for training deep networks with discontinuous activations. This motivates the question of whether there is another continuous (almost everywhere) smooth activation that achieves the minimal width for the uniform universal approximation of continuous functions. According to our 5, such an activation cannot be Lipschitz continuous and monotone, which excludes most of the popular choices 2 , such as ReLU, LReLU, tangens hyperbolicus, logistic sigmoid and many other sigmoidal activations. Some possible candidates for relevant activation functions, as e.g. presented by Dubey et al. 2, that might still be able to achieve uniform UAP of continuous functions with a width of max⁡{dx,dy}subscript𝑑𝑥subscript𝑑𝑦\\max\\{d_{x},d_{y}\\}, could include the swish activation given by swish⁡(x)=x1+exp⁡(−x)swish𝑥𝑥1𝑥\\operatorname{swish}(x)=\\frac{x}{1+\\exp(-x)} or the adaptive piecewise linear (APL) activation, which is more adaptive due to learnable parameters similar to LReLU, both of which are not monotone. Additionally, as of now, the leaky ReLUs are the only known activation functions to achieve universal approximation for Lpsuperscript𝐿𝑝L^{p} integrable functions with the minimal width wmin∗superscriptsubscript𝑤w_{\\min}^{*} over all possible activation functions for all but three choices of input and output dimensions. Therefore another interesting open question for future work is whether there exist other activation functions, besides the leaky ReLU, which achieve the minimal width for Lpsuperscript𝐿𝑝L^{p} universal approximation for all but finitely many choices of the dimensions, or perhaps even for all input and output dimensions. Note that ReLU does not accomplish this, since Park et al.  showed that FNNs with ReLU activations for Lpsuperscript𝐿𝑝L^{p} universal approximation of the Lpsuperscript𝐿𝑝L^{p} integrable function have a minimal width of max⁡{dx+1,dy}subscript𝑑𝑥1subscript𝑑𝑦\\max\\{d_{x}+1,d_{y}\\} and therefore cannot achieve the smallest possible width of wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} if dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Analysis III",
      "abstract": "",
      "year": "2008",
      "venue": "Grundstudium Mathematik. Birkhäuser Basel",
      "authors": "H. Amann and J. Escher"
    },
    {
      "index": 1,
      "title": "Learning Deep Architectures for AI",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Y. Bengio"
    },
    {
      "index": 2,
      "title": "lpsuperscript𝑙𝑝l^{p} approximation of maps by diffeomorphisms",
      "abstract": "",
      "year": "2001",
      "venue": "Calculus of Variations and Partial Differential Equations",
      "authors": "Y. Brenier and W. Gangbo"
    },
    {
      "index": 3,
      "title": "Functional Analysis, Sobolev Spaces and Partial Differential Equations",
      "abstract": "",
      "year": "2010",
      "venue": "Universitext. Springer New York",
      "authors": "H. Brezis"
    },
    {
      "index": 4,
      "title": "Achieve the minimum width of neural networks for universal approximation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Y. Cai"
    },
    {
      "index": 5,
      "title": "LU-Net: Invertible Neural Networks Based on Matrix Factorization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "R. Chan, S. Penquitt, and H. Gottschalk",
      "orig_title": "Lu-net: Invertible neural networks based on matrix factorization",
      "paper_id": "2302.10524v1"
    },
    {
      "index": 6,
      "title": "Neural Ordinary Differential Equations",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud",
      "orig_title": "Neural ordinary differential equations",
      "paper_id": "1806.07366v5"
    },
    {
      "index": 7,
      "title": "Approximation by superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of Control, Signals, and Systems (MCSS)",
      "authors": "G. Cybenko"
    },
    {
      "index": 8,
      "title": "Activation functions and their characteristics in deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Chinese Control And Decision Conference (CCDC)",
      "authors": "B. Ding, H. Qian, and J. Zhou"
    },
    {
      "index": 9,
      "title": "Density estimation using Real NVP",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "L. Dinh, J. Sohl-Dickstein, and S. Bengio",
      "orig_title": "Density estimation using real NVP",
      "paper_id": "1605.08803v3"
    },
    {
      "index": 10,
      "title": "Vanilla feedforward neural networks as a discretization of dynamic systems",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "Y. Duan, L. Li, G. Ji, and Y. Cai",
      "orig_title": "Vanilla feedforward neural networks as a discretization of dynamical systems",
      "paper_id": "2209.10909v3"
    },
    {
      "index": 11,
      "title": "Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "S. R. Dubey, S. K. Singh, and B. B. Chaudhuri",
      "orig_title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "paper_id": "2109.14545v3"
    },
    {
      "index": 12,
      "title": "Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "S. R. Dubey, S. K. Singh, and B. B. Chaudhuri",
      "orig_title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "paper_id": "2109.14545v3"
    },
    {
      "index": 13,
      "title": "Real Analysis",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "D. B. Emmanuele"
    },
    {
      "index": 14,
      "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "B. Hanin and M. Sellke",
      "orig_title": "Approximating continuous functions by relu nets of minimal width",
      "paper_id": "1710.11278v2"
    },
    {
      "index": 15,
      "title": "Analysis 2",
      "abstract": "",
      "year": "2013",
      "venue": "Springer-Lehrbuch. Springer Berlin Heidelberg",
      "authors": "S. Hildebrandt"
    },
    {
      "index": 16,
      "title": "Multilayer feedforward networks are universal approximators",
      "abstract": "",
      "year": "1989",
      "venue": "Neural Networks",
      "authors": "K. Hornik, M. Stinchcombe, and H. White"
    },
    {
      "index": 17,
      "title": "Deep, skinny neural networks are not universal approximators",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "J. Johnson"
    },
    {
      "index": 18,
      "title": "Universal Approximation with Deep Narrow Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "P. Kidger and T. Lyons",
      "orig_title": "Universal approximation with deep narrow networks",
      "paper_id": "1905.08539v2"
    },
    {
      "index": 19,
      "title": "Glow: Generative Flow with Invertible 1×1 Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "D. P. Kingma and P. Dhariwal",
      "orig_title": "Glow: Generative flow with invertible 1x1 convolutions",
      "paper_id": "1807.03039v2"
    },
    {
      "index": 20,
      "title": "Normalizing Flows: An Introduction and Review of Current Methods",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "I. Kobyzev, S. J. Prince, and M. A. Brubaker",
      "orig_title": "Normalizing flows: An introduction and review of current methods",
      "paper_id": "1908.09257v4"
    },
    {
      "index": 21,
      "title": "Deep Learning via Dynamical Systems: An Approximation Perspective",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Q. Li, T. Lin, and Z. Shen",
      "orig_title": "Deep learning via dynamical systems: An approximation perspective",
      "paper_id": "1912.10382v2"
    },
    {
      "index": 22,
      "title": "Linear Algebra",
      "abstract": "",
      "year": "2015",
      "venue": "Springer Undergraduate Mathematics Series. Springer International Publishing",
      "authors": "J. Liesen and V. Mehrmann"
    },
    {
      "index": 23,
      "title": "The Expressive Power of Neural Networks: A View from the Width",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang",
      "orig_title": "The expressive power of neural networks: A view from the width",
      "paper_id": "1709.02540v3"
    },
    {
      "index": 24,
      "title": "Distribution learning via neural differential equations: a nonparametric statistical perspective",
      "abstract": "",
      "year": "2024",
      "venue": "Journal of Machine Learning Research",
      "authors": "Y. Marzouk, Z. R. Ren, S. Wang, and J. Zech",
      "orig_title": "Distribution learning via neural differential equations: a nonparametric statistical perspective",
      "paper_id": "2309.01043v1"
    },
    {
      "index": 25,
      "title": "Neural networks and deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "M. A. Nielsen"
    },
    {
      "index": 26,
      "title": "Applied Nonlinear Functional Analysis",
      "abstract": "",
      "year": "2018",
      "venue": "De Gruyter, Berlin, Boston",
      "authors": "N. S. Papageorgiou and P. Winkert"
    },
    {
      "index": 27,
      "title": "Minimum width for universal approximation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "S. Park, C. Yun, J. Lee, and J. Shin"
    },
    {
      "index": 28,
      "title": "On the difficulty of training recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "30th International Conference on Machine Learning, ICML’13",
      "authors": "R. Pascanu, T. Mikolov, and Y. Bengio"
    },
    {
      "index": 29,
      "title": "Approximation theory of the mlp model in neural networks",
      "abstract": "",
      "year": "1999",
      "venue": "Acta Numerica",
      "authors": "A. Pinkus"
    },
    {
      "index": 30,
      "title": "Intermediate Calculus",
      "abstract": "",
      "year": "2012",
      "venue": "Undergraduate Texts in Mathematics. Springer New York",
      "authors": "M. Protter and C. Morrey"
    },
    {
      "index": 31,
      "title": "Remarks on a Multivariate Transformation",
      "abstract": "",
      "year": "1952",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "M. Rosenblatt"
    },
    {
      "index": 32,
      "title": "Neural ode control for classification, approximation, and transport",
      "abstract": "",
      "year": "2023",
      "venue": "SIAM Review",
      "authors": "D. Ruiz-Balet and E. Zuazua"
    },
    {
      "index": 33,
      "title": "Optimal transport for applied mathematicians",
      "abstract": "",
      "year": "2015",
      "venue": "Birkäuser, NY",
      "authors": "F. Santambrogio"
    },
    {
      "index": 34,
      "title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "T. Teshima, I. Ishikawa, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama",
      "orig_title": "Coupling-based invertible neural networks are universal diffeomorphism approximators",
      "paper_id": "2006.11469v2"
    },
    {
      "index": 35,
      "title": "Optimal transport: old and new",
      "abstract": "",
      "year": "2009",
      "venue": "Springer",
      "authors": "C. Villani et al."
    }
  ]
}