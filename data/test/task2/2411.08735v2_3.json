{
  "paper_id": "2411.08735v2",
  "title": "New advances in universal approximation with neural networks of minimal width",
  "sections": {
    "conclusion and outlook": "The minimal widths of FNNs with ReLU or leaky ReLU activations for universal approximation properties have been specified exactly by Park etÂ al. , Cai [ref]5 and our 1. Note additionally that, due to the coding scheme construction and because the memorizer has dimension one, the universal approximation results of Park etÂ al.  and ours have minimal interior dimension dmin=1subscriptğ‘‘1d_{\\min}=1. Therefore, while Cai [ref]5 first proved the minimal width for leaky ReLU FNNs, our alternative proof of the upper bound of the minimal width in 1 constructs the approximating sequences explicitly, and shows that there always exists an approximating sequence of minimal interior dimension dmin=1subscriptğ‘‘1d_{\\min}=1. Moreover, Cai [ref]5 use results, connected to the Theorem 2.5 (i) of Brenier and Gangbo , to infer the UAP in Lpsuperscriptğ¿ğ‘L^{p} with LReLU activations 1 from the UAP  with respect to flow maps for neuralODEs . We, in contrast, directly prove the UAP in Lpsuperscriptğ¿ğ‘L^{p} and thereby provide a constructive proof of Brenier and Gangboâ€™s result as a corollary of 4. For proving additional theoretical results regarding the approximation with FNNs with leaky ReLU activations, it might therefore be advantageous to consider the explicit construction obtained in the proof of our 1 and 4. For example, as we constructed the memorizer as a leaky ReLU FNN of input dimension 111, it is clear that, in general, a neural network in our approximating sequences for Lpsuperscriptğ¿ğ‘L^{p} integrable functions has at least one hidden layer with one neuron, corresponding to a minimal interior dimension dmin=1subscriptğ‘‘1d_{\\min}=1, independent of the input and output dimension dx,dyâˆˆâ„•subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦â„•d_{x},d_{y}\\in\\mathbb{N}, respectively. In particular, this allowed us to prove universal approximation for autoencoders with arbitrary interior feature dimension in CorollaryÂ 2.9. In contrast, Cai [ref]5 shows that for every Lpsuperscriptğ¿ğ‘L^{p} integrable function there exists an approximating sequence of LReLU FNNs of width maxâ¡{dx,dy,2}subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦2\\max\\{d_{x},d_{y},2\\}, but more specifics of the FNNs, such as the minimal interior dimension they attain, are unclear. Moreover, note that Cai [ref]5 showed that the minimal width for universal approximation of FNNs with arbitrary activations is lower bounded by wminâˆ—=maxâ¡{dx,dy}superscriptsubscriptğ‘¤subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}, which means that FNNs with leaky ReLU activations achieve the minimal width over all possible activation functions if minâ¡{dx,dy}â‰¥2subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦2\\min\\{d_{x},d_{y}\\}\\geq 2. Thus, the leaky ReLU activation achieves the minimal width for all but infinitely many choices of input and output dimensions except for the three special cases (dx,dy)âˆˆ{(1,1),(1,2),(2,1)}subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦111221(d_{x},d_{y})\\in\\{(1,1),(1,2),(2,1)\\}. Moreover as the minimal width for universal approximation for FNNs with ReLU activations is given by maxâ¡{dx+1,dy}subscriptğ‘‘ğ‘¥1subscriptğ‘‘ğ‘¦\\max\\{d_{x}+1,d_{y}\\} by Park etÂ al.  the ReLU fails to achieve this property in the infinitely many cases where dxâ‰¥dysubscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦d_{x}\\geq d_{y}. An interesting open question lies in exactly specifying the minimal width for universal approximation of continuous functions on Euclidean spaces with respect to the supremum norm for FNNs with the commonly used activation functions, such as ReLU or leaky ReLU. Cai [ref]5 proved that wminâˆ—=maxâ¡{dx,dy}superscriptsubscriptğ‘¤subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} is the lowest minimal width that any activation function can achieve, both for uniform and Lpsuperscriptğ¿ğ‘L^{p} universal approximation. Furthermore, our 5 shows that dx+1subscriptğ‘‘ğ‘¥1d_{x}+1 is a lower bound for the minimal width for uniform universal approximation when dxâ‰¥dysubscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦d_{x}\\geq d_{y}, for Lipschitz continuous monotone activations, thus including ReLU and leaky ReLU. Park etÂ al.  showed that the minimal width for uniform universal approximation of functions in C0â€‹( ,â„2)superscriptğ¶001superscriptâ„2C^{0}( ,\\mathbb{R}^{2}) with ReLU FNNs is wmin=3subscriptğ‘¤3w_{\\min}=3, hence the lower bound maxâ¡{dx,dy}=2subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦2\\max\\{d_{x},d_{y}\\}=2 is not tight in this case. However, in general, even for ReLU and leaky ReLU activations, it is not known whether these bounds are tight, making it an important topic for further research to extract upper bounds for the universal approximation of continuous functions with respect to the supremum norm, especially for ReLU and leaky ReLU, but also for other activations of practical and theoretical interest. Furthermore, our 5 demonstrates that, in the case dxâ‰¥dysubscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦d_{x}\\geq d_{y}, FNNs with leaky ReLU activations are unable to attain the smallest possible minimal width over all activation functions, given by wminâˆ—=maxâ¡{dx,dy}=dxsuperscriptsubscriptğ‘¤subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦subscriptğ‘‘ğ‘¥w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}=d_{x}, while they achieve it for Lpsuperscriptğ¿ğ‘L^{p} universal approximation of Lpsuperscriptğ¿ğ‘L^{p} integrable functions when dxâ‰¥2subscriptğ‘‘ğ‘¥2d_{x}\\geq 2 and dyâ‰¥2subscriptğ‘‘ğ‘¦2d_{y}\\geq 2, as shown in 1 and by Cai [ref]5. Moreover, it should be noted that Cai [ref]5 proved that the minimal width for uniform universal approximation of continuous functions with FNNs with ReLU+FLOOR activations is given by wmin=maxâ¡{2,dx,dy}subscriptğ‘¤2subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦w_{\\min}=\\max\\{2,d_{x},d_{y}\\}. Nevertheless, despite this result being optimal for approximation in theory, it is difficult for practical purposes, as using discontinuous activations in neural networks fundamentally breaks backpropagation and gradient descent. These methods are designed for smooth functions for computing gradients  9, and while there are workarounds, they might be inefficient and impractical for training deep networks with discontinuous activations. This motivates the question of whether there is another continuous (almost everywhere) smooth activation that achieves the minimal width for the uniform universal approximation of continuous functions. According to our 5, such an activation cannot be Lipschitz continuous and monotone, which excludes most of the popular choices 2 , such as ReLU, LReLU, tangens hyperbolicus, logistic sigmoid and many other sigmoidal activations. Some possible candidates for relevant activation functions, as e.g. presented by Dubey etÂ al. 2, that might still be able to achieve uniform UAP of continuous functions with a width of maxâ¡{dx,dy}subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦\\max\\{d_{x},d_{y}\\}, could include the swish activation given by swishâ¡(x)=x1+expâ¡(âˆ’x)swishğ‘¥ğ‘¥1ğ‘¥\\operatorname{swish}(x)=\\frac{x}{1+\\exp(-x)} or the adaptive piecewise linear (APL) activation, which is more adaptive due to learnable parameters similar to LReLU, both of which are not monotone. Additionally, as of now, the leaky ReLUs are the only known activation functions to achieve universal approximation for Lpsuperscriptğ¿ğ‘L^{p} integrable functions with the minimal width wminâˆ—superscriptsubscriptğ‘¤w_{\\min}^{*} over all possible activation functions for all but three choices of input and output dimensions. Therefore another interesting open question for future work is whether there exist other activation functions, besides the leaky ReLU, which achieve the minimal width for Lpsuperscriptğ¿ğ‘L^{p} universal approximation for all but finitely many choices of the dimensions, or perhaps even for all input and output dimensions. Note that ReLU does not accomplish this, since Park etÂ al.  showed that FNNs with ReLU activations for Lpsuperscriptğ¿ğ‘L^{p} universal approximation of the Lpsuperscriptğ¿ğ‘L^{p} integrable function have a minimal width of maxâ¡{dx+1,dy}subscriptğ‘‘ğ‘¥1subscriptğ‘‘ğ‘¦\\max\\{d_{x}+1,d_{y}\\} and therefore cannot achieve the smallest possible width of wminâˆ—=maxâ¡{dx,dy}superscriptsubscriptğ‘¤subscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} if dxâ‰¥dysubscriptğ‘‘ğ‘¥subscriptğ‘‘ğ‘¦d_{x}\\geq d_{y}."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Analysis III",
      "abstract": "",
      "year": "2008",
      "venue": "Grundstudium Mathematik. BirkhÃ¤user Basel",
      "authors": "H. Amann and J. Escher"
    },
    {
      "index": 1,
      "title": "Learning Deep Architectures for AI",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Y. Bengio"
    },
    {
      "index": 2,
      "title": "lpsuperscriptğ‘™ğ‘l^{p} approximation of maps by diffeomorphisms",
      "abstract": "",
      "year": "2001",
      "venue": "Calculus of Variations and Partial Differential Equations",
      "authors": "Y. Brenier and W. Gangbo"
    },
    {
      "index": 3,
      "title": "Functional Analysis, Sobolev Spaces and Partial Differential Equations",
      "abstract": "",
      "year": "2010",
      "venue": "Universitext. Springer New York",
      "authors": "H. Brezis"
    },
    {
      "index": 4,
      "title": "Achieve the minimum width of neural networks for universal approximation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Y. Cai"
    },
    {
      "index": 5,
      "title": "LU-Net: Invertible Neural Networks Based on Matrix Factorization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "R. Chan, S. Penquitt, and H. Gottschalk",
      "orig_title": "Lu-net: Invertible neural networks based on matrix factorization",
      "paper_id": "2302.10524v1"
    },
    {
      "index": 6,
      "title": "Neural Ordinary Differential Equations",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud",
      "orig_title": "Neural ordinary differential equations",
      "paper_id": "1806.07366v5"
    },
    {
      "index": 7,
      "title": "Approximation by superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of Control, Signals, and Systems (MCSS)",
      "authors": "G. Cybenko"
    },
    {
      "index": 8,
      "title": "Activation functions and their characteristics in deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Chinese Control And Decision Conference (CCDC)",
      "authors": "B. Ding, H. Qian, and J. Zhou"
    },
    {
      "index": 9,
      "title": "Density estimation using Real NVP",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "L. Dinh, J. Sohl-Dickstein, and S. Bengio",
      "orig_title": "Density estimation using real NVP",
      "paper_id": "1605.08803v3"
    },
    {
      "index": 10,
      "title": "Vanilla feedforward neural networks as a discretization of dynamic systems",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "Y. Duan, L. Li, G. Ji, and Y. Cai",
      "orig_title": "Vanilla feedforward neural networks as a discretization of dynamical systems",
      "paper_id": "2209.10909v3"
    },
    {
      "index": 11,
      "title": "Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "S. R. Dubey, S. K. Singh, and B. B. Chaudhuri",
      "orig_title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "paper_id": "2109.14545v3"
    },
    {
      "index": 12,
      "title": "Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "S. R. Dubey, S. K. Singh, and B. B. Chaudhuri",
      "orig_title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "paper_id": "2109.14545v3"
    },
    {
      "index": 13,
      "title": "Real Analysis",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "D. B. Emmanuele"
    },
    {
      "index": 14,
      "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "B. Hanin and M. Sellke",
      "orig_title": "Approximating continuous functions by relu nets of minimal width",
      "paper_id": "1710.11278v2"
    },
    {
      "index": 15,
      "title": "Analysis 2",
      "abstract": "",
      "year": "2013",
      "venue": "Springer-Lehrbuch. Springer Berlin Heidelberg",
      "authors": "S. Hildebrandt"
    },
    {
      "index": 16,
      "title": "Multilayer feedforward networks are universal approximators",
      "abstract": "",
      "year": "1989",
      "venue": "Neural Networks",
      "authors": "K. Hornik, M. Stinchcombe, and H. White"
    },
    {
      "index": 17,
      "title": "Deep, skinny neural networks are not universal approximators",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "J. Johnson"
    },
    {
      "index": 18,
      "title": "Universal Approximation with Deep Narrow Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "P. Kidger and T. Lyons",
      "orig_title": "Universal approximation with deep narrow networks",
      "paper_id": "1905.08539v2"
    },
    {
      "index": 19,
      "title": "Glow: Generative Flow with Invertible 1Ã—1 Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "D. P. Kingma and P. Dhariwal",
      "orig_title": "Glow: Generative flow with invertible 1x1 convolutions",
      "paper_id": "1807.03039v2"
    },
    {
      "index": 20,
      "title": "Normalizing Flows: An Introduction and Review of Current Methods",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "I. Kobyzev, S. J. Prince, and M. A. Brubaker",
      "orig_title": "Normalizing flows: An introduction and review of current methods",
      "paper_id": "1908.09257v4"
    },
    {
      "index": 21,
      "title": "Deep Learning via Dynamical Systems: An Approximation Perspective",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Q. Li, T. Lin, and Z. Shen",
      "orig_title": "Deep learning via dynamical systems: An approximation perspective",
      "paper_id": "1912.10382v2"
    },
    {
      "index": 22,
      "title": "Linear Algebra",
      "abstract": "",
      "year": "2015",
      "venue": "Springer Undergraduate Mathematics Series. Springer International Publishing",
      "authors": "J. Liesen and V. Mehrmann"
    },
    {
      "index": 23,
      "title": "The Expressive Power of Neural Networks: A View from the Width",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang",
      "orig_title": "The expressive power of neural networks: A view from the width",
      "paper_id": "1709.02540v3"
    },
    {
      "index": 24,
      "title": "Distribution learning via neural differential equations: a nonparametric statistical perspective",
      "abstract": "",
      "year": "2024",
      "venue": "Journal of Machine Learning Research",
      "authors": "Y. Marzouk, Z. R. Ren, S. Wang, and J. Zech",
      "orig_title": "Distribution learning via neural differential equations: a nonparametric statistical perspective",
      "paper_id": "2309.01043v1"
    },
    {
      "index": 25,
      "title": "Neural networks and deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "M. A. Nielsen"
    },
    {
      "index": 26,
      "title": "Applied Nonlinear Functional Analysis",
      "abstract": "",
      "year": "2018",
      "venue": "De Gruyter, Berlin, Boston",
      "authors": "N. S. Papageorgiou and P. Winkert"
    },
    {
      "index": 27,
      "title": "Minimum width for universal approximation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "S. Park, C. Yun, J. Lee, and J. Shin"
    },
    {
      "index": 28,
      "title": "On the difficulty of training recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "30th International Conference on Machine Learning, ICMLâ€™13",
      "authors": "R. Pascanu, T. Mikolov, and Y. Bengio"
    },
    {
      "index": 29,
      "title": "Approximation theory of the mlp model in neural networks",
      "abstract": "",
      "year": "1999",
      "venue": "Acta Numerica",
      "authors": "A. Pinkus"
    },
    {
      "index": 30,
      "title": "Intermediate Calculus",
      "abstract": "",
      "year": "2012",
      "venue": "Undergraduate Texts in Mathematics. Springer New York",
      "authors": "M. Protter and C. Morrey"
    },
    {
      "index": 31,
      "title": "Remarks on a Multivariate Transformation",
      "abstract": "",
      "year": "1952",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "M. Rosenblatt"
    },
    {
      "index": 32,
      "title": "Neural ode control for classification, approximation, and transport",
      "abstract": "",
      "year": "2023",
      "venue": "SIAM Review",
      "authors": "D. Ruiz-Balet and E. Zuazua"
    },
    {
      "index": 33,
      "title": "Optimal transport for applied mathematicians",
      "abstract": "",
      "year": "2015",
      "venue": "BirkÃ¤user, NY",
      "authors": "F. Santambrogio"
    },
    {
      "index": 34,
      "title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "T. Teshima, I. Ishikawa, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama",
      "orig_title": "Coupling-based invertible neural networks are universal diffeomorphism approximators",
      "paper_id": "2006.11469v2"
    },
    {
      "index": 35,
      "title": "Optimal transport: old and new",
      "abstract": "",
      "year": "2009",
      "venue": "Springer",
      "authors": "C. Villani et al."
    }
  ]
}