{
  "paper_id": "2006.04152v3",
  "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit",
  "sections": {
    "dynamic approaches: input-adaptive inference": "A parallel line of research for improving the efficiency of neural networks is to enable adaptive inference for various input instances. Adaptive Computation Time [ref]24 [ref]25 proposed to use a trainable halting mechanism to perform input-adaptive inference. However, training the halting model requires extra effort and also introduces additional parameters and inference cost. To alleviate this problem, BranchyNet [ref]26 calculated the entropy of the prediction probability distribution as a proxy for the confidence of branch classifiers to enable early exit. Shallow-Deep Nets  leveraged the softmax scores of predictions of branch classifiers to mitigate the overthinking problem of DNNs. More recently, Hu et al.  leveraged this approach in adversarial training to improve the adversarial robustness of DNNs. In addition, existing approaches [ref]24  trained separate models to determine passing through or skipping each layer. Very recently, FastBERT  and DeeBERT  adapted confidence-based BranchyNet [ref]26 for PLMs while RightTool  leveraged\nthe same early-exit criterion as in the Shallow-Deep Network . However, Schwartz et al.  recently revealed that prediction probability based methods often lead to substantial performance drop compared to an oracle that identifies the smallest model needed to solve a given instance. In addition, these methods only support classification and leave out regression, which limits their applications. Different from the recent work that directly employs existing efficient inference methods on top of PLMs, PABEE is a novel early-exit criterion that captures the inner-agreement between earlier and later internal classifiers and exploit multiple classifiers for inference, leading to better accuracy."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 1,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 2,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov",
      "orig_title": "Roberta: A robustly optimized BERT pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 3,
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",
      "orig_title": "ALBERT: A lite BERT for self-supervised learning of language representations",
      "paper_id": "1909.11942v6"
    },
    {
      "index": 4,
      "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras",
      "orig_title": "Shallow-deep networks: Understanding and mitigating network overthinking",
      "paper_id": "1810.07052v3"
    },
    {
      "index": 5,
      "title": "Is BERT really robust? natural language attack on text classification and entailment",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits"
    },
    {
      "index": 6,
      "title": "Generalization and parameter estimation in feedforward netws: Some experiments",
      "abstract": "",
      "year": "1989",
      "venue": "NeurIPS",
      "authors": "Nelson Morgan and Hervé Bourlard"
    },
    {
      "index": 7,
      "title": "Early stopping-but when?",
      "abstract": "",
      "year": "1998",
      "venue": "Neural Networks: Tricks of the trade",
      "authors": "Lutz Prechelt"
    },
    {
      "index": 8,
      "title": "Once-for-all: Train one network and specialize it for efficient deployment",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han"
    },
    {
      "index": 9,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 10,
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.04861",
      "authors": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam"
    },
    {
      "index": 11,
      "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun",
      "orig_title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "paper_id": "1707.01083v2"
    },
    {
      "index": 12,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Mingxing Tan and Quoc V. Le",
      "orig_title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 13,
      "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1510.00149",
      "authors": "Song Han, Huizi Mao, and William J Dally"
    },
    {
      "index": 14,
      "title": "Quantized convolutional neural networks for mobile devices",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng"
    },
    {
      "index": 15,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 16,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01108",
      "authors": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 17,
      "title": "Patient knowledge distillation for BERT model compression",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu"
    },
    {
      "index": 18,
      "title": "Tinybert: Distilling bert for natural language understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.10351",
      "authors": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu"
    },
    {
      "index": 19,
      "title": "Are Sixteen Heads Really Better than One?",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Paul Michel, Omer Levy, and Graham Neubig",
      "orig_title": "Are sixteen heads really better than one?",
      "paper_id": "1905.10650v3"
    },
    {
      "index": 20,
      "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov"
    },
    {
      "index": 21,
      "title": "Reducing Transformer Depth on Demand with Structured Dropout",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Angela Fan, Edouard Grave, and Armand Joulin",
      "orig_title": "Reducing transformer depth on demand with structured dropout",
      "paper_id": "1909.11556v1"
    },
    {
      "index": 22,
      "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP",
      "authors": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou",
      "orig_title": "Bert-of-theseus: Compressing bert by progressive module replacing",
      "paper_id": "2002.02925v4"
    },
    {
      "index": 23,
      "title": "Adaptive computation time for recurrent neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1603.08983",
      "authors": "Alex Graves"
    },
    {
      "index": 24,
      "title": "Universal Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser",
      "orig_title": "Universal transformers",
      "paper_id": "1807.03819v3"
    },
    {
      "index": 25,
      "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ICPR",
      "authors": "Surat Teerapittayanon, Bradley McDanel, and H. T. Kung",
      "orig_title": "Branchynet: Fast inference via early exiting from deep neural networks",
      "paper_id": "1709.01686v1"
    },
    {
      "index": 26,
      "title": "Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Ting-Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang",
      "orig_title": "Triple wins: Boosting accuracy, robustness and efficiency together by enabling input-adaptive inference",
      "paper_id": "2002.10025v2"
    },
    {
      "index": 27,
      "title": "Skipnet: Learning dynamic routing in convolutional networks",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez"
    },
    {
      "index": 28,
      "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.02178",
      "authors": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju",
      "orig_title": "Fastbert: a self-distilling bert with adaptive inference time",
      "paper_id": "2004.02178v2"
    },
    {
      "index": 29,
      "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.12993",
      "authors": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin",
      "orig_title": "Deebert: Dynamic early exiting for accelerating bert inference",
      "paper_id": "2004.12993v1"
    },
    {
      "index": 30,
      "title": "The Right Tool for the Job: Matching Model and Instance Complexities",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Roy Schwartz, Gabi Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A. Smith",
      "orig_title": "The right tool for the job: Matching model and instance complexities",
      "paper_id": "2004.07453v2"
    },
    {
      "index": 31,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "ICLR",
      "authors": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus"
    },
    {
      "index": 32,
      "title": "To Trust Or Not To Trust A Classifier",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta",
      "orig_title": "To trust or not to trust a classifier",
      "paper_id": "1805.11783v2"
    },
    {
      "index": 33,
      "title": "Neural network ensembles, cross validation, and active learning",
      "abstract": "",
      "year": "1994",
      "venue": "NeurIPS",
      "authors": "Anders Krogh and Jesper Vedelsby"
    },
    {
      "index": 34,
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman",
      "orig_title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "paper_id": "1804.07461v3"
    },
    {
      "index": 35,
      "title": "Automatically constructing a corpus of sentential paraphrases",
      "abstract": "",
      "year": "2005",
      "venue": "IWP@IJCNLP",
      "authors": "William B. Dolan and Chris Brockett"
    },
    {
      "index": 36,
      "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations",
      "abstract": "",
      "year": "2018",
      "venue": "LREC",
      "authors": "Alexis Conneau and Douwe Kiela",
      "orig_title": "Senteval: An evaluation toolkit for universal sentence representations",
      "paper_id": "1803.05449v1"
    },
    {
      "index": 37,
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "abstract": "",
      "year": "2013",
      "venue": "EMNLP",
      "authors": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts"
    },
    {
      "index": 38,
      "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "Adina Williams, Nikita Nangia, and Samuel R. Bowman",
      "orig_title": "A broad-coverage challenge corpus for sentence understanding through inference",
      "paper_id": "1704.05426v4"
    },
    {
      "index": 39,
      "title": "Squad: 100, 000+ questions for machine comprehension of text",
      "abstract": "",
      "year": "2016",
      "venue": "EMNLP",
      "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang"
    },
    {
      "index": 40,
      "title": "Neural Network Acceptability Judgments",
      "abstract": "",
      "year": "2019",
      "venue": "TACL",
      "authors": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman",
      "orig_title": "Neural network acceptability judgments",
      "paper_id": "1805.12471v3"
    },
    {
      "index": 41,
      "title": "The winograd schema challenge",
      "abstract": "",
      "year": "2011",
      "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning",
      "authors": "Hector J. Levesque"
    },
    {
      "index": 42,
      "title": "Huggingface’s transformers: State-of-the-art natural language processing",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:1910.03771",
      "authors": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush"
    },
    {
      "index": 43,
      "title": "Megatron-lm: Training multi-billion parameter language models using gpu model parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.08053",
      "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro"
    },
    {
      "index": 44,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.10683",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 45,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.14165",
      "authors": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 46,
      "title": "Adversarial examples in the physical world",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR (Workshop)",
      "authors": "Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio",
      "orig_title": "Adversarial examples in the physical world",
      "paper_id": "1607.02533v4"
    },
    {
      "index": 47,
      "title": "A large annotated corpus for learning natural language inference",
      "abstract": "",
      "year": "2015",
      "venue": "EMNLP",
      "authors": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning",
      "orig_title": "A large annotated corpus for learning natural language inference",
      "paper_id": "1508.05326v1"
    },
    {
      "index": 48,
      "title": "Character-level convolutional networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Xiang Zhang, Junbo Jake Zhao, and Yann LeCun"
    },
    {
      "index": 49,
      "title": "Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1709.03423",
      "authors": "Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer",
      "orig_title": "Ensemble methods as a defense to adversarial perturbations against deep neural networks",
      "paper_id": "1709.03423v2"
    },
    {
      "index": 50,
      "title": "Ensemble Adversarial Training: Attacks and Defenses",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel",
      "orig_title": "Ensemble adversarial training: Attacks and defenses",
      "paper_id": "1705.07204v5"
    },
    {
      "index": 51,
      "title": "Improving Adversarial Robustness via Promoting Ensemble Diversity",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu",
      "orig_title": "Improving adversarial robustness via promoting ensemble diversity",
      "paper_id": "1901.08846v3"
    },
    {
      "index": 52,
      "title": "Learning Transferable Architectures for Scalable Image Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le",
      "orig_title": "Learning transferable architectures for scalable image recognition",
      "paper_id": "1707.07012v4"
    },
    {
      "index": 53,
      "title": "Green AI",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.10597",
      "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni"
    },
    {
      "index": 54,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Alex Krizhevsky et al."
    }
  ]
}