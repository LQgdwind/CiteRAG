{
  "paper_id": "2004.02163v1",
  "title": "On the Convergence Analysis of Asynchronous SGD for Solving Consistent Linear Systems",
  "sections": {
    "introduction": "In the era of big data and artificial intelligence, optimization problems have become increasingly complex in nature. Although the computers are now more powerful and inexpensive, the problems have grown continuously larger in size and they are difficult to be maneuvered by a single processor. Owing to the nature of the high-volume of the data, an emerging interest is to device and analyze scalable, parallel, and distributed algorithms that can handle the data more efficiently as compared to the traditional optimization algorithms designed to run on a single processor.¬†To deal with the large-scale data, these new class of algorithms can take advantage of a multi-processor system where each processor has access to its own data-partition and it processes the data-partitions in mini-batches. Shalev-Shwartz et al.  in 2007 and Gimpel et al.  in 2010, explored the idea of mini-batches for stochastic algorithms in both the serial and parallel settings. In 2011, Dekel et al. [ref]9 proposed a distributed mini-batch algorithm¬†(for online predictions)‚Äîa method that converts many serial gradient-based online prediction algorithms into distributed algorithms with an asymptotically optimal regret bound. However, in a distributed environment, the synchronous parallel algorithms tend to slow down due to unpredictable communication faults, significant network latency, and processors with different processing speeds. To overcome the above issues posed by the synchronous parallel algorithms in a distributed environment, there has been a recent focus on developing and analyzing asynchronous algorithms. In asynchronous algorithms, processors with different storage capacity and processing speeds perform updates without synchronizing with others. Asynchronous algorithms were first introduced by Chazan and Miranker on chaotic relaxation in 1969  (also, see Frommer and Szyld  and ). However, not only the inherent dynamics of asynchronous algorithms are challenging compared to their synchronous counterparts, but also their convergence analyses are much more mathematically involved. Historically, in the literature, the comparisons between the convergence rates of the asynchronous algorithms and their synchronous counterparts are also not vastly present and less understood. In this paper, our goal is to understand the convergence rates of the asynchronous and synchronous SGD in a fairly simple set-up. However, before discussing problem formulation, set-up, and contribution, we start with a brief overview of stochastic optimization. In machine learning and data-fitting applications, stochastic optimization is a broadly studied field. Consider the stochastic optimization problem: where ùíüùíü{\\cal D} is an user inferred distribution and ùêíùêí{\\bf S} is a random sample drawn from that distribution.¬†In supervised machine learning or deep learning, the above problem is known as empirical risk minimization¬†(ERM) problem: where fi‚Äã(x)subscriptùëìùëñùë•f_{i}(x)‚Äôs are instantiated by different distributions ùíüisubscriptùíüùëñ{\\cal D}_{i} and ùêíisubscriptùêíùëñ{\\bf S}_{i} is sampled from ùíüisubscriptùíüùëñ{\\cal D}_{i}.\nIn the distributed setting, nùëõn in the ERM problem denotes number of processors/workers. Therefore, (2) is an important problem from the deep learning perspective as it captures data-parallelism¬†(distributed over nùëõn processors (GPUs/CPUs etc.)). One of the most popular algorithms for solving (1) is the stochastic gradient descent (SGD)¬†6. For a given sequence of stepsize parameters {œâk}subscriptùúîùëò\\{\\omega_{k}\\} with œâk>0subscriptùúîùëò0\\omega_{k}>0 and the sequence of iterates {xk}subscriptùë•ùëò\\{x_{k}\\}, the updates of SGD take the form: where ‚àáfùêík‚Äã(xk)‚àásubscriptùëìsubscriptùêíùëòsubscriptùë•ùëò\\nabla f_{{\\bf S}_{k}}(x_{k}) is the stochastic gradient arising from the sample ùêík‚àºùíüsimilar-tosubscriptùêíùëòùíü{\\bf S}_{k}\\sim{\\cal D} drawn afresh in each iteration and is an unbiased estimator of the gradient of fùëìf. A natural next direction in solving (1) is to design a synchronized parallel update by using SGD . If more than one processors are available, then they can work simultaneously and each one of them can calculate a stochastic gradient independent of the other processors. At the end, the user can average the stochastic gradients from all processors to obtain the update as: where ‚àáfùêík‚Äãi‚Äã(xk)‚àásubscriptùëìsubscriptùêíùëòùëñsubscriptùë•ùëò\\nabla f_{{\\bf S}_{ki}}(x_{k}) is the stochastic gradient arising from the independent sample ùêík‚Äãi‚àºùíüsimilar-tosubscriptùêíùëòùëñùíü{\\bf S}_{ki}\\sim{\\cal D} from each of the œÑùúè\\tau processors and œâk>0subscriptùúîùëò0\\omega_{k}>0 is a uniform stepsize across kthsuperscriptùëòthk^{\\rm th} iteration. Note that, if œÑ=1ùúè1\\tau=1, then the update scheme in (4) is (3).\nIn this paper, we compare the convergence rates of the asynchronous and synchronous algorithms in a fairly simple set-up of solving any arbitrary consistent linear systems by reformulating them into stochastic optimization problems. Before explaining our main contributions, we introduce the stochastic reformulation of a linear system."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Analysis and Implementation of an Asynchronous Optimization Algorithm for the Parameter Server",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "A. Aytekin, H. R. Feyzmahdavian, and M. Johansson",
      "orig_title": "Analysis and implementation of an asynchronous optimization algorithm for the parameter server",
      "paper_id": "1610.05507v1"
    },
    {
      "index": 1,
      "title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "T. Ben-Nun and T. Hoefler",
      "orig_title": "Demystifying parallel and distributed deep learning: An in-depth concurrency analysis",
      "paper_id": "1802.09941v2"
    },
    {
      "index": 2,
      "title": "Parallel and distributed computation: numerical methods",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "D. P Bertsekas and J. N Tsitsiklis"
    },
    {
      "index": 3,
      "title": "Parallel coordinate descent for l1-regularized loss minimization",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Machine Learning",
      "authors": "J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin"
    },
    {
      "index": 4,
      "title": "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don‚Äôt care",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Chaturapruek, J. C. Duchi, and C. R√©"
    },
    {
      "index": 5,
      "title": "Chaotic relaxation",
      "abstract": "",
      "year": "1969",
      "venue": "Linear Algebra and its Applications",
      "authors": "D. Chazan and W. Miranker"
    },
    {
      "index": 6,
      "title": "Understanding and optimizing asynchronous low-precision stochastic gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "ACM SIGARCH Computer Architecture News",
      "authors": "C. M. De Sa, M. Feldman, C. R√©, and K. Olukotun"
    },
    {
      "index": 7,
      "title": "Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "C. M. De Sa, C. Zhang, K. Olukotun, and C. R√©",
      "orig_title": "Taming the wild: A unified analysis of Hogwild-style algorithms",
      "paper_id": "1506.06438v2"
    },
    {
      "index": 8,
      "title": "Optimal distributed online prediction using mini-batches",
      "abstract": "",
      "year": "2012",
      "venue": "Journal of Machine Learning Research",
      "authors": "O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao"
    },
    {
      "index": 9,
      "title": "On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "A. Dutta, E. Bergou, A. M. Abdelmoniem, C. Y. Ho, A. N. Sahu, M. Canini, and P. Kalnis",
      "orig_title": "On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning",
      "paper_id": "1911.08250v1"
    },
    {
      "index": 10,
      "title": "Fast distributed coordinate descent for non-strongly convex losses",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE International Workshop on Machine Learning for Signal Processing",
      "authors": "O. Fercoq, Z. Qu, P. Richt√°rik, and M. Tak√°ƒç"
    },
    {
      "index": 11,
      "title": "Accelerated, parallel, and proximal coordinate descent",
      "abstract": "",
      "year": "2015",
      "venue": "SIAM Journal on Optimization",
      "authors": "O. Fercoq and P. Richt√°rik"
    },
    {
      "index": 12,
      "title": "A delayed proximal gradient method with linear convergence rate",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE International Workshop on Machine Learning for Signal Processing (MLSP)",
      "authors": "H. R. Feyzmahdavian, A. Aytekin, and M. Johansson"
    },
    {
      "index": 13,
      "title": "On asynchronous iterations",
      "abstract": "",
      "year": "2001",
      "venue": "Journal of Computational and Applied Mathematics",
      "authors": "A. Frommer and D. B. Szyld"
    },
    {
      "index": 14,
      "title": "Distributed asynchronous online learning for natural language processing",
      "abstract": "",
      "year": "2010",
      "venue": "Conference on Computational Natural Language Learning",
      "authors": "K. Gimpel, D. Das, and N. A. Smith"
    },
    {
      "index": 15,
      "title": "Randomized iterative methods for linear systems",
      "abstract": "",
      "year": "2015",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "R. M. Gower and P. Richt√°rik"
    },
    {
      "index": 16,
      "title": "Stochastic Dual Ascent for Solving Linear Systems",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "R. M. Gower and P. Richt√°rik",
      "orig_title": "Stochastic dual ascent for solving linear systems",
      "paper_id": "1512.06890v2"
    },
    {
      "index": 17,
      "title": "Communication-Efficient Distributed Dual Coordinate Ascent",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Jaggi, V. Smith, M. Tak√°ƒç, J. Terhorst, S. Krishnan, T. Hofmann, and M. Jordan",
      "orig_title": "Communication-efficient distributed dual coordinate ascent",
      "paper_id": "1409.1458v2"
    },
    {
      "index": 18,
      "title": "Accelerating stochastic gradient descent using predictive variance reduction",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "R. Johnson and T. Zhang"
    },
    {
      "index": 19,
      "title": "ASAGA: Asynchronous Parallel SAGA",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "R. Leblond, F. Pedregosa, and S. Lacoste-Julien"
    },
    {
      "index": 20,
      "title": "Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "J. D. Lee, Q Lin, T. Ma, and T. Yang",
      "orig_title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity",
      "paper_id": "1507.07595v2"
    },
    {
      "index": 21,
      "title": "Asynchronous parallel stochastic gradient for nonconvex optimization",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "X. Lian, Y. Huang, Y. Li, and J. Liu"
    },
    {
      "index": 22,
      "title": "Adding vs. averaging in distributed primal dual optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "C. Ma, V. Smith, M. Jaggi, M.Jordan, P. Richt√°rik, and M. Tak√°ƒç"
    },
    {
      "index": 23,
      "title": "Perturbed Iterate Analysis for Asynchronous Stochastic Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM Journal on Optimization",
      "authors": "H. Mania, X. Pan, D. Papailiopoulos, B. Recht, K. Ramchandran, and M. Jordan",
      "orig_title": "Perturbed iterate analysis for asynchronous stochastic optimization",
      "paper_id": "1507.06970v2"
    },
    {
      "index": 24,
      "title": "Distributed block coordinate descent for minimizing partially separable functions",
      "abstract": "",
      "year": "2015",
      "venue": "Numerical Analysis and Optimization",
      "authors": "J. Mareƒçek, P. Richt√°rik, and M. Tak√°ƒç"
    },
    {
      "index": 25,
      "title": "Matrix analysis and applied linear algebra, 2000",
      "abstract": "",
      "year": "2000",
      "venue": "SIAM",
      "authors": "C. D. Meyer"
    },
    {
      "index": 26,
      "title": "Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Process Control",
      "authors": "I. Necoara and D. Clipici"
    },
    {
      "index": 27,
      "title": "SGD and Hogwild! convergence without the bounded gradients Assumption",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "L. Nguyen, P. H. Nguyen, M. Dijk, P. Richt√°rik, K. Scheinberg, and M. Tak√°ƒç"
    },
    {
      "index": 28,
      "title": "Dogwild!-distributed hogwild for CPU & GPU",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS Workshop on Distributed Machine Learning and Matrix Computations",
      "authors": "C. Noel and S. Osindero"
    },
    {
      "index": 29,
      "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing System",
      "authors": "F. Pedregosa, R. Leblond, F. Pedregosa, and S. Lacoste-Julien",
      "orig_title": "Breaking the nonsmooth barrier: A scalable parallel method for composite optimization",
      "paper_id": "1707.06468v3"
    },
    {
      "index": 30,
      "title": "Polynomials, 2009",
      "abstract": "",
      "year": "2009",
      "venue": "Springer",
      "authors": "V. V. Prasolov"
    },
    {
      "index": 31,
      "title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Recht, C. Re, S. Wright, and F. Niu"
    },
    {
      "index": 32,
      "title": "Distributed coordinate descent method for learning with big data",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "P. Richt√°rik and M. Tak√°ƒç"
    },
    {
      "index": 33,
      "title": "Parallel coordinate descent methods for big data optimization",
      "abstract": "",
      "year": "2016",
      "venue": "Mathematical Programming",
      "authors": "P. Richt√°rik and M. Tak√°ƒç"
    },
    {
      "index": 34,
      "title": "Stochastic Reformulations of Linear Systems: Algorithms and Convergence Theory",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "P. Richt√°rik and M. Tak√°ƒç",
      "orig_title": "Stochastic reformulations of linear systems: Algorithms and convergence theory",
      "paper_id": "1706.01108v4"
    },
    {
      "index": 35,
      "title": "A stochastic approximation method",
      "abstract": "",
      "year": "1951",
      "venue": "Annals of Mathematical Statistics",
      "authors": "H. Robbins and S. Monro"
    },
    {
      "index": 36,
      "title": "Pegasos: Primal estimated sub-gradient solver for SVM",
      "abstract": "",
      "year": "2007",
      "venue": "24th International Conference on Machine Learning (ICML)",
      "authors": "S. Shalev-Shwartz, Y. Singer, and N. Srebro"
    },
    {
      "index": 37,
      "title": "Accelerated mini-batch stochastic dual coordinate ascent",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Shalev-Shwartz and T. Zhang"
    },
    {
      "index": 38,
      "title": "Communication-efficient distributed optimization using an approximate newton-type method",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "O. Shamir, N. Srebro, and T. Zhang"
    },
    {
      "index": 39,
      "title": "Mini-batch primal and dual methods for SVMs",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Tak√°c, A. Singh Bijral, P. Richt√°rik, and N. Srebro"
    },
    {
      "index": 40,
      "title": "Compressed communication for distributed deep learning: Survey and quantitative evaluation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "H. Xu, C.-Y. Ho, A. M. Abdelmoniem, A. Dutta, E. H. Bergou, K. Karatsenidis, M. Canini, and P. Kalnis"
    },
    {
      "index": 41,
      "title": "Trading computation for communication: Distributed stochastic dual coordinate ascent",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "T. Yang"
    },
    {
      "index": 42,
      "title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "S. Y. Zhao and W. J. Li"
    },
    {
      "index": 43,
      "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Zheng, Q. Meng, T. Wang, W. Chen, N. Yu, Z.-M. Ma, and T.-Y. Liu",
      "orig_title": "Asynchronous stochastic gradient descent with delay compensation",
      "paper_id": "1609.08326v6"
    },
    {
      "index": 44,
      "title": "Parallelized stochastic gradient descent",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Zinkevich, M. Weimer, L. Li, and A. J. Smola"
    }
  ]
}