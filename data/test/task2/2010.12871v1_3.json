{
  "paper_id": "2010.12871v1",
  "title": "Large Scale Legal Text Classification Using Transformer Models",
  "sections": {
    "iv-d transformer models": "In the experiments we study the performance of BERT, RoBERTa, DistilBERT and XLNet on the given text\nclassification tasks. BERT is an early, and very popular, transformer model, RoBERTa is a modified version of BERT trained on a larger corpus, DistilBERT is a distilled version of BERT and thereby with lower computational cost,\nand finally, XLNet can be fed with larger input token sequences. BERT: BERT [ref]10 is a bidirectional language model which aims to learn contextual relations between words\nusing the transformer architecture . We use an official release of the pre-trained models,\ndetails about the specific hyperparameters are found in Section V-A. The input to BERT is either a single text (a sentence or document), or a text pair.\nThe first token of each sequence is the special classification token [CLS], followed by WordPiece tokens of the first text A, then a separator token [SEP], and (optionally) after that WordPiece tokens for the second text B. In addition to token embeddings, BERT uses positional embeddings to represent the position of tokens in the sequence.\nFor training, BERT applies Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives.\nIn MLM, BERT randomly masks 15% of all WordPiece tokens in each sequence and learns to predict these masked tokens.\nFor NSP, BERT is fed in 50% of cases with the actual next sentence B, in the other cases\nwith a random sentence B from the corpus. RoBERTa: RoBERTa, introduced by Liu et al. [ref]11, retrains BERT with an improved methodology, much more data,\nlarger batch size and longer training times. In RoBERTa the training strategy of BERT is modified by removing the NSP objective.\nFurther, RoBERTa uses byte pair encoding (BPE) as a tokenization algorithm instead of WordPiece tokenization in BERT. DistilBERT: We use a distilled version of BERT released by Sanh et al. [ref]12.\nDistilBERT provides a lighter and faster version of BERT, reducing the size of the model by 40% while retaining 97% of its capabilities on language understanding tasks [ref]12.\nThe distillation process includes training a complete BERT model (the teacher) using the improved methodology proposed by Liu et al. [ref]11,\nthen DistilBERT (the student) is trained to reproduce the behaviour of the teacher by using cosine embedding loss. XLNet: The previously discussed transformer-based models are limited to a fixed context length\n(such as 512 tokens), while legal documents are often long and exceed this context length limit.\nXLNet  includes segments recurrence, introduced in Transformer-XL ,\nallowing it to digest longer documents. XLNet follows RoBERTa in removing the NSP objective, while introducing a novel permutation language model objective.\nIn our work with XLNet, we fine-tune the classifier directly without LM fine-tuning\n(as LM fine-tuning of XLNet was computationally not possible on the hardware available for our experiments)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Machine learning in automated text categorization",
      "abstract": "",
      "year": "2002",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "F. Sebastiani"
    },
    {
      "index": 1,
      "title": "Explainable Prediction of Medical Codes from Clinical Text",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.05695",
      "authors": "J. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, and J. Eisenstein",
      "orig_title": "Explainable prediction of medical codes from clinical text",
      "paper_id": "1802.05695v2"
    },
    {
      "index": 2,
      "title": "Few-shot and zero-shot multi-label learning for structured label spaces",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "A. Rios and R. Kavuluru"
    },
    {
      "index": 3,
      "title": "LSHTC: A Benchmark for Large-Scale Text Classification",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.08581",
      "authors": "P. Ioannis et al.",
      "orig_title": "Lshtc: A benchmark for large-scale text classification",
      "paper_id": "1503.08581v1"
    },
    {
      "index": 4,
      "title": "Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain",
      "abstract": "",
      "year": "2010",
      "venue": "Springer",
      "authors": "E. Loza Mencía and J. Fürnkranz"
    },
    {
      "index": 5,
      "title": "Large-Scale Multi-Label Text Classification on EU Legislation",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the ACL",
      "authors": "I. Chalkidis, E. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos",
      "orig_title": "Large-scale multi-label text classification on EU legislation",
      "paper_id": "1906.02192v1"
    },
    {
      "index": 6,
      "title": "Eurepean Union Law Website",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 7,
      "title": "Transfer learning in natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "2019 NAACL: Tutorials",
      "authors": "S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf"
    },
    {
      "index": 8,
      "title": "The European Union’s multilingual and multidisciplinary thesaurus",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 9,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 NAACL: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 10,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 11,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01108",
      "authors": "V. Sanh, L. Debut, J. Chaumond, and T. Wolf",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 12,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 13,
      "title": "On the stratification of multi-label data",
      "abstract": "",
      "year": "2011",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "K. Sechidis, G. Tsoumakas, and I. Vlahavas"
    },
    {
      "index": 14,
      "title": "X-bert: extreme multi-label text classification using bidirectional encoder representations from transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.02331",
      "authors": "W.-C. Chang, H.-F. Yu, K. Zhong, Y. Yang, and I. Dhillon"
    },
    {
      "index": 15,
      "title": "Jrc eurovoc indexer jex-a freely available multi-label categorisation tool",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1309.5223",
      "authors": "R. Steinberger, M. Ebrahim, and M. Turchi"
    },
    {
      "index": 16,
      "title": "Linking legal open data: breaking the accessibility and language barrier in european legislation and case law",
      "abstract": "",
      "year": "2015",
      "venue": "15th International Conference on Artificial Intelligence and Law",
      "authors": "G. Boella et al."
    },
    {
      "index": 17,
      "title": "Deep learning for extreme multi-label text classification",
      "abstract": "",
      "year": "2017",
      "venue": "40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": "J. Liu, W.-C. Chang, Y. Wu, and Y. Yang"
    },
    {
      "index": 18,
      "title": "Sparse local embeddings for extreme multi-label classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain"
    },
    {
      "index": 19,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "J. Pennington, R. Socher, and C. D. Manning"
    },
    {
      "index": 20,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "2018 NAACL: Human Language Technologies, Volume 1 (Long Papers)",
      "authors": "M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer",
      "orig_title": "Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 21,
      "title": "Attentionxml: Extreme multi-label text classification with multi-label attention based recurrent neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.01727",
      "authors": "R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu"
    },
    {
      "index": 22,
      "title": "SKOS Simple Knowledge Organization System",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 23,
      "title": "Resource Description Framework",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 24,
      "title": "RDF 1.1 Turtle",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 25,
      "title": "Exploiting eurovoc’s hierarchical structure for classifying legal documents",
      "abstract": "",
      "year": "2019",
      "venue": "OTM Confederated International Conferences” On the Move to Meaningful Internet Systems",
      "authors": "E. Filtz, S. Kirrane, A. Polleres, and G. Wohlgenannt"
    },
    {
      "index": 26,
      "title": "JRC-Acquis",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 27,
      "title": "EURLEX57K dataset",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 28,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.06146",
      "authors": "J. Howard and S. Ruder",
      "orig_title": "Universal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 29,
      "title": "Fastai documentation",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 30,
      "title": "Huggingface transformers",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 31,
      "title": "Legal Documents, Large Multi-Label Text Classification",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 32,
      "title": "Regularizing and Optimizing LSTM Language Models",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.02182",
      "authors": "S. Merity, N. S. Keskar, and R. Socher",
      "orig_title": "Regularizing and optimizing lstm language models",
      "paper_id": "1708.02182v1"
    },
    {
      "index": 33,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 34,
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.02860",
      "authors": "Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov",
      "orig_title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "paper_id": "1901.02860v3"
    },
    {
      "index": 35,
      "title": "How many labels? determining the number of labels in multi-label text classification",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference of the Cross-Language Evaluation Forum for European Languages",
      "authors": "H. Azarbonyad and M. Marx"
    },
    {
      "index": 36,
      "title": "Multi-label data stratification",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 37,
      "title": "BERT, Multi-Lingual Model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "Huggingface BERT base uncased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 39,
      "title": "Huggingface RoBERTa base model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 40,
      "title": "Huggingface DistilBERT cased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "Huggingface XLNET cased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
      "abstract": "",
      "year": "2016",
      "venue": "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "authors": "H. Jain, Y. Prabhu, and M. Varma"
    },
    {
      "index": 43,
      "title": "Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning and its Application to Cross-Lingual Text Classification",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Transactions on Information Systems (TOIS)",
      "authors": "A. Esuli, A. Moreo, and F. Sebastiani",
      "orig_title": "Funnelling: A new ensemble method for heterogeneous transfer learning and its application to cross-lingual text classification",
      "paper_id": "1901.11459v2"
    },
    {
      "index": 44,
      "title": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network",
      "abstract": "",
      "year": "2020",
      "venue": "12th Int. Conf. on Agents and Artificial Intelligence - Volume 2: ICAART, INSTICC",
      "authors": "A. Pal, M. Selvakumar, and M. Sankarasubbu",
      "orig_title": "Magnet: Multi-label text classification using attention-based graph neural network",
      "paper_id": "2003.11644v1"
    }
  ]
}