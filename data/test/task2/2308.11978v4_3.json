{
  "paper_id": "2308.11978v4",
  "title": "Will More Expressive Graph Neural Networks do Better on Generative Tasks?",
  "sections": {
    "c.1 gnn architectures": "The graph convolution operation of the original GCN Â  can be defined as follows: where ciâ€‹jsubscriptğ‘ğ‘–ğ‘—c_{ij} is a normalisation constant for each edge â„°iâ€‹jsubscriptâ„°ğ‘–ğ‘—\\mathcal{E}_{ij} which originates from using the symmetrically normalised adjacency matrix Dâˆ’12â€‹ADâˆ’12superscriptD12superscriptAD12\\textbf{D}^{-\\frac{1}{2}}\\textbf{A}\\textbf{D}^{-\\frac{1}{2}}, W(l)superscriptWğ‘™\\textbf{W}^{(l)} is a learnable weight matrix, and Ïƒğœ\\sigma is a non-linear activation function. R-GCN Â [ref]6 makes use of the relational data of the graphs, and extends the graph convolution operation to the following: let â„›â„›\\mathcal{R} be the edge relation type (for molecular graphs, this can be the bond type), then where ğ’©irsubscriptsuperscriptğ’©ğ‘Ÿğ‘–\\mathcal{N}^{r}_{i} denotes the set of neighbouring nodes of node iğ‘–i under relation râˆˆâ„›ğ‘Ÿâ„›r\\in\\mathcal{R}, ci,rsubscriptğ‘ğ‘–ğ‘Ÿc_{i,r} is a problem-specific normalisation constant that can either be learnt or chosen in advance and Wr(l)superscriptsubscriptWğ‘Ÿğ‘™\\textbf{W}_{r}^{(l)} denotes the learnable matrix for edge type rğ‘Ÿr. It has been shown that neither GCN nor R-GCN is as expressive as the 1-WL testÂ [ref]29. Each GIN Â [ref]29 layer updates the node features as follows: where Ï•(l)superscriptitalic-Ï•ğ‘™\\phi^{(l)} is an MLP, and Ïµ(l)superscriptitalic-Ïµğ‘™\\epsilon^{(l)} is a learnable scalar. GIN is provably as expressive as the 1-WL test,\nwhich makes it one of the maximally-expressive GNNs (proof inÂ [ref]29). The PNAÂ  operator defines its aggregation function â¨direct-sum\\bigoplus as a combination of neighbourhood-aggregators and degree-scalers, as defined by the following equation, with âŠ—tensor-product\\otimes being the tensor product: The PNA operator can then be inserted into the standard MPNN framework, obtaining the PNA layer: where Ï•(l)superscriptitalic-Ï•ğ‘™\\phi^{(l)} and Ïˆ(l)superscriptğœ“ğ‘™\\psi^{(l)} are MLPs. According to the theorem that in order to discriminate between multisets of size nğ‘›n whose underlying set is Rğ‘…R, at least nğ‘›n aggregators are needed (proof inÂ ), PNA pushes its expressivity closer towards the 1-WL limit than GIN, by including more aggregators, thereby increasing the probability that at least one of the aggregators can distinguish different graphs. GSNÂ  adopts a feature-augmented message passing style by counting the appearance of certain graph substructures and encoding them into the features. The feature augmentation of GSN then works as follows: let ğ’¢={G1,â‹¯,GK}ğ’¢subscriptğº1â‹¯subscriptğºğ¾\\mathcal{G}=\\left\\{G_{1},\\cdots,G_{K}\\right\\} be a set of pre-computed small (connected) graphs. For each Gk=(ğ’±k,â„°k)subscriptğºğ‘˜subscriptğ’±ğ‘˜subscriptâ„°ğ‘˜G_{k}=(\\mathcal{V}_{k},\\mathcal{E}_{k}) in ğ’¢ğ’¢\\mathcal{G}, we first find its isomorphic subgraphs Gkâ€²=(ğ’±kâ€²,â„°kâ€²)superscriptsubscriptğºğ‘˜â€²superscriptsubscriptğ’±ğ‘˜â€²superscriptsubscriptâ„°ğ‘˜â€²G_{k}^{\\prime}=(\\mathcal{V}_{k}^{\\prime},\\mathcal{E}_{k}^{\\prime}) in G=(ğ’±,â„°)ğºğ’±â„°G=(\\mathcal{V},\\mathcal{E}). Then, for each node iâˆˆVğ‘–ğ‘‰i\\in V and 1â‰¤kâ‰¤K1ğ‘˜ğ¾1\\leq k\\leq K, we count the number of subgraphs Gkâ€²superscriptsubscriptğºğ‘˜â€²G_{k}^{\\prime} node iğ‘–i belongs to, as defined by the equation below: We then obtain the node structural features for each node iâˆˆğ’±ğ‘–ğ’±i\\in\\mathcal{V}: xiğ’±=[xG1ğ’±â€‹(i),â‹¯,xGkğ’±â€‹(i)]âˆˆâ„•Ksubscriptsuperscriptxğ’±ğ‘–subscriptsuperscriptğ‘¥ğ’±subscriptğº1ğ‘–â‹¯subscriptsuperscriptğ‘¥ğ’±subscriptğºğ‘˜ğ‘–superscriptâ„•ğ¾\\textbf{x}^{\\mathcal{V}}_{i}=\\left[x^{\\mathcal{V}}_{G_{1}}(i),\\cdots,x^{\\mathcal{V}}_{G_{k}}(i)\\right]\\in\\mathbb{N}^{K}. Similarly, we can derive the edge structural features for each edge (i,j)âˆˆâ„°ğ‘–ğ‘—â„°(i,j)\\in\\mathcal{E}: xiâ€‹jâ„°=[xG1â„°â€‹(i,j),â‹¯,xGkâ„°â€‹(i,j)]âˆˆâ„•Ksubscriptsuperscriptxâ„°ğ‘–ğ‘—subscriptsuperscriptğ‘¥â„°subscriptğº1ğ‘–ğ‘—â‹¯subscriptsuperscriptğ‘¥â„°subscriptğºğ‘˜ğ‘–ğ‘—superscriptâ„•ğ¾\\textbf{x}^{\\mathcal{E}}_{ij}=\\left[x^{\\mathcal{E}}_{G_{1}}(i,j),\\cdots,x^{\\mathcal{E}}_{G_{k}}(i,j)\\right]\\in\\mathbb{N}^{K} by counting the numbers of subgraphs it belongs to: The augmented features can then be inserted into the messages and follow the standard MPNN, obtaining two variants of GSN layer, GSN-v (vertex-count) and GSN-e (edge-count): where Ï•(l)superscriptitalic-Ï•ğ‘™\\phi^{(l)} and Ïˆ(l)superscriptğœ“ğ‘™\\psi^{(l)} are MLPs and â¨direct-sum\\bigoplus is a permutation-invariant local neighbourhood aggregation function, such as sum, mean or max. It can be proved that GSN is strictly more expressive than the 1-WL test, when Gksubscriptğºğ‘˜G_{k} is any graph except for the star graphs (i.e., one center nodes connected to one of multiple outer nodes) of any size, and structural features are inferred by subgraph matching (proof inÂ ). This essentially suggests that GSN is more expressive than R-GCN which is at most as expressive as the 1-WL test in general. GearNetÂ  uses an R-GCNÂ [ref]6 as a fundamental framework to develop a node features and edge features message passing mechanism. Each GearNet layer updates the node features as follows: where â„›â„›\\mathcal{R} is the edge relation type, ğ’©irsubscriptsuperscriptğ’©ğ‘Ÿğ‘–\\mathcal{N}^{r}_{i} denotes the set of neighbouring nodes of node iğ‘–i under relation râˆˆâ„›ğ‘Ÿâ„›r\\in\\mathcal{R}, BN denotes a batch normalisation layer, Wr(l)superscriptsubscriptWğ‘Ÿğ‘™\\textbf{W}_{r}^{(l)} is the learnable convolutional kernel matrix for edge type rğ‘Ÿr, and Ïƒğœ\\sigma is a non-linear activation function. To model the interactions between edges, we first construct a relational graph Gâ€²=(ğ’±â€²,â„°â€²,â„›â€²)superscriptğºâ€²superscriptğ’±â€²superscriptâ„°â€²superscriptâ„›â€²G^{\\prime}=\\left(\\mathcal{V}^{\\prime},\\mathcal{E}^{\\prime},\\mathcal{R}^{\\prime}\\right) among edges. Each node in the graph Gâ€²superscriptğºâ€²G^{\\prime} corresponds to an edge in the original graph. Gâ€²superscriptğºâ€²G^{\\prime} links edge (i,j,r1)ğ‘–ğ‘—subscriptğ‘Ÿ1(i,j,r_{1}) in the original graph to edge (w,k,r2)ğ‘¤ğ‘˜subscriptğ‘Ÿ2(w,k,r_{2}) if and only if j=wğ‘—ğ‘¤j=w and iâ‰ kğ‘–ğ‘˜i\\neq k. The type of this edge is determined by the angle between (i,j,r1)ğ‘–ğ‘—subscriptğ‘Ÿ1(i,j,r_{1}) and (w,k,r2)ğ‘¤ğ‘˜subscriptğ‘Ÿ2(w,k,r_{2}). The angular information reflects the relative position between two edges that determines the strength of their interactionÂ . Similar to R-GCN, the GearNet edge message passing layer is defined as: Similar as Eq.Â (12), the message function for edge (i,j,r1)ğ‘–ğ‘—subscriptğ‘Ÿ1(i,j,r_{1}) will be updated by aggregating features from its neighbours ğ’©â€²(i,j,r1)rsubscriptsuperscriptsuperscriptğ’©â€²ğ‘Ÿğ‘–ğ‘—subscriptğ‘Ÿ1\\mathcal{N^{\\prime}}^{r}_{(i,j,r_{1})}, where ğ’©â€²(i,j,r1)r={(w,k,r2)âˆˆğ’±â€²|((w,k,r2),(i,j,r1),r)âˆˆâ„°â€²}subscriptsuperscriptsuperscriptğ’©â€²ğ‘Ÿğ‘–ğ‘—subscriptğ‘Ÿ1conditional-setğ‘¤ğ‘˜subscriptğ‘Ÿ2superscriptğ’±â€²ğ‘¤ğ‘˜subscriptğ‘Ÿ2ğ‘–ğ‘—subscriptğ‘Ÿ1ğ‘Ÿsuperscriptâ„°â€²\\mathcal{N^{\\prime}}^{r}_{(i,j,r_{1})}=\\{(w,k,r_{2})\\in\\mathcal{V}^{\\prime}|((w,k,r_{2}),(i,j,r_{1}),r)\\in\\mathcal{E}^{\\prime}\\}. Finally, the entire GearNet message passing layer can be expressed as: where FC denotes a linear transformation on the message function. GearNet is more expressive than R-GCN for its sparse edge message passing mechanism which encodes spatial information of a graph. In order to generalise the standard averaging or max-pooling aggregators in GNNs, GATÂ  applies attention-based neighbourhood aggregation as its aggregation function to obtain sufficient expressive power to transform the input features into higher-level features. The normalised masked attention coefficient for node iğ‘–i is defined as: where a is a learnable weight vector, representing the attention mechanism ağ‘a: a single-layer feedforward neural network, W is a learnable input linear transformationâ€™s weight matrix and âˆ¥âˆ¥\\| represents concatenation operation. Each Kğ¾K multi-head attention GAT layer updates the node features as follows: where âˆ¥âˆ¥\\| represents concatenation, Î±iâ€‹jksuperscriptsubscriptğ›¼ğ‘–ğ‘—ğ‘˜\\alpha_{ij}^{k} are normalised attention coefficients computed by the kğ‘˜k-th attention mechanism (ak)superscriptğ‘ğ‘˜(a^{k}), and WksuperscriptWğ‘˜\\textbf{W}^{k} is the corresponding input linear transformationâ€™s weight matrix. GAT computes a representation for every node as a weighted average of its neighbours through the attention mechanism, which is more flexible than the neighbourhood aggregation in R-GCN. GATv2Â [ref]7 adopts a strictly more expressive dynamic graph attention mechanismÂ [ref]7 in its aggregation function to learn the graph representation. The graph attention variant that has a universal approximator attention function. The normalised masked dynamic attention coefficient for node iğ‘–i is defined as: where a is a learnable weight vector, representing the attention mechanism ağ‘a: a single-layer feedforward neural network, W is a learnable input linear transformationâ€™s weight matrix and âˆ¥âˆ¥\\| represents concatenation operation. Each Kğ¾K multi-head attention GATv2 layer updates the node features as follows: where âˆ¥âˆ¥\\| represents concatenation, Î±iâ€‹jksuperscriptsubscriptğ›¼ğ‘–ğ‘—ğ‘˜\\alpha_{ij}^{k} are normalised dynamic attention coefficients computed by the kğ‘˜k-th attention mechanism (ak)superscriptğ‘ğ‘˜(a^{k}), and WksuperscriptWğ‘˜\\textbf{W}^{k} is the corresponding input linear transformationâ€™s weight matrix. It has been proved that GATv2 â€“ a graph attention variant that has a universal approximator attention function, and is thus strictly more expressive than GAT (proof inÂ [ref]7)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Molecular De-Novo Design through Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Cheminformatics",
      "authors": "Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen",
      "orig_title": "Molecular de-novo design through deep reinforcement learning",
      "paper_id": "1704.07555v2"
    },
    {
      "index": 1,
      "title": "Estimation of the size of drug-like chemical space based on GDB-17 data",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Computer-Aided Molecular Design",
      "authors": "PavelÂ G Polishchuk, TimurÂ I Madzhidov, and Alexandre Varnek"
    },
    {
      "index": 2,
      "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec",
      "orig_title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation",
      "paper_id": "1806.02473v3"
    },
    {
      "index": 3,
      "title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang",
      "orig_title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation",
      "paper_id": "2001.09382v2"
    },
    {
      "index": 4,
      "title": "GraphEBM: Molecular Graph Generation with Energy-Based Models",
      "abstract": "",
      "year": "2021a",
      "venue": "ICLR",
      "authors": "Meng Liu, Keqiang Yan, Bora Oztekin, and Shuiwang Ji",
      "orig_title": "GraphEBM: Molecular Graph Generation with Energy-Based Models",
      "paper_id": "2102.00546v2"
    },
    {
      "index": 5,
      "title": "Modeling Relational Data with Graph Convolutional Networks",
      "abstract": "",
      "year": "2018",
      "venue": "The Semantic Web: 15th International Conference",
      "authors": "Michael Schlichtkrull, ThomasÂ N. Kipf, Peter Bloem, Rianne vanÂ den Berg, Ivan Titov, and Max Welling"
    },
    {
      "index": 6,
      "title": "How Attentive are Graph Attention Networks?",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Shaked Brody, Uri Alon, and Eran Yahav",
      "orig_title": "How Attentive are Graph Attention Networks?",
      "paper_id": "2105.14491v3"
    },
    {
      "index": 7,
      "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and MichaelÂ M. Bronstein",
      "orig_title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting",
      "paper_id": "2006.09252v3"
    },
    {
      "index": 8,
      "title": "Protein Representation Learning by Geometric Structure Pretraining",
      "abstract": "",
      "year": "2023",
      "venue": "ICLR",
      "authors": "Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang"
    },
    {
      "index": 9,
      "title": "Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization",
      "abstract": "",
      "year": "2022a",
      "venue": "NeurIPS",
      "authors": "Wenhao Gao, Tianfan Fu, Jimeng Sun, and ConnorÂ W. Coley",
      "orig_title": "Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization",
      "paper_id": "2206.12411v2"
    },
    {
      "index": 10,
      "title": "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design",
      "abstract": "",
      "year": "2022b",
      "venue": "ICLR",
      "authors": "Wenhao Gao, RocÃ­o Mercado, and ConnorÂ W Coley",
      "orig_title": "Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design",
      "paper_id": "2110.06389v2"
    },
    {
      "index": 11,
      "title": "Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and AlÃ¡n Aspuru-Guzik",
      "orig_title": "Augmenting genetic algorithms with deep neural networks for exploring the chemical space",
      "paper_id": "1909.11655v4"
    },
    {
      "index": 12,
      "title": "A fresh look at de novo molecular design benchmarks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS 2021 AI for Science Workshop",
      "authors": "Austin Tripp, GregorÂ NC Simm, and JosÃ©Â Miguel HernÃ¡ndez-Lobato"
    },
    {
      "index": 13,
      "title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
      "abstract": "",
      "year": "2018",
      "venue": "ACS central science",
      "authors": "Rafael GÃ³mez-Bombarelli, JenniferÂ N Wei, David Duvenaud, JosÃ©Â Miguel HernÃ¡ndez-Lobato, BenjamÃ­n SÃ¡nchez-Lengeling, Jorge Aguilera-Iparraguirre DennisÂ Sheberla, TimothyÂ D Hirzel, RyanÂ P Adams, and AlÃ¡n Aspuru-Guzik",
      "orig_title": "Automatic chemical design using a data-driven continuous representation of molecules",
      "paper_id": "1610.02415v3"
    },
    {
      "index": 14,
      "title": "A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space",
      "abstract": "",
      "year": "2019",
      "venue": "Chemical science",
      "authors": "JanÂ H Jensen"
    },
    {
      "index": 15,
      "title": "GuacaMol: Benchmarking Models for De Novo Molecular Design",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Nathan Brown, Marco Fiscato, MarwinÂ H.S. Segler, and AlainÂ C. Vaucher",
      "orig_title": "GuacaMol: Benchmarking Models for de Novo Molecular Design",
      "paper_id": "1811.09621v2"
    },
    {
      "index": 16,
      "title": "Barking up the right tree: an approach to search over molecule synthesis DAGs",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "John Bradshaw, Brooks Paige, MattÂ J Kusner, Marwin Segler, and JosÃ©Â Miguel HernÃ¡ndez-Lobato",
      "orig_title": "Barking up the right tree: an approach to search over molecule synthesis dags",
      "paper_id": "2012.11522v1"
    },
    {
      "index": 17,
      "title": "Optimization of Molecules via Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Scientific reports",
      "authors": "Zhenpeng Zhou, Steven Kearnes, LiÂ Li, RichardÂ N Zare, and Patrick Riley",
      "orig_title": "Optimization of molecules via deep reinforcement learning",
      "paper_id": "1810.08678v3"
    },
    {
      "index": 18,
      "title": "Differentiable Scaffolding Tree for Molecular Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, ConnorÂ W. Coley, and Jimeng Sun",
      "orig_title": "Differentiable Scaffolding Tree for Molecular Optimization",
      "paper_id": "2109.10469v2"
    },
    {
      "index": 19,
      "title": "A review of Bayesian optimization",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE",
      "authors": "Bobak Shahriari, Kevin Swersky, Ziyu Wang, RyanÂ P Adams, and NandoÂ De Freitas"
    },
    {
      "index": 20,
      "title": "Auto-encoding variational Bayes",
      "abstract": "",
      "year": "2014",
      "venue": "ICLR",
      "authors": "DiederikÂ P Kingma and Max Welling"
    },
    {
      "index": 21,
      "title": "Bayesian learning via stochastic gradient langevin dynamics",
      "abstract": "",
      "year": "2011",
      "venue": "ICML",
      "authors": "Max Welling and YeeÂ W Teh"
    },
    {
      "index": 22,
      "title": "ZINC 15â€“ligand discovery for everyone",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of chemical information and modeling",
      "authors": "Teague Sterling and JohnÂ J Irwin"
    },
    {
      "index": 23,
      "title": "Quantum chemistry structures and properties of 134 kilo molecules",
      "abstract": "",
      "year": "2014",
      "venue": "Scientific data",
      "authors": "Raghunathan Ramakrishnan, PavloÂ O Dral, Matthias Rupp, and OÂ Anatole von Lilienfeld"
    },
    {
      "index": 24,
      "title": "Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17",
      "abstract": "",
      "year": "2012",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Lars Ruddigkeit, Ruud van Deursen, LorenzÂ C. Blum, and Jean-Louis Reymond"
    },
    {
      "index": 25,
      "title": "TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.08320",
      "authors": "Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, Chang Ma, Runcheng Liu, Louis-Pascal Xhonneux, Meng Qu, and Jian Tang",
      "orig_title": "TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery",
      "paper_id": "2202.08320v1"
    },
    {
      "index": 26,
      "title": "DIG: A Turnkey Library for Diving into Graph Deep Learning Research",
      "abstract": "",
      "year": "2021b",
      "venue": "Journal of Machine Learning Research",
      "authors": "Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, YiÂ Liu, Keqiang Yan, Haoran Liu, Cong Fu, BoraÂ M Oztekin, Xuan Zhang, and Shuiwang Ji",
      "orig_title": "DIG: A Turnkey Library for Diving into Graph Deep Learning Research",
      "paper_id": "2103.12608v3"
    },
    {
      "index": 27,
      "title": "Automatic differentiation in PyTorch",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS 2017 Workshop Autodiff",
      "authors": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer"
    },
    {
      "index": 28,
      "title": "How Powerful are Graph Neural Networks?",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka",
      "orig_title": "How Powerful are Graph Neural Networks?",
      "paper_id": "1810.00826v3"
    },
    {
      "index": 29,
      "title": "Graph Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio",
      "orig_title": "Graph Attention Networks",
      "paper_id": "1710.10903v3"
    },
    {
      "index": 30,
      "title": "Principal Neighbourhood Aggregation for Graph Nets",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro LiÃ², and Petar VeliÄkoviÄ‡",
      "orig_title": "Principal Neighbourhood Aggregation for Graph Nets",
      "paper_id": "2004.05718v5"
    },
    {
      "index": 31,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning",
      "authors": "Sergey Ioffe and Christian Szegedy"
    },
    {
      "index": 32,
      "title": "Deep Learning using Rectified Linear Units (ReLU)",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.08375",
      "authors": "AbienÂ Fred Agarap",
      "orig_title": "Deep Learning using Rectified Linear Units (ReLU)",
      "paper_id": "1803.08375v2"
    },
    {
      "index": 33,
      "title": "Proximal Policy Optimization Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov"
    },
    {
      "index": 34,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference for Learning Representations",
      "authors": "DiederikÂ P. Kingma and Jimmy Ba"
    },
    {
      "index": 35,
      "title": "Optuna: A Next-generation Hyperparameter Optimization Framework",
      "abstract": "",
      "year": "2019",
      "venue": "25th ACM SIGKDD international conference on knowledge discovery and data mining",
      "authors": "Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama",
      "orig_title": "Optuna: A Next-generation Hyperparameter Optimization Framework",
      "paper_id": "1907.10902v1"
    },
    {
      "index": 36,
      "title": "Grammar Variational Autoencoder",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning",
      "authors": "MattÂ J. Kusner, Brooks Paige, and JosÃ©Â Miguel HernÃ¡ndez-Lobato"
    },
    {
      "index": 37,
      "title": "Quantifying the chemical beauty of drugs",
      "abstract": "",
      "year": "2012",
      "venue": "Nature Chemistry",
      "authors": "G.Â Richard Bickerton, GaiaÂ V. Paolini, JÃ©rÃ©my Besnard, Sorel Muresan, and AndrewÂ L. Hopkins"
    },
    {
      "index": 38,
      "title": "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Cheminformatics",
      "authors": "Peter Ertl and Ansgar Schuffenhauer"
    },
    {
      "index": 39,
      "title": "Therapeutics data commons: Machine learning datasets and tasks for therapeutics",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS Track Datasets and Benchmarks",
      "authors": "Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, ConnorÂ W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik"
    },
    {
      "index": 40,
      "title": "Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations?",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Cheminformatics",
      "authors": "DÃ¡vid Bajusz, Anita RÃ¡cz, and KÃ¡roly HÃ©berger"
    },
    {
      "index": 41,
      "title": "ChEMBL: a large-scale bioactivity database for drug discovery",
      "abstract": "",
      "year": "2012",
      "venue": "Nucleic Acids Research",
      "authors": "Anna Gaulton, LouisaÂ J. Bellis, A.Â Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, and JohnÂ P. Overington"
    },
    {
      "index": 42,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "ThomasÂ N. Kipf and Max Welling",
      "orig_title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 43,
      "title": "On the expressive power of Graph Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Graph Neural Networks: Foundations, Frontiers, and Applications",
      "authors": "Pan Li and Jure Leskovec",
      "orig_title": "The Expressive Power of Graph Neural Networks",
      "paper_id": "2401.01626v2"
    },
    {
      "index": 44,
      "title": "Approximation by superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of control, signals and systems",
      "authors": "George Cybenko"
    },
    {
      "index": 45,
      "title": "Approximation Theory of the MLP Model in Neural Networks",
      "abstract": "",
      "year": "1999",
      "venue": "Acta Numerica",
      "authors": "Allan Pinkus"
    },
    {
      "index": 46,
      "title": "Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES",
      "abstract": "",
      "year": "2021",
      "venue": "Chemical science",
      "authors": "AkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos PassosÂ Gomes, and Alan Aspuru-Guzik"
    },
    {
      "index": 47,
      "title": "MARS: Markov Molecular Sampling for Multi-objective Drug Discovery",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li",
      "orig_title": "MARS: Markov molecular sampling for multi-objective drug discovery",
      "paper_id": "2103.10432v1"
    },
    {
      "index": 48,
      "title": "Accelerating High-Throughput Virtual Screening Through Molecular Pool-Based Active Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Chemical science",
      "authors": "DavidÂ E Graff, EugeneÂ I Shakhnovich, and ConnorÂ W Coley",
      "orig_title": "Accelerating high-throughput virtual screening through molecular pool-based active learning",
      "paper_id": "2012.07127v1"
    },
    {
      "index": 49,
      "title": "MIMOSA: Multi-constraint molecule sampling for molecule optimization",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Tianfan Fu, Cao Xiao, Xinhao Li, LucasÂ M Glass, and Jimeng Sun"
    },
    {
      "index": 50,
      "title": "Improving screening efficiency through iterative screening using docking and conformal prediction",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of chemical information and modeling",
      "authors": "Fredrik Svensson, Ulf Norinder, and Andreas Bender"
    },
    {
      "index": 51,
      "title": "GFlowNet Foundations",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Yoshua Bengio, Tristan Deleu, EdwardÂ J. Hu, Salem Lahlou, MoÂ Tiwari, and Emmanuel Bengio",
      "orig_title": "GFlowNet foundations",
      "paper_id": "2111.09266v4"
    },
    {
      "index": 52,
      "title": "Deep Molecular Dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning: Science and Technology",
      "authors": "Cynthia Shen, Mario Krenn, Sagi Eppel, and Alan Aspuru-Guzik",
      "orig_title": "Deep molecular dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations",
      "paper_id": "2012.09712v1"
    },
    {
      "index": 53,
      "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Wengong Jin, Regina Barzilay, and Tommi Jaakkola",
      "orig_title": "Junction tree variational autoencoder for molecular graph generation",
      "paper_id": "1802.04364v4"
    }
  ]
}