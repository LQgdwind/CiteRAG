{
  "paper_id": "2206.06359v1",
  "title": "EnergyMatch: Energy-based Pseudo-Labeling for Semi-Supervised Learning",
  "sections": {
    "related work": "Semi-Supervised Learning. Research in SSL emerged a few decades ago  4. Since then various directions have been proposed including entropy minimization [ref]9, graph-based methods  4 , and virtual adversarial training . Recent advances in self-training   and consistency regularization   have significantly pushed its frontier. In particular, MeanTeacher  proposes a teacher-student framework where the teacher is updated with an exponential moving average of a student model. Mixup-based methods   incorporate Mixup  into consistency regularization. FixMatch  and UDA  predict pseudo-labels on weakly-augmented views of unlabeled images and train the model to predict those pseudo labels on strongly-augmented views. Most of these state-of-the-art methods use confidence thresholding to retain high-quality pseudo-labels  . Few works have attempted to revise the design of the pseudo-labeling approach. FlexMatch  uses curriculum learning to dynamically adjust the confidence thresholds for different classes, while UPS 9 uses an uncertainty estimation with MC-Dropout , in addition to the confidence score to select pseudo-labels. Our work proposes a different approach for pseudo-labeling. While prior works consider the confidence or uncertainty of model predictions, ours is the first to view the pseudo-labeling process from an “in-distribution vs. out-of-distribution” perspective. In particular, unlike UPS 9, which uses the disagreement between a model ensemble to measure uncertainty in the pseudo-label for an unlabeled instance, we instead use the energy score from a single model’s output to estimate its likelihood of occurrence. We show that this leads to better performance for SSL while being more computationally efficient. Class-Imbalanced Semi-Supervised Learning. While SSL has been extensively studied in the balanced setting in which all categories have (roughly) the same number of instances, class-imbalanced SSL has only begun to be explored recently. A key challenge is to avoid overfitting to the majority classes while capturing the minority classes. Prior works have devised different approaches tailored for this setting.\nDARP 5 refines the pseudo-labels through convex optimization targeted specifically for the imbalanced scenario. CReST  achieves class-rebalancing by pseudo-labeling unlabeled samples with frequency that is inversely proportional to the class frequency. ABC 1 introduces an auxiliary classifier that is trained with class-balanced sampling. Our method is in parallel to these developments; it can be easily integrated into those prior approaches by replacing the confidence-based thresholding with an energy-based one, and achieve significant performance gain. Out-of-Distribution Detection. OOD detection aims to detect outliers that are substantially different from the training data, and is important when deploying ML models to ensure safety and reliability in real-world settings. The softmax score was used as a baseline for OOD detection in 3 but has since been proven to be an unreliable measure by subsequent work 2 3. Improvements have been made in OOD detection through temperatured softmax 2 and the energy score 3. Exploring new methods in OOD detection is not the focus of our work. Instead, we show that leveraging the concept of OOD detection, and in particular, the energy score 9 3 for pseudo-labeling, provides a new perspective in SSL that results in competitive and robust performance."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "MixMatch: A holistic approach to semi-supervised learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel"
    },
    {
      "index": 1,
      "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel",
      "orig_title": "ReMixMatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
      "paper_id": "1911.09785v2"
    },
    {
      "index": 2,
      "title": "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "O. Chapelle, B. Scholkopf, and A. Zien"
    },
    {
      "index": 3,
      "title": "An analysis of single-layer networks in unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "fourteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings",
      "authors": "A. Coates, A. Ng, and H. Lee"
    },
    {
      "index": 4,
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le"
    },
    {
      "index": 5,
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie",
      "orig_title": "Class-balanced loss based on effective number of samples",
      "paper_id": "1901.05555v1"
    },
    {
      "index": 6,
      "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04552",
      "authors": "T. DeVries and G. W. Taylor",
      "orig_title": "Improved regularization of convolutional neural networks with cutout",
      "paper_id": "1708.04552v2"
    },
    {
      "index": 7,
      "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning, PMLR",
      "authors": "Y. Gal and Z. Ghahramani",
      "orig_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "paper_id": "1506.02142v6"
    },
    {
      "index": 8,
      "title": "Semi-supervised learning by entropy minimization",
      "abstract": "",
      "year": "2004",
      "venue": "Advances in neural information processing systems",
      "authors": "Y. Grandvalet and Y. Bengio"
    },
    {
      "index": 9,
      "title": "Your classifier is secretly an energy based model and you should treat it like one",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "W. Grathwohl, K.-C. Wang, J.-H. Jacobsen, D. Duvenaud, M. Norouzi, and K. Swersky",
      "orig_title": "Your classifier is secretly an energy based model and you should treat it like one",
      "paper_id": "1912.03263v3"
    },
    {
      "index": 10,
      "title": "On Calibration of Modern Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning, PMLR",
      "authors": "C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger",
      "orig_title": "On calibration of modern neural networks",
      "paper_id": "1706.04599v2"
    },
    {
      "index": 11,
      "title": "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "M. Hein, M. Andriushchenko, and J. Bitterwolf",
      "orig_title": "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem",
      "paper_id": "1812.05720v2"
    },
    {
      "index": 12,
      "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Hendrycks and K. Gimpel",
      "orig_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "paper_id": "1610.02136v3"
    },
    {
      "index": 13,
      "title": "Label Propagation for Deep Semi-supervised Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Iscen, G. Tolias, Y. Avrithis, and O. Chum",
      "orig_title": "Label propagation for deep semi-supervised learning",
      "paper_id": "1904.04717v1"
    },
    {
      "index": 14,
      "title": "Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Kim, Y. Hur, S. Park, E. Yang, S. J. Hwang, and J. Shin",
      "orig_title": "Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning",
      "paper_id": "2007.08844v2"
    },
    {
      "index": 15,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 16,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "Technical report, University of Toronto",
      "authors": "A. Krizhevsky and G. Hinton"
    },
    {
      "index": 17,
      "title": "Temporal Ensembling for Semi-Supervised Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "S. Laine and T. Aila",
      "orig_title": "Temporal ensembling for semi-supervised learning",
      "paper_id": "1610.02242v3"
    },
    {
      "index": 18,
      "title": "A tutorial on energy-based learning",
      "abstract": "",
      "year": "2006",
      "venue": "Predicting structured data",
      "authors": "Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang"
    },
    {
      "index": 19,
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Workshop on challenges in representation learning, ICML",
      "authors": "D.-H. Lee et al."
    },
    {
      "index": 20,
      "title": "ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "H. Lee, S. Shin, and H. Kim",
      "orig_title": "ABC: Auxiliary balanced classifier for class-imbalanced semi-supervised learning",
      "paper_id": "2110.10368v1"
    },
    {
      "index": 21,
      "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. Liang, Y. Li, and R. Srikant",
      "orig_title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
      "paper_id": "1706.02690v5"
    },
    {
      "index": 22,
      "title": "Energy-based Out-of-distribution Detection",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "W. Liu, X. Wang, J. Owens, and Y. Li",
      "orig_title": "Energy-based out-of-distribution detection",
      "paper_id": "2010.03759v4"
    },
    {
      "index": 23,
      "title": "Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis",
      "abstract": "",
      "year": "1975",
      "venue": "Journal of the American Statistical Association",
      "authors": "G. J. McLachlan"
    },
    {
      "index": 24,
      "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii",
      "orig_title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
      "paper_id": "1704.03976v2"
    },
    {
      "index": 25,
      "title": "Reading digits in natural images with unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "Workshop on Deep Learning and Unsupervised Feature Learning, NIPS",
      "authors": "Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng"
    },
    {
      "index": 26,
      "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "A. Nguyen, J. Yosinski, and J. Clune",
      "orig_title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "paper_id": "1412.1897v4"
    },
    {
      "index": 27,
      "title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Goodfellow",
      "orig_title": "Realistic evaluation of deep semi-supervised learning algorithms",
      "paper_id": "1804.09170v4"
    },
    {
      "index": 28,
      "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "M. N. Rizve, K. Duarte, Y. S. Rawat, and M. Shah",
      "orig_title": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
      "paper_id": "2101.06329v3"
    },
    {
      "index": 29,
      "title": "Semi-supervised self-training of object detection models",
      "abstract": "",
      "year": "2005",
      "venue": "Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION’05)-Volume 1",
      "authors": "C. Rosenberg, M. Hebert, and H. Schneiderman"
    },
    {
      "index": 30,
      "title": "Probability of error of some adaptive pattern-recognition machines",
      "abstract": "",
      "year": "1965",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "H. Scudder"
    },
    {
      "index": 31,
      "title": "FixMatch: Simplifying semi-supervised learning with consistency and confidence",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li"
    },
    {
      "index": 32,
      "title": "Graph-based semi-supervised learning: A comprehensive review",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Z. Song, X. Yang, Z. Xu, and I. King"
    },
    {
      "index": 33,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus"
    },
    {
      "index": 34,
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Tarvainen and H. Valpola",
      "orig_title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "paper_id": "1703.01780v6"
    },
    {
      "index": 35,
      "title": "CReST: A class-rebalancing self-training framework for imbalanced semi-supervised learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "C. Wei, K. Sohn, C. Mellina, A. Yuille, and F. Yang"
    },
    {
      "index": 36,
      "title": "Unsupervised data augmentation for consistency training",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le"
    },
    {
      "index": 37,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le"
    },
    {
      "index": 38,
      "title": "Wide Residual Networks",
      "abstract": "",
      "year": "2016",
      "venue": "British Machine Vision Conference, British Machine Vision Association",
      "authors": "S. Zagoruyko and N. Komodakis",
      "orig_title": "Wide residual networks",
      "paper_id": "1605.07146v4"
    },
    {
      "index": 39,
      "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and T. Shinozaki",
      "orig_title": "FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "paper_id": "2110.08263v3"
    },
    {
      "index": 40,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 41,
      "title": "Learning from labeled and unlabeled data with label propagation",
      "abstract": "",
      "year": "2002",
      "venue": "Technical report, CMU-CALD-02-107, Carnegie Mellon University",
      "authors": "X. Zhu and Z. Ghahramani"
    },
    {
      "index": 42,
      "title": "Introduction to semi-supervised learning",
      "abstract": "",
      "year": "2009",
      "venue": "Synthesis lectures on artificial intelligence and machine learning",
      "authors": "X. Zhu and A. B. Goldberg"
    },
    {
      "index": 43,
      "title": "Semi-supervised learning literature survey",
      "abstract": "",
      "year": "2005",
      "venue": "Technical report, University of Wisconsin-Madison Department of Computer Sciences",
      "authors": "X. J. Zhu"
    }
  ]
}