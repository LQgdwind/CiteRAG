{
  "paper_id": "2105.14803v1",
  "title": "Gradient-based Data Subversion Attack Against Binary Classifiers",
  "sections": {
    "summary and remarks on defenses": "Related works on defenses. In the early days, whenever a classifier was defeated, the only solution to the problem was to retrain the classifier. Dalvi et al  viewed classification as a game between the classifier and the adversary, thereby created a heuristic based algorithm to produce a classifier that defended against the attacks made by the adversary. Biggio et al [ref]7 have a good survey of defensive mechanisms developed over years. Many encouraging mechanisms have been developed by employing concepts in game theory and robust statistics  [ref]2  that are stable theoretically. Defensive mechanisms can be categorized into reactive and proactive defenses where reactive defenses aim to counter past attacks while proactive defenses aim to prevent future attacks. Adversarial attacks do not have a clear-cut set of rules. An attacker can formulate an attack from scratch and execute it. The two main difficulties while developing defense strategies are that 1. the attacker’s goals are hard to estimate 2. most of the defense strategies assume that the data they handle is clean whereas in reality gathering reliable labeled information is expensive considering the human intervention involved. Previous studies have shown that data sanitization can be used for mitigating the data poisoning attack  . For example, if the given data point is far away from the distribution of the training data, then it is likely that the data point is poisoned. Besides, we can make the underlying model robust enough to prevent the attack. Training multiple classifiers and creating robust examples for the training phase are some of the strategies used to improve the robustness of the classifier  0. Apart from that, Zhao et al 3 discuss identification of the attack point and its adjacent points as one defensive mechanism. They also study game models for developing more secure learning algorithms.\nMoreover, transfer attacks gained growing interest in the recent past. In a recent study, Shumailov et al.  discuss blocking of transferability of attacks against neural networks in computer vision applications. Possible future direction on defenses. The learning capability of a model depends on the method used to convert raw features to model features. For example, Anderson et al  used a combination of techniques such as hashing trick and histogram representation for creating suitable malware classifier features. Given that the victim model is a black-box to the attacker, he would have to choose a suitable feature conversion procedure for being able achieve his goal. Hence, we speculate that the choice of methods for raw feature conversion may affect the defensive capability of a classifier. When the dataset is huge, defensive mechanisms such as training multiple classifiers or generating adversarial training points to make robust classifiers are expensive procedures in terms of cost and complexity. Moreover, centralized systems used to store data results in increased exposure of sensitive data and can lead to many attacks that cause extensive damages. Blockchains could be a potential remedy for this problem. A blockchain is a system in which a record of transactions is maintained across several computers that are linked in a peer-to-peer network. The data is stored across a network of nodes, instead of a single central node since a blockchain is decentralized 5. If the data is maintained in a peer-to-peer network and each node runs its own different ‘sanity check’ algorithm, then the attacker has a real tough job at hand. Although blockchain is not immune to problems, it would be interesting to see if we are able to use it to maximum advantage and create a defensive mechanism based on it."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Explicit defense actions against test-set attacks",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Alfeld, S., Zhu, X., Barford, P."
    },
    {
      "index": 1,
      "title": "Stackelberg security games (ssg) basics and application overview",
      "abstract": "",
      "year": "2016",
      "venue": "Improving Homeland Security Decisions. Cambridge Univ. Press",
      "authors": "An, B., Tambe, M., Sinha, A."
    },
    {
      "index": 2,
      "title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.04637",
      "authors": "Anderson, H.S., Roth, P.",
      "orig_title": "Ember: An open dataset for training static pe malware machine learning models",
      "paper_id": "1804.04637v2"
    },
    {
      "index": 3,
      "title": "Bagging classifiers for fighting poisoning attacks in adversarial classification tasks",
      "abstract": "",
      "year": "2011",
      "venue": "International workshop on multiple classifier systems",
      "authors": "Biggio, B., Corona, I., Fumera, G., Giacinto, G., Roli, F."
    },
    {
      "index": 4,
      "title": "Support vector machines under adversarial label noise",
      "abstract": "",
      "year": "2011",
      "venue": "Asian Conference on Machine Learning",
      "authors": "Biggio, B., Nelson, B., Laskov, P."
    },
    {
      "index": 5,
      "title": "Poisoning attacks against support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "29th International Coference on International Conference on Machine Learning. Omnipress",
      "authors": "Biggio, B., Nelson, B., Laskov, P."
    },
    {
      "index": 6,
      "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "Biggio, B., Roli, F.",
      "orig_title": "Wild patterns: Ten years after the rise of adversarial machine learning",
      "paper_id": "1712.03141v2"
    },
    {
      "index": 7,
      "title": "XGBoost: A Scalable Tree Boosting System",
      "abstract": "",
      "year": "2016",
      "venue": "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM",
      "authors": "Chen, T., Guestrin, C.",
      "orig_title": "Xgboost: A scalable tree boosting system",
      "paper_id": "1603.02754v3"
    },
    {
      "index": 8,
      "title": "Casting out demons: Sanitizing training data for anomaly sensors",
      "abstract": "",
      "year": "2008",
      "venue": "2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE",
      "authors": "Cretu, G.F., Stavrou, A., Locasto, M.E., Stolfo, S.J., Keromytis, A.D."
    },
    {
      "index": 9,
      "title": "Adversarial classification",
      "abstract": "",
      "year": "2004",
      "venue": "tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM",
      "authors": "Dalvi, N., Domingos, P., Sanghai, S., Verma, D., et al."
    },
    {
      "index": 10,
      "title": "Securing Machine Learning against Adversarial Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Ph.D. thesis, University of Cagliari",
      "authors": "Demontis, A."
    },
    {
      "index": 11,
      "title": "Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "28th USENIX Security Symposium (USENIX Security 19). USENIX Association, Santa Clara, CA",
      "authors": "Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru, C., Roli, F.",
      "orig_title": "Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks",
      "paper_id": "1809.02861v4"
    },
    {
      "index": 12,
      "title": "UCI machine learning repository",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Dua, D., Graff, C."
    },
    {
      "index": 13,
      "title": "Greedy function approximation: a gradient boosting machine",
      "abstract": "",
      "year": "2001",
      "venue": "Annals of statistics",
      "authors": "Friedman, J.H."
    },
    {
      "index": 14,
      "title": "Making machine learning robust against adversarial inputs",
      "abstract": "",
      "year": "2018",
      "venue": "Communications of the ACM",
      "authors": "Goodfellow, I., McDaniel, P., Papernot, N."
    },
    {
      "index": 15,
      "title": "Insight into Insiders and IT: A Survey of Insider Threat Taxonomies, Analysis, Modeling, and Countermeasures",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "Homoliak, I., Toffalini, F., Guarnizo, J., Elovici, Y., Ochoa, M.",
      "orig_title": "Insight into insiders and it: A survey of insider threat taxonomies, analysis, modeling, and countermeasures",
      "paper_id": "1805.01612v2"
    },
    {
      "index": 16,
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.Y."
    },
    {
      "index": 17,
      "title": "Understanding Black-box Predictions via Influence Functions",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70. JMLR. org",
      "authors": "Koh, P.W., Liang, P.",
      "orig_title": "Understanding black-box predictions via influence functions",
      "paper_id": "1703.04730v3"
    },
    {
      "index": 18,
      "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.00741",
      "authors": "Koh, P.W., Steinhardt, J., Liang, P.",
      "orig_title": "Stronger data poisoning attacks break data sanitization defenses",
      "paper_id": "1811.00741v2"
    },
    {
      "index": 19,
      "title": "Poster: Attacking malware classifiers by crafting gradient-attacks that preserve functionality",
      "abstract": "",
      "year": "2019",
      "venue": "2019 ACM SIGSAC Conference on Computer and Communications Security. ACM",
      "authors": "Labaca-Castro, R., Biggio, B., Dreo Rodosek, G."
    },
    {
      "index": 20,
      "title": "Shilling recommender systems for fun and profit",
      "abstract": "",
      "year": "2004",
      "venue": "13th international conference on World Wide Web. ACM",
      "authors": "Lam, S.K., Riedl, J."
    },
    {
      "index": 21,
      "title": "Powers of Tensors and Fast Matrix Multiplication",
      "abstract": "",
      "year": "2014",
      "venue": "39th international symposium on symbolic and algebraic computation. ACM",
      "authors": "Le Gall, F.",
      "orig_title": "Powers of tensors and fast matrix multiplication",
      "paper_id": "1401.7714v1"
    },
    {
      "index": 22,
      "title": "Trust region newton methods for large-scale logistic regression",
      "abstract": "",
      "year": "2007",
      "venue": "24th international conference on Machine learning. ACM",
      "authors": "Lin, C.J., Weng, R.C., Keerthi, S.S."
    },
    {
      "index": 23,
      "title": "Good word attacks on statistical spam filters",
      "abstract": "",
      "year": "2005",
      "venue": "Second Conference on Email and Anti-Spam (CEAS)",
      "authors": "Lowd, D."
    },
    {
      "index": 24,
      "title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection",
      "abstract": "",
      "year": "2013",
      "venue": "8th ACM SIGSAC symposium on Information, computer and communications security. ACM",
      "authors": "Maiorca, D., Corona, I., Giacinto, G."
    },
    {
      "index": 25,
      "title": "On the complexity of linear programming",
      "abstract": "",
      "year": "1986",
      "venue": "IBM Thomas J. Watson Research Division",
      "authors": "Megiddo, N., et al."
    },
    {
      "index": 26,
      "title": "Using machine teaching to identify optimal training-set attacks on machine learners",
      "abstract": "",
      "year": "2015",
      "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "authors": "Mei, S., Zhu, X."
    },
    {
      "index": 27,
      "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "10th ACM Workshop on Artificial Intelligence and Security. ACM",
      "authors": "Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E.C., Roli, F.",
      "orig_title": "Towards poisoning of deep learning algorithms with back-gradient optimization",
      "paper_id": "1708.08689v1"
    },
    {
      "index": 28,
      "title": "Exploiting machine learning to subvert your spam filter",
      "abstract": "",
      "year": "2008",
      "venue": "LEET",
      "authors": "Nelson, B., Barreno, M., Chi, F.J., Joseph, A.D., Rubinstein, B.I., Saini, U., Sutton, C.A., Tygar, J.D., Xia, K."
    },
    {
      "index": 29,
      "title": "Distillation as a defense to adversarial perturbations against deep neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE Symposium on Security and Privacy (SP). IEEE",
      "authors": "Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A."
    },
    {
      "index": 30,
      "title": "Label Sanitization against Label Flipping Poisoning Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer",
      "authors": "Paudice, A., Muñoz-González, L., Lupu, E.C.",
      "orig_title": "Label sanitization against label flipping poisoning attacks",
      "paper_id": "1803.00992v2"
    },
    {
      "index": 31,
      "title": "Microsoft is deleting its ai chatbot’s incredibly racist tweets",
      "abstract": "",
      "year": "2016",
      "venue": "Business Insider",
      "authors": "Price, R."
    },
    {
      "index": 32,
      "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers",
      "abstract": "",
      "year": "2018",
      "venue": "International Symposium on Research in Attacks, Intrusions, and Defenses. Springer",
      "authors": "Rosenberg, I., Shabtai, A., Rokach, L., Elovici, Y.",
      "orig_title": "Generic black-box end-to-end attack against state of the art api call based malware classifiers",
      "paper_id": "1707.05970v5"
    },
    {
      "index": 33,
      "title": "Sitatapatra: Blocking the Transfer of Adversarial Samples",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.08121",
      "authors": "Shumailov, I., Gao, X., Zhao, Y., Mullins, R., Anderson, R., Xu, C.Z.",
      "orig_title": "Sitatapatra: Blocking the transfer of adversarial samples",
      "paper_id": "1901.08121v2"
    },
    {
      "index": 34,
      "title": "Certified defenses for data poisoning attacks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Steinhardt, J., Koh, P.W.W., Liang, P.S."
    },
    {
      "index": 35,
      "title": "When does machine learning {{\\{FAIL\\}}}? generalized transferability for evasion and poisoning attacks",
      "abstract": "",
      "year": "2018",
      "venue": "27th USENIX Security Symposium (USENIX Security 18)",
      "authors": "Suciu, O., Marginean, R., Kaya, Y., Daume III, H., Dumitras, T."
    },
    {
      "index": 36,
      "title": "Data Poisoning Attacks against Online Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.08994",
      "authors": "Wang, Y., Chaudhuri, K.",
      "orig_title": "Data poisoning attacks against online learning",
      "paper_id": "1808.08994v1"
    },
    {
      "index": 37,
      "title": "An Investigation of Data Poisoning Defenses for Online Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.12121",
      "authors": "Wang, Y., Chaudhuri, K.",
      "orig_title": "An investigation of data poisoning defenses for online learning",
      "paper_id": "1905.12121v3"
    },
    {
      "index": 38,
      "title": "Adversarial label flips attack on support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "ECAI",
      "authors": "Xiao, H., Xiao, H., Eckert, C."
    },
    {
      "index": 39,
      "title": "Is Feature Selection Secure against Training Data Poisoning?",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F.",
      "orig_title": "Is feature selection secure against training data poisoning?",
      "paper_id": "1804.07933v1"
    },
    {
      "index": 40,
      "title": "Dual coordinate descent methods for logistic regression and maximum entropy models",
      "abstract": "",
      "year": "2011",
      "venue": "Machine Learning",
      "authors": "Yu, H.F., Huang, F.L., Lin, C.J."
    },
    {
      "index": 41,
      "title": "Online Data Poisoning Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1903.01666",
      "authors": "Zhang, X., Zhu, X.",
      "orig_title": "Online data poisoning attack",
      "paper_id": "1903.01666v2"
    },
    {
      "index": 42,
      "title": "Efficient label contamination attacks against black-box learning models",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Zhao, M., An, B., Gao, W., Zhang, T."
    },
    {
      "index": 43,
      "title": "Generalized resilience and robust statistics",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Zhu, B., Jiao, J., Steinhardt, J."
    },
    {
      "index": 44,
      "title": "Decentralizing privacy: Using blockchain to protect personal data",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Security and Privacy Workshops. IEEE",
      "authors": "Zyskind, G., Nathan, O., et al."
    }
  ]
}