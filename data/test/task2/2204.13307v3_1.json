{
  "paper_id": "2204.13307v3",
  "title": "AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time",
  "sections": {
    "v-a related work": "Self-play RL has a long tradition in game learning, with Tesauro’s TD-Gammon  being a very early TD-learning application to Backgammon.\nThe seminal papers of Silver et al. on\nAlphaGo and AlphaZero   lifted this for the games Go, chess and shogi to a new complexity and performance level. They have stirred the interest of many researchers to achieve similar things with smaller hardware requirements and/or fewer training cycles. Thakoor et al.  provided in 2017 a general AlphaZero implementation in Python with less computational demands than the original.\nBut even their architecture requires for 6x6 Othello 3 days of training on a specialized cloud computing service (Google Compute Engine with GPU support).\nSeveral works of Wang et al. [ref]29 0  focus on different aspects of the AlphaZero architecture: alternative loss functions, hyperparameter tuning and warm-start enhancements. They test these aspects\non smaller games like 6x6 Othello or 5x5 ConnectFour. The work of Chang et al. 1 covered several AlphaZero improvements applied to 6x6 Othello. van der Ree and Wiering 2 investigate TD-, Q- and Sarsa-learning for 8x8 Othello with a simple neural network (one hidden layer with 50 neurons). Dawson  introduces a CNN-based and AlphaZero-inspired RL agent named ConnectZero for ConnectFour, which can be played online and which reaches a good playing strength against MCTS1000. Young et al. 2 report on an AlphaZero implementation applied to ConnectFour. Here, training took between 21 and 77 hours of GPU time. Recently in 2022,\nNorelli and Panconesi  presented an approach that is close to our work: They as well pursue the goal to set up an AlphaZero-inspired algorithm at much lower cost than the original AlphaZero . The agent in  is based on a residual DNN and is trained solely by self-play. It is able to play 8x8 Othello and to defeat the strong Othello program Edax  up to level 10. Although much less computationally demanding than the original AlphaZero , their training time took roughly one month on Colaboratory, a free Google cloud computing service offering GPUs and TPUs. See Sec. V-C for a direct comparison between Norelli and Panconesi  and our work. Apart from Norelli and Panconesi , there are only few works on Othello game learning that actually benchmark against Edax: Liskowski et al. 3 presented in 2018 an agent obtained by training a convolutional neural network (CNN) with the help of a database of expert moves. Their agent could defeat Edax up to and including level 2. Our work presented here is based on an earlier Bachelor thesis 4 published in 2020 (but only in German); it presents an n-tuple RL agent for Othello trained in 1.8 hours on standard hardware (no GPU) that can defeat Edax up to and including level 7. For the puzzle Rubik’s Cube, the pioneering work of McAleer 5 and Agostinelli 6 in 2018 and 2019 shows that the 3x3x3 cube can\nbe solved without putting human knowledge or positional-pattern databases into the agent. They solve arbitrary scrambled cubes with a method that is partly inspired by AlphaZero but also contains special tricks for Rubik’s Cube. A work related to GBG   is the general game system Ludii 7.\nLudii is an efficient general game system based on a ludeme library implemented in Java, allowing to play as well as to generate a large variety of strategy games. Currently, all AI agents implemented in Ludii are tree-based agents (MCTS variants or AlphaBeta). GBG, on the other hand,\noffers the possibility to train RL-based algorithms on several games. Soemers et al. 8 describe a bridge between Ludii 7 and Polygames 9, the latter providing DNN algorithms for strategy games.\nSimilar to our work, they couple approximator networks (DNNs) with MCTS, but for different games.\nWith 20 hours of training time, 8 GPUs, 80 CPU cores, and 475 GB of memory allocation per training job, their resource usage is in a different dimension than our training process. \n"
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al."
    },
    {
      "index": 1,
      "title": "Mastering the game of Go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al."
    },
    {
      "index": 2,
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.01815",
      "authors": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al.",
      "orig_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
      "paper_id": "1712.01815v1"
    },
    {
      "index": 3,
      "title": "Online adaptable learning rates for the game Connect-4",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
      "authors": "S. Bagheri, M. Thill, P. Koch, and W. Konen"
    },
    {
      "index": 4,
      "title": "Learning to play Othello with n-tuple systems",
      "abstract": "",
      "year": "2008",
      "venue": "Australian Journal of Intelligent Information Processing",
      "authors": "S. M. Lucas"
    },
    {
      "index": 5,
      "title": "Temporal difference learning of n-tuple networks for the game 2048",
      "abstract": "",
      "year": "2014",
      "venue": "Computational Intelligence and Games (CIG), 2014 IEEE Conference on",
      "authors": "M. Szubert and W. Jaśkowski"
    },
    {
      "index": 6,
      "title": "General Board Game Playing for Education and Research in Generic AI Game Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Games (London)",
      "authors": "W. Konen",
      "orig_title": "General board game playing for education and research in generic AI game learning",
      "paper_id": "1907.06508v1"
    },
    {
      "index": 7,
      "title": "The GBG class interface tutorial V2.3: General board game playing and learning",
      "abstract": "",
      "year": "2022",
      "venue": "TH Köln, Tech. Rep.",
      "authors": "——"
    },
    {
      "index": 8,
      "title": "Reinforcement learning for n-player games: The importance of final adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Bioinspired Methods and Their Applications",
      "authors": "W. Konen and S. Bagheri"
    },
    {
      "index": 9,
      "title": "Final adaptation reinforcement learning for n-player games",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.14375",
      "authors": "——"
    },
    {
      "index": 10,
      "title": "Mastering 2048 with delayed temporal coherence learning, multistage weight promotion, redundant encoding, and carousel shaping",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Trans. on Games",
      "authors": "W. Jaśkowski"
    },
    {
      "index": 11,
      "title": "Reinforcement learning in the game of Othello: Learning against a fixed opponent and learning from self-play.",
      "abstract": "",
      "year": "2013",
      "venue": "Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)",
      "authors": "M. van der Ree and M. Wiering"
    },
    {
      "index": 12,
      "title": "Reinforcement learning for board games: The temporal difference algorithm",
      "abstract": "",
      "year": "2015",
      "venue": "TH Köln, Tech. Rep.",
      "authors": "W. Konen"
    },
    {
      "index": 13,
      "title": "Temporal coherence and prediction decay in TD learning",
      "abstract": "",
      "year": "1999",
      "venue": "Int. Joint Conf. on Artificial Intelligence (IJCAI)",
      "authors": "D. F. Beal and M. C. Smith"
    },
    {
      "index": 14,
      "title": "Bandit based Monte-Carlo planning",
      "abstract": "",
      "year": "2006",
      "venue": "17th European Conference on Machine Learning, ECML’06",
      "authors": "L. Kocsis and C. Szepesvári"
    },
    {
      "index": 15,
      "title": "A survey of Monte Carlo tree search methods",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
      "authors": "C. B. Browne, E. Powley et al."
    },
    {
      "index": 16,
      "title": "Multi-armed bandits with episode context",
      "abstract": "",
      "year": "2011",
      "venue": "Annals of Mathematics and Artificial Intelligence",
      "authors": "C. D. Rosin"
    },
    {
      "index": 17,
      "title": "Pattern recognition and reading by machine",
      "abstract": "",
      "year": "1959",
      "venue": "Eastern Joint Computer Conference",
      "authors": "W. W. Bledsoe and I. Browning"
    },
    {
      "index": 18,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT Press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 19,
      "title": "Edax, version 4.4",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "R. Delorme"
    },
    {
      "index": 20,
      "title": "OLIVAW: Mastering Othello without human knowledge, nor a penny",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Games",
      "authors": "A. Norelli and A. Panconesi"
    },
    {
      "index": 21,
      "title": "Temporal difference learning with eligibility traces for the game Connect-4",
      "abstract": "",
      "year": "2014",
      "venue": "International Conf on Computational Intelligence in Games (CIG), Dortmund",
      "authors": "M. Thill, S. Bagheri, P. Koch, and W. Konen"
    },
    {
      "index": 22,
      "title": "Warm-Start AlphaZero Self-Play Search Enhancements",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Parallel Problem Solving from Nature",
      "authors": "H. Wang, M. Preuss, and A. Plaat",
      "orig_title": "Warm-start AlphaZero self-play search enhancements",
      "paper_id": "2004.12357v1"
    },
    {
      "index": 23,
      "title": "Learning to play Connect-4 with deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "R. Dawson"
    },
    {
      "index": 24,
      "title": "The diameter of the Rubik’s Cube group is twenty",
      "abstract": "",
      "year": "2014",
      "venue": "SIAM Review",
      "authors": "T. Rokicki, H. Kociemba, M. Davidson, and J. Dethridge"
    },
    {
      "index": 25,
      "title": "Towards learning Rubik’s Cube with n-tuple-based reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "TH Köln (in preparation), Tech. Rep.",
      "authors": "W. Konen"
    },
    {
      "index": 26,
      "title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play",
      "abstract": "",
      "year": "1994",
      "venue": "Neural computation",
      "authors": "G. Tesauro"
    },
    {
      "index": 27,
      "title": "Learning to play Othello without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "S. Thakoor, S. Nair, and M. Jhunjhunwala"
    },
    {
      "index": 28,
      "title": "Alternative loss functions in AlphaZero-like self-play",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE Symposium Series on Computational Intelligence (SSCI)",
      "authors": "H. Wang, M. Emmerich, M. Preuss, and A. Plaat"
    },
    {
      "index": 29,
      "title": "Hyper-Parameter Sweep on AlphaZero General",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1903.08129",
      "authors": "——",
      "orig_title": "Hyper-parameter sweep on AlphaZero general",
      "paper_id": "1903.08129v1"
    },
    {
      "index": 30,
      "title": "The big win strategy on multi-value network: An improvement over AlphaZero approach for 6x6 Othello",
      "abstract": "",
      "year": "2018",
      "venue": "2018 International Conference on Machine Learning and Machine Intelligence",
      "authors": "N.-Y. Chang, C.-H. Chen, S.-S. Lin, and S. Nair"
    },
    {
      "index": 31,
      "title": "Lessons from implementing AlphaZero, part 6",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "A. Young, A. Prasad, and I. Abrams"
    },
    {
      "index": 32,
      "title": "Learning to Play Othello with Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Games",
      "authors": "P. Liskowski, W. Jaśkowski, and K. Krawiec",
      "orig_title": "Learning to play Othello with deep neural networks",
      "paper_id": "1711.06583v1"
    },
    {
      "index": 33,
      "title": "AlphaZero-inspirierte KI-Agenten im General Board Game Playing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "J. Scheiermann"
    },
    {
      "index": 34,
      "title": "Solving the Rubik’s Cube with approximate policy iteration",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "S. McAleer, F. Agostinelli, A. Shmakov, and P. Baldi"
    },
    {
      "index": 35,
      "title": "Solving the Rubik’s Cube with deep reinforcement learning and search",
      "abstract": "",
      "year": "2019",
      "venue": "Nature Machine Intelligence",
      "authors": "F. Agostinelli, S. McAleer, A. Shmakov, and P. Baldi"
    },
    {
      "index": 36,
      "title": "Ludii - the ludemic general game system",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "É. Piette, D. J. N. J. Soemers, M. Stephenson, C. F. Sironi, M. H. M. Winands, and C. Browne"
    },
    {
      "index": 37,
      "title": "Deep Learning for General Game Playing with Ludii and Polygames",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.09562",
      "authors": "D. J. Soemers, V. Mella, C. Browne, and O. Teytaud",
      "orig_title": "Deep learning for general game playing with Ludii and Polygames",
      "paper_id": "2101.09562v1"
    },
    {
      "index": 38,
      "title": "Polygames: Improved Zero Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICGA Journal",
      "authors": "T. Cazenave, Y.-C. Chen, G.-W. Chen, S.-Y. Chen, X.-D. Chiu, J. Dehos, M. Elsa, Q. Gong, H. Hu, V. Khalidov et al.",
      "orig_title": "Polygames: Improved zero learning",
      "paper_id": "2001.09832v1"
    },
    {
      "index": 39,
      "title": "An agent for EinStein Würfelt Nicht! using n-tuple networks",
      "abstract": "",
      "year": "2017",
      "venue": "Conf. on Technologies and Applications of AI (TAAI)",
      "authors": "Y. R. Chu, Y. Chen, C. Hsueh, and I. Wu"
    },
    {
      "index": 40,
      "title": "High-dimensional function approximation for knowledge-free reinforcement learning: A case study in SZ-Tetris",
      "abstract": "",
      "year": "2015",
      "venue": "Conf. on Genetic and Evolutionary Computation",
      "authors": "W. Jaśkowski, M. Szubert, P. Liskowski, and K. Krawiec"
    },
    {
      "index": 41,
      "title": "Self-adaptive MCTS for general video game playing",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on the Applications of Evolutionary Computation",
      "authors": "C. F. Sironi, J. Liu, D. Perez-Liebana, R. D. Gaina, I. Bravi, S. M. Lucas, and M. H. Winands"
    },
    {
      "index": 42,
      "title": "KI-Agenten für das Spiel 2048: Untersuchung von Lernalgorithmen für nichtdeterministische Spiele",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "J. Kutsch"
    }
  ]
}