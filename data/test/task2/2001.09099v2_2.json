{
  "paper_id": "2001.09099v2",
  "title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval",
  "sections": {
    "related work": "The goal of natural language-based moment retrieval is to retrieve relevant moments from a single video¬†[ref]14 [ref]9 or from a large video corpus¬†[ref]8.\nIn the following, we present a brief overview of the community efforts on these tasks and make distinctions between existing works and ours. Datasets.\nSeveral datasets have been proposed for the task, e.g., DiDeMo¬†[ref]14, ActivityNet Captions¬†, CharadesSTA¬†[ref]9, and TACoS¬†, where queries can be localized solely from video.\nTVR differs from them by requiring additional text (subtitle) information in localizing the queries.\nTwo types of data annotation have been explored in previous works:\n(iùëñi) uniformly chunking videos into segments and letting an annotator pick one (or more) and write an unambiguous description¬†[ref]14.\nFor example, moments in DiDeMo¬†[ref]14 are created from fixed 5-second segments.\nHowever, such coarse temporal annotations are not well aligned with natural moments.\nIn TVR, temporal windows are freely selected to more accurately capture important moments.\n(i‚Äãiùëñùëñii) converting a paragraph written for a whole video into separate query sentences¬† [ref]9 .\nWhile it is natural for people to use temporal connectives (e.g., ‚Äòfirst‚Äô, ‚Äòthen‚Äô) and anaphora (e.g., pronouns)¬† in a paragraph, these words make individual sentences less suitable as retrieval queries.\nIn comparison, the TVR annotation process encourages annotators to write queries individually without requiring the context of a paragraph.\nBesides, TVR also has a larger size and greater linguistic diversity, see Sec.¬†3.2. Methods.\nExisting works¬†[ref]14 [ref]9    [ref]8 pose moment retrieval as ranking a predefined set of moment proposals.\nThese proposals are typically generated with handcrafted rules¬†[ref]14  or sliding windows¬†[ref]9   [ref]8.\nTypically, such proposals are not temporally precise and are not scalable to long videos due to high computational cost.¬†[ref]9   alleviate the first with a regression branch that offsets the proposals. However, they are still restricted by the coarseness of the initial proposals.\nInspired by span predictors in reading comprehension¬†  and action localization¬†, we use start-end predictors to predict start-end probabilities from early fused query-video representations.\nThough these methods can be more flexibly applied to long videos and have shown promising performance on single video moment retrieval, the time cost of early fusion becomes unbearable when dealing with the corpus level moment retrieval problem: they require early fusing every possible query-video pair¬†[ref]8.\nProposal based approaches MCN¬†[ref]14 and CAL¬†[ref]8 use a late fusion design, in which the video representations can be pre-computed and stored, making the retrieval more efficient.\nThe final moment predictions are then made by ranking the Squared Euclidean Distances between the proposals w.r.t. a given query.\nHowever, as they rely on predefined proposals, MCN and CAL still suffer from the aforementioned drawbacks, leading to less precise predictions and higher costs (especially for long videos).\nRecent works¬†  9 consider word-level early fusion with the videos, which can be even more expensive.\nIn contrast, XML uses a late fusion design with a novel Convolutional Start-End (ConvSE) detector, which produces more accurate moment predictions while reducing the computational cost."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Layer normalization",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint",
      "authors": "Ba, J.L., Kiros, J.R., Hinton, G.E."
    },
    {
      "index": 1,
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Carreira, J., Zisserman, A.",
      "orig_title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "paper_id": "1705.07750v3"
    },
    {
      "index": 2,
      "title": "Reading Wikipedia to Answer Open-Domain Questions",
      "abstract": "",
      "year": "2017",
      "venue": "ACL",
      "authors": "Chen, D., Fisch, A., Weston, J., Bordes, A.",
      "orig_title": "Reading wikipedia to answer open-domain questions",
      "paper_id": "1704.00051v2"
    },
    {
      "index": 3,
      "title": "Temporally grounding natural sentence in video",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Chen, J., Chen, X., Ma, L., Jie, Z., Chua, T.S."
    },
    {
      "index": 4,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L."
    },
    {
      "index": 5,
      "title": "Meteor universal: Language specific translation evaluation for any target language",
      "abstract": "",
      "year": "2014",
      "venue": "ninth workshop on statistical machine translation",
      "authors": "Denkowski, M., Lavie, A."
    },
    {
      "index": 6,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 7,
      "title": "Temporal localization of moments in video collections with natural language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Escorcia, V., Soldan, M., Sivic, J., Ghanem, B., Russell, B."
    },
    {
      "index": 8,
      "title": "TALL: Temporal Activity Localization via Language Query",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Gao, J., Sun, C., Yang, Z., Nevatia, R.",
      "orig_title": "Tall: Temporal activity localization via language query",
      "paper_id": "1705.02101v2"
    },
    {
      "index": 9,
      "title": "MAC: Mining Activity Concepts for Language-based Temporal Localization",
      "abstract": "",
      "year": "2019",
      "venue": "WACV",
      "authors": "Ge, R., Gao, J., Chen, K., Nevatia, R.",
      "orig_title": "Mac: Mining activity concepts for language-based temporal localization",
      "paper_id": "1811.08925v1"
    },
    {
      "index": 10,
      "title": "ExCL: Extractive Clip Localization Using Natural Language Descriptions",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Ghosh, S., Agarwal, A., Parekh, Z., Hauptmann, A.",
      "orig_title": "Excl: Extractive clip localization using natural language descriptions",
      "paper_id": "1904.02755v1"
    },
    {
      "index": 11,
      "title": "Deep sparse rectifier neural networks",
      "abstract": "",
      "year": "2011",
      "venue": "AISTATS",
      "authors": "Glorot, X., Bordes, A., Bengio, Y."
    },
    {
      "index": 12,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "He, K., Zhang, X., Ren, S., Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 13,
      "title": "Localizing Moments in Video with Natural Language",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.",
      "orig_title": "Localizing moments in video with natural language",
      "paper_id": "1708.01641v1"
    },
    {
      "index": 14,
      "title": "Localizing Moments in Video with Temporal Language",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Hendricks, L.A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.",
      "orig_title": "Localizing moments in video with temporal language",
      "paper_id": "1809.01337v1"
    },
    {
      "index": 15,
      "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J., et¬†al."
    },
    {
      "index": 16,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Hochreiter, S., Schmidhuber, J."
    },
    {
      "index": 17,
      "title": "Billion-scale similarity search with GPUs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Big Data",
      "authors": "Johnson, J., Douze, M., J√©gou, H.",
      "orig_title": "Billion-scale similarity search with gpus",
      "paper_id": "1702.08734v1"
    },
    {
      "index": 18,
      "title": "The Kinetics Human Action Video Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et¬†al.",
      "orig_title": "The kinetics human action video dataset",
      "paper_id": "1705.06950v1"
    },
    {
      "index": 19,
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T."
    },
    {
      "index": 20,
      "title": "Deepstory: Video story qa by deep embedded memory networks",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Kim, K.M., Heo, M.O., Choi, S.H., Zhang, B.T."
    },
    {
      "index": 21,
      "title": "Dense-Captioning Events in Videos",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.",
      "orig_title": "Dense-captioning events in videos",
      "paper_id": "1705.00754v1"
    },
    {
      "index": 22,
      "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T.L., Bansal, M.",
      "orig_title": "Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning",
      "paper_id": "2005.05402v1"
    },
    {
      "index": 23,
      "title": "TVQA: Localized, Compositional Video Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Lei, J., Yu, L., Bansal, M., Berg, T.L.",
      "orig_title": "Tvqa: Localized, compositional video question answering",
      "paper_id": "1809.01696v2"
    },
    {
      "index": 24,
      "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Lei, J., Yu, L., Berg, T.L., Bansal, M.",
      "orig_title": "Tvqa+: Spatio-temporal grounding for video question answering",
      "paper_id": "1904.11574v2"
    },
    {
      "index": 25,
      "title": "Rouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "ACL",
      "authors": "Lin, C.Y."
    },
    {
      "index": 26,
      "title": "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.",
      "orig_title": "Bsn: Boundary sensitive network for temporal action proposal generation",
      "paper_id": "1806.02964v3"
    },
    {
      "index": 27,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 28,
      "title": "Learning a Text-Video Embedding from Incomplete and Heterogeneous Data",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "Miech, A., Laptev, I., Sivic, J.",
      "orig_title": "Learning a text-video embedding from incomplete and heterogeneous data",
      "paper_id": "1804.02516v2"
    },
    {
      "index": 29,
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "ACL",
      "authors": "Papineni, K., Roukos, S., Ward, T., Zhu, W.J."
    },
    {
      "index": 30,
      "title": "Automatic differentiation in PyTorch",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS Autodiff Workshop",
      "authors": "Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A."
    },
    {
      "index": 31,
      "title": "Grounding action descriptions in videos",
      "abstract": "",
      "year": "2013",
      "venue": "TACL",
      "authors": "Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., Pinkal, M."
    },
    {
      "index": 32,
      "title": "The watershed transform: Definitions, algorithms and parallelization strategies",
      "abstract": "",
      "year": "2000",
      "venue": "Fundamenta informaticae",
      "authors": "Roerdink, J.B., Meijster, A."
    },
    {
      "index": 33,
      "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail",
      "abstract": "",
      "year": "2014",
      "venue": "GCPR",
      "authors": "Rohrbach, A., Rohrbach, M., Qiu, W., Friedrich, A., Pinkal, M., Schiele, B.",
      "orig_title": "Coherent multi-sentence video description with variable level of detail",
      "paper_id": "1403.6173v1"
    },
    {
      "index": 34,
      "title": "Movie description",
      "abstract": "",
      "year": "2017",
      "venue": "IJCV",
      "authors": "Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H., Courville, A., Schiele, B."
    },
    {
      "index": 35,
      "title": "Bi-Directional Attention Flow for Machine Comprehension",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Seo, M., Kembhavi, A., Farhadi, A., Hajishirzi, H.",
      "orig_title": "Bidirectional attention flow for machine comprehension",
      "paper_id": "1611.01603v6"
    },
    {
      "index": 36,
      "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.",
      "orig_title": "Hollywood in homes: Crowdsourcing data collection for activity understanding",
      "paper_id": "1604.01753v3"
    },
    {
      "index": 37,
      "title": "Computer vision: algorithms and applications",
      "abstract": "",
      "year": "2010",
      "venue": "Springer Science & Business Media",
      "authors": "Szeliski, R."
    },
    {
      "index": 38,
      "title": "Movieqa: Understanding stories in movies through question-answering",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S."
    },
    {
      "index": 39,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å., Polosukhin, I.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 40,
      "title": "CIDEr: Consensus-based Image Description Evaluation",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Vedantam, R., Lawrence¬†Zitnick, C., Parikh, D.",
      "orig_title": "Cider: Consensus-based image description evaluation",
      "paper_id": "1411.5726v2"
    },
    {
      "index": 41,
      "title": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y."
    },
    {
      "index": 42,
      "title": "Multilevel Language and Vision Integration for Text-to-Clip Retrieval",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Xu, H., He, K., Plummer, B.A., Sigal, L., Sclaroff, S., Saenko, K.",
      "orig_title": "Multilevel language and vision integration for text-to-clip retrieval",
      "paper_id": "1804.05113v3"
    },
    {
      "index": 43,
      "title": "Msr-vtt: A large video description dataset for bridging video and language",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Xu, J., Mei, T., Yao, T., Rui, Y."
    },
    {
      "index": 44,
      "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Yu, A.W., Dohan, D., Luong, M.T., Zhao, R., Chen, K., Norouzi, M., Le, Q.V.",
      "orig_title": "Qanet: Combining local convolution with global self-attention for reading comprehension",
      "paper_id": "1804.09541v1"
    },
    {
      "index": 45,
      "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.",
      "orig_title": "Mattnet: Modular attention network for referring expression comprehension",
      "paper_id": "1801.08186v3"
    },
    {
      "index": 46,
      "title": "From Recognition to Cognition: Visual Commonsense Reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.",
      "orig_title": "From recognition to cognition: Visual commonsense reasoning",
      "paper_id": "1811.10830v2"
    },
    {
      "index": 47,
      "title": "MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhang, D., Dai, X., Wang, X., fang Wang, Y., Davis, L.S.",
      "orig_title": "Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment",
      "paper_id": "1812.00087v2"
    },
    {
      "index": 48,
      "title": "Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos",
      "abstract": "",
      "year": "2019",
      "venue": "SIGIR",
      "authors": "Zhang, Z., Lin, Z., Zhao, Z., Xiao, Z.",
      "orig_title": "Cross-modal interaction networks for query-based moment retrieval in videos",
      "paper_id": "1906.02497v2"
    },
    {
      "index": 49,
      "title": "Temporal action detection with structured segment networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin, D."
    },
    {
      "index": 50,
      "title": "Towards automatic learning of procedures from web instructional videos",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Zhou, L., Xu, C., Corso, J.J."
    }
  ]
}