{
  "paper_id": "2004.14973v2",
  "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
  "sections": {
    "preliminaries: self-supervised learning from the web": "Recent works have demonstrated that high-capacity models trained under self-supervised objectives on large-scale web data can learn strong, generalizable representations for both language and images [ref]7  [ref]18  6  . We build upon these works as a basis for transfer and describe them briefly here. Language Modeling with BERT.\nThe BERT [ref]7 model is a large transformer-based  architecture for language modeling. The model takes as input sequences of tokenized words augmented with positional embeddings and outputs a representation for each. For example, a two sentence input could be written as where CLS, and SEP are special tokens. To train this approach, [ref]7 introduce two self-supervised objectives ‚Äì masked language modelling and next sentence prediction. Given two input sentences from a text corpus, the masked language modelling objective masks out some percentage of tokens and tasks the model to predict their values given the remaining tokens as context. The next sentence prediction objective requires the model to predict whether the two sentences follow each other in the original corpus or not. BERT is then trained under these objectives on large language corpuses from the web (Wikipedia and BooksCorpus¬†). This model forms the basis for both our approach and the visiolinguistic representation learning discussed next. Visiolinguistic Representations Learning with ViLBERT.\nExtending BERT, ViLBERT¬†[ref]18 (and a number of similar approaches  [ref]18  6  ) focuses on learning joint visiolinguistic representations from paired image-text data, specifically web images and their associated alt-text collected in the Conceptual Captions dataset . ViLBERT is composed of two BERT-like processing streams that operate on visual and textual inputs, respectively. The input to the visual stream is composed of image regions (generated by an object detector¬† 2 pretrained on Visual Genome¬†) that act as ‚Äúwords‚Äù in the visual domain. Concretely, given a single image IùêºI consisting of a set of image regions {v1,‚Ä¶,vk}subscriptùë£1‚Ä¶subscriptùë£ùëò\\{v_{1},\\dots,v_{k}\\} and a text sequence (i.e. a caption) w1,‚Ä¶,wLsubscriptùë§1‚Ä¶subscriptùë§ùêøw_{1},\\dots,w_{L}, we can write the input to ViLBERT as the sequence where IMG, CLS, and SEP are special tokens marking the different modality sub-sequences. The two streams are connected using co-attention¬† transformer layers, which attend from the visual stream over language stream and vice versa. Notably, the language stream of ViLBERT is designed to mirror BERT such that it can be initialized by a pretrained BERT model. After processing, the model produces a contextualized output representation for each input token. In analogy to the training objectives in BERT, ViLBERT introduces the masked multimodal modelling and multimodal alignment tasks. In the first, a random subset of language tokens and image regions are masked and must be predicted given the remaining context. For image regions, this amounts to predicting a distribution over object classes present in the masked region. Masked text tokens are handled as in BERT. The multimodal alignment objective trains the model to determine if an image-text pair matches, i.e. if the text describes the image content. Individual token outputs are used to predict masked inputs in the masking objective, and the IMG and CLS tokens are used for the image-caption alignment objective. We build upon this model extensively in this work."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "On evaluation of embodied navigation agents",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.06757",
      "authors": "Anderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et¬†al."
    },
    {
      "index": 1,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 2,
      "title": "Chasing ghosts: Instruction following as bayesian state tracking",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Anderson, P., Shrivastava, A., Parikh, D., Batra, D., Lee, S."
    },
    {
      "index": 3,
      "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S√ºnderhauf, N., Reid, I., Gould, S., van¬†den Hengel, A.",
      "orig_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
      "paper_id": "1711.07280v3"
    },
    {
      "index": 4,
      "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on 3D Vision (3DV)",
      "authors": "Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.",
      "orig_title": "Matterport3d: Learning from rgb-d data in indoor environments",
      "paper_id": "1709.06158v1"
    },
    {
      "index": 5,
      "title": "Microsoft coco captions: Data collection and evaluation server",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1504.00325",
      "authors": "Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll√°r, P., Zitnick, C.L."
    },
    {
      "index": 6,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 7,
      "title": "Speaker-Follower Models for Vision-and-Language Navigation",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.P., Berg-Kirkpatrick, T., Saenko, K., Klein, D., Darrell, T.",
      "orig_title": "Speaker-follower models for vision-and-language navigation",
      "paper_id": "1806.02724v2"
    },
    {
      "index": 8,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.",
      "orig_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 9,
      "title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.10638",
      "authors": "Hao, W., Li, C., Li, X., Carin, L., Gao, J."
    },
    {
      "index": 10,
      "title": "Multi-modal Discriminative Model for Vision-and-Language Navigation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.13358",
      "authors": "Huang, H., Jain, V., Mehta, H., Baldridge, J., Ie, E.",
      "orig_title": "Multi-modal discriminative model for vision-and-language navigation",
      "paper_id": "1905.13358v1"
    },
    {
      "index": 11,
      "title": "Referit game: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L."
    },
    {
      "index": 12,
      "title": "Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Ke, L., Li, X., Bisk, Y., Holtzman, A., Gan, Z., Liu, J., Gao, J., Choi, Y., Srinivasa, S.",
      "orig_title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation",
      "paper_id": "1903.02547v2"
    },
    {
      "index": 13,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1602.07332",
      "authors": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L."
    },
    {
      "index": 14,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.06066",
      "authors": "Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 15,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 16,
      "title": "Robust navigation with language pretraining and stochastic sampling",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.02244",
      "authors": "Li, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N., Choi, Y."
    },
    {
      "index": 17,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Lu, J., Batra, D., Parikh, D., Lee, S."
    },
    {
      "index": 18,
      "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Lu, J., Yang, J., Batra, D., Parikh, D.",
      "orig_title": "Hierarchical question-image co-attention for visual question answering",
      "paper_id": "1606.00061v5"
    },
    {
      "index": 19,
      "title": "Self-monitoring navigation agent via auxiliary progress estimation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.03035",
      "authors": "Ma, C.Y., Lu, J., Wu, Z., AlRegib, G., Kira, Z., Socher, R., Xiong, C."
    },
    {
      "index": 20,
      "title": "The regretful agent: Heuristic-aided navigation through progress estimation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Ma, C.Y., Wu, Z., AlRegib, G., Xiong, C., Kira, Z."
    },
    {
      "index": 21,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NIPS)",
      "authors": "Ren, S., He, K., Girshick, R., Sun, J.",
      "orig_title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 22,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International journal of computer vision",
      "authors": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et¬†al.",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 23,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Sharma, P., Ding, N., Goodman, S., Soricut, R."
    },
    {
      "index": 24,
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6034",
      "authors": "Simonyan, K., Vedaldi, A., Zisserman, A."
    },
    {
      "index": 25,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.08530",
      "authors": "Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J."
    },
    {
      "index": 26,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Tan, H., Bansal, M.",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 27,
      "title": "Learning to navigate unseen environments: Back translation with environmental dropout",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.04195",
      "authors": "Tan, H., Yu, L., Bansal, M."
    },
    {
      "index": 28,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å., Polosukhin, I.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 29,
      "title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L."
    },
    {
      "index": 30,
      "title": "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Wang, X., Xiong, W., Wang, H., Yang¬†Wang, W.",
      "orig_title": "Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation",
      "paper_id": "1803.07729v2"
    },
    {
      "index": 31,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.11059",
      "authors": "Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 32,
      "title": "Vision-language navigation with self-supervised auxiliary reasoning tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.07883",
      "authors": "Zhu, F., Zhu, Y., Chang, X., Liang, X."
    },
    {
      "index": 33,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S."
    }
  ]
}