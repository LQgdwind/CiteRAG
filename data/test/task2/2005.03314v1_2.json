{
  "paper_id": "2005.03314v1",
  "title": "Boosting Cloud Data Analytics using Multi-Objective Optimization",
  "sections": {
    "introduction": "As the volume of data generated by enterprises has continued to grow, big data analytics\nhas become commonplace for obtaining business insights from this voluminous data. Today, such big data analytics tasks often run on the enterprise‚Äôs private cloud or on machines\nleased by the enterprise in the public cloud. Despite its wide adoption, current\nbig data analytics systems remain best effort in nature and typically lack the ability\nto take user objectives such as performance goals or cost constraints into account. Determining an optimal hardware and software configuration for a big-data analytic task based on user-specified objectives is a complex task and one that is largely performed manually. Consider an enterprise user who wishes to run a mix of analytic tasks on their private or the public cloud. First, the user needs to choose the server hardware configuration from the set\nof available choices. Amazon‚Äôs EC2 public cloud platforms currently offer over 190 hardware configurations , while Microsoft Azure offers over 30 different hardware configurations .\nThese configurations differ in the number of cores, RAM size, and special hardware available.\nAfter determining the hardware configuration, the user also needs to determine the software configuration for the task by choosing various runtime parameters. For the popular Spark runtime engine, for example, these runtime parameters include\nparallelism (for reduce-style transformations),\nnumber of executors,\nnumber cores per executor,\nmemory per executor,\nRdd compression (boolean),\nMemory fraction (of heap space),\nto name a few.\n The choice of a configuration is\nfurther complicated by the need to optimize multiple, possibly conflicting, user objectives. Consider the following real-world use cases at several large data analytics companies and cloud providers (anonymized for confidentiality) that elaborate on these challenges and motivate our work: Use Case 1 (Data-driven Business Users). A data-driven security company runs thousands of analytic tasks in the cloud every day. The engineers managing these tasks have two objectives: keep the latency low in order to quickly detect fraudulent behaviors and also reduce cloud costs that impose substantial operational expenses on the company. For cloud analytics, task latency can always be reduced further by allocating more hardware resources, but this comes at the expense of higher cloud costs.\nHence, the engineers face the challenge of deciding the cluster configuration and other runtime parameters that balance latency and cost. Use Case 2 (Severless Databases). Cloud providers now offer hosted databases in the form of serverless offerings (e.g., ) where the database is turned off during idle periods, dynamically turned on when new queries arrive, and scaled up or down as the load changes over time. A media company that uses this serverless database to run a news site sees peak loads in the morning or as news stories break,\nand a lighter load at other times.\nThe news application specifies the minimum and maximum number of computing units (CUs) to service its workload across peak and off-peak periods; it prefers to minimize cost when the load is light and expects the cloud provider to dynamically scale CUs for the morning peak or breaking news. In this case, the cloud provider needs to balance between latency under different data rates and user cost, which directly depends on the number of CUs used. To do so, the cloud provider needs automated methods to choose appropriate configurations under different workloads that address both objectives. Overall, choosing a configuration that balances multiple conflicting objectives is non-trivial. Studies have show that even expert engineers are often unable to choose between two cluster options for a single objective like latency¬†5, let alone choosing between dozens of cluster options for multiple competing objectives. In this paper, we introduce a multi-objective optimizer that can automate the task of determining on optimal configuration for each task based on multiple task objectives. Such an optimizer takes as input an analytic task in the form of a dataflow program (which subsumes SQL queries) and a set of objectives, and produces as output a job configuration with a suitable number of cores as well as other runtime system parameters that best meet the task objectives. At the core of our work is a principled multi-objective optimization (MOO) approach that takes multiple, possibly conflicting, objectives and computes a Pareto-optimal set of job configurations.\nA final configuration is then chosen from this Pareto-optimal set. We note several differences of our work from SQL optimization:\nFirst, our work is complementary to SQL optimization for database workloads. For a given query, SQL optimization chooses a query plan, viewed as a dataflow program, that is then mapped to cluster resources by choosing an appropriate number of cores and memory per core, and configured with many parameters of a distributed engine such as Spark. Our optimizer addresses this later step of optimization, yielding a cluster execution plan. Second, MOO for SQL queries¬†6 1   examines a finite set of query plans based on relational algebra,\nand selects the Pareto optimal ones based on estimated cost of each plan.\nIn contrast, our optimizer searches through a parameter space that mixes numerical and categorial parameters, with potentially an infinite set of possible configurations, and\nfinds those that are Pareto optimal.\nTo suit the property of the parameter space, our optimizer employs a numerical optimization approach to MOO. Third, our MOO-based optimizer aims to support a broad set of analytic tasks that are commonly mixed in data analytics pipelines, including SQL queries, ETL tasks based on SQL with UDFs, and machine learning tasks for deep analysis, all in the general paradigm of dataflow programs. To do so, our optimizer leverages recent machine learning based modeling approaches¬†  that can automatically learn a predictive model for each objective using the runtime behavior of a user task (i.e., runtime metrics), without necessarily requiring the use of query plans.\nIn particular, we view our MOO work as a synergy with recent work on workload modeling. Such a synergy is reminiscent of the past work on SQL optimization: our MOO framework is analogous to the Dynamic Programming based optimization framework, although it has been extended to the multi-objective settings due to the needs of today‚Äôs cloud analytics, while recent modeling work is analogous to cost modeling of SQL query plans but extended to automated learning of such cost models from runtime observations. As we show, working with learned models brings new challenges for optimization.\n More specifically, our design of a multi-objective optimizer addresses the following technical challenges: 1. Infinite Parameter Space: There are potentially infinite configurations in our parameter space, but only a small fraction of them belong to the Pareto set‚Äîmost configurations are dominated by some Pareto optimal configuration for all objectives.\nHence, we must address the challenge of efficiently searching through an\ninfinite parameter space to find these Pareto optimal configurations. 2. Coverage of the Pareto Frontier: The Pareto set over the multi-objective space is also called the Pareto frontier. Since we aim to use the Pareto frontier to recommend a new configuration that best explores tradeoffs between different objectives, the frontier should provide good coverage of the overall objective space and have a fine resolution for the regions when the tradeoffs are significant.\nAs we show later, classical MOO algorithms¬†0 often fail to provide sufficient coverage of the Pareto frontier. 3. Efficiency:\nSince our optimizer uses learned models of user objectives, it has to handle the high complexity of such models (e.g., using Deep Neural Networks) and frequent updates of these models as new training data becomes available. Before running a user task, the learned models may have changed and the optimizer may have to recompute the Pareto frontier in order to make recommendations. Therefore, the speed of computing the Pareto frontier, e.g., within a few seconds, is crucial for adapting to bursty data loads quickly (in the serverless database case) or reducing the delay of starting a recurrent workload at a scheduled time.\nMost existing MOO algorithms, including Weighted Sum¬†0, Normal Constraints¬†0, and Evolutional Methods¬†3, are not designed to meet such stringent performance requirements. By addressing the above challenges, our paper makes the following contributions: (1) We address the infinite search space issue by presenting a new approach for incrementally transforming a MOO problem to a set of constrained optimization (CO) problems, where each CO problem can be solved individually to return a Pareto optimal point. (2) We then address the coverage and efficiency challenges by designing Progressive Frontier (PF) algorithms to realize our approach. (iùëñi)¬†Our first PF algorithm is designed to be incremental, i.e., gradually expanding the Pareto frontier as more computing time is invested, and uncertainty-aware, i.e., returning more points in regions of the frontier that lack sufficient information. (i‚Äãiùëñùëñii)¬†We also develop an approximate PF algorithm that given complex learned models, solves each CO problem efficiently based on these models. (i‚Äãi‚Äãiùëñùëñùëñiii)¬†We finally devise a parallel, approximate PF algorithm to further improve efficiency. (3) We implement our algorithms into a Spark-based prototype. Evaluation results using benchmarks for batch and streaming workloads show that our approach produces a Pareto frontier in less than 2.5 seconds (2-50X faster that other MOO methods¬†0 3), provides greater coverage over the frontier, and enables exploration of tradeoffs such as cost-latency or latency-throughput. When compared to Ottertune¬†, a state-of-the-art performance tuning system, our approach recommends configurations that yield 26%-49% reduction of total running time of the TPCx-BB benchmark¬† while adapting to different application preferences on multiple objectives and being able to accommodate a broader set of models.\nAs database research continues to deliver new results on learned models¬†1 8 6  , the generality of our optimizer allows it to achieve even better results once the new learned models are made available."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Amazon ec2 instance types",
      "abstract": "",
      "year": "2019",
      "venue": "aws.amazon.com/ec2/instance-types",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Amazon aurora serverless",
      "abstract": "",
      "year": "",
      "venue": "aws.amazon.com/rds/aurora/serverless",
      "authors": ""
    },
    {
      "index": 2,
      "title": "Virtual machine sizes in azure",
      "abstract": "",
      "year": "2019",
      "venue": "docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes",
      "authors": ""
    },
    {
      "index": 3,
      "title": "Bomin: Basic open-source nonlinear mixed integer programming",
      "abstract": "",
      "year": "",
      "venue": "coin-or.org/Bonmin",
      "authors": ""
    },
    {
      "index": 4,
      "title": "Mixed-integer nonlinear programming",
      "abstract": "",
      "year": "2003",
      "venue": "SIAG/OPT Newsletter: Views & News",
      "authors": "M. R. Bussieck and A. Pruessner"
    },
    {
      "index": 5,
      "title": "Couenne: Convex over and under envelopes for nonlinear estimation",
      "abstract": "",
      "year": "",
      "venue": "projects.coin-or.org/Couenne",
      "authors": ""
    },
    {
      "index": 6,
      "title": "Cplex optimizer",
      "abstract": "",
      "year": "",
      "venue": "ibm.com/analytics/cplex-optimizer",
      "authors": ""
    },
    {
      "index": 7,
      "title": "A closer look at drawbacks of minimizing weighted sums of objectives for pareto set generation in multicriteria optimization problems",
      "abstract": "",
      "year": "1997",
      "venue": "Structural Optimization",
      "authors": "I. Das and J. E. Dennis"
    },
    {
      "index": 8,
      "title": "A fast and elitist multiobjective genetic algorithm: Nsga-ii",
      "abstract": "",
      "year": "2002",
      "venue": "Trans. Evol. Comp",
      "authors": "K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan"
    },
    {
      "index": 9,
      "title": "Parallel database systems: the future of high performance database systems",
      "abstract": "",
      "year": "1992",
      "venue": "Commun. ACM",
      "authors": "D. DeWitt and J. Gray"
    },
    {
      "index": 10,
      "title": "Tuning database configuration parameters with ituned",
      "abstract": "",
      "year": "2009",
      "venue": "PVLDB",
      "authors": "S. Duan, V. Thummala, and S. Babu"
    },
    {
      "index": 11,
      "title": "Optimistic recovery for iterative dataflows in action",
      "abstract": "",
      "year": "2015",
      "venue": "ACM SIGMOD International Conference on Management of Data",
      "authors": "S. Dudoladov, C. Xu, S. Schelter, A. Katsifodimos, S. Ewen, K. Tzoumas, and V. Markl"
    },
    {
      "index": 12,
      "title": "A tutorial on multiobjective optimization: Fundamentals and evolutionary methods",
      "abstract": "",
      "year": "2018",
      "venue": "Natural Computing: an international journal",
      "authors": "M. T. Emmerich and A. H. Deutz"
    },
    {
      "index": 13,
      "title": "Dhalion: Self-regulating stream processing in heron",
      "abstract": "",
      "year": "2017",
      "venue": "Proc. VLDB Endow.",
      "authors": "A. Floratou, A. Agrawal, B. Graham, S. Rao, and K. Ramasamy"
    },
    {
      "index": 14,
      "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Gal and Z. Ghahramani",
      "orig_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "paper_id": "1506.02142v6"
    },
    {
      "index": 15,
      "title": "Query optimization for parallel execution",
      "abstract": "",
      "year": "1992",
      "venue": "ACM SIGMOD International Conference on Management of Data",
      "authors": "S. Ganguly, W. Hasan, and R. Krishnamurthy"
    },
    {
      "index": 16,
      "title": "Undecidability and hardness in mixed-integer nonlinear programming",
      "abstract": "",
      "year": "2018",
      "venue": "CNRS, Ecole Polytechnique",
      "authors": "L. liberti"
    },
    {
      "index": 17,
      "title": "Jmetal: an object-oriented java-based framework for multi-objective optimization with metaheuristics",
      "abstract": "",
      "year": "",
      "venue": "jmetal.sourceforge.net",
      "authors": ""
    },
    {
      "index": 18,
      "title": "Morpheus: Towards automated slos for enterprise clusters",
      "abstract": "",
      "year": "2016",
      "venue": "USENIX Symposium on Operating Systems Design and Implementation",
      "authors": "S. A. Jyothi, C. Curino, I. Menache, S. M. Narayanamurthy, A. Tumanov, J. Yaniv, R. Mavlyutov, I. Goiri, S. Krishnan, J. Kulkarni, and S. Rao"
    },
    {
      "index": 19,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 20,
      "title": "Schedule optimization for data processing flows on the cloud",
      "abstract": "",
      "year": "2011",
      "venue": "ACM SIGMOD International Conference on Management of Data",
      "authors": "H. Kllapi, E. Sitaridi, M. M. Tsangaris, and Y. Ioannidis"
    },
    {
      "index": 21,
      "title": "Artelys knitro user‚Äôs manual",
      "abstract": "",
      "year": "",
      "venue": "artelys.com/docs/knitro/index.html",
      "authors": ""
    },
    {
      "index": 22,
      "title": "Supporting scalable analytics with latency constraints",
      "abstract": "",
      "year": "2015",
      "venue": "PVLDB",
      "authors": "B. Li, Y. Diao, and P. J. Shenoy"
    },
    {
      "index": 23,
      "title": "Resource bricolage for parallel database systems",
      "abstract": "",
      "year": "2014",
      "venue": "PVLDB",
      "authors": "J. Li, J. F. Naughton, and R. V. Nehme"
    },
    {
      "index": 24,
      "title": "Model-Free Control for Distributed Stream Data Processing using Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. VLDB Endow.",
      "authors": "T. Li, Z. Xu, J. Tang, and Y. Wang",
      "orig_title": "Model-free control for distributed stream data processing using deep reinforcement learning",
      "paper_id": "1803.01016v1"
    },
    {
      "index": 25,
      "title": "Neo: A Learned Query Optimizer",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. VLDB Endow.",
      "authors": "R. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Papaemmanouil, and N. Tatbul",
      "orig_title": "Neo: A learned query optimizer",
      "paper_id": "1904.03711v1"
    },
    {
      "index": 26,
      "title": "WiSeDB: A Learning-based Workload Management Advisor for Cloud Databases",
      "abstract": "",
      "year": "2016",
      "venue": "PVLDB",
      "authors": "R. Marcus and O. Papaemmanouil",
      "orig_title": "Wisedb: A learning-based workload management advisor for cloud databases",
      "paper_id": "1601.08221v3"
    },
    {
      "index": 27,
      "title": "Plan-Structured Deep Neural Network Models for Query Performance Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. VLDB Endow.",
      "authors": "R. Marcus and O. Papaemmanouil",
      "orig_title": "Plan-structured deep neural network models for query performance prediction",
      "paper_id": "1902.00132v1"
    },
    {
      "index": 28,
      "title": "A learning-based service for cost and performance management of cloud databases",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Data Engineering",
      "authors": "R. Marcus, S. Semenova, and O. Papaemmanouil"
    },
    {
      "index": 29,
      "title": "Survey of multi-objective optimization methods for engineering",
      "abstract": "",
      "year": "2004",
      "venue": "Structural and Multidisciplinary Optimization",
      "authors": "R. Marler and J. S. Arora"
    },
    {
      "index": 30,
      "title": "From dubious construction of objective functions to the application of physical programming",
      "abstract": "",
      "year": "2012",
      "venue": "AIAA Journal",
      "authors": "A. Messac"
    },
    {
      "index": 31,
      "title": "The normalized normal constraint method for generating the pareto frontier",
      "abstract": "",
      "year": "2003",
      "venue": "Structural and Multidisciplinary Optimization",
      "authors": "A. Messac, A. Ismailyahaya, and C. A. Mattson"
    },
    {
      "index": 32,
      "title": "Neos guide: Nonlinear programming software",
      "abstract": "",
      "year": "",
      "venue": "neos-guide.org/content/nonlinear-programming",
      "authors": ""
    },
    {
      "index": 33,
      "title": "Neos solvers: Nonlinear programming software",
      "abstract": "",
      "year": "",
      "venue": "neos-server.org/neos/solvers/index.html",
      "authors": ""
    },
    {
      "index": 34,
      "title": "Perforator: eloquent performance models for resource optimization",
      "abstract": "",
      "year": "2016",
      "venue": "ACM Symposium on Cloud Computing",
      "authors": "K. Rajan, D. Kakadia, C. Curino, and S. Krishnan"
    },
    {
      "index": 35,
      "title": "A tutorial on gaussian process regression: Modelling, exploring, and exploiting functions",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Mathematical Psychology",
      "authors": "E. Schulz, M. Speekenbrink, and A. Krause"
    },
    {
      "index": 36,
      "title": "Mrtuner: A toolkit to enable holistic optimization for mapreduce jobs",
      "abstract": "",
      "year": "2014",
      "venue": "PVLDB",
      "authors": "J. Shi, J. Zou, J. Lu, Z. Cao, S. Li, and C. Wang"
    },
    {
      "index": 37,
      "title": "Boosting Cloud Data Analytics using Multi-Objective Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "F. Song, K. Zaouk, C. Lyu, A. Sinha, Q. Fan, Y. Diao, and P. Shenoy",
      "orig_title": "Boosting cloud data analytics using multi-objective optimization",
      "paper_id": "2005.03314v1"
    },
    {
      "index": 38,
      "title": "Tempo: robust and self-tuning resource management in multi-tenant parallel databases",
      "abstract": "",
      "year": "2016",
      "venue": "VLDB Endowment",
      "authors": "Z. Tan and S. Babu"
    },
    {
      "index": 39,
      "title": "TPCx-BB",
      "abstract": "",
      "year": "",
      "venue": "tpc.org/tpcx-bb",
      "authors": ""
    },
    {
      "index": 40,
      "title": "Approximation schemes for many-objective query optimization",
      "abstract": "",
      "year": "2014",
      "venue": "ACM SIGMOD International Conference on Management of Data",
      "authors": "I. Trummer and C. Koch"
    },
    {
      "index": 41,
      "title": "An incremental anytime algorithm for multi-objective query optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ACM SIGMOD International Conference on Management of Data",
      "authors": "I. Trummer and C. Koch"
    },
    {
      "index": 42,
      "title": "Automatic database management system tuning through large-scale machine learning",
      "abstract": "",
      "year": "2017",
      "venue": "ACM International Conference on Management of Data",
      "authors": "D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang"
    },
    {
      "index": 43,
      "title": "Ernest: Efficient performance prediction for large-scale advanced analytics",
      "abstract": "",
      "year": "2016",
      "venue": "USENIX Conference on Networked Systems Design and Implementation",
      "authors": "S. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica"
    },
    {
      "index": 44,
      "title": "Towards a learning optimizer for shared clouds",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. VLDB Endow.",
      "authors": "C. Wu, A. Jindal, S. Amizadeh, H. Patel, W. Le, S. Qiao, and S. Rao"
    },
    {
      "index": 45,
      "title": "Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing",
      "abstract": "",
      "year": "2012",
      "venue": "USENIX Conference on Networked Systems Design and Implementation",
      "authors": "M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica"
    },
    {
      "index": 46,
      "title": "Discretized streams: fault-tolerant streaming computation at scale",
      "abstract": "",
      "year": "2013",
      "venue": "ACM Symposium on Operating Systems Principles",
      "authors": "M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica"
    },
    {
      "index": 47,
      "title": "UDAO: A next-generation unified data analytics optimizer (vldb 2019 demo)",
      "abstract": "",
      "year": "2019",
      "venue": "PVLDB",
      "authors": "K. Zaouk, F. Song, C. Lyu, A. Sinha, Y. Diao, and P. J. Shenoy"
    },
    {
      "index": 48,
      "title": "A demonstration of the ottertune automatic database management system tuning service",
      "abstract": "",
      "year": "2018",
      "venue": "PVLDB",
      "authors": "B. Zhang, D. V. Aken, J. Wang, T. Dai, S. Jiang, J. Lao, S. Sheng, A. Pavlo, and G. J. Gordon"
    },
    {
      "index": 49,
      "title": "An end-to-end automatic cloud database tuning system using deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Management of Data",
      "authors": "J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang, T. Cheng, L. Liu, M. Ran, and Z. Li"
    }
  ]
}