{
  "paper_id": "2007.08557v1",
  "title": "Unsupervised Text Generation by Learning from Search",
  "sections": {
    "introduction": "Text generation refers to a wide range of tasks involving generating natural language, including but not limited to machine translation [ref]20  , sentence simplification  , and text summarization  . Recent success of neural-based text generation relies heavily on a large parallel dataset for training, which\nmay not be available in real-world natural language processing (NLP) applications.\nIn this work, we consider unsupervised text generation, where no parallel data is available. This setting is more challenging, and has significant potential in both scientific research (e.g., low-resource language processing) and industrial applications (e.g., cold start for a new NLP application). Early work tackles unsupervised text generation by rules or templates 5 . While such approaches do not require parallel corpora, the generated sentences are highly subject to the rules, and hence lack the flexibility of natural language. Other work constructs pseudo-parallel data, which is only feasible for certain tasks like unsupervised machine translation [ref]20. Recently, researchers have developed search-based techniques for unsupervised text generation [ref]29 8  , where a heuristically defined scoring function evaluates the quality of a sentence, involving language fluency, semantic compliance, and other task-specific aspects. Then, the algorithm performs word-level edits (such as word deletion, insertion, and replacement) to search towards a (possibly local) optimum of the scoring function. With a reasonably designed scoring function, such approaches are shown to be effective in a variety of applications like paraphrase generation [ref]29 , sentence summarization , and text simplification 8. However, the search-based approach has two major drawbacks: 1) The inference efficiency is low. To obtain an output sentence, the search algorithm would perform a few hundred steps of local edits and re-evaluations. This could be considerably slower than an autoregressive decoder, which generates words sequentially. 2) The search could yield noisy results, since the scoring function is defined heuristically and the search is conducted locally in a discrete sentence space. To this end, we propose a new framework for unsupervised text generation by learning from search (tgls), which contains a strong search module that explores the sentence space, as well as a learning module that learns from the search results.\nFor the search module, we adopt the simulated annealing (SA) algorithm. At each step, SA proposes a local edit by a neural network, and then either accepts or rejects the proposal based on a heuristically defined scoring function.\nFor learning, we employ two methods to train the conditional generative model, word-level cross-entropy loss and the sequence-level max-margin loss. Within tgls, the search and learning can be boosted by each other in an iterative fashion. That is, the search results serve as the pseudo-reference for training the conditional generator, which in turn benefits SA search by serving as a more meaningful initial state.\nAs for implementation, tgls involves two pretrained language models: a) the uni-directional GPT2 , which is suitable for likelihood-based fluency evaluation and conditional generation; and b) the bi-directional RoBERTa , which is better at semantic evaluation and contextual word-level prediction. The main contributions of our paper include: 1) We propose tgls, a generic learning-from-search framework for unsupervised text generation. 2) We demonstrate efficient methods of incorporating the large-scale pretrained language models into our tgls framework. 3) We conducted experiments on two different tasks: paraphrasing and text formalization. In both experiments, tgls significantly outperforms unsupervised baseline methods. Moreover, tgls achieves comparable performance to recent supervised models  in the paraphrasing task. 4) For text formalization (an example of text style transfer), we are also the first to design a search-based\nmethod, and further extend it into the proposed tgls framework."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Unsupervised opinion summarization with noising and denoising",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "R. K. Amplayo and M. Lapata"
    },
    {
      "index": 1,
      "title": "Generating Sentences from Disentangled Syntactic and Semantic Spaces",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Y. Bao, H. Zhou, S. Huang, L. Li, L. Mou, O. Vechtomova, X. Dai, and J. Chen",
      "orig_title": "Generating sentences from disentangled syntactic and semantic spaces",
      "paper_id": "1907.05789v1"
    },
    {
      "index": 2,
      "title": "Generating sentences from a continuous space",
      "abstract": "",
      "year": "2015",
      "venue": "CoNLL",
      "authors": "S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio"
    },
    {
      "index": 3,
      "title": "MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "E. Chu and P. J. Liu",
      "orig_title": "MeanSum: A neural model for unsupervised multi-document abstractive summarization",
      "paper_id": "1810.05739v4"
    },
    {
      "index": 4,
      "title": "Learning as search optimization: Approximate large margin methods for structured prediction",
      "abstract": "",
      "year": "2005",
      "venue": "ICML",
      "authors": "H. Daumé III and D. Marcu"
    },
    {
      "index": 5,
      "title": "Learning as search optimization: Approximate large margin methods for structured prediction",
      "abstract": "",
      "year": "2005",
      "venue": "ICML",
      "authors": "H. Daumé III and D. Marcu"
    },
    {
      "index": 6,
      "title": "An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "W. Du and Y. Ji",
      "orig_title": "An empirical comparison on imitation learning and reinforcement learning for paraphrase generation",
      "paper_id": "1908.10835v1"
    },
    {
      "index": 7,
      "title": "Paraphrase generation with latent bag of words",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Y. Fu, Y. Feng, and J. P. Cunningham"
    },
    {
      "index": 8,
      "title": "Style transfer in text: Exploration and evaluation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Z. Fu, X. Tan, N. Peng, D. Zhao, and R. Yan"
    },
    {
      "index": 9,
      "title": "An empirical investigation of global and local normalization for recurrent neural sequence models using a continuous relaxation to beam search",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "K. Goyal, C. Dyer, and T. Berg-Kirkpatrick"
    },
    {
      "index": 10,
      "title": "Zero-Shot Paraphrase Generation with Multilingual Language Models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.03597",
      "authors": "Y. Guo, Y. Liao, X. Jiang, Q. Zhang, Y. Zhang, and Q. Liu",
      "orig_title": "Zero-shot paraphrase generation with multilingual language models",
      "paper_id": "1911.03597v1"
    },
    {
      "index": 11,
      "title": "A Deep Generative Framework for Paraphrase Generation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "A. Gupta, A. Agarwal, P. Singh, and P. Rai",
      "orig_title": "A deep generative framework for paraphrase generation",
      "paper_id": "1709.05074v1"
    },
    {
      "index": 12,
      "title": "Training products of experts by minimizing contrastive divergence",
      "abstract": "",
      "year": "2002",
      "venue": "Neural Computation",
      "authors": "G. E. Hinton"
    },
    {
      "index": 13,
      "title": "Toward Controlled Generation of Text",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing",
      "orig_title": "Toward controlled generation of text",
      "paper_id": "1703.00955v4"
    },
    {
      "index": 14,
      "title": "Auto-encoding variational bayes",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6114",
      "authors": "D. P. Kingma and M. Welling"
    },
    {
      "index": 15,
      "title": "Optimization by simulated annealing",
      "abstract": "",
      "year": "1983",
      "venue": "Science",
      "authors": "S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi"
    },
    {
      "index": 16,
      "title": "Learning to Search Better than Your Teacher",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1502.02206",
      "authors": "A. Krishnamurthy, H. D. CMU EDU III, and U. EDU",
      "orig_title": "Learning to search better than your teacher",
      "paper_id": "1502.02206v2"
    },
    {
      "index": 17,
      "title": "Iterative Edit-Based Unsupervised Sentence Simplification",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "D. Kumar, L. Mou, L. Golab, and O. Vechtomova",
      "orig_title": "Iterative edit-based unsupervised sentence simplification",
      "paper_id": "2006.09639v1"
    },
    {
      "index": 18,
      "title": "Cross-lingual Language Model Pretraining",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "G. Lample and A. Conneau",
      "orig_title": "Cross-lingual language model pretraining",
      "paper_id": "1901.07291v1"
    },
    {
      "index": 19,
      "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "G. Lample, A. Conneau, L. Denoyer, and M. Ranzato",
      "orig_title": "Unsupervised machine translation using monolingual corpora only",
      "paper_id": "1711.00043v2"
    },
    {
      "index": 20,
      "title": "Phrase-Based & Neural Unsupervised Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "G. Lample, M. Ott, A. Conneau, L. Denoyer, and M. Ranzato",
      "orig_title": "Phrase-based & neural unsupervised machine translation",
      "paper_id": "1804.07755v2"
    },
    {
      "index": 21,
      "title": "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "J. Li, R. Jia, H. He, and P. Liang",
      "orig_title": "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
      "paper_id": "1804.06437v1"
    },
    {
      "index": 22,
      "title": "Paraphrase generation with deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.00279",
      "authors": "Z. Li, X. Jiang, L. Shang, and H. Li"
    },
    {
      "index": 23,
      "title": "Unsupervised paraphrasing by simulated annealing",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "X. Liu, L. Mou, F. Meng, H. Zhou, J. Zhou, and S. Song"
    },
    {
      "index": 24,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 25,
      "title": "A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer",
      "abstract": "",
      "year": "2019",
      "venue": "IJCAI",
      "authors": "F. Luo, P. Li, J. Zhou, P. Yang, B. Chang, Z. Sui, and X. Sun",
      "orig_title": "A dual reinforcement learning framework for unsupervised text style transfer",
      "paper_id": "1905.10060v1"
    },
    {
      "index": 26,
      "title": "Paraphrasing revisited with neural machine translation",
      "abstract": "",
      "year": "2017",
      "venue": "ACL",
      "authors": "J. Mallinson, R. Sennrich, and M. Lapata"
    },
    {
      "index": 27,
      "title": "An augmented template-based approach to text realization",
      "abstract": "",
      "year": "2003",
      "venue": "Natural Language Engineering",
      "authors": "S. W. McRoy, S. Channarukul, and S. S. Ali"
    },
    {
      "index": 28,
      "title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "N. Miao, H. Zhou, L. Mou, R. Yan, and L. Li",
      "orig_title": "CGMH: Constrained sentence generation by metropolis-hastings sampling",
      "paper_id": "1811.10996v1"
    },
    {
      "index": 29,
      "title": "Unsupervised Sentence Simplification Using Deep Semantics",
      "abstract": "",
      "year": "2015",
      "venue": "INLG",
      "authors": "S. Narayan and C. Gardent",
      "orig_title": "Unsupervised sentence simplification using deep semantics",
      "paper_id": "1507.08452v3"
    },
    {
      "index": 30,
      "title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",
      "abstract": "",
      "year": "2015",
      "venue": "ACL",
      "authors": "E. Pavlick, P. Rastogi, J. Ganitkevitch, B. Van Durme, and C. Callison-Burch"
    },
    {
      "index": 31,
      "title": "Style Transfer Through Back-Translation",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "S. Prabhumoye, Y. Tsvetkov, R. Salakhutdinov, and A. W. Black",
      "orig_title": "Style transfer through back-translation",
      "paper_id": "1804.09000v3"
    },
    {
      "index": 32,
      "title": "Exploring diverse expressions for paraphrase generation",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "L. Qian, L. Qiu, W. Zhang, X. Jiang, and Y. Yu"
    },
    {
      "index": 33,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI Blog",
      "authors": "A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever"
    },
    {
      "index": 34,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "M. Ranzato, S. Chopra, M. Auli, and W. Zaremba",
      "orig_title": "Sequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 35,
      "title": "Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "S. Rao and J. R. Tetreault",
      "orig_title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
      "paper_id": "1803.06535v2"
    },
    {
      "index": 36,
      "title": "Automatic keyword extraction from individual documents",
      "abstract": "",
      "year": "2010",
      "venue": "Text Mining: Applications and Theory",
      "authors": "S. Rose, D. Engel, N. Cramer, and W. Cowley"
    },
    {
      "index": 37,
      "title": "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "R. Schumann, L. Mou, Y. Lu, O. Vechtomova, and K. Markert",
      "orig_title": "Discrete optimization for unsupervised sentence summarization with word-level extraction",
      "paper_id": "2005.01791v1"
    },
    {
      "index": 38,
      "title": "Style Transfer from Non-Parallel Text by Cross-Alignment",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "T. Shen, T. Lei, R. Barzilay, and T. Jaakkola",
      "orig_title": "Style transfer from non-parallel text by cross-alignment",
      "paper_id": "1705.09655v2"
    },
    {
      "index": 39,
      "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
      "abstract": "",
      "year": "2018",
      "venue": "Science",
      "authors": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al."
    },
    {
      "index": 40,
      "title": "Joint learning of a dual SMT system for paraphrase generation",
      "abstract": "",
      "year": "2012",
      "venue": "ACL",
      "authors": "H. Sun and M. Zhou"
    },
    {
      "index": 41,
      "title": "Unsupervised Neural Text Simplification",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "S. Surya, A. Mishra, A. Laha, P. Jain, and K. Sankaranarayanan",
      "orig_title": "Unsupervised neural text simplification",
      "paper_id": "1810.07931v6"
    },
    {
      "index": 42,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 43,
      "title": "Topic-Guided Variational Autoencoders for Text Generation",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "W. Wang, Z. Gan, H. Xu, R. Zhang, G. Wang, D. Shen, C. Chen, and L. Carin",
      "orig_title": "Topic-guided variational auto-encoder for text generation",
      "paper_id": "1903.07137v1"
    },
    {
      "index": 44,
      "title": "Eliza—a computer program for the study of natural language communication between man and machine",
      "abstract": "",
      "year": "1966",
      "venue": "Communications of the ACM",
      "authors": "J. Weizenbaum"
    },
    {
      "index": 45,
      "title": "Sequence-to-sequence learning as beam-search optimization",
      "abstract": "",
      "year": "2016",
      "venue": "EMNLP",
      "authors": "S. Wiseman and A. M. Rush"
    },
    {
      "index": 46,
      "title": "On Variational Learning of Controllable Representations for Text without Supervision",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "P. Xu, J. C. K. Cheung, and Y. Cao",
      "orig_title": "On variational learning of controllable representations for text without supervision",
      "paper_id": "1905.11975v4"
    },
    {
      "index": 47,
      "title": "Formality Style Transfer with Hybrid Textual Annotations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv 1903.06353",
      "authors": "R. Xu, T. Ge, and F. Wei",
      "orig_title": "Formality style transfer with hybrid textual annotations",
      "paper_id": "1903.06353v1"
    },
    {
      "index": 48,
      "title": "Paraphrasing for style",
      "abstract": "",
      "year": "2012",
      "venue": "COLING",
      "authors": "W. Xu, A. Ritter, B. Dolan, R. Grishman, and C. Cherry"
    },
    {
      "index": 49,
      "title": "An end-to-end generative architecture for paraphrase generation",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "Q. Yang, D. Shen, Y. Cheng, W. Wang, G. Wang, L. Carin, et al."
    },
    {
      "index": 50,
      "title": "SeqGAN: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "L. Yu, W. Zhang, J. Wang, and Y. Yu"
    },
    {
      "index": 51,
      "title": "Bridging the Gap between Training and Inference for Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "W. Zhang, Y. Feng, F. Meng, D. You, and Q. Liu",
      "orig_title": "Bridging the gap between training and inference for neural machine translation",
      "paper_id": "1906.02448v2"
    },
    {
      "index": 52,
      "title": "Syntax-Infused Variational Autoencoder for Text Generation",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "X. Zhang, Y. Yang, S. Yuan, D. Shen, and L. Carin",
      "orig_title": "Syntax-infused variational autoencoder for text generation",
      "paper_id": "1906.02181v1"
    },
    {
      "index": 53,
      "title": "Style Transfer as Unsupervised Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.07894",
      "authors": "Z. Zhang, S. Ren, S. Liu, J. Wang, P. Chen, M. Li, M. Zhou, and E. Chen",
      "orig_title": "Style transfer as unsupervised machine translation",
      "paper_id": "1808.07894v1"
    },
    {
      "index": 54,
      "title": "Leveraging multiple mt engines for paraphrase generation",
      "abstract": "",
      "year": "2010",
      "venue": "COLING",
      "authors": "S. Zhao, H. Wang, X. Lan, and T. Liu"
    },
    {
      "index": 55,
      "title": "Sentence Centrality Revisited for Unsupervised Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the Association for Computational Linguistics",
      "authors": "H. Zheng and M. Lapata",
      "orig_title": "Sentence centrality revisited for unsupervised summarization",
      "paper_id": "1906.03508v1"
    }
  ]
}