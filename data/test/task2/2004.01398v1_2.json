{
  "paper_id": "2004.01398v1",
  "title": "TEA: Temporal Excitation and Aggregation for Action Recognition",
  "sections": {
    "comparisons with the state-of-the-arts": "In this section, we first compare TEA with the existing state-of-the-art action recognition methods on Something-Something V1 and Kinetics400. The comprehensive statistics, including the classification results, inference protocols, and the corresponding FLOPs, are shown in Table 2 and 3. In both tables, the first compartment contains the methods based on 3D CNNs or the mixup of 2D and 3D CNNs, and the methods in the second compartment are all based on 2D or (2+1)D CNNs. Due to the high computation costs of 3D CNNs, the FLOPs of methods in the first compartment are typically higher than others. Among all existing methods, the most efficient ones are TSN8f 4 and TSM8f [ref]27 with only 33G FLOPs. Compared with these methods, the FLOPs of our proposed TEA network slightly increases to 35G (1.06×\\times), but the performance is increased by a big margin, a relative improvement of 5.4 % (48.8% vs. 43.4%). 1. “ImgNet” denotes ImageNet dataset   and “None” indicates training models from scratch. 2. “N/A” represents that the authors do not report the inference protocol in their paper. The superiority of our TEA on Something-Something is quite impressive. It confirms the remarkable ability of TEA for temporal modeling. Using efficient inference protocol (center crop×\\times1 clip) and 8 input frames, the proposed TEA obtains 48.8%, which significantly outperforms TSN and TSM with similar FLOPs (19.7%/43.4%). This results even exceeds the ensemble result of TSM, which combines the two models using 8 and 16 frames, respectively (TSMEn, 46.8%). When utilizing 16 frames as input and applying a more laborious accuracy evaluation protocol (full resolution×\\times10 clips), the FLOPs of our method increase to ∼similar-to\\sim2000G, which is similar to NL I3D+GCN 6. But the proposed method significantly surpasses NL I3D+GCN and all other existing methods (52.3% vs. 46.1%) on the validation set. Our performance on the test set (46.6%) also outperforms most of the existing methods. Moreover, we do not require additional COCO images  to pre-train an object detector as in 6. When compared with the methods utilizing both RGB and optical flow modalities, i.e., ECOEn-(RGB+Flow)  (49.5%) and TSM-(RGB+Flow) [ref]27 (50.2%), the obtained result (52.3%) also shows substantial improvements. On Kinetics400, the performance of our method (76.1%) is inferior to that of SlowFast  (79.8%). However, the SlowFast networks adopt the deeper networks (ResNet101) based on 3D CNNs and utilize time-consuming non-local 5 operations. When comparing methods with similar efficiency, such as TSM [ref]27 and STM , TEA obtains better performance. When adopting 8 frames as input, TEA gains ∼similar-to\\sim1% higher accuracy than TSM (75.0% vs. 74.1%). While utilizing 16 input frames, our TEA method outperforms both TSM16f and STM 16f with a large margin (76.1% vs. 74.7%/73.7%). Finally, we report comparison results on HMDB51 and UCF101 in Table 4. Our method achieves 73.3% on HMDB51 and 96.9% on UCF101 with the accuracy inference protocol. The performance of our model (TEA16f) outperforms most of the existing methods except for I3D . I3D is based on 3D CNNs and additional input modality; thus, its computational FLOPs will be far more than ours. 1. MCA denotes mean class accuracy. 2. TSM does not report MCA results, and the listed results are cited from STM ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Action recognition with dynamic image networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Hakan Bilen, Basura Fernando, Efstratios Gavves, and Andrea Vedaldi"
    },
    {
      "index": 1,
      "title": "Dynamic image networks for action recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Hakan Bilen, Basura Fernando, Efstratios Gavves, Andrea Vedaldi, and Stephen Gould"
    },
    {
      "index": 2,
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Joao Carreira and Andrew Zisserman",
      "orig_title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "paper_id": "1705.07750v3"
    },
    {
      "index": 3,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 4,
      "title": "Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.08200",
      "authors": "Ali Diba, Mohsen Fayyaz, Vivek Sharma, Amir Hossein Karami, Mohammad Mahdi Arzani, Rahman Yousefzadeh, and Luc Van Gool",
      "orig_title": "Temporal 3d convnets: New architecture and transfer learning for video classification",
      "paper_id": "1711.08200v1"
    },
    {
      "index": 5,
      "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell",
      "orig_title": "Long-term recurrent convolutional networks for visual recognition and description",
      "paper_id": "1411.4389v4"
    },
    {
      "index": 6,
      "title": "SlowFast Networks for Video Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He",
      "orig_title": "Slowfast networks for video recognition",
      "paper_id": "1812.03982v3"
    },
    {
      "index": 7,
      "title": "Spatiotemporal multiplier networks for video action recognition",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes"
    },
    {
      "index": 8,
      "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman",
      "orig_title": "Convolutional two-stream network fusion for video action recognition",
      "paper_id": "1604.06573v2"
    },
    {
      "index": 9,
      "title": "Res2Net: A New Multi-scale Backbone Architecture",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.01169",
      "authors": "Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr",
      "orig_title": "Res2net: A new multi-scale backbone architecture",
      "paper_id": "1904.01169v3"
    },
    {
      "index": 10,
      "title": "ActionVLAD: Learning spatio-temporal aggregation for action classification",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, and Bryan Russell",
      "orig_title": "Actionvlad: Learning spatio-temporal aggregation for action classification",
      "paper_id": "1704.02895v1"
    },
    {
      "index": 11,
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.02677",
      "authors": "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He",
      "orig_title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "paper_id": "1706.02677v2"
    },
    {
      "index": 12,
      "title": "The “something something” video database for learning and evaluating visual common sense",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.",
      "orig_title": "The “something something” video database for learning and evaluating visual common sense",
      "paper_id": "1706.04261v2"
    },
    {
      "index": 13,
      "title": "StNet: Local and Global Spatial-Temporal Modeling for Action Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, and Shilei Wen",
      "orig_title": "Stnet: Local and global spatial-temporal modeling for action recognition",
      "paper_id": "1811.01549v3"
    },
    {
      "index": 14,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 15,
      "title": "Identity Mappings in Deep Residual Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Identity mappings in deep residual networks",
      "paper_id": "1603.05027v3"
    },
    {
      "index": 16,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber"
    },
    {
      "index": 17,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "J Hu, L Shen, S Albanie, G Sun, and E Wu",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 18,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Jie Hu, Li Shen, and Gang Sun",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 19,
      "title": "Densely connected convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger"
    },
    {
      "index": 20,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1502.03167",
      "authors": "Sergey Ioffe and Christian Szegedy"
    },
    {
      "index": 21,
      "title": "STM: SpatioTemporal and Motion Encoding for Action Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan",
      "orig_title": "Stm: Spatiotemporal and motion encoding for action recognition",
      "paper_id": "1908.02486v2"
    },
    {
      "index": 22,
      "title": "Large-scale video classification with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "CVPR",
      "authors": "Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei"
    },
    {
      "index": 23,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "NIPS",
      "authors": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton"
    },
    {
      "index": 24,
      "title": "Hmdb: a large video database for human motion recognition",
      "abstract": "",
      "year": "2011",
      "venue": "ICCV",
      "authors": "Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre"
    },
    {
      "index": 25,
      "title": "Temporal bilinear networks for video action recognition",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Yanghao Li, Sijie Song, Yuqi Li, and Jiaying Liu"
    },
    {
      "index": 26,
      "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Ji Lin, Chuang Gan, and Song Han",
      "orig_title": "Tsm: Temporal shift module for efficient video understanding",
      "paper_id": "1811.08383v3"
    },
    {
      "index": 27,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 28,
      "title": "Temporal difference networks for video action recognition",
      "abstract": "",
      "year": "2018",
      "venue": "WACV",
      "authors": "Joe Yue-Hei Ng and Larry S Davis"
    },
    {
      "index": 29,
      "title": "Video and learning: a systematic review (2007–2017)",
      "abstract": "",
      "year": "2018",
      "venue": "ICLAK",
      "authors": "Oleksandra Poquet, Lisa Lim, Negin Mirriahi, and Shane Dawson"
    },
    {
      "index": 30,
      "title": "Learning spatio-temporal representation with local and global diffusion",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei"
    },
    {
      "index": 31,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 32,
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Two-stream convolutional networks for action recognition in videos",
      "paper_id": "1406.2199v2"
    },
    {
      "index": 33,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 34,
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1212.0402",
      "authors": "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah"
    },
    {
      "index": 35,
      "title": "Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi",
      "orig_title": "Human action recognition using factorized spatio-temporal convolutional networks",
      "paper_id": "1510.00562v1"
    },
    {
      "index": 36,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 37,
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri"
    },
    {
      "index": 38,
      "title": "Video classification with channel-separated convolutional networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.02811",
      "authors": "Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli"
    },
    {
      "index": 39,
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri"
    },
    {
      "index": 40,
      "title": "Long-term Temporal Convolutions for Action Recognition",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Gül Varol, Ivan Laptev, and Cordelia Schmid",
      "orig_title": "Long-term temporal convolutions for action recognition",
      "paper_id": "1604.04494v2"
    },
    {
      "index": 41,
      "title": "Residual attention network for image classification",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang"
    },
    {
      "index": 42,
      "title": "Appearance-and-Relation Networks for Video Classification",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Limin Wang, Wei Li, Wen Li, and Luc Van Gool",
      "orig_title": "Appearance-and-relation networks for video classification",
      "paper_id": "1711.09125v2"
    },
    {
      "index": 43,
      "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool",
      "orig_title": "Temporal segment networks: Towards good practices for deep action recognition",
      "paper_id": "1608.00859v1"
    },
    {
      "index": 44,
      "title": "Non-local Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He",
      "orig_title": "Non-local neural networks",
      "paper_id": "1711.07971v3"
    },
    {
      "index": 45,
      "title": "Videos as Space-Time Region Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Xiaolong Wang and Abhinav Gupta",
      "orig_title": "Videos as space-time region graphs",
      "paper_id": "1806.01810v2"
    },
    {
      "index": 46,
      "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy",
      "orig_title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
      "paper_id": "1712.04851v2"
    },
    {
      "index": 47,
      "title": "Describing Videos by Exploiting Temporal Structure",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville",
      "orig_title": "Describing videos by exploiting temporal structure",
      "paper_id": "1502.08029v5"
    },
    {
      "index": 48,
      "title": "Beyond Short Snippets: Deep Networks for Video Classification",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici",
      "orig_title": "Beyond short snippets: Deep networks for video classification",
      "paper_id": "1503.08909v2"
    },
    {
      "index": 49,
      "title": "A duality based approach for realtime tv-l 1 optical flow",
      "abstract": "",
      "year": "2007",
      "venue": "Joint Pattern Recognition Symposium",
      "authors": "Christopher Zach, Thomas Pock, and Horst Bischof"
    },
    {
      "index": 50,
      "title": "Recognize actions by disentangling components of dynamics",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Yue Zhao, Yuanjun Xiong, and Dahua Lin"
    },
    {
      "index": 51,
      "title": "Trajectory convolution for action recognition",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Yue Zhao, Yuanjun Xiong, and Dahua Lin"
    },
    {
      "index": 52,
      "title": "Temporal Relational Reasoning in Videos",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba",
      "orig_title": "Temporal relational reasoning in videos",
      "paper_id": "1711.08496v2"
    },
    {
      "index": 53,
      "title": "Eco: Efficient convolutional network for online video understanding",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox"
    }
  ]
}