{
  "paper_id": "2008.00177v1",
  "title": "Multi-node Bert-pretraining: Cost-efficient Approach",
  "sections": {
    "distributed training": "As deep learning models become more powerful and complex, the training of those models also demands more computation resources. Large models like ResNet¬†[ref]25 and DeepSpeech2¬† can take weeks to train on a single GPU device¬†[ref]27. The need for shortening the training time of large deep learning models has brought up distributed training algorithms. Among the distributed training algorithms, data parallelism¬†[ref]28 and model parallelism¬† are the two most popular types. Data parallelism¬†[ref]28 is a natural way to scale out the training process by slicing and distributing the training data into multiple devices. Each worker will retain a full replica of the model on different data. Workers will synchronize over the updates of the model by exchanging gradients. In contrast, model parallelism¬† divides the model into different pieces and distributes those pieces into each devices to form a training pipeline. Workers will train on the same date but for different parts of the model, which allows devices to fit bigger model. Workers will synchronize over the activation maps. However, as the training graphs of deep learning models are typically directed-acyclic, model parallelism¬†, which partitions the execution graph, essentially introduces strong sequential dependencies: at most one device will be fully utilized in computing at any given time of training. To gain more device utilization, pipeline devices to overlap computation was proposed¬†. Such overlapping requires activation maps that are supposed to be synchronized to be stacked and stored. Assuming mini-batch data enters the pipeline one at a time, and the pipeline has length nùëõn, this will impose an extra linear scale of memory storage for devices in the pipeline, which is not scalable as the number of devices increases. Furthermore, the hard limit on device memory that model parallelism¬† brings leaves very little room for researchers to optimize the training throughput. Compared with model parallelism¬†, data parallelism¬†[ref]28 also introduces hard limit: each device must be able to fit in one complete replica of the model. One might argue that this is a bottleneck as models are getting bigger, it‚Äôs the feature maps that consumes most memory rather than the model itself. Although model size usually is not a bottleneck, data parallelism suffers from trade-offs between synchronization cost and model parameter staleness in parameter updates¬†[ref]28. Network topologies have also been explored in the implementation of data parallelism in order to reduce the synchronization cost. For example, a ring based system topology¬† has been proposed to maximize the inter-device communication bandwidth. By having all devices jointly form a ring topology, each device only passes the computed weight gradient to its neighbor in the ring. Such approach guarantees that communication channel between any two devices will be filled up with maximum one model‚Äôs gradient, avoiding traffic congestions. Further benchmarks have shown this approach guarantees linear scalability of bandwidth with respect to the number of devices¬†."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 1,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler"
    },
    {
      "index": 2,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N. Gomez, Lukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention Is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 3,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Sepp Hochreiter and J√ºrgen Schmidhuber"
    },
    {
      "index": 4,
      "title": "Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc¬†V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, ≈Åukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean",
      "orig_title": "Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 5,
      "title": "Sequence to Sequence Learning with Neural Networks",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv e-prints",
      "authors": "Ilya Sutskever, Oriol Vinyals, and Quoc¬†V. Le"
    },
    {
      "index": 6,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv e-prints",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 7,
      "title": "Massive Exploration of Neural Machine Translation Architectures",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le"
    },
    {
      "index": 8,
      "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv e-prints",
      "authors": "Bojian Zheng, Abhishek Tiwari, Nandita Vijaykumar, and Gennady Pekhimenko",
      "orig_title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
      "paper_id": "1805.08899v5"
    },
    {
      "index": 9,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Emma Strubell, Ananya Ganesh, and Andrew McCallum",
      "orig_title": "Energy and policy considerations for deep learning in nlp",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 10,
      "title": "Deep learning example",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 11,
      "title": "Nvidia dgx-1 with tesla v100 system architecture",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 12,
      "title": "Nvidia‚Äôs dgx-2: Sixteen tesla v100s, 30 tb of nvme, only $400k",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Ian Cutress"
    },
    {
      "index": 13,
      "title": "NVIDIA Tensor Core Programmability, Performance & Precision",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Stefano Markidis, Steven Wei¬†Der Chien, Erwin Laure, Ivy¬†Bo Peng, and Jeffrey¬†S. Vetter",
      "orig_title": "NVIDIA tensor core programmability, performance & precision",
      "paper_id": "1803.04014v1"
    },
    {
      "index": 14,
      "title": "Mixed Precision Training",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu",
      "orig_title": "Mixed Precision Training",
      "paper_id": "1710.03740v3"
    },
    {
      "index": 15,
      "title": "Mixed precision grappler optimizer",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Matt Conley, Minmin Sun, and Yan Jun"
    },
    {
      "index": 16,
      "title": "Nvidia t4",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 17,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"
    },
    {
      "index": 18,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime¬†G. Carbonell, Ruslan Salakhutdinov, and Quoc¬†V. Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 19,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov",
      "orig_title": "Roberta: A robustly optimized BERT pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 20,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Tom¬†B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel¬†M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 21,
      "title": "MLPerf Training Benchmark",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv e-prints",
      "authors": "Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim Hazelwood, Andrew Hock, Xinyuan Huang, Atsushi Ike, Bill Jia, Daniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St. John, Tsuguchika Tabaru, Carole-Jean Wu, Lingjie Xu, Masafumi Yamazaki, Cliff Young, and Matei Zaharia"
    },
    {
      "index": 22,
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Priya Goyal, Piotr Doll√°r, Ross¬†B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He",
      "orig_title": "Accurate, large minibatch SGD: training imagenet in 1 hour",
      "paper_id": "1706.02677v2"
    },
    {
      "index": 23,
      "title": "Reducing BERT pre-training time from 3 days to 76 minutes",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh"
    },
    {
      "index": 24,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 25,
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse¬†H. Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni¬†Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew¬†Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi¬†Wang, Zhiqian Wang, Chong Wang, Bo¬†Xiao, Dani Yogatama, Jun Zhan, and Zhenyao Zhu",
      "orig_title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "paper_id": "1512.02595v1"
    },
    {
      "index": 26,
      "title": "Ai and compute",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Dario Amodei, Danny Hernandez, Girish Sastry, Jack Clark, Greg Brockman, and Ilya Sutskever"
    },
    {
      "index": 27,
      "title": "Measuring the Effects of Data Parallelism on Neural Network Training",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Christopher¬†J. Shallue, Jaehoon Lee, Joseph¬†M. Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George¬†E. Dahl",
      "orig_title": "Measuring the effects of data parallelism on neural network training",
      "paper_id": "1811.03600v3"
    },
    {
      "index": 28,
      "title": "One weird trick for parallelizing convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "Alex Krizhevsky",
      "orig_title": "One weird trick for parallelizing convolutional neural networks",
      "paper_id": "1404.5997v2"
    },
    {
      "index": 29,
      "title": "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc¬†V. Le, and Zhifeng Chen"
    },
    {
      "index": 30,
      "title": "Fast multi-gpu collectives with nccl",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Nathan Luehr"
    },
    {
      "index": 31,
      "title": "Nccl tests",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Nathan Luehr"
    },
    {
      "index": 32,
      "title": "Nvidia tesla v100 gpu architecture.",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 33,
      "title": "Wikiextractor",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Giuseppe Attardi"
    },
    {
      "index": 34,
      "title": "Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc¬†V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, ≈Åukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean",
      "orig_title": "Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 35,
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang"
    },
    {
      "index": 36,
      "title": "High level introduction to hdf5",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "The¬†HDF Group"
    },
    {
      "index": 37,
      "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Dan Hendrycks and Kevin Gimpel"
    },
    {
      "index": 38,
      "title": "Layer Normalization",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Jimmy Lei Ba, Jamie¬†Ryan Kiros, and Geoffrey¬†E. Hinton"
    },
    {
      "index": 39,
      "title": "Apex (A PyTorch Extension)",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 40,
      "title": "Nvidia p100",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 41,
      "title": "Nvidia rtx2080ti",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "NVIDIA"
    },
    {
      "index": 42,
      "title": "Gradient sparsification for communication-efficient distributed optimization",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Jianqiao Wangni, Jialei Wang, Ji¬†Liu, and Tong Zhang"
    },
    {
      "index": 43,
      "title": "Gpus pricing",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Google"
    }
  ]
}