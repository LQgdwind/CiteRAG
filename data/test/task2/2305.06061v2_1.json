{
  "paper_id": "2305.06061v2",
  "title": "Visual Tuning",
  "sections": {
    "sequential adapter": "Sequential adapter refers to the technique of inserting parameters into a sequential forward network shown in Fig. 3(a), which typically includes a linear down projection, a non-linear activation function, an up projection, and a residual connection. This approach is commonly applied after the multi-head attention layer and/or the feed-forward layer to enhance model performance. In particular, given a d𝑑d-dimensional input feature map Zlsuperscript𝑍𝑙Z^{l}, the number of parameters of adapter can be adjusted by a hyperparameter db​o​t​t​l​esubscript𝑑𝑏𝑜𝑡𝑡𝑙𝑒d_{bottle} (db​o​t​t​l​e≪d)much-less-thansubscript𝑑𝑏𝑜𝑡𝑡𝑙𝑒𝑑(d_{bottle}\\ll d). The sequential adapter module first uses a down-projection (i.e., downsampling) with Wd​o​w​n∈ℝd×db​o​t​t​l​esubscript𝑊𝑑𝑜𝑤𝑛superscriptℝ𝑑subscript𝑑𝑏𝑜𝑡𝑡𝑙𝑒W_{down}\\in\\mathbb{R}^{d{\\times}d_{bottle}} to project the feature to the lower-dimensional representation, followed by a ReLU activation function and an up-projection (i.e., upsampling) with Wu​p∈ℝdb​o​t​t​l​e×dsubscript𝑊𝑢𝑝superscriptℝsubscript𝑑𝑏𝑜𝑡𝑡𝑙𝑒𝑑W_{up}\\in\\mathbb{R}^{d_{bottle}{\\times}d}.\nThe above formulation can be written as below: where Z^lsuperscript^𝑍𝑙\\hat{Z}^{l} denotes the optimized features outputted by the sequential adapter. In sequential adapter strategies, research efforts can be roughly categorized into two groups: inserting residual blocks directly, and using more parameter optimization techniques to minimize the size of the adapter.\nStudies of the first family  [ref]130 [ref]17  emerge in the earlier stage without large-scale models, the initial development of Res-adapt  involves a customized deep network structure that utilizes adapter residual modules to dynamically adjust to different visual domains in real-time.\nFollowing Res-adapt, DAN [ref]130 once converges to a comparable or even a higher level of performance while requiring a fraction of the parameters (typically 13%) of standard fine-tuning procedures.\nTheir recent efforts [ref]17 develop LST, a technique that trains a separate ladder side network using intermediate activations from backbone networks via shortcut connections to make predictions. This approach improves the model’s accuracy while reducing computational complexity.\nIn addition, Conv-Adapter systematically investigates feasible solutions to learn task-specific knowledge, which adapts intermediate features of each residual block in pre-trained ConvNets using four different variants, including the parallel adapter discussed below. A second family of approaches        focuses on optimizing the network architecture in order to better understand the efficiency of sequential adapters.\nAlternatively, EPM  suggests that universal parametric neural network families are used, but with limited parameters. It discusses a number of methods for parametrization, including parallel and series residual adapters, compression of joint adapters, and parameter allocation.\nPolyhistor  decomposes a hyper-network into a pair of separate hyper-networks and factorizes an adapter weight matrix into two kernels for parameter reduction in multi-tasking architecture.\nPro-tuning  exploits semantic information from diverse levels to enrich the feature space by building multiple stage-wise prompt blocks, which can be simply plugged into a model for quickly adapting to a new task.\nBy generating attention weights without token-token interactions, AMixer  captures long-term and short-term spatial dependencies without self-attention.\nShysheya et. al  proposed a novel technique called Fit, which scales and shifts activations and leverages a Naive Bayes final layer classifier for image classification, and is automatically configured and achieves promising results.\nMarouf et. al  introduced TINA, an approach that iteratively reduces the size of every adapter using a scoring function compared to neuron importance. This process enables the automatic estimation of every adapter and improves the efficiency of the overall model.\nLuo et. al  proposed RepAdapter, which uses re-parameterization of sparse structure to approach nearby projection weights and further reduces the parameters of the model while maintaining its effectiveness and lightweight nature. Adapters have become a popular technique for foundation tasks in computer vision, where the pre-training task is often image classification. However, other tasks such as high-level vision tasks [ref]78      , low-level vision tasks  , video understanding [ref]78  , and robotic control  all require designs that are tailored to their specific architecture, in order to efficiently transfer learned parameters and achieve good performance through PETL.\nIn addition to these task differences, recent research has proposed innovative ways to utilize adapters in different applications.\nFor instance, BDTL and ViTDet   adjust a plain backbone with minimal adaptions only during fine-tuning, thus eliminating the hierarchical constraint from the object detection task.\nIn Florence [ref]78, representations are expanded from coarse to fine, from static to dynamic, and from RGB to a number of modalities (caption, depth). It can be used to perform a wide range of vision tasks by incorporating universal visual-language representations from Web-scale image-text data.\nSND  utilizes a dynamic stacked network as the adapter to activate the pre-trained model in a plug-and-play method for image restoration.\nMK-Adapter  introduces a learnable multi-knowledge adapter to adaptively blend the predictions from CLIP and DINO for few-shot classification.\nADA  is developed to perform continual learning using pre-trained transformers and adapters, which adds an adapter before layer norm and feed-forward layers (MLP).\nAIM  introduces spatial, temporal, and joint adapter separately and ST-Adapter  propose a spatiotemporal adapter, which both aim to equip an image model with spatiotemporal reasoning capability for video understanding.\nPEA  addresses the limitations of classical fine-tuning in robotic manipulation by utilizing the classic bottleneck architecture widely used in image classification as a lossless adaptation.\nCAOA  proposes a content-adaptive optimization framework for image compression that trains adapters inserted into the decoder through optimization in terms of rate-distortion, with adapter parameters being additionally transmitted. In the field of multi-modal learning, with the development of large-scale cross-modal pre-trained models, i.e., CLIP  and ALIGN , adapter technique has been widely adopted, using a design analogous to the one mentioned above, to adapt various downstream tasks for efficient fine-tuning      3 6 5 4 7 with excellent results.\nHA  firstly analyses empirically and recommends general recipes for efficient multi-modal transfer learning including LayerNorm-tuning.\nCLIP-Adapter  combines residual-style feature blending with an additional bottleneck adapter to learn new features on either a visual or a language branch.\nOn this basis, Tip-Adapter  aims to enhance the few-shot capability, but does not require backpropagation during training by using a key-value cache.\nMAGMA  combines visual and textual inputs to produce text using adapter-based fine-tuning, resulting in more complex generative language models.\nBALLAD  augments the representations of tail classes on balanced training samples by employing an additional adapter layer for long-tailed vision language learning.\nBuilding on previous work, subsequent methods focus more on parameter efficiency and multi-modal effectiveness.\nThrough a hierarchical adapter module structure, Hierarchical3D 2 integrates multi-modal content into a trained textual summarizer, while tuning only a few model parameters.\nVL-Adapter  adjusts the pre-trained model with sequential adapter layers for the cross-modal domain (i.e., image-text, and video-text) to construct the benchmark.\nHyperPELT 3 is an approach for fine-tuning small modules in a pre-trained language model using a shared hyper-network that takes trainable hyper-embeddings as input.\nThen CrossModal-Adapter and MV-Adapter 6 7 both attempt to make further improvements with a notable difference: allowing cross-modal interactions early in the training process by the weight-sharing mechanism.\nTo perform better on challenging datasets, SVL-Adapter 4 brings together the complementary strengths of vision-language pre-training and self-supervised representation learning for extreme scenes like few-shot learning.\nLAVISH 5 firstly migrates the adapters for pre-trained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT.\nThese innovative approaches demonstrate the versatility of adapters and their potential for various applications beyond traditional classification tasks."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "A Survey on Visual Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Han, K. et al.",
      "orig_title": "A survey on vision transformer",
      "paper_id": "2012.12556v6"
    },
    {
      "index": 2,
      "title": "Transformers in Vision: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "Khan, S. et al.",
      "orig_title": "Transformers in vision: A survey",
      "paper_id": "2101.01169v5"
    },
    {
      "index": 3,
      "title": "On the opportunities and risks of foundation models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Bommasani, R. et al."
    },
    {
      "index": 4,
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zhou, C. et al."
    },
    {
      "index": 5,
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wu, C. et al.",
      "orig_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "paper_id": "2303.04671v1"
    },
    {
      "index": 6,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Brown, T. et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 7,
      "title": "Efficient Transformers: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "authors": "Tay, Y., Dehghani, M., Bahri, D. & Metzler, D.",
      "orig_title": "Efficient transformers: A survey",
      "paper_id": "2009.06732v3"
    },
    {
      "index": 8,
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, P. et al."
    },
    {
      "index": 9,
      "title": "Meta Pseudo Labels",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Pham, H., Dai, Z., Xie, Q. & Le, Q. V.",
      "orig_title": "Meta pseudo labels",
      "paper_id": "2003.10580v4"
    },
    {
      "index": 10,
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yu, J. et al.",
      "orig_title": "Coca: Contrastive captioners are image-text foundation models",
      "paper_id": "2205.01917v2"
    },
    {
      "index": 11,
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Dehghani, M. et al.",
      "orig_title": "Scaling vision transformers to 22 billion parameters",
      "paper_id": "2302.05442v1"
    },
    {
      "index": 12,
      "title": "Palm-e: An embodied multimodal language model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.03378",
      "authors": "Driess, D. et al."
    },
    {
      "index": 13,
      "title": "A roadmap for big model",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yuan, S. et al."
    },
    {
      "index": 14,
      "title": "A Comprehensive Survey on Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE",
      "authors": "Zhuang, F. et al.",
      "orig_title": "A comprehensive survey on transfer learning",
      "paper_id": "1911.02685v3"
    },
    {
      "index": 15,
      "title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Sung, Y.-L., Cho, J. & Bansal, M.",
      "orig_title": "Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks",
      "paper_id": "2112.06825v2"
    },
    {
      "index": 16,
      "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "authors": "Lin, Z., Madotto, A. & Fung, P.",
      "orig_title": "Exploring versatile generative language model via parameter-efficient transfer learning",
      "paper_id": "2004.03829v2"
    },
    {
      "index": 17,
      "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning",
      "abstract": "",
      "year": "2022",
      "venue": "NIPS 2022",
      "authors": "Sung, Y.-L., Cho, J. & Bansal, M.",
      "orig_title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning",
      "paper_id": "2206.06522v2"
    },
    {
      "index": 18,
      "title": "Pca-sift: A more distinctive representation for local image descriptors",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "Ke, Y. & Sukthankar, R."
    },
    {
      "index": 19,
      "title": "Brief: Binary robust independent elementary features",
      "abstract": "",
      "year": "2010",
      "venue": "Computer Vision–ECCV 2010: 11th European Conference on Computer Vision",
      "authors": "Calonder, M., Lepetit, V., Strecha, C. & Fua, P."
    },
    {
      "index": 20,
      "title": "Orb: An efficient alternative to sift or surf",
      "abstract": "",
      "year": "2011",
      "venue": "2011 International conference on computer vision",
      "authors": "Rublee, E., Rabaud, V., Konolige, K. & Bradski, G."
    },
    {
      "index": 21,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "LeCun, Y., Bengio, Y. & Hinton, G.",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 22,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Deng, J. et al."
    },
    {
      "index": 23,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Szegedy, C. et al.",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 24,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "He, K., Zhang, X., Ren, S. & Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 25,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Communications of the ACM",
      "authors": "Krizhevsky, A., Sutskever, I. & Hinton, G. E."
    },
    {
      "index": 26,
      "title": "A Survey on Deep Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on artificial neural networks",
      "authors": "Tan, C. et al.",
      "orig_title": "A survey on deep transfer learning",
      "paper_id": "1808.01974v1"
    },
    {
      "index": 27,
      "title": "Learning to learn",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Thrun, S. & Pratt, L."
    },
    {
      "index": 28,
      "title": "The Kinetics Human Action Video Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Kay, W. et al.",
      "orig_title": "The kinetics human action video dataset",
      "paper_id": "1705.06950v1"
    },
    {
      "index": 29,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Vaswani, A. et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 30,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Radford, A. et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 31,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Jia, C. et al."
    },
    {
      "index": 32,
      "title": "A Generalist Agent",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Reed, S. et al.",
      "orig_title": "A generalist agent",
      "paper_id": "2205.06175v3"
    },
    {
      "index": 33,
      "title": "FLAVA: A Foundational Language And Vision Alignment Model",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Singh, A. et al.",
      "orig_title": "Flava: A foundational language and vision alignment model",
      "paper_id": "2112.04482v3"
    },
    {
      "index": 34,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Alayrac, J.-B. et al."
    },
    {
      "index": 35,
      "title": "Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Singh, M. et al.",
      "orig_title": "Revisiting weakly supervised pre-training of visual perception models",
      "paper_id": "2201.08371v2"
    },
    {
      "index": 36,
      "title": "Receptive fields of single neurones in the cat’s striate cortex",
      "abstract": "",
      "year": "1959",
      "venue": "The Journal of physiology",
      "authors": "Hubel, D. H. & Wiesel, T. N."
    },
    {
      "index": 37,
      "title": "Backpropagation and the brain",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Reviews Neuroscience",
      "authors": "Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. & Hinton, G."
    },
    {
      "index": 38,
      "title": "Atoms of recognition in human and computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the National Academy of Sciences",
      "authors": "Ullman, S., Assif, L., Fetaya, E. & Harari, D."
    },
    {
      "index": 39,
      "title": "Vision: A computational investigation into the human representation and processing of visual information",
      "abstract": "",
      "year": "2010",
      "venue": "MIT press",
      "authors": "Marr, D."
    },
    {
      "index": 40,
      "title": "The ecological approach to visual perception: classic edition",
      "abstract": "",
      "year": "2014",
      "venue": "Psychology press",
      "authors": "Gibson, J. J."
    },
    {
      "index": 41,
      "title": "Human-level concept learning through probabilistic program induction",
      "abstract": "",
      "year": "2015",
      "venue": "Science",
      "authors": "Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B."
    },
    {
      "index": 42,
      "title": "Ten Lessons We Have Learned in the New “Sparseland”: A Short Handbook for Sparse Neural Network Researchers",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Liu, S. & Wang, Z.",
      "orig_title": "Ten lessons we have learned in the new” sparseland”: A short handbook for sparse neural network researchers",
      "paper_id": "2302.02596v3"
    },
    {
      "index": 43,
      "title": "A Modern Introduction to Probability and Statistics: Understanding why and how",
      "abstract": "",
      "year": "2005",
      "venue": "Springer",
      "authors": "Dekking, F. M., Kraaikamp, C., Lopuhaä, H. P. & Meester, L. E."
    },
    {
      "index": 44,
      "title": "Towards a Theoretical Framework of Out-of-Distribution Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ye, H. et al.",
      "orig_title": "Towards a theoretical framework of out-of-distribution generalization",
      "paper_id": "2106.04496v3"
    },
    {
      "index": 45,
      "title": "Who is closer: A computational method for domain gap evaluation",
      "abstract": "",
      "year": "2022",
      "venue": "Pattern Recognition",
      "authors": "Liu, X. & Zhang, S."
    },
    {
      "index": 46,
      "title": "Bridging Theory and Algorithm for Domain Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Zhang, Y., Liu, T., Long, M. & Jordan, M.",
      "orig_title": "Bridging theory and algorithm for domain adaptation",
      "paper_id": "1904.05801v2"
    },
    {
      "index": 47,
      "title": "On the Theory of Transfer Learning: The Importance of Task Diversity",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tripuraneni, N., Jordan, M. & Jin, C.",
      "orig_title": "On the theory of transfer learning: The importance of task diversity",
      "paper_id": "2006.11650v2"
    },
    {
      "index": 48,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Simonyan, K. & Zisserman, A.",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 49,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z."
    },
    {
      "index": 50,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Tan, M. & Le, Q.",
      "orig_title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 51,
      "title": "2017 international conference on engineering and technology (ICET), 1–6 (Ieee, 2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Albawi, S., Mohammed, T. A. &\nAl-Zawi, S. Understanding of a\nconvolutional neural network."
    },
    {
      "index": 52,
      "title": "A survey of convolutional neural networks: analysis, applications, and prospects",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE transactions on neural networks and\nlearning systems (",
      "authors": "Li, Z., Liu, F., Yang,\nW., Peng, S. & Zhou, J."
    },
    {
      "index": 53,
      "title": "K. et al. Incorporating convolution designs into visual transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Yuan, K. et al. Incorporating\nconvolution designs into visual transformers."
    },
    {
      "index": 54,
      "title": "Mmnet: A model-based multimodal network for human action recognition in rgb-d videos",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Bruce, X., Liu, Y., Zhang,\nX., Zhong, S.-h. & Chan, K. C."
    },
    {
      "index": 55,
      "title": "M. Learning spatiotemporal features with 3d convolutional networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Tran, D., Bourdev, L.,\nFergus, R., Torresani, L. &\nPaluri, M. Learning spatiotemporal\nfeatures with 3d convolutional networks."
    },
    {
      "index": 56,
      "title": "action recognition? a new model and the kinetics dataset",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Carreira, J. & Zisserman, A.\nQuo vadis, action recognition? a new model and the\nkinetics dataset."
    },
    {
      "index": 57,
      "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Xie, S., Sun, C., Huang,\nJ., Tu, Z. & Murphy, K.\nRethinking spatiotemporal feature learning:\nSpeed-accuracy trade-offs in video classification.",
      "orig_title": "K. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
      "paper_id": "1712.04851v2"
    },
    {
      "index": 58,
      "title": "C. X3d: Expanding architectures for efficient video recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Feichtenhofer, C. X3d: Expanding\narchitectures for efficient video recognition."
    },
    {
      "index": 59,
      "title": "Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, 47–54 (Springer, 2016)",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Lea, C., Vidal, R.,\nReiter, A. & Hager, G. D.\nTemporal convolutional networks: A unified approach to\naction segmentation."
    },
    {
      "index": 60,
      "title": "Temporal Convolutional Networks for Action Segmentation and Detection",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Lea, C., Flynn, M. D.,\nVidal, R., Reiter, A. &\nHager, G. D. Temporal convolutional\nnetworks for action segmentation and detection.",
      "orig_title": "Temporal convolutional networks for action segmentation and detection",
      "paper_id": "1611.05267v1"
    },
    {
      "index": 61,
      "title": "3D human pose estimation in video with temporal convolutions and semi-supervised training",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Pavllo, D., Feichtenhofer, C.,\nGrangier, D. & Auli, M.\n3d human pose estimation in video with temporal\nconvolutions and semi-supervised training.",
      "orig_title": "M. 3d human pose estimation in video with temporal convolutions and semi-supervised training",
      "paper_id": "1811.11742v2"
    },
    {
      "index": 62,
      "title": "R. et al. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, R. et al. Attention\nmechanism exploits temporal contexts: Real-time 3d human pose\nreconstruction."
    },
    {
      "index": 63,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Dosovitskiy, A. et al. An image\nis worth 16x16 words: Transformers for image recognition at scale."
    },
    {
      "index": 64,
      "title": "International conference on machine learning, 10347–10357 (PMLR, 2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Touvron, H. et al. Training\ndata-efficient image transformers & distillation through attention."
    },
    {
      "index": 65,
      "title": "Transformer in Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 15908–15919\n(",
      "authors": "Han, K. et al.",
      "orig_title": "Transformer in transformer",
      "paper_id": "2103.00112v3"
    },
    {
      "index": 66,
      "title": "L. et al. Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Yuan, L. et al. Tokens-to-token\nvit: Training vision transformers from scratch on imagenet."
    },
    {
      "index": 67,
      "title": "W. et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wang, W. et al. Pyramid vision\ntransformer: A versatile backbone for dense prediction without\nconvolutions."
    },
    {
      "index": 68,
      "title": "Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, Z. et al. Swin transformer:\nHierarchical vision transformer using shifted windows."
    },
    {
      "index": 69,
      "title": "Z. et al. Video swin transformer",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Liu, Z. et al. Video swin\ntransformer."
    },
    {
      "index": 70,
      "title": "Conditional Positional Encodings for Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chu, X. et al.",
      "orig_title": "Conditional positional encodings for vision transformers",
      "paper_id": "2102.10882v3"
    },
    {
      "index": 71,
      "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 9355–9366\n(",
      "authors": "Chu, X. et al.",
      "orig_title": "Twins: Revisiting the design of spatial attention in vision transformers",
      "paper_id": "2104.13840v4"
    },
    {
      "index": 72,
      "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Huang, Z. et al.",
      "orig_title": "Shuffle transformer: Rethinking spatial shuffle for vision transformer",
      "paper_id": "2106.03650v1"
    },
    {
      "index": 73,
      "title": "J. et al. Cmt: Convolutional neural networks meet vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Guo, J. et al. Cmt:\nConvolutional neural networks meet vision transformers."
    },
    {
      "index": 74,
      "title": "Volo: Vision outlooker for visual recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Yuan, L., Hou, Q., Jiang,\nZ., Feng, J. & Yan, S."
    },
    {
      "index": 75,
      "title": "A. Revisiting unreasonable effectiveness of data in deep learning era",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Sun, C., Shrivastava, A.,\nSingh, S. & Gupta, A.\nRevisiting unreasonable effectiveness of data in deep\nlearning era."
    },
    {
      "index": 76,
      "title": "K. et al. Masked autoencoders are scalable vision learners",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "He, K. et al. Masked\nautoencoders are scalable vision learners."
    },
    {
      "index": 77,
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tong, Z., Song, Y., Wang,\nJ. & Wang, L.",
      "orig_title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "paper_id": "2203.12602v3"
    },
    {
      "index": 78,
      "title": "Florence: A New Foundation Model for Computer Vision",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yuan, L. et al.",
      "orig_title": "Florence: A new foundation model for computer vision",
      "paper_id": "2111.11432v1"
    },
    {
      "index": 79,
      "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, W. et al.",
      "orig_title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "paper_id": "2208.10442v2"
    },
    {
      "index": 80,
      "title": "ClimaX: A foundation model for weather and climate",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Nguyen, T., Brandstetter, J.,\nKapoor, A., Gupta, J. K. &\nGrover, A.",
      "orig_title": "Climax: A foundation model for weather and climate",
      "paper_id": "2301.10343v5"
    },
    {
      "index": 81,
      "title": "Language models generalize beyond natural proteins",
      "abstract": "",
      "year": "2022",
      "venue": "bioRxiv 2022–12\n(",
      "authors": "Verkuil, R. et al."
    },
    {
      "index": 82,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, 709–727 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jia, M. et al. Visual prompt\ntuning."
    },
    {
      "index": 83,
      "title": "S-Prompts Learning with Pre-trained Transformers: An Occam’s Razor for Domain Incremental Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, Y., Huang, Z. &\nHong, X.",
      "orig_title": "S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning",
      "paper_id": "2207.12819v2"
    },
    {
      "index": 84,
      "title": "Visual Prompt Tuning For Test-time Domain Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, Y. et al.",
      "orig_title": "Visual prompt tuning for test-time domain adaptation",
      "paper_id": "2210.04831v2"
    },
    {
      "index": 85,
      "title": "ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhou, Z., Zhang, B., Lei,\nY., Liu, L. & Liu, Y.",
      "orig_title": "Zegclip: Towards adapting clip for zero-shot semantic segmentation",
      "paper_id": "2212.03588v3"
    },
    {
      "index": 86,
      "title": "Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Dong, R. et al."
    },
    {
      "index": 87,
      "title": "PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Herzig, R. et al.",
      "orig_title": "Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data",
      "paper_id": "2212.04821v3"
    },
    {
      "index": 88,
      "title": "Understanding Zero-shot Adversarial Robustness for Large-Scale Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Mao, C., Geng, S., Yang,\nJ., Wang, X. & Vondrick, C.",
      "orig_title": "Understanding zero-shot adversarial robustness for large-scale models",
      "paper_id": "2212.07016v2"
    },
    {
      "index": 89,
      "title": "Unleashing the Power of Visual Prompting At the Pixel Level",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wu, J. et al.",
      "orig_title": "Unleashing the power of visual prompting at the pixel level",
      "paper_id": "2212.10556v2"
    },
    {
      "index": 90,
      "title": "ProSFDA: Prompt Learning based Source-free Domain Adaptation for Medical Image Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hu, S., Liao, Z. & Xia,\nY.",
      "orig_title": "Prosfda: Prompt learning based source-free domain adaptation for medical image segmentation",
      "paper_id": "2211.11514v1"
    },
    {
      "index": 91,
      "title": "\\\\\\backslasha-la-carte prompt tuning (apt): Combining distinct data via composable prompting",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bowman, B. et al."
    },
    {
      "index": 92,
      "title": "Prompting and tuning: A two-stage unsupervised domain adaptive person re-identification method on vision transformer backbone",
      "abstract": "",
      "year": "2023",
      "venue": "Tsinghua Science and Technology\n28, 799–810\n(",
      "authors": "Yu, S., Dou, Z. & Wang,\nS."
    },
    {
      "index": 93,
      "title": "LPT: Long-tailed Prompt Tuning for Image Classification",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Dong, B., Zhou, P., Yan,\nS. & Zuo, W.",
      "orig_title": "Lpt: Long-tailed prompt tuning for image classification",
      "paper_id": "2210.01033v2"
    },
    {
      "index": 94,
      "title": "R. et al. Pointclip: Point cloud understanding by clip",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhang, R. et al. Pointclip:\nPoint cloud understanding by clip."
    },
    {
      "index": 95,
      "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, Z., Yu, X., Rao,\nY., Zhou, J. & Lu, J.",
      "orig_title": "P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting",
      "paper_id": "2208.02812v2"
    },
    {
      "index": 96,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Wu, C. H., Motamed, S.,\nSrivastava, S. & De la Torre, F.\nGenerative visual prompt: Unifying distributional\ncontrol of pre-trained generative models."
    },
    {
      "index": 97,
      "title": "Neural Prompt Search",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Zhou, K. &\nLiu, Z.",
      "orig_title": "Neural prompt search",
      "paper_id": "2206.04673v2"
    },
    {
      "index": 98,
      "title": "Prompt generation networks for efficient adaptation of frozen vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Loedeman, J., Stol, M. C.,\nHan, T. & Asano, Y. M."
    },
    {
      "index": 99,
      "title": "Feature-Proxy Transformer for Few-Shot Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, J.-W., Sun, Y.,\nYang, Y. & Chen, W.",
      "orig_title": "Feature-proxy transformer for few-shot segmentation",
      "paper_id": "2210.06908v1"
    },
    {
      "index": 100,
      "title": "Fine-grained retrieval prompt tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, S. et al."
    },
    {
      "index": 101,
      "title": "Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, K., Chen, L., Zhang,\nH., Xiao, J. & Sun, Q.",
      "orig_title": "Compositional prompt tuning with motion cues for open-vocabulary video relation detection",
      "paper_id": "2302.00268v1"
    },
    {
      "index": 102,
      "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gu, X., Lin, T.-Y., Kuo,\nW. & Cui, Y.",
      "orig_title": "Open-vocabulary object detection via vision and language knowledge distillation",
      "paper_id": "2104.13921v3"
    },
    {
      "index": 103,
      "title": "LION: Implicit Vision Prompt Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, H. et al.",
      "orig_title": "Lion: Implicit vision prompt tuning",
      "paper_id": "2303.09992v3"
    },
    {
      "index": 104,
      "title": "Learning to prompt for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "International Journal of Computer Vision\n130, 2337–2348\n(",
      "authors": "Zhou, K., Yang, J., Loy,\nC. C. & Liu, Z."
    },
    {
      "index": 105,
      "title": "Understanding and mitigating overfitting in prompt tuning for vision-language models",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for\nVideo Technology (",
      "authors": "Ma, C. et al."
    },
    {
      "index": 106,
      "title": "Multi-prompt alignment for multi-source unsupervised domain adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, H., Wu, Z. & Jiang,\nY.-G."
    },
    {
      "index": 107,
      "title": "Zegot: Zero-shot segmentation through optimal transport of text prompts",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kim, K., Oh, Y. & Ye,\nJ. C."
    },
    {
      "index": 108,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IV, 1–18 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ni, B. et al. Expanding\nlanguage-image pretrained models for general video recognition."
    },
    {
      "index": 109,
      "title": "Prompt-aligned Gradient for Prompt Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhu, B., Niu, Y., Han,\nY., Wu, Y. & Zhang, H.",
      "orig_title": "Prompt-aligned gradient for prompt tuning",
      "paper_id": "2205.14865v3"
    },
    {
      "index": 110,
      "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Berg, H. et al.",
      "orig_title": "A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning",
      "paper_id": "2203.11933v4"
    },
    {
      "index": 111,
      "title": "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Fei, H., Li,\nD., Yu, T. & Li, P.",
      "orig_title": "Prompting through prototype: A prototype-based prompt learning on pretrained vision-language models",
      "paper_id": "2210.10841v1"
    },
    {
      "index": 112,
      "title": "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Park, J. et al.",
      "orig_title": "Lanit: Language-driven image-to-image translation for unlabeled data",
      "paper_id": "2208.14889v4"
    },
    {
      "index": 113,
      "title": "SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for Few-shot Image Classification",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Peng, F., Yang, X. & Xu,\nC.",
      "orig_title": "Sgva-clip: Semantic-guided visual adapting of vision-language models for few-shot image classification",
      "paper_id": "2211.16191v2"
    },
    {
      "index": 114,
      "title": "Language-aware soft prompting for vision & language foundation models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bulat, A. & Tzimiropoulos, G."
    },
    {
      "index": 115,
      "title": "DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Sun, X., Hu, P. & Saenko,\nK.",
      "orig_title": "Dualcoop: Fast adaptation to multi-label recognition with limited annotations",
      "paper_id": "2206.09541v1"
    },
    {
      "index": 116,
      "title": "Prompt learning with optimal transport for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, G. et al."
    },
    {
      "index": 117,
      "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "He, X. et al.",
      "orig_title": "Cpl: Counterfactual prompt learning for vision and language models",
      "paper_id": "2210.10362v3"
    },
    {
      "index": 118,
      "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, F. et al.",
      "orig_title": "Learning to decompose visual features with latent textual prompts",
      "paper_id": "2210.04287v1"
    },
    {
      "index": 119,
      "title": "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tao, M., Bao, B.-K., Tang,\nH. & Xu, C.",
      "orig_title": "Galip: Generative adversarial clips for text-to-image synthesis",
      "paper_id": "2301.12959v1"
    },
    {
      "index": 120,
      "title": "Z. Conditional prompt learning for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhou, K., Yang, J., Loy,\nC. C. & Liu, Z. Conditional prompt\nlearning for vision-language models."
    },
    {
      "index": 121,
      "title": "Pointclip v2: Adapting clip for powerful 3d open-world learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhu, X. et al."
    },
    {
      "index": 122,
      "title": "Unified Vision and Language Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zang, Y., Li, W., Zhou,\nK., Huang, C. & Loy, C. C.",
      "orig_title": "Unified vision and language prompt learning",
      "paper_id": "2210.07225v1"
    },
    {
      "index": 123,
      "title": "Class-aware visual prompt tuning for vision-language pre-trained model",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xing, Y. et al."
    },
    {
      "index": 124,
      "title": "MaPLe: Multi-modal Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Khattak, M. U., Rasheed, H.,\nMaaz, M., Khan, S. &\nKhan, F. S.",
      "orig_title": "Maple: Multi-modal prompt learning",
      "paper_id": "2210.03117v3"
    },
    {
      "index": 125,
      "title": "Multitask Vision-Language Prompt Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shen, S. et al.",
      "orig_title": "Multitask vision-language prompt tuning",
      "paper_id": "2211.11720v3"
    },
    {
      "index": 126,
      "title": "Learning Domain Invariant Prompt for Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhao, C. et al.",
      "orig_title": "Learning domain invariant prompt for vision-language models",
      "paper_id": "2212.04196v2"
    },
    {
      "index": 127,
      "title": "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shu, M. et al.",
      "orig_title": "Test-time prompt tuning for zero-shot generalization in vision-language models",
      "paper_id": "2209.07511v1"
    },
    {
      "index": 128,
      "title": "Advances in neural information processing systems (2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Rebuffi, A., Bilen, H. &\nVedaldi, A. Learning multiple visual\ndomains with residual adapters."
    },
    {
      "index": 129,
      "title": "A. Efficient parametrization of multi-domain deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Rebuffi, S.-A., Bilen, H. &\nVedaldi, A. Efficient parametrization\nof multi-domain deep neural networks."
    },
    {
      "index": 130,
      "title": "Incremental Learning Through Deep Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 42,\n651–663 (",
      "authors": "Rosenfeld, A. & Tsotsos, J. K.",
      "orig_title": "Incremental learning through deep adaptation",
      "paper_id": "1705.04228v2"
    },
    {
      "index": 131,
      "title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, H. et al.",
      "orig_title": "Conv-adapter: Exploring parameter efficient transfer learning for convnets",
      "paper_id": "2208.07463v4"
    },
    {
      "index": 132,
      "title": "Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liu, Y.-C., Ma, C.-Y.,\nTian, J., He, Z. &\nKira, Z."
    },
    {
      "index": 133,
      "title": "Pro-tuning: Unified Prompt Tuning for Vision Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Nie, X. et al.",
      "orig_title": "Pro-tuning: Unified prompt tuning for vision tasks",
      "paper_id": "2207.14381v3"
    },
    {
      "index": 134,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI, 50–67 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Rao, Y., Zhao, W., Zhou,\nJ. & Lu, J. Amixer: Adaptive weight\nmixing for self-attention free vision transformers."
    },
    {
      "index": 135,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=9aokcgBVIj1",
      "authors": "Shysheya, A., Bronskill, J. F.,\nPatacchiola, M., Nowozin, S. &\nTurner, R. E. Fit: Parameter\nefficient few-shot transfer learning for personalized and federated image\nclassification."
    },
    {
      "index": 136,
      "title": "Tiny adapters for vision transformers (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=V0Vo9eW2nzL",
      "authors": "MAROUF, I. E., Tartaglione, E. &\nLathuilière, S."
    },
    {
      "index": 137,
      "title": "Towards Efficient Visual Adaption via Structural Re-parameterization",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Luo, G. et al.",
      "orig_title": "Towards efficient visual adaption via structural re-parameterization",
      "paper_id": "2302.08106v2"
    },
    {
      "index": 138,
      "title": "Benchmarking Detection Transfer Learning with Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Li, Y. et al.",
      "orig_title": "Benchmarking detection transfer learning with vision transformers",
      "paper_id": "2111.11429v1"
    },
    {
      "index": 139,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IX, 280–296 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Li, Y., Mao, H., Girshick,\nR. & He, K. Exploring plain vision\ntransformer backbones for object detection."
    },
    {
      "index": 140,
      "title": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, 446–462 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wang, H. et al. Stacking\nnetworks dynamically for image restoration based on the plug-and-play\nframework."
    },
    {
      "index": 141,
      "title": "Collaboration of Pre-trained Models Makes Better Few-shot Learner",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, R. et al.",
      "orig_title": "Collaboration of pre-trained models makes better few-shot learner",
      "paper_id": "2209.12255v2"
    },
    {
      "index": 142,
      "title": "C. Continual learning with transformers for image classification",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ermis, B., Zappella, G.,\nWistuba, M., Rawal, A. &\nArchambeau, C. Continual learning\nwith transformers for image classification."
    },
    {
      "index": 143,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=CIoSZ_HKHS7",
      "authors": "Yang, T. et al. AIM: Adapting\nimage models for efficient video action recognition."
    },
    {
      "index": 144,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Pan, J., Lin, Z., Zhu,\nX., Shao, J. & Li, H.\nSt-adapter: Parameter-efficient image-to-video transfer\nlearning."
    },
    {
      "index": 145,
      "title": "International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=5IND3TXJRb-",
      "authors": "Sharma, M. et al. Lossless\nadaptation of pretrained vision models for robotic manipulation."
    },
    {
      "index": 146,
      "title": "Universal Deep Image Compression via Content-Adaptive Optimization with Adapters",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Tsubota, K., Akutsu, H. &\nAizawa, K. Universal deep image\ncompression via content-adaptive optimization with adapters.",
      "orig_title": "K. Universal deep image compression via content-adaptive optimization with adapters",
      "paper_id": "2211.00918v1"
    },
    {
      "index": 147,
      "title": "How to adapt your large-scale vision-and-language model",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://openreview.net/forum?id=EhwEUb2ynIa (",
      "authors": "Kim, K., Laskin, M.,\nMordatch, I. & Pathak, D."
    },
    {
      "index": 148,
      "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, P. et al.",
      "orig_title": "Clip-adapter: Better vision-language models with feature adapters",
      "paper_id": "2110.04544v2"
    },
    {
      "index": 149,
      "title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, R. et al.",
      "orig_title": "Tip-adapter: Training-free clip-adapter for better vision-language modeling",
      "paper_id": "2111.03930v2"
    },
    {
      "index": 150,
      "title": "A Simple Long-Tailed Recognition Baseline via Vision-Language Model",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Ma, T. et al.",
      "orig_title": "A simple long-tailed recognition baseline via vision-language model",
      "paper_id": "2111.14745v1"
    },
    {
      "index": 151,
      "title": "Magma–multimodal augmentation of generative models through adapter-based finetuning",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Eichenberg, C., Black, S.,\nWeinbach, S., Parcalabescu, L. &\nFrank, A."
    },
    {
      "index": 152,
      "title": "Hierarchical3D Adapters for Long Video-to-text Summarization",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Papalampidi, P. & Lapata, M.",
      "orig_title": "Hierarchical3d adapters for long video-to-text summarization",
      "paper_id": "2210.04829v1"
    },
    {
      "index": 153,
      "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Z. et al.",
      "orig_title": "Hyperpelt: Unified parameter-efficient language model tuning for both language and vision-and-language tasks",
      "paper_id": "2203.03878v1"
    },
    {
      "index": 154,
      "title": "Svl-adapter: Self-supervised adapter for vision-language pretrained models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Pantazis, O., Brostow, G.,\nJones, K. & Mac Aodha, O."
    },
    {
      "index": 155,
      "title": "Vision Transformers are Parameter-Efficient Audio-Visual Learners",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Lin, Y.-B., Sung, Y.-L.,\nLei, J., Bansal, M. &\nBertasius, G.",
      "orig_title": "Vision transformers are parameter-efficient audio-visual learners",
      "paper_id": "2212.07983v2"
    },
    {
      "index": 156,
      "title": "Cross-Modal Adapter for Text-Video Retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jiang, H. et al.",
      "orig_title": "Cross-modal adapter for text-video retrieval",
      "paper_id": "2211.09623v1"
    },
    {
      "index": 157,
      "title": "Multimodal Video Adapter for Parameter Efficient Video Text Retrieval",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, B. et al.",
      "orig_title": "Multimodal video adapter for parameter efficient video text retrieval",
      "paper_id": "2301.07868v2"
    },
    {
      "index": 158,
      "title": "Vision Transformer Adapter for Dense Predictions",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, Z. et al.",
      "orig_title": "Vision transformer adapter for dense predictions",
      "paper_id": "2205.08534v4"
    },
    {
      "index": 159,
      "title": "Parameter-Efficient and Student-Friendly Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Rao, J., Meng, X., Ding,\nL., Qi, S. & Tao, D.",
      "orig_title": "Parameter-efficient and student-friendly knowledge distillation",
      "paper_id": "2205.15308v1"
    },
    {
      "index": 160,
      "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, S. et al.",
      "orig_title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
      "paper_id": "2205.13535v3"
    },
    {
      "index": 161,
      "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jie, S. & Deng, Z.-H.",
      "orig_title": "Convolutional bypasses are better vision transformer adapters",
      "paper_id": "2207.07039v3"
    },
    {
      "index": 162,
      "title": "Rethinking vision transformer and masked autoencoder in multimodal face anti-spoofing",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yu, Z. et al."
    },
    {
      "index": 163,
      "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Lu, H. et al.",
      "orig_title": "Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling",
      "paper_id": "2302.06605v2"
    },
    {
      "index": 164,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=J_Cja7cpgW",
      "authors": "Hao, T., Chen, H., Guo,\nY. & Ding, G. Consolidator:\nMergable adapter with group connections for visual adaptation."
    },
    {
      "index": 165,
      "title": "Exploring Efficient Few-shot Adaptation for Vision Transformers",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xu, C. et al.",
      "orig_title": "Exploring efficient few-shot adaptation for vision transformers",
      "paper_id": "2301.02419v1"
    },
    {
      "index": 166,
      "title": "Parameter-efficient fine-tuning for vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "He, X., Li, C., Zhang,\nP., Yang, J. & Wang, X. E."
    },
    {
      "index": 167,
      "title": "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Valipour, M., Rezagholizadeh, M.,\nKobyzev, I. & Ghodsi, A."
    },
    {
      "index": 168,
      "title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shimomoto, E. K. et al.",
      "orig_title": "Towards parameter-efficient integration of pre-trained language models in temporal video grounding",
      "paper_id": "2209.13359v2"
    },
    {
      "index": 169,
      "title": "Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tu, C.-H., Mai, Z. &\nChao, W.-L.",
      "orig_title": "Visual query tuning: Towards effective usage of intermediate representations for parameter and memory efficient transfer learning",
      "paper_id": "2212.03220v2"
    },
    {
      "index": 170,
      "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zaken, E. B., Goldberg, Y. &\nRavfogel, S. Bitfit: Simple\nparameter-efficient fine-tuning for transformer-based masked\nlanguage-models.",
      "orig_title": "S. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "paper_id": "2106.10199v5"
    },
    {
      "index": 171,
      "title": "Side Adapter Network for Open-Vocabulary Semantic Segmentation",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xu, M., Zhang, Z., Wei,\nF., Hu, H. & Bai, X.",
      "orig_title": "Side adapter network for open-vocabulary semantic segmentation",
      "paper_id": "2302.12242v2"
    },
    {
      "index": 172,
      "title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Fu, C.-L., Chen, Z.-C.,\nLee, Y.-R. & Lee, H.-y.",
      "orig_title": "Adapterbias: Parameter-efficient token-dependent representation shift for adapters in nlp tasks",
      "paper_id": "2205.00305v4"
    },
    {
      "index": 173,
      "title": "Differentially private bias-term only fine-tuning of foundation models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bu, Z., Wang, Y.-X., Zha,\nS. & Karypis, G."
    },
    {
      "index": 174,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "URL https://openreview.net/forum?id=nZeVKeeFYf9",
      "authors": "Hu, E. J. et al. LoRA:\nLow-rank adaptation of large language models."
    },
    {
      "index": 175,
      "title": "Motion Style Transfer: Modular Low-Rank Adaptation for Deep Motion Forecasting",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kothari, P., Li, D., Liu,\nY. & Alahi, A.",
      "orig_title": "Motion style transfer: Modular low-rank adaptation for deep motion forecasting",
      "paper_id": "2211.03165v1"
    },
    {
      "index": 176,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XX, 239–256 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jiang, Z. et al. Dna: Improving\nfew-shot transfer learning with low-rank decomposition and alignment."
    },
    {
      "index": 177,
      "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 1022–1035\n(",
      "authors": "Karimi Mahabadi, R., Henderson, J. &\nRuder, S.",
      "orig_title": "Compacter: Efficient low-rank hypercomplex adapter layers",
      "paper_id": "2106.04647v2"
    },
    {
      "index": 178,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, A. et al. Beyond\nfully-connected layers with quaternions: Parameterization of hypercomplex\nmultiplications with 1/n1𝑛1/n parameters."
    },
    {
      "index": 179,
      "title": "PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and\nLearning Systems (",
      "authors": "Grassucci, E., Zhang, A. &\nComminiello, D.",
      "orig_title": "Phnns: Lightweight neural networks via parameterized hypercomplex convolutions",
      "paper_id": "2110.04176v2"
    },
    {
      "index": 180,
      "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Transactions of the Association for\nComputational Linguistics 10",
      "authors": "Hou, Z., Salazar, J. &\nPolovets, G.",
      "orig_title": "Meta-learning the difference: Preparing large language models for efficient adaptation",
      "paper_id": "2207.03509v1"
    },
    {
      "index": 181,
      "title": "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jie, S. & Deng, Z.-H.",
      "orig_title": "Fact: Factor-tuning for lightweight adaptation on vision transformer",
      "paper_id": "2212.03145v2"
    },
    {
      "index": 182,
      "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Edalati, A. et al.",
      "orig_title": "Krona: Parameter efficient tuning with kronecker adapter",
      "paper_id": "2212.10650v1"
    },
    {
      "index": 183,
      "title": "Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Li, T. et al."
    },
    {
      "index": 184,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing\nSystems (",
      "authors": "Lian, D., Daquan, Z.,\nFeng, J. & Wang, X.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 185,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hinton, G., Vinyals, O. &\nDean, J.",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 186,
      "title": "International Conference on Learning Representations (2015)",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Romero, A. et al. Fitnets: Hints\nfor thin deep nets."
    },
    {
      "index": 187,
      "title": "Learning Student Networks via Feature Embedding",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and\nLearning Systems 32, 25–35\n(",
      "authors": "Chen, H., Wang, Y., Xu,\nC., Xu, C. & Tao, D.",
      "orig_title": "Learning student networks via feature embedding",
      "paper_id": "1812.06597v1"
    },
    {
      "index": 188,
      "title": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16, 469–484 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Guan, Y. et al. Differentiable\nfeature aggregation search for knowledge distillation."
    },
    {
      "index": 189,
      "title": "Knowledge distillation via adaptive instance normalization",
      "abstract": "",
      "year": "2020",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yang, J., Martinez, B.,\nBulat, A. & Tzimiropoulos, G.",
      "orig_title": "Knowledge distillation via adaptive instance normalization",
      "paper_id": "2003.04289v1"
    },
    {
      "index": 190,
      "title": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16, 664–680 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Xu, K., Rui, L., Li, Y.\n& Gu, L. Feature normalized\nknowledge distillation for image classification."
    },
    {
      "index": 191,
      "title": "Heterogeneous Knowledge Distillation using Information Flow Modeling",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Passalis, N., Tzelepi, M. &\nTefas, A. Heterogeneous knowledge\ndistillation using information flow modeling.",
      "orig_title": "A. Heterogeneous knowledge distillation using information flow modeling",
      "paper_id": "2005.00727v1"
    },
    {
      "index": 192,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Hao, Z. et al. Learning\nefficient vision transformers via fine-grained manifold distillation."
    },
    {
      "index": 193,
      "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing\nsystems 31 (",
      "authors": "Kim, J., Park, S. & Kwak,\nN.",
      "orig_title": "Paraphrasing complex network: Network compression via factor transfer",
      "paper_id": "1802.04977v3"
    },
    {
      "index": 194,
      "title": "Relational Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Park, W., Kim, D., Lu, Y.\n& Cho, M. Relational knowledge\ndistillation.",
      "orig_title": "M. Relational knowledge distillation",
      "paper_id": "1904.05068v2"
    },
    {
      "index": 195,
      "title": "Y. et al. Search to distill: Pearls are everywhere but not the eyes",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, Y. et al. Search to\ndistill: Pearls are everywhere but not the eyes."
    },
    {
      "index": 196,
      "title": "D. et al. Cross-layer distillation with semantic calibration",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chen, D. et al. Cross-layer\ndistillation with semantic calibration."
    },
    {
      "index": 197,
      "title": "S. et al. Distilling holistic knowledge with graph neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhou, S. et al. Distilling\nholistic knowledge with graph neural networks."
    },
    {
      "index": 198,
      "title": "Distilling Knowledge via Knowledge Review",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chen, P., Liu, S., Zhao,\nH. & Jia, J. Distilling knowledge\nvia knowledge review.",
      "orig_title": "J. Distilling knowledge via knowledge review",
      "paper_id": "2104.09044v1"
    },
    {
      "index": 199,
      "title": "Decoupled Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhao, B., Cui, Q., Song,\nR., Qiu, Y. & Liang, J.\nDecoupled knowledge distillation.",
      "orig_title": "J. Decoupled knowledge distillation",
      "paper_id": "2203.08679v2"
    },
    {
      "index": 200,
      "title": "International Conference on Learning Representations (2016)",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Chen, T., Goodfellow, I. &\nShlens, J. Net2net: Accelerating\nlearning via knowledge transfer."
    },
    {
      "index": 201,
      "title": "J. Efficient architecture search by network transformation",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Cai, H., Chen, T., Zhang,\nW., Yu, Y. & Wang, J.\nEfficient architecture search by network\ntransformation."
    },
    {
      "index": 202,
      "title": "International Conference on Learning Representations (2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Ashok, A., Rhinehart, N.,\nBeainy, F. & Kitani, K. M.\nN2n learning: Network to network compression via policy\ngradient reinforcement learning."
    },
    {
      "index": 203,
      "title": "International Conference on Learning Representations, Workshop Track Proceedings (2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Elsken, T., Metzen, J.-H. &\nHutter, F. Simple and efficient\narchitecture search for convolutional neural networks."
    },
    {
      "index": 204,
      "title": "International Conference on Machine Learning, 678–687 (PMLR, 2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Cai, H., Yang, J., Zhang,\nW., Han, S. & Yu, Y.\nPath-level network transformation for efficient\narchitecture search."
    },
    {
      "index": 205,
      "title": "International Conference on Learning Representations (2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Fang, J. et al. Fast neural\nnetwork adaptation via parameter remapping and architecture search."
    },
    {
      "index": 206,
      "title": "FNA++: Fast Network Adaptation via Parameter Remapping and Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence 43",
      "authors": "Fang, J. et al.",
      "orig_title": "Fna++: Fast network adaptation via parameter remapping and architecture search",
      "paper_id": "2006.12986v2"
    },
    {
      "index": 207,
      "title": "International Conference on Learning Representations (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, H., Simonyan, K. &\nYang, Y. Darts: Differentiable\narchitecture search."
    },
    {
      "index": 208,
      "title": "Advances in neural information processing systems, 874–884 (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chang, J. et al. DATA:\ndifferentiable architecture approximation."
    },
    {
      "index": 209,
      "title": "DATA: differentiable architecture approximation with distribution guided sampling",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.\n43, 2905–2920\n(",
      "authors": "Zhang, X. et al."
    },
    {
      "index": 210,
      "title": "Q. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chen, X., Xie, L., Wu, J.\n& Tian, Q. Progressive\ndifferentiable architecture search: Bridging the depth gap between search and\nevaluation."
    },
    {
      "index": 211,
      "title": "DARTS+: Improved Differentiable Architecture Search with Early Stopping",
      "abstract": "",
      "year": "2019",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liang, H. et al.",
      "orig_title": "Darts+: Improved differentiable architecture search with early stopping",
      "paper_id": "1909.06035v2"
    },
    {
      "index": 212,
      "title": "G. et al. Sgas: Sequential greedy architecture search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Li, G. et al. Sgas: Sequential\ngreedy architecture search."
    },
    {
      "index": 213,
      "title": "International Conference on Learning Representations (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Xie, S., Zheng, H., Liu,\nC. & Lin, L. Snas: stochastic\nneural architecture search."
    },
    {
      "index": 214,
      "title": "MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "He, C., Ye, H., Shen, L.\n& Zhang, T. Milenas: Efficient\nneural architecture search via mixed-level reformulation.",
      "orig_title": "T. Milenas: Efficient neural architecture search via mixed-level reformulation",
      "paper_id": "2003.12238v1"
    },
    {
      "index": 215,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chu, X. et al. Darts-: robustly\nstepping out of performance collapse without indicators."
    },
    {
      "index": 216,
      "title": "St-adapter: Parameter-efficient image-to-video transfer learning for action recognition",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Pan, J., Lin, Z., Zhu,\nX., Shao, J. & Li, H."
    },
    {
      "index": 217,
      "title": "International conference on machine learning, 1691–1703 (PMLR, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Chen, M. et al. Generative\npretraining from pixels."
    },
    {
      "index": 218,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Li, C. et al. Efficient\nself-supervised vision transformers for representation learning."
    },
    {
      "index": 219,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "URL https://openreview.net/forum?id=0RDcd5Axok",
      "authors": "He, J., Zhou, C., Ma, X.,\nBerg-Kirkpatrick, T. & Neubig, G.\nTowards a unified view of parameter-efficient transfer\nlearning."
    },
    {
      "index": 220,
      "title": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740–755 (Springer, 2014)",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Lin, T.-Y. et al. Microsoft\ncoco: Common objects in context."
    },
    {
      "index": 221,
      "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 42",
      "authors": "Liu, J. et al.",
      "orig_title": "Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding",
      "paper_id": "1905.04757v2"
    },
    {
      "index": 222,
      "title": "V. & Sminchisescu",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 36",
      "authors": "Ionescu, C., Papava, D.,\nOlaru, V. & Sminchisescu, C."
    },
    {
      "index": 223,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Kumar, A., Raghunathan, A.,\nJones, R. M., Ma, T. &\nLiang, P. Fine-tuning can distort\npretrained features and underperform out-of-distribution."
    },
    {
      "index": 224,
      "title": "T. et al. Else-net: Elastic semantic network for continual action recognition from skeleton data",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Li, T. et al. Else-net: Elastic\nsemantic network for continual action recognition from skeleton data."
    },
    {
      "index": 225,
      "title": "Visual prompt tuning for generative transfer learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Sohn, K. et al."
    },
    {
      "index": 226,
      "title": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kim, M., Kim, H.-I. & Ro,\nY. M.",
      "orig_title": "Prompt tuning of deep neural networks for speaker-adaptive visual speech recognition",
      "paper_id": "2302.08102v2"
    },
    {
      "index": 227,
      "title": "Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gan, Y. et al.",
      "orig_title": "Decorate the newcomers: Visual domain prompt for continual test time adaptation",
      "paper_id": "2212.04145v2"
    },
    {
      "index": 228,
      "title": "J. et al. Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Lin, J. et al. Being comes from\nnot-being: Open-vocabulary text-to-motion generation with wordless\ntraining."
    },
    {
      "index": 229,
      "title": "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV, 105–124 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ju, C., Han, T., Zheng,\nK., Zhang, Y. & Xie, W.\nPrompting visual-language models for efficient video\nunderstanding."
    },
    {
      "index": 230,
      "title": "Prefix conditioning unifies language and label supervision",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Saito, K. et al."
    },
    {
      "index": 231,
      "title": "International Conference on Machine Learning, 2790–2799 (PMLR, 2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Houlsby, N. et al.\nParameter-efficient transfer learning for nlp."
    },
    {
      "index": 232,
      "title": "L. et al. Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, L. et al. Cross-modal\ncollaborative representation learning and a large-scale rgbt benchmark for\ncrowd counting."
    },
    {
      "index": 233,
      "title": "A Survey on Deep Hashing Methods",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Transactions on Knowledge Discovery from\nData 17, 1–50\n(",
      "authors": "Luo, X. et al.",
      "orig_title": "A survey on deep hashing methods",
      "paper_id": "2003.03369v5"
    },
    {
      "index": 234,
      "title": "Prompt-Matched Semantic Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liu, L., Yu, B. X., Chang,\nJ., Tian, Q. & Chen, C.-W.",
      "orig_title": "Prompt-matched semantic segmentation",
      "paper_id": "2208.10159v3"
    },
    {
      "index": 235,
      "title": "Towards a unified view on visual parameter-efficient transfer learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yu, B. X., Chang, J., Liu,\nL., Tian, Q. & Chen, C. W."
    },
    {
      "index": 236,
      "title": "Pruning adapters with lottery ticket",
      "abstract": "",
      "year": "2022",
      "venue": "Algorithms 15,\n63 (",
      "authors": "Wu, J. & Chen, Q."
    },
    {
      "index": 237,
      "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "abstract": "",
      "year": "2023",
      "venue": "Nature Machine Intelligence\n1–16 (",
      "authors": "Ding, N. et al."
    },
    {
      "index": 238,
      "title": "Role of Bias Terms in Dot-Product Attention",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Namazifar, M., Hazarika, D. &\nHakkani-Tur, D.",
      "orig_title": "Role of bias terms in dot-product attention",
      "paper_id": "2302.08626v1"
    },
    {
      "index": 239,
      "title": "S.-W. Attribute injection for pretrained language models: A new benchmark and an efficient method",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Amplayo, R. K., Yoo, K. M. &\nLee, S.-W. Attribute injection for\npretrained language models: A new benchmark and an efficient method."
    },
    {
      "index": 240,
      "title": "A multilinear singular value decomposition",
      "abstract": "",
      "year": "2000",
      "venue": "SIAM journal on Matrix Analysis and\nApplications 21, 1253–1278\n(",
      "authors": "De Lathauwer, L., De Moor, B. &\nVandewalle, J."
    },
    {
      "index": 241,
      "title": "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing\n31, 3386–3398\n(",
      "authors": "Luo, G. et al.",
      "orig_title": "Towards lightweight transformer via group-wise transformation for vision-and-language tasks",
      "paper_id": "2204.07780v1"
    },
    {
      "index": 242,
      "title": "Unipelt: A unified framework for parameter-efficient language model tuning",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Mao, Y. et al."
    },
    {
      "index": 243,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jiang, Z. et al. Back razor:\nMemory-efficient transfer learning by self-sparsified backpropagation."
    },
    {
      "index": 244,
      "title": "Low-Rank Winograd Transformation for 3D Convolutional Neural Networks",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Qin, Z., Lin, M. & Lin,\nW.",
      "orig_title": "Low-rank winograd transformation for 3d convolutional neural networks",
      "paper_id": "2301.11180v1"
    },
    {
      "index": 245,
      "title": "S. Fast algorithms for convolutional neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Lavin, A. & Gray, S.\nFast algorithms for convolutional neural networks."
    },
    {
      "index": 246,
      "title": "Compressing Deep Convolutional Networks using Vector Quantization",
      "abstract": "",
      "year": "2014",
      "venue": "Preprint at https://arxiv.org/abs/1412.6115\n(",
      "authors": "Gong, Y., Liu, L., Yang,\nM. & Bourdev, L.",
      "orig_title": "Compressing deep convolutional networks using vector quantization",
      "paper_id": "1412.6115v1"
    },
    {
      "index": 247,
      "title": "Learning Structured Sparsity in Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing\nsystems 29 (",
      "authors": "Wen, W., Wu, C., Wang,\nY., Chen, Y. & Li, H.",
      "orig_title": "Learning structured sparsity in deep neural networks",
      "paper_id": "1608.03665v4"
    },
    {
      "index": 248,
      "title": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4783–4787 (IEEE, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ma, Z. et al. Dhwp: Learning\nhigh-quality short hash codes via weight pruning."
    },
    {
      "index": 249,
      "title": "R. & Niculescu-Mizil",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Buciluǎ, C., Caruana, R. &\nNiculescu-Mizil, A. Model\ncompression."
    },
    {
      "index": 250,
      "title": "Do deep nets really need to be deep?",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing\nsystems 27 (",
      "authors": "Ba, J. & Caruana, R."
    },
    {
      "index": 251,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 35",
      "authors": "Bengio, Y., Courville, A. &\nVincent, P."
    },
    {
      "index": 252,
      "title": "Y. et al. Knowledge distillation via instance relationship graph",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, Y. et al. Knowledge\ndistillation via instance relationship graph."
    },
    {
      "index": 253,
      "title": "B. et al. Correlation congruence for knowledge distillation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Peng, B. et al. Correlation\ncongruence for knowledge distillation."
    },
    {
      "index": 254,
      "title": "J. Mimicking very efficient network for object detection",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Li, Q., Jin, S. & Yan,\nJ. Mimicking very efficient network for object\ndetection."
    },
    {
      "index": 255,
      "title": "J. et al. Distilling object detectors via decoupled features",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Guo, J. et al. Distilling object\ndetectors via decoupled features."
    },
    {
      "index": 256,
      "title": "Z. et al. Localization distillation for dense object detection",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zheng, Z. et al. Localization\ndistillation for dense object detection."
    },
    {
      "index": 257,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, L. & Ma, K.\nImprove object detection with feature-based knowledge\ndistillation: Towards accurate and efficient detectors."
    },
    {
      "index": 258,
      "title": "Y. et al. Structured knowledge distillation for semantic segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, Y. et al. Structured\nknowledge distillation for semantic segmentation."
    },
    {
      "index": 259,
      "title": "Structured knowledge distillation for dense prediction",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence (",
      "authors": "Liu, Y., Shu, C., Wang,\nJ. & Shen, C."
    },
    {
      "index": 260,
      "title": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16, 346–362 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wang, Y., Zhou, W., Jiang,\nT., Bai, X. & Xu, Y.\nIntra-class feature variation distillation for semantic\nsegmentation."
    },
    {
      "index": 261,
      "title": "C. et al. Cross-image relational knowledge distillation for semantic segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yang, C. et al. Cross-image\nrelational knowledge distillation for semantic segmentation."
    },
    {
      "index": 262,
      "title": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X 16, 93–110 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Porrello, A., Bergamini, L. &\nCalderara, S. Robust\nre-identification by multiple views knowledge distillation."
    },
    {
      "index": 263,
      "title": "2022 IEEE International Conference on Image Processing (ICIP), 3853–3557 (IEEE, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Remigereau, F. et al. Knowledge\ndistillation for multi-target domain adaptation in real-time person\nre-identification."
    },
    {
      "index": 264,
      "title": "Y. et al. Data-free knowledge distillation for image super-resolution",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, Y. et al. Data-free\nknowledge distillation for image super-resolution."
    },
    {
      "index": 265,
      "title": "Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Angarano, S., Salvetti, F.,\nMartini, M. & Chiaberge, M.",
      "orig_title": "Generative adversarial super-resolution at the edge with knowledge distillation",
      "paper_id": "2209.03355v2"
    },
    {
      "index": 266,
      "title": "Boosting light-weight depth estimation via knowledge distillation",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hu, J. et al."
    },
    {
      "index": 267,
      "title": "Z. Knowledge distillation for fast and accurate monocular depth estimation on mobile devices",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wang, Y., Li, X., Shi,\nM., Xian, K. & Cao, Z.\nKnowledge distillation for fast and accurate monocular\ndepth estimation on mobile devices."
    },
    {
      "index": 268,
      "title": "L. et al. Efficient crowd counting via structured knowledge transfer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, L. et al. Efficient crowd\ncounting via structured knowledge transfer."
    },
    {
      "index": 269,
      "title": "Bidirectional recurrent neural networks",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE transactions on Signal Processing\n45, 2673–2681\n(",
      "authors": "Schuster, M. & Paliwal, K. K."
    },
    {
      "index": 270,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Russell, S. J.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 271,
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Sandler, M., Howard, A.,\nZhu, M., Zhmoginov, A. &\nChen, L.-C. Mobilenetv2: Inverted\nresiduals and linear bottlenecks.",
      "orig_title": "L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks",
      "paper_id": "1801.04381v4"
    },
    {
      "index": 272,
      "title": "International conference on machine learning, 4095–4104 (PMLR, 2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Pham, H., Guan, M., Zoph,\nB., Le, Q. & Dean, J.\nEfficient neural architecture search via parameters\nsharing."
    },
    {
      "index": 273,
      "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chu, X., Zhang, B. & Xu,\nR. Fairnas: Rethinking evaluation fairness of weight\nsharing neural architecture search.",
      "orig_title": "R. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
      "paper_id": "1907.01845v5"
    },
    {
      "index": 274,
      "title": "International Conference on Learning Representations (2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Xu, Y. et al. Pc-darts: Partial\nchannel connections for memory-efficient architecture search."
    },
    {
      "index": 275,
      "title": "A Survey on Multi-Task Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data\nEngineering (",
      "authors": "Zhang, Y. & Yang, Q.",
      "orig_title": "A survey on multi-task learning",
      "paper_id": "1707.08114v3"
    },
    {
      "index": 276,
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM\n65, 99–106\n(",
      "authors": "Mildenhall, B. et al.",
      "orig_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
      "paper_id": "2003.08934v2"
    },
    {
      "index": 277,
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Rombach, R., Blattmann, A.,\nLorenz, D., Esser, P. &\nOmmer, B. High-resolution image\nsynthesis with latent diffusion models.",
      "orig_title": "B. High-resolution image synthesis with latent diffusion models",
      "paper_id": "2112.10752v2"
    },
    {
      "index": 278,
      "title": "The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Chen, T., Zhang, Z.,\nCheng, Y., Awadallah, A. &\nWang, Z. The principle of diversity:\nTraining stronger vision transformers calls for reducing all levels of\nredundancy.",
      "orig_title": "Z. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy",
      "paper_id": "2203.06345v1"
    },
    {
      "index": 279,
      "title": "Scaling and assessment of data quality",
      "abstract": "",
      "year": "2006",
      "venue": "Acta Crystallographica Section D: Biological\nCrystallography 62, 72–82\n(",
      "authors": "Evans, P."
    },
    {
      "index": 280,
      "title": "H. Data security and privacy protection issues in cloud computing",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Chen, D. & Zhao, H.\nData security and privacy protection issues in cloud\ncomputing."
    },
    {
      "index": 281,
      "title": "Data security and privacy in cloud computing",
      "abstract": "",
      "year": "2014",
      "venue": "International Journal of Distributed Sensor\nNetworks 10, 190903\n(",
      "authors": "Sun, Y., Zhang, J., Xiong,\nY. & Zhu, G."
    },
    {
      "index": 282,
      "title": "Secure, privacy-preserving and federated machine learning in medical imaging",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Machine Intelligence\n2, 305–311\n(",
      "authors": "Kaissis, G. A., Makowski, M. R.,\nRückert, D. & Braren, R. F."
    },
    {
      "index": 283,
      "title": "Recent advances of continual learning in computer vision: An overview",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Qu, H., Rahmani, H., Xu,\nL., Williams, B. & Liu, J."
    },
    {
      "index": 284,
      "title": "Adversarial interference and its mitigations in privacy-preserving collaborative machine learning",
      "abstract": "",
      "year": "2021",
      "venue": "Nature Machine Intelligence\n3, 749–758\n(",
      "authors": "Usynin, D. et al."
    },
    {
      "index": 285,
      "title": "Nvidia hopper h100 gpu: Scaling performance",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Micro (",
      "authors": "Choquette, J."
    },
    {
      "index": 286,
      "title": "Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges",
      "abstract": "",
      "year": "2021",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery e",
      "authors": "Bischl, B. et al.",
      "orig_title": "Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges",
      "paper_id": "2107.05847v3"
    },
    {
      "index": 287,
      "title": "Weight-sharing neural architecture search: A battle to shrink the optimization gap",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)\n54, 1–37 (",
      "authors": "Xie, L. et al."
    },
    {
      "index": 288,
      "title": "Dynamic Neural Networks: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Han, Y. et al.",
      "orig_title": "Dynamic neural networks: A survey",
      "paper_id": "2102.04906v4"
    },
    {
      "index": 289,
      "title": "Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction",
      "abstract": "",
      "year": "2022",
      "venue": "North American Chapter of the Association for\nComputational Linguistics (",
      "authors": "Chen, X. et al.",
      "orig_title": "Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction",
      "paper_id": "2205.03521v1"
    },
    {
      "index": 290,
      "title": "Exploring a large-scale multi-modal transportation recommendation system",
      "abstract": "",
      "year": "2021",
      "venue": "Transportation Research Part C: Emerging\nTechnologies 126, 103070\n(",
      "authors": "Liu, Y., Lyu, C., Liu, Z.\n& Cao, J."
    },
    {
      "index": 291,
      "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, X., Wang, W., Cao,\nY., Shen, C. & Huang, T.",
      "orig_title": "Images speak in images: A generalist painter for in-context visual learning",
      "paper_id": "2212.02499v2"
    },
    {
      "index": 292,
      "title": "What Makes Good Examples for Visual In-Context Learning?",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Zhou, K. &\nLiu, Z.",
      "orig_title": "What makes good examples for visual in-context learning?",
      "paper_id": "2301.13670v2"
    },
    {
      "index": 293,
      "title": "Chatgpt for robotics: Design principles and model abilities",
      "abstract": "",
      "year": "2023",
      "venue": "Microsoft (",
      "authors": "Vemprala, S., Bonatti, R.,\nBucker, A. & Kapoor, A."
    }
  ]
}