{
  "paper_id": "2207.05420v2",
  "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP",
  "sections": {
    "introduction": "Convolutional Neural Networks (CNNs) dominate the learning of visual representations and show effectiveness on various visual tasks, including image classification, object detection, semantic segmentation, etc.\nRecently, convolution-free backbones show impressive performances on image classification [ref]10. Vision Transformer (ViT) [ref]11 demonstrates that pure transformer architecture that is mainly built on multi-head self-attentions (MSAs) can attain state-of-the-art performance when trained on large-scale datasets (e.g., ImageNet-21K, JFT-300M).\nMLP-Mixer [ref]41 introduced a pure multi-layer perceptron (MLP) architecture that can almost match ViT’s performance without using the time-consuming attention mechanism.\nThe main operators in those networks perform differently in terms of efficiency and data utilization.\nOn the one hand, convolutions in CNNs are locally connected and their weights are input-independent, which makes it effective at extracting low-level representations and efficient under the low-data regime. On the other hand, MSAs in the transformer capture long-range dependency, and the attention weights are dynamically dependent on the input representations. Hence, it is more data and computation demanding. The token-mixing in MLP-Mixer performs like a depthwise convolution of a full receptive field with parameter sharing, which is also data demanding. It is an important topic to study how to combine them effectively to form high-performance hybrid visual architectures, which, however, remains a challenge.\n There were recent papers on attempting to manually combine the different types of operators to form hybrid visual networks. In ViT [ref]11, a hybrid architecture using ResNet and transformer is also studied and improves upon pure transformers for smaller model sizes. Besides, many other works        also explored the combination of convolution and transformer to form hybrid architectures to improve data or computation efficiency. Furthermore, the combination of convolution and MLP is studied in , and the combination of gated MLP and MSA is studied in . Those previous approaches focus on combining two distinct operators and can achieve satisfactory performances to some extent. However, a unified view and a systematical study are missed in prior arts. We identify two key challenges when building high-performance hybrid architectures: (1) The operators can be implemented with various styles, and it is infeasible to manually explore all possible implementations and combinations. Although we can automate the exploration with Neural Architecture Search (NAS) techniques, the search space should be properly designed so that the search cost is affordable. (2) Each operator has its own characteristics, and simply combining them together does not lead to optimal results. We conduct a simple pilot study on directly stacking different operators to form hybrid networks. As shown in Table 1, however, the straightforward stacking of different operators achieves even worse performance than the vanilla ViT. In this paper, we study the learnable combination of convolution, transformer, and MLP by proposing a novel unified architecture search approach. Our approach has two key designs to address the challenges mentioned above.\nFirst, we model distinct operators in a unified form, and use the same\nset of searchable configuration parameters (i.e., OP type, expansion, channels, etc) to characterize each of the different operators.\nThe unified design enables us to greatly reduce the overall search space, and as a result, the total search cost becomes affordable. Besides, we propose context-aware downsampling modules (DSMs) to harmonize the combination of different operators. The proposed DSMs can be instantiated into three types, i.e., Local-DSM (L-DSM), Local-Global-DSM (LG-DSM), and Global-DSM (G-DSM), aiming to better adapt the representations from one operator to another.\nBased on these designs, we build a unified search space consisting of a large family of different general operators (GOPs), DSMs, and network size, and jointly optimize model accuracy and FLOPs for identifying high-performance hybrid networks. We illustrate the search space and the backbone in Figure 6. The discovered network, named UniNet, exhibits strong performance and efficiency improvements over common ConvNets, Transformers, or hybrid architectures on various visual benchmarks. Our experiments show that UniNet has the following characteristics: (1) placing convolutions in the shallow layers and transformers in the deep layers, (2) allocating a similar amount of FLOPs for both convolutions and transformers, and (3) inserting L-DSM to downsample for convolutions and LG-DSM for transformers. Our analysis shows that the conclusion is consistent among the top-5 models. To go even further, we build a family of high-performance UniNet models by scaling up the searched baseline network, which achieves better accuracy and efficiency in both small and large model sizes. In particular, our UniNet-B5 achieves comparable accuracy (+0.1%) to EfficientNet-B7 while requires much less computation cost (-44%) (Figure 1 (a)). By pretraining on large-scale ImageNet-21K, our UniNet-B6 achieves 87.4% accuracy, outperforming Swin-L with fewer FLOPs (-51%) and parameters (-41%) (Figure 1 (c))."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Layer normalization",
      "abstract": "",
      "year": "2016",
      "venue": "ArXiv",
      "authors": "Ba, J., Kiros, J.R., Hinton, G.E."
    },
    {
      "index": 1,
      "title": "High-performance large-scale image recognition without normalization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Brock, A., De, S., Smith, S.L., Simonyan, K."
    },
    {
      "index": 2,
      "title": "Once-for-all: Train one network and specialize it for efficient deployment",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S."
    },
    {
      "index": 3,
      "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Chollet, F.",
      "orig_title": "Xception: Deep learning with depthwise separable convolutions",
      "paper_id": "1610.02357v3"
    },
    {
      "index": 4,
      "title": "Conditional Positional Encodings for Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Chu, X., Tian, Z., Bo Zhang, X.W., Wei, X., Xia, H., Shen, C.",
      "orig_title": "Conditional positional encodings for vision transformers",
      "paper_id": "2102.10882v3"
    },
    {
      "index": 5,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V."
    },
    {
      "index": 6,
      "title": "Fbnetv3: Joint architecture-recipe search using neural acquisition function",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Dai, X., Wan, A., Zhang, P., Wu, B., He, Z., Wei, Z., Chen, K., Tian, Y., Yu, M., Vajda, P., et al."
    },
    {
      "index": 7,
      "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Dai, Z., Liu, H., Le, Q.V., Tan, M.",
      "orig_title": "Coatnet: Marrying convolution and attention for all data sizes",
      "paper_id": "2106.04803v2"
    },
    {
      "index": 8,
      "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "d’Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., Sagun, L.",
      "orig_title": "Convit: Improving vision transformers with soft convolutional inductive biases",
      "paper_id": "2103.10697v2"
    },
    {
      "index": 9,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L."
    },
    {
      "index": 10,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 11,
      "title": "Container: Context Aggregation Network",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Gao, P., Lu, J., Li, H., Mottaghi, R., Kembhavi, A.",
      "orig_title": "Container: Context aggregation network",
      "paper_id": "2106.01401v2"
    },
    {
      "index": 12,
      "title": "Lip: Local importance-based pooling",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Gao, Z., Wang, L., Wu, G."
    },
    {
      "index": 13,
      "title": "Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Gong, C., Wang, D., Li, M., Chen, X., Yan, Z., Tian, Y., Chandra, V., et al."
    },
    {
      "index": 14,
      "title": "Levit: a vision transformer in convnet’s clothing for faster inference",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., Jégou, H., Douze, M."
    },
    {
      "index": 15,
      "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Guo, J., Han, K., Wu, H., Xu, C., Tang, Y., Xu, C., Wang, Y.",
      "orig_title": "Cmt: Convolutional neural networks meet vision transformers",
      "paper_id": "2107.06263v3"
    },
    {
      "index": 16,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "He, K., Zhang, X., Ren, S., Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 17,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv: Learning",
      "authors": "Hendrycks, D., Gimpel, K.",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 18,
      "title": "Augment your batch: Improving generalization through instance repetition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., Soudry, D."
    },
    {
      "index": 19,
      "title": "Vision permutator: A permutable mlp-like architecture for visual recognition",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Hou, Q., Jiang, Z., Yuan, L., Cheng, M.M., Yan, S., Feng, J."
    },
    {
      "index": 20,
      "title": "Deep Networks with Stochastic Depth",
      "abstract": "",
      "year": "2016",
      "venue": "European conference on computer vision",
      "authors": "Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.",
      "orig_title": "Deep networks with stochastic depth",
      "paper_id": "1603.09382v3"
    },
    {
      "index": 21,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "ArXiv",
      "authors": "Ioffe, S., Szegedy, C."
    },
    {
      "index": 22,
      "title": "How Much Position Information Do Convolutional Neural Networks Encode?",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Islam, M.A., Jia, S., Bruce, N.D.",
      "orig_title": "How much position information do convolutional neural networks encode?",
      "paper_id": "2001.08248v1"
    },
    {
      "index": 23,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Kingma, D.P., Ba, J."
    },
    {
      "index": 24,
      "title": "Convmlp: Hierarchical convolutional mlps for vision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Li, J., Hassani, A., Walton, S., Shi, H."
    },
    {
      "index": 25,
      "title": "Pay Attention to MLPs",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems 34",
      "authors": "Liu, H., Dai, Z., So, D., Le, Q.",
      "orig_title": "Pay attention to mlps",
      "paper_id": "2105.08050v2"
    },
    {
      "index": 26,
      "title": "Darts: Differentiable architecture search",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, H., Simonyan, K., Yang, Y."
    },
    {
      "index": 27,
      "title": "FNAS: Uncertainty-Aware Fast Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Liu, J., Zhang, M., Sun, Y., Liu, B., Song, G., Liu, Y., Li, H.",
      "orig_title": "Fnas: Uncertainty-aware fast neural architecture search",
      "paper_id": "2105.11694v3"
    },
    {
      "index": 28,
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B."
    },
    {
      "index": 29,
      "title": "Designing Network Design Spaces",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Dollár, P.",
      "orig_title": "Designing network design spaces",
      "paper_id": "2003.13678v1"
    },
    {
      "index": 30,
      "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "S, Z., J, L., H, Z., X, Z., Z, L., Y, W., Y, F., J, F., T, X., PH, T., L., Z."
    },
    {
      "index": 31,
      "title": "Detail-preserving pooling in deep networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Saeedan, F., Weber, N., Goesele, M., Roth, S."
    },
    {
      "index": 32,
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.",
      "orig_title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "paper_id": "1801.04381v4"
    },
    {
      "index": 33,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O."
    },
    {
      "index": 34,
      "title": "Bottleneck Transformers for Visual Recognition",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.",
      "orig_title": "Bottleneck transformers for visual recognition",
      "paper_id": "2101.11605v2"
    },
    {
      "index": 35,
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., Beyer, L.",
      "orig_title": "How to train your vit? data, augmentation, and regularization in vision transformers",
      "paper_id": "2106.10270v2"
    },
    {
      "index": 36,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 37,
      "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.",
      "orig_title": "Mnasnet: Platform-aware neural architecture search for mobile",
      "paper_id": "1807.11626v3"
    },
    {
      "index": 38,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Tan, M., Le, Q.",
      "orig_title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 39,
      "title": "Efficientnetv2: Smaller models and faster training",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Tan, M., Le, Q.V."
    },
    {
      "index": 40,
      "title": "MLP-Mixer: An all-MLP Architecture for Vision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., et al.",
      "orig_title": "Mlp-mixer: An all-mlp architecture for vision",
      "paper_id": "2105.01601v4"
    },
    {
      "index": 41,
      "title": "ResMLP: Feedforward networks for image classification with data-efficient training",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Joulin, A., Synnaeve, G., Verbeek, J., Jégou, H.",
      "orig_title": "Resmlp: Feedforward networks for image classification with data-efficient training",
      "paper_id": "2105.03404v2"
    },
    {
      "index": 42,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 43,
      "title": "Going deeper with image transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., Jégou, H."
    },
    {
      "index": 44,
      "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Vaswani, A., Ramachandran, P., Srinivas, A., Parmar, N., Hechtman, B., Shlens, J.",
      "orig_title": "Scaling local self-attention for parameter efficient visual backbones",
      "paper_id": "2103.12731v3"
    },
    {
      "index": 45,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 46,
      "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Wang, D., Gong, C., Li, M., Liu, Q., Chandra, V.",
      "orig_title": "Alphanet: Improved training of supernets with alpha-divergence",
      "paper_id": "2102.07954v2"
    },
    {
      "index": 47,
      "title": "AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Wang, D., Li, M., Gong, C., Chandra, V.",
      "orig_title": "Attentivenas: Improving neural architecture search via attentive sampling",
      "paper_id": "2011.09011v2"
    },
    {
      "index": 48,
      "title": "CARAFE++: Unified Content-Aware ReAssembly of FEatures",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Wang, J., Chen, K., Xu, R., Liu, Z., Loy, C.C., Lin, D.",
      "orig_title": "Carafe++: Unified content-aware reassembly of features",
      "paper_id": "2012.04733v1"
    },
    {
      "index": 49,
      "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.",
      "orig_title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "paper_id": "2102.12122v2"
    },
    {
      "index": 50,
      "title": "CvT: Introducing Convolutions to Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.",
      "orig_title": "Cvt: Introducing convolutions to vision transformers",
      "paper_id": "2103.15808v1"
    },
    {
      "index": 51,
      "title": "Early Convolutions Help Transformers See Better",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., Girshick, R.",
      "orig_title": "Early convolutions help transformers see better",
      "paper_id": "2106.14881v3"
    },
    {
      "index": 52,
      "title": "Incorporating Convolution Designs into Visual Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., Wu, W.",
      "orig_title": "Incorporating convolution designs into visual transformers",
      "paper_id": "2103.11816v2"
    },
    {
      "index": 53,
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.J."
    },
    {
      "index": 54,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Zhang, H., Cissé, M., Dauphin, Y., Lopez-Paz, D.",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 55,
      "title": "Making convolutional networks shift-invariant again",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Zhang, R."
    }
  ]
}