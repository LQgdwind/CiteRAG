{
  "paper_id": "2004.06190v3",
  "title": "A Divide-and-Conquer Approach to the Summarization of Long Documents",
  "sections": {
    "introduction": "Summarization is closely related to data compression and information understanding, both of which are key to information science and retrieval. Being able to produce informative and well-written document summaries has the potential to greatly improve the success of both information discovery systems and human readers that are trying to quickly skim large numbers of documents for important information. Indeed, automatic summarization has been recently recognized as one of the most important natural language processing (NLP) tasks, yet one of the least solved ones . This work is concerned with the neural summarization of long documents, such as academic articles and financial reports. In previous years, neural summarization approaches have mainly focused on short pieces of text that typically come from news articles   [ref]4     . This is also reflected in the amount of datasets that exist for this particular problem 0 1 2 3. Summarizing long documents is a very different problem to newswire summarization. In academic articles for example, the input text can range from 2,000 to 7,000 words, while in the case of newswire articles it rarely exceeds 700 words 4. Similarly, the expected summary of a news article is less than 100 words long, while the abstract of an academic article can easily exceed 200 words. The increased input and output length lead neural summarization methods to a much higher computational complexity, making it extremely hard to train models that have enough capacity to perform this task. This is more prominent with abstractive summarization models where the complexity of text generation becomes prohibitive for very long sequences. Most importantly, long documents introduce a lot of noise to the summarization process. Indeed, one of the major difficulties in summarizing a long document is that large parts of the document are not really key to its narrative and thus should be ignored. Finally, long summaries typically contain a number of diverse key information points from a document, which are more difficult to produce, compared to the more focused information contained in short summaries. Certain methods have tried to address these problems by limiting the size of the input document, either by selecting specific sections that are more informative 5, or by first employing a more efficient extractive model that learns to identify and select the most important parts of the input 6 7. While this reduces the noise and the computational cost in processing a long document, there remain the computational cost and information diversity issues in producing a long summary. In the context of Transformer models with self-attention, sparse attention mechanisms such as Big Bird 8 manage to increase the input length by a large amount but they still cannot scale to very long summaries. In contrast to the above methods that aim to produce a complete summary at once, we propose a novel divide-and-conquer approach that first breaks both the document and its target summary into multiple smaller source-target pairs, then trains a neural model that learns to summarize these smaller document parts, and finally during inference aggregates the partial summaries in order to produce a final complete summary.\nBy decomposing the problem of long document summarization into smaller summarization problems, our approach reduces the computational complexity of the summarization task. At the same time, our approach increases the number and, more importantly, the quality of the training examples by having source and target summary pairs that are focused on a specific aspect of the text, which results in better alignment between them and less noise. This leads to a decomposition of the summarization problem into simpler summarization problems that are easier to learn.\nEmpirical results on two publicly available datasets of academic articles, show that our approach can enhance the ability of summarization models and lead to overall improved results. We show that using a 3 years-old sequence-to-sequence model [ref]4, our approach manages to achieve surprisingly good results, surpassing recent more advanced models 4 5. In addition, when paired with a very strong Transformer model such as PEGASUS 9 our method produces results that are on par with the state-of-the-art on both datasets. This paper is based on past work 0 that assumed the existence of structured summaries, such as those available for some of the biomedical articles indexed in PubMed. Here we lift this assumption by using sentence level Rouge similarities in order to match sentences of the summary with parts of the document and automatically create source-target pairs for training. This is a key advancement, since the vast majority of academic documents are not accompanied by structured abstracts. Also, such an approach makes this work applicable to any type of document, from academic articles to blog posts and financial documents. Ultimately, our proposed method allows advanced summarization methods to be used in a number of different applications that previously might have been infeasible. The rest of this work is structured as follows. Section 2 gives a brief overview of the related work. Section 3 describes in detail the proposed method. Section 4 presents the experimental setup and Section 5 discusses the results of our experiments. Finally, Section 6 concludes this works and points to future work directions."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Boiling the Information Ocean",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "R. Socher"
    },
    {
      "index": 1,
      "title": "Abstractive sentence summarization with attentive recurrent neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "S. Chopra, M. Auli, and A. M. Rush"
    },
    {
      "index": 2,
      "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond",
      "abstract": "",
      "year": "2016",
      "venue": "2016 SIGNLL Conference on Computational Natural Language Learning",
      "authors": "R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang"
    },
    {
      "index": 3,
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "abstract": "",
      "year": "2017",
      "venue": "2017 Annual Meeting of the Association for Computational Linguistics",
      "authors": "A. See, P. J. Liu, and C. D. Manning",
      "orig_title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "paper_id": "1704.04368v2"
    },
    {
      "index": 4,
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "",
      "year": "2018",
      "venue": "2018 International Conference on Learning Representations",
      "authors": "R. Paulus, C. Xiong, and R. Socher",
      "orig_title": "A deep reinforced model for abstractive summarization",
      "paper_id": "1705.04304v3"
    },
    {
      "index": 5,
      "title": "Text Summarization with Pretrained Encoders",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "authors": "Y. Liu and M. Lapata",
      "orig_title": "Text Summarization with Pretrained Encoders",
      "paper_id": "1908.08345v2"
    },
    {
      "index": 6,
      "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
      "abstract": "",
      "year": "2019",
      "venue": "2019 International Conference on Machine Learning",
      "authors": "K. Song, X. Tan, T. Qin, J. Lu, and T. Y. Liu",
      "orig_title": "MASS: Masked sequence to sequence pre-training for language generation",
      "paper_id": "1905.02450v5"
    },
    {
      "index": 7,
      "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon",
      "orig_title": "Unified language model pre-training for natural language understanding and generation",
      "paper_id": "1905.03197v3"
    },
    {
      "index": 8,
      "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Y. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou",
      "orig_title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training",
      "paper_id": "2001.04063v3"
    },
    {
      "index": 9,
      "title": "Teaching machines to read and comprehend",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom"
    },
    {
      "index": 10,
      "title": "The new york times annotated corpus",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "E. Sandhaus"
    },
    {
      "index": 11,
      "title": "Annotated gigaword",
      "abstract": "",
      "year": "2012",
      "venue": "Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction",
      "authors": "C. Napoles, M. Gormley, and B. Van Durme"
    },
    {
      "index": 12,
      "title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "M. Grusky, M. Naaman, and Y. Artzi"
    },
    {
      "index": 13,
      "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "A. Cohan, F. Dernoncourt, D. S. Kim, T. Bui, S. Kim, W. Chang, and N. Goharian",
      "orig_title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
      "paper_id": "1804.05685v2"
    },
    {
      "index": 14,
      "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "S. Subramanian, R. Li, J. Pilault, and C. Pal",
      "orig_title": "On Extractive and Abstractive Neural Document Summarizationwith Transformer Language Models",
      "paper_id": "1909.03186v2"
    },
    {
      "index": 15,
      "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Annual Meeting of the Association for Computational Linguistics",
      "authors": "Y. C. Chen and M. Bansal",
      "orig_title": "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "paper_id": "1805.11080v1"
    },
    {
      "index": 16,
      "title": "Bottom-Up Abstractive Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "2018 Conference on Empirical Methods in Natural Language Processing",
      "authors": "S. Gehrmann, Y. Deng, and A. Rush",
      "orig_title": "Bottom-Up Abstractive Summarization",
      "paper_id": "1808.10792v2"
    },
    {
      "index": 17,
      "title": "Big bird: Transformers for longer sequences",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, and L. Yang"
    },
    {
      "index": 18,
      "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu"
    },
    {
      "index": 19,
      "title": "Structured Summarization of Academic Publications",
      "abstract": "",
      "year": "2020",
      "venue": "Communications in Computer and Information Science",
      "authors": "A. Gidiotis and G. Tsoumakas",
      "orig_title": "Structured Summarization of Academic Publications",
      "paper_id": "1905.07695v2"
    },
    {
      "index": 20,
      "title": "Using latent semantic analysis in text summarization and summary evaluation",
      "abstract": "",
      "year": "2004",
      "venue": "2004 International Conference on Information System Implementation and Modeling",
      "authors": "J. Steinberger and K. Jezek"
    },
    {
      "index": 21,
      "title": "Beyond SumBasic: Task-focused summarization with sentence simplification and lexical expansion",
      "abstract": "",
      "year": "2007",
      "venue": "Information Processing & Management",
      "authors": "L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova"
    },
    {
      "index": 22,
      "title": "LexRank: Graph-based lexical centrality as salience in text summarization",
      "abstract": "",
      "year": "2004",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "G. Erkan and D. R. Radev"
    },
    {
      "index": 23,
      "title": "Query-oriented text summarization based on hypergraph transversals",
      "abstract": "",
      "year": "2019",
      "venue": "Information Processing & Management",
      "authors": "H. Van Lierde and T. W. Chow",
      "orig_title": "Query-oriented text summarization based on hypergraph transversals",
      "paper_id": "1902.00672v1"
    },
    {
      "index": 24,
      "title": "Sentence Centrality Revisited for Unsupervised Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Annual Meeting of the Association for Computational Linguistics",
      "authors": "H. Zheng and M. Lapata",
      "orig_title": "Sentence Centrality Revisited for Unsupervised Summarization",
      "paper_id": "1906.03508v1"
    },
    {
      "index": 25,
      "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis",
      "abstract": "",
      "year": "2019",
      "venue": "Information Processing & Management",
      "authors": "M. Mohamed and M. Oussalah"
    },
    {
      "index": 26,
      "title": "Sequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "I. Sutskever, O. Vinyals, and Q. V. Le"
    },
    {
      "index": 27,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2015",
      "venue": "2015 International Conference on Learning Representations",
      "authors": "D. Bahdanau, K. H. Cho, and Y. Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 28,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 29,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 30,
      "title": "Language Models are Unsupervised Multitask Learners — Enhanced Reader",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI Blog",
      "authors": "Radford Alec, Wu Jeffrey, Child Rewon, Luan David, Amodei Dario, and Sutskever Ilya"
    },
    {
      "index": 31,
      "title": "Classify or Select: Neural Architectures for Extractive Document Summarization",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "R. Nallapati, B. Zhou, and M. Ma",
      "orig_title": "Classify or select: Neural architectures for extractive document summarization",
      "paper_id": "1611.04244v1"
    },
    {
      "index": 32,
      "title": "A Supervised Approach to Extractive Summarisation of Scientific Papers",
      "abstract": "",
      "year": "2017",
      "venue": "2017 Conference on Computational Natural Language Learning",
      "authors": "E. Collins, I. Augenstein, and S. Riedel",
      "orig_title": "A Supervised Approach to Extractive Summarisation of Scientific Papers",
      "paper_id": "1706.03946v1"
    },
    {
      "index": 33,
      "title": "A neural attention model for abstractive sentence summarization",
      "abstract": "",
      "year": "2015",
      "venue": "2015 Conference on Empirical Methods in Natural Language Processing",
      "authors": "A. M. Rush, S. Chopra, and J. Weston"
    },
    {
      "index": 34,
      "title": "Deep Communicating Agents for Abstractive Summarization",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "A. Celikyilmaz, A. Bosselut, X. He, and Y. Choi",
      "orig_title": "Deep Communicating Agents for Abstractive Summarization",
      "paper_id": "1803.10357v3"
    },
    {
      "index": 35,
      "title": "Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications",
      "abstract": "",
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "R. Dangovski, L. Jing, P. Nakov, M. Tatalović, and M. Soljačić"
    },
    {
      "index": 36,
      "title": "Self-critical Sequence Training for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "2017 Conference on Computer Vision and Pattern Recognition",
      "authors": "S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel",
      "orig_title": "Self-critical sequence training for image captioning",
      "paper_id": "1612.00563v2"
    },
    {
      "index": 37,
      "title": "Deep Transfer Reinforcement Learning for Text Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "2019 SIAM International Conference on Data Mining",
      "authors": "Y. Keneshloo, N. Ramakrishnan, and C. K. Reddy",
      "orig_title": "Deep transfer reinforcement learning for text summarization",
      "paper_id": "1810.06667v2"
    },
    {
      "index": 38,
      "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "S. Narayan, S. B. Cohen, and M. Lapata",
      "orig_title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "paper_id": "1802.08636v2"
    },
    {
      "index": 39,
      "title": "Generating extractive summaries of scientific paradigms",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "V. Qazvinian, D. R. Radev, S. M. Mohammad, B. Dorr, D. Zajic, M. Whidby, and T. Moon"
    },
    {
      "index": 40,
      "title": "Scientific Article Summarization Using Citation-Context and Article’s Discourse Structure",
      "abstract": "",
      "year": "2015",
      "venue": "2015 Conference on Empirical Methods in Natural Language Processing",
      "authors": "A. Cohan and N. Goharian",
      "orig_title": "Scientific Article Summarization Using Citation-Context and Article Discourse Structure",
      "paper_id": "1704.06619v1"
    },
    {
      "index": 41,
      "title": "Scientific document summarization via citation contextualization and scientific discourse",
      "abstract": "",
      "year": "2018",
      "venue": "International Journal on Digital Libraries",
      "authors": "A. Cohan and N. Goharian"
    },
    {
      "index": 42,
      "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "R. Nallapati, F. Zhai, and B. Zhou",
      "orig_title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents.",
      "paper_id": "1611.04230v1"
    },
    {
      "index": 43,
      "title": "Overview of the CL-SciSumm 2016 Shared Task",
      "abstract": "",
      "year": "2016",
      "venue": "2016 joint workshop on Bibliometric-enhanced Information Retrieval and Natural language processing for Digital Libraries",
      "authors": "K. Jaidka, M. K. Chandrasekaran, S. Rustagi, and M. Y. Kan"
    },
    {
      "index": 44,
      "title": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks",
      "abstract": "",
      "year": "2019",
      "venue": "2019 AAAI Conference on Artificial Intelligence",
      "authors": "M. Yasunaga, J. Kasai, R. Zhang, A. R. Fabbri, I. Li, D. Friedman, and D. R. Radev"
    },
    {
      "index": 45,
      "title": "Rouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "2004 Workshop on Text Summarization Branches Out, Post Conference Workshop of ACL",
      "authors": "C.-Y. Lin"
    },
    {
      "index": 46,
      "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity",
      "abstract": "",
      "year": "1995",
      "venue": "Backpropagation: Theory, architectures, and applications",
      "authors": "R. J. Williams and D. Zipser"
    },
    {
      "index": 47,
      "title": "MultiLing 2019: Financial Narrative Summarisation",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources",
      "authors": "M. El-Haj"
    },
    {
      "index": 48,
      "title": "Pointer networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "O. Vinyals, M. Fortunato, and N. Jaitly"
    },
    {
      "index": 49,
      "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Annual Meeting of the Association for Computational Linguistics",
      "authors": "T. Kudo"
    },
    {
      "index": 50,
      "title": "Neural machine translation of rare words with subword units",
      "abstract": "",
      "year": "2016",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",
      "authors": "R. Sennrich, B. Haddow, and A. Birch"
    },
    {
      "index": 51,
      "title": "Sequence transduction with recurrent neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv",
      "authors": "A. Graves"
    },
    {
      "index": 52,
      "title": "Audio chord recognition with recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "2013 International Society for Music Information Retrieval Conference",
      "authors": "N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent"
    },
    {
      "index": 53,
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of Machine Learning Research",
      "authors": "J. Duchi, E. Hazan, and Y. Singer"
    },
    {
      "index": 54,
      "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
      "abstract": "",
      "year": "2018",
      "venue": "35th International Conference on Machine Learning, ICML 2018",
      "authors": "N. Shazeer and M. Stern",
      "orig_title": "Adafactor: Adaptive learning rates with sublinear memory cost",
      "paper_id": "1804.04235v1"
    }
  ]
}