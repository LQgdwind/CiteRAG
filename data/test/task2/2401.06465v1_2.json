{
  "paper_id": "2401.06465v1",
  "title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test",
  "sections": {
    "introduction": "The problem of evaluating the quality of an explanation method in eXplainable Artificial Intelligence (XAI) remains unsolved due to the frequent absence of ground truth explanation labels  [ref]2 . To address this issue, numerous quantitative evaluation methods have been proposed  [ref]5  [ref]2  [ref]5   0 1 2 3  4 [ref]2 5 6.\nAmong these, the Model Parameter Randomisation Test (MPRT) 5 measures the degree to which an explanation deteriorates as the model parameters are progressively randomised, i.e., layer-by-layer, starting from the output. This test posits that a greater difference in the explanation function‚Äôs output, in response to parameter randomisation, signifies a higher quality of the explanation method. In recent years, however, multiple independent research groups have provided constructive observations on the methodological choices in the original MPRT 7 8 9 [ref]20.\nThese encompass the selection of pairwise similarity for measuring explanation difference [ref]20, the order of layer randomisation [ref]20, the choice of explanation preprocessing 7 [ref]20 and the dependency on the model task 9 8. Given the considerable adoption of MPRT in the XAI community and its potential to influence the acceptance or rejection of different explanation methods, these raised concerns deserve a closer examination.\nDo evaluations based on the original MPRT optimally inform us about the quality of an explanation method, or can we improve its reliability? The foundational idea behind MPRT‚Äîthat the explanation function should be sensitive to the model‚Äôs parameters‚Äîis both insightful and important. Our work seeks to build upon this initial work, addressing its shortcomings, namely, the order of layer randomisation and the selection of pairwise similarity measures, discussed in detail in Section 2.1.1 and illustrated in Figure 1.\nWe introduce two MPRT variants, Smooth MPRT (sMPRT) and Efficient MPRT (eMPRT), where the sMPRT mitigates sensitivity to shattering noise (cf. [ref]20) by denoising, i.e., averaging attributions over NùëÅN perturbed inputs and the eMPRT evaluates explanation faithfulness by quantifying the rise in complexity after full parameter randomisation, thus obviating a biased similarity measurement. We release these methods under the existing Quantus  evaluation framework111Code at: https://github.com/understandable-machine-intelligence-lab/Quantus.. Our methods, despite being an iteration, carry substantial importance. To deploy XAI in real-world applications, such as safety-critical domains, we need to focus on developing reliable quantitative evaluation metrics for existing explanation methods. This work contributes to ensuring that the continuous development of XAI methods remains both theoretically sound and empirically valid."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Do backpropagation trained neural networks have normal weight distributions?",
      "abstract": "",
      "year": "1993",
      "venue": "ICANN ‚Äô93",
      "authors": "I. Bellido and E. Fiesler"
    },
    {
      "index": 1,
      "title": "Framework for Evaluating Faithfulness of Local Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Dasgupta, N. Frost, and M. Moshkovitz",
      "orig_title": "Framework for evaluating faithfulness of local explanations",
      "paper_id": "2202.00734v1"
    },
    {
      "index": 2,
      "title": "Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Hedstr√∂m, L. Weber, D. Krakowczyk, D. Bareeva, F. Motzkus, W. Samek, S. Lapuschkin, and M. M.-C. H√∂hne",
      "orig_title": "Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond",
      "paper_id": "2202.06861v3"
    },
    {
      "index": 3,
      "title": "Methods for interpreting and understanding deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "Digit. Signal Process.",
      "authors": "G. Montavon, W. Samek, and K. M√ºller"
    },
    {
      "index": 4,
      "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "D. Alvarez-Melis and T. S. Jaakkola",
      "orig_title": "Towards robust interpretability with self-explaining neural networks",
      "paper_id": "1806.07538v2"
    },
    {
      "index": 5,
      "title": "On the (In)fidelity and Sensitivity of Explanations",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "C. Yeh, C. Hsieh, A. S. Suggala, D. I. Inouye, and P. Ravikumar",
      "orig_title": "On the (in)fidelity and sensitivity of explanations",
      "paper_id": "1901.09392v4"
    },
    {
      "index": 6,
      "title": "Evaluating and Aggregating Feature-based Model Explanations",
      "abstract": "",
      "year": "2020",
      "venue": "International Joint Conference on Artificial Intelligence, IJCAI 2020",
      "authors": "U. Bhatt, A. Weller, and J. M. F. Moura",
      "orig_title": "Evaluating and aggregating feature-based model explanations",
      "paper_id": "2005.00631v1"
    },
    {
      "index": 7,
      "title": "Rethinking Stability for Attribution-based Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "C. Agarwal, N. Johnson, M. Pawelczyk, S. Krishna, E. Saxena, M. Zitnik, and H. Lakkaraju",
      "orig_title": "Rethinking stability for attribution-based explanations",
      "paper_id": "2203.06877v1"
    },
    {
      "index": 8,
      "title": "On quantitative aspects of model interpretability",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "A. Nguyen and M. R. Martinez",
      "orig_title": "On quantitative aspects of model interpretability",
      "paper_id": "2007.07584v1"
    },
    {
      "index": 9,
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "abstract": "",
      "year": "2015",
      "venue": "PloS one",
      "authors": "S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M√ºller, and W. Samek"
    },
    {
      "index": 10,
      "title": "Evaluating the visualization of what a Deep Neural Network has learned",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. M√ºller",
      "orig_title": "Evaluating the visualization of what a deep neural network has learned",
      "paper_id": "1509.06321v1"
    },
    {
      "index": 11,
      "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations, ICLR 2018",
      "authors": "M. Ancona, E. Ceolini, C. √ñztireli, and M. Gross",
      "orig_title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "paper_id": "1711.06104v4"
    },
    {
      "index": 12,
      "title": "IROF: a low resource evaluation metric for explanation methods",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "L. Rieger and L. K. Hansen",
      "orig_title": "IROF: a low resource evaluation metric for explanation methods",
      "paper_id": "2003.08747v1"
    },
    {
      "index": 13,
      "title": "A Consistent and Efficient Evaluation Strategy for Attribution Methods",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning, ICML 2022",
      "authors": "Y. Rong, T. Leemann, V. Borisov, G. Kasneci, and E. Kasneci",
      "orig_title": "A consistent and efficient evaluation strategy for attribution methods",
      "paper_id": "2202.00449v2"
    },
    {
      "index": 14,
      "title": "Sanity checks for saliency maps",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "J. Adebayo, J. Gilmer, M. Muelly, I. J. Goodfellow, M. Hardt, and B. Kim"
    },
    {
      "index": 15,
      "title": "When Explanations Lie: Why Many Modified BP Attributions Fail",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning, ICML 2020",
      "authors": "L. Sixt, M. Granz, and T. Landgraf",
      "orig_title": "When explanations lie: Why many modified BP attributions fail",
      "paper_id": "1912.09818v7"
    },
    {
      "index": 16,
      "title": "A Note about: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "M. Sundararajan and A. Taly",
      "orig_title": "A note about: Local explanation methods for deep neural networks lack sensitivity to parameter values",
      "paper_id": "1806.04205v1"
    },
    {
      "index": 17,
      "title": "Investigating Sanity Checks for Saliency Maps with Image and Text Classification",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "N. Kokhlikyan, V. Miglani, B. Alsallakh, M. Martin, and O. Reblitz-Richardson",
      "orig_title": "Investigating sanity checks for saliency maps with image and text classification",
      "paper_id": "2106.07475v1"
    },
    {
      "index": 18,
      "title": "Revisiting Sanity Checks for Saliency Maps",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "G. Yona and D. Greenfeld",
      "orig_title": "Revisiting sanity checks for saliency maps",
      "paper_id": "2110.14297v1"
    },
    {
      "index": 19,
      "title": "Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023",
      "authors": "A. Binder, L. Weber, S. Lapuschkin, G. Montavon, K. M√ºller, and W. Samek",
      "orig_title": "Shortcomings of top-down randomization-based sanity checks for evaluations of deep neural network explanations",
      "paper_id": "2211.12486v1"
    },
    {
      "index": 20,
      "title": "SmoothGrad: removing noise by adding noise",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "D. Smilkov, N. Thorat, B. Kim, F. B. Vi√©gas, and M. Wattenberg",
      "orig_title": "Smoothgrad: removing noise by adding noise",
      "paper_id": "1706.03825v1"
    },
    {
      "index": 21,
      "title": "Visualizing and understanding convolutional networks",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision - ECCV 2014",
      "authors": "M. D. Zeiler and R. Fergus"
    },
    {
      "index": 22,
      "title": "Axiomatic attribution for deep networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning, ICML 2017",
      "authors": "M. Sundararajan, A. Taly, and Q. Yan"
    },
    {
      "index": 23,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2020",
      "venue": "International Journal of Computer Vision",
      "authors": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 24,
      "title": "NoiseGrad ‚Äî Enhancing Explanations by Introducing Stochasticity to Model Weights",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence, AAAI 2022",
      "authors": "K. Bykov, A. Hedstr√∂m, S. Nakajima, and M. M. H√∂hne",
      "orig_title": "Noisegrad - enhancing explanations by introducing stochasticity to model weights",
      "paper_id": "2106.10185v3"
    },
    {
      "index": 25,
      "title": "Are artificial neural networks black boxes?",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. M. Benitez, J. L. Castro, and I. Requena"
    },
    {
      "index": 26,
      "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
      "abstract": "",
      "year": "2023",
      "venue": "Transactions on Machine Learning Research",
      "authors": "A. Hedstr√∂m, P. Bommer, K. K. Wickstr√∏m, W. Samek, S. Lapuschkin, and M. M. C. H√∂hne",
      "orig_title": "The meta-evaluation problem in explainable ai: Identifying reliable estimators with metaquantus",
      "paper_id": "2302.07265v2"
    },
    {
      "index": 27,
      "title": "Concise Explanations of Neural Networks using Adversarial Training",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning, ICML 2020",
      "authors": "P. Chalasani, J. Chen, A. R. Chowdhury, X. Wu, and S. Jha",
      "orig_title": "Concise explanations of neural networks using adversarial training",
      "paper_id": "1810.06583v9"
    },
    {
      "index": 28,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 29,
      "title": "Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "T. Han, S. Srinivas, and H. Lakkaraju",
      "orig_title": "Which explanation should I choose? A function approximation perspective to characterizing post hoc explanations",
      "paper_id": "2206.01254v3"
    },
    {
      "index": 30,
      "title": "Finding the right XAI method ‚Äî A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science",
      "abstract": "",
      "year": "2023",
      "venue": "CoRR",
      "authors": "P. Bommer, M. Kretschmer, A. Hedstr√∂m, D. Bareeva, and M. M. H√∂hne",
      "orig_title": "Finding the right XAI method - A guide for the evaluation and ranking of explainable AI methods in climate science",
      "paper_id": "2303.00652v2"
    },
    {
      "index": 31,
      "title": "A mathematical theory of communication",
      "abstract": "",
      "year": "1948",
      "venue": "Bell Syst. Tech. J.",
      "authors": "C. E. Shannon"
    },
    {
      "index": 32,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations, ICLR 2015",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 33,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 34,
      "title": "Visualizing the impact of feature attribution baselines",
      "abstract": "",
      "year": "2020",
      "venue": "Distill",
      "authors": "P. Sturmfels, S. Lundberg, and S.-I. Lee"
    },
    {
      "index": 35,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 36,
      "title": "Software for dataset-wide xai: From local explanations to global insights with Zennit, CoRelAy, and ViRelAy",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "C. J. Anders, D. Neumann, W. Samek, K.-R. M√ºller, and S. Lapuschkin"
    },
    {
      "index": 37,
      "title": "Captum: A unified and generic model interpretability library for PyTorch",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh, J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan, and O. Reblitz-Richardson",
      "orig_title": "Captum: A unified and generic model interpretability library for pytorch",
      "paper_id": "2009.07896v1"
    },
    {
      "index": 38,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 39,
      "title": "Mnist handwritten digit database",
      "abstract": "",
      "year": "2010",
      "venue": "ATT Labs",
      "authors": "Y. LeCun, C. Cortes, and C. Burges"
    },
    {
      "index": 40,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "H. Xiao, K. Rasul, and R. Vollgraf",
      "orig_title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 41,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "Proc. IEEE",
      "authors": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner"
    },
    {
      "index": 42,
      "title": "Visualization of neural networks using saliency maps",
      "abstract": "",
      "year": "1995",
      "venue": "International Conference on Neural Networks (ICNN‚Äô95)",
      "authors": "N. J. S. Morch, U. Kjems, L. K. Hansen, C. Svarer, I. Law, B. Lautrup, S. C. Strother, and K. Rehm"
    },
    {
      "index": 43,
      "title": "How to explain individual classification decisions",
      "abstract": "",
      "year": "2010",
      "venue": "J. Mach. Learn. Res.",
      "authors": "D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K. M√ºller"
    },
    {
      "index": 44,
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations, ICLR 2014",
      "authors": "K. Simonyan, A. Vedaldi, and A. Zisserman"
    },
    {
      "index": 45,
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "A. Shrikumar, P. Greenside, A. Shcherbina, and A. Kundaje"
    },
    {
      "index": 46,
      "title": "A Unified Approach to Interpreting Model Predictions",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30",
      "authors": "S. M. Lundberg and S. Lee",
      "orig_title": "A unified approach to interpreting model predictions",
      "paper_id": "1705.07874v2"
    },
    {
      "index": 47,
      "title": "Striving for Simplicity: The All Convolutional Net",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR (workshop track)",
      "authors": "J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller",
      "orig_title": "Striving for simplicity: The all convolutional net",
      "paper_id": "1412.6806v3"
    },
    {
      "index": 48,
      "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
      "abstract": "",
      "year": "2017",
      "venue": "Pattern Recognit.",
      "authors": "G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K. M√ºller",
      "orig_title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "paper_id": "1512.02479v1"
    },
    {
      "index": 49,
      "title": "Understanding SSIM",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "J. Nilsson and T. Akenine-M√∂ller"
    },
    {
      "index": 50,
      "title": "On the histogram as a density estimator: L2 theory",
      "abstract": "",
      "year": "1981",
      "venue": "Z. Wahrscheinlichkeitstheorie verw Gebiete",
      "authors": "D. Freedman and P. Diaconis"
    },
    {
      "index": 51,
      "title": "On optimal and data-based histograms",
      "abstract": "",
      "year": "1979",
      "venue": "Biometrika",
      "authors": "D. W. Scott"
    }
  ]
}