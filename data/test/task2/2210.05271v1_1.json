{
  "paper_id": "2210.05271v1",
  "title": "GAN you hear me? Reclaiming unconditional speech synthesis from diffusion models",
  "sections": {
    "related work": "We start by distinguishing what we call unconditional speech synthesis to the related but different task of generative spoken language modeling (GSLM).\nIn GSLM, a large autoregressive language model is typically trained on some discrete units\n(e.g.¬†HuBERT¬† clusters or clustered spectrogram features),\nsimilar to how a language model is trained on text¬†[ref]19 .\nWhile this also enables the generation of speech without any conditioning input, GSLM implies a\nmodel structure consisting of an encoder to discretize speech,\na language model, and a decoder .\nThis means that during generation, you are bound by the discrete units in the model.\nE.g.,\nit is not possible to interpolate between two utterances in a latent space\nor to directly\ncontrol speaker characteristics during generation.\nIf this is desired, additional components must be explicitly built into the model¬†. In contrast, in unconditional speech synthesis we\ndo not assume any knowledge of particular aspects of speech beforehand.\nInstead of using some intermediate discretization step, such models typically\nuse noise to directly generate speech,\noften via some latent representation.\nThe latent space should ideally be disentangled,\nallowing for modelling and control of the generated speech.\nIn contrast to GSLM, the synthesis model should learn to disentangle without being explicitly designed\nto control specific speech characteristics.\nIn some sense this is a more challenging task than GSLM, which is why most unconditional speech synthesis models are still evaluated on short utterances of isolated spoken words¬†[ref]1 (as we also do here). Within\nunconditional speech synthesis,\na substantial body of work focuses on either autoregressive  models ‚Äì generating a current sample based on previous outputs ‚Äì\nor diffusion models¬†.\nDiffusion models iteratively de-noise a\nsampled signal into a\nwaveform through a Markov chain with a constant number of steps¬†.\nAt each inference step, the original noise signal is slightly de-noised until ‚Äì in the last step ‚Äì it resembles coherent speech.\nAutoregressive and diffusion\nmodels\nare\nrelatively slow\nbecause they require\nrepeated forward passes through the\nmodel during inference. Earlier studies¬†[ref]1 [ref]10\nattempted\nto use\nGANs  for unconditional speech synthesis, which has the advantage of requiring only a single pass through the model.\nWhile\nresults showed some initial promise, performance was poor\nin terms of speech quality and\ndiversity, with the more recent diffusion models performing much better¬†.\nHowever, there have been substantial improvements in GAN-based modelling for image synthesis\nin the intervening years¬†  .\nOur goal is to\nimprove the performance of the earlier GAN-based unconditional speech synthesis models by adapting\nlessons from these recent image synthesis studies. Some of these\ninnovations in GANs\nare\nmodality-agnostic:\nR1subscriptùëÖ1R_{1} regularization  and\nexponential moving\naveraging\nof generator weights  can be directly transferred from the vision domain to speech.\nOther techniques, such as the\ncarefully designed anti-aliasing filters between layers in StyleGAN3 \nrequire specific adaptation;\nin contrast to images, there is little meaningful information in speech below 300¬†Hz,\nnecessitating a redesign of the anti-aliasing filters. In a very related research direction, Begu≈°¬† [ref]10 has been studying how GAN-based unconditional speech synthesis models internally perform lexical and phonological learning, and how this relates to human learning.\nThese studies, however, have been relying on the older GAN synthesis models.\nWe hope that by developing better performing GANs for unconditional speech synthesis, such investigations\nwill also be improved.\nRecently,  attempted to directly use StyleGAN2 for conditional and\nunconditional synthesis of emotional vocal bursts.\nThis further motivates a reinvestigation of GANs, but here we look specifically at the generation of speech rather than paralinguistic sounds."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Adversarial Audio Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Chris Donahue, Julian McAuley, and Miller Puckette",
      "orig_title": "Adversarial audio synthesis",
      "paper_id": "1802.04208v3"
    },
    {
      "index": 1,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter",
      "orig_title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 2,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 3,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, et¬†al.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 4,
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.06125",
      "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen",
      "orig_title": "Hierarchical text-conditional image generation with CLIP latents",
      "paper_id": "2204.06125v1"
    },
    {
      "index": 5,
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.11487",
      "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, et¬†al.",
      "orig_title": "Photorealistic text-to-image diffusion models with deep language understanding",
      "paper_id": "2205.11487v1"
    },
    {
      "index": 6,
      "title": "It‚Äôs Raw! Audio Generation with State-Space Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.09729",
      "authors": "Karan Goel, Albert Gu, Chris Donahue, and Christopher R√©",
      "orig_title": "It‚Äôs raw! Audio generation with state-space models",
      "paper_id": "2202.09729v1"
    },
    {
      "index": 7,
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.09761",
      "authors": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro"
    },
    {
      "index": 8,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, et¬†al."
    },
    {
      "index": 9,
      "title": "Generative Adversarial Phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Frontiers in artificial intelligence",
      "authors": "Ga≈°per Begu≈°",
      "orig_title": "Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "paper_id": "2006.03965v1"
    },
    {
      "index": 10,
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, and Timo Aila",
      "orig_title": "A style-based generator architecture for generative adversarial networks",
      "paper_id": "1812.04948v3"
    },
    {
      "index": 11,
      "title": "Analyzing and Improving the Image Quality of StyleGAN",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, et¬†al.",
      "orig_title": "Analyzing and improving the image quality of StyleGAN",
      "paper_id": "1912.04958v2"
    },
    {
      "index": 12,
      "title": "Alias-Free Generative Adversarial Networks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H√§rk√∂nen, Janne Hellsten, et¬†al.",
      "orig_title": "Alias-free generative adversarial networks",
      "paper_id": "2106.12423v4"
    },
    {
      "index": 13,
      "title": "Training Generative Adversarial Networks with Limited Data",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, et¬†al.",
      "orig_title": "Training generative adversarial networks with limited data",
      "paper_id": "2006.06676v2"
    },
    {
      "index": 14,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, et¬†al.",
      "orig_title": "Improved techniques for training GANs",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 15,
      "title": "Activation Maximization Generative Adversarial Nets",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, et¬†al.",
      "orig_title": "Activation maximization generative adversarial nets",
      "paper_id": "1703.02000v9"
    },
    {
      "index": 16,
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.03209",
      "authors": "Pete Warden"
    },
    {
      "index": 17,
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.07447",
      "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung¬†Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, et¬†al.",
      "orig_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "paper_id": "2106.07447v1"
    },
    {
      "index": 18,
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "abstract": "",
      "year": "2021",
      "venue": "Interspeech",
      "authors": "Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux"
    },
    {
      "index": 19,
      "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, et¬†al.",
      "orig_title": "Text-free prosody-aware generative spoken language modeling",
      "paper_id": "2109.03264v2"
    },
    {
      "index": 20,
      "title": "Generative spoken language modeling from raw audio",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.01192",
      "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, et¬†al."
    },
    {
      "index": 21,
      "title": "WaveNet: A Generative Model for Raw Audio",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.03499",
      "authors": "Aaron van¬†den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, et¬†al.",
      "orig_title": "WaveNet: A generative model for raw audio",
      "paper_id": "1609.03499v2"
    },
    {
      "index": 22,
      "title": "Which training methods for GANs do actually converge?",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin"
    },
    {
      "index": 23,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 24,
      "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Computer Speech & Language",
      "authors": "Ga≈°per Begu≈°",
      "orig_title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks",
      "paper_id": "2009.12711v2"
    },
    {
      "index": 25,
      "title": "Generating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms",
      "abstract": "",
      "year": "2022",
      "venue": "ICML ExVo Generate",
      "authors": "Marco Jiralerspong and Gauthier Gidel",
      "orig_title": "Generating diverse vocal bursts with StyleGAN2 and mel-spectrograms",
      "paper_id": "2206.12563v1"
    },
    {
      "index": 26,
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae",
      "orig_title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "paper_id": "2010.05646v2"
    },
    {
      "index": 27,
      "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Matthew Tancik, Pratul¬†P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, et¬†al.",
      "orig_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
      "paper_id": "2006.10739v1"
    },
    {
      "index": 28,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Saining Xie, Ross¬†B Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 29,
      "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Swaminathan Gurumurthy, Ravi Kiran¬†Sarvadevabhatla, and R¬†Venkatesh¬†Babu",
      "orig_title": "DeLiGAN: Generative adversarial networks for diverse and limited data",
      "paper_id": "1706.02071v1"
    },
    {
      "index": 30,
      "title": "The VoiceMOS Challenge 2022",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11389",
      "authors": "Wen-Chin Huang, Erica Cooper, Yu¬†Tsao, Hsin-Min Wang, Tomoki Toda, et¬†al.",
      "orig_title": "The VoiceMOS challenge 2022",
      "paper_id": "2203.11389v3"
    },
    {
      "index": 31,
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "abstract": "",
      "year": "2015",
      "venue": "ICASSP",
      "authors": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur"
    },
    {
      "index": 32,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Diederik¬†P. Kingma and Jimmy Ba"
    },
    {
      "index": 33,
      "title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Song Tao and J¬†Wang",
      "orig_title": "Alleviation of gradient exploding in GANs: Fake can be real",
      "paper_id": "1912.12485v2"
    },
    {
      "index": 34,
      "title": "Digital filters",
      "abstract": "",
      "year": "1966",
      "venue": "System analysis by digital computer",
      "authors": "James¬†F Kaiser"
    }
  ]
}