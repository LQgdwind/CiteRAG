{
  "paper_id": "2305.06061v2",
  "title": "Visual Tuning",
  "sections": {
    "model architecture": "Pre-trained foundation models for vision have been surveyed in , which develops from CNN- and GAN-based models to recent Transformer-based models. We recommend readers refer to  for the detailed pre-training strategies. This section briefly introduces these representative models‚Äô basic structures: CNN-based, Transformer-based, and CNN+Transformer. CNN is one of the most popular deep learning models such as AlexNet , VGGNet 8, Inception 9, ResNet 4, EfficientNet [ref]50, etc., which has been surveyed time to time  . EfficientNet is lightweight yet can achieve comparable performance to Transformer-based models via pre-trained initialization on various visual tasks such as image classification  and video understanding . Except for 2D CNN, a couple of 3D CNN models such as C3D , I3D , S3D , and X3D  have been introduced for video understanding tasks. In addition, temporal convolutional network  has also been proposed for tasks such as segmentation  and pose estimation  . Notably, the basic layer of the temporal convolutional network is implemented with 2D convolution while Transformer is implemented via 1D convolution in addition to the attention mechanism. The typical architecture of a Transformer model is structured with several basic Transformer layers. Each layer can be made of a varied number of Transformer blocks composed of a multi-head self-attention (Attention) module and a fully connected feed-forward network (FFN) implemented with a 2-layer multilayer perceptron (MLP). Layer normalization (LN) and residual connection are respectively performed before and after both FFN and Attention modules. One such Transformer block can be represented as: where Z^lsuperscript^ùëçùëô\\hat{Z}^{l} and ZlsuperscriptùëçùëôZ^{l} respectively indicate the output of Attention and FNN modules, and Zl‚àí1superscriptùëçùëô1Z^{l-1} denotes the output of a previous FNN module.\nBuilding upon the basic Transformer model, Transformer has been dominating increasing tasks  . Early Transformer models for vision are Vision Transformer (ViT) , Data-efficient image transformer\n(DeiT) , while their representative variations are TNT , T2T , PVT , Swin-Vit , Video Swin Transformer , CPVT . Transformer models are well known for their ability to capture long-range dependencies of input data. Whereas CNNs might be better at representing local features. Models combining Transformer and CNN can achieve better performance.\nTwins-SVT  used 2D convolutional to calculate the attention module, leading to improved performance on image-based tasks with considerably more model parameters. Representative methods combining CNN and Transformer are Shuffle ,\nCMT ,\nVOLO , etc. Although they can achieve superior performance, how they can be used for visual tuning seems under-explored."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "A Survey on Visual Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Han, K. et al.",
      "orig_title": "A survey on vision transformer",
      "paper_id": "2012.12556v6"
    },
    {
      "index": 2,
      "title": "Transformers in Vision: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "Khan, S. et al.",
      "orig_title": "Transformers in vision: A survey",
      "paper_id": "2101.01169v5"
    },
    {
      "index": 3,
      "title": "On the opportunities and risks of foundation models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Bommasani, R. et al."
    },
    {
      "index": 4,
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zhou, C. et al."
    },
    {
      "index": 5,
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wu, C. et al.",
      "orig_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "paper_id": "2303.04671v1"
    },
    {
      "index": 6,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Brown, T. et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 7,
      "title": "Efficient Transformers: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "authors": "Tay, Y., Dehghani, M., Bahri, D. & Metzler, D.",
      "orig_title": "Efficient transformers: A survey",
      "paper_id": "2009.06732v3"
    },
    {
      "index": 8,
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, P. et al."
    },
    {
      "index": 9,
      "title": "Meta Pseudo Labels",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Pham, H., Dai, Z., Xie, Q. & Le, Q. V.",
      "orig_title": "Meta pseudo labels",
      "paper_id": "2003.10580v4"
    },
    {
      "index": 10,
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yu, J. et al.",
      "orig_title": "Coca: Contrastive captioners are image-text foundation models",
      "paper_id": "2205.01917v2"
    },
    {
      "index": 11,
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Dehghani, M. et al.",
      "orig_title": "Scaling vision transformers to 22 billion parameters",
      "paper_id": "2302.05442v1"
    },
    {
      "index": 12,
      "title": "Palm-e: An embodied multimodal language model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.03378",
      "authors": "Driess, D. et al."
    },
    {
      "index": 13,
      "title": "A roadmap for big model",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yuan, S. et al."
    },
    {
      "index": 14,
      "title": "A Comprehensive Survey on Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE",
      "authors": "Zhuang, F. et al.",
      "orig_title": "A comprehensive survey on transfer learning",
      "paper_id": "1911.02685v3"
    },
    {
      "index": 15,
      "title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Sung, Y.-L., Cho, J. & Bansal, M.",
      "orig_title": "Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks",
      "paper_id": "2112.06825v2"
    },
    {
      "index": 16,
      "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "authors": "Lin, Z., Madotto, A. & Fung, P.",
      "orig_title": "Exploring versatile generative language model via parameter-efficient transfer learning",
      "paper_id": "2004.03829v2"
    },
    {
      "index": 17,
      "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning",
      "abstract": "",
      "year": "2022",
      "venue": "NIPS 2022",
      "authors": "Sung, Y.-L., Cho, J. & Bansal, M.",
      "orig_title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning",
      "paper_id": "2206.06522v2"
    },
    {
      "index": 18,
      "title": "Pca-sift: A more distinctive representation for local image descriptors",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "Ke, Y. & Sukthankar, R."
    },
    {
      "index": 19,
      "title": "Brief: Binary robust independent elementary features",
      "abstract": "",
      "year": "2010",
      "venue": "Computer Vision‚ÄìECCV 2010: 11th European Conference on Computer Vision",
      "authors": "Calonder, M., Lepetit, V., Strecha, C. & Fua, P."
    },
    {
      "index": 20,
      "title": "Orb: An efficient alternative to sift or surf",
      "abstract": "",
      "year": "2011",
      "venue": "2011 International conference on computer vision",
      "authors": "Rublee, E., Rabaud, V., Konolige, K. & Bradski, G."
    },
    {
      "index": 21,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "LeCun, Y., Bengio, Y. & Hinton, G.",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 22,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Deng, J. et al."
    },
    {
      "index": 23,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Szegedy, C. et al.",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 24,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "He, K., Zhang, X., Ren, S. & Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 25,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Communications of the ACM",
      "authors": "Krizhevsky, A., Sutskever, I. & Hinton, G. E."
    },
    {
      "index": 26,
      "title": "A Survey on Deep Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on artificial neural networks",
      "authors": "Tan, C. et al.",
      "orig_title": "A survey on deep transfer learning",
      "paper_id": "1808.01974v1"
    },
    {
      "index": 27,
      "title": "Learning to learn",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Thrun, S. & Pratt, L."
    },
    {
      "index": 28,
      "title": "The Kinetics Human Action Video Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Kay, W. et al.",
      "orig_title": "The kinetics human action video dataset",
      "paper_id": "1705.06950v1"
    },
    {
      "index": 29,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Vaswani, A. et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 30,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Radford, A. et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 31,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Jia, C. et al."
    },
    {
      "index": 32,
      "title": "A Generalist Agent",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Reed, S. et al.",
      "orig_title": "A generalist agent",
      "paper_id": "2205.06175v3"
    },
    {
      "index": 33,
      "title": "FLAVA: A Foundational Language And Vision Alignment Model",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Singh, A. et al.",
      "orig_title": "Flava: A foundational language and vision alignment model",
      "paper_id": "2112.04482v3"
    },
    {
      "index": 34,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Alayrac, J.-B. et al."
    },
    {
      "index": 35,
      "title": "Revisiting Weakly Supervised Pre-Training of Visual Perception Models",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Singh, M. et al.",
      "orig_title": "Revisiting weakly supervised pre-training of visual perception models",
      "paper_id": "2201.08371v2"
    },
    {
      "index": 36,
      "title": "Receptive fields of single neurones in the cat‚Äôs striate cortex",
      "abstract": "",
      "year": "1959",
      "venue": "The Journal of physiology",
      "authors": "Hubel, D. H. & Wiesel, T. N."
    },
    {
      "index": 37,
      "title": "Backpropagation and the brain",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Reviews Neuroscience",
      "authors": "Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. & Hinton, G."
    },
    {
      "index": 38,
      "title": "Atoms of recognition in human and computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the National Academy of Sciences",
      "authors": "Ullman, S., Assif, L., Fetaya, E. & Harari, D."
    },
    {
      "index": 39,
      "title": "Vision: A computational investigation into the human representation and processing of visual information",
      "abstract": "",
      "year": "2010",
      "venue": "MIT press",
      "authors": "Marr, D."
    },
    {
      "index": 40,
      "title": "The ecological approach to visual perception: classic edition",
      "abstract": "",
      "year": "2014",
      "venue": "Psychology press",
      "authors": "Gibson, J. J."
    },
    {
      "index": 41,
      "title": "Human-level concept learning through probabilistic program induction",
      "abstract": "",
      "year": "2015",
      "venue": "Science",
      "authors": "Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B."
    },
    {
      "index": 42,
      "title": "Ten Lessons We Have Learned in the New ‚ÄúSparseland‚Äù: A Short Handbook for Sparse Neural Network Researchers",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Liu, S. & Wang, Z.",
      "orig_title": "Ten lessons we have learned in the new‚Äù sparseland‚Äù: A short handbook for sparse neural network researchers",
      "paper_id": "2302.02596v3"
    },
    {
      "index": 43,
      "title": "A Modern Introduction to Probability and Statistics: Understanding why and how",
      "abstract": "",
      "year": "2005",
      "venue": "Springer",
      "authors": "Dekking, F. M., Kraaikamp, C., Lopuha√§, H. P. & Meester, L. E."
    },
    {
      "index": 44,
      "title": "Towards a Theoretical Framework of Out-of-Distribution Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ye, H. et al.",
      "orig_title": "Towards a theoretical framework of out-of-distribution generalization",
      "paper_id": "2106.04496v3"
    },
    {
      "index": 45,
      "title": "Who is closer: A computational method for domain gap evaluation",
      "abstract": "",
      "year": "2022",
      "venue": "Pattern Recognition",
      "authors": "Liu, X. & Zhang, S."
    },
    {
      "index": 46,
      "title": "Bridging Theory and Algorithm for Domain Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Zhang, Y., Liu, T., Long, M. & Jordan, M.",
      "orig_title": "Bridging theory and algorithm for domain adaptation",
      "paper_id": "1904.05801v2"
    },
    {
      "index": 47,
      "title": "On the Theory of Transfer Learning: The Importance of Task Diversity",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tripuraneni, N., Jordan, M. & Jin, C.",
      "orig_title": "On the theory of transfer learning: The importance of task diversity",
      "paper_id": "2006.11650v2"
    },
    {
      "index": 48,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Simonyan, K. & Zisserman, A.",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 49,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z."
    },
    {
      "index": 50,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Tan, M. & Le, Q.",
      "orig_title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 51,
      "title": "2017 international conference on engineering and technology (ICET), 1‚Äì6 (Ieee, 2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Albawi, S., Mohammed, T.¬†A. &\nAl-Zawi, S. Understanding of a\nconvolutional neural network."
    },
    {
      "index": 52,
      "title": "A survey of convolutional neural networks: analysis, applications, and prospects",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE transactions on neural networks and\nlearning systems (",
      "authors": "Li, Z., Liu, F., Yang,\nW., Peng, S. & Zhou, J."
    },
    {
      "index": 53,
      "title": "K. et al. Incorporating convolution designs into visual transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Yuan, K. et¬†al. Incorporating\nconvolution designs into visual transformers."
    },
    {
      "index": 54,
      "title": "Mmnet: A model-based multimodal network for human action recognition in rgb-d videos",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Bruce, X., Liu, Y., Zhang,\nX., Zhong, S.-h. & Chan, K.¬†C."
    },
    {
      "index": 55,
      "title": "M. Learning spatiotemporal features with 3d convolutional networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Tran, D., Bourdev, L.,\nFergus, R., Torresani, L. &\nPaluri, M. Learning spatiotemporal\nfeatures with 3d convolutional networks."
    },
    {
      "index": 56,
      "title": "action recognition? a new model and the kinetics dataset",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Carreira, J. & Zisserman, A.\nQuo vadis, action recognition? a new model and the\nkinetics dataset."
    },
    {
      "index": 57,
      "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Xie, S., Sun, C., Huang,\nJ., Tu, Z. & Murphy, K.\nRethinking spatiotemporal feature learning:\nSpeed-accuracy trade-offs in video classification.",
      "orig_title": "K. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
      "paper_id": "1712.04851v2"
    },
    {
      "index": 58,
      "title": "C. X3d: Expanding architectures for efficient video recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Feichtenhofer, C. X3d: Expanding\narchitectures for efficient video recognition."
    },
    {
      "index": 59,
      "title": "Computer Vision‚ÄìECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, 47‚Äì54 (Springer, 2016)",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Lea, C., Vidal, R.,\nReiter, A. & Hager, G.¬†D.\nTemporal convolutional networks: A unified approach to\naction segmentation."
    },
    {
      "index": 60,
      "title": "Temporal Convolutional Networks for Action Segmentation and Detection",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Lea, C., Flynn, M.¬†D.,\nVidal, R., Reiter, A. &\nHager, G.¬†D. Temporal convolutional\nnetworks for action segmentation and detection.",
      "orig_title": "Temporal convolutional networks for action segmentation and detection",
      "paper_id": "1611.05267v1"
    },
    {
      "index": 61,
      "title": "3D human pose estimation in video with temporal convolutions and semi-supervised training",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Pavllo, D., Feichtenhofer, C.,\nGrangier, D. & Auli, M.\n3d human pose estimation in video with temporal\nconvolutions and semi-supervised training.",
      "orig_title": "M. 3d human pose estimation in video with temporal convolutions and semi-supervised training",
      "paper_id": "1811.11742v2"
    },
    {
      "index": 62,
      "title": "R. et al. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, R. et¬†al. Attention\nmechanism exploits temporal contexts: Real-time 3d human pose\nreconstruction."
    },
    {
      "index": 63,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Dosovitskiy, A. et¬†al. An image\nis worth 16x16 words: Transformers for image recognition at scale."
    },
    {
      "index": 64,
      "title": "International conference on machine learning, 10347‚Äì10357 (PMLR, 2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Touvron, H. et¬†al. Training\ndata-efficient image transformers & distillation through attention."
    },
    {
      "index": 65,
      "title": "Transformer in Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 15908‚Äì15919\n(",
      "authors": "Han, K. et¬†al.",
      "orig_title": "Transformer in transformer",
      "paper_id": "2103.00112v3"
    },
    {
      "index": 66,
      "title": "L. et al. Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Yuan, L. et¬†al. Tokens-to-token\nvit: Training vision transformers from scratch on imagenet."
    },
    {
      "index": 67,
      "title": "W. et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wang, W. et¬†al. Pyramid vision\ntransformer: A versatile backbone for dense prediction without\nconvolutions."
    },
    {
      "index": 68,
      "title": "Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, Z. et¬†al. Swin transformer:\nHierarchical vision transformer using shifted windows."
    },
    {
      "index": 69,
      "title": "Z. et al. Video swin transformer",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Liu, Z. et¬†al. Video swin\ntransformer."
    },
    {
      "index": 70,
      "title": "Conditional Positional Encodings for Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chu, X. et¬†al.",
      "orig_title": "Conditional positional encodings for vision transformers",
      "paper_id": "2102.10882v3"
    },
    {
      "index": 71,
      "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 9355‚Äì9366\n(",
      "authors": "Chu, X. et¬†al.",
      "orig_title": "Twins: Revisiting the design of spatial attention in vision transformers",
      "paper_id": "2104.13840v4"
    },
    {
      "index": 72,
      "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Huang, Z. et¬†al.",
      "orig_title": "Shuffle transformer: Rethinking spatial shuffle for vision transformer",
      "paper_id": "2106.03650v1"
    },
    {
      "index": 73,
      "title": "J. et al. Cmt: Convolutional neural networks meet vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Guo, J. et¬†al. Cmt:\nConvolutional neural networks meet vision transformers."
    },
    {
      "index": 74,
      "title": "Volo: Vision outlooker for visual recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Yuan, L., Hou, Q., Jiang,\nZ., Feng, J. & Yan, S."
    },
    {
      "index": 75,
      "title": "A. Revisiting unreasonable effectiveness of data in deep learning era",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Sun, C., Shrivastava, A.,\nSingh, S. & Gupta, A.\nRevisiting unreasonable effectiveness of data in deep\nlearning era."
    },
    {
      "index": 76,
      "title": "K. et al. Masked autoencoders are scalable vision learners",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "He, K. et¬†al. Masked\nautoencoders are scalable vision learners."
    },
    {
      "index": 77,
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tong, Z., Song, Y., Wang,\nJ. & Wang, L.",
      "orig_title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "paper_id": "2203.12602v3"
    },
    {
      "index": 78,
      "title": "Florence: A New Foundation Model for Computer Vision",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yuan, L. et¬†al.",
      "orig_title": "Florence: A new foundation model for computer vision",
      "paper_id": "2111.11432v1"
    },
    {
      "index": 79,
      "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, W. et¬†al.",
      "orig_title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "paper_id": "2208.10442v2"
    },
    {
      "index": 80,
      "title": "ClimaX: A foundation model for weather and climate",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Nguyen, T., Brandstetter, J.,\nKapoor, A., Gupta, J.¬†K. &\nGrover, A.",
      "orig_title": "Climax: A foundation model for weather and climate",
      "paper_id": "2301.10343v5"
    },
    {
      "index": 81,
      "title": "Language models generalize beyond natural proteins",
      "abstract": "",
      "year": "2022",
      "venue": "bioRxiv 2022‚Äì12\n(",
      "authors": "Verkuil, R. et¬†al."
    },
    {
      "index": 82,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXXIII, 709‚Äì727 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jia, M. et¬†al. Visual prompt\ntuning."
    },
    {
      "index": 83,
      "title": "S-Prompts Learning with Pre-trained Transformers: An Occam‚Äôs Razor for Domain Incremental Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, Y., Huang, Z. &\nHong, X.",
      "orig_title": "S-prompts learning with pre-trained transformers: An occam‚Äôs razor for domain incremental learning",
      "paper_id": "2207.12819v2"
    },
    {
      "index": 84,
      "title": "Visual Prompt Tuning For Test-time Domain Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, Y. et¬†al.",
      "orig_title": "Visual prompt tuning for test-time domain adaptation",
      "paper_id": "2210.04831v2"
    },
    {
      "index": 85,
      "title": "ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhou, Z., Zhang, B., Lei,\nY., Liu, L. & Liu, Y.",
      "orig_title": "Zegclip: Towards adapting clip for zero-shot semantic segmentation",
      "paper_id": "2212.03588v3"
    },
    {
      "index": 86,
      "title": "Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Dong, R. et¬†al."
    },
    {
      "index": 87,
      "title": "PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Herzig, R. et¬†al.",
      "orig_title": "Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data",
      "paper_id": "2212.04821v3"
    },
    {
      "index": 88,
      "title": "Understanding Zero-shot Adversarial Robustness for Large-Scale Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Mao, C., Geng, S., Yang,\nJ., Wang, X. & Vondrick, C.",
      "orig_title": "Understanding zero-shot adversarial robustness for large-scale models",
      "paper_id": "2212.07016v2"
    },
    {
      "index": 89,
      "title": "Unleashing the Power of Visual Prompting At the Pixel Level",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wu, J. et¬†al.",
      "orig_title": "Unleashing the power of visual prompting at the pixel level",
      "paper_id": "2212.10556v2"
    },
    {
      "index": 90,
      "title": "ProSFDA: Prompt Learning based Source-free Domain Adaptation for Medical Image Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hu, S., Liao, Z. & Xia,\nY.",
      "orig_title": "Prosfda: Prompt learning based source-free domain adaptation for medical image segmentation",
      "paper_id": "2211.11514v1"
    },
    {
      "index": 91,
      "title": "\\\\\\backslasha-la-carte prompt tuning (apt): Combining distinct data via composable prompting",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bowman, B. et¬†al."
    },
    {
      "index": 92,
      "title": "Prompting and tuning: A two-stage unsupervised domain adaptive person re-identification method on vision transformer backbone",
      "abstract": "",
      "year": "2023",
      "venue": "Tsinghua Science and Technology\n28, 799‚Äì810\n(",
      "authors": "Yu, S., Dou, Z. & Wang,\nS."
    },
    {
      "index": 93,
      "title": "LPT: Long-tailed Prompt Tuning for Image Classification",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Dong, B., Zhou, P., Yan,\nS. & Zuo, W.",
      "orig_title": "Lpt: Long-tailed prompt tuning for image classification",
      "paper_id": "2210.01033v2"
    },
    {
      "index": 94,
      "title": "R. et al. Pointclip: Point cloud understanding by clip",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhang, R. et¬†al. Pointclip:\nPoint cloud understanding by clip."
    },
    {
      "index": 95,
      "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, Z., Yu, X., Rao,\nY., Zhou, J. & Lu, J.",
      "orig_title": "P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting",
      "paper_id": "2208.02812v2"
    },
    {
      "index": 96,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Wu, C.¬†H., Motamed, S.,\nSrivastava, S. & De¬†la Torre, F.\nGenerative visual prompt: Unifying distributional\ncontrol of pre-trained generative models."
    },
    {
      "index": 97,
      "title": "Neural Prompt Search",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Zhou, K. &\nLiu, Z.",
      "orig_title": "Neural prompt search",
      "paper_id": "2206.04673v2"
    },
    {
      "index": 98,
      "title": "Prompt generation networks for efficient adaptation of frozen vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Loedeman, J., Stol, M.¬†C.,\nHan, T. & Asano, Y.¬†M."
    },
    {
      "index": 99,
      "title": "Feature-Proxy Transformer for Few-Shot Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, J.-W., Sun, Y.,\nYang, Y. & Chen, W.",
      "orig_title": "Feature-proxy transformer for few-shot segmentation",
      "paper_id": "2210.06908v1"
    },
    {
      "index": 100,
      "title": "Fine-grained retrieval prompt tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, S. et¬†al."
    },
    {
      "index": 101,
      "title": "Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, K., Chen, L., Zhang,\nH., Xiao, J. & Sun, Q.",
      "orig_title": "Compositional prompt tuning with motion cues for open-vocabulary video relation detection",
      "paper_id": "2302.00268v1"
    },
    {
      "index": 102,
      "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gu, X., Lin, T.-Y., Kuo,\nW. & Cui, Y.",
      "orig_title": "Open-vocabulary object detection via vision and language knowledge distillation",
      "paper_id": "2104.13921v3"
    },
    {
      "index": 103,
      "title": "LION: Implicit Vision Prompt Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, H. et¬†al.",
      "orig_title": "Lion: Implicit vision prompt tuning",
      "paper_id": "2303.09992v3"
    },
    {
      "index": 104,
      "title": "Learning to prompt for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "International Journal of Computer Vision\n130, 2337‚Äì2348\n(",
      "authors": "Zhou, K., Yang, J., Loy,\nC.¬†C. & Liu, Z."
    },
    {
      "index": 105,
      "title": "Understanding and mitigating overfitting in prompt tuning for vision-language models",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for\nVideo Technology (",
      "authors": "Ma, C. et¬†al."
    },
    {
      "index": 106,
      "title": "Multi-prompt alignment for multi-source unsupervised domain adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, H., Wu, Z. & Jiang,\nY.-G."
    },
    {
      "index": 107,
      "title": "Zegot: Zero-shot segmentation through optimal transport of text prompts",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kim, K., Oh, Y. & Ye,\nJ.¬†C."
    },
    {
      "index": 108,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part IV, 1‚Äì18 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ni, B. et¬†al. Expanding\nlanguage-image pretrained models for general video recognition."
    },
    {
      "index": 109,
      "title": "Prompt-aligned Gradient for Prompt Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhu, B., Niu, Y., Han,\nY., Wu, Y. & Zhang, H.",
      "orig_title": "Prompt-aligned gradient for prompt tuning",
      "paper_id": "2205.14865v3"
    },
    {
      "index": 110,
      "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Berg, H. et¬†al.",
      "orig_title": "A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning",
      "paper_id": "2203.11933v4"
    },
    {
      "index": 111,
      "title": "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Fei, H., Li,\nD., Yu, T. & Li, P.",
      "orig_title": "Prompting through prototype: A prototype-based prompt learning on pretrained vision-language models",
      "paper_id": "2210.10841v1"
    },
    {
      "index": 112,
      "title": "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Park, J. et¬†al.",
      "orig_title": "Lanit: Language-driven image-to-image translation for unlabeled data",
      "paper_id": "2208.14889v4"
    },
    {
      "index": 113,
      "title": "SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for Few-shot Image Classification",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Peng, F., Yang, X. & Xu,\nC.",
      "orig_title": "Sgva-clip: Semantic-guided visual adapting of vision-language models for few-shot image classification",
      "paper_id": "2211.16191v2"
    },
    {
      "index": 114,
      "title": "Language-aware soft prompting for vision & language foundation models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bulat, A. & Tzimiropoulos, G."
    },
    {
      "index": 115,
      "title": "DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Sun, X., Hu, P. & Saenko,\nK.",
      "orig_title": "Dualcoop: Fast adaptation to multi-label recognition with limited annotations",
      "paper_id": "2206.09541v1"
    },
    {
      "index": 116,
      "title": "Prompt learning with optimal transport for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, G. et¬†al."
    },
    {
      "index": 117,
      "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "He, X. et¬†al.",
      "orig_title": "Cpl: Counterfactual prompt learning for vision and language models",
      "paper_id": "2210.10362v3"
    },
    {
      "index": 118,
      "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, F. et¬†al.",
      "orig_title": "Learning to decompose visual features with latent textual prompts",
      "paper_id": "2210.04287v1"
    },
    {
      "index": 119,
      "title": "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tao, M., Bao, B.-K., Tang,\nH. & Xu, C.",
      "orig_title": "Galip: Generative adversarial clips for text-to-image synthesis",
      "paper_id": "2301.12959v1"
    },
    {
      "index": 120,
      "title": "Z. Conditional prompt learning for vision-language models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhou, K., Yang, J., Loy,\nC.¬†C. & Liu, Z. Conditional prompt\nlearning for vision-language models."
    },
    {
      "index": 121,
      "title": "Pointclip v2: Adapting clip for powerful 3d open-world learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhu, X. et¬†al."
    },
    {
      "index": 122,
      "title": "Unified Vision and Language Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zang, Y., Li, W., Zhou,\nK., Huang, C. & Loy, C.¬†C.",
      "orig_title": "Unified vision and language prompt learning",
      "paper_id": "2210.07225v1"
    },
    {
      "index": 123,
      "title": "Class-aware visual prompt tuning for vision-language pre-trained model",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xing, Y. et¬†al."
    },
    {
      "index": 124,
      "title": "MaPLe: Multi-modal Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Khattak, M.¬†U., Rasheed, H.,\nMaaz, M., Khan, S. &\nKhan, F.¬†S.",
      "orig_title": "Maple: Multi-modal prompt learning",
      "paper_id": "2210.03117v3"
    },
    {
      "index": 125,
      "title": "Multitask Vision-Language Prompt Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shen, S. et¬†al.",
      "orig_title": "Multitask vision-language prompt tuning",
      "paper_id": "2211.11720v3"
    },
    {
      "index": 126,
      "title": "Learning Domain Invariant Prompt for Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhao, C. et¬†al.",
      "orig_title": "Learning domain invariant prompt for vision-language models",
      "paper_id": "2212.04196v2"
    },
    {
      "index": 127,
      "title": "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shu, M. et¬†al.",
      "orig_title": "Test-time prompt tuning for zero-shot generalization in vision-language models",
      "paper_id": "2209.07511v1"
    },
    {
      "index": 128,
      "title": "Advances in neural information processing systems (2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Rebuffi, A., Bilen, H. &\nVedaldi, A. Learning multiple visual\ndomains with residual adapters."
    },
    {
      "index": 129,
      "title": "A. Efficient parametrization of multi-domain deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Rebuffi, S.-A., Bilen, H. &\nVedaldi, A. Efficient parametrization\nof multi-domain deep neural networks."
    },
    {
      "index": 130,
      "title": "Incremental Learning Through Deep Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 42,\n651‚Äì663 (",
      "authors": "Rosenfeld, A. & Tsotsos, J.¬†K.",
      "orig_title": "Incremental learning through deep adaptation",
      "paper_id": "1705.04228v2"
    },
    {
      "index": 131,
      "title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, H. et¬†al.",
      "orig_title": "Conv-adapter: Exploring parameter efficient transfer learning for convnets",
      "paper_id": "2208.07463v4"
    },
    {
      "index": 132,
      "title": "Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liu, Y.-C., Ma, C.-Y.,\nTian, J., He, Z. &\nKira, Z."
    },
    {
      "index": 133,
      "title": "Pro-tuning: Unified Prompt Tuning for Vision Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Nie, X. et¬†al.",
      "orig_title": "Pro-tuning: Unified prompt tuning for vision tasks",
      "paper_id": "2207.14381v3"
    },
    {
      "index": 134,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXI, 50‚Äì67 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Rao, Y., Zhao, W., Zhou,\nJ. & Lu, J. Amixer: Adaptive weight\nmixing for self-attention free vision transformers."
    },
    {
      "index": 135,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=9aokcgBVIj1",
      "authors": "Shysheya, A., Bronskill, J.¬†F.,\nPatacchiola, M., Nowozin, S. &\nTurner, R.¬†E. Fit: Parameter\nefficient few-shot transfer learning for personalized and federated image\nclassification."
    },
    {
      "index": 136,
      "title": "Tiny adapters for vision transformers (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=V0Vo9eW2nzL",
      "authors": "MAROUF, I.¬†E., Tartaglione, E. &\nLathuili√®re, S."
    },
    {
      "index": 137,
      "title": "Towards Efficient Visual Adaption via Structural Re-parameterization",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Luo, G. et¬†al.",
      "orig_title": "Towards efficient visual adaption via structural re-parameterization",
      "paper_id": "2302.08106v2"
    },
    {
      "index": 138,
      "title": "Benchmarking Detection Transfer Learning with Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Li, Y. et¬†al.",
      "orig_title": "Benchmarking detection transfer learning with vision transformers",
      "paper_id": "2111.11429v1"
    },
    {
      "index": 139,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part IX, 280‚Äì296 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Li, Y., Mao, H., Girshick,\nR. & He, K. Exploring plain vision\ntransformer backbones for object detection."
    },
    {
      "index": 140,
      "title": "Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XIII 16, 446‚Äì462 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wang, H. et¬†al. Stacking\nnetworks dynamically for image restoration based on the plug-and-play\nframework."
    },
    {
      "index": 141,
      "title": "Collaboration of Pre-trained Models Makes Better Few-shot Learner",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, R. et¬†al.",
      "orig_title": "Collaboration of pre-trained models makes better few-shot learner",
      "paper_id": "2209.12255v2"
    },
    {
      "index": 142,
      "title": "C. Continual learning with transformers for image classification",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ermis, B., Zappella, G.,\nWistuba, M., Rawal, A. &\nArchambeau, C. Continual learning\nwith transformers for image classification."
    },
    {
      "index": 143,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=CIoSZ_HKHS7",
      "authors": "Yang, T. et¬†al. AIM: Adapting\nimage models for efficient video action recognition."
    },
    {
      "index": 144,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Pan, J., Lin, Z., Zhu,\nX., Shao, J. & Li, H.\nSt-adapter: Parameter-efficient image-to-video transfer\nlearning."
    },
    {
      "index": 145,
      "title": "International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=5IND3TXJRb-",
      "authors": "Sharma, M. et¬†al. Lossless\nadaptation of pretrained vision models for robotic manipulation."
    },
    {
      "index": 146,
      "title": "Universal Deep Image Compression via Content-Adaptive Optimization with Adapters",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Tsubota, K., Akutsu, H. &\nAizawa, K. Universal deep image\ncompression via content-adaptive optimization with adapters.",
      "orig_title": "K. Universal deep image compression via content-adaptive optimization with adapters",
      "paper_id": "2211.00918v1"
    },
    {
      "index": 147,
      "title": "How to adapt your large-scale vision-and-language model",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://openreview.net/forum?id=EhwEUb2ynIa (",
      "authors": "Kim, K., Laskin, M.,\nMordatch, I. & Pathak, D."
    },
    {
      "index": 148,
      "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gao, P. et¬†al.",
      "orig_title": "Clip-adapter: Better vision-language models with feature adapters",
      "paper_id": "2110.04544v2"
    },
    {
      "index": 149,
      "title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, R. et¬†al.",
      "orig_title": "Tip-adapter: Training-free clip-adapter for better vision-language modeling",
      "paper_id": "2111.03930v2"
    },
    {
      "index": 150,
      "title": "A Simple Long-Tailed Recognition Baseline via Vision-Language Model",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Ma, T. et¬†al.",
      "orig_title": "A simple long-tailed recognition baseline via vision-language model",
      "paper_id": "2111.14745v1"
    },
    {
      "index": 151,
      "title": "Magma‚Äìmultimodal augmentation of generative models through adapter-based finetuning",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Eichenberg, C., Black, S.,\nWeinbach, S., Parcalabescu, L. &\nFrank, A."
    },
    {
      "index": 152,
      "title": "Hierarchical3D Adapters for Long Video-to-text Summarization",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Papalampidi, P. & Lapata, M.",
      "orig_title": "Hierarchical3d adapters for long video-to-text summarization",
      "paper_id": "2210.04829v1"
    },
    {
      "index": 153,
      "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Z. et¬†al.",
      "orig_title": "Hyperpelt: Unified parameter-efficient language model tuning for both language and vision-and-language tasks",
      "paper_id": "2203.03878v1"
    },
    {
      "index": 154,
      "title": "Svl-adapter: Self-supervised adapter for vision-language pretrained models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Pantazis, O., Brostow, G.,\nJones, K. & Mac¬†Aodha, O."
    },
    {
      "index": 155,
      "title": "Vision Transformers are Parameter-Efficient Audio-Visual Learners",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Lin, Y.-B., Sung, Y.-L.,\nLei, J., Bansal, M. &\nBertasius, G.",
      "orig_title": "Vision transformers are parameter-efficient audio-visual learners",
      "paper_id": "2212.07983v2"
    },
    {
      "index": 156,
      "title": "Cross-Modal Adapter for Text-Video Retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jiang, H. et¬†al.",
      "orig_title": "Cross-modal adapter for text-video retrieval",
      "paper_id": "2211.09623v1"
    },
    {
      "index": 157,
      "title": "Multimodal Video Adapter for Parameter Efficient Video Text Retrieval",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, B. et¬†al.",
      "orig_title": "Multimodal video adapter for parameter efficient video text retrieval",
      "paper_id": "2301.07868v2"
    },
    {
      "index": 158,
      "title": "Vision Transformer Adapter for Dense Predictions",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, Z. et¬†al.",
      "orig_title": "Vision transformer adapter for dense predictions",
      "paper_id": "2205.08534v4"
    },
    {
      "index": 159,
      "title": "Parameter-Efficient and Student-Friendly Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Rao, J., Meng, X., Ding,\nL., Qi, S. & Tao, D.",
      "orig_title": "Parameter-efficient and student-friendly knowledge distillation",
      "paper_id": "2205.15308v1"
    },
    {
      "index": 160,
      "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Chen, S. et¬†al.",
      "orig_title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
      "paper_id": "2205.13535v3"
    },
    {
      "index": 161,
      "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jie, S. & Deng, Z.-H.",
      "orig_title": "Convolutional bypasses are better vision transformer adapters",
      "paper_id": "2207.07039v3"
    },
    {
      "index": 162,
      "title": "Rethinking vision transformer and masked autoencoder in multimodal face anti-spoofing",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yu, Z. et¬†al."
    },
    {
      "index": 163,
      "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Lu, H. et¬†al.",
      "orig_title": "Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling",
      "paper_id": "2302.06605v2"
    },
    {
      "index": 164,
      "title": "The Eleventh International Conference on Learning Representations (2023)",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=J_Cja7cpgW",
      "authors": "Hao, T., Chen, H., Guo,\nY. & Ding, G. Consolidator:\nMergable adapter with group connections for visual adaptation."
    },
    {
      "index": 165,
      "title": "Exploring Efficient Few-shot Adaptation for Vision Transformers",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xu, C. et¬†al.",
      "orig_title": "Exploring efficient few-shot adaptation for vision transformers",
      "paper_id": "2301.02419v1"
    },
    {
      "index": 166,
      "title": "Parameter-efficient fine-tuning for vision transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "He, X., Li, C., Zhang,\nP., Yang, J. & Wang, X.¬†E."
    },
    {
      "index": 167,
      "title": "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Valipour, M., Rezagholizadeh, M.,\nKobyzev, I. & Ghodsi, A."
    },
    {
      "index": 168,
      "title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Shimomoto, E.¬†K. et¬†al.",
      "orig_title": "Towards parameter-efficient integration of pre-trained language models in temporal video grounding",
      "paper_id": "2209.13359v2"
    },
    {
      "index": 169,
      "title": "Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Tu, C.-H., Mai, Z. &\nChao, W.-L.",
      "orig_title": "Visual query tuning: Towards effective usage of intermediate representations for parameter and memory efficient transfer learning",
      "paper_id": "2212.03220v2"
    },
    {
      "index": 170,
      "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zaken, E.¬†B., Goldberg, Y. &\nRavfogel, S. Bitfit: Simple\nparameter-efficient fine-tuning for transformer-based masked\nlanguage-models.",
      "orig_title": "S. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "paper_id": "2106.10199v5"
    },
    {
      "index": 171,
      "title": "Side Adapter Network for Open-Vocabulary Semantic Segmentation",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Xu, M., Zhang, Z., Wei,\nF., Hu, H. & Bai, X.",
      "orig_title": "Side adapter network for open-vocabulary semantic segmentation",
      "paper_id": "2302.12242v2"
    },
    {
      "index": 172,
      "title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Fu, C.-L., Chen, Z.-C.,\nLee, Y.-R. & Lee, H.-y.",
      "orig_title": "Adapterbias: Parameter-efficient token-dependent representation shift for adapters in nlp tasks",
      "paper_id": "2205.00305v4"
    },
    {
      "index": 173,
      "title": "Differentially private bias-term only fine-tuning of foundation models",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Bu, Z., Wang, Y.-X., Zha,\nS. & Karypis, G."
    },
    {
      "index": 174,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "URL https://openreview.net/forum?id=nZeVKeeFYf9",
      "authors": "Hu, E.¬†J. et¬†al. LoRA:\nLow-rank adaptation of large language models."
    },
    {
      "index": 175,
      "title": "Motion Style Transfer: Modular Low-Rank Adaptation for Deep Motion Forecasting",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kothari, P., Li, D., Liu,\nY. & Alahi, A.",
      "orig_title": "Motion style transfer: Modular low-rank adaptation for deep motion forecasting",
      "paper_id": "2211.03165v1"
    },
    {
      "index": 176,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XX, 239‚Äì256 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jiang, Z. et¬†al. Dna: Improving\nfew-shot transfer learning with low-rank decomposition and alignment."
    },
    {
      "index": 177,
      "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34, 1022‚Äì1035\n(",
      "authors": "Karimi¬†Mahabadi, R., Henderson, J. &\nRuder, S.",
      "orig_title": "Compacter: Efficient low-rank hypercomplex adapter layers",
      "paper_id": "2106.04647v2"
    },
    {
      "index": 178,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, A. et¬†al. Beyond\nfully-connected layers with quaternions: Parameterization of hypercomplex\nmultiplications with 1/n1ùëõ1/n parameters."
    },
    {
      "index": 179,
      "title": "PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and\nLearning Systems (",
      "authors": "Grassucci, E., Zhang, A. &\nComminiello, D.",
      "orig_title": "Phnns: Lightweight neural networks via parameterized hypercomplex convolutions",
      "paper_id": "2110.04176v2"
    },
    {
      "index": 180,
      "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Transactions of the Association for\nComputational Linguistics 10",
      "authors": "Hou, Z., Salazar, J. &\nPolovets, G.",
      "orig_title": "Meta-learning the difference: Preparing large language models for efficient adaptation",
      "paper_id": "2207.03509v1"
    },
    {
      "index": 181,
      "title": "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Jie, S. & Deng, Z.-H.",
      "orig_title": "Fact: Factor-tuning for lightweight adaptation on vision transformer",
      "paper_id": "2212.03145v2"
    },
    {
      "index": 182,
      "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Edalati, A. et¬†al.",
      "orig_title": "Krona: Parameter efficient tuning with kronecker adapter",
      "paper_id": "2212.10650v1"
    },
    {
      "index": 183,
      "title": "Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Li, T. et¬†al."
    },
    {
      "index": 184,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing\nSystems (",
      "authors": "Lian, D., Daquan, Z.,\nFeng, J. & Wang, X.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 185,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hinton, G., Vinyals, O. &\nDean, J.",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 186,
      "title": "International Conference on Learning Representations (2015)",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Romero, A. et¬†al. Fitnets: Hints\nfor thin deep nets."
    },
    {
      "index": 187,
      "title": "Learning Student Networks via Feature Embedding",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and\nLearning Systems 32, 25‚Äì35\n(",
      "authors": "Chen, H., Wang, Y., Xu,\nC., Xu, C. & Tao, D.",
      "orig_title": "Learning student networks via feature embedding",
      "paper_id": "1812.06597v1"
    },
    {
      "index": 188,
      "title": "Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XVII 16, 469‚Äì484 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Guan, Y. et¬†al. Differentiable\nfeature aggregation search for knowledge distillation."
    },
    {
      "index": 189,
      "title": "Knowledge distillation via adaptive instance normalization",
      "abstract": "",
      "year": "2020",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yang, J., Martinez, B.,\nBulat, A. & Tzimiropoulos, G.",
      "orig_title": "Knowledge distillation via adaptive instance normalization",
      "paper_id": "2003.04289v1"
    },
    {
      "index": 190,
      "title": "Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XXV 16, 664‚Äì680 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Xu, K., Rui, L., Li, Y.\n& Gu, L. Feature normalized\nknowledge distillation for image classification."
    },
    {
      "index": 191,
      "title": "Heterogeneous Knowledge Distillation using Information Flow Modeling",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Passalis, N., Tzelepi, M. &\nTefas, A. Heterogeneous knowledge\ndistillation using information flow modeling.",
      "orig_title": "A. Heterogeneous knowledge distillation using information flow modeling",
      "paper_id": "2005.00727v1"
    },
    {
      "index": 192,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Hao, Z. et¬†al. Learning\nefficient vision transformers via fine-grained manifold distillation."
    },
    {
      "index": 193,
      "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing\nsystems 31 (",
      "authors": "Kim, J., Park, S. & Kwak,\nN.",
      "orig_title": "Paraphrasing complex network: Network compression via factor transfer",
      "paper_id": "1802.04977v3"
    },
    {
      "index": 194,
      "title": "Relational Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Park, W., Kim, D., Lu, Y.\n& Cho, M. Relational knowledge\ndistillation.",
      "orig_title": "M. Relational knowledge distillation",
      "paper_id": "1904.05068v2"
    },
    {
      "index": 195,
      "title": "Y. et al. Search to distill: Pearls are everywhere but not the eyes",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, Y. et¬†al. Search to\ndistill: Pearls are everywhere but not the eyes."
    },
    {
      "index": 196,
      "title": "D. et al. Cross-layer distillation with semantic calibration",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chen, D. et¬†al. Cross-layer\ndistillation with semantic calibration."
    },
    {
      "index": 197,
      "title": "S. et al. Distilling holistic knowledge with graph neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhou, S. et¬†al. Distilling\nholistic knowledge with graph neural networks."
    },
    {
      "index": 198,
      "title": "Distilling Knowledge via Knowledge Review",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chen, P., Liu, S., Zhao,\nH. & Jia, J. Distilling knowledge\nvia knowledge review.",
      "orig_title": "J. Distilling knowledge via knowledge review",
      "paper_id": "2104.09044v1"
    },
    {
      "index": 199,
      "title": "Decoupled Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhao, B., Cui, Q., Song,\nR., Qiu, Y. & Liang, J.\nDecoupled knowledge distillation.",
      "orig_title": "J. Decoupled knowledge distillation",
      "paper_id": "2203.08679v2"
    },
    {
      "index": 200,
      "title": "International Conference on Learning Representations (2016)",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Chen, T., Goodfellow, I. &\nShlens, J. Net2net: Accelerating\nlearning via knowledge transfer."
    },
    {
      "index": 201,
      "title": "J. Efficient architecture search by network transformation",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Cai, H., Chen, T., Zhang,\nW., Yu, Y. & Wang, J.\nEfficient architecture search by network\ntransformation."
    },
    {
      "index": 202,
      "title": "International Conference on Learning Representations (2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Ashok, A., Rhinehart, N.,\nBeainy, F. & Kitani, K.¬†M.\nN2n learning: Network to network compression via policy\ngradient reinforcement learning."
    },
    {
      "index": 203,
      "title": "International Conference on Learning Representations, Workshop Track Proceedings (2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Elsken, T., Metzen, J.-H. &\nHutter, F. Simple and efficient\narchitecture search for convolutional neural networks."
    },
    {
      "index": 204,
      "title": "International Conference on Machine Learning, 678‚Äì687 (PMLR, 2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Cai, H., Yang, J., Zhang,\nW., Han, S. & Yu, Y.\nPath-level network transformation for efficient\narchitecture search."
    },
    {
      "index": 205,
      "title": "International Conference on Learning Representations (2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Fang, J. et¬†al. Fast neural\nnetwork adaptation via parameter remapping and architecture search."
    },
    {
      "index": 206,
      "title": "FNA++: Fast Network Adaptation via Parameter Remapping and Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence 43",
      "authors": "Fang, J. et¬†al.",
      "orig_title": "Fna++: Fast network adaptation via parameter remapping and architecture search",
      "paper_id": "2006.12986v2"
    },
    {
      "index": 207,
      "title": "International Conference on Learning Representations (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, H., Simonyan, K. &\nYang, Y. Darts: Differentiable\narchitecture search."
    },
    {
      "index": 208,
      "title": "Advances in neural information processing systems, 874‚Äì884 (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chang, J. et¬†al. DATA:\ndifferentiable architecture approximation."
    },
    {
      "index": 209,
      "title": "DATA: differentiable architecture approximation with distribution guided sampling",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.\n43, 2905‚Äì2920\n(",
      "authors": "Zhang, X. et¬†al."
    },
    {
      "index": 210,
      "title": "Q. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chen, X., Xie, L., Wu, J.\n& Tian, Q. Progressive\ndifferentiable architecture search: Bridging the depth gap between search and\nevaluation."
    },
    {
      "index": 211,
      "title": "DARTS+: Improved Differentiable Architecture Search with Early Stopping",
      "abstract": "",
      "year": "2019",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liang, H. et¬†al.",
      "orig_title": "Darts+: Improved differentiable architecture search with early stopping",
      "paper_id": "1909.06035v2"
    },
    {
      "index": 212,
      "title": "G. et al. Sgas: Sequential greedy architecture search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Li, G. et¬†al. Sgas: Sequential\ngreedy architecture search."
    },
    {
      "index": 213,
      "title": "International Conference on Learning Representations (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Xie, S., Zheng, H., Liu,\nC. & Lin, L. Snas: stochastic\nneural architecture search."
    },
    {
      "index": 214,
      "title": "MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "He, C., Ye, H., Shen, L.\n& Zhang, T. Milenas: Efficient\nneural architecture search via mixed-level reformulation.",
      "orig_title": "T. Milenas: Efficient neural architecture search via mixed-level reformulation",
      "paper_id": "2003.12238v1"
    },
    {
      "index": 215,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chu, X. et¬†al. Darts-: robustly\nstepping out of performance collapse without indicators."
    },
    {
      "index": 216,
      "title": "St-adapter: Parameter-efficient image-to-video transfer learning for action recognition",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Pan, J., Lin, Z., Zhu,\nX., Shao, J. & Li, H."
    },
    {
      "index": 217,
      "title": "International conference on machine learning, 1691‚Äì1703 (PMLR, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Chen, M. et¬†al. Generative\npretraining from pixels."
    },
    {
      "index": 218,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Li, C. et¬†al. Efficient\nself-supervised vision transformers for representation learning."
    },
    {
      "index": 219,
      "title": "International Conference on Learning Representations (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "URL https://openreview.net/forum?id=0RDcd5Axok",
      "authors": "He, J., Zhou, C., Ma, X.,\nBerg-Kirkpatrick, T. & Neubig, G.\nTowards a unified view of parameter-efficient transfer\nlearning."
    },
    {
      "index": 220,
      "title": "Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740‚Äì755 (Springer, 2014)",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Lin, T.-Y. et¬†al. Microsoft\ncoco: Common objects in context."
    },
    {
      "index": 221,
      "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 42",
      "authors": "Liu, J. et¬†al.",
      "orig_title": "Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding",
      "paper_id": "1905.04757v2"
    },
    {
      "index": 222,
      "title": "V. & Sminchisescu",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 36",
      "authors": "Ionescu, C., Papava, D.,\nOlaru, V. & Sminchisescu, C."
    },
    {
      "index": 223,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Kumar, A., Raghunathan, A.,\nJones, R.¬†M., Ma, T. &\nLiang, P. Fine-tuning can distort\npretrained features and underperform out-of-distribution."
    },
    {
      "index": 224,
      "title": "T. et al. Else-net: Elastic semantic network for continual action recognition from skeleton data",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Li, T. et¬†al. Else-net: Elastic\nsemantic network for continual action recognition from skeleton data."
    },
    {
      "index": 225,
      "title": "Visual prompt tuning for generative transfer learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Sohn, K. et¬†al."
    },
    {
      "index": 226,
      "title": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Kim, M., Kim, H.-I. & Ro,\nY.¬†M.",
      "orig_title": "Prompt tuning of deep neural networks for speaker-adaptive visual speech recognition",
      "paper_id": "2302.08102v2"
    },
    {
      "index": 227,
      "title": "Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Gan, Y. et¬†al.",
      "orig_title": "Decorate the newcomers: Visual domain prompt for continual test time adaptation",
      "paper_id": "2212.04145v2"
    },
    {
      "index": 228,
      "title": "J. et al. Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Lin, J. et¬†al. Being comes from\nnot-being: Open-vocabulary text-to-motion generation with wordless\ntraining."
    },
    {
      "index": 229,
      "title": "Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXXV, 105‚Äì124 (Springer, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ju, C., Han, T., Zheng,\nK., Zhang, Y. & Xie, W.\nPrompting visual-language models for efficient video\nunderstanding."
    },
    {
      "index": 230,
      "title": "Prefix conditioning unifies language and label supervision",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Saito, K. et¬†al."
    },
    {
      "index": 231,
      "title": "International Conference on Machine Learning, 2790‚Äì2799 (PMLR, 2019)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Houlsby, N. et¬†al.\nParameter-efficient transfer learning for nlp."
    },
    {
      "index": 232,
      "title": "L. et al. Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Liu, L. et¬†al. Cross-modal\ncollaborative representation learning and a large-scale rgbt benchmark for\ncrowd counting."
    },
    {
      "index": 233,
      "title": "A Survey on Deep Hashing Methods",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Transactions on Knowledge Discovery from\nData 17, 1‚Äì50\n(",
      "authors": "Luo, X. et¬†al.",
      "orig_title": "A survey on deep hashing methods",
      "paper_id": "2003.03369v5"
    },
    {
      "index": 234,
      "title": "Prompt-Matched Semantic Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Liu, L., Yu, B.¬†X., Chang,\nJ., Tian, Q. & Chen, C.-W.",
      "orig_title": "Prompt-matched semantic segmentation",
      "paper_id": "2208.10159v3"
    },
    {
      "index": 235,
      "title": "Towards a unified view on visual parameter-efficient transfer learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Yu, B.¬†X., Chang, J., Liu,\nL., Tian, Q. & Chen, C.¬†W."
    },
    {
      "index": 236,
      "title": "Pruning adapters with lottery ticket",
      "abstract": "",
      "year": "2022",
      "venue": "Algorithms 15,\n63 (",
      "authors": "Wu, J. & Chen, Q."
    },
    {
      "index": 237,
      "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "abstract": "",
      "year": "2023",
      "venue": "Nature Machine Intelligence\n1‚Äì16 (",
      "authors": "Ding, N. et¬†al."
    },
    {
      "index": 238,
      "title": "Role of Bias Terms in Dot-Product Attention",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Namazifar, M., Hazarika, D. &\nHakkani-Tur, D.",
      "orig_title": "Role of bias terms in dot-product attention",
      "paper_id": "2302.08626v1"
    },
    {
      "index": 239,
      "title": "S.-W. Attribute injection for pretrained language models: A new benchmark and an efficient method",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Amplayo, R.¬†K., Yoo, K.¬†M. &\nLee, S.-W. Attribute injection for\npretrained language models: A new benchmark and an efficient method."
    },
    {
      "index": 240,
      "title": "A multilinear singular value decomposition",
      "abstract": "",
      "year": "2000",
      "venue": "SIAM journal on Matrix Analysis and\nApplications 21, 1253‚Äì1278\n(",
      "authors": "De¬†Lathauwer, L., De¬†Moor, B. &\nVandewalle, J."
    },
    {
      "index": 241,
      "title": "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing\n31, 3386‚Äì3398\n(",
      "authors": "Luo, G. et¬†al.",
      "orig_title": "Towards lightweight transformer via group-wise transformation for vision-and-language tasks",
      "paper_id": "2204.07780v1"
    },
    {
      "index": 242,
      "title": "Unipelt: A unified framework for parameter-efficient language model tuning",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Mao, Y. et¬†al."
    },
    {
      "index": 243,
      "title": "Advances in Neural Information Processing Systems (2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jiang, Z. et¬†al. Back razor:\nMemory-efficient transfer learning by self-sparsified backpropagation."
    },
    {
      "index": 244,
      "title": "Low-Rank Winograd Transformation for 3D Convolutional Neural Networks",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Qin, Z., Lin, M. & Lin,\nW.",
      "orig_title": "Low-rank winograd transformation for 3d convolutional neural networks",
      "paper_id": "2301.11180v1"
    },
    {
      "index": 245,
      "title": "S. Fast algorithms for convolutional neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Lavin, A. & Gray, S.\nFast algorithms for convolutional neural networks."
    },
    {
      "index": 246,
      "title": "Compressing Deep Convolutional Networks using Vector Quantization",
      "abstract": "",
      "year": "2014",
      "venue": "Preprint at https://arxiv.org/abs/1412.6115\n(",
      "authors": "Gong, Y., Liu, L., Yang,\nM. & Bourdev, L.",
      "orig_title": "Compressing deep convolutional networks using vector quantization",
      "paper_id": "1412.6115v1"
    },
    {
      "index": 247,
      "title": "Learning Structured Sparsity in Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing\nsystems 29 (",
      "authors": "Wen, W., Wu, C., Wang,\nY., Chen, Y. & Li, H.",
      "orig_title": "Learning structured sparsity in deep neural networks",
      "paper_id": "1608.03665v4"
    },
    {
      "index": 248,
      "title": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4783‚Äì4787 (IEEE, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ma, Z. et¬†al. Dhwp: Learning\nhigh-quality short hash codes via weight pruning."
    },
    {
      "index": 249,
      "title": "R. & Niculescu-Mizil",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Bucilu«é, C., Caruana, R. &\nNiculescu-Mizil, A. Model\ncompression."
    },
    {
      "index": 250,
      "title": "Do deep nets really need to be deep?",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing\nsystems 27 (",
      "authors": "Ba, J. & Caruana, R."
    },
    {
      "index": 251,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence 35",
      "authors": "Bengio, Y., Courville, A. &\nVincent, P."
    },
    {
      "index": 252,
      "title": "Y. et al. Knowledge distillation via instance relationship graph",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, Y. et¬†al. Knowledge\ndistillation via instance relationship graph."
    },
    {
      "index": 253,
      "title": "B. et al. Correlation congruence for knowledge distillation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Peng, B. et¬†al. Correlation\ncongruence for knowledge distillation."
    },
    {
      "index": 254,
      "title": "J. Mimicking very efficient network for object detection",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Li, Q., Jin, S. & Yan,\nJ. Mimicking very efficient network for object\ndetection."
    },
    {
      "index": 255,
      "title": "J. et al. Distilling object detectors via decoupled features",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Guo, J. et¬†al. Distilling object\ndetectors via decoupled features."
    },
    {
      "index": 256,
      "title": "Z. et al. Localization distillation for dense object detection",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zheng, Z. et¬†al. Localization\ndistillation for dense object detection."
    },
    {
      "index": 257,
      "title": "International Conference on Learning Representations (2021)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, L. & Ma, K.\nImprove object detection with feature-based knowledge\ndistillation: Towards accurate and efficient detectors."
    },
    {
      "index": 258,
      "title": "Y. et al. Structured knowledge distillation for semantic segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Liu, Y. et¬†al. Structured\nknowledge distillation for semantic segmentation."
    },
    {
      "index": 259,
      "title": "Structured knowledge distillation for dense prediction",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and\nmachine intelligence (",
      "authors": "Liu, Y., Shu, C., Wang,\nJ. & Shen, C."
    },
    {
      "index": 260,
      "title": "Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part VII 16, 346‚Äì362 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wang, Y., Zhou, W., Jiang,\nT., Bai, X. & Xu, Y.\nIntra-class feature variation distillation for semantic\nsegmentation."
    },
    {
      "index": 261,
      "title": "C. et al. Cross-image relational knowledge distillation for semantic segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yang, C. et¬†al. Cross-image\nrelational knowledge distillation for semantic segmentation."
    },
    {
      "index": 262,
      "title": "Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part X 16, 93‚Äì110 (Springer, 2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Porrello, A., Bergamini, L. &\nCalderara, S. Robust\nre-identification by multiple views knowledge distillation."
    },
    {
      "index": 263,
      "title": "2022 IEEE International Conference on Image Processing (ICIP), 3853‚Äì3557 (IEEE, 2022)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Remigereau, F. et¬†al. Knowledge\ndistillation for multi-target domain adaptation in real-time person\nre-identification."
    },
    {
      "index": 264,
      "title": "Y. et al. Data-free knowledge distillation for image super-resolution",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhang, Y. et¬†al. Data-free\nknowledge distillation for image super-resolution."
    },
    {
      "index": 265,
      "title": "Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Angarano, S., Salvetti, F.,\nMartini, M. & Chiaberge, M.",
      "orig_title": "Generative adversarial super-resolution at the edge with knowledge distillation",
      "paper_id": "2209.03355v2"
    },
    {
      "index": 266,
      "title": "Boosting light-weight depth estimation via knowledge distillation",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Hu, J. et¬†al."
    },
    {
      "index": 267,
      "title": "Z. Knowledge distillation for fast and accurate monocular depth estimation on mobile devices",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wang, Y., Li, X., Shi,\nM., Xian, K. & Cao, Z.\nKnowledge distillation for fast and accurate monocular\ndepth estimation on mobile devices."
    },
    {
      "index": 268,
      "title": "L. et al. Efficient crowd counting via structured knowledge transfer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, L. et¬†al. Efficient crowd\ncounting via structured knowledge transfer."
    },
    {
      "index": 269,
      "title": "Bidirectional recurrent neural networks",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE transactions on Signal Processing\n45, 2673‚Äì2681\n(",
      "authors": "Schuster, M. & Paliwal, K.¬†K."
    },
    {
      "index": 270,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Russell, S.¬†J.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 271,
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Sandler, M., Howard, A.,\nZhu, M., Zhmoginov, A. &\nChen, L.-C. Mobilenetv2: Inverted\nresiduals and linear bottlenecks.",
      "orig_title": "L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks",
      "paper_id": "1801.04381v4"
    },
    {
      "index": 272,
      "title": "International conference on machine learning, 4095‚Äì4104 (PMLR, 2018)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Pham, H., Guan, M., Zoph,\nB., Le, Q. & Dean, J.\nEfficient neural architecture search via parameters\nsharing."
    },
    {
      "index": 273,
      "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chu, X., Zhang, B. & Xu,\nR. Fairnas: Rethinking evaluation fairness of weight\nsharing neural architecture search.",
      "orig_title": "R. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
      "paper_id": "1907.01845v5"
    },
    {
      "index": 274,
      "title": "International Conference on Learning Representations (2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Xu, Y. et¬†al. Pc-darts: Partial\nchannel connections for memory-efficient architecture search."
    },
    {
      "index": 275,
      "title": "A Survey on Multi-Task Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data\nEngineering (",
      "authors": "Zhang, Y. & Yang, Q.",
      "orig_title": "A survey on multi-task learning",
      "paper_id": "1707.08114v3"
    },
    {
      "index": 276,
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM\n65, 99‚Äì106\n(",
      "authors": "Mildenhall, B. et¬†al.",
      "orig_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
      "paper_id": "2003.08934v2"
    },
    {
      "index": 277,
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Rombach, R., Blattmann, A.,\nLorenz, D., Esser, P. &\nOmmer, B. High-resolution image\nsynthesis with latent diffusion models.",
      "orig_title": "B. High-resolution image synthesis with latent diffusion models",
      "paper_id": "2112.10752v2"
    },
    {
      "index": 278,
      "title": "The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Chen, T., Zhang, Z.,\nCheng, Y., Awadallah, A. &\nWang, Z. The principle of diversity:\nTraining stronger vision transformers calls for reducing all levels of\nredundancy.",
      "orig_title": "Z. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy",
      "paper_id": "2203.06345v1"
    },
    {
      "index": 279,
      "title": "Scaling and assessment of data quality",
      "abstract": "",
      "year": "2006",
      "venue": "Acta Crystallographica Section D: Biological\nCrystallography 62, 72‚Äì82\n(",
      "authors": "Evans, P."
    },
    {
      "index": 280,
      "title": "H. Data security and privacy protection issues in cloud computing",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Chen, D. & Zhao, H.\nData security and privacy protection issues in cloud\ncomputing."
    },
    {
      "index": 281,
      "title": "Data security and privacy in cloud computing",
      "abstract": "",
      "year": "2014",
      "venue": "International Journal of Distributed Sensor\nNetworks 10, 190903\n(",
      "authors": "Sun, Y., Zhang, J., Xiong,\nY. & Zhu, G."
    },
    {
      "index": 282,
      "title": "Secure, privacy-preserving and federated machine learning in medical imaging",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Machine Intelligence\n2, 305‚Äì311\n(",
      "authors": "Kaissis, G.¬†A., Makowski, M.¬†R.,\nR√ºckert, D. & Braren, R.¬†F."
    },
    {
      "index": 283,
      "title": "Recent advances of continual learning in computer vision: An overview",
      "abstract": "",
      "year": "2021",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Qu, H., Rahmani, H., Xu,\nL., Williams, B. & Liu, J."
    },
    {
      "index": 284,
      "title": "Adversarial interference and its mitigations in privacy-preserving collaborative machine learning",
      "abstract": "",
      "year": "2021",
      "venue": "Nature Machine Intelligence\n3, 749‚Äì758\n(",
      "authors": "Usynin, D. et¬†al."
    },
    {
      "index": 285,
      "title": "Nvidia hopper h100 gpu: Scaling performance",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Micro (",
      "authors": "Choquette, J."
    },
    {
      "index": 286,
      "title": "Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges",
      "abstract": "",
      "year": "2021",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery e",
      "authors": "Bischl, B. et¬†al.",
      "orig_title": "Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges",
      "paper_id": "2107.05847v3"
    },
    {
      "index": 287,
      "title": "Weight-sharing neural architecture search: A battle to shrink the optimization gap",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)\n54, 1‚Äì37 (",
      "authors": "Xie, L. et¬†al."
    },
    {
      "index": 288,
      "title": "Dynamic Neural Networks: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and\nMachine Intelligence (",
      "authors": "Han, Y. et¬†al.",
      "orig_title": "Dynamic neural networks: A survey",
      "paper_id": "2102.04906v4"
    },
    {
      "index": 289,
      "title": "Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction",
      "abstract": "",
      "year": "2022",
      "venue": "North American Chapter of the Association for\nComputational Linguistics (",
      "authors": "Chen, X. et¬†al.",
      "orig_title": "Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction",
      "paper_id": "2205.03521v1"
    },
    {
      "index": 290,
      "title": "Exploring a large-scale multi-modal transportation recommendation system",
      "abstract": "",
      "year": "2021",
      "venue": "Transportation Research Part C: Emerging\nTechnologies 126, 103070\n(",
      "authors": "Liu, Y., Lyu, C., Liu, Z.\n& Cao, J."
    },
    {
      "index": 291,
      "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Wang, X., Wang, W., Cao,\nY., Shen, C. & Huang, T.",
      "orig_title": "Images speak in images: A generalist painter for in-context visual learning",
      "paper_id": "2212.02499v2"
    },
    {
      "index": 292,
      "title": "What Makes Good Examples for Visual In-Context Learning?",
      "abstract": "",
      "year": "2023",
      "venue": "Preprint at\nhttps://arxiv.org/abs/",
      "authors": "Zhang, Y., Zhou, K. &\nLiu, Z.",
      "orig_title": "What makes good examples for visual in-context learning?",
      "paper_id": "2301.13670v2"
    },
    {
      "index": 293,
      "title": "Chatgpt for robotics: Design principles and model abilities",
      "abstract": "",
      "year": "2023",
      "venue": "Microsoft (",
      "authors": "Vemprala, S., Bonatti, R.,\nBucker, A. & Kapoor, A."
    }
  ]
}