{
  "paper_id": "2003.01200v4",
  "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
  "sections": {
    "v-e2 neural machine translation": "It was after the success of the neural network in image classification tasks that researchers started to use neural networks in machine translation (NMT).\nAround 2013, research groups started to achieve breakthrough results in NMT.\nUnlike traditional statistical machine translation, NMT is based on an end-to-end neural networkÂ . This implies that there is no need for extensive preprocessing and word alignments. Instead, the focus shifted toward network structure. Fig.Â 11 shows an example of an end-to-end recurrent neural network for machine translation.\nA sequence of input tokens is fed into the network.\nOnce it reaches an end-of-sentence (EOS) token, it starts generating the output sequence.\nThe output sequence is generated in the same recurrent manner as the input sequence until it reaches an end-of-sentence token.\nOne major advantage of this approach is that there is no need to specify the length of the sequence; the network takes it into account automatically.\nIn other words, the end-of-sentence token determines the length of the sequence.\nNetworks implicitly learn that longer input sentences usually lead to longer output sentences with varying length, and that ordering can change.\nFor instance, the second example in Fig.Â 9 shows that adjectives generally come before nouns in English but after nouns in Spanish.\nThere is no need to explicitly specify this since the network can capture such properties.\nMoreover, the amount of memory that is used by NMT is just a fraction of the memory that is used in traditional statistical machine translationÂ .  was one of the early works that incorporated recurrent neural networks for machine translation. They were able to achieve a perplexity (a measure where lower values indicate better models) that was 43% less than the state-of-the-art alignment based translation models.\nTheir recurrent continuous translation model (RCTM) is able to capture word ordering, syntax, and meaning of the source sentence explicitly.\nIt maps a source sentence into a probability distribution over sentences in the target language. RCTM estimates the probability Pâ€‹(f|e)ğ‘ƒconditionalğ‘“ğ‘’P(f|e) of translating a sentence e=e1+â€¦+ekğ‘’subscriptğ‘’1â€¦subscriptğ‘’ğ‘˜e=e_{1}+...+e_{k} in the source language to target language sentence f=f1+â€¦+fmğ‘“subscriptğ‘“1â€¦subscriptğ‘“ğ‘šf=f_{1}+...+f_{m}.\nRCTM estimates Pâ€‹(f|e)ğ‘ƒconditionalğ‘“ğ‘’P(f|e) by considering source sentence eğ‘’e as well as the preceding words in the target language f1:iâˆ’1subscriptğ‘“:1ğ‘–1f_{1:i-1}: The representation generated by RCTM acts on n-grams in the lower layers, and acts more on the whole sentence as one moves to the upper layers.\nThis hierarchical representation is performed by applying different layers of convolution.\nFirst a continuous representation of each word is generated; i.e., if the sentence is e=e1â€‹â€¦â€‹ekğ‘’subscriptğ‘’1â€¦subscriptğ‘’ğ‘˜e=e_{1}...e_{k}, the representation of the word eisubscriptğ‘’ğ‘–e_{i} will be vâ€‹(ei)âˆˆâ„qÃ—1ğ‘£subscriptğ‘’ğ‘–superscriptâ„ğ‘1v(e_{i})\\in\\mathbb{R}^{q\\times 1}.\nThis will result in sentence matrix Eeâˆˆâ„qÃ—ksuperscriptEğ‘’superscriptâ„ğ‘ğ‘˜\\textbf{E}^{e}\\in\\mathbb{R}^{q\\times{k}} in which E:,ie=vâ€‹(ei)superscriptsubscriptE:ğ‘–ğ‘’ğ‘£subscriptğ‘’ğ‘–\\textbf{E}_{:,i}^{e}=v(e_{i}).\nThis matrix representation of the sentence will be fed into a series of convolution layers in order to generate the final representation e for the recurrent neural network. The approach is illustrated in Fig.Â 10.\nEquations for the pipeline are as follows. In order to take into account the sentence length, the authors introduced RCTM II which estimates the length of the target sentence.\nRCTM II was able to achieve better perplexity on WMT datasets (see top portion of TableÂ I) than other existing machine translation systems. In another line of work, Â [ref]170 presented an end-to-end sequence learning approach without heavy assumptions on the structure of the sequence.\nTheir approach consists of two LSTMs, one for mapping the input to a vector of fixed dimension and another LSTM for decoding the output sequence from the vector.\nTheir model was able to handle long sentences as well as sentence representations that are sensitive to word order.\nAs shown in Fig.Â 11, the model reads â€ABCâ€ as an input sequence and produces â€WXYZâ€ as output sequence.\nThe <Eâ€‹Oâ€‹S>expectationğ¸ğ‘‚ğ‘†<EOS> token indicates the end of prediction.\nThe network was trained by maximizing the log probability of the translation (Î·ğœ‚\\eta) given the input sequence (Î¶ğœ\\zeta).\nIn other words, the objective function is: ğ’Ÿğ’Ÿ\\mathcal{D} is the training set and |ğ’Ÿ|ğ’Ÿ|\\mathcal{D}| is its size.\nOne of the novelties of their approach was reversing word order of the source sentence.\nThis helps the LSTM to learn long term dependencies. Having a fixed-length vector in the decoder phase is one of the bottlenecks of the encoder-decoder approach. Â  argues that a network will have a hard time compressing all the information from the input sentence into a fixed-size vector.\nThey address this by allowing the network to search segments of the source sentence that are useful for predicting the translation.\nInstead of representing the input sentence as a fixed-size vector, in  the input sentence is encoded to a sequence of vectors and a subset of them is chosen by using a method called attention mechanism as shown in Fig.Â 12. In their approach Pâ€‹(yi|y1,â€¦,yiâˆ’1,X)=gâ€‹(yiâˆ’1,si,ci)ğ‘ƒconditionalsubscriptğ‘¦ğ‘–subscriptğ‘¦1â€¦subscriptğ‘¦ğ‘–1ğ‘‹ğ‘”subscriptğ‘¦ğ‘–1subscriptğ‘ ğ‘–subscriptğ‘ğ‘–P(y_{i}|y_{1},...,y_{i-1},X)=g(y_{i-1},s_{i},c_{i}), in which si=fâ€‹(siâˆ’1,yiâˆ’1,ci)subscriptğ‘ ğ‘–ğ‘“subscriptğ‘ ğ‘–1subscriptğ‘¦ğ‘–1subscriptğ‘ğ‘–s_{i}=f(s_{i-1},y_{i-1},c_{i}).\nWhile previously cğ‘c was the same for all time steps, here cğ‘c takes a different value, cisubscriptğ‘ğ‘–c_{i}, at each time step.\nThis accounts for the attention mechasim (context vector) around that specific time step.\ncisubscriptğ‘ğ‘–c_{i} is computed according to the following: ci=âˆ‘j=1TxÎ±iâ€‹jâ€‹hj,Î±iâ€‹j=eâ€‹xâ€‹pâ€‹(eiâ€‹j)âˆ‘k=1Txeâ€‹xâ€‹pâ€‹(eiâ€‹k),eiâ€‹j=aâ€‹(siâˆ’1,hj)formulae-sequencesubscriptğ‘ğ‘–superscriptsubscriptğ‘—1subscriptğ‘‡ğ‘¥subscriptğ›¼ğ‘–ğ‘—subscriptâ„ğ‘—formulae-sequencesubscriptğ›¼ğ‘–ğ‘—ğ‘’ğ‘¥ğ‘subscriptğ‘’ğ‘–ğ‘—superscriptsubscriptğ‘˜1subscriptğ‘‡ğ‘¥ğ‘’ğ‘¥ğ‘subscriptğ‘’ğ‘–ğ‘˜subscriptğ‘’ğ‘–ğ‘—ğ‘subscriptğ‘ ğ‘–1subscriptâ„ğ‘—c_{i}=\\sum_{j=1}^{T_{x}}\\alpha_{ij}h_{j},\\ \\alpha_{ij}=\\frac{exp(e_{ij})}{{\\sum_{k=1}^{T_{x}}exp(e_{ik})}},\\ e_{ij}=a(s_{i-1},h_{j}). Here ağ‘a is the alignment model that is represented by a feed forward neural network.\nAlso hj=[hjTâ†’,hjTâ†]subscriptâ„ğ‘—â†’superscriptsubscriptâ„ğ‘—ğ‘‡â†superscriptsubscriptâ„ğ‘—ğ‘‡h_{j}=[\\overset{\\rightarrow}{h_{j}^{T}},\\overset{\\leftarrow}{h_{j}^{T}}], which is a way to include information both about preceding and following words in hjsubscriptâ„ğ‘—h_{j}.\nThe model was able to outperform the simple encoder-decoder approach regardless of input sentence length. Improved machine translation models continue to emerge, driven in part by the growth in peopleâ€™s interest and need to understand other languages\nMost of them are variants of the end-to-end decoder-encoder approach. For example, Â  tries to deal with the problem of rare words.\nTheir LSTM network consists of encoder and decoder layers using residual layers along with the attention mechanism.\nTheir system was able to decrease training time, speed up inference, and handle translation of rare words.\nComparisons between some of the state-of-the-art neural machine translation models are summarized in TableÂ VII. More recently, [ref]177 provides an interesting single-model implementation of massively multilingual NMT. In , authors use BERT to extract contextual embeddings and combine BERT with an attention-based NMT model and provide state-of-the-art results on various benchmark datasets. [ref]179 proposes mBART which is a seq-to-seq denoising autoencoder and reports that using a pretrained, locked (i.e. no modifications) mBART improves performance in terms of the BLEU point.  proposes an interesting adversarial framework for robustifying NMT against noisy inputs and reports performance gains over the Transformer model.  is also an insightful recent work where the authors sample context words from the predicted sequence as well as the ground truth to try to reconcile the training and inference processes. Finally,  is a successful recent effort to prevent the forgetting that often accompanies in translating pre-trained language models to other NMT task.  achieves that aim primarily by using a dynamically gated model and asymptotic distillation."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Foundations of statistical natural language processing",
      "abstract": "",
      "year": "1999",
      "venue": "MIT Press",
      "authors": "C. D. Manning, C. D. Manning, and H. SchÃ¼tze"
    },
    {
      "index": 1,
      "title": "Character-level convolutional networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Zhang, J. Zhao, and Y. LeCun"
    },
    {
      "index": 2,
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1406.1078",
      "authors": "K. Cho, B. Van MerriÃ«nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio"
    },
    {
      "index": 3,
      "title": "Deep learning in clinical natural language processing: a methodical review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of the American Medical Informatics Association",
      "authors": "S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei, Y. Xiang, B. Zhao, and H. Xu"
    },
    {
      "index": 4,
      "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "abstract": "",
      "year": "2008",
      "venue": "25th international conference on Machine learning",
      "authors": "R. Collobert and J. Weston"
    },
    {
      "index": 5,
      "title": "Large-scale video classification with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei"
    },
    {
      "index": 6,
      "title": "Learning and transferring mid-level image representations using convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "M. Oquab, L. Bottou, I. Laptev, and J. Sivic"
    },
    {
      "index": 7,
      "title": "Learning from simulated and unsupervised images through adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb"
    },
    {
      "index": 8,
      "title": "Deep Learning for Computer Vision: A Brief Review",
      "abstract": "",
      "year": "2018",
      "venue": "Computational Intelligence and Neuroscience",
      "authors": "A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis"
    },
    {
      "index": 9,
      "title": "Deep learning vs. traditional computer vision",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Computer Vision",
      "authors": "N. Oâ€™Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh"
    },
    {
      "index": 10,
      "title": "Towards end-to-end speech recognition with recurrent neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Graves and N. Jaitly"
    },
    {
      "index": 11,
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al.",
      "orig_title": "Deep speech 2: End-to-end speech recognition in English and Mandarin",
      "paper_id": "1512.02595v1"
    },
    {
      "index": 12,
      "title": "Deep learning for NLP and speech recognition",
      "abstract": "",
      "year": "2019",
      "venue": "Springer",
      "authors": "U. Kamath, J. Liu, and J. Whitaker"
    },
    {
      "index": 13,
      "title": "Learning character-level representations for part-of-speech tagging",
      "abstract": "",
      "year": "2014",
      "venue": "31st International Conference on Machine Learning (ICML-14)",
      "authors": "C. D. Santos and B. Zadrozny"
    },
    {
      "index": 14,
      "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1604.05529",
      "authors": "B. Plank, A. SÃ¸gaard, and Y. Goldberg",
      "orig_title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "paper_id": "1604.05529v3"
    },
    {
      "index": 15,
      "title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics",
      "authors": "C. D. Manning"
    },
    {
      "index": 16,
      "title": "Deep learning techniques for part of speech tagging by natural language processing",
      "abstract": "",
      "year": "2020",
      "venue": "2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)",
      "authors": "R. D. Deshmukh and A. Kiwelekar"
    },
    {
      "index": 17,
      "title": "Neural Architectures for Named Entity Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1603.01360",
      "authors": "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer",
      "orig_title": "Neural architectures for named entity recognition",
      "paper_id": "1603.01360v3"
    },
    {
      "index": 18,
      "title": "Named entity recognition with bidirectional LSTM-CNNs",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.08308",
      "authors": "J. P. Chiu and E. Nichols"
    },
    {
      "index": 19,
      "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.11470",
      "authors": "V. Yadav and S. Bethard",
      "orig_title": "A survey on recent advances in named entity recognition from deep learning models",
      "paper_id": "1910.11470v1"
    },
    {
      "index": 20,
      "title": "A Survey on Deep Learning for Named Entity Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "J. Li, A. Sun, J. Han, and C. Li",
      "orig_title": "A survey on deep learning for named entity recognition",
      "paper_id": "1812.09449v3"
    },
    {
      "index": 21,
      "title": "End-to-end learning of semantic role labeling using recurrent neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "authors": "J. Zhou and W. Xu"
    },
    {
      "index": 22,
      "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.02593",
      "authors": "D. Marcheggiani, A. Frolov, and I. Titov",
      "orig_title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling",
      "paper_id": "1701.02593v2"
    },
    {
      "index": 23,
      "title": "Deep semantic role labeling: What works and whatâ€™s next",
      "abstract": "",
      "year": "2017",
      "venue": "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "L. He, K. Lee, M. Lewis, and L. Zettlemoyer"
    },
    {
      "index": 24,
      "title": "Syntax-aware Multilingual Semantic Role Labeling",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.00310",
      "authors": "S. He, Z. Li, and H. Zhao",
      "orig_title": "Syntax-aware multilingual semantic role labeling",
      "paper_id": "1909.00310v3"
    },
    {
      "index": 25,
      "title": "Recent Trends in Deep Learning Based Natural Language Processing",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine",
      "authors": "T. Young, D. Hazarika, S. Poria, and E. Cambria",
      "orig_title": "Recent trends in deep learning based natural language processing",
      "paper_id": "1708.02709v8"
    },
    {
      "index": 26,
      "title": "Natural language processing (NLP) in management research: A literature review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Management Analytics",
      "authors": "Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu"
    },
    {
      "index": 27,
      "title": "What exactly is artificial intelligence, anyway?.",
      "abstract": "",
      "year": "2018",
      "venue": "Wall Street Journal Online Article",
      "authors": "T. Greenwald"
    },
    {
      "index": 28,
      "title": "Critical analysis of big data challenges and analytical methods",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Business Research",
      "authors": "U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody"
    },
    {
      "index": 29,
      "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1506.00019",
      "authors": "Z. C. Lipton, J. Berkowitz, and C. Elkan",
      "orig_title": "A critical review of recurrent neural networks for sequence learning",
      "paper_id": "1506.00019v4"
    },
    {
      "index": 30,
      "title": "Convolutional Neural Networks for Sentence Classification",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1408.5882",
      "authors": "Y. Kim",
      "orig_title": "Convolutional neural networks for sentence classification",
      "paper_id": "1408.5882v2"
    },
    {
      "index": 31,
      "title": "Parsing natural scenes and natural language with recursive neural networks",
      "abstract": "",
      "year": "2011",
      "venue": "28th international conference on machine learning (ICML-11)",
      "authors": "R. Socher, C. C. Lin, C. Manning, and A. Y. Ng"
    },
    {
      "index": 32,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 33,
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "C. dos Santos and M. Gatti"
    },
    {
      "index": 34,
      "title": "Effective use of word order for text categorization with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.1058",
      "authors": "R. Johnson and T. Zhang"
    },
    {
      "index": 35,
      "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Johnson and T. Zhang",
      "orig_title": "Semi-supervised convolutional neural networks for text categorization via region embedding",
      "paper_id": "1504.01255v3"
    },
    {
      "index": 36,
      "title": "Relation classification via convolutional deep neural network",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao"
    },
    {
      "index": 37,
      "title": "Relation extraction: Perspective from convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "1st Workshop on Vector Space Modeling for Natural Language Processing",
      "authors": "T. H. Nguyen and R. Grishman"
    },
    {
      "index": 38,
      "title": "Recurrent neural network based language model",
      "abstract": "",
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association",
      "authors": "T. Mikolov, M. KarafiÃ¡t, L. Burget, J. ÄŒernocká»³, and S. Khudanpur"
    },
    {
      "index": 39,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "S. Hochreiter and J. Schmidhuber"
    },
    {
      "index": 40,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio"
    },
    {
      "index": 41,
      "title": "Wasserstein t-SNE",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.07875",
      "authors": "M. Arjovsky, S. Chintala, and L. Bottou",
      "orig_title": "Wasserstein gan",
      "paper_id": "2205.07531v2"
    },
    {
      "index": 42,
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel"
    },
    {
      "index": 43,
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06434",
      "authors": "A. Radford, L. Metz, and S. Chintala",
      "orig_title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "paper_id": "1511.06434v2"
    },
    {
      "index": 44,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.10196",
      "authors": "T. Karras, T. Aila, S. Laine, and J. Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 45,
      "title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.03135",
      "authors": "N. Tavaf, A. Torfi, K. Ugurbil, and P.-F. Van de Moortele",
      "orig_title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "paper_id": "2101.03135v2"
    },
    {
      "index": 46,
      "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "L. Yu, W. Zhang, J. Wang, and Y. Yu"
    },
    {
      "index": 47,
      "title": "Adversarial Learning for Neural Dialogue Generation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.06547",
      "authors": "J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky",
      "orig_title": "Adversarial learning for neural dialogue generation",
      "paper_id": "1701.06547v5"
    },
    {
      "index": 48,
      "title": "Thumbs up?: sentiment classification using machine learning techniques",
      "abstract": "",
      "year": "2002",
      "venue": "ACL-02 conference on Empirical methods in natural language processing-Volume 10",
      "authors": "B. Pang, L. Lee, and S. Vaithyanathan"
    },
    {
      "index": 49,
      "title": "Distributional structure",
      "abstract": "",
      "year": "1954",
      "venue": "Word",
      "authors": "Z. S. Harris"
    },
    {
      "index": 50,
      "title": "â€œA neural probabilistic language model",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "Distributed Representations of Sentences and Documents",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDistributed representations of sentences and documents",
      "paper_id": "1405.4053v2"
    },
    {
      "index": 52,
      "title": "â€œDistributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "Skip-Thought Vectors",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSkip-thought vectors",
      "paper_id": "1506.06726v1"
    },
    {
      "index": 54,
      "title": "â€œEfficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "LAP LAMBERT Academic Publishing, 2015",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "G.Â Lebanon etÂ al., Riemannian geometry and statistical machine\nlearning."
    },
    {
      "index": 56,
      "title": "Cambridge University Press, 2014",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "J.Â Leskovec, A.Â Rajaraman, and J.Â D. Ullman, Mining of massive datasets."
    },
    {
      "index": 57,
      "title": "â€œNeural network methods for natural language processing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "â€œA character-based convolutional neural network for language-agnostic Twitter sentiment analysis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEnriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 60,
      "title": "Compositional Morphology for Word Representations and Language Modelling",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "â€œCompositional morphology for word representations and language modelling",
      "paper_id": "1405.4273v1"
    },
    {
      "index": 61,
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGet to the point: Summarization with pointer-generator networks",
      "paper_id": "1704.04368v2"
    },
    {
      "index": 62,
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA deep reinforced model for abstractive summarization",
      "paper_id": "1705.04304v3"
    },
    {
      "index": 63,
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œScheduled sampling for sequence prediction with recurrent neural networks",
      "paper_id": "1506.03099v3"
    },
    {
      "index": 64,
      "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA continuous relaxation of beam search for end-to-end training of neural sequence models",
      "paper_id": "1708.00111v2"
    },
    {
      "index": 65,
      "title": "Stochastic Beams and Where to Find Them: The Gumbel-Top-ğ‘˜ Trick for Sampling Sequences Without Replacement",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œStochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement",
      "paper_id": "1903.06059v2"
    },
    {
      "index": 66,
      "title": "â€œRouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "â€œBLEU: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "â€œMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "â€œDeep reinforcement learning for sequence to sequence models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 71,
      "title": "â€œReinforcement learning neural Turing machines-revised",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "â€œSimple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "R.Â S. Sutton and A.Â G. Barto, Reinforcement learning: An introduction."
    },
    {
      "index": 74,
      "title": "â€ Machine Learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "â€œSearch-based structured prediction",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEnd-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 77,
      "title": "â€œRecurrent models of visual attention",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œUtilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
      "paper_id": "1903.09588v1"
    },
    {
      "index": 79,
      "title": "â€œEvaluation of NLP systems",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "â€œAsk me anything: Dynamic memory networks for natural language processing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBidirectional LSTM-CRF models for sequence tagging",
      "paper_id": "1508.01991v1"
    },
    {
      "index": 82,
      "title": "Globally Normalized Transition-Based Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGlobally normalized transition-based neural networks",
      "paper_id": "1603.06042v2"
    },
    {
      "index": 83,
      "title": "â€œPart-of-speech tagging of building codes empowered by deep learning and transformational rules",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "Empower Sequence Labeling with Task-Aware Neural Language Model",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEmpower sequence labeling with task-aware neural language model",
      "paper_id": "1709.04109v4"
    },
    {
      "index": 85,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEnd-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
      "paper_id": "1603.01354v5"
    },
    {
      "index": 87,
      "title": "â€œRobust multilingual part-of-speech tagging via adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "â€œFinding function in form: Compositional character models for open vocabulary word representation",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "â€œContextual string embeddings for sequence labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œMorphosyntactic tagging with a Meta-BiLSTM model over context sensitive token encodings",
      "paper_id": "1805.08237v1"
    },
    {
      "index": 91,
      "title": "â€œJoint RNN-based greedy parsing and word composition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "â€œDeep neural networks for syntactic parsing of morphologically rich languages",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 93,
      "title": "â€œWhat do recurrent neural network grammars learn about syntax?",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "â€œIn-order transition-based constituent parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "â€œImproving neural parsing by disentangling model combination and reranking effects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "Constituency Parsing with a Self-Attentive Encoder",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œConstituency parsing with a self-attentive encoder",
      "paper_id": "1805.01052v1"
    },
    {
      "index": 97,
      "title": "â€œA fast and accurate dependency parser using neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 99,
      "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSimple and accurate dependency parsing using bidirectional LSTM feature representations",
      "paper_id": "1603.04351v3"
    },
    {
      "index": 100,
      "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œTransition-based dependency parsing with stack long short-term memory",
      "paper_id": "1505.08075v1"
    },
    {
      "index": 101,
      "title": "â€œDeep learning for natural language parsing",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "â€œParsing clinical text using the state-of-the-art deep learning based parsers: a systematic comparison",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEfficient second-order treecrf for neural dependency parsing",
      "paper_id": "2005.00975v2"
    },
    {
      "index": 104,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 105,
      "title": "â€œDeep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEncoding sentences with graph convolutional networks for semantic role labeling",
      "paper_id": "1703.04826v4"
    },
    {
      "index": 107,
      "title": "Linguistically-Informed Self-Attention for Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLinguistically-informed self-attention for semantic role labeling",
      "paper_id": "1804.08199v3"
    },
    {
      "index": 108,
      "title": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œJointly predicting predicates and arguments in neural semantic role labeling",
      "paper_id": "1805.04787v2"
    },
    {
      "index": 109,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 110,
      "title": "â€œDeep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "â€œDependency or span",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "â€œTowards robust linguistic analysis using OntoNotes",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "â€œA convolutional neural network for modelling sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "â€œDeep sentence embedding using long short-term memory networks: Analysis and application to information retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "â€œHierarchical attention networks for document classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "â€œRecurrent convolutional neural networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "A C-LSTM Neural Network for Text Classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA C-LSTM neural network for text classification",
      "paper_id": "1511.08630v2"
    },
    {
      "index": 118,
      "title": "Deep Learning Based Text Classification: A Comprehensive Review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeep learning based text classification: A comprehensive review",
      "paper_id": "2004.03705v3"
    },
    {
      "index": 119,
      "title": "â€œA comparative review on deep learning models for text classification",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "Very Deep Convolutional Networks for Text Classification",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œVery deep convolutional networks for text classification",
      "paper_id": "1606.01781v2"
    },
    {
      "index": 121,
      "title": "â€œDeep pyramid convolutional neural networks for text categorization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSupervised and semi-supervised text categorization using LSTM for region embeddings",
      "paper_id": "1602.02373v2"
    },
    {
      "index": 123,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œUniversal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 124,
      "title": "â€œNatural language processing (almost) from scratch",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "â€œInvestigation of recurrent-neural-network architectures and learning methods for spoken language understanding",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "NeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œNeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "paper_id": "1705.05487v1"
    },
    {
      "index": 127,
      "title": "Cloze-driven Pretraining of Self-attention Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œCloze-driven pretraining of self-attention networks",
      "paper_id": "1903.07785v1"
    },
    {
      "index": 128,
      "title": "â€œIntroduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Semi-Supervised Sequence Modeling with Cross-View Training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSemi-supervised sequence modeling with cross-view training",
      "paper_id": "1809.08370v1"
    },
    {
      "index": 130,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 131,
      "title": "â€œSemantic compositionality through recursive matrix-vector spaces",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "â€œSemantic relation extraction using sequential and tree-structured lstm with attention",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "more context and more openness: A review and outlook for relation extraction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "â€œDeep reinforcement learning for mention-ranking coreference models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "â€œHigher-order coreference resolution with coarse-to-fine inference",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "â€œEnd-to-end deep reinforcement learning based coreference resolution",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "CorefQA: Coreference Resolution as Query-based Span Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œCorefqa: Coreference resolution as query-based span prediction",
      "paper_id": "1911.01746v4"
    },
    {
      "index": 138,
      "title": "â€œEvent extraction via dynamic multi-pooling convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "â€œGraph convolutional networks with argument-aware pooling for event detection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "â€œJoint entity and event extraction with generative adversarial imitation learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "â€œA novel joint biomedical event extraction framework via two-level modeling of documents",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "â€œSentiment analysis: Capturing favorability using natural language processing",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "â€œMining the peanut gallery: Opinion extraction and semantic classification of product reviews",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "â€œApplication of deep learning approaches for sentiment analysis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "â€œSentiment analysis using deep learning architectures: a review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "â€œDocument modeling with gated recurrent neural network for sentiment classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "â€œDomain adaptation for large-scale sentiment classification: A deep learning approach",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "â€œLstm with sentence representations for document-level sentiment classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "â€œA cnn-bilstm model for document-level sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "â€œSemi-supervised recursive autoencoders for predicting sentiment distributions",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "â€œPredicting polarities of tweets by composing word embeddings with long short-term memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "â€œRecursive deep models for semantic compositionality over a sentiment treebank",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "â€œClassification of sentence level sentiment analysis using cloud machine learning techniques",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "â€ Information Processing & Management",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "â€œAttention-based LSTM for aspect-level sentiment classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "â€œSentic lstm: a hybrid network for targeted aspect-based sentiment analysis",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "paper_id": "1904.02232v2"
    },
    {
      "index": 158,
      "title": "Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDouble embeddings and CNN-based sequence labeling for aspect extraction",
      "paper_id": "1805.04601v1"
    },
    {
      "index": 159,
      "title": "â€œDeep learning for aspect-based sentiment analysis: a comparative review",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "â€œA multi-layer dual attention deep learning model with refined word embeddings for aspect-based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "â€œA novel aspect-guided deep transition model for aspect based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Prentice Hall, 2008",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "D.Â Jurafsky and J.Â H. Martin, Speech and Language Processing."
    },
    {
      "index": 163,
      "title": "â€œRecurrent continuous translation models",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "â€œMachine translation using deep learning: An overview",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "â€œA survey of deep learning techniques for neural machine translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "â€œThe Georgetown-IBM experiment",
      "abstract": "",
      "year": "1955",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "â€œNeural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 168,
      "title": "B. Van MerriÃ«nboer",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "â€œSequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGoogleâ€™s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 171,
      "title": "â€œConvolutional sequence to sequence learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAttention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 173,
      "title": "Weighted Transformer Network for Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œWeighted transformer network for machine translation",
      "paper_id": "1711.02132v1"
    },
    {
      "index": 174,
      "title": "Self-Attention with Relative Position Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSelf-attention with relative position representations",
      "paper_id": "1803.02155v2"
    },
    {
      "index": 175,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œUnderstanding back-translation at scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 176,
      "title": "Massively Multilingual Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œMassively multilingual neural machine translation",
      "paper_id": "1903.00089v3"
    },
    {
      "index": 177,
      "title": "Incorporating BERT into Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œIncorporating bert into neural machine translation",
      "paper_id": "2002.06823v1"
    },
    {
      "index": 178,
      "title": "M. Ghazvininejad",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œRobust neural machine translation with doubly adversarial inputs",
      "paper_id": "1906.02443v1"
    },
    {
      "index": 180,
      "title": "Bridging the Gap between Training and Inference for Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBridging the gap between training and inference for neural machine translation",
      "paper_id": "1906.02448v2"
    },
    {
      "index": 181,
      "title": "Towards Making the Most of BERT in Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œTowards making the most of bert in neural machine translation",
      "paper_id": "1908.05672v5"
    },
    {
      "index": 182,
      "title": "â€œQuestion answering with subgraph embeddings",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "â€œBaseball: an automatic question-answerer",
      "abstract": "",
      "year": "1961",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "â€œIBMâ€™s statistical question answering system",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "â€œQuestion answering passage retrieval using dependency relations",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "â€œConvolutional neural tensor network architecture for community-based question answering",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "â€œA machine learning approach to answering questions for reading comprehension tests",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "Dynamic Coattention Networks for Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDynamic coattention networks for question answering",
      "paper_id": "1611.01604v4"
    },
    {
      "index": 189,
      "title": "C. Lawrence Zitnick",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAsk your neurons: A neural-based approach to answering questions about images",
      "paper_id": "1505.01121v3"
    },
    {
      "index": 191,
      "title": "attend and answer: Exploring question-guided spatial attention for visual question answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œHuman attention in visual question answering: Do humans and deep networks look at the same regions?",
      "paper_id": "1606.03556v2"
    },
    {
      "index": 193,
      "title": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBlock: Bilinear superdiagonal fusion for visual question answering and visual relationship detection",
      "paper_id": "1902.00038v2"
    },
    {
      "index": 194,
      "title": "â€œSelf-critical reasoning for robust visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 195,
      "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents",
      "paper_id": "1611.04230v1"
    },
    {
      "index": 196,
      "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œRanking sentences for extractive summarization with reinforcement learning",
      "paper_id": "1802.08636v2"
    },
    {
      "index": 197,
      "title": "â€œA neural attention model for abstractive sentence summarization",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "â€œAbstractive document summarization with a graph-based attentional neural model",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "â€œAbstractive text summarization using sequence-to-sequence RNNs and beyond",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "â€œTeaching machines to read and comprehend",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œIncorporating copying mechanism in sequence-to-sequence learning",
      "paper_id": "1603.06393v3"
    },
    {
      "index": 202,
      "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFast abstractive summarization with reinforce-selected sentence rewriting",
      "paper_id": "1805.11080v1"
    },
    {
      "index": 203,
      "title": "Neural Document Summarization by Jointly Learning to Score and Select Sentences",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œNeural document summarization by jointly learning to score and select sentences",
      "paper_id": "1807.02305v1"
    },
    {
      "index": 204,
      "title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œNeural abstractive text summarization with sequence-to-sequence models",
      "paper_id": "1812.02303v4"
    },
    {
      "index": 205,
      "title": "â€œMulti-document summarization via deep learning techniques: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "â€œA hybrid deep learning architecture for opinion-oriented multi-document summarization based on multi-feature fusion",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 207,
      "title": "â€œHibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "â€œDialogue systems for intelligent human computer interactions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 209,
      "title": "â€œMulti-domain joint semantic frame parsing using bi-directional RNN-LSTM",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "Understanding Chatbot-mediated Task Management",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œUnderstanding chatbot-mediated task management",
      "paper_id": "1802.03109v1"
    },
    {
      "index": 211,
      "title": "Goal-Oriented Chatbot Dialog Management Bootstrapping with Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGoal-oriented chatbot dialog management bootstrapping with transfer learning",
      "paper_id": "1802.00500v2"
    },
    {
      "index": 212,
      "title": "â€œDeep reinforcement learning for dialogue generation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA network-based end-to-end trainable task-oriented dialogue system",
      "paper_id": "1604.04562v3"
    },
    {
      "index": 214,
      "title": "â€œEnd-to-end LSTM-based dialog control optimized with supervised and reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "â€œEnd-to-end memory networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "Learning End-to-End Goal-Oriented Dialog",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLearning end-to-end goal-oriented dialog",
      "paper_id": "1605.07683v4"
    },
    {
      "index": 217,
      "title": "â€œData-driven response generation in social media",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "â€œAn information retrieval approach to short text conversation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "â€œConvolutional neural network architectures for matching natural language sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œThe Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "paper_id": "1506.08909v3"
    },
    {
      "index": 221,
      "title": "â€œLearning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "â€œMulti-turn response selection for chatbots with deep attention matching network",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "â€œGenerating sentences from a continuous space",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "Adversarial Evaluation of Dialogue Models",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAdversarial evaluation of dialogue models",
      "paper_id": "1701.08198v1"
    },
    {
      "index": 225,
      "title": "A Neural Conversational Model",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA neural conversational model",
      "paper_id": "1506.05869v3"
    }
  ]
}