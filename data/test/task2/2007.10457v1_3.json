{
  "paper_id": "2007.10457v1",
  "title": "Multi-agent Reinforcement Learning in Bayesian Stackelberg Markov Games for Adaptive Moving Target Defense",
  "sections": {
    "introduction": "The complexity of modern-day software technology has made the goal of deploying fully secure cyber-systems impossible. Furthermore, an attacker often has ample time to explore a deployed system before exploiting it. To level the playing field, researchers have introduced the idea of proactive cyber defenses such as Moving Target Defense.\nIn Moving Target Defense (MTD), the defender shifts between various configurations of the cyber-system . This makes the attacker’s knowledge, gathered during the reconnaissance phase, useless at attack time as the system may have shifted to a new configuration in the window between reconnaissance and attack. To ensure that an MTD system is effective at maximizing security and minimizing the impact on the system’s performance, the consideration of an optimal movement strategy is important [ref]2 . MTD systems render themselves naturally to a game-theoretic formulation– modeling the cyber-system as a two-player game between the defender and an attacker is commonplace. The expectation is that the equilibrium of these games yields an optimal (mixed) strategy that guides the defender on how to move their dynamic cyber-system in the presence of a strategic and rational adversary. The notion of Strong Stackelberg Equilibrium predominantly underlies the definition of optimal strategies in these settings [ref]4  as the defender deploys a system first (acting as a leader) while the attacker, who seeks to attack the deployed system, assumes the role of the follower. In many real-world scenarios, single-stage normal-form games do not provide sufficient expressiveness to capture the switching costs of actions [ref]4  or reason about the adversary’s sequential behavior  . On the other hand, works that consider modeling the MTD as a multi-stage stochastic game  0 1 , do not model incomplete information about adversaries, a key aspect of the single-stage normal-form formalism (known as Bayesian Stackelberg Games (BSG) 2 ). To address these concerns about expressiveness, while remaining scalable for use in cyber-security settings, we propose the unifying framework of Bayesian Stackelberg Markov Games (BSMG).\nWe show that BSMGs can be used to model various Moving Target Defense scenarios, capturing the uncertainty over attacker types and sequential impacts of attacks and switching defenses. We characterize the notion of optimal strategy as the Strong Stackelberg Equilibrium of BSMGs and show that the robust (movement) strategy improves the state-of-the-art found by previous game-theoretic modeling. While multi-stage game models are ubiquitous in security settings, expecting experts to provide detailed models about rewards and system transitions is considered unrealistic. Thus, researchers have considered techniques in reinforcement learning to learn optimal movement policies over time 3 4 5 6. Unfortunately, these works ignore (1) the strategic nature and the rational behavior of an adversary and (2) the incomplete knowledge a defender may possess about their opponent. This, as we show in our experiments, results in a new attack surface where the defender’s movement policy can be exploited by an adversary. To mitigate this, we bridge the knowledge gap between existing work, and techniques in multi-agent reinforcement learning by proposing a Bayesian Strong Stackelberg Q-learning (BSS-Q) approach (graphically shown in Figure 1). First, we can show that BSS-Q converges to the Strong Stackelberg Equilibrium of BSMGs. Second, we design an Open-AI gym 7 style multi-agent environment for two Moving Target Defenses (one for web-application and the other for cloud-network security) and compare the effectiveness of policies learned by BSS-Q against existing state-of-the-art static policies and other reinforcement learning agents. In the next section, we motivate the need for a unifying framework and formally describe the proposed game-theoretic model of BSMGs. We briefly discuss how two Moving Target Defenses are modeled as BSGMs. We then introduce the Bayesian Strong Stackelberg Q-learning approach and show that it converges to the SSE of BSMGs, followed by a section showcasing experimental results. Finally, before concluding, we discuss related work."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Moving target defense: creating asymmetric uncertainty for cyber threats, volume 54",
      "abstract": "",
      "year": "2011",
      "venue": "Springer Science & Business Media",
      "authors": "Sushil Jajodia, Anup K Ghosh, Vipin Swarup, Cliff Wang, and X Sean Wang"
    },
    {
      "index": 1,
      "title": "Moving target defense: a symbiotic framework for ai & security",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta"
    },
    {
      "index": 2,
      "title": "A Survey of Moving Target Defenses for Network Security",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Communications Surveys & Tutorials",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Abdulhakim Sabur, Adel Alshamrani, Dijiang Huang, and Subbarao Kambhampati",
      "orig_title": "A survey of moving target defenses for network security",
      "paper_id": "1905.00964v2"
    },
    {
      "index": 3,
      "title": "From physical security to cybersecurity",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Cybersecurity",
      "authors": "Arunesh Sinha, Thanh H Nguyen, Debarun Kar, Matthew Brown, Milind Tambe, and Albert Xin Jiang"
    },
    {
      "index": 4,
      "title": "Moving Target Defense for Web Applications using Bayesian Stackelberg Games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 International Conference on Autonomous Agents & Multiagent Systems",
      "authors": "Satya Gautam Vadlamudi, Sailik Sengupta, Marthony Taguinod, Ziming Zhao, Adam Doupé, Gail-Joon Ahn, and Subbarao Kambhampati",
      "orig_title": "Moving target defense for web applications using bayesian stackelberg games",
      "paper_id": "1602.07024v3"
    },
    {
      "index": 5,
      "title": "A game theoretic approach to strategy generation for moving target defense in web applications",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta, Satya Gautam Vadlamudi, Subbarao Kambhampati, Adam Doupé, Ziming Zhao, Marthony Taguinod, and Gail-Joon Ahn"
    },
    {
      "index": 6,
      "title": "Computing stackelberg equilibria in discounted stochastic games",
      "abstract": "",
      "year": "2012",
      "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence",
      "authors": "Yevgeniy Vorobeychik and Satinder Singh"
    },
    {
      "index": 7,
      "title": "Markov game modeling of moving target defense for strategic detection of threats in cloud networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.09660",
      "authors": "Ankur Chowdhary, Sailik Sengupta, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 8,
      "title": "Markov modeling of moving target defense games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 ACM Workshop on Moving Target Defense",
      "authors": "Hoda Maleki, Saeed Valizadeh, William Koch, Azer Bestavros, and Marten van Dijk"
    },
    {
      "index": 9,
      "title": "Game-theoretic approach to feedback-driven multi-stage moving target defense",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on decision and game theory for security",
      "authors": "Quanyan Zhu and Tamer Başar"
    },
    {
      "index": 10,
      "title": "Spatial-Temporal Moving Target Defense: A Markov Stackelberg Game Model",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.10390",
      "authors": "Henger Li, Wen Shen, and Zizhan Zheng",
      "orig_title": "Spatial-temporal moving target defense: A markov stackelberg game model",
      "paper_id": "2002.10390v1"
    },
    {
      "index": 11,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "7th international joint conference on Autonomous agents and multiagent systems-Volume 2",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 12,
      "title": "Markov security games: Learning in spatial security problems",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems (2016)",
      "authors": "Richard Klima, Karl Tuyls, and Frans Oliehoek"
    },
    {
      "index": 13,
      "title": "Dynamic ids configuration in the presence of intruder type uncertainty",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Global Communications Conference (GLOBECOM)",
      "authors": "Xiaofan He, Huaiyu Dai, Peng Ning, and Rudra Dutta"
    },
    {
      "index": 14,
      "title": "Playing adaptively against stealthy opponents: A reinforcement learning strategy for the flipit security game",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.11938",
      "authors": "Lisa Oakley and Alina Oprea"
    },
    {
      "index": 15,
      "title": "Deep reinforcement learning based adaptive moving target defense",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.11972",
      "authors": "Taha Eghtesad, Yevgeniy Vorobeychik, and Aron Laszka"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 17,
      "title": "Stochastic games",
      "abstract": "",
      "year": "1953",
      "venue": "Proceedings of the national academy of sciences",
      "authors": "Lloyd S Shapley"
    },
    {
      "index": 18,
      "title": "Solving security games on graphs via marginal probabilities",
      "abstract": "",
      "year": "2013",
      "venue": "Twenty-Seventh AAAI Conference on Artificial Intelligence",
      "authors": "Joshua Letchford and Vincent Conitzer"
    },
    {
      "index": 19,
      "title": "General sum markov games for strategic detection of advanced persistent threats using moving target defense in cloud networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 20,
      "title": "Dynamic policy-based ids configuration",
      "abstract": "",
      "year": "2009",
      "venue": "48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference",
      "authors": "Quanyan Zhu and Tamer Başar"
    },
    {
      "index": 21,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "AAMAS",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 22,
      "title": "On generalized stackelberg strategies",
      "abstract": "",
      "year": "1978",
      "venue": "Journal of Optimization Theory and Applications",
      "authors": "George Leitmann"
    },
    {
      "index": 23,
      "title": "Computing the optimal strategy to commit to",
      "abstract": "",
      "year": "2006",
      "venue": "7th ACM Conference on Electronic Commerce, EC ’06",
      "authors": "Vincent Conitzer and Tuomas Sandholm"
    },
    {
      "index": 24,
      "title": "Leadership with commitment to mixed strategies",
      "abstract": "",
      "year": "2004",
      "venue": "CDAM Research Report LSE-CDAM-2004-01",
      "authors": "BV Stengel and S Zamir"
    },
    {
      "index": 25,
      "title": "Lecture notes on non-cooperative game theory",
      "abstract": "",
      "year": "2010",
      "venue": "Game Theory Module of the Graduate Program in Network Mathematics",
      "authors": "Tamer Basar et al."
    },
    {
      "index": 26,
      "title": "National vulnerability database",
      "abstract": "",
      "year": "",
      "venue": "https://nvd.nist.gov",
      "authors": ""
    },
    {
      "index": 27,
      "title": "An initial study of targeted personality models in the flipit game",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Anjon Basak, Jakub Černỳ, Marcus Gutierrez, Shelby Curtis, Charles Kamhoua, Daniel Jones, Branislav Bošanskỳ, and Christopher Kiekintveld"
    },
    {
      "index": 28,
      "title": "Automated generation and analysis of attack graphs",
      "abstract": "",
      "year": "2002",
      "venue": "2002 IEEE Symposium on Security and Privacy",
      "authors": "Oleg Sheyner, Joshua Haines, Somesh Jha, Richard Lippmann, and Jeannette M Wing"
    },
    {
      "index": 29,
      "title": "A game theoretic approach to strategy determination for dynamic platform defenses",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Kevin M Carter, James F Riordan, and Hamed Okhravi"
    },
    {
      "index": 30,
      "title": "Deceiving cyber adversaries: A game theoretic approach",
      "abstract": "",
      "year": "2018",
      "venue": "17th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Aaron Schlenker, Omkar Thakoor, Haifeng Xu, Fei Fang, Milind Tambe, Long Tran-Thanh, Phebe Vayanos, and Yevgeniy Vorobeychik"
    },
    {
      "index": 31,
      "title": "Asymmetric multiagent reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "Web Intelligence and Agent Systems: An international journal",
      "authors": "Ville Könönen"
    },
    {
      "index": 32,
      "title": "Towards a theory of moving target defense",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Rui Zhuang, Scott A DeLoach, and Xinming Ou"
    },
    {
      "index": 33,
      "title": "Multiagent reinforcement learning: theoretical framework and an algorithm",
      "abstract": "",
      "year": "1998",
      "venue": "ICML",
      "authors": "Junling Hu, Michael P Wellman, et al."
    },
    {
      "index": 34,
      "title": "Multi-agent reinforcement learning: a critical survey",
      "abstract": "",
      "year": "2003",
      "venue": "Web manuscript",
      "authors": "Yoav Shoham, Rob Powers, and Trond Grenager"
    },
    {
      "index": 35,
      "title": "Markov games as a framework for multi-agent reinforcement learning",
      "abstract": "",
      "year": "1994",
      "venue": "Machine learning proceedings 1994",
      "authors": "Michael L Littman"
    },
    {
      "index": 36,
      "title": "Friend-or-foe q-learning in general-sum games",
      "abstract": "",
      "year": "2001",
      "venue": "ICML",
      "authors": "Michael L Littman"
    },
    {
      "index": 37,
      "title": "Correlated q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "ICML",
      "authors": "Amy Greenwald, Keith Hall, and Roberto Serrano"
    },
    {
      "index": 38,
      "title": "Finite depth of reasoning and equilibrium play in games with incomplete information",
      "abstract": "",
      "year": "2013",
      "venue": "Discussion Paper, Center for Mathematical Studies in Economics and ...",
      "authors": "Willemien Kets"
    },
    {
      "index": 39,
      "title": "On markov games played by bayesian and boundedly-rational players",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Muthukumaran Chandrasekaran, Yingke Chen, and Prashant Doshi"
    },
    {
      "index": 40,
      "title": "Contributions to the Theory of Games, volume 2",
      "abstract": "",
      "year": "1953",
      "venue": "Princeton University Press",
      "authors": "Harold William Kuhn and Albert William Tucker"
    },
    {
      "index": 41,
      "title": "Markov games of incomplete information for multi-agent reinforcement learning",
      "abstract": "",
      "year": "2011",
      "venue": "Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence",
      "authors": "Liam MacDermed, Charles Isbell, and Lora Weiss"
    },
    {
      "index": 42,
      "title": "Leader-follower semi-markov decision problems: theoretical framework and approximate solution",
      "abstract": "",
      "year": "2007",
      "venue": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
      "authors": "Kurian Tharakunnel and Siddhartha Bhattacharyya"
    },
    {
      "index": 43,
      "title": "A multi-agent reinforcement learning algorithm based on stackelberg game",
      "abstract": "",
      "year": "2017",
      "venue": "2017 6th Data Driven Control and Learning Systems (DDCLS)",
      "authors": "Chi Cheng, Zhangqing Zhu, Bo Xin, and Chunlin Chen"
    },
    {
      "index": 44,
      "title": "Leader-follower mdp models with factored state space and many followers-followers abstraction, structured dynamics and state aggregation",
      "abstract": "",
      "year": "2016",
      "venue": "Twenty-second European Conference on Artificial Intelligence",
      "authors": "Régis Sabbadin and Anne-France Viet"
    },
    {
      "index": 45,
      "title": "M3RL: Mind-aware Multi-agent Management Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00147",
      "authors": "Tianmin Shu and Yuandong Tian",
      "orig_title": "M3rl: Mind-aware multi-agent management reinforcement learning",
      "paper_id": "1810.00147v3"
    },
    {
      "index": 46,
      "title": "Learning expensive coordination: An event-based deep rl approach",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, and Bo An"
    },
    {
      "index": 47,
      "title": "A unified analysis of value-function-based reinforcement-learning algorithms",
      "abstract": "",
      "year": "1999",
      "venue": "Neural computation",
      "authors": "Csaba Szepesvári and Michael L Littman"
    },
    {
      "index": 48,
      "title": "Bayesian games for threat prediction and situation analysis",
      "abstract": "",
      "year": "2004",
      "venue": "International Conference on Information Fusion",
      "authors": "Joel Brynielsson and Stefan Arnborg"
    }
  ]
}