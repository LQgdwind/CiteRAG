{
  "paper_id": "2401.00850v2",
  "title": "Refining Pre-Trained Motion Models",
  "sections": {
    "ii related work": "Learning motion from color and smoothness.\nSoon after the introduction of deep-learned flow methods [ref]1, there has been interest in doing the learning self-supervised [ref]6, converting optical flow’s classic assumptions , color constancy, and motion smoothness, into supervision objectives.\nColor constancy implies that when flow is accurate, corresponding pixels in consecutive frames should have the same color. By warping images to align with estimated flow, reducing per-pixel differences can improve flow accuracy, particularly when computed across multiple scales  . The smoothness assumption helps to resolve ambiguities and occlusions which can be simply implemented by penalizing spatial gradients on the output flow. Minute details in this setup can affect performance greatly . Instead of relying on color and smoothness, which can be understood as indirectly encouraging correct motion, we compute pseudo-labels for unlabelled data, providing direct regression targets. Bootstrapping with student-teacher setups.\nSome recent works stabilize the losses from color-matching by using student-teacher setups. In this setting, the teacher is a moving average copy of the student, and the student receives harder data than the teacher   . Our work can also be considered a student-teacher setup, but the teacher is a frozen copy of a pre-trained model, and the student is a trainable version of the same model. Our setup is in line with knowledge distillation , except our pair of models begins with the same architecture and weights (rather than the student being a smaller model). Learning motion from cycle-consistency.\nColor matching is known to break down under some conditions (e.g., specularities and occlusions), so many works focus instead on learning directly from cycle-consistency   [ref]11. The key idea is that after tracking a target from a given startpoint to an estimated endpopint, reversing the video and re-applying the tracker from the endpoint should lead back to the original startpoint. This core idea is typically combined with patch-level affinity matrices, which can be traversed with spatial transformers [ref]11, region-level motion averages , or random walks   . Unlike existing self-supervision works which initialize weights from ImageNet  or randomly, our method begins with state-of-the-art models whose architectures and weights are pre-optimized for motion estimation   and attempts to further refine them on a new test domain. Compared to previous works which directly leverage cycle consistency as part of training objective [ref]11, we use it to design a filter to select cycle-consistent motion estimates for the finetuning. Semi-supervised motion estimation.\nSemi-supervised methods in motion estimation typically exploit a mix of labeled and unlabelled data, hoping to achieve better performance than exclusively using labeled data.\nLai et al.  propose to learn a discriminator on warp errors from ground truth warps to provide feedback on the overall quality of the warps,replacing the noisy per-pixel color cues.\nJeong et al.  add a separate segmentation module to a flow network, supervise this module with occlusion masks applied onto unlabelled images, and also ask flows to be consistent across transformations applied to the input, which is similar to our goal but requires modified architecture for segmentation. Training from pseudo labels.\nThe technique of training on self-generated estimates (i.e., pseudo labels) was first proposed by Lee et al. . This core idea is often paired with methods to filter the training targets to \"confident\" ones, where confidence may be approximated with the help of ensemble methods  . Our work computes confidence with the help of domain knowledge: if a trajectory is within some cycle-consistency margin and color constancy, we consider it a pseudo-label. Similar to Chen et al. , we assume that it is possible to train on a small number of confident samples and generalize to other regions of the domain."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox",
      "orig_title": "Flownet: Learning optical flow with convolutional networks",
      "paper_id": "1504.06852v2"
    },
    {
      "index": 1,
      "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox",
      "orig_title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
      "paper_id": "1512.02134v1"
    },
    {
      "index": 2,
      "title": "A naturalistic open source movie for optical flow evaluation",
      "abstract": "",
      "year": "2012",
      "venue": "ECCV",
      "authors": "D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black"
    },
    {
      "index": 3,
      "title": "Tartanair: A dataset to push the limits of visual slam",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer"
    },
    {
      "index": 4,
      "title": "PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking",
      "abstract": "",
      "year": "2023",
      "venue": "ICCV",
      "authors": "Y. Zheng, A. W. Harley, B. Shen, G. Wetzstein, and L. J. Guibas",
      "orig_title": "Pointodyssey: A large-scale synthetic dataset for long-term point tracking",
      "paper_id": "2307.15055v1"
    },
    {
      "index": 5,
      "title": "Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness",
      "abstract": "",
      "year": "2016",
      "venue": "ECCVW",
      "authors": "J. J. Yu, A. W. Harley, and K. G. Derpanis",
      "orig_title": "Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness",
      "paper_id": "1608.05842v1"
    },
    {
      "index": 6,
      "title": "SelFlow: Self-Supervised Learning of Optical Flow",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "P. Liu, M. Lyu, I. King, and J. Xu",
      "orig_title": "Selflow: Self-supervised learning of optical flow",
      "paper_id": "1904.09117v1"
    },
    {
      "index": 7,
      "title": "SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Stone, D. Maurer, A. Ayvaci, A. Angelova, and R. Jonschkowski",
      "orig_title": "Smurf: Self-teaching multi-frame unsupervised raft with full-image warping",
      "paper_id": "2105.07014v1"
    },
    {
      "index": 8,
      "title": "Unsupervised Deep Tracking",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, and H. Li",
      "orig_title": "Unsupervised deep tracking",
      "paper_id": "1904.01828v1"
    },
    {
      "index": 9,
      "title": "Joint-task Self-supervised Learning for Temporal Correspondence",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang",
      "orig_title": "Joint-task self-supervised learning for temporal correspondence",
      "paper_id": "1909.11895v1"
    },
    {
      "index": 10,
      "title": "Learning correspondence from the cycle-consistency of time",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "X. Wang, A. Jabri, and A. A. Efros"
    },
    {
      "index": 11,
      "title": "Space-Time Correspondence as a Contrastive Random Walk",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Jabri, A. Owens, and A. A. Efros",
      "orig_title": "Space-time correspondence as a contrastive random walk",
      "paper_id": "2006.14613v2"
    },
    {
      "index": 12,
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Z. Teed and J. Deng",
      "orig_title": "RAFT: Recurrent all-pairs field transforms for optical flow",
      "paper_id": "2003.12039v3"
    },
    {
      "index": 13,
      "title": "Particle video revisited: Tracking through occlusions using point trajectories",
      "abstract": "",
      "year": "2022",
      "venue": "ECCV",
      "authors": "A. W. Harley, Z. Fang, and K. Fragkiadaki"
    },
    {
      "index": 14,
      "title": "Tracking Pedestrian Heads in Dense Crowd",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "R. Sundararaman, C. De Almeida Braga, E. Marchand, and J. Pettre",
      "orig_title": "Tracking pedestrian heads in dense crowd",
      "paper_id": "2103.13516v1"
    },
    {
      "index": 15,
      "title": "Pretraining boosts out-of-domain robustness for pose estimation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "A. Mathis, T. Biasi, S. Schneider, M. Yuksekgonul, B. Rogers, M. Bethge, and M. W. Mathis",
      "orig_title": "Pretraining boosts out-of-domain robustness for pose estimation",
      "paper_id": "1909.11229v2"
    },
    {
      "index": 16,
      "title": "TAP-Vid: A Benchmark for Tracking Any Point in a Video",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.03726",
      "authors": "C. Doersch, A. Gupta, L. Markeeva, A. Recasens, L. Smaira, Y. Aytar, J. Carreira, A. Zisserman, and Y. Yang",
      "orig_title": "Tap-vid: A benchmark for tracking any point in a video",
      "paper_id": "2211.03726v2"
    },
    {
      "index": 17,
      "title": "An iterative image registration technique with an application to stereo vision",
      "abstract": "",
      "year": "1981",
      "venue": "",
      "authors": "B. D. Lucas, T. Kanade, et al."
    },
    {
      "index": 18,
      "title": "The laplacian pyramid as a compact image code",
      "abstract": "",
      "year": "1987",
      "venue": "Readings in computer vision",
      "authors": "P. J. Burt and E. H. Adelson"
    },
    {
      "index": 19,
      "title": "What Matters in Unsupervised Optical Flow",
      "abstract": "",
      "year": "2020",
      "venue": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Part II 16",
      "authors": "R. Jonschkowski, A. Stone, J. T. Barron, A. Gordon, K. Konolige, and A. Angelova",
      "orig_title": "What matters in unsupervised optical flow",
      "paper_id": "2006.04902v2"
    },
    {
      "index": 20,
      "title": "Semi-Supervised Learning of Optical Flow by Flow Supervisor",
      "abstract": "",
      "year": "2022",
      "venue": "ECCV",
      "authors": "W. Im, S. Lee, and S.-E. Yoon",
      "orig_title": "Semi-supervised learning of optical flow by flow supervisor",
      "paper_id": "2207.10314v1"
    },
    {
      "index": 21,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "G. Hinton, O. Vinyals, and J. Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 22,
      "title": "Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.05838",
      "authors": "Y. Tang, Z. Jiang, Z. Xie, Y. Cao, Z. Zhang, P. H. Torr, and H. Hu",
      "orig_title": "Breaking shortcut: Exploring fully convolutional cycle-consistency for video correspondence learning",
      "paper_id": "2105.05838v2"
    },
    {
      "index": 23,
      "title": "Learning Pixel Trajectories with Multiscale Contrastive Random Walks",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Z. Bian, A. Jabri, A. A. Efros, and A. Owens",
      "orig_title": "Learning pixel trajectories with multiscale contrastive random walks",
      "paper_id": "2201.08379v2"
    },
    {
      "index": 24,
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR09",
      "authors": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei"
    },
    {
      "index": 25,
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Z. Teed and J. Deng",
      "orig_title": "RAFT: Recurrent all-pairs field transforms for optical flow",
      "paper_id": "2003.12039v3"
    },
    {
      "index": 26,
      "title": "Semi-supervised learning for optical flow with generative adversarial networks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "W.-S. Lai, J.-B. Huang, and M.-H. Yang"
    },
    {
      "index": 27,
      "title": "Imposing Consistency for Optical Flow Estimation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Jeong, J. M. Lin, F. Porikli, and N. Kwak",
      "orig_title": "Imposing consistency for optical flow estimation",
      "paper_id": "2204.07262v2"
    },
    {
      "index": 28,
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Workshop on challenges in representation learning, ICML",
      "authors": "D.-H. Lee et al."
    },
    {
      "index": 29,
      "title": "Toward an architecture for never-ending language learning",
      "abstract": "",
      "year": "2010",
      "venue": "Twenty-Fourth AAAI Conference on Artificial Intelligence",
      "authors": "A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka, and T. M. Mitchell"
    },
    {
      "index": 30,
      "title": "NEIL: Extracting visual knowledge from web data",
      "abstract": "",
      "year": "2013",
      "venue": "ICCV",
      "authors": "X. Chen, A. Shrivastava, and A. Gupta"
    },
    {
      "index": 31,
      "title": "Dense point trajectories by GPU-accelerated large displacement optical flow",
      "abstract": "",
      "year": "2010",
      "venue": "ECCV",
      "authors": "N. Sundaram, T. Brox, and K. Keutzer"
    },
    {
      "index": 32,
      "title": "The 2017 DAVIS challenge on video object segmentation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv:1704.00675",
      "authors": "J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung, and L. Van Gool"
    },
    {
      "index": 33,
      "title": "Emerging properties in self-supervised vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin"
    },
    {
      "index": 34,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 35,
      "title": "MAST: A memory-augmented self-supervised tracker",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Z. Lai, E. Lu, and W. Xie"
    },
    {
      "index": 36,
      "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "J. Lin, C. Gan, and S. Han",
      "orig_title": "Tsm: Temporal shift module for efficient video understanding",
      "paper_id": "1811.08383v3"
    },
    {
      "index": 37,
      "title": "COTR: Correspondence Transformer for Matching Across Images",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "W. Jiang, E. Trulls, J. Hosang, A. Tagliasacchi, and K. M. Yi"
    },
    {
      "index": 38,
      "title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox",
      "orig_title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
      "paper_id": "1612.01925v1"
    },
    {
      "index": 39,
      "title": "PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "D. Sun, X. Yang, M.-Y. Liu, and J. Kautz"
    },
    {
      "index": 40,
      "title": "Volumetric correspondence networks for optical flow",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "G. Yang and D. Ramanan"
    }
  ]
}