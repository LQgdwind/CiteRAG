{
  "paper_id": "2412.18933v1",
  "title": "TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment",
  "sections": {
    "introduction": "The rapid advancements in video processing and transmission technologies have led to an exponential increase in diverse video content, significantly influencing daily life. To evaluate the perceived quality of these videos, several user-generated content (UGC) video quality assessment datasets 7   have been developed, driving the evolution of video quality assessment (VQA) methods. Recently, with the demand for higher resolution content, super-resolution (SR) techniques have been widely applied in the UGC scenario , but developing a VQA metric suitable for both UGC and SR scenarios is still a challenge. Given the impracticality of acquiring perfect reference videos for real-world content, blind video quality assessment (BVQA) methods          1 9 7  2 8 1 have gained significant attention. Temporal inconsistency, referring to irregularities or discrepancies in dynamic scenes (including motion, transitions, visual changes, etc.) over time that deviate from the expected smooth flow of visual information between consecutive frames, is a key aspect in BVQA. Latest BVQA methods typically model temporal relationships through various techniques, such as frame differences 1 , optical flow 1, temporal slicing 8 , natural scene statistics  2, and 3D CNNs 1  applied to distorted videos. However, none of the above methods try to design quality assessment metrics from the perspective of temporal inconsistency. Moreover, the growth of SR technologies 8  has made temporal inconsistencies amplified due to the upsampling processes. The recently proposed VSR-QAD dataset 8 was specifically designed to assess SR video quality, but the effectiveness of current BVQA techniques for such videos remains unstable 8. Since temporal inconsistency is a common attribute in both UGC and SR videos, proposing a BVQA method guided by temporal inconsistency can be suitable for both scenarios. Considering that temporal inconsistency in SR videos is different from that in UGC videos, we compute the inconsistency information\nspecifically for UGC and SR scenarios, respectively. Then we propose the Temporal Inconsistency Guided Blind Video Quality Assessment (TINQ) metric, which integrates temporal inconsistency as guidance in both spatial and temporal dimensions during model training. For the SR scenario, we propose to derive temporal inconsistency by calculating the difference between the optical flows of SR and the corresponding reference videos. These videos effectively display motion artifacts in distorted videos relative to their references (as shown in Figure 1). For the UGC scenario, the reference is unavailable, but such temporal inconsistency can be reasonably measured by optical flow, following the analysis in Fig. 2, which provides empirical support for using temporal inconsistency information to guide quality prediction for both UGC and SR videos. As the proposed TINQ belongs to BVQA, during training, we highlight temporally inconsistent areas of distorted videos by weighting with the extracted temporal inconsistency information. During testing, the distorted video is directly input for quality prediction. We extract spatial features at both coarse and fine levels. For coarse grain, we design a deformable window super attention (DW-SA) Transformer to capture inconsistencies in major scene changes or fast movements, leveraging the global receptive field of Transformers 3. For fine grain, CNNs are employed to detect subtle inconsistencies in slower scene transitions or minor motions, drawing on their effectiveness in capturing local details 5. The final spatial features are obtained by concatenating both coarse and fine features. We then propose a two-stage temporal aggregation guided by inconsistency. Specifically, the first stage is based on the visual working memory (VWM) mechanism. Current BVQA methods’ exploration of the VWM is limited  2. Although these methods address the VWM mechanism, they overlook the critical capacity limitation that occurs in VWM  3 9 . Our work designs a visual memory capacity block, dynamically aggregating temporal features based on the level of temporal inconsistency. In the second stage, the time dimension is aggregated by key feature selection. In both stages, a Consistency-aware Fusion Unit is introduced to model temporal relationships, finally leading to a cross-multi-time scale quality prediction. The contributions of this paper are as follows: 1. We propose the Temporal Inconsistency Guided Blind Video Quality Assessment (TINQ) method, leveraging temporal inconsistency to guide quality assessment and validate both its rationale and effectiveness. The rule of temporal inconsistency computation differs for UGC and SR scenarios. 2. We introduce the Inconsistency Highlighted Spatial Module, emphasizing pixel-level temporally inconsistent areas. This module incorporates a DW-SA Transformer for coarse-grained spatial feature extraction, along with a CNN to provide fine-grained details. 3. We present the Inconsistency Guided Temporal Module, featuring a visual memory capacity block that dynamically allocates memory threshold for temporal feature segmentation based on inconsistency levels. Additionally, Consistency-aware Fusion Units are proposed for temporal aggregation, enabling a cross-time-scale prediction of video quality. 4. Experimental results demonstrate the superiority of our BVQA model over existing methods on several video quality datasets, covering both UGC 7  and SR 8."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "SpEED-QA: Spatial efficient entropic differencing for image and video quality",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Signal Processing Letters",
      "authors": "Christos G. Bampis, Praful Gupta, Rajiv Soundararajan, and Alan C. Bovik"
    },
    {
      "index": 1,
      "title": "No-reference video quality assessment based on visual memory modeling",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Broadcasting",
      "authors": "Mehdi Banitalebi-Dehkordi, Abbas Ebrahimi-Moghadam, Morteza Khademi, and Hadi Hadizadeh"
    },
    {
      "index": 2,
      "title": "Blind video quality assessment based on spatio-temporal feature resolver",
      "abstract": "",
      "year": "2024",
      "venue": "Neurocomputing",
      "authors": "Xiaodong Bi, Xiaohai He, Shuhua Xiong, Zeming Zhao, Honggang Chen, and Raymond Edward Sheriff"
    },
    {
      "index": 3,
      "title": "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE CVPR",
      "authors": "Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy",
      "orig_title": "BasicVSR++: Improving video super-resolution with enhanced propagation and alignment",
      "paper_id": "2104.13371v1"
    },
    {
      "index": 4,
      "title": "Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE TCSVT",
      "authors": "Baoliang Chen, Lingyu Zhu, Guo Li, Fangbo Lu, Hongfei Fan, and Shiqi Wang",
      "orig_title": "Learning generalized spatial-temporal deep feature representation for no-reference video quality assessment",
      "paper_id": "2012.13936v2"
    },
    {
      "index": 5,
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "abstract": "",
      "year": "2014",
      "venue": "ArXiv",
      "authors": "Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "paper_id": "1412.3555v1"
    },
    {
      "index": 6,
      "title": "The magical number 4 in short-term memory: A reconsideration of mental storage capacity",
      "abstract": "",
      "year": "2001",
      "venue": "Behavioral and Brain Sciences",
      "authors": "Nelson Cowan"
    },
    {
      "index": 7,
      "title": "Deformable convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE ICCV",
      "authors": "Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei"
    },
    {
      "index": 8,
      "title": "Effective visual working memory capacity: An emergent effect from the neural dynamics in an attractor network",
      "abstract": "",
      "year": "2012",
      "venue": "PLoS ONE",
      "authors": "Laura Dempere-Marco, David Melcher, and Gustavo Deco"
    },
    {
      "index": 9,
      "title": "ImageNet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE CVPR",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 10,
      "title": "Distinct prioritization of visual working memory representations for search and for recall",
      "abstract": "",
      "year": "2019",
      "venue": "Attention, Perception, & Psychophysics",
      "authors": "Blaire Dube and Naseem Al-Aidroos"
    },
    {
      "index": 11,
      "title": "ChipQA: No-Reference Video Quality Prediction via Space-Time Chips",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE TIP",
      "authors": "Joshua Peter Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Sriram Sethuraman, and Alan C. Bovik",
      "orig_title": "ChipQA: No-reference video quality prediction via space-time chips",
      "paper_id": "2109.08726v1"
    },
    {
      "index": 12,
      "title": "Digital image processing, third edition",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Biomedical Optics",
      "authors": "Rafael Gonzalez, Richard Woods, and Barry Masters"
    },
    {
      "index": 13,
      "title": "Comparing the prioritization of items and feature-dimensions in visual working memory",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Vision",
      "authors": "Jasper E. Hajonides, Freek van Ede, Mark G. Stokes, and Anna Christina Nobre"
    },
    {
      "index": 14,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 15,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber"
    },
    {
      "index": 16,
      "title": "The konstanz natural video database (KoNViD-1k)",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Quality of Multimedia Experience",
      "authors": "Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tamás Szirányi, Shujun Li, and Dietmar Saupe"
    },
    {
      "index": 17,
      "title": "A New Dataset and Transformer for Stereoscopic Video Super-Resolution",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE CVPRW",
      "authors": "Hassan Imani, Md Baharul Islam, and Lai-Kuan Wong",
      "orig_title": "A new dataset and transformer for stereoscopic video super-resolution",
      "paper_id": "2204.10039v1"
    },
    {
      "index": 18,
      "title": "Two-level approach for no-reference consumer video quality assessment",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE TIP",
      "authors": "Jari Korhonen"
    },
    {
      "index": 19,
      "title": "Image super-resolution: A comprehensive review, recent trends, challenges and applications",
      "abstract": "",
      "year": "2023",
      "venue": "Information Fusion",
      "authors": "Dawa Chyophel Lepcha, Bhawna Goyal, Ayush Dogra, and Vishal Goyal"
    },
    {
      "index": 20,
      "title": "Blindly assess quality of in-the-wild videos via quality-aware pre-training and motion perception",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE TCSVT",
      "authors": "Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, and Xianpei Wang"
    },
    {
      "index": 21,
      "title": "Quality Assessment of In-the-Wild Videos",
      "abstract": "",
      "year": "2019",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Dingquan Li, Tingting Jiang, and Ming Jiang",
      "orig_title": "Quality assessment of in-the-wild videos",
      "paper_id": "1908.00375v3"
    },
    {
      "index": 22,
      "title": "Swin Transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo"
    },
    {
      "index": 23,
      "title": "Multiview contrastive learning for completely blind video quality assessment of user generated content",
      "abstract": "",
      "year": "2022",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Shankhanil Mitra and Rajiv Soundararajan"
    },
    {
      "index": 24,
      "title": "Video Multimethod Assessment Fusion (VMAF) on 360VR contents",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Consumer Electronics",
      "authors": "Marta Orduna, César Díaz, Lara Muñoz, Pablo Pérez, Ignacio Benito, and Narciso García",
      "orig_title": "Video multimethod assessment fusion (VMAF) on 360VR contents",
      "paper_id": "1901.06279v1"
    },
    {
      "index": 25,
      "title": "The cognitive neuroscience of visual short-term memory",
      "abstract": "",
      "year": "2015",
      "venue": "Current Opinion in Behavioral Sciences",
      "authors": "Bradley R. Postle"
    },
    {
      "index": 26,
      "title": "Blind prediction of natural video quality",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE TIP",
      "authors": "Michele A. Saad, Alan C. Bovik, and Christophe Charrier"
    },
    {
      "index": 27,
      "title": "Visual salience improves spatial working memory via enhanced parieto-temporal functional connectivity",
      "abstract": "",
      "year": "2013",
      "venue": "The Journal of Neuroscience",
      "authors": "Valerio Santangelo and Emiliano Macaluso"
    },
    {
      "index": 28,
      "title": "Capacity limit of visual short-term memory in human posterior parietal cortex",
      "abstract": "",
      "year": "2004",
      "venue": "Nature",
      "authors": "Maike Schmidt, Suk-Won Jin, Alane M. Gray, Dimitris Beis, Thinh Pham, Gretchen D. Frantz, Susan Palmieri, Kenneth San Francisco Hillan, Didier Y. R. Stainier, Frederic J. de Sauvage, and Weilan Ye"
    },
    {
      "index": 29,
      "title": "Image information and visual quality",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE TIP",
      "authors": "H.R. Sheikh and A.C. Bovik"
    },
    {
      "index": 30,
      "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE CVPR",
      "authors": "Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang",
      "orig_title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
      "paper_id": "1609.05158v2"
    },
    {
      "index": 31,
      "title": "Editorial: The cognitive neuroscience of visual working memory, volume II",
      "abstract": "",
      "year": "2022",
      "venue": "Frontiers in Systems Neuroscience",
      "authors": "Natasha Sigala, Zsuzsa Kaldy, and Greg D Reynolds"
    },
    {
      "index": 32,
      "title": "Large-scale study of perceptual video quality",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE TIP",
      "authors": "Zeina Sinno and Alan Conrad Bovik"
    },
    {
      "index": 33,
      "title": "A Deep Learning based No-reference Quality Assessment Model for UGC Videos",
      "abstract": "",
      "year": "2022",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai",
      "orig_title": "A deep learning based no-reference quality assessment model for UGC videos",
      "paper_id": "2204.14047v2"
    },
    {
      "index": 34,
      "title": "A reduced-reference quality assessment metric for super-resolution reconstructed images with information gain and texture similarity",
      "abstract": "",
      "year": "2019",
      "venue": "Signal Processing: Image Communication",
      "authors": "Lijuan Tang, Kezheng Sun, Luping Liu, Guangcheng Wang, and Yutao Liu"
    },
    {
      "index": 35,
      "title": "2BiVQA: Double bi-lstm-based video quality assessment of ugc videos",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl.",
      "authors": "Ahmed Telili, Sid Ahmed Fezza, Wassim Hamidouche, and Hanene F. Z. Brachemi Meftah"
    },
    {
      "index": 36,
      "title": "Video quality assessment of user generated content: A benchmark study and a new model",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Conference on lmage Processing",
      "authors": "Zhengzhong Tu, Chia-Ju Chen, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik"
    },
    {
      "index": 37,
      "title": "Image quality assessment: from error visibility to structural similarity",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE TIP",
      "authors": "Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli"
    },
    {
      "index": 38,
      "title": "Modular Blind Video Quality Assessment",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE CVPR",
      "authors": "Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, and Kede Ma",
      "orig_title": "Modular blind video quality assessment",
      "paper_id": "2402.19276v4"
    },
    {
      "index": 39,
      "title": "Perceptual quality assessment of internet videos",
      "abstract": "",
      "year": "2021",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Jiahua Xu, Jing Li, Xingguang Zhou, Wei Zhou, Baichao Wang, and Zhibo Chen"
    },
    {
      "index": 40,
      "title": "Patch-VQ: ‘Patching Up’ the video quality problem",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE CVPR",
      "authors": "Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik"
    },
    {
      "index": 41,
      "title": "Efficient transformer with locally shared attention for video quality assessment",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE International Conference on lmage Processing",
      "authors": "Junyong You and Yuan Lin"
    },
    {
      "index": 42,
      "title": "Discrete fixed-resolution representations in visual working memory",
      "abstract": "",
      "year": "2008",
      "venue": "Nature",
      "authors": "Weiwei Zhang and Steven J. Luck"
    },
    {
      "index": 43,
      "title": "Learning-Based Quality Assessment for Image Super-Resolution",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE TMM",
      "authors": "Tiesong Zhao, Yuting Lin, Yiwen Xu, Weiling Chen, and Zhou Wang",
      "orig_title": "Learning-based quality assessment for image super-resolution",
      "paper_id": "2012.08732v1"
    },
    {
      "index": 44,
      "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE ICCV",
      "authors": "Heliang Zheng, Huan Yang, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo",
      "orig_title": "Learning conditional knowledge distillation for degraded-reference image quality assessment",
      "paper_id": "2108.07948v1"
    },
    {
      "index": 45,
      "title": "A completely blind video quality evaluator",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Signal Processing Letters",
      "authors": "Qi Zheng, Zhengzhong Tu, Xiaoyang Zeng, Alan C. Bovik, and Yibo Fan"
    },
    {
      "index": 46,
      "title": "Super-resolution image visual quality assessment based on structure–texture features",
      "abstract": "",
      "year": "2023",
      "venue": "Signal Processing: Image Communication",
      "authors": "Fei Zhou, Wei Sheng, Zitao Lu, Bo Kang, Mianyi Chen, and Guoping Qiu"
    },
    {
      "index": 47,
      "title": "A database and model for the visual quality assessment of super-resolution videos",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE Transactions on Broadcasting",
      "authors": "Fei Zhou, Wei Sheng, Zitao Lu, and Guoping Qiu"
    },
    {
      "index": 48,
      "title": "Blind Quality Assessment for Image Superresolution Using Deep Two-Stream Convolutional Networks",
      "abstract": "",
      "year": "2020",
      "venue": "Information Sciences",
      "authors": "Wei Zhou, Qiuping Jiang, Yuwang Wang, Zhibo Chen, and Weiping Li",
      "orig_title": "Blind quality assessment for image superresolution using deep two-stream convolutional networks",
      "paper_id": "2004.06163v1"
    },
    {
      "index": 49,
      "title": "Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity",
      "abstract": "",
      "year": "2022",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Wei Zhou and Zhou Wang",
      "orig_title": "Quality assessment of image super-resolution: Balancing deterministic and statistical fidelity",
      "paper_id": "2207.08689v1"
    },
    {
      "index": 50,
      "title": "Learning spatiotemporal interactions for user-generated video quality assessment",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE TCSVT",
      "authors": "Hanwei Zhu, Baoliang Chen, Lingyu Zhu, and Shiqi Wang"
    },
    {
      "index": 51,
      "title": "Deformable ConvNets v2: More Deformable, Better Results",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE CVPR",
      "authors": "Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai",
      "orig_title": "Deformable Convnets V2: More deformable, better results",
      "paper_id": "1811.11168v2"
    }
  ]
}