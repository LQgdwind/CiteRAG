{
  "paper_id": "2210.05271v1",
  "title": "GAN you hear me? Reclaiming unconditional speech synthesis from diffusion models",
  "sections": {
    "asgan implementation": "We train two variants of our model: a log mel-spectrogram based model and a HuBERT feature based model .\nThe former\nis shown in Fig.Â 1, where\nthe model outputs\n128 mel-frequency bins at\na hop and window size of\n10Â ms and 64Â ms, respectively.\nThe HuBERT\nmodel is identical\nexcept\nthat it only uses\nhalf the sequence length\n(since HuBERT features are 20Â ms instead of the 10Â ms spectrogram frames)\nand\nhas\na different number\nof output channels in the four groups of Style Blocks:    1024768512512    convolution channels instead of       . The HiFi-GAN vocoder for both the HuBERT and mel-spectrogram\nmodel is\nbased on the original\nimplementation [ref]27.\nThe HuBERT HiFi-GAN is trained on the Librispeech train-clean-100 subsetÂ \nto vocode activations from layer 6 of the\nHuBERT Base model .\nThe mel-spectrogram HiFi-GAN is trained on the Google Speech Commands dataset. Both\nASGAN variants are trained with Adam  (Î²1=0,Î²2=0.99formulae-sequencesubscriptğ›½10subscriptğ›½20.99\\beta_{1}=0,\\beta_{2}=0.99), clipping gradient norms at 10, and a learning rate of 3â‹…10âˆ’3â‹…3superscript1033\\cdot 10^{-3} for 520k iterations with a batch size of 32.\nSeveral\ntricks are\nused to stabilize GAN training:\n(i) equalized learning rate\n[ref]24, (ii) leaky ReLU activations with Î±=0.1ğ›¼0.1\\alpha=0.1, (iii) exponential moving averaging\nfor the generator weights [ref]24, (iv) R1subscriptğ‘…1R_{1} regularization , and (v)\na 0.01-times smaller learning rate for\nthe mapping network Wğ‘ŠW\n. We also introduce a new technique for updating the discriminator.\nConcretely, we first scale Dğ·Dâ€™s learning rate by 0.1 compared to the generator as otherwise we find it overwhelms\nGğºG\nearly on in training.\nAdditionally we employ a dynamic method for updating Dğ·D, inspired by adaptive discriminator augmentation [ref]14:\nduring each iteration, we skip Dğ·Dâ€™s update with probability pğ‘p.\nThe probability pğ‘p is initialized at 0.1 and is updated every 16th generator step or whenever the discriminator is updated.\nWe keep a running average rtsubscriptğ‘Ÿğ‘¡r_{t} of the proportion of Dğ·Dâ€™s outputs on real data Dâ€‹(X)ğ·ğ‘‹D(X) that are positive (i.e. that Dğ·D can confidently identify as real).\nThen, if rtsubscriptğ‘Ÿğ‘¡r_{t} is greater than 0.6 we increment pğ‘p by 0.05 (capped at 1.0), and if rtsubscriptğ‘Ÿğ‘¡r_{t} is less than 0.6 we decrease pğ‘p by 0.05 (capped at 0.0).\nIn this way we adaptively skip discriminator updates. When Dğ·D becomes too strong, rtsubscriptğ‘Ÿğ‘¡r_{t} and pğ‘p rise, and so Dğ·D is updated less frequently. When Dğ·D becomes too weak (i.e. fails to distinguish between real and fake inputs), then the opposite happens.\nWe found this new modification to be critical for ensuring that the Dğ·D does not overwhelm GğºG\nduring training. We also use the traditional adaptive discriminator augmentation [ref]14\nwhere we apply the following transforms with the same probability pğ‘p:\n(i) adding Gaussian noise with Ïƒ=0.05ğœ0.05\\sigma=0.05, (ii) random scaling by a factor of 1Â±0.05plus-or-minus10.051\\pm 0.05, and (iii) randomly replacing a subsequence of frames from the generated speech features with a subsequence of frames taken from a real speech feature sequence.\nThis last augmentation is based on the fake-as-real GAN method  and is important to prevent gradient explosion later in training. For the anti-aliasing LPF filters we use\nwindowed sinc filters with a width-9 Kaiser window .\nFor the generator, the first Style Block has a cutoff\nat\nfc=0.125â€‹cycles/samplesubscriptğ‘“ğ‘0.125cycles/samplef_{c}=0.125\\ \\text{cycles/sample} which is increased in an even logarithmic scale to fc=0.45â€‹cycles/samplesubscriptğ‘“ğ‘0.45cycles/samplef_{c}=0.45\\ \\text{cycles/sample} in the second-to-last layer, keeping this value for the last two layers to fill in the last high frequency detail.\nEven in these last layers we use a cutoff below the Nyquist frequency.\nFor the discriminator we are less concerned about aliasing as\nit does not generate a sequence, so we\nuse a\nfc=0.5â€‹cycles/samplesubscriptğ‘“ğ‘0.5cycles/samplef_{c}=0.5\\ \\text{cycles/sample}\ncutoff\nfor all ConvD Blocks. All models are trained on a single NVIDIA Quadro RTX 6000 using PyTorch 1.11."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Adversarial Audio Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Chris Donahue, Julian McAuley, and Miller Puckette",
      "orig_title": "Adversarial audio synthesis",
      "paper_id": "1802.04208v3"
    },
    {
      "index": 1,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter",
      "orig_title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 2,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 3,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, etÂ al.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 4,
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.06125",
      "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen",
      "orig_title": "Hierarchical text-conditional image generation with CLIP latents",
      "paper_id": "2204.06125v1"
    },
    {
      "index": 5,
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.11487",
      "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, etÂ al.",
      "orig_title": "Photorealistic text-to-image diffusion models with deep language understanding",
      "paper_id": "2205.11487v1"
    },
    {
      "index": 6,
      "title": "Itâ€™s Raw! Audio Generation with State-Space Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.09729",
      "authors": "Karan Goel, Albert Gu, Chris Donahue, and Christopher RÃ©",
      "orig_title": "Itâ€™s raw! Audio generation with state-space models",
      "paper_id": "2202.09729v1"
    },
    {
      "index": 7,
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.09761",
      "authors": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro"
    },
    {
      "index": 8,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, etÂ al."
    },
    {
      "index": 9,
      "title": "Generative Adversarial Phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Frontiers in artificial intelligence",
      "authors": "GaÅ¡per BeguÅ¡",
      "orig_title": "Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "paper_id": "2006.03965v1"
    },
    {
      "index": 10,
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, and Timo Aila",
      "orig_title": "A style-based generator architecture for generative adversarial networks",
      "paper_id": "1812.04948v3"
    },
    {
      "index": 11,
      "title": "Analyzing and Improving the Image Quality of StyleGAN",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, etÂ al.",
      "orig_title": "Analyzing and improving the image quality of StyleGAN",
      "paper_id": "1912.04958v2"
    },
    {
      "index": 12,
      "title": "Alias-Free Generative Adversarial Networks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik HÃ¤rkÃ¶nen, Janne Hellsten, etÂ al.",
      "orig_title": "Alias-free generative adversarial networks",
      "paper_id": "2106.12423v4"
    },
    {
      "index": 13,
      "title": "Training Generative Adversarial Networks with Limited Data",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, etÂ al.",
      "orig_title": "Training generative adversarial networks with limited data",
      "paper_id": "2006.06676v2"
    },
    {
      "index": 14,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, etÂ al.",
      "orig_title": "Improved techniques for training GANs",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 15,
      "title": "Activation Maximization Generative Adversarial Nets",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, etÂ al.",
      "orig_title": "Activation maximization generative adversarial nets",
      "paper_id": "1703.02000v9"
    },
    {
      "index": 16,
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.03209",
      "authors": "Pete Warden"
    },
    {
      "index": 17,
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.07447",
      "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-HungÂ Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, etÂ al.",
      "orig_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "paper_id": "2106.07447v1"
    },
    {
      "index": 18,
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "abstract": "",
      "year": "2021",
      "venue": "Interspeech",
      "authors": "Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux"
    },
    {
      "index": 19,
      "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, etÂ al.",
      "orig_title": "Text-free prosody-aware generative spoken language modeling",
      "paper_id": "2109.03264v2"
    },
    {
      "index": 20,
      "title": "Generative spoken language modeling from raw audio",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.01192",
      "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, etÂ al."
    },
    {
      "index": 21,
      "title": "WaveNet: A Generative Model for Raw Audio",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.03499",
      "authors": "Aaron vanÂ den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, etÂ al.",
      "orig_title": "WaveNet: A generative model for raw audio",
      "paper_id": "1609.03499v2"
    },
    {
      "index": 22,
      "title": "Which training methods for GANs do actually converge?",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin"
    },
    {
      "index": 23,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 24,
      "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Computer Speech & Language",
      "authors": "GaÅ¡per BeguÅ¡",
      "orig_title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks",
      "paper_id": "2009.12711v2"
    },
    {
      "index": 25,
      "title": "Generating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms",
      "abstract": "",
      "year": "2022",
      "venue": "ICML ExVo Generate",
      "authors": "Marco Jiralerspong and Gauthier Gidel",
      "orig_title": "Generating diverse vocal bursts with StyleGAN2 and mel-spectrograms",
      "paper_id": "2206.12563v1"
    },
    {
      "index": 26,
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae",
      "orig_title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "paper_id": "2010.05646v2"
    },
    {
      "index": 27,
      "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Matthew Tancik, PratulÂ P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, etÂ al.",
      "orig_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
      "paper_id": "2006.10739v1"
    },
    {
      "index": 28,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Saining Xie, RossÂ B Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 29,
      "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Swaminathan Gurumurthy, Ravi KiranÂ Sarvadevabhatla, and RÂ VenkateshÂ Babu",
      "orig_title": "DeLiGAN: Generative adversarial networks for diverse and limited data",
      "paper_id": "1706.02071v1"
    },
    {
      "index": 30,
      "title": "The VoiceMOS Challenge 2022",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11389",
      "authors": "Wen-Chin Huang, Erica Cooper, YuÂ Tsao, Hsin-Min Wang, Tomoki Toda, etÂ al.",
      "orig_title": "The VoiceMOS challenge 2022",
      "paper_id": "2203.11389v3"
    },
    {
      "index": 31,
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "abstract": "",
      "year": "2015",
      "venue": "ICASSP",
      "authors": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur"
    },
    {
      "index": 32,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "DiederikÂ P. Kingma and Jimmy Ba"
    },
    {
      "index": 33,
      "title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Song Tao and JÂ Wang",
      "orig_title": "Alleviation of gradient exploding in GANs: Fake can be real",
      "paper_id": "1912.12485v2"
    },
    {
      "index": 34,
      "title": "Digital filters",
      "abstract": "",
      "year": "1966",
      "venue": "System analysis by digital computer",
      "authors": "JamesÂ F Kaiser"
    }
  ]
}