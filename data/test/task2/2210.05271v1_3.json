{
  "paper_id": "2210.05271v1",
  "title": "GAN you hear me? Reclaiming unconditional speech synthesis from diffusion models",
  "sections": {
    "asgan implementation": "We train two variants of our model: a log mel-spectrogram based model and a HuBERT feature based model .\nThe former\nis shown in Fig. 1, where\nthe model outputs\n128 mel-frequency bins at\na hop and window size of\n10 ms and 64 ms, respectively.\nThe HuBERT\nmodel is identical\nexcept\nthat it only uses\nhalf the sequence length\n(since HuBERT features are 20 ms instead of the 10 ms spectrogram frames)\nand\nhas\na different number\nof output channels in the four groups of Style Blocks:    1024768512512    convolution channels instead of       . The HiFi-GAN vocoder for both the HuBERT and mel-spectrogram\nmodel is\nbased on the original\nimplementation [ref]27.\nThe HuBERT HiFi-GAN is trained on the Librispeech train-clean-100 subset \nto vocode activations from layer 6 of the\nHuBERT Base model .\nThe mel-spectrogram HiFi-GAN is trained on the Google Speech Commands dataset. Both\nASGAN variants are trained with Adam  (β1=0,β2=0.99formulae-sequencesubscript𝛽10subscript𝛽20.99\\beta_{1}=0,\\beta_{2}=0.99), clipping gradient norms at 10, and a learning rate of 3⋅10−3⋅3superscript1033\\cdot 10^{-3} for 520k iterations with a batch size of 32.\nSeveral\ntricks are\nused to stabilize GAN training:\n(i) equalized learning rate\n[ref]24, (ii) leaky ReLU activations with α=0.1𝛼0.1\\alpha=0.1, (iii) exponential moving averaging\nfor the generator weights [ref]24, (iv) R1subscript𝑅1R_{1} regularization , and (v)\na 0.01-times smaller learning rate for\nthe mapping network W𝑊W\n. We also introduce a new technique for updating the discriminator.\nConcretely, we first scale D𝐷D’s learning rate by 0.1 compared to the generator as otherwise we find it overwhelms\nG𝐺G\nearly on in training.\nAdditionally we employ a dynamic method for updating D𝐷D, inspired by adaptive discriminator augmentation [ref]14:\nduring each iteration, we skip D𝐷D’s update with probability p𝑝p.\nThe probability p𝑝p is initialized at 0.1 and is updated every 16th generator step or whenever the discriminator is updated.\nWe keep a running average rtsubscript𝑟𝑡r_{t} of the proportion of D𝐷D’s outputs on real data D​(X)𝐷𝑋D(X) that are positive (i.e. that D𝐷D can confidently identify as real).\nThen, if rtsubscript𝑟𝑡r_{t} is greater than 0.6 we increment p𝑝p by 0.05 (capped at 1.0), and if rtsubscript𝑟𝑡r_{t} is less than 0.6 we decrease p𝑝p by 0.05 (capped at 0.0).\nIn this way we adaptively skip discriminator updates. When D𝐷D becomes too strong, rtsubscript𝑟𝑡r_{t} and p𝑝p rise, and so D𝐷D is updated less frequently. When D𝐷D becomes too weak (i.e. fails to distinguish between real and fake inputs), then the opposite happens.\nWe found this new modification to be critical for ensuring that the D𝐷D does not overwhelm G𝐺G\nduring training. We also use the traditional adaptive discriminator augmentation [ref]14\nwhere we apply the following transforms with the same probability p𝑝p:\n(i) adding Gaussian noise with σ=0.05𝜎0.05\\sigma=0.05, (ii) random scaling by a factor of 1±0.05plus-or-minus10.051\\pm 0.05, and (iii) randomly replacing a subsequence of frames from the generated speech features with a subsequence of frames taken from a real speech feature sequence.\nThis last augmentation is based on the fake-as-real GAN method  and is important to prevent gradient explosion later in training. For the anti-aliasing LPF filters we use\nwindowed sinc filters with a width-9 Kaiser window .\nFor the generator, the first Style Block has a cutoff\nat\nfc=0.125​cycles/samplesubscript𝑓𝑐0.125cycles/samplef_{c}=0.125\\ \\text{cycles/sample} which is increased in an even logarithmic scale to fc=0.45​cycles/samplesubscript𝑓𝑐0.45cycles/samplef_{c}=0.45\\ \\text{cycles/sample} in the second-to-last layer, keeping this value for the last two layers to fill in the last high frequency detail.\nEven in these last layers we use a cutoff below the Nyquist frequency.\nFor the discriminator we are less concerned about aliasing as\nit does not generate a sequence, so we\nuse a\nfc=0.5​cycles/samplesubscript𝑓𝑐0.5cycles/samplef_{c}=0.5\\ \\text{cycles/sample}\ncutoff\nfor all ConvD Blocks. All models are trained on a single NVIDIA Quadro RTX 6000 using PyTorch 1.11."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Adversarial Audio Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Chris Donahue, Julian McAuley, and Miller Puckette",
      "orig_title": "Adversarial audio synthesis",
      "paper_id": "1802.04208v3"
    },
    {
      "index": 1,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter",
      "orig_title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 2,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 3,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, et al.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 4,
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.06125",
      "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen",
      "orig_title": "Hierarchical text-conditional image generation with CLIP latents",
      "paper_id": "2204.06125v1"
    },
    {
      "index": 5,
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.11487",
      "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, et al.",
      "orig_title": "Photorealistic text-to-image diffusion models with deep language understanding",
      "paper_id": "2205.11487v1"
    },
    {
      "index": 6,
      "title": "It’s Raw! Audio Generation with State-Space Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.09729",
      "authors": "Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré",
      "orig_title": "It’s raw! Audio generation with state-space models",
      "paper_id": "2202.09729v1"
    },
    {
      "index": 7,
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.09761",
      "authors": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro"
    },
    {
      "index": 8,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, et al."
    },
    {
      "index": 9,
      "title": "Generative Adversarial Phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Frontiers in artificial intelligence",
      "authors": "Gašper Beguš",
      "orig_title": "Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "paper_id": "2006.03965v1"
    },
    {
      "index": 10,
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, and Timo Aila",
      "orig_title": "A style-based generator architecture for generative adversarial networks",
      "paper_id": "1812.04948v3"
    },
    {
      "index": 11,
      "title": "Analyzing and Improving the Image Quality of StyleGAN",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, et al.",
      "orig_title": "Analyzing and improving the image quality of StyleGAN",
      "paper_id": "1912.04958v2"
    },
    {
      "index": 12,
      "title": "Alias-Free Generative Adversarial Networks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, et al.",
      "orig_title": "Alias-free generative adversarial networks",
      "paper_id": "2106.12423v4"
    },
    {
      "index": 13,
      "title": "Training Generative Adversarial Networks with Limited Data",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, et al.",
      "orig_title": "Training generative adversarial networks with limited data",
      "paper_id": "2006.06676v2"
    },
    {
      "index": 14,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, et al.",
      "orig_title": "Improved techniques for training GANs",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 15,
      "title": "Activation Maximization Generative Adversarial Nets",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, et al.",
      "orig_title": "Activation maximization generative adversarial nets",
      "paper_id": "1703.02000v9"
    },
    {
      "index": 16,
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.03209",
      "authors": "Pete Warden"
    },
    {
      "index": 17,
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.07447",
      "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, et al.",
      "orig_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "paper_id": "2106.07447v1"
    },
    {
      "index": 18,
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "abstract": "",
      "year": "2021",
      "venue": "Interspeech",
      "authors": "Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux"
    },
    {
      "index": 19,
      "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, et al.",
      "orig_title": "Text-free prosody-aware generative spoken language modeling",
      "paper_id": "2109.03264v2"
    },
    {
      "index": 20,
      "title": "Generative spoken language modeling from raw audio",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.01192",
      "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, et al."
    },
    {
      "index": 21,
      "title": "WaveNet: A Generative Model for Raw Audio",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.03499",
      "authors": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, et al.",
      "orig_title": "WaveNet: A generative model for raw audio",
      "paper_id": "1609.03499v2"
    },
    {
      "index": 22,
      "title": "Which training methods for GANs do actually converge?",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin"
    },
    {
      "index": 23,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 24,
      "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Computer Speech & Language",
      "authors": "Gašper Beguš",
      "orig_title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks",
      "paper_id": "2009.12711v2"
    },
    {
      "index": 25,
      "title": "Generating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms",
      "abstract": "",
      "year": "2022",
      "venue": "ICML ExVo Generate",
      "authors": "Marco Jiralerspong and Gauthier Gidel",
      "orig_title": "Generating diverse vocal bursts with StyleGAN2 and mel-spectrograms",
      "paper_id": "2206.12563v1"
    },
    {
      "index": 26,
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae",
      "orig_title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "paper_id": "2010.05646v2"
    },
    {
      "index": 27,
      "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, et al.",
      "orig_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
      "paper_id": "2006.10739v1"
    },
    {
      "index": 28,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Saining Xie, Ross B Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 29,
      "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu",
      "orig_title": "DeLiGAN: Generative adversarial networks for diverse and limited data",
      "paper_id": "1706.02071v1"
    },
    {
      "index": 30,
      "title": "The VoiceMOS Challenge 2022",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11389",
      "authors": "Wen-Chin Huang, Erica Cooper, Yu Tsao, Hsin-Min Wang, Tomoki Toda, et al.",
      "orig_title": "The VoiceMOS challenge 2022",
      "paper_id": "2203.11389v3"
    },
    {
      "index": 31,
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "abstract": "",
      "year": "2015",
      "venue": "ICASSP",
      "authors": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur"
    },
    {
      "index": 32,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Diederik P. Kingma and Jimmy Ba"
    },
    {
      "index": 33,
      "title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Song Tao and J Wang",
      "orig_title": "Alleviation of gradient exploding in GANs: Fake can be real",
      "paper_id": "1912.12485v2"
    },
    {
      "index": 34,
      "title": "Digital filters",
      "abstract": "",
      "year": "1966",
      "venue": "System analysis by digital computer",
      "authors": "James F Kaiser"
    }
  ]
}