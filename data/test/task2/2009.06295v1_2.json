{
  "paper_id": "2009.06295v1",
  "title": "Deep intrinsic decomposition trained on surreal scenes yet with realistic light effects",
  "sections": {
    "datasets for intrinsic decomposition": "The main problems regarding datasets for intrinsic image estimation swings between the realism of the physical lighting properties of the scene, and the number of images they provide to be enough to train deep architectures. The performance of trained architectures usually rely on a large number of training samples and a high variable appearance between them. MIT Intrinsic was the first dataset in this domain, and it is a non synthetic dataset, it was captured under very controlled conditions. It has 202020 objects, captured in 111111 different lighting conditions, although only one shading (white object acquisition) and one reflectance pair is provided the final dataset ground-truth. Different intrinsic image versions can be generated by introducing a scalar Œ±ùõº\\alpha that minimizes the difference, I‚Äã(x,y)‚àí(Œ±‚ÄãR‚Äãe‚Äã(x,y)‚ãÖS‚Äãh‚Äã(x,y))ùêºùë•ùë¶‚ãÖùõºùëÖùëíùë•ùë¶ùëÜ‚Ñéùë•ùë¶I(x,y)-(\\alpha Re(x,y)\\cdot Sh(x,y)), since even in such controlled environment the product model does not hold for all pixels in objects. The ground-truth is just provided for the pixels in the object mask. MPI Sintel is a synthetic dataset based on an animated movie. It has 181818 scenes with 505050 frames each, except one that has 404040, totaling 890890890 images. Sintel has been the first large dataset giving the opportunity to train deep architectures in the last years. However, it presents unnatural shading, and some color bias mostly on blue and brown colors. Thus, the generalization of networks trained on Sintel are affected by these color biases. And, like in all synthetic datasets, some erroneous pixels are found around boundary of objects. IIW: Intrinsic Images in the Wild is a large and realistic dataset of 5230 images. It only provides sparse reflectance pairwise judgments as training data. These judgments does not present a spatial coherent map for reflectance. Consequently, networks trained using this dataset present too smooth reflectance estimation with lack of texture variation. MIII dataset proposed by Beighpour et al. 9 is the extension of the MIT Intinsic to a multi illuminant intrinsic dataset. This work is a mixture of a realistic and synthetic framework. First they do a 3‚ÄãD3ùê∑3D reconstruction of the objects and make a scene of multiple objects in a synthetic world. Objects are colored in a synthetic world and captured in different lighting conditions. Dataset has 555 scenes and each scene is captured in 151515 different illumination conditions giving the total of 757575 images. Only one reflectance, specular and depth image is provided for each scene. Although their approach is very original and the dataset can be easily scaled by increasing light colors, objects and viewpoints, the 3‚ÄãD3ùê∑3D reconstruction based on computer vision algorithms of the objects is not perfect. Small differences between real and reconstructed object edges, can ultimately result in erroneous shading and reflectance in some parts of the image. ShapeNet[ref]5 is the first large dataset in this field with 330,000330000330,000 images based on the Shapenet 3‚ÄãD3ùê∑3D objects dataset . They used environmental maps in render software to create shading, reflectance and specular component of objects. The final ground-truth is only based on masked regions of objects without cast shadows information. Baslamisli et al. in [ref]6, follow a similar approach to the previous one. They created an intrinsic dataset of 20,0002000020,000 images called Shapenet-Intrinsic. Instead of using original shapenet object textures, they used homogeneous color reflectances for each object part to have more reflectance variation and to disassociate the shape from texture. The final dataset is again only based on masked object region enlightened by environmental maps. Baslamisli et al. in  present a new synthetic dataset of 35‚ÄãK35ùêæ35K images with new synthetic natural objects to form more complex natural scenes. The ground-truth not only provides intrinsic components but also annotations for semantic segmentation purpose. Images are created using sky HDR environmental maps with parallel lighting in the background . These maps introduce different daylight conditions such as clear sky, cloudy, night etc. The whole foreground area has been considerably increased with respect to the previous two mentioned datasets, sky portion areas are masked out in the ground-truth images. CGIntrinsics[ref]8 is another recent synthetic ground-truth of 20,0002000020,000 images. They use the 3‚ÄãD3ùê∑3D indoor objects of SUNCG dataset 0 with 3D textures. It contains images representing complex indoor scenes of objects, combining a mixture of indoor and outdoor illumination. Outdoor illumination sources are also based on HDR environmental maps. Both indoor and outdoor lighting sources are masked out in ground-truth. InteriorNet is a very large scale dataset recently presented in 1. It is formed by 202020 milions of images with an extense ground-truth for a variety of applications such as segmentation, object boundary detection, depth map estimation or motion blur removal. They also provide an interactive simulator ViSIM and the rendering software to randomly select objects or change lighting configuration beyond more tools. Although this dataset is not just made for intrinsic decomposition, it provides a good starting point to create a large photo-realistic and physically consistent dataset. In its current state the dataset does not provide intrinsic ground-truth data and the product model is not held. Before to sum up on current datasets it is worth mentioning an additional way to solve the lack of data for training, it is using specific data-augmentation for intrinsic estimation, as suggested by Sial et al.. in 2. They propose chromaticity rotation to add new reflectance images while keeping the same shading. It is a flexible augmentation strategy that can considerably increase the size of the dataset while reducing the problem of color bias in some datasets. To conclude the review of the different available datasets we provide a complete comparison in Table 1 where we analyse the different datasets according to several properties we have organized in columns referring to: (a) the size or the number of available images with the corresponding ground-truth data; (b) if the dataset fulfills the product model given by Equation1; (c) if the full image can be used in the training, with no mask that reduces the number of samples and meaningful spatial coherence in the training stages; (d) if the ground-truth captures the influence of a diversified background that provoke a large diversity of lighting effects on the object surfaces; (e) if the ground-truth images present cast shadows that add realism to the full scene; (f) if the global lighting is physically consistent with real interaction between all the scene objects, environmental maps usually alter this coherence. Size (#‚ÄãI‚Äãm‚Äãa‚Äãg‚Äãe‚Äãs)#ùêºùëöùëéùëîùëíùë†(\\#Images) Model Fulfillment Training on full Image Diversified Background Cast Shadows Consistent Lighting From Table1 we can conclude that the majority of available datasets are synthetic, only the first two rows correspond to realistic ones. MIT is the only realistic dataset that provides full reflectance and shading images, since IIW only give reflectance judgement data for specific pairs. MII is a synthetic dataset carefully created for intrinsic decomposition, whereas Sintel was not created for this problem but has been one of the most used, since it was the first presenting an enough number of images to train deep architectures, its main problem is the high level of correlation between all the images. The three subsequent rows in Table 1 correspond to Shapenet-based datasets, including our proposal. ShapeNet [ref]5 emerged as a tool to create new larger datasets where synthetic objects are located in multiple different environmental maps. Following this idea, Baslamisli et al. [ref]6 used the same approach but using homogeneous reflectance for each object mesh. In both cases the ground-truth is just given by the object area. However, our proposal, also based on Shapenet, uses a single reflectance per mesh like in [ref]6, but substituting environmental maps by multiple elements in the scene surrounding the object that inserts a diversified background, extends the training area to the full image, and adds realism to light effects thanks to shadows and to the physical consistency on rendered light effects. This dataset is presented here as a tunable baseline to easily generate a high diversity of ligthing conditions, that can be adapted depending on the task at hand. We explain all the details on how this dataset is built in the next section. At the bottom of table 1 we have grouped 3 recent datasets that increase the complexity of the scenes, extend the ground-truth to larger areas that can contain cast shadows, but keeping environmental maps in some other parts, which can not be included in the ground-truth and provoke some lack of coherence in the global lighting of the scene. These datasets can be used for more generic applications where a high accuracy in the estimation of light conditions is not required."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Recovering intrinsic scene characteristics",
      "abstract": "",
      "year": "1978",
      "venue": "Comput. Vis. Syst",
      "authors": "H. Barrow and J. Tenenbaum"
    },
    {
      "index": 1,
      "title": "Ground-truth dataset and baseline evaluations for intrinsic image algorithms",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Computer Vision",
      "authors": "R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Freeman"
    },
    {
      "index": 2,
      "title": "Intrinsic images in the wild",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Trans. on Graphics (SIGGRAPH)",
      "authors": "S. Bell, K. Bala, and N. Snavely"
    },
    {
      "index": 3,
      "title": "A naturalistic open source movie for optical flow evaluation",
      "abstract": "",
      "year": "2012",
      "venue": "European Conf. on Computer Vision (ECCV)",
      "authors": "D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black"
    },
    {
      "index": 4,
      "title": "Learning non-lambertian object intrinsics across shapenet categories",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "J. Shi, Y. Dong, H. Su, and S. X. Yu"
    },
    {
      "index": 5,
      "title": "CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition",
      "abstract": "",
      "year": "2018",
      "venue": "Computer Vision and Pattern Recognition",
      "authors": "A. S. Baslamisli, H. Le, and T. Gevers",
      "orig_title": "CNN based learning using reflection and retinex models for intrinsic image decomposition",
      "paper_id": "1712.01056v2"
    },
    {
      "index": 6,
      "title": "Joint Learning of Intrinsic Images and Semantic Segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "A. S. Baslamisli, T. T. Groenestege, P. Das, H. A. Le, S. Karaoglu, and T. Gevers",
      "orig_title": "Joint learning of intrinsic images and semantic segmentation",
      "paper_id": "1807.11857v1"
    },
    {
      "index": 7,
      "title": "Cgintrinsics: Better intrinsic image decomposition through physically-based rendering",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Z. Li and N. Snavely"
    },
    {
      "index": 8,
      "title": "ShapeNet: An Information-Rich 3D Model Repository",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu",
      "orig_title": "Shapenet: An information-rich 3d model repository",
      "paper_id": "1512.03012v1"
    },
    {
      "index": 9,
      "title": "Lightness and retinex theory",
      "abstract": "",
      "year": "1971",
      "venue": "Journal of the Optical Society of America",
      "authors": "E. H. Land and J. McCann"
    },
    {
      "index": 10,
      "title": "Determining lightness from an image",
      "abstract": "",
      "year": "1974",
      "venue": "Computer graphics and image processing",
      "authors": "B. K. Horn"
    },
    {
      "index": 11,
      "title": "Deriving intrinsic images from image sequences",
      "abstract": "",
      "year": "2001",
      "venue": "International Conference on Computer Vision",
      "authors": "Y. Weiss"
    },
    {
      "index": 12,
      "title": "Recovering intrinsic images from a single image",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "M. F. Tappen, W. T. Freeman, and E. H. Adelson"
    },
    {
      "index": 13,
      "title": "Estimating intrinsic component images using non-linear regression",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. F. Tappen, E. H. Adelson, and W. T. Freeman"
    },
    {
      "index": 14,
      "title": "Recovering shading from color images",
      "abstract": "",
      "year": "1992",
      "venue": "European Conference on Computer Vision",
      "authors": "B. Funt, M. Drew, and M. Brockington"
    },
    {
      "index": 15,
      "title": "Recovering intrinsic images with a global sparsity prior on reflectance",
      "abstract": "",
      "year": "2011",
      "venue": "Neural Information Processing Systems",
      "authors": "P. V. Gehler, C. Rother, M. Kiefel, L. Zhang, and B. Sch√∂lkopf"
    },
    {
      "index": 16,
      "title": "Intrinsic image decomposition using a sparse representation of reflectance",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "L. Shen, C. Yeo, and B.-S. Hua"
    },
    {
      "index": 17,
      "title": "Shape, Illumination, and Reflectance from Shading",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "J. T. Barron and J. Malik",
      "orig_title": "Shape, illumination, and reflectance from shading",
      "paper_id": "2010.03592v1"
    },
    {
      "index": 18,
      "title": "Names and shades of color for intrinsic image estimation",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. Serra, O. Penacchio, R. Benavente, and M. Vanrell"
    },
    {
      "index": 19,
      "title": "Intrinsic scene properties from a single rgb-d image",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. T. Barron and J. Malik"
    },
    {
      "index": 20,
      "title": "Estimation of intrinsic image sequences from image depth video",
      "abstract": "",
      "year": "2012",
      "venue": "European Conference on Computer Vision",
      "authors": "K. J. Lee, Q. Zhao, X. Tong, M. Gong, S. Izadi, S. U. Lee, P. Tan, and S. Lin"
    },
    {
      "index": 21,
      "title": "A simple model for intrinsic image decomposition with depth cues",
      "abstract": "",
      "year": "2013",
      "venue": "Computer Vision (ICCV)",
      "authors": "Q. Chen and V. Koltun"
    },
    {
      "index": 22,
      "title": "Intrinsic image decomposition using structure-texture separation and surface normals",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision ‚Äì ECCV",
      "authors": "J. Jeon, S. Cho, X. Tong, and S. Lee"
    },
    {
      "index": 23,
      "title": "Estimating intrinsic images from image sequences with biased illumination",
      "abstract": "",
      "year": "2004",
      "venue": "Computer Vision - ECCV",
      "authors": "Y. Matsushita, S. Lin, S. B. Kang, and H.-Y. Shum"
    },
    {
      "index": 24,
      "title": "Intrinsic decomposition of image sequences from local temporal variations",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "authors": "P. Laffont and J. Bazin"
    },
    {
      "index": 25,
      "title": "Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Computer Vision (ICCV)",
      "authors": "T. Narihira, M. Maire, and S. X. Yu",
      "orig_title": "Direct intrinsics: Learning albedo-shading decomposition by convolutional regression",
      "paper_id": "1512.02311v1"
    },
    {
      "index": 26,
      "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS",
      "authors": "D. Eigen, C. Puhrsch, and R. Fergus",
      "orig_title": "Depth map prediction from a single image using a multi-scale deep network",
      "paper_id": "1406.2283v1"
    },
    {
      "index": 27,
      "title": "Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "authors": "T. Zhou, P. Kr√§henb√ºhl, and A. A. Efros",
      "orig_title": "Learning data-driven reflectance priors for intrinsic image decomposition",
      "paper_id": "1510.02413v1"
    },
    {
      "index": 28,
      "title": "Scene intrinsics and depth from a single image",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV) Workshops",
      "authors": "E. Shelhamer, J. T. Barron, and T. Darrell"
    },
    {
      "index": 29,
      "title": "Deep convolutional neural fields for depth estimation from a single image",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "F. Liu, C. Shen, and G. Lin"
    },
    {
      "index": 30,
      "title": "Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "S. Kim, K. Park, K. Sohn, and S. Lin",
      "orig_title": "Unified depth prediction and intrinsic image decomposition from a single image via joint convolutional neural fields",
      "paper_id": "1603.06359v1"
    },
    {
      "index": 31,
      "title": "Reflectance adaptive filtering improves intrinsic image estimation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "T. Nestmeyer and P. V. Gehler"
    },
    {
      "index": 32,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS",
      "authors": "I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio"
    },
    {
      "index": 33,
      "title": "DARN: a Deep Adversarial Residual Network for Intrinsic Image Decomposition",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "authors": "L. Lettry, K. Vanhoey, and L. V. Gool",
      "orig_title": "Darn: A deep adversarial residual network for intrinsic image decomposition",
      "paper_id": "1612.07899v2"
    },
    {
      "index": 34,
      "title": "Single image intrinsic decomposition without a single intrinsic image",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "W.-C. Ma, H. Chu, B. Zhou, R. Urtasun, and A. Torralba"
    },
    {
      "index": 35,
      "title": "Signature verification using a siamese time delay neural network",
      "abstract": "",
      "year": "1993",
      "venue": "IJPRAI",
      "authors": "J. Bromley, I. Guyon, Y. LeCun, E. S√§ckinger, and R. Shah"
    },
    {
      "index": 36,
      "title": "Learning Intrinsic Image Decomposition from Watching the World",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Z. Li and N. Snavely",
      "orig_title": "Learning intrinsic image decomposition from watching the world",
      "paper_id": "1804.00582v1"
    },
    {
      "index": 37,
      "title": "Revisiting Deep Intrinsic Image Decompositions",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Q. Fan, J. Yang, G. Hua, B. Chen, and D. P. Wipf",
      "orig_title": "Revisiting deep intrinsic image decompositions",
      "paper_id": "1701.02965v8"
    },
    {
      "index": 38,
      "title": "Multi-view multi-illuminant intrinsic dataset",
      "abstract": "",
      "year": "2016",
      "venue": "BMVC",
      "authors": "S. Beigpour, M. L. Ha, S. Kunz, A. Kolb, and V. Blanz"
    },
    {
      "index": 39,
      "title": "Semantic scene completion from a single depth image",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser"
    },
    {
      "index": 40,
      "title": "InteriorNet: Mega-scale Multi-sensor =‚ÄìPhoto-realistic Indoor Scenes Dataset",
      "abstract": "",
      "year": "2018",
      "venue": "British Machine Vision Conference (BMVC)",
      "authors": "W. Li, S. Saeedi, J. McCormac, R. Clark, D. Tzoumanikas, Q. Ye, Y. Huang, R. Tang, and S. Leutenegger",
      "orig_title": "Interiornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset",
      "paper_id": "1809.00716v1"
    },
    {
      "index": 41,
      "title": "Color-based data augmentation for reflectance estimation",
      "abstract": "",
      "year": "2018",
      "venue": "IS&T Color and Imaging Conference",
      "authors": "H. Sial, S. Sancho-Asensio, R. Baldrich, R. Benavente, and M. Vanrell"
    },
    {
      "index": 42,
      "title": "Corel datasets",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 43,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna"
    },
    {
      "index": 44,
      "title": "Keras",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "F. Chollet"
    },
    {
      "index": 45,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 46,
      "title": "Adam: A method for stochastic optimization.",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 47,
      "title": "Intrinsic images decomposition using a local and global sparse representation of reflectance",
      "abstract": "",
      "year": "2011",
      "venue": "CVPR",
      "authors": "L. Shen and C. Yeo"
    },
    {
      "index": 48,
      "title": "Intrinsic images by clustering",
      "abstract": "",
      "year": "2012",
      "venue": "Comput. Graph. Forum",
      "authors": "E. Garces, A. Munoz, J. Lopez-Moreno, and D. Gutierrez"
    },
    {
      "index": 49,
      "title": "A closed-form solution to retinex with nonlocal texture constraints",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "Q. Zhao, P. Tan, Q. Dai, L. Shen, E. Wu, and S. Lin"
    },
    {
      "index": 50,
      "title": "An l1 image transform for edge-preserving smoothing and scene-level intrinsic decomposition",
      "abstract": "",
      "year": "2015",
      "venue": "ACM Transactions on Graphics",
      "authors": "S. Bi, X. Han, and Y. Yu"
    }
  ]
}