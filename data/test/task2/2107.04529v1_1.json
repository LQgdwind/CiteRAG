{
  "paper_id": "2107.04529v1",
  "title": "Entropy, Information, and the Updating of Probabilities",
  "sections": {
    "introduction": "Inductive inference is a framework for coping with uncertainty, for reasoning\nwith incomplete information. The framework must include a means to represent a\nstate of partial knowledge ‚Äî this is handled through the introduction of\nprobabilities ‚Äî and it must allow us to change from one state of partial\nknowledge to another when new information becomes available. Indeed any\ninductive method that recognizes that a situation of incomplete information is\nin some way unfortunate ‚Äî by which we mean that it constitutes a problem in\nneed of a solution ‚Äî would be severely deficient if it failed to address the\nquestion of how to proceed in those fortunate circumstances when new\ninformation becomes available. The theory of probability, if it is to be\nuseful at all, demands a method for assigning and updating probabilities. The challenge is to develop updating methods that are both systematic,\nobjective and practical. When the information consists of data and a\nlikelihood function Bayesian updating is the uniquely natural method of\nchoice. Its foundation lies on recognizing the value of prior information:\nwhatever was learned in the past is valuable and should not be disregarded,\nwhich amounts to requiring that beliefs ought to be revised but only to the\nextent required by the new data. This immediately raises a number of\nquestions: How do we update when the information is not in the form of data?\nIf the information is not data, what else could it possibly be? Indeed what,\nafter all, is ‚Äòinformation‚Äô? On a separate line of development the method of\nMaximum Gibbs-Shannon Entropy (MaxEnt) allows one to process information in\nthe form of constraints on the allowed probability distributions. This\nprovides a partial answer to one of our questions: in addition to data,\ninformation can also take the form of constraints. But it immediately raises\nseveral other questions: What is the interpretation of entropy? Is there a\nunique entropy? Are Bayesian and entropic methods mutually compatible? The purpose of this paper is to review one particular approach to entropic\nupdating. The presentation below, which is meant to be pedagogical andOn a\nseparate line of development the method of Maximum Gibbs-Shannon Entropy\n(MaxEnt) allows one to process information inOn a separate line of development\nthe method of Maximum Gibbs-Shannon Entropy (MaxEnt) allows one to process\ninformation in self-contained, is based on work presented in a sequence of\npapers  and in the sets of lectures\n[ref]8. As we shall see\nbelow we adopt a pragmatic approach in which entropy is a tool designed for\nthe specific purpose of updating probabilities. Historically the method of Maximum relative Entropy (ME) is a direct\ndescendant of the MaxEnt method pioneered by Jaynes 0. In the MaxEnt framework entropy is interpreted\nthrough the Shannon axioms as a measure of the amount of information¬†that is\nmissing¬†in a probability distribution. This approach has its limitations. The\nShannon axioms refer to probabilities of discrete variables; for continuous\nvariables the Shannon entropy is not defined. A more serious objection is that\neven if we grant that the Shannon axioms do lead to a reasonable expression\nfor the entropy, to what extent do we believe the axioms themselves? Shannon‚Äôs\nthird axiom, the grouping property, is indeed sort of reasonable, but is it\nnecessary? Is entropy the only consistent measure of uncertainty or of\ninformation? Indeed, there exist examples in which the Shannon entropy does\nnot seem to reflect one‚Äôs intuitive notion of information [ref]81. One could introduce other entropies justified by\ndifferent choices of axioms (e.g., 234). But this move raises problems of its own: Which\nentropy should one adopt? If different systems are to handled using different\nentropies, how does one handle composite systems? From our perspective the problem can be traced to the fact that neither\nShannon nor Jaynes were concerned with the task of updating probabilities.\nShannon‚Äôs communication theory aimed to characterize the sources of\ninformation, to measure the capacity of the communication channels, and to\nlearn how to control the degrading effects of noise. On the other hand, Jaynes\nconceived MaxEnt as a method to assign probabilities on the basis of\nconstraint information and a fixed underlying measure and not from an\narbitrary prior. Considerations such as these motivated several attempts to develop ME directly\nas a method for updating probabilities without invoking questionable measures\nof information 567. The important contribution by\nShore and Johnson was the realization that one could axiomatize the updating\nmethod itself rather than the information measure. Their axioms have, however,\nraised criticisms 1890¬†and counter-criticisms [ref]812. Despite the controversies, Shore and Johnson‚Äôs\npioneering papers have had an enormous influence: they identified the correct\ngoal to be achieved. The concept of relative entropy is introduced as a tool for updating\nprobabilities. From now we drop the qualifier ‚Äúrelative‚Äùand adopt the simpler term ‚Äúentropy‚Äù. The reasons for the improved nomenclature are the\nfollowing: (1) the general concept should receive the general name\n‚Äúentropy‚Äù, while the more specialized\nconcepts should be the ones receiving a qualifier such as ‚Äúthermodynamic‚Äù¬†or ‚ÄúClausius‚Äù¬†entropy, and ‚ÄúGibbs-Shannon‚Äù¬†entropy. (2) All entropies are relative, even\nif they happen to be relative to an implicit uniform prior. Making this fact\nexplicit has tremendous pedagogical value. (3) The practice is already in use\nwith the concept of energy: all energies are relative too, but there is no\nadvantage in constantly referring to a ‚Äúrelative\nenergy‚Äù. Accordingly, ME will be read as ‚ÄúMaximum Entropy‚Äù; additional qualifiers are redundant. As with all tools, entropy too is designed to perform a certain\nfunction and its performance must meet certain design criteria\nor¬†specifications. There is no implication that the method is\n‚Äútrue‚Äù, or that it succeeds¬†because it\nachieves some special contact with reality. Instead the claim is that the\nmethod succeeds in the pragmatic sense that it works as designed ‚Äî and that\nthis is satisfactory because when properly deployed it leads to empirically\nadequate models. In this approach entropy needs no interpretation\nwhether it be in terms of heat, multiplicity of states, disorder, uncertainty,\nor even in terms of an amount of information. Incidentally, this may\nexplain why the search for the meaning of entropy has proved so elusive: we\nneed not know what ‚Äòentropy‚Äô means; we only need to know how to use it. Since our topic is the updating of probabilities when confronted with new\ninformation, our starting point is to address the question ‚Äòwhat is\ninformation?‚Äô. In Section 2 we develop a concept of information that is both\npragmatic and Bayesian. ‚ÄòInformation‚Äô is defined in terms of its effects on\nthe beliefs of rational agents. The design of entropy as a tool for updating\nis the topic of Section 3. There we state the design specifications that\ndefine what function entropy is supposed to perform and we derive its\nfunctional form. To streamline the presentation some of the mathematical\nderivations are left to the appendices. To conclude we present two further developments. In Section 4 we show that\nBayes‚Äô rule can be derived as a special case of the ME method. An earlier\nderivation of this important result following a different line of argument was\ngiven by Williams 3 before a sufficient understanding of\nentropy as an updating tool had been achieved. It is not, therefore,\nsurprising that Williams‚Äô achievement has not received the widespread\nappreciation it deserves. Thus, within the ME framework entropic and Bayesian\nmethods are unified into a single consistent theory of inference. One\nadvantage of this insight is that it allows a number of generalizations of\nBayes‚Äô rule [ref]8. Another is that it\nprovides an important missing piece for the old puzzles of quantum mechanics\nconcerning the so-called collapse of the wave function and the quantum\nmeasurement problem 45. There is yet another function that the ME method must perform in order to\nfully qualify as a method of inductive inference. Once we have decided that\nthe distribution of maximum entropy is to be preferred over all others the\nfollowing question arises immediately: the maximum of the entropy functional\nis never infinitely sharp, are we really confident that distributions that lie\nvery close to the maximum are totally ruled out? In Section 5 the ME method is\ndeployed to assess quantitatively the extent to which distributions with lower\nentropy are ruled out. The significance of this result is that it provides a\ndirect link to the theories of fluctuations and large deviations. Concluding\nremarks are given in Section 6."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Relative Entropy and Inductive Inference",
      "abstract": "",
      "year": "2004",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 1,
      "title": "Updating Probabilities",
      "abstract": "",
      "year": "2006",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha and A. Giffin"
    },
    {
      "index": 2,
      "title": "Information and Entropy",
      "abstract": "",
      "year": "2007",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 3,
      "title": "Towards an Informational Pragmatic Realism",
      "abstract": "",
      "year": "2014",
      "venue": "Mind and Machines",
      "authors": "A. Caticha"
    },
    {
      "index": 4,
      "title": "Entropic Updating of Probabilities and Density Matrices",
      "abstract": "",
      "year": "2017",
      "venue": "Entropy",
      "authors": "K. Vanslette"
    },
    {
      "index": 5,
      "title": "Lectures on Probability, Entropy, and Statistical Physics",
      "abstract": "",
      "year": "2008",
      "venue": "MaxEnt 2008, S√£o Paulo, Brazil",
      "authors": "A. Caticha"
    },
    {
      "index": 6,
      "title": "Entropic Inference and the Foundations of Physics",
      "abstract": "",
      "year": "2012",
      "venue": "EBEB 2012, S√£o Paulo, Brazil",
      "authors": "A. Caticha"
    },
    {
      "index": 7,
      "title": "Entropic Physics: Probability, Entropy, and the Foundations of Physics",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "A. Caticha"
    },
    {
      "index": 8,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "1957",
      "venue": "Phys. Rev.",
      "authors": "E. T. Jaynes",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 9,
      "title": "Papers on Probability, Statistics and Statistical Physics",
      "abstract": "",
      "year": "1983",
      "venue": "",
      "authors": "E. T. Jaynes"
    },
    {
      "index": 10,
      "title": "Can the Maximum Entropy Principle be explained as a consistency requirement?",
      "abstract": "",
      "year": "1995",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "J. Uffink"
    },
    {
      "index": 11,
      "title": "On measures of entropy and information",
      "abstract": "",
      "year": "1961",
      "venue": "Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability",
      "authors": "A. Renyi"
    },
    {
      "index": 12,
      "title": "On Measures of Information and their Characterizations",
      "abstract": "",
      "year": "1975",
      "venue": "",
      "authors": "J. Acz√©l and Z. Dar√≥czy"
    },
    {
      "index": 13,
      "title": "Possible Generalization of Boltzmann-Gibbs Statistics",
      "abstract": "",
      "year": "1988",
      "venue": "J. Stat. Phys.",
      "authors": "C. Tsallis"
    },
    {
      "index": 14,
      "title": "Axiomatic derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy",
      "abstract": "",
      "year": "1980",
      "venue": "IEEE Trans. Inf. Theory",
      "authors": "J. E. Shore and R. W. Johnson"
    },
    {
      "index": 15,
      "title": "The Axioms of Maximum Entropy",
      "abstract": "",
      "year": "1988",
      "venue": "Maximum-Entropy and Bayesian Methods in Science and Engineering",
      "authors": "J. Skilling"
    },
    {
      "index": 16,
      "title": "Classic Maximum Entropy",
      "abstract": "",
      "year": "1989",
      "venue": "Maximum Entropy and Bayesian Methods",
      "authors": "J. Skilling"
    },
    {
      "index": 17,
      "title": "On the axiomatic approach to the maximum entropy principle of inference",
      "abstract": "",
      "year": "1986",
      "venue": "Pramana ‚Äì J. Phys.",
      "authors": "S. N. Karbelkar"
    },
    {
      "index": 18,
      "title": "Conceptual Inadequacy of the Shore and Johnson Axioms for Wide Classes of Complex systems",
      "abstract": "",
      "year": "2015",
      "venue": "Entropy",
      "authors": "C. Tsallis"
    },
    {
      "index": 19,
      "title": "Maximum Entropy Principle in Statistical Inference: Case for Non-Shannonian Entropies",
      "abstract": "",
      "year": "2019",
      "venue": "Phys. Rev. Lett.",
      "authors": "P. Jizba and J. Korbel"
    },
    {
      "index": 20,
      "title": "Nonadditive Entropies Yield Probability Distributions with Biases not Warranted by the Data",
      "abstract": "",
      "year": "2013",
      "venue": "Phys. Rev. Lett.",
      "authors": "S. Press√©, K. Ghosh, J. Lee, and K. A. Dill"
    },
    {
      "index": 21,
      "title": "Reply to Tsallis‚Äô ‚ÄúConceptual inadequacy of the Shore and Johnson axioms for wide classes of complex systems",
      "abstract": "",
      "year": "2015",
      "venue": "Entropy",
      "authors": "S. Press√©, K. Ghosh, J. Lee, and K. A. Dill"
    },
    {
      "index": 22,
      "title": "Bayesian Conditionalization and the Principle of Minimum Relative Information",
      "abstract": "",
      "year": "1980",
      "venue": "Brit. J. Phil. Sci.",
      "authors": "P. M. Williams"
    },
    {
      "index": 23,
      "title": "Entropic dynamics and the quantum measurement problem",
      "abstract": "",
      "year": "2012",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "D. T. Johnson and A. Caticha"
    },
    {
      "index": 24,
      "title": "Quantum measurement and weak values in entropic quantum dynamics",
      "abstract": "",
      "year": "2017",
      "venue": "AIP Conf. Proc.",
      "authors": "K. Vanslette and A. Caticha"
    },
    {
      "index": 25,
      "title": "Elements of Information Theory",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": "T. Cover and J. Thomas"
    },
    {
      "index": 26,
      "title": "Foundations of Info-Metrics: Modeling, Inference, and Imperfect Information",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "A. Golan"
    },
    {
      "index": 27,
      "title": "Modeling by shortest data description",
      "abstract": "",
      "year": "1978",
      "venue": "Automatica",
      "authors": "J. Rissanen"
    },
    {
      "index": 28,
      "title": "Information is Physical",
      "abstract": "",
      "year": "1991",
      "venue": "Physics Today",
      "authors": "R. Landauer"
    },
    {
      "index": 29,
      "title": "The thermodynamics of computation‚ÄîA review",
      "abstract": "",
      "year": "1982",
      "venue": "Int. J. Th. Phys.",
      "authors": "C. Bennett"
    },
    {
      "index": 30,
      "title": "Notes on Landauer‚Äôs principle, reversible computation, and Maxwell‚Äôs demon",
      "abstract": "",
      "year": "2003",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "C. Bennett"
    },
    {
      "index": 31,
      "title": "Waiting for Landauer",
      "abstract": "",
      "year": "2011",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "J. D. Norton"
    },
    {
      "index": 32,
      "title": "The End of the Thermodynamics of Computation:¬†A No-Go Result",
      "abstract": "",
      "year": "2013",
      "venue": "Philosophy of Science",
      "authors": "J. D. Norton"
    },
    {
      "index": 33,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 34,
      "title": "Entropic dynamics on Gibbs statistical manifolds",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "P. Pessoa, F. X. Costa, and A. Caticha"
    },
    {
      "index": 35,
      "title": "The Entropic Dynamics approach to Quantum Mechanics",
      "abstract": "",
      "year": "2019",
      "venue": "Entropy",
      "authors": "A. Caticha"
    },
    {
      "index": 36,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 37,
      "title": "Lattice duality: The origin of probability and entropy",
      "abstract": "",
      "year": "2005",
      "venue": "Neurocomputing",
      "authors": "K. H. Knuth"
    },
    {
      "index": 38,
      "title": "Foundations of Inference",
      "abstract": "",
      "year": "2012",
      "venue": "Axioms",
      "authors": "K. H. Knuth, J. Skilling"
    },
    {
      "index": 39,
      "title": "Updating Probabilities with Data and Moments",
      "abstract": "",
      "year": "2007",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Giffin and A. Caticha"
    },
    {
      "index": 40,
      "title": "Differential-Geometrical Methods in Statistics",
      "abstract": "",
      "year": "1985",
      "venue": "",
      "authors": "S. Amari"
    },
    {
      "index": 41,
      "title": "Maximum entropy and Bayesian data analysis: entropic prior distributions",
      "abstract": "",
      "year": "2004",
      "venue": "Phys. Rev. E",
      "authors": "A. Caticha and R. Preuss"
    },
    {
      "index": 42,
      "title": "Maximum Probability and Maximum Entropy Methods: Bayesian interpretation",
      "abstract": "",
      "year": "2004",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "M. Grendar, Jr. and M. Grendar"
    },
    {
      "index": 43,
      "title": "Tsallis maximum entropy principle and the law of large numbers",
      "abstract": "",
      "year": "2000",
      "venue": "Phys. Rev E",
      "authors": "B.R. La Cour and W. C. Schieve"
    },
    {
      "index": 44,
      "title": "Critique of qùëûq-entropy for thermal statistics",
      "abstract": "",
      "year": "2003",
      "venue": "Phys. Rev E",
      "authors": "M. Nauenberg"
    },
    {
      "index": 45,
      "title": "From Gibbs microcanonical ensemble to Tsallis generalized canonical distribution",
      "abstract": "",
      "year": "1994",
      "venue": "Phys. Lett. A",
      "authors": "A. R. Plastino and A. Plastino"
    },
    {
      "index": 46,
      "title": "Dynamical Foundations of nonextensive Statistical Mechanics",
      "abstract": "",
      "year": "2001",
      "venue": "Phys. Rev. Lett.",
      "authors": "C. Beck"
    },
    {
      "index": 47,
      "title": "Superstatistics",
      "abstract": "",
      "year": "2003",
      "venue": "Physica A",
      "authors": "C. Beck and E.G.D. Cohen"
    },
    {
      "index": 48,
      "title": "Beyond Boltzmann-Gibbs statistics: Maximum entropy hyperensembles out of equilibrium",
      "abstract": "",
      "year": "2007",
      "venue": "Phys. Rev. E",
      "authors": "G. E. Crooks"
    },
    {
      "index": 49,
      "title": "Entropic inference: some pitfalls and paradoxes we can avoid",
      "abstract": "",
      "year": "2013",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 50,
      "title": "The Undivided Universe: an ontological interpretation on quantum theory",
      "abstract": "",
      "year": "1993",
      "venue": "",
      "authors": "D. Bohm and B. J. Hiley"
    },
    {
      "index": 51,
      "title": "On the foundations of decision theory",
      "abstract": "",
      "year": "2017",
      "venue": "Homo Oecon",
      "authors": "K. Binmore"
    },
    {
      "index": 52,
      "title": "Information Theory for Agents in Artificial Intelligence, Psychology, and Economics",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "M.S. Harre"
    },
    {
      "index": 53,
      "title": "A Maximum Entropy Model of Bounded Rational Decision-Making with Prior Beliefs and Market Feedback",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "B. P. Evans, M. Prokopenko"
    },
    {
      "index": 54,
      "title": "An Entropic framework for Modeling Economies",
      "abstract": "",
      "year": "2014",
      "venue": "Physica A",
      "authors": "A. Caticha and A. Golan"
    }
  ]
}