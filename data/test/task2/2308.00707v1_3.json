{
  "paper_id": "2308.00707v1",
  "title": "Approximate Model-Based Shielding for Safe Reinforcement Learning",
  "sections": {
    "world models": "World models were first introduced in the titular paper [ref]18. Built on the recurrent state space model (RSSM) [ref]20, world models are trained to learn a compact latent representation of the environment state and dynamics. Once the dynamics are learnt the world model can be used to â€˜imagineâ€™ possible future trajectories. We leverage DreamerV3  as our stand-in dynamics model for look-ahead shielding and policy optimisation. DreamerV3 consists of the following components: (1) the image encoder ztâˆ¼qÎ¸â€‹(ztâˆ£ot,ht)similar-tosubscriptğ‘§ğ‘¡subscriptğ‘ğœƒconditionalsubscriptğ‘§ğ‘¡subscriptğ‘œğ‘¡subscriptâ„ğ‘¡z_{t}\\sim q_{\\theta}(z_{t}\\mid o_{t},h_{t}) that learns a posterior latent representation given the observation otsubscriptğ‘œğ‘¡o_{t} and recurrent state htsubscriptâ„ğ‘¡h_{t}. (2) The recurrent model ht=fÎ¸â€‹(htâˆ’1,ztâˆ’1,atâˆ’1)subscriptâ„ğ‘¡subscriptğ‘“ğœƒsubscriptâ„ğ‘¡1subscriptğ‘§ğ‘¡1subscriptğ‘ğ‘¡1h_{t}=f_{\\theta}(h_{t-1},z_{t-1},a_{t-1}), which computes the next deterministic latents given the past state stâˆ’1=(htâˆ’1,ztâˆ’1)subscriptğ‘ ğ‘¡1subscriptâ„ğ‘¡1subscriptğ‘§ğ‘¡1s_{t-1}=(h_{t-1},z_{t-1}) and action atâˆ’1subscriptğ‘ğ‘¡1a_{t-1}. (3) The transition predictor z^tâˆ¼pÎ¸â€‹(z^tâˆ£ht)similar-tosubscript^ğ‘§ğ‘¡subscriptğ‘ğœƒconditionalsubscript^ğ‘§ğ‘¡subscriptâ„ğ‘¡\\hat{z}_{t}\\sim p_{\\theta}(\\hat{z}_{t}\\mid h_{t}), which is used as the prior distribution over the stochastic latents (without otsubscriptğ‘œğ‘¡o_{t}). (4) The image decoder o^tâˆ¼pÎ¸â€‹(o^tâˆ£ht,zt)similar-tosubscript^ğ‘œğ‘¡subscriptğ‘ğœƒconditionalsubscript^ğ‘œğ‘¡subscriptâ„ğ‘¡subscriptğ‘§ğ‘¡\\hat{o}_{t}\\sim p_{\\theta}(\\hat{o}_{t}\\mid h_{t},z_{t}) which is used to provide high quality image gradients by minimising reconstruction loss. (5) The prediction heads r^tâˆ¼pÎ¸â€‹(r^tâˆ£ht,zt)similar-tosubscript^ğ‘Ÿğ‘¡subscriptğ‘ğœƒconditionalsubscript^ğ‘Ÿğ‘¡subscriptâ„ğ‘¡subscriptğ‘§ğ‘¡\\hat{r}_{t}\\sim p_{\\theta}(\\hat{r}_{t}\\mid h_{t},z_{t}) and Î³^tâˆ¼pÎ¸â€‹(Î³^tâˆ£ht,zt)similar-tosubscript^ğ›¾ğ‘¡subscriptğ‘ğœƒconditionalsubscript^ğ›¾ğ‘¡subscriptâ„ğ‘¡subscriptğ‘§ğ‘¡\\hat{\\gamma}_{t}\\sim p_{\\theta}(\\hat{\\gamma}_{t}\\mid h_{t},z_{t}) trained to predict reward signals and episode termination respectively. In DreamerV3 the latents represent categorical variables and the reward and image prediction heads both parameterise a twohot symlog distributions . All components are implemented as neural networks and optimised with straight through gradients. In addition, KL-balancing  and free-bits [ref]19 are used to regularise the prior and posterior representations and prevent a degenerate latent space representation. Policy optimisation is performed entirely on experience â€˜imaginedâ€™ in the latent space of the world model. We refer to Ï€tasksuperscriptğœ‹task\\pi^{\\text{task}} as the task policy trained in the world model to maximise expected discounted accumulated reward, that is, optimise the following, In addition to the task policy Ï€tasksuperscriptğœ‹task\\pi^{\\text{task}}, a task critic denoted vtasksuperscriptğ‘£taskv^{\\text{task}}, is used to construct TD-Î»ğœ†\\lambda targets  which are used to help guide the task policy Ï€tasksuperscriptğœ‹task\\pi^{\\text{task}} in a TD-Î»ğœ†\\lambda actor-critic style algorithm. We refer the reader to  for more precise details."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel",
      "orig_title": "Constrained policy optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 1,
      "title": "A general class of coefficients of divergence of one distribution from another",
      "abstract": "",
      "year": "1966",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "SyedÂ Mumtaz Ali and SamuelÂ D Silvey"
    },
    {
      "index": 2,
      "title": "Safe Reinforcement Learning via Shielding",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Mohammed Alshiekh, Roderick Bloem, RÃ¼diger Ehlers, Bettina KÃ¶nighofer, Scott Niekum, and Ufuk Topcu",
      "orig_title": "Safe reinforcement learning via shielding",
      "paper_id": "1708.08611v2"
    },
    {
      "index": 3,
      "title": "Constrained Markov decision processes: stochastic modeling",
      "abstract": "",
      "year": "1999",
      "venue": "Routledge",
      "authors": "Eitan Altman"
    },
    {
      "index": 4,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.06565",
      "authors": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan ManÃ©",
      "orig_title": "Concrete problems in ai safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 5,
      "title": "Constrained Policy Optimization via Bayesian World Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.09802",
      "authors": "Yarden As, Ilnura Usmanova, Sebastian Curi, and Andreas Krause",
      "orig_title": "Constrained policy optimization via bayesian world models",
      "paper_id": "2201.09802v4"
    },
    {
      "index": 6,
      "title": "Principles of model checking",
      "abstract": "",
      "year": "2008",
      "venue": "MIT press",
      "authors": "Christel Baier and Joost-Pieter Katoen"
    },
    {
      "index": 7,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "M.Â G. Bellemare, Y.Â Naddaf, J.Â Veness, and M.Â Bowling"
    },
    {
      "index": 8,
      "title": "Shield synthesis: Runtime enforcement for reactive systems",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on tools and algorithms for the construction and analysis of systems",
      "authors": "Roderick Bloem, Bettina KÃ¶nighofer, Robert KÃ¶nighofer, and Chao Wang"
    },
    {
      "index": 9,
      "title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "PabloÂ Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and MarcÂ G. Bellemare",
      "orig_title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "paper_id": "1812.06110v1"
    },
    {
      "index": 10,
      "title": "Lyapunov-based safe policy optimization for continuous control",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.10031",
      "authors": "Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh"
    },
    {
      "index": 11,
      "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Will Dabney, Georg Ostrovski, David Silver, and RÃ©mi Munos",
      "orig_title": "Implicit quantile networks for distributional reinforcement learning",
      "paper_id": "1806.06923v1"
    },
    {
      "index": 12,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Scott Fujimoto, Herke Hoof, and David Meger",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 13,
      "title": "Learning Belief Representations for Imitation Learning in POMDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Tanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng",
      "orig_title": "Learning belief representations for imitation learning in pomdps",
      "paper_id": "1906.09510v1"
    },
    {
      "index": 14,
      "title": "Shielding Atari Games with Bounded Prescience",
      "abstract": "",
      "year": "2021",
      "venue": "International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
      "authors": "MÂ Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar Wijk",
      "orig_title": "Shielding atari games with bounded prescience",
      "paper_id": "2101.08153v2"
    },
    {
      "index": 15,
      "title": "Approximate Shielding of Atari Agents for Safe Exploration",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.11104",
      "authors": "AlexanderÂ W Goodall and Francesco Belardinelli",
      "orig_title": "Approximate shielding of atari agents for safe exploration",
      "paper_id": "2304.11104v1"
    },
    {
      "index": 16,
      "title": "Neurips 2019 competition: the minerl competition on sample efficient reinforcement learning using human priors",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.10079",
      "authors": "WilliamÂ H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, DiegoÂ Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, etÂ al."
    },
    {
      "index": 17,
      "title": "Recurrent World Models Facilitate Policy Evolution",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Ha and JÃ¼rgen Schmidhuber",
      "orig_title": "Recurrent world models facilitate policy evolution",
      "paper_id": "1809.01999v1"
    },
    {
      "index": 18,
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi",
      "orig_title": "Dream to control: Learning behaviors by latent imagination",
      "paper_id": "1912.01603v3"
    },
    {
      "index": 19,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 20,
      "title": "Mastering Atari with Discrete World Models",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Danijar Hafner, TimothyÂ P Lillicrap, Mohammad Norouzi, and Jimmy Ba",
      "orig_title": "Mastering atari with discrete world models",
      "paper_id": "2010.02193v4"
    },
    {
      "index": 21,
      "title": "Mastering diverse domains through world models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.04104",
      "authors": "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap"
    },
    {
      "index": 22,
      "title": "Safe exploration for reinforcement learning.",
      "abstract": "",
      "year": "2008",
      "venue": "ESANN",
      "authors": "Alexander Hans, Daniel SchneegaÃŸ, AntonÂ Maximilian SchÃ¤fer, and Steffen Udluft"
    },
    {
      "index": 23,
      "title": "Do Androids Dream of Electric Fences? Safety-Aware Reinforcement Learning with Latent Shielding",
      "abstract": "",
      "year": "",
      "venue": "CEUR Workshop Proceedings",
      "authors": "PÂ He, BÂ GonzalezÂ Leon, and FÂ Belardinelli",
      "orig_title": "Do androids dream of electric fences? safety-aware reinforcement learning with latent shielding",
      "paper_id": "2112.11490v1"
    },
    {
      "index": 24,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "Matteo Hessel, Joseph Modayil, Hado VanÂ Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver",
      "orig_title": "Rainbow: Combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 25,
      "title": "Probability inequalities for sums of bounded random variables",
      "abstract": "",
      "year": "1994",
      "venue": "The collected works of Wassily Hoeffding",
      "authors": "Wassily Hoeffding"
    },
    {
      "index": 26,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine"
    },
    {
      "index": 27,
      "title": "Near-optimal reinforcement learning in polynomial time",
      "abstract": "",
      "year": "2002",
      "venue": "Machine learning",
      "authors": "Michael Kearns and Satinder Singh"
    },
    {
      "index": 28,
      "title": "Learning in POMDPs is Sample-Efficient with Hindsight Observability",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.13857",
      "authors": "JonathanÂ N Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang",
      "orig_title": "Learning in pomdps is sample-efficient with hindsight observability",
      "paper_id": "2301.13857v2"
    },
    {
      "index": 29,
      "title": "IPO: Interior-point policy optimization under constraints",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "Yongshuai Liu, Jiaxin Ding, and Xin Liu"
    },
    {
      "index": 30,
      "title": "Constrained Model-based Reinforcement Learning with Robust Cross-Entropy Method",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.07968",
      "authors": "Zuxin Liu, Hongyi Zhou, Baiming Chen, Sicheng Zhong, Martial Hebert, and Ding Zhao",
      "orig_title": "Constrained model-based reinforcement learning with robust cross-entropy method",
      "paper_id": "2010.07968v2"
    },
    {
      "index": 31,
      "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yuping Luo and Tengyu Ma",
      "orig_title": "Learning barrier certificates: Towards safe reinforcement learning with zero training-time violations",
      "paper_id": "2108.01846v2"
    },
    {
      "index": 32,
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "MarlosÂ C. Machado, MarcÂ G. Bellemare, Erik Talvitie, Joel Veness, MatthewÂ J. Hausknecht, and Michael Bowling",
      "orig_title": "Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents",
      "paper_id": "1709.06009v2"
    },
    {
      "index": 33,
      "title": "Shielded reinforcement learning: A review of reactive methods for safe learning",
      "abstract": "",
      "year": "2023",
      "venue": "2023 IEEE/SICE International Symposium on System Integration (SII)",
      "authors": "Haritz Odriozola-Olalde, Maider Zamalloa, and Nestor Arana-Arexolaleiba"
    },
    {
      "index": 34,
      "title": "Markov decision processes",
      "abstract": "",
      "year": "1990",
      "venue": "Handbooks in operations research and management science",
      "authors": "MartinÂ L Puterman"
    },
    {
      "index": 35,
      "title": "A Game Theoretic Framework for Model Based Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar",
      "orig_title": "A game theoretic framework for model based reinforcement learning",
      "paper_id": "2004.07804v2"
    },
    {
      "index": 36,
      "title": "Benchmarking safe exploration in deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01708",
      "authors": "Alex Ray, Joshua Achiam, and Dario Amodei"
    },
    {
      "index": 37,
      "title": "Dyna, an integrated architecture for learning, planning, and reacting",
      "abstract": "",
      "year": "1991",
      "venue": "ACM Sigart Bulletin",
      "authors": "RichardÂ S Sutton"
    },
    {
      "index": 38,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "RichardÂ S Sutton and AndrewÂ G Barto"
    },
    {
      "index": 39,
      "title": "DeepMind Control Suite",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.00690",
      "authors": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego deÂ Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, etÂ al.",
      "orig_title": "Deepmind control suite",
      "paper_id": "1801.00690v1"
    },
    {
      "index": 40,
      "title": "Safe Reinforcement Learning by Imagining the Near Future",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Garrett Thomas, Yuping Luo, and Tengyu Ma",
      "orig_title": "Safe reinforcement learning by imagining the near future",
      "paper_id": "2202.07789v1"
    },
    {
      "index": 41,
      "title": "Gaussian processes for machine learning",
      "abstract": "",
      "year": "2006",
      "venue": "MIT press Cambridge, MA",
      "authors": "ChristopherÂ KI Williams and CarlÂ Edward Rasmussen"
    },
    {
      "index": 42,
      "title": "Numerical optimization",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Jorge Nocedal StephenÂ J Wright"
    },
    {
      "index": 43,
      "title": "Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.06281",
      "authors": "Wenli Xiao, Yiwei Lyu, and John Dolan",
      "orig_title": "Model-based dynamic shielding for safe and efficient multi-agent reinforcement learning",
      "paper_id": "2304.06281v1"
    },
    {
      "index": 44,
      "title": "Projection-Based Constrained Policy Optimization",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and PeterÂ J Ramadge",
      "orig_title": "Projection-based constrained policy optimization",
      "paper_id": "2010.03152v1"
    }
  ]
}