{
  "paper_id": "2308.00707v1",
  "title": "Approximate Model-Based Shielding for Safe Reinforcement Learning",
  "sections": {
    "world models": "World models were first introduced in the titular paper [ref]18. Built on the recurrent state space model (RSSM) [ref]20, world models are trained to learn a compact latent representation of the environment state and dynamics. Once the dynamics are learnt the world model can be used to ‘imagine’ possible future trajectories. We leverage DreamerV3  as our stand-in dynamics model for look-ahead shielding and policy optimisation. DreamerV3 consists of the following components: (1) the image encoder zt∼qθ​(zt∣ot,ht)similar-tosubscript𝑧𝑡subscript𝑞𝜃conditionalsubscript𝑧𝑡subscript𝑜𝑡subscriptℎ𝑡z_{t}\\sim q_{\\theta}(z_{t}\\mid o_{t},h_{t}) that learns a posterior latent representation given the observation otsubscript𝑜𝑡o_{t} and recurrent state htsubscriptℎ𝑡h_{t}. (2) The recurrent model ht=fθ​(ht−1,zt−1,at−1)subscriptℎ𝑡subscript𝑓𝜃subscriptℎ𝑡1subscript𝑧𝑡1subscript𝑎𝑡1h_{t}=f_{\\theta}(h_{t-1},z_{t-1},a_{t-1}), which computes the next deterministic latents given the past state st−1=(ht−1,zt−1)subscript𝑠𝑡1subscriptℎ𝑡1subscript𝑧𝑡1s_{t-1}=(h_{t-1},z_{t-1}) and action at−1subscript𝑎𝑡1a_{t-1}. (3) The transition predictor z^t∼pθ​(z^t∣ht)similar-tosubscript^𝑧𝑡subscript𝑝𝜃conditionalsubscript^𝑧𝑡subscriptℎ𝑡\\hat{z}_{t}\\sim p_{\\theta}(\\hat{z}_{t}\\mid h_{t}), which is used as the prior distribution over the stochastic latents (without otsubscript𝑜𝑡o_{t}). (4) The image decoder o^t∼pθ​(o^t∣ht,zt)similar-tosubscript^𝑜𝑡subscript𝑝𝜃conditionalsubscript^𝑜𝑡subscriptℎ𝑡subscript𝑧𝑡\\hat{o}_{t}\\sim p_{\\theta}(\\hat{o}_{t}\\mid h_{t},z_{t}) which is used to provide high quality image gradients by minimising reconstruction loss. (5) The prediction heads r^t∼pθ​(r^t∣ht,zt)similar-tosubscript^𝑟𝑡subscript𝑝𝜃conditionalsubscript^𝑟𝑡subscriptℎ𝑡subscript𝑧𝑡\\hat{r}_{t}\\sim p_{\\theta}(\\hat{r}_{t}\\mid h_{t},z_{t}) and γ^t∼pθ​(γ^t∣ht,zt)similar-tosubscript^𝛾𝑡subscript𝑝𝜃conditionalsubscript^𝛾𝑡subscriptℎ𝑡subscript𝑧𝑡\\hat{\\gamma}_{t}\\sim p_{\\theta}(\\hat{\\gamma}_{t}\\mid h_{t},z_{t}) trained to predict reward signals and episode termination respectively. In DreamerV3 the latents represent categorical variables and the reward and image prediction heads both parameterise a twohot symlog distributions . All components are implemented as neural networks and optimised with straight through gradients. In addition, KL-balancing  and free-bits [ref]19 are used to regularise the prior and posterior representations and prevent a degenerate latent space representation. Policy optimisation is performed entirely on experience ‘imagined’ in the latent space of the world model. We refer to πtasksuperscript𝜋task\\pi^{\\text{task}} as the task policy trained in the world model to maximise expected discounted accumulated reward, that is, optimise the following, In addition to the task policy πtasksuperscript𝜋task\\pi^{\\text{task}}, a task critic denoted vtasksuperscript𝑣taskv^{\\text{task}}, is used to construct TD-λ𝜆\\lambda targets  which are used to help guide the task policy πtasksuperscript𝜋task\\pi^{\\text{task}} in a TD-λ𝜆\\lambda actor-critic style algorithm. We refer the reader to  for more precise details."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel",
      "orig_title": "Constrained policy optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 1,
      "title": "A general class of coefficients of divergence of one distribution from another",
      "abstract": "",
      "year": "1966",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "Syed Mumtaz Ali and Samuel D Silvey"
    },
    {
      "index": 2,
      "title": "Safe Reinforcement Learning via Shielding",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina Könighofer, Scott Niekum, and Ufuk Topcu",
      "orig_title": "Safe reinforcement learning via shielding",
      "paper_id": "1708.08611v2"
    },
    {
      "index": 3,
      "title": "Constrained Markov decision processes: stochastic modeling",
      "abstract": "",
      "year": "1999",
      "venue": "Routledge",
      "authors": "Eitan Altman"
    },
    {
      "index": 4,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.06565",
      "authors": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané",
      "orig_title": "Concrete problems in ai safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 5,
      "title": "Constrained Policy Optimization via Bayesian World Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.09802",
      "authors": "Yarden As, Ilnura Usmanova, Sebastian Curi, and Andreas Krause",
      "orig_title": "Constrained policy optimization via bayesian world models",
      "paper_id": "2201.09802v4"
    },
    {
      "index": 6,
      "title": "Principles of model checking",
      "abstract": "",
      "year": "2008",
      "venue": "MIT press",
      "authors": "Christel Baier and Joost-Pieter Katoen"
    },
    {
      "index": 7,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling"
    },
    {
      "index": 8,
      "title": "Shield synthesis: Runtime enforcement for reactive systems",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on tools and algorithms for the construction and analysis of systems",
      "authors": "Roderick Bloem, Bettina Könighofer, Robert Könighofer, and Chao Wang"
    },
    {
      "index": 9,
      "title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare",
      "orig_title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "paper_id": "1812.06110v1"
    },
    {
      "index": 10,
      "title": "Lyapunov-based safe policy optimization for continuous control",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.10031",
      "authors": "Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh"
    },
    {
      "index": 11,
      "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos",
      "orig_title": "Implicit quantile networks for distributional reinforcement learning",
      "paper_id": "1806.06923v1"
    },
    {
      "index": 12,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Scott Fujimoto, Herke Hoof, and David Meger",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 13,
      "title": "Learning Belief Representations for Imitation Learning in POMDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Tanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng",
      "orig_title": "Learning belief representations for imitation learning in pomdps",
      "paper_id": "1906.09510v1"
    },
    {
      "index": 14,
      "title": "Shielding Atari Games with Bounded Prescience",
      "abstract": "",
      "year": "2021",
      "venue": "International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",
      "authors": "M Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar Wijk",
      "orig_title": "Shielding atari games with bounded prescience",
      "paper_id": "2101.08153v2"
    },
    {
      "index": 15,
      "title": "Approximate Shielding of Atari Agents for Safe Exploration",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.11104",
      "authors": "Alexander W Goodall and Francesco Belardinelli",
      "orig_title": "Approximate shielding of atari agents for safe exploration",
      "paper_id": "2304.11104v1"
    },
    {
      "index": 16,
      "title": "Neurips 2019 competition: the minerl competition on sample efficient reinforcement learning using human priors",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.10079",
      "authors": "William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al."
    },
    {
      "index": 17,
      "title": "Recurrent World Models Facilitate Policy Evolution",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Ha and Jürgen Schmidhuber",
      "orig_title": "Recurrent world models facilitate policy evolution",
      "paper_id": "1809.01999v1"
    },
    {
      "index": 18,
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi",
      "orig_title": "Dream to control: Learning behaviors by latent imagination",
      "paper_id": "1912.01603v3"
    },
    {
      "index": 19,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 20,
      "title": "Mastering Atari with Discrete World Models",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba",
      "orig_title": "Mastering atari with discrete world models",
      "paper_id": "2010.02193v4"
    },
    {
      "index": 21,
      "title": "Mastering diverse domains through world models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.04104",
      "authors": "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap"
    },
    {
      "index": 22,
      "title": "Safe exploration for reinforcement learning.",
      "abstract": "",
      "year": "2008",
      "venue": "ESANN",
      "authors": "Alexander Hans, Daniel Schneegaß, Anton Maximilian Schäfer, and Steffen Udluft"
    },
    {
      "index": 23,
      "title": "Do Androids Dream of Electric Fences? Safety-Aware Reinforcement Learning with Latent Shielding",
      "abstract": "",
      "year": "",
      "venue": "CEUR Workshop Proceedings",
      "authors": "P He, B Gonzalez Leon, and F Belardinelli",
      "orig_title": "Do androids dream of electric fences? safety-aware reinforcement learning with latent shielding",
      "paper_id": "2112.11490v1"
    },
    {
      "index": 24,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver",
      "orig_title": "Rainbow: Combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 25,
      "title": "Probability inequalities for sums of bounded random variables",
      "abstract": "",
      "year": "1994",
      "venue": "The collected works of Wassily Hoeffding",
      "authors": "Wassily Hoeffding"
    },
    {
      "index": 26,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine"
    },
    {
      "index": 27,
      "title": "Near-optimal reinforcement learning in polynomial time",
      "abstract": "",
      "year": "2002",
      "venue": "Machine learning",
      "authors": "Michael Kearns and Satinder Singh"
    },
    {
      "index": 28,
      "title": "Learning in POMDPs is Sample-Efficient with Hindsight Observability",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.13857",
      "authors": "Jonathan N Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang",
      "orig_title": "Learning in pomdps is sample-efficient with hindsight observability",
      "paper_id": "2301.13857v2"
    },
    {
      "index": 29,
      "title": "IPO: Interior-point policy optimization under constraints",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "Yongshuai Liu, Jiaxin Ding, and Xin Liu"
    },
    {
      "index": 30,
      "title": "Constrained Model-based Reinforcement Learning with Robust Cross-Entropy Method",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.07968",
      "authors": "Zuxin Liu, Hongyi Zhou, Baiming Chen, Sicheng Zhong, Martial Hebert, and Ding Zhao",
      "orig_title": "Constrained model-based reinforcement learning with robust cross-entropy method",
      "paper_id": "2010.07968v2"
    },
    {
      "index": 31,
      "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yuping Luo and Tengyu Ma",
      "orig_title": "Learning barrier certificates: Towards safe reinforcement learning with zero training-time violations",
      "paper_id": "2108.01846v2"
    },
    {
      "index": 32,
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling",
      "orig_title": "Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents",
      "paper_id": "1709.06009v2"
    },
    {
      "index": 33,
      "title": "Shielded reinforcement learning: A review of reactive methods for safe learning",
      "abstract": "",
      "year": "2023",
      "venue": "2023 IEEE/SICE International Symposium on System Integration (SII)",
      "authors": "Haritz Odriozola-Olalde, Maider Zamalloa, and Nestor Arana-Arexolaleiba"
    },
    {
      "index": 34,
      "title": "Markov decision processes",
      "abstract": "",
      "year": "1990",
      "venue": "Handbooks in operations research and management science",
      "authors": "Martin L Puterman"
    },
    {
      "index": 35,
      "title": "A Game Theoretic Framework for Model Based Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar",
      "orig_title": "A game theoretic framework for model based reinforcement learning",
      "paper_id": "2004.07804v2"
    },
    {
      "index": 36,
      "title": "Benchmarking safe exploration in deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01708",
      "authors": "Alex Ray, Joshua Achiam, and Dario Amodei"
    },
    {
      "index": 37,
      "title": "Dyna, an integrated architecture for learning, planning, and reacting",
      "abstract": "",
      "year": "1991",
      "venue": "ACM Sigart Bulletin",
      "authors": "Richard S Sutton"
    },
    {
      "index": 38,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Richard S Sutton and Andrew G Barto"
    },
    {
      "index": 39,
      "title": "DeepMind Control Suite",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.00690",
      "authors": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al.",
      "orig_title": "Deepmind control suite",
      "paper_id": "1801.00690v1"
    },
    {
      "index": 40,
      "title": "Safe Reinforcement Learning by Imagining the Near Future",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Garrett Thomas, Yuping Luo, and Tengyu Ma",
      "orig_title": "Safe reinforcement learning by imagining the near future",
      "paper_id": "2202.07789v1"
    },
    {
      "index": 41,
      "title": "Gaussian processes for machine learning",
      "abstract": "",
      "year": "2006",
      "venue": "MIT press Cambridge, MA",
      "authors": "Christopher KI Williams and Carl Edward Rasmussen"
    },
    {
      "index": 42,
      "title": "Numerical optimization",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Jorge Nocedal Stephen J Wright"
    },
    {
      "index": 43,
      "title": "Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.06281",
      "authors": "Wenli Xiao, Yiwei Lyu, and John Dolan",
      "orig_title": "Model-based dynamic shielding for safe and efficient multi-agent reinforcement learning",
      "paper_id": "2304.06281v1"
    },
    {
      "index": 44,
      "title": "Projection-Based Constrained Policy Optimization",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge",
      "orig_title": "Projection-based constrained policy optimization",
      "paper_id": "2010.03152v1"
    }
  ]
}