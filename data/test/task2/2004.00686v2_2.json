{
  "paper_id": "2004.00686v2",
  "title": "Bias in Machine Learning - What is it Good for?",
  "sections": {
    "sampling bias": "Sampling bias occurs when there is an underrepresentation or overrepresentation of observations from a segment of the population .\nSuch bias, which is sometimes called selection bias , or population bias , may result in a classifier that performs bad in general, or bad for certain demographic groups. One example of underrepresentation is a reported case where a New Zealand passport robot rejected an Asian man’s eyes because ‘subject eyes are closed’666CNN World, Dec. 9, 2016. Accessed July 29, 2020..\nA possible reason could have been that the robot was trained with too few pictures of Asian men, and therefor made bad predictions on this demographic group. There are many reasons for sampling bias in a dataset. One\nkind is denoted self-selection bias  and can be exemplified with an online survey about computer use. Such a survey is likely to attract people more interested in technology than is typical for the entire population and therefor creates a bias in data. Another example is a system that predicts crime rates in different parts of a city. Since areas with more crimes typically have more police present, the number of reported arrests would become unfairly high in these areas. If such a system would be used to determine the distribution of police presence, a viscous circle may even be created  . An opposite example demonstrates how the big data era with its automatic data gathering can create ‘dark zones or shadows where some citizens and communities are overlooked’ . The author Kate Crawford points to Street Bump, a phone app that uses the phone’s built in accelerometer to detect and report information about road problems to the city. Due to the uneven distribution of smartphones across different parts of the city, data from Street Bump will have a sampling bias. It is important to note that sampling bias does not only refer to unbalanced categories of humans, and furthermore not even to unbalanced categories. Unbalances may also concern features that have to appear in a balanced fashion.\nOne example is given in [ref]46, and is there denoted dataset bias. Focusing on image data, the authors argue that ‘… computer vision datasets are supposed to be a representation of the world’, but in reality, many commonly used datasets represent the world in a very biased way. Objects may, for example, always appear in the center of the image. This bias makes it hard for a classifier to recognize objects that are not centered in the image [ref]46.\nThe authors compared six common datasets of images used for object detection, and found that performance on another dataset than the one used during training in average was cut to less than half.\nA similar effect is reported in . If all images in a dataset containing a snowmobile also contain snow, a machine learning algorithm may find snow cues useful to detect snowmobiles. While this may work fine for images in the dataset used for training, it becomes problematic to analyze images with snowmobiles placed indoors. Another kind of sampling bias is survivorship bias . It occurs when the sampled data does not represent the population of interest, since some data items ‘died’. One example is when a bank’s stock fund management is assessed by sampling the performance of the bank’s current funds. This leads to a biased assessment since poorly-performing funds are often removed or merged into other funds ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "False positives, false negatives, and false analyses: A rejoinder to ”machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks”",
      "abstract": "",
      "year": "2016",
      "venue": "Federal Probation Journal",
      "authors": "Kristin Bechtel Anthony W. Flores and Christopher T. Lowenkamp"
    },
    {
      "index": 1,
      "title": "Learning de-biased representations with biased representations",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh"
    },
    {
      "index": 2,
      "title": "Big data’s disparate impact",
      "abstract": "",
      "year": "2014",
      "venue": "California law Review",
      "authors": "Solon Barocas and Andrew D. Selbst"
    },
    {
      "index": 3,
      "title": "Hyper-parameter optimization in deep learning and transfer learning : applications to medical imaging",
      "abstract": "",
      "year": "2019",
      "venue": "Theses, Université Paris-Saclay",
      "authors": "Hadrien Bertrand"
    },
    {
      "index": 4,
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "30th Conference on Neural Information Processing Systems (NIPS 2016)",
      "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai"
    },
    {
      "index": 5,
      "title": "Understanding the Origins of Bias in Word Embeddings",
      "abstract": "",
      "year": "2019",
      "venue": "36th Int. Conf. on Machine Learning",
      "authors": "Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel",
      "orig_title": "Understanding the origins of bias inword embeddings",
      "paper_id": "1810.03611v2"
    },
    {
      "index": 6,
      "title": "Semantics derived automatically from language corpora necessarily contain human biases",
      "abstract": "",
      "year": "2017",
      "venue": "Science",
      "authors": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan"
    },
    {
      "index": 7,
      "title": "Ai now 2017 report",
      "abstract": "",
      "year": "2018",
      "venue": "AI Now 2017 Symposium and Workshop",
      "authors": "Alex Campolo, Madelyn Sanfilippo, Meredith Whittaker, and Kate Crawford"
    },
    {
      "index": 8,
      "title": "CRISP-DM 1.0: Step-by-step data mining guide",
      "abstract": "",
      "year": "2000",
      "venue": "SPSS",
      "authors": "Peter Chapman, Janet Clinton, Randy Kerber, Tom Khabaza, Thomas Reinartz, C. Russell H. Shearer, and Robert Wirth"
    },
    {
      "index": 9,
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "abstract": "",
      "year": "2016",
      "venue": "Big data",
      "authors": "Alexandra Chouldechova",
      "orig_title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "paper_id": "1703.00056v1"
    },
    {
      "index": 10,
      "title": "Algorithmic discrimination is an information problem",
      "abstract": "",
      "year": "2019",
      "venue": "Hastings Law Journal",
      "authors": "Ignacio N. Cofone"
    },
    {
      "index": 11,
      "title": "Think again: Big data",
      "abstract": "",
      "year": "2013",
      "venue": "Foreign Policy",
      "authors": "Kate Crawford"
    },
    {
      "index": 12,
      "title": "Algorithmic bias in autonomous systems",
      "abstract": "",
      "year": "2017",
      "venue": "26th International Joint Conference on Artificial Intelligence, IJCAI’17",
      "authors": "David Danks and Alex John London"
    },
    {
      "index": 13,
      "title": "Proxy non-discrimination in data-driven systems",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen"
    },
    {
      "index": 14,
      "title": "Chapter 6 research design - sampling bias",
      "abstract": "",
      "year": "",
      "venue": "Online Statistics Education: A Multimedia Course of Study",
      "authors": "Rice University David M. Lane"
    },
    {
      "index": 15,
      "title": "Fairness through awareness",
      "abstract": "",
      "year": "2012",
      "venue": "3rd Innovations in Theoretical Computer Science Conference, ITCS ’12",
      "authors": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel"
    },
    {
      "index": 16,
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "abstract": "",
      "year": "2019",
      "venue": "Int. Conf. on Learning Representations",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel",
      "orig_title": "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.",
      "paper_id": "1811.12231v3"
    },
    {
      "index": 17,
      "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Hila Gonen and Yoav Goldberg",
      "orig_title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "paper_id": "1903.03862v2"
    },
    {
      "index": 18,
      "title": "European Union regulations on algorithmic decision-making and a “right to explanation”",
      "abstract": "",
      "year": "2017",
      "venue": "AI Magazine",
      "authors": "Bryce Goodman and Seth Flaxman",
      "orig_title": "European union regulations on algorithmic decision-making and a “right to explanation”",
      "paper_id": "1606.08813v3"
    },
    {
      "index": 19,
      "title": "Truth and Method",
      "abstract": "",
      "year": "1975",
      "venue": "Continuum",
      "authors": "Gadamer Hans-Georg"
    },
    {
      "index": 20,
      "title": "Equality of Opportunity in Supervised Learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems 29",
      "authors": "Moritz Hardt, Eric Price, and Nati Srebro",
      "orig_title": "Equality of opportunity in supervised learning",
      "paper_id": "1610.02413v1"
    },
    {
      "index": 21,
      "title": "Handbook of Research on Machine Learning Innovations and Trends",
      "abstract": "",
      "year": "2017",
      "venue": "IGI Global, USA",
      "authors": "Aboul Ella Hassanien, Aboul Ella Hassanien, and Tarek Gaber"
    },
    {
      "index": 22,
      "title": "Privacy as protection of the incomputable self: From agnostic to agonistic machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "Theoretical Inquiries in Law",
      "authors": "Mireille Hildebrandt"
    },
    {
      "index": 23,
      "title": "Detecting biased statements in wikipedia",
      "abstract": "",
      "year": "2018",
      "venue": "WWW’18: Companion Proceedings of the The Web Conference 2018",
      "authors": "Christoph Hube and Besnik Fetahu"
    },
    {
      "index": 24,
      "title": "Towards bias detection in online text corpora",
      "abstract": "",
      "year": "2018",
      "venue": "International Workshop on Bias in Information, Algorithms, and Systems (BIAS)",
      "authors": "Christoph Hube, Besnik Fetahu, and Robert Jäschke"
    },
    {
      "index": 25,
      "title": "Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks",
      "abstract": "",
      "year": "2016",
      "venue": "ProPublica",
      "authors": "Surya Mattu Julia Angwin, Jeff Larson and Lauren Kirchner"
    },
    {
      "index": 26,
      "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Svetlana Kiritchenko and Saif M. Mohammad",
      "orig_title": "Examining gender and race bias in two hundred sentiment analysis systems",
      "paper_id": "1805.04508v1"
    },
    {
      "index": 27,
      "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan",
      "orig_title": "Inherent trade-offs in the fair determination of risk scores",
      "paper_id": "1609.05807v2"
    },
    {
      "index": 28,
      "title": "Generalization versus discrimination in machine learning",
      "abstract": "",
      "year": "2012",
      "venue": "Encyclopedia of the Sciences of Learning",
      "authors": "Tim Kovacs and Andy J. Wills"
    },
    {
      "index": 29,
      "title": "Repair: Removing representation bias by dataset resampling",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Yi Li and Nuno Vasconcelos"
    },
    {
      "index": 30,
      "title": "Causal Reasoning for Algorithmic Fairness",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Joshua R. Loftus, Chris Russell, Matt J. Kusner, and Ricardo Silva",
      "orig_title": "Causal reasoning for algorithmic fairness",
      "paper_id": "1805.05859v1"
    },
    {
      "index": 31,
      "title": "Returns from investing in equity mutual funds 1971 to 1991",
      "abstract": "",
      "year": "1995",
      "venue": "The Journal of Finance",
      "authors": "Burton G. Malkiel"
    },
    {
      "index": 32,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "CoRR",
      "authors": "Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean"
    },
    {
      "index": 33,
      "title": "The need for biases in learning generalizations",
      "abstract": "",
      "year": "1980",
      "venue": "Technical report, Rutgers University, New Brunswick, NJ",
      "authors": "Tom M. Mitchell"
    },
    {
      "index": 34,
      "title": "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "abstract": "",
      "year": "2019",
      "venue": "Frontiers in Big Data",
      "authors": "Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kıcıman"
    },
    {
      "index": 35,
      "title": "Artificial intelligence and algorithmic bias: implications for health systems",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of global health",
      "authors": "Trishan Panch, Heather Mattie, and Rifat Atun"
    },
    {
      "index": 36,
      "title": "Discrimination-aware data mining",
      "abstract": "",
      "year": "2008",
      "venue": "14th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, KDD ’08",
      "authors": "Dino Pedreshi, Salvatore Ruggieri, and Franco Turini"
    },
    {
      "index": 37,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning"
    },
    {
      "index": 38,
      "title": "A Dictionary of Epidemiology",
      "abstract": "",
      "year": "2008",
      "venue": "Oxford University Press, USA",
      "authors": "Miquel Porta"
    },
    {
      "index": 39,
      "title": "Linguistic models for analyzing and detecting biased language",
      "abstract": "",
      "year": "2013",
      "venue": "51st Annual Meeting of the Association for Computational Linguistics",
      "authors": "Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky"
    },
    {
      "index": 40,
      "title": "Dirty data, bad predictions: How civil rights violations impact police data",
      "abstract": "",
      "year": "2019",
      "venue": "Predictive Policing Systems, and Justice",
      "authors": "Rashida Richardson, Jason Schultz, and Kate Crawford"
    },
    {
      "index": 41,
      "title": "Modern Epidemiology",
      "abstract": "",
      "year": "2015",
      "venue": "Wolters Kluwer Health/Lippincott Williams & Wilkins",
      "authors": "K.J. Rothman, S. Greenland, and T.L. Lash"
    },
    {
      "index": 42,
      "title": "The risk of racial bias in hate speech detection",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith"
    },
    {
      "index": 43,
      "title": "Mitigating gender bias in natural language processing: Literature review",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang"
    },
    {
      "index": 44,
      "title": "A framework for understanding unintended consequences of machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Harini Suresh and John V. Guttag"
    },
    {
      "index": 45,
      "title": "Unbiased look at dataset bias",
      "abstract": "",
      "year": "2011",
      "venue": "2011 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ’11",
      "authors": "A. Torralba and A. A. Efros"
    },
    {
      "index": 46,
      "title": "It’s a Man’s Wikipedia? Assessing Gender Inequality in an Online Encyclopedia",
      "abstract": "",
      "year": "2015",
      "venue": "ICWSM",
      "authors": "Claudia Wagner, David García, Mohsen Jadidi, and Markus Strohmaier",
      "orig_title": "It’s a man’s wikipedia? assessing gender inequality in an online encyclopedia",
      "paper_id": "1501.06307v2"
    },
    {
      "index": 47,
      "title": "Disciplined convex-concave programming",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE 55th Conference on Decision and Control (CDC)",
      "authors": "Shen Xinyue, Diamond Steven, Gu Yuantao, and Boyd Stephen"
    },
    {
      "index": 48,
      "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment",
      "abstract": "",
      "year": "2017",
      "venue": "26th International Conference on World Wide Web, WWW 2017",
      "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi",
      "orig_title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "paper_id": "1610.08452v2"
    },
    {
      "index": 49,
      "title": "Equality of opportunity in classification: A causal approach",
      "abstract": "",
      "year": "2018",
      "venue": "32nd Conference on Neural Information Processing Systems (NIPS 2018)",
      "authors": "Junzhe Zhang and Elias Bareinboim"
    },
    {
      "index": 50,
      "title": "Deep Neural Network Hyperparameter Optimization with Orthogonal Array Tuning",
      "abstract": "",
      "year": "2019",
      "venue": "ICONIP",
      "authors": "Xu Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong",
      "orig_title": "Deep neural network hyperparameter optimization with orthogonal array tuning",
      "paper_id": "1907.13359v2"
    },
    {
      "index": 51,
      "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
      "abstract": "",
      "year": "2017",
      "venue": "2017 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang",
      "orig_title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "paper_id": "1707.09457v1"
    }
  ]
}