{
  "paper_id": "2009.11939v2",
  "title": "Deep Multi-Scale Feature Learning for Defocus Blur Estimation",
  "sections": {
    "iv-c experimental validation": "In order to validate the proposed defocus blur estimation method, we use the dataset introduced in¬† that contains 222222 real defocused images with resolution 360√ó360360360360\\times 360 captured by a Light Field camera, and it is (to our knowledge) the only dataset in the literature that provides the ground truth defocus blur values for each pixel. Although it also presents images corrupted by artificial noise, we restricted our analysis to the subset with natural image noise only since our model was not trained with artificial noise. We use the popular Mean Absolute Error (MAE) to quantitatively compare our defocus blur estimation approach with SOTA methods, and show edge map images with highlighted depth edges for qualitative evaluation of E-NET on the same dataset. We first illustrate examples of pattern vs. depth edge classification produced by E-NET in Fig.¬†6. In the first blurry image, three different depth layers can be seen (from left to right), and E-NET manages to distinguish most of the edge points that present depth discontinuities (abrupt blur change).\nIn the second image, there is an abrupt depth transition from the red wall to the background, and E-NET correctly labels these boundary points as depth edges, with a few false negatives. Note that edge classification is an intermediate step of our approach, and it will be evaluated implicitly by showing that it does improve the final\ngoal (dense blur estimation), as will be shown next. Furthermore, the amount of data used to train E-NET is rather limited, so we only split the data into train and validation (no test set). For the sake of illustration, the accuracy in the validation set was 88%. We computed the MAE of the raw blur values for each of the 222222 images in the database and reported the average MAE and the standard deviation of the full blur maps obtained by the proposed method222Our code is available at github.com/alikaraali/DepthEdgeAwareBENet and competitive approaches in Table¬†I. We also computed and reported the MAE of the relative blur values by re-scaling the raw blur values to   , as done in¬†[ref]28. We can see that our method outperforms all the competitive approaches in both raw and relative blur333Although we used official implementation of¬†[ref]28 from github.com/ake/DMENet, we obtained an average relative blur MAE slightly different from the value reported in their paper.. It is slightly better than region-based methods¬† [ref]22, which are considerably slower. Please note that as the method ¬†[ref]28 produces a ‚Äúrelative blur map‚Äù, they normalize the GT blur map by the maximum value to report their accuracy. For a fair comparison with the other approaches, we do the opposite: multiply their blur map by the maximum blur value to report the raw blur accuracy. Since the methods described in¬†   model the blur with a Gaussian PSF, we re-scale their results from Gaussian scale to disk radii via the mapping function provided by¬†. Also, since the method presented in¬† is trained with a maximum Gaussian blur scale œÉg=2.0subscriptùúéùëî2.0\\sigma_{g}=2.0, we clipped the ground truth values to 222 when computing the results of¬†, which favored their results significantly. The average running times for all the analyzed methods considering all the images are also shown in Table¬†I. Although¬†[ref]28 has the fastest execution speed, our method presents a very good compromise between MAE and running time when compared to other SOTA methods. The supplementary material provides a comprehensive study of the dataset. Since an important application related to blur estimation is deblurring, we have also evaluated how well our method integrates in this task.\nFollowing¬†, we used the combination of the methods proposed in  and  as a deblurring baseline approach,\nand used as input the ground truth data provided in¬†, the defocus maps produced by SOTA approaches and by the proposed method.\nThe average PSNR and SSIM values are summarized in Table¬†II, showing that our method produced the highest average gain for both PSNR and SSIM, being inferior only to the deblurring results obtained with GT blur estimates.\nFig.¬†8 shows a visual comparison of some deblurring results, highlighting regions with high structural\nor textural information. Our method presents visual results similar to those obtained with the ground truth blur map and the blur map produced with¬†[ref]22, while being much faster than¬†[ref]22. A detailed analysis of this experiment, along with the visual results of all methods, is provided in the supplementary material. Finally, we tested our method in the related task of defocus blur detection (DBD), which aims to find in-focus and out-of-focus regions of a given image. For this task, we use the CUHK blur detection dataset¬†, which contains 704 defocused images along with the corresponding binary blur maps as ground truth. Since the main focus of the proposed method is defocus blur estimation (i.e., finding the actual blur level at each pixel), DBD is performed by thresholding the estimated blur maps.\nMore precisely, we used the same adaptive thresholding approach adopted in¬† and¬†[ref]28, which consists of defining a threshold where vm‚Äãa‚Äãxsubscriptùë£ùëöùëéùë•v_{max} and vm‚Äãi‚Äãnsubscriptùë£ùëöùëñùëõv_{min} are the maximum and the minimum blur values of the estimated defocus blur map, and Œ±ùõº\\alpha is an empirically chosen parameter, which is set so as to maximize the accuracy of each method individually. Fig.¬†9 shows the precision-recall curves for our approach and competitive methods, along with the corresponding accuracy (see legend of the figure). Since only a subset of 200 images was used to test the method presented in¬†[ref]28 (the remaining 504 were used to train it), all results refer to this smaller test subset for a fair comparison.\nIt is important to mention that the proposed method, as well as¬†, , and , does not use any images from CUHK dataset at any part of the algorithm design or training, while [ref]28 uses images from CUHK dataset in the training phase for domain adaptation. As an illustration, the last two rows of Fig.¬†10 show some results produced by our method on images from the CUHK dataset (before thresholding), and more results are provided in the supplementary material."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Single image depth estimation trained via depth from defocus cues",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "S. Gur and L. Wolf"
    },
    {
      "index": 1,
      "title": "A local metric for defocus blur detection based on cnn feature learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "K. Zeng, Y. Wang, J. Mao, J. Liu, W. Peng, and N. Chen"
    },
    {
      "index": 2,
      "title": "A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation",
      "abstract": "",
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition (CVPR)",
      "authors": "J. Park, Y.-W. Tai, D. Cho, and I. S. Kweon",
      "orig_title": "A unified approach of multi-scale deep and hand-crafted features for defocus estimation",
      "paper_id": "1704.08992v1"
    },
    {
      "index": 3,
      "title": "Salient region detection by ufo: Uniqueness, focusness and objectness",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "authors": "P. Jiang, H. Ling, J. Yu, and J. Peng"
    },
    {
      "index": 4,
      "title": "Image retargeting based on spatially varying defocus blur map",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE International Conference on Image Processing (ICIP)",
      "authors": "A. Karaali and C. R. Jung"
    },
    {
      "index": 5,
      "title": "Fast high-quality non-blind deconvolution using sparse adaptive priors",
      "abstract": "",
      "year": "2014",
      "venue": "The Visual Computer",
      "authors": "H. E. Fortunato and M. M. Oliveira"
    },
    {
      "index": 6,
      "title": "Edge-based defocus blur estimation with adaptive scale selection",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "A. Karaali and C. R. Jung"
    },
    {
      "index": 7,
      "title": "Spatially varying defocus blur estimation and applications",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "A. Karaali"
    },
    {
      "index": 8,
      "title": "Defocus map estimation from a single image",
      "abstract": "",
      "year": "2011",
      "venue": "Pattern Recognition",
      "authors": "S. Zhuo and T. Sim"
    },
    {
      "index": 9,
      "title": "Single-image refocusing and defocusing",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "W. Zhang and W. K. Cham"
    },
    {
      "index": 10,
      "title": "Spatially variant defocus blur map estimation and deblurring from a single image",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Visual Communication and Image Representation",
      "authors": "X. Zhang, R. Wang, X. Jiang, W. Wang, and W. Gao"
    },
    {
      "index": 11,
      "title": "A new sense for depth of field",
      "abstract": "",
      "year": "1987",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "A. P. Pentland"
    },
    {
      "index": 12,
      "title": "Local scale control for edge detection and blur estimation",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "J. H. Elder and S. W. Zucker"
    },
    {
      "index": 13,
      "title": "A closed-form solution to natural image matting",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "A. Levin, D. Lischinski, and Y. Weiss"
    },
    {
      "index": 14,
      "title": "Digital multi-focusing from a single photograph taken with an uncalibrated conventional camera",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Y. Cao, S. Fang, and Z. Wang"
    },
    {
      "index": 15,
      "title": "Adaptive scale selection for multiresolution defocus blur estimation",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE International Conference on Image Processing (ICIP)",
      "authors": "A. Karaali and C. R. Jung"
    },
    {
      "index": 16,
      "title": "Fast defocus map estimation",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE International Conference on Image Processing (ICIP)",
      "authors": "D. J. Chen, H. T. Chen, and L. W. Chang"
    },
    {
      "index": 17,
      "title": "Slic superpixels compared to state-of-the-art superpixel methods",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S√ºsstrunk"
    },
    {
      "index": 18,
      "title": "Analyzing spatially-varying blur",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Chakrabarti, T. Zickler, and W. T. Freeman"
    },
    {
      "index": 19,
      "title": "Estimating spatially varying defocus blur from a single image",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "X. Zhu, S. Cohen, S. Schiller, and P. Milanfar"
    },
    {
      "index": 20,
      "title": "Non-parametric blur map regression for depth of field extension",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "L. D‚ÄôAndr√®s, J. Salvador, A. Kochale, and S. S√ºsstrunk"
    },
    {
      "index": 21,
      "title": "Defocus map estimation from a single image using improved likelihood feature and edge-based basis",
      "abstract": "",
      "year": "2020",
      "venue": "Pattern Recognition",
      "authors": "S. Liu, Q. Liao, J.-H. Xue, and F. Zhou"
    },
    {
      "index": 22,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)",
      "authors": "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei",
      "orig_title": "ImageNet Large Scale Visual Recognition Challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 23,
      "title": "Fast Spatio-Temporal Residual Network for Video Super-Resolution",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "S. Li, F. He, B. Du, L. Zhang, Y. Xu, and D. Tao",
      "orig_title": "Fast spatio-temporal residual network for video super-resolution",
      "paper_id": "1904.02870v1"
    },
    {
      "index": 24,
      "title": "Dynamic scene deblurring with parameter selective sharing and nested skip connections",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "H. Gao, X. Tao, X. Shen, and J. Jia"
    },
    {
      "index": 25,
      "title": "A local metric for defocus blur detection based on cnn feature learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "K. Zeng, Y. Wang, J. Mao, J. Liu, W. Peng, and N. Chen"
    },
    {
      "index": 26,
      "title": "Learning to understand image blur",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "S. Zhang, X. Shen, Z. Lin, R. Mƒõch, J. P. Costeira, and J. M. F. Moura"
    },
    {
      "index": 27,
      "title": "Deep defocus map estimation using domain adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "J. Lee, S. Lee, S. Cho, and S. Lee"
    },
    {
      "index": 28,
      "title": "Unsupervised Domain Adaptation by Backpropagation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Y. Ganin and V. Lempitsky",
      "orig_title": "Unsupervised domain adaptation by backpropagation",
      "paper_id": "1409.7495v2"
    },
    {
      "index": 29,
      "title": "R2mrf: Defocus blur detection via recurrently refining multi-scale residual features",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "C. Tang, X. Liu, X. Zhu, E. Zhu, K. Sun, P. Wang, L. Wang, and A. Zomaya"
    },
    {
      "index": 30,
      "title": "Defocus map estimation from a single image based on two-parameter defocus model",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "S. Liu, F. Zhou, and Q. Liao"
    },
    {
      "index": 31,
      "title": "Real-time lens blur effects and focus control",
      "abstract": "",
      "year": "2010",
      "venue": "ACM Trans. Graph.",
      "authors": "S. Lee, E. Eisemann, and H.-P. Seidel"
    },
    {
      "index": 32,
      "title": "A computational approach to edge detection",
      "abstract": "",
      "year": "1986",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "J. Canny"
    },
    {
      "index": 33,
      "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "D. Eigen, C. Puhrsch, and R. Fergus",
      "orig_title": "Depth map prediction from a single image using a multi-scale deep network",
      "paper_id": "1406.2283v1"
    },
    {
      "index": 34,
      "title": "Hierarchical saliency detection",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Q. Yan, L. Xu, J. Shi, and J. Jia"
    },
    {
      "index": 35,
      "title": "Domain transform for edge-aware image and video processing",
      "abstract": "",
      "year": "2011",
      "venue": "ACM TOG",
      "authors": "E. S. L. Gastal and M. M. Oliveira"
    },
    {
      "index": 36,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll√°r, and C. L. Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 37,
      "title": "Visual Saliency Based on Multiscale Deep Features",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Guanbin Li and Y. Yu",
      "orig_title": "Visual saliency based on multiscale deep features",
      "paper_id": "1503.08663v3"
    },
    {
      "index": 38,
      "title": "Flickr",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "riesebusch"
    },
    {
      "index": 39,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 40,
      "title": "Fast image deconvolution using hyper-laplacian priors",
      "abstract": "",
      "year": "2009",
      "venue": "Neural Information Processing Systems (NIPS)",
      "authors": "D. Krishnan and R. Fergus"
    },
    {
      "index": 41,
      "title": "Image and depth from a conventional camera with a coded aperture",
      "abstract": "",
      "year": "2007",
      "venue": "ACM transactions on graphics (TOG)",
      "authors": "A. Levin, R. Fergus, F. Durand, and W. T. Freeman"
    },
    {
      "index": 42,
      "title": "Discriminative blur detection features",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Shi, L. Xu, and J. Jia"
    }
  ]
}