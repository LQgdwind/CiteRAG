{
  "paper_id": "2107.01832v3",
  "title": "Provable Convergence of Nesterov‚Äôs Accelerated Gradient Method for Over-Parameterized Neural Networks",
  "sections": {
    "introduction": "Momentum methods play a crucial role in numerous areas, including machine learning¬†, signal processing¬†, and control¬†.\nTypical momentum techniques, including heavy-ball method¬†(HB)¬† and Nesterov‚Äôs accelerated gradient method¬†(NAG)¬†, improve the performance of gradient descent¬†(GD) for tackling convex tasks both in theoretical and empirical performance.\nIn the case of a quadratic strongly convex problem, HB has an accelerated convergence rate compared to GD¬†, implying that HB requires fewer iterations than GD to reach the same training error.\nIn 1983, Nesterov¬† proposed the NAG method and proved that it has the optimal convergence rate for convex problem with Lipschitz gradient. Given the success of momentum methods in convex optimization, they have also been widely adopted in training neural networks for faster convergence¬†  .\nNowadays, many popular modern methods have taken advantage of momentum techniques, such as Adam¬†, AMSGrad¬†0, and AdaBound¬†1.\nIn many popular deep learning libraries, momentum methods and their variants are implemented as the default optimizers¬†2 3 4.\nNonetheless, the optimization problem for the neural network is both non-convex and non-smooth due to the usage of the non-linear activation functions.\nIn general, it is NP-hard to obtain the global-optimal solution for handling non-convex problems¬†5.\nFrom a theoretical view, it remains unclear whether momentum methods are capable of learning a neural network with low training loss, let alone the acceleration of momentum methods over GD. Recently, some theoretical progress has been made towards bridging this gap by analyzing the convergence of (stochastic) GD for training an over-parameterized two-layer ReLU neural network¬†6 7 8 9 0 1, where the number of the parameters is much larger than that of the training data.\nThe main idea is to investigate the trajectory of gradient-based methods via a kernel matrix called neural tangent kernel¬†(NTK), which was first introduced by Jacot¬†2 to study the optimization of infinite wide neural networks.\nHowever, most existing literature is concerned with GD.\nTo our knowledge, there are only two recent papers on the convergence of momentum methods in training neural networks¬†3 4.\nFocusing on a discrete-time setting, Wang et al.¬†3 proved HB is able to achieve a linear convergence rate to the global optimum and attains an acceleration beyond GD.\nFrom a continuous-time perspective, Bu et al.¬†4 found a similar result for HB.\nNevertheless, their analysis relies on the approximation between a second-order ordinary differential equation¬†(ODE) and the momentum method with an infinitesimal learning rate, which is far from practical implementations.\nMoreover, their result showed that NAG with a time-varying momentum coefficient converges at an asymoptotic sublinear rate, which is inferior to GD¬†6 5 and HB¬†3.\nIn contrast, when optimizing a neural network, it was empirically observed that NAG outperforms GD and exhibits comparable (even better) performance compared to HB¬† 6.\nTherefore, there is a lack of enough understandings about the acceleration of NAG. In this work, we consider training a randomly initialized over-parameterized two-layer ReLU neural network with NAG.\nIn fact, there are several variants of NAG proposed by Nesterov¬†7.\nWe focus on NAG with a constant momentum parameter, which is the default scheme of NAG implemented in PyTorch¬†2, Keras¬†3 and TensorFlow¬†4.\nInspired by¬†6 3, we exploit the connection between the NTK and the wide neural network to establish theoretical convergence guarantees for NAG.\nSpecifically, our contributions can be summarized as follows: Firstly, we intuitively show that the residual dynamics of an infinite width neural network trained by NAG can be approximated by a linear discrete dynamical system, whose coefficient matrix is determined by NAG‚Äôs hyperparameters and the NTK matrix.\nWhen the spectral norm of the coefficient matrix is less than 1, NAG is able to attain a global minimum at an asymptotic linear convergence rate according to Gelfand‚Äôs formula¬†8. Secondly, borrowing the idea from the infinite width case, we establish the residual dynamics of NAG in training a finite width neural network.\nBy analyzing the dynamics, we show that NAG converges to a global minimum at a non-asymptotic rate (1‚àíŒò‚Äã(1/Œ∫))tsuperscript1Œò1ùúÖùë°(1-\\Theta(1/\\sqrt{\\kappa}))^{t}, where Œ∫>1ùúÖ1\\kappa>1 is the condition number of the NTK matrix and tùë°t is the number of the iterations.\nMoreover, compared to the convergence rate (1‚àíŒò‚Äã(1/Œ∫))tsuperscript1Œò1ùúÖùë°(1-\\Theta(1/{\\kappa}))^{t} of GD¬†6 5, our result provides theoretical guarantees for the acceleration of NAG over GD. Thirdly, we demonstrate that NAG exhibits a different residual dynamics compared to HB¬†3, but the corresponding coefficient matrix shares a similar spectral norm, which results in a comparable convergence rate as HB.\nOur analysis of the residual dynamics induced by NAG is of independent interest and may further extend to study other NAG-like algorithms and the convergence of NAG in training other types of neural network. Finally, we conduct extensive experiments on six benchmark datasets.\nIn the convergence analysis, we empirically show that NAG outperforms GD and obtains a comparable and even better performance compared to HB, which verifies our theoretical results.\nFurthermore, using all six datasets, we investigate the impact of the over-parameterization on two quantities related to our proof. The result also suggests the correctness of our findings."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Accelerated optimization for machine learning: First-order algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "Z. Lin, H. Li, C. Fang"
    },
    {
      "index": 1,
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences",
      "authors": "A. Beck, M. Teboulle"
    },
    {
      "index": 2,
      "title": "Accelerated distributed nesterov gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "G. Qu, N. Li"
    },
    {
      "index": 3,
      "title": "Some methods of speeding up the convergence of iteration methods",
      "abstract": "",
      "year": "1964",
      "venue": "USSR Computational Mathematics and Mathematical Physics",
      "authors": "B. T. Polyak"
    },
    {
      "index": 4,
      "title": "A method for solving the convex programming problem with convergence rate o (1/k^ 2)",
      "abstract": "",
      "year": "1983",
      "venue": "Dokl. akad. nauk Sssr",
      "authors": "Y. E. Nesterov"
    },
    {
      "index": 5,
      "title": "On the importance of initialization and momentum in deep learning",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "I. Sutskever, J. Martens, G. Dahl, G. Hinton"
    },
    {
      "index": 6,
      "title": "Incorporating nesterov momentum into adam",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "T. Dozat"
    },
    {
      "index": 7,
      "title": "Quasi-hyperbolic momentum and Adam for deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Ma, D. Yarats",
      "orig_title": "Quasi-hyperbolic momentum and adam for deep learning",
      "paper_id": "1810.06801v4"
    },
    {
      "index": 8,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. P. Kingma, J. Ba"
    },
    {
      "index": 9,
      "title": "On the convergence of Adam and Beyond",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. J. Reddi, S. Kale, S. Kumar",
      "orig_title": "On the convergence of adam and beyond",
      "paper_id": "1904.09237v1"
    },
    {
      "index": 10,
      "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.09843",
      "authors": "L. Luo, Y. Xiong, Y. Liu, X. Sun",
      "orig_title": "Adaptive gradient methods with dynamic bound of learning rate",
      "paper_id": "1902.09843v1"
    },
    {
      "index": 11,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K√∂pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 12,
      "title": "Deep learning with Keras",
      "abstract": "",
      "year": "2017",
      "venue": "Packt Publishing Ltd",
      "authors": "A. Gulli, S. Pal"
    },
    {
      "index": 13,
      "title": "TensorFlow: A system for large-scale machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "Symposium on Operating Systems Design and Implementation",
      "authors": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X. Zheng",
      "orig_title": "Tensorflow: A system for large-scale machine learning",
      "paper_id": "1605.08695v2"
    },
    {
      "index": 14,
      "title": "Some np-complete problems in quadratic and nonlinear programming",
      "abstract": "",
      "year": "1987",
      "venue": "Mathematical Programming",
      "authors": "K. G. Murty, S. N. Kabadi"
    },
    {
      "index": 15,
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. S. Du, X. Zhai, B. Poczos, A. Singh",
      "orig_title": "Gradient descent provably optimizes over-parameterized neural networks",
      "paper_id": "1810.02054v2"
    },
    {
      "index": 16,
      "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Y. Li, Y. Liang",
      "orig_title": "Learning overparameterized neural networks via stochastic gradient descent on structured data",
      "paper_id": "1808.01204v3"
    },
    {
      "index": 17,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. S. Du, J. D. Lee, H. Li, L. Wang, X. Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 18,
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Z. Allen-Zhu, Y. Li, Z. Song",
      "orig_title": "A convergence theory for deep learning via over-parameterization",
      "paper_id": "1811.03962v5"
    },
    {
      "index": 19,
      "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Wang",
      "orig_title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
      "paper_id": "1901.08584v2"
    },
    {
      "index": 20,
      "title": "Quadratic suffices for over-parametrization via matrix chernoff bound",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.03593",
      "authors": "Z. Song, X. Yang"
    },
    {
      "index": 21,
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Jacot, F. Gabriel, C. Hongler",
      "orig_title": "Neural tangent kernel: Convergence and generalization in neural networks",
      "paper_id": "1806.07572v4"
    },
    {
      "index": 22,
      "title": "A Modular Analysis of Provable Acceleration via Polyak‚Äôs Momentum: Training a Wide ReLU Network and a Deep Linear Network",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Wang, C. Lin, J. D. Abernethy",
      "orig_title": "A modular analysis of provable acceleration via polyak‚Äôs momentum: Training a wide relu network and a deep linear network",
      "paper_id": "2010.01618v6"
    },
    {
      "index": 23,
      "title": "A Dynamical View on Optimization Algorithms of Overparameterized Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Z. Bu, S. Xu, K. Chen",
      "orig_title": "A dynamical view on optimization algorithms of overparameterized neural networks",
      "paper_id": "2010.13165v2"
    },
    {
      "index": 24,
      "title": "Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07111",
      "authors": "X. Wu, S. S. Du, R. Ward",
      "orig_title": "Global convergence of adaptive gradient methods for an over-parameterized neural network",
      "paper_id": "1902.07111v2"
    },
    {
      "index": 25,
      "title": "Descending through a crowded valley - benchmarking deep learning optimizers",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "R. M. Schmidt, F. Schneider, P. Hennig"
    },
    {
      "index": 26,
      "title": "Introductory lectures on convex optimization: A basic course",
      "abstract": "",
      "year": "2003",
      "venue": "Springer Science & Business Media",
      "authors": "Y. Nesterov"
    },
    {
      "index": 27,
      "title": "Normierte ringe",
      "abstract": "",
      "year": "1941",
      "venue": "Recueil Math√©matique",
      "authors": "I. Gelfand"
    },
    {
      "index": 28,
      "title": "The strength of nesterov‚Äôs extrapolation in the individual convergence of nonsmooth optimization",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "W. Tao, Z. Pan, G. Wu, Q. Tao"
    },
    {
      "index": 29,
      "title": "Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints",
      "abstract": "",
      "year": "2016",
      "venue": "SIAM Journal on Optimization",
      "authors": "L. Lessard, B. Recht, A. Packard",
      "orig_title": "Analysis and design of optimization algorithms via integral quadratic constraints",
      "paper_id": "1408.3595v7"
    },
    {
      "index": 30,
      "title": "A differential equation for modeling nesterov‚Äôs accelerated gradient method: Theory and insights",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "W. Su, S. Boyd, E. J. Cand√®s"
    },
    {
      "index": 31,
      "title": "Understanding the acceleration phenomenon via high-resolution differential equations",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematical Programming",
      "authors": "B. Shi, S. Du, M. Jordan, W. Su"
    },
    {
      "index": 32,
      "title": "Lower bounds for finding stationary points I",
      "abstract": "",
      "year": "2020",
      "venue": "Mathematical Programming",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 33,
      "title": "How to Escape Saddle Points Efficiently",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, M. I. Jordan",
      "orig_title": "How to escape saddle points efficiently",
      "paper_id": "1703.00887v1"
    },
    {
      "index": 34,
      "title": "Convex until proven guilty: Dimension-free acceleration of gradient descent on non-convex functions",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 35,
      "title": "Generalized momentum-based methods: A hamiltonian perspective",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Optimization",
      "authors": "J. Diakonikolas, M. I. Jordan"
    },
    {
      "index": 36,
      "title": "On exact computation with an infinitely wide neural net",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, R. Wang"
    },
    {
      "index": 37,
      "title": "Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? ‚Äî A Neural Tangent Kernel Perspective",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. Huang, Y. Wang, M. Tao, T. Zhao",
      "orig_title": "Why do deep residual networks generalize better than deep feedforward networks? - A neural tangent kernel perspective",
      "paper_id": "2002.06262v2"
    },
    {
      "index": 38,
      "title": "Graph neural tangent kernel: Fusing graph neural networks with graph kernels",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. S. Du, K. Hou, R. Salakhutdinov, B. P√≥czos, R. Wang, K. Xu"
    },
    {
      "index": 39,
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "National Academy of Sciences",
      "authors": "S. Mei, A. Montanari, P.-M. Nguyen",
      "orig_title": "A mean field view of the landscape of two-layers neural networks",
      "paper_id": "1804.06561v2"
    },
    {
      "index": 40,
      "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, F. Bach"
    },
    {
      "index": 41,
      "title": "Acceleration via symplectic discretization of high-resolution differential equations",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Shi, S. S. Du, W. Su, M. I. Jordan"
    },
    {
      "index": 42,
      "title": "Optimization Landscape and Expressivity of Deep CNNs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Q. Nguyen, M. Hein",
      "orig_title": "Optimization landscape and expressivity of deep cnns",
      "paper_id": "1710.10928v2"
    },
    {
      "index": 43,
      "title": "On Lazy Training in Differentiable Programming",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, E. Oyallon, F. Bach",
      "orig_title": "On lazy training in differentiable programming",
      "paper_id": "1812.07956v5"
    },
    {
      "index": 44,
      "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, J. Pennington"
    },
    {
      "index": 45,
      "title": "From averaging to acceleration, there is only a step-size",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "N. Flammarion, F. Bach"
    },
    {
      "index": 46,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.07747",
      "authors": "H. Xiao, K. Rasul, R. Vollgraf",
      "orig_title": "Fashion-mnist: A novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 47,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Y. LeCun, L. Bottou, Y. Bengio, P. Haffner"
    },
    {
      "index": 48,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "A. Krizhevsky, G. Hinton, et al."
    },
    {
      "index": 49,
      "title": "Finite Versus Infinite Neural Networks: an Empirical Study",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systemsl",
      "authors": "J. Lee, S. S. Schoenholz, J. Pennington, B. Adlam, L. Xiao, R. Novak, J. Sohl-Dickstein",
      "orig_title": "Finite versus infinite neural networks: An empirical study",
      "paper_id": "2007.15801v2"
    },
    {
      "index": 50,
      "title": "JAX: Composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, Q. Zhang"
    },
    {
      "index": 51,
      "title": "Position-transitional particle swarm optimization-incorporated latent factor analysis",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "X. Luo, Y. Yuan, S. Chen, N. Zeng, Z. Wang"
    },
    {
      "index": 52,
      "title": "A data-characteristic-aware latent factor model for web services qos prediction",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "D. Wu, X. Luo, M. Shang, Y. He, G. Wang, X. Wu"
    },
    {
      "index": 53,
      "title": "Fast and accurate non-negative latent factor analysis on high-dimensional and sparse matrices in recommender systems",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge & Data Engineering",
      "authors": "X. Luo, Y. Zhou, Z. Liu, M. Zhou"
    },
    {
      "index": 54,
      "title": "A novel approach to large-scale dynamically weighted directed network representation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "X. Luo, H. Wu, Z. Wang, J. Wang, D. Meng"
    }
  ]
}