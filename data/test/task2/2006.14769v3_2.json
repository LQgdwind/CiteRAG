{
  "paper_id": "2006.14769v3",
  "title": "Supermasks in Superposition",
  "sections": {
    "scenarios 𝖦𝖭𝗌𝖦𝖭𝗌\\mathsf{gns} & 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{gnu}: task identity information given during train only": "Our solutions for 𝖦𝖭𝗌𝖦𝖭𝗌\\mathsf{GNs} and 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu} are very similar. Because 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu} is strictly more difficult, we focus on only evaluating in Scenario 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu}. For relevant figures we provide a corresponding table in Section H. Datasets  Experiments are conducted on PermutedMNIST, RotatedMNIST, and SplitMNIST. For PermutedMNIST , new tasks are created with a fixed random permutation of the pixels of MNIST. For RotatedMNIST, images are rotated by 10 degrees to form a new task with 36 tasks in total (similar to ). Finally SplitMNIST partitions MNIST into 5 different 2-way classification tasks, each containing consecutive classes from the original dataset. Training  We consider two architectures: 1) a fully connected network with two hidden layers of size 1024 (denoted FC 1024-1024 and used in ) 2) the LeNet 300-100 architecture  as used in [ref]8 .\nFor each task we train for 1000 batches of size 128 using the RMSProp optimizer 8 with learning rate 0.00010.00010.0001 which follows the hyperparameters of . Supermasks are found using the algorithm of Mallya et al. 1 with threshold value 0. However, we initialize the real valued “scores” with Kaiming uniform as in . Training the mask is not a focus of this work, we choose this method as it is fast and we are not concerned about controlling mask sparsity as in Section 4.1. Evaluation  At test time we perform inference of task identity once for each batch. If task is not inferred correctly then accuracy is 0 for the batch. Unless noted otherwise we showcase results for the most challenging scenario — when the task identity is inferred using a single image. We use “Full Batch” to indicate that all 128 images are used to infer task identity. Moreover, we experiment with both the the entropy ℋℋ\\mathcal{H} and 𝒢𝒢\\mathcal{G} (Section 3.6) objectives to perform task identity inference. Results  Figure 4 illustrates that SupSup is able to sequentially learn 2500 permutations of MNIST—SupSup succeeds in performing 25,000-way classification. This experiment is conducted with the One-Shot algorithm (requiring one gradient computation) using single images to infer task identity. The same trends hold in Figure 3, where SupSup outperforms methods which operate in Scenario 𝖦𝖦𝖦𝖦\\mathsf{GG} by using the One-Shot algorithm to infer task identity. In Figure 3, output sizes of 100 and 500 are respectively used for LeNet 300-100 and FC 1024-1024. The left hand side of Figure 5 illustrates that SupSup is able to infer task identity even when tasks are similar—SupSup is able to distinguish between rotations of 10 degrees. Since this is a more challenging problem, we use a full batch and the Binary algorithm to perform task identity inference. Figure 7 (appendix) shows that for HopSupSup on SplitMNIST, the new mask 𝐦𝐦\\mathbf{m} converges to the correct supermask in <30absent30<30 gradient steps. Baselines & Ablations  Figure 5 (left) shows that even in Scenario 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu}, SupSup is able to outperform PSP  and BatchE [ref]51 in Scenario 𝖦𝖦𝖦𝖦\\mathsf{GG}—methods using task identity. We compare SupSup in 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu} with methods in this strictly easier scenario as they are more competitive. For instance, 9 considers sequential learning problems with only 5-10 tasks. SupSup, after sequentially learning 250 permutations of MNIST, outperforms all non-replay methods from  in the 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu} scenario after they have learned only 10 permutations of MNIST with a similar network. In 𝖦𝖭𝗎𝖦𝖭𝗎\\mathsf{GNu}, Online EWC achieves 33.88% & SI achieves 29.31% on 10 permutations of MNIST 9 while SupSup achieves 94.91% accuracy after 250 permutations (see Table 5 in 9 vs. Table 7). In Figure 5 (right) we equip BatchE with task inference using our One-Shot algorithm. Instead of attaching a weight αisubscript𝛼𝑖\\alpha_{i} to each supermask, we attach a weight αisubscript𝛼𝑖\\alpha_{i} to each rank-one matrix [ref]51. Moreover, in Section C of the appendix we augment BatchE to perform task-inference using large batch sizes.\n“Upper Bound” and “Lower Bound” are the same as in Section 4.1. Moreover, Figure 6 illustrates the importance of output size. Further investigation of this phenomena is provided by Section 3.6 and Lemma 1 of Section I."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Convex neural networks",
      "abstract": "",
      "year": "2006",
      "venue": "Advances in neural information processing systems",
      "authors": "Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte"
    },
    {
      "index": 1,
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency",
      "authors": "Joy Buolamwini and Timnit Gebru"
    },
    {
      "index": 2,
      "title": "Efficient lifelong learning with a-gem",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.00420",
      "authors": "Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny"
    },
    {
      "index": 3,
      "title": "Superposition of many models into one",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen",
      "orig_title": "Superposition of many models into one",
      "paper_id": "1902.05522v2"
    },
    {
      "index": 4,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR 2009",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 5,
      "title": "Sparse Networks from Scratch: Faster Training without Losing Performance",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.04840",
      "authors": "Tim Dettmers and Luke Zettlemoyer",
      "orig_title": "Sparse networks from scratch: Faster training without losing performance",
      "paper_id": "1907.04840v2"
    },
    {
      "index": 6,
      "title": "An algorithm for quadratic programming",
      "abstract": "",
      "year": "1956",
      "venue": "Naval research logistics quarterly",
      "authors": "Marguerite Frank and Philip Wolfe"
    },
    {
      "index": 7,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.03635",
      "authors": "Jonathan Frankle and Michael Carbin",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 8,
      "title": "Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.00152",
      "authors": "Jonathan Frankle, David J Schwab, and Ari S Morcos",
      "orig_title": "Training batchnorm and only batchnorm: On the expressive power of random features in cnns",
      "paper_id": "2003.00152v3"
    },
    {
      "index": 9,
      "title": "Catastrophic forgetting in connectionist networks",
      "abstract": "",
      "year": "1999",
      "venue": "Trends in cognitive sciences",
      "authors": "Robert M French"
    },
    {
      "index": 10,
      "title": "Continual learning via neural pruning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1903.04476",
      "authors": "Siavash Golkar, Michael Kagan, and Kyunghyun Cho"
    },
    {
      "index": 11,
      "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6211",
      "authors": "Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio"
    },
    {
      "index": 12,
      "title": "Your classifier is secretly an energy based model and you should treat it like one",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.03263",
      "authors": "Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky",
      "orig_title": "Your classifier is secretly an energy based model and you should treat it like one",
      "paper_id": "1912.03263v3"
    },
    {
      "index": 13,
      "title": "On Calibration of Modern Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger",
      "orig_title": "On calibration of modern neural networks",
      "paper_id": "1706.04599v2"
    },
    {
      "index": 14,
      "title": "Hypernetworks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.09106",
      "authors": "David Ha, Andrew Dai, and Quoc V Le"
    },
    {
      "index": 15,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.05722",
      "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 16,
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun"
    },
    {
      "index": 17,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 18,
      "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1610.02136",
      "authors": "Dan Hendrycks and Kevin Gimpel",
      "orig_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "paper_id": "1610.02136v3"
    },
    {
      "index": 19,
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "abstract": "",
      "year": "1982",
      "venue": "national academy of sciences",
      "authors": "John J Hopfield"
    },
    {
      "index": 20,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1502.03167",
      "authors": "Sergey Ioffe and Christian Szegedy"
    },
    {
      "index": 21,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 22,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "national academy of sciences",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 23,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "University of Toronto",
      "authors": "Alex Krizhevsky"
    },
    {
      "index": 24,
      "title": "Backpropagation applied to handwritten zip code recognition",
      "abstract": "",
      "year": "1989",
      "venue": "Neural computation",
      "authors": "Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel"
    },
    {
      "index": 25,
      "title": "Mnist handwritten digit database",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Yann LeCun, Corinna Cortes, and CJ Burges"
    },
    {
      "index": 26,
      "title": "Gradient Episodic Memory for Continual Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Lopez-Paz and Marc’Aurelio Ranzato",
      "orig_title": "Gradient episodic memory for continual learning",
      "paper_id": "1706.08840v6"
    },
    {
      "index": 27,
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Sgdr: Stochastic gradient descent with warm restarts",
      "paper_id": "1608.03983v5"
    },
    {
      "index": 28,
      "title": "Proving the Lottery Ticket Hypothesis: Pruning is All You Need",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.00585",
      "authors": "Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir",
      "orig_title": "Proving the lottery ticket hypothesis: Pruning is all you need",
      "paper_id": "2002.00585v1"
    },
    {
      "index": 29,
      "title": "Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Arun Mallya, Dillon Davis, and Svetlana Lazebnik",
      "orig_title": "Piggyback: Adapting a single network to multiple tasks by learning to mask weights",
      "paper_id": "1801.06519v2"
    },
    {
      "index": 30,
      "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Arun Mallya and Svetlana Lazebnik",
      "orig_title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
      "paper_id": "1711.05769v2"
    },
    {
      "index": 31,
      "title": "Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization",
      "abstract": "",
      "year": "2018",
      "venue": "National Academy of Sciences",
      "authors": "Nicolas Y Masse, Gregory D Grant, and David J Freedman",
      "orig_title": "Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization",
      "paper_id": "1802.01569v2"
    },
    {
      "index": 32,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of learning and motivation",
      "authors": "Michael McCloskey and Neal J Cohen"
    },
    {
      "index": 33,
      "title": "Model Cards for Model Reporting",
      "abstract": "",
      "year": "2019",
      "venue": "conference on fairness, accountability, and transparency",
      "authors": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru",
      "orig_title": "Model cards for model reporting",
      "paper_id": "1810.03993v2"
    },
    {
      "index": 34,
      "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
      "abstract": "",
      "year": "2018",
      "venue": "Nature communications",
      "authors": "Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta"
    },
    {
      "index": 35,
      "title": "Revisiting natural gradient for deep networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1301.3584",
      "authors": "Razvan Pascanu and Yoshua Bengio"
    },
    {
      "index": 36,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 37,
      "title": "Searching for Activation Functions",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.05941",
      "authors": "Prajit Ramachandran, Barret Zoph, and Quoc V Le",
      "orig_title": "Searching for activation functions",
      "paper_id": "1710.05941v2"
    },
    {
      "index": 38,
      "title": "What’s Hidden in a Randomly Weighted Neural Network?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.13299",
      "authors": "Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari",
      "orig_title": "What’s hidden in a randomly weighted neural network?",
      "paper_id": "1911.13299v2"
    },
    {
      "index": 39,
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert",
      "orig_title": "icarl: Incremental classifier and representation learning",
      "paper_id": "1611.07725v2"
    },
    {
      "index": 40,
      "title": "Experience Replay for Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne",
      "orig_title": "Experience replay for continual learning",
      "paper_id": "1811.11682v2"
    },
    {
      "index": 41,
      "title": "Progressive Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.04671",
      "authors": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell",
      "orig_title": "Progressive neural networks",
      "paper_id": "1606.04671v4"
    },
    {
      "index": 42,
      "title": "An overview of reservoir computing: theory, applications and implementations",
      "abstract": "",
      "year": "2007",
      "venue": "15th european symposium on artificial neural networks",
      "authors": "Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout"
    },
    {
      "index": 43,
      "title": "Green ai. corr abs/1907.10597 (2019)",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.10597",
      "authors": "Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni"
    },
    {
      "index": 44,
      "title": "Continual Learning with Deep Generative Replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim",
      "orig_title": "Continual learning with deep generative replay",
      "paper_id": "1705.08690v3"
    },
    {
      "index": 45,
      "title": "Increasing the capacity of a hopfield network without sacrificing functionality",
      "abstract": "",
      "year": "1997",
      "venue": "International Conference on Artificial Neural Networks",
      "authors": "Amos Storkey"
    },
    {
      "index": 46,
      "title": "Lifelong learning algorithms",
      "abstract": "",
      "year": "1998",
      "venue": "Learning to learn",
      "authors": "Sebastian Thrun"
    },
    {
      "index": 47,
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "abstract": "",
      "year": "2012",
      "venue": "COURSERA: Neural networks for machine learning",
      "authors": "Tijmen Tieleman and Geoffrey Hinton"
    },
    {
      "index": 48,
      "title": "Three scenarios for continual learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.07734",
      "authors": "Gido M van de Ven and Andreas S Tolias"
    },
    {
      "index": 49,
      "title": "Continual learning with hypernetworks",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin F. Grewe"
    },
    {
      "index": 50,
      "title": "BatchEnsemble: An alternative approach to Efficient Ensemble and Lifelong Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.06715",
      "authors": "Yeming Wen, Dustin Tran, and Jimmy Ba",
      "orig_title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning",
      "paper_id": "2002.06715v2"
    },
    {
      "index": 51,
      "title": "Reinforced Continual Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ju Xu and Zhanxing Zhu",
      "orig_title": "Reinforced continual learning",
      "paper_id": "1805.12369v1"
    },
    {
      "index": 52,
      "title": "Lifelong Learning with Dynamically Expandable Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.01547",
      "authors": "Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang",
      "orig_title": "Lifelong learning with dynamically expandable networks",
      "paper_id": "1708.01547v11"
    },
    {
      "index": 53,
      "title": "Continual learning through synaptic intelligence",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "Friedemann Zenke, Ben Poole, and Surya Ganguli"
    },
    {
      "index": 54,
      "title": "Task Agnostic Continual Learning Using Online Variational Bayes",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.10123",
      "authors": "Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry",
      "orig_title": "Task agnostic continual learning using online variational bayes",
      "paper_id": "1803.10123v3"
    },
    {
      "index": 55,
      "title": "Incremental self-improvement for life-time multi-agent reinforcement learning",
      "abstract": "",
      "year": "1996",
      "venue": "Fourth International Conference on Simulation of Adaptive Behavior, Cambridge, MA",
      "authors": "Jieyu Zhao and Jurgen Schmidhuber"
    },
    {
      "index": 56,
      "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski",
      "orig_title": "Deconstructing lottery tickets: Zeros, signs, and the supermask",
      "paper_id": "1905.01067v4"
    }
  ]
}