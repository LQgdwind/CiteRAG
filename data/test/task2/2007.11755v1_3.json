{
  "paper_id": "2007.11755v1",
  "title": "History Repeats Itself: Human Motion Prediction via Motion Attention",
  "sections": {
    "introduction": "Human motion prediction consists of forecasting the future poses of a person given a history of their previous motion. Predicting human motion can be highly beneficial for tasks such as human tracking , human-robot interaction , and human motion generation for computer graphics   .\nTo tackle the problem effectively, recent approaches use deep neural networks  [ref]5  to model the temporal historical data. Traditional methods, such as hidden Markov models  and Gaussian Process Dynamical Models 2, have proven effective for simple motions, such as walking and golf swings. However, they are typically outperformed by deep learning ones on more complex motions.\nThe most common trend in modeling the sequential data that constitutes human motion consists of using Recurrent Neural Networks (RNNs) [ref]5 . However, as discussed in 7, in the mid- to long-term horizon, RNNs tend to generate static poses because they struggle to keep track of long-term history. To tackle this problem, existing works 7  either rely on Generative Adversarial Networks (GANs), which are notoriously hard to train , or introduce an additional long-term encoder to represent information from the further past 7. Unfortunately, such an encoder treats the entire motion history equally, thus not allowing the model to put more emphasis on some parts of the past motion that better reflect the context of the current motion. In this paper, by contrast, we introduce an attention-based motion prediction approach that effectively exploits historical information by dynamically adapting its focus on the previous motions to the current context. Our method is motivated by the observation that humans tend to repeat their motion, not only in short periodical activities, such as walking, but also in more complex actions occurring across longer time periods, such as sports and cooking activities .Therefore, we aim to find the relevant historical information to predict future motion. To the best of our knowledge, only 0 has attempted to leverage attention for motion prediction. This, however, was achieved in a frame-wise manner, by comparing the human pose from the last observable frame with each one in the historical sequence. As such, this approach fails to reflect the motion direction and is affected by the fact that similar poses may appear in completely different motions. For instance, in most Human3.6M activities, the actor will at some point be standing with their arm resting along their body. To overcome this, we therefore propose to model motion attention, and thus compare the last visible sub-sequence with a history of motion sub-sequences. To this end, inspired by [ref]20, we represent each sub-sequence in trajectory space using the Discrete Cosine Transform (DCT). We then exploit our motion attention as weights to aggregate the entire DCT-encoded motion history into a future motion estimate. This estimate is combined with the latest observed motion, and the result acts as input to a graph convolutional network (GCN), which lets us better encode spatial dependencies between different joints.As evidenced by our experiments on Human3.6M 0, AMASS 9, and 3DPW , and illustrated in Fig. 1, our motion attention-based approach consistently outperforms the state of the art on short-term and long-term motion prediction by training a single unified model for both settings. This contrasts with the previous-best model LTD [ref]20, which requires training different models for different settings to achieve its best performance.\nFurthermore, we demonstrate that it can effectively leverage the repetitiveness of motion in longer sequences. Our contributions can be summarized as follows. (i) We introduce an attention-based model that exploits motions instead of static frames to better leverage historical information for motion prediction; (ii) Our motion attention allows us to train a unified model for both short-term and long-term prediction; (iii) Our approach can effectively make use of motion repetitiveness in long-term history; (iv) It yields state-of-the-art results and generalizes better than existing methods across datasets and actions."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "L.: Towards principled methods for training generative adversarial networks. In: ICLR (2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Towards Principled Methods for Training Generative Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Arjovsky, M., Bottou, L.",
      "orig_title": "Towards principled methods for training generative adversarial networks",
      "paper_id": "1701.04862v1"
    },
    {
      "index": 2,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Bahdanau, D., Cho, K., Bengio, Y.",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 3,
      "title": "Style machines",
      "abstract": "",
      "year": "2000",
      "venue": "27th annual conference on Computer graphics and interactive techniques",
      "authors": "Brand, M., Hertzmann, A."
    },
    {
      "index": 4,
      "title": "Deep representation learning for human motion prediction and classification",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Butepage, J., Black, M.J., Kragic, D., Kjellstrom, H.",
      "orig_title": "Deep representation learning for human motion prediction and classification",
      "paper_id": "1702.07486v2"
    },
    {
      "index": 5,
      "title": "Recurrent Network Models for Human Dynamics",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Fragkiadaki, K., Levine, S., Felsen, P., Malik, J.",
      "orig_title": "Recurrent network models for human dynamics",
      "paper_id": "1508.00271v2"
    },
    {
      "index": 6,
      "title": "Multi-hypothesis motion planning for visual object tracking",
      "abstract": "",
      "year": "2011",
      "venue": "ICCV",
      "authors": "Gong, H., Sim, J., Likhachev, M., Shi, J."
    },
    {
      "index": 7,
      "title": "A neural temporal model for human motion prediction",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Gopalakrishnan, A., Mali, A., Kifer, D., Giles, L., Ororbia, A.G."
    },
    {
      "index": 8,
      "title": "Adversarial geometry-aware human motion prediction",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Gui, L.Y., Wang, Y.X., Liang, X., Moura, J.M."
    },
    {
      "index": 9,
      "title": "Human Motion Prediction via Spatio-Temporal Inpainting",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Hernandez, A., Gall, J., Moreno-Noguer, F.",
      "orig_title": "Human motion prediction via spatio-temporal inpainting",
      "paper_id": "1812.05478v2"
    },
    {
      "index": 10,
      "title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
      "abstract": "",
      "year": "2014",
      "venue": "TPAMI",
      "authors": "Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C."
    },
    {
      "index": 11,
      "title": "Structural-rnn: Deep learning on spatio-temporal graphs",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Jain, A., Zamir, A.R., Savarese, S., Saxena, A."
    },
    {
      "index": 12,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Kingma, D.P., Ba, J."
    },
    {
      "index": 13,
      "title": "Skip-Thought Vectors",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "Kiros, R., Zhu, Y., Salakhutdinov, R.R., Zemel, R., Urtasun, R., Torralba, A., Fidler, S.",
      "orig_title": "Skip-thought vectors",
      "paper_id": "1506.06726v1"
    },
    {
      "index": 14,
      "title": "Anticipating human activities for reactive robotic response",
      "abstract": "",
      "year": "2013",
      "venue": "IROS",
      "authors": "Koppula, H.S., Saxena, A."
    },
    {
      "index": 15,
      "title": "Motion graphs",
      "abstract": "",
      "year": "2008",
      "venue": "ACM SIGGRAPH 2008 classes",
      "authors": "Kovar, L., Gleicher, M., Pighin, F."
    },
    {
      "index": 16,
      "title": "Continuous character control with low-dimensional embeddings",
      "abstract": "",
      "year": "2012",
      "venue": "ACM Transactions on Graphics",
      "authors": "Levine, S., Wang, J.M., Haraux, A., Popović, Z., Koltun, V."
    },
    {
      "index": 17,
      "title": "Convolutional Sequence to Sequence Model for Human Dynamics",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Li, C., Zhang, Z., Lee, W.S., Lee, G.H.",
      "orig_title": "Convolutional sequence to sequence model for human dynamics",
      "paper_id": "1805.00655v1"
    },
    {
      "index": 18,
      "title": "SMPL: A skinned multi-person linear model",
      "abstract": "",
      "year": "2015",
      "venue": "ACM Trans. Graphics (SIGGRAPH Asia)",
      "authors": "Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J."
    },
    {
      "index": 19,
      "title": "AMASS: Archive of Motion Capture as Surface Shapes",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.",
      "orig_title": "Amass: Archive of motion capture as surface shapes",
      "paper_id": "1904.03278v1"
    },
    {
      "index": 20,
      "title": "Learning Trajectory Dependencies for Human Motion Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Mao, W., Liu, M., Salzmann, M., Li, H.",
      "orig_title": "Learning trajectory dependencies for human motion prediction",
      "paper_id": "1908.05436v3"
    },
    {
      "index": 21,
      "title": "Recovering accurate 3d human pose in the wild using imus and a moving camera",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "von Marcard, T., Henschel, R., Black, M., Rosenhahn, B., Pons-Moll, G."
    },
    {
      "index": 22,
      "title": "On human motion prediction using recurrent neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Martinez, J., Black, M.J., Romero, J."
    },
    {
      "index": 23,
      "title": "Rectified linear units improve restricted boltzmann machines",
      "abstract": "",
      "year": "2010",
      "venue": "ICML",
      "authors": "Nair, V., Hinton, G.E."
    },
    {
      "index": 24,
      "title": "Automatic differentiation in pytorch",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS-W",
      "authors": "Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A."
    },
    {
      "index": 25,
      "title": "Modeling human motion with quaternion-based neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "IJCV",
      "authors": "Pavllo, D., Feichtenhofer, C., Auli, M., Grangier, D."
    },
    {
      "index": 26,
      "title": "Embodied Hands: Modeling and Capturing Hands and Bodies Together",
      "abstract": "",
      "year": "2017",
      "venue": "ACM Transactions on Graphics (SIGGRAPH Asia)",
      "authors": "Romero, J., Tzionas, D., Black, M.J.",
      "orig_title": "Embodied hands: Modeling and capturing hands and bodies together",
      "paper_id": "2201.02610v1"
    },
    {
      "index": 27,
      "title": "Real-World Repetition Estimation by Div, Grad and Curl",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Runia, T.F., Snoek, C.G., Smeulders, A.W.",
      "orig_title": "Real-world repetition estimation by div, grad and curl",
      "paper_id": "1802.09971v1"
    },
    {
      "index": 28,
      "title": "Implicit probabilistic models of human motion for synthesis and tracking",
      "abstract": "",
      "year": "2002",
      "venue": "ECCV",
      "authors": "Sidenbladh, H., Black, M.J., Sigal, L."
    },
    {
      "index": 29,
      "title": "Generating text with recurrent neural networks",
      "abstract": "",
      "year": "2011",
      "venue": "ICML",
      "authors": "Sutskever, I., Martens, J., Hinton, G.E."
    },
    {
      "index": 30,
      "title": "Long-Term Human Motion Prediction by Modeling Motion Context and Enhancing Motion Dynamic",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "Tang, Y., Ma, L., Liu, W., Zheng, W.S.",
      "orig_title": "Long-term human motion prediction by modeling motion context and enhancing motion dynamics",
      "paper_id": "1805.02513v1"
    },
    {
      "index": 31,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    }
  ]
}