{
  "paper_id": "2208.07365v3",
  "title": "Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective",
  "sections": {
    "introduction": "Over the past decades, unsupervised domain adaptation (UDA) has attracted extensive research attention [ref]41. Numerous UDA methods have been proposed and successfully applied to various real-world applications, e.g., object recognition 8  [ref]47, semantic segmentation    , and object detection   . However, most of these methods and their applications are limited to the image domain, while much less attention has been devoted to video-based UDA, where the latter is undoubtedly more challenging. Compared with image-based UDA, the source and target domains also differ temporally in video-based UDA. Images are spatially well-structured data, while videos are sequences of images with both spatial and temporal relations. Existing image-based UDA methods can hardly achieve satisfactory performance on video-based UDA tasks as they fail to consider the temporal dependency of video frames in handling the domain gaps. For instance, in video-based cross-domain action recognition tasks, domain gaps are presented by not only the actions of different persons in different scenarios but also the actions that appear at different timestamps or last at different time lengths. Recently, few works have been proposed for video-based UDA.\nThe key idea is to achieve domain alignment by aligning both frame- and video-level features through adversarial learning  , contrastive learning 7 1, attention , or combination of these mechanisms, e.g., adversarial learning with attention  . Though they have advanced video-based UDA, there is still room for improvement. Generally, existing methods follow an all-in-one way, where both spatial and temporal domain divergence are handled together, for adaptation (Fig.Â 1, â– â– \\blacksquareâ–¡â–¡\\square). However, cross-domain videos are highly complex data containing diverse mixed-up information, e.g., domain, semantic, and temporal information, which makes the simultaneous elimination of spatial and temporal divergence insufficient. This motivates us to handle the video-based UDA from a disentanglement perspective (Fig.Â 1, â– â– \\blacksquareâ–¡â–¡\\square) so that the spatial and temporal divergence can be well handled separately. To achieve this goal, we first consider the generation process of cross-domain videos, as shown in Fig.Â 2, where a video is generated from two sets of latent factors: one set consists of a sequence of random variables, which are dynamic and incline to encode the semantic information for downstream tasks, e.g., action recognition;\nanother set is static and introduces some domain-related spatial information to the generated video, e.g., style or appearance. Specifically, the blue / red nodes are the observed source / target videos ğ±ğ’®superscriptğ±ğ’®\\mathbf{x}^{\\mathcal{S}} / ğ±ğ’¯superscriptğ±ğ’¯\\mathbf{x}^{\\mathcal{T}}, respectively, over tğ‘¡t timestamps. Static latent variables ğ³dğ’®superscriptsubscriptğ³ğ‘‘ğ’®\\mathbf{z}_{d}^{\\mathcal{S}} and ğ³dğ’¯superscriptsubscriptğ³ğ‘‘ğ’¯\\mathbf{z}_{d}^{\\mathcal{T}} follow a joint distribution and combining either of them with dynamic latent variables ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t} constructs one video data of a domain. With the above generative model, we develop a Transfer Sequential Variational AutoEncoder (TranSVAE) for video-based UDA. TranSVAE handles the cross-domain divergence in two levels, where the first level removes the spatial divergence by disentangling ğ³dsubscriptğ³ğ‘‘\\mathbf{z}_{d} from ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t}; while the second level eliminates the temporal divergence of ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t}. To achieve this, we leverage appropriate constraints to ensure that the disentanglement indeed serves the adaptation purpose. Firstly, we enable a good decoupling of the two sets of latent factors by minimizing their mutual dependence. This encourages these two latent factor sets to be mutually independent. We then consider constraining each latent factor set. For ğ³dğ’Ÿsuperscriptsubscriptğ³ğ‘‘ğ’Ÿ\\mathbf{z}_{d}^{\\mathcal{D}} with ğ’Ÿâˆˆ{ğ’®,ğ’¯}ğ’Ÿğ’®ğ’¯\\mathcal{D}\\in\\{\\mathcal{S},\\mathcal{T}\\}, we propose a contrastive triplet loss to make them static and domain-specific. This makes us readily handle spatial divergence by disentangling ğ³dğ’Ÿsuperscriptsubscriptğ³ğ‘‘ğ’Ÿ\\mathbf{z}_{d}^{\\mathcal{D}} out. For ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t}, we propose to align them across domains at both frame and video levels through adversarial learning so as to further eliminate the temporal divergence. Meanwhile, as downstream tasks use ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t} as input, we also add the task-specific supervision on ğ³tsubscriptğ³ğ‘¡\\mathbf{z}_{t} extracted from source data (w/ ground-truth). To the best of our knowledge, this is the first work that tackles the challenging video-based UDA from a domain disentanglement view. We conduct extensive experiments on popular benchmarks (UCF-HMDB, Jester, Epic-Kitchens) and the results show that TranSVAE consistently outperforms previous state-of-the-art methods by large margins. We also conduct comprehensive ablation studies and disentanglement analyses to verify the effectiveness of the latent factor decoupling. The main contribution of the paper is summarized as follows: We provide a generative perspective on solving video-based UDA problems. We develop a generative graphical model for the cross-domain video generation process and propose to utilize the sequential VAE as the base generative model. Based on the above generative view, we propose a TranSVAE framework for video-based UDA. By developing four constraints on the latent factors to enable disentanglement to benefit adaptation, the proposed framework is capable of handling the cross-domain divergence from both spatial and temporal levels. We conduct extensive experiments on several benchmark datasets to verify the effectiveness of TranSVAE. A comprehensive ablation study also demonstrates the positive effect of each loss term on video domain adaptation."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Contrastively Disentangled Sequential Variational Autoencoder",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Junwen Bai, Weiran Wang, and CarlaÂ P Gomes",
      "orig_title": "Contrastively disentangled sequential variational autoencoder",
      "paper_id": "2110.12091v1"
    },
    {
      "index": 1,
      "title": "Mutual information analysis: a comprehensive study",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of Cryptology",
      "authors": "Lejla Batina, Benedikt Gierlichs, Emmanuel Prouff, Matthieu Rivain, FranÃ§ois-Xavier Standaert, and Nicolas Veyrat-Charvillon"
    },
    {
      "index": 2,
      "title": "Exploring object relation in mean teacher for cross-domain detection",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "QiÂ Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, and Ting Yao"
    },
    {
      "index": 3,
      "title": "Learning disentangled semantic representation for domain adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "International Joint Conferences on Artificial Intelligence",
      "authors": "Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao"
    },
    {
      "index": 4,
      "title": "Graph Domain Adaptation: A Generative View",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.07482",
      "authors": "Ruichu Cai, Fengzhu Wu, Zijian Li, Pengfei Wei, Lingling Yi, and Kun Zhang",
      "orig_title": "Graph domain adaptation: A generative view",
      "paper_id": "2106.07482v1"
    },
    {
      "index": 5,
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Joao Carreira and Andrew Zisserman",
      "orig_title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "paper_id": "1705.07750v3"
    },
    {
      "index": 6,
      "title": "Temporal Attentive Alignment for Large-Scale Video Domain Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng",
      "orig_title": "Temporal attentive alignment for large-scale video domain adaptation",
      "paper_id": "1907.12743v6"
    },
    {
      "index": 7,
      "title": "Multi-level attentive adversarial learning with temporal dilation for unsupervised video domain adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "Peipeng Chen, Yuan Gao, and AndyÂ J Ma"
    },
    {
      "index": 8,
      "title": "Isolating sources of disentanglement in variational autoencoders",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "RickyÂ TQ Chen, Xuechen Li, RogerÂ B Grosse, and DavidÂ K Duvenaud"
    },
    {
      "index": 9,
      "title": "Shuffle and attend: Video domain adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Jinwoo Choi, Gaurav Sharma, Samuel Schulter, and Jia-Bin Huang"
    },
    {
      "index": 10,
      "title": "Scaling egocentric vision: The epic-kitchens dataset",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Dima Damen, Hazel Doughty, GiovanniÂ Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, etÂ al."
    },
    {
      "index": 11,
      "title": "Overcoming label noise for source-free unsupervised video domain adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing",
      "authors": "Avijit Dasgupta, CVÂ Jawahar, and Karteek Alahari"
    },
    {
      "index": 12,
      "title": "Informative feature disentanglement for unsupervised domain adaptation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "Wanxia Deng, Lingjun Zhao, Qing Liao, Deke Guo, Gangyao Kuang, Dewen Hu, Matti PietikÃ¤inen, and LiÂ Liu"
    },
    {
      "index": 13,
      "title": "Domain-Adversarial Training of Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky",
      "orig_title": "Domain-adversarial training of neural networks",
      "paper_id": "1505.07818v4"
    },
    {
      "index": 14,
      "title": "Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, and Yanpeng Cao",
      "orig_title": "Uncertainty-aware unsupervised domain adaptation in object detection",
      "paper_id": "2103.00236v2"
    },
    {
      "index": 15,
      "title": "DIVA: Domain Invariant Variational Autoencoders",
      "abstract": "",
      "year": "2020",
      "venue": "Medical Imaging with Deep Learning",
      "authors": "Maximilian Ilse, JakubÂ M Tomczak, Christos Louizos, and Max Welling",
      "orig_title": "Diva: Domain invariant variational autoencoders",
      "paper_id": "1905.10427v2"
    },
    {
      "index": 16,
      "title": "The Kinetics Human Action Video Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.06950",
      "authors": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, etÂ al.",
      "orig_title": "The kinetics human action video dataset",
      "paper_id": "1705.06950v1"
    },
    {
      "index": 17,
      "title": "Learning cross-modal contrastive features for video domain adaptation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, and Manmohan Chandraker"
    },
    {
      "index": 18,
      "title": "ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE International Conference on Robotics and Automation",
      "authors": "Lingdong Kong, Niamul Quader, and VeniceÂ Erin Liong",
      "orig_title": "Conda: Unsupervised domain adaptation for lidar segmentation via regularized domain concatenation",
      "paper_id": "2111.15242v3"
    },
    {
      "index": 19,
      "title": "Hmdb: a large video database for human motion recognition",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Hildegard Kuehne, Hueihan Jhuang, EstÃ­baliz Garrote, Tomaso Poggio, and Thomas Serre"
    },
    {
      "index": 20,
      "title": "Adaptive batch normalization for practical domain adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Jiaying Liu"
    },
    {
      "index": 21,
      "title": "Disentangled Sequential Autoencoder",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Yingzhen Li and Stephan Mandt",
      "orig_title": "Disentangled sequential autoencoder",
      "paper_id": "1803.02991v2"
    },
    {
      "index": 22,
      "title": "Cycda: Unsupervised cycle domain adaptation to learn from image to video",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "Wei Lin, Anna Kukleva, Kunyang Sun, Horst Possegger, Hilde Kuehne, and Horst Bischof"
    },
    {
      "index": 23,
      "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2306.09347",
      "authors": "Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu",
      "orig_title": "Segment any point cloud sequences by distilling vision foundation models",
      "paper_id": "2306.09347v2"
    },
    {
      "index": 24,
      "title": "Deep Transfer Learning with Joint Adaptation Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Mingsheng Long, Han Zhu, Jianmin Wang, and MichaelÂ I Jordan",
      "orig_title": "Deep transfer learning with joint adaptation networks",
      "paper_id": "1605.06636v2"
    },
    {
      "index": 25,
      "title": "Adversarial Bipartite Graph Learning for Video Domain Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Yadan Luo, ZiÂ Huang, Zijian Wang, Zheng Zhang, and Mahsa Baktashmotlagh",
      "orig_title": "Adversarial bipartite graph learning for video domain adaptation",
      "paper_id": "2007.15829v1"
    },
    {
      "index": 26,
      "title": "The jester dataset: A large-scale video dataset of human gestures",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision Workshops",
      "authors": "Joanna Materzynska, Guillaume Berger, Ingo Bax, and Roland Memisevic"
    },
    {
      "index": 27,
      "title": "Multi-Modal Domain Adaptation for Fine-Grained Action Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jonathan Munro and Dima Damen",
      "orig_title": "Multi-modal domain adaptation for fine-grained action recognition",
      "paper_id": "2001.09691v2"
    },
    {
      "index": 28,
      "title": "Adversarial Cross-Domain Action Recognition with Co-Attention",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Boxiao Pan, Zhangjie Cao, Ehsan Adeli, and JuanÂ Carlos Niebles",
      "orig_title": "Adversarial cross-domain action recognition with co-attention",
      "paper_id": "1912.10405v1"
    },
    {
      "index": 29,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, LuÂ Fang, Junjie Bai, and Soumith Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 30,
      "title": "Contrast and mix: Temporal contrastive video domain adaptation with background mixing",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, and Abir Das"
    },
    {
      "index": 31,
      "title": "Maximum Classifier Discrepancy for Unsupervised Domain Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada",
      "orig_title": "Maximum classifier discrepancy for unsupervised domain adaptation",
      "paper_id": "1712.02560v4"
    },
    {
      "index": 32,
      "title": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick PÃ©rez, and Matthieu Cord",
      "orig_title": "Multi-head distillation for continual unsupervised domain adaptation in semantic segmentation",
      "paper_id": "2204.11667v1"
    },
    {
      "index": 33,
      "title": "Spatio-temporal contrastive domain adaptation for action recognition",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Xiaolin Song, Sicheng Zhao, Jingyu Yang, Huanjing Yue, Pengfei Xu, Runbo Hu, and Hua Chai"
    },
    {
      "index": 34,
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1212.0402",
      "authors": "Khurram Soomro, AmirÂ Roshan Zamir, and Mubarak Shah"
    },
    {
      "index": 35,
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhan Tong, Yibing Song, Jue Wang, and Limin Wang",
      "orig_title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "paper_id": "2203.12602v3"
    },
    {
      "index": 36,
      "title": "Dual-head contrastive domain adaptation for video action recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "VictorÂ G Turrisi, Giacomo Zara, Paolo Rota, Thiago Oliveira-Santos, Nicu Sebe, Vittorio Murino, and Elisa Ricci"
    },
    {
      "index": 37,
      "title": "Adversarial Discriminative Domain Adaptation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell",
      "orig_title": "Adversarial discriminative domain adaptation",
      "paper_id": "1702.05464v1"
    },
    {
      "index": 38,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research",
      "authors": "Laurens VanÂ der Maaten and Geoffrey Hinton",
      "orig_title": "Visualizing data using t-sne",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 39,
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, YiÂ Wang, Yali Wang, and YuÂ Qiao",
      "orig_title": "Videomae v2: Scaling video masked autoencoders with dual masking",
      "paper_id": "2303.16727v2"
    },
    {
      "index": 40,
      "title": "A Survey of Unsupervised Deep Domain Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "authors": "Garrett Wilson and DianeÂ J Cook",
      "orig_title": "A survey of unsupervised deep domain adaptation",
      "paper_id": "1812.02849v3"
    },
    {
      "index": 41,
      "title": "Dynamic Weighted Learning for Unsupervised Domain Adaptation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "NiÂ Xiao and Lei Zhang",
      "orig_title": "Dynamic weighted learning for unsupervised domain adaptation",
      "paper_id": "2103.13814v1"
    },
    {
      "index": 42,
      "title": "Interact before align: Leveraging cross-modal knowledge for domain adaptive action recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Lijin Yang, Yifei Huang, Yusuke Sugano, and Yoichi Sato"
    },
    {
      "index": 43,
      "title": "Mix-dann and dynamic-modal-distillation for video domain adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Yuehao Yin, Bin Zhu, Jingjing Chen, Lechao Cheng, and Yu-Gang Jiang"
    },
    {
      "index": 44,
      "title": "Sc-uda: Style and content gaps aware unsupervised domain adaptation for object detection",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "Fuxun Yu, DiÂ Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, and Xiang Chen"
    },
    {
      "index": 45,
      "title": "Unsupervised representation learning with deep convolutional neural network for remote sensing images",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Image and Graphics",
      "authors": "Yang Yu, Zhiqiang Gong, Ping Zhong, and Jiaxin Shan"
    },
    {
      "index": 46,
      "title": "Spectral unsupervised domain adaptation for visual recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jingyi Zhang, Jiaxing Huang, Zichen Tian, and Shijian Lu"
    },
    {
      "index": 47,
      "title": "Audio-Adaptive Activity Recognition Across Video Domains",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Yunhua Zhang, Hazel Doughty, Ling Shao, and CeesÂ GM Snoek",
      "orig_title": "Audio-adaptive activity recognition across video domains",
      "paper_id": "2203.14240v2"
    },
    {
      "index": 48,
      "title": "Temporal Relational Reasoning in Videos",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba",
      "orig_title": "Temporal relational reasoning in videos",
      "paper_id": "1711.08496v2"
    },
    {
      "index": 49,
      "title": "S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Yizhe Zhu, MartinÂ Renqiang Min, Asim Kadav, and HansÂ Peter Graf",
      "orig_title": "S3vae: Self-supervised sequential vae for representation disentanglement and data generation",
      "paper_id": "2005.11437v1"
    },
    {
      "index": 50,
      "title": "Unsupervised domain adaptation for semantic segmentation via class-balanced self-training",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang"
    }
  ]
}