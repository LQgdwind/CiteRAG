{
  "paper_id": "2204.04812v2",
  "title": "OutfitTransformer: Learning Outfit Representations for Fashion Recommendation",
  "sections": {
    "related work": "Outfit Compatibility Prediction. Prior work on fashion outfit compatibility\noften considers pairwise item comparisons to predict the final compatibility score   [ref]25  .\nHowever, these algorithms usually perform aggregation of item-level scores to compute the entire outift compatibility score.\nTo add global constraints in scoring outfit compatibility, a number of methods  [ref]6  [ref]5 aggregate inputs from all constituent items.\nHan et al.  use a bidirectional LSTM to model outfit composition as a sequential process, considering outfits as ordered sequences. However, outfit compatibility should be invariant to the order of items in an outfit.\nSome recent approaches [ref]6  use a graph convolutional network (GCN) for outfit compatibility prediction.\nCucurull et al. [ref]6 train a graph neural network that generates embeddings conditioned on the representations of neighboring nodes.\nThey use GCN to model the node relationship and predict the outfit compatibility but require large neighbor information for best performance, which is impractical for new items as mentioned in .\nCui et al.  model an outfit as a graph, where each node represents a category and each edge represents interaction between two categories.\nChen et al. [ref]5 (hrta. POG) propose a transformer-based encoder-decoder architecture for generating compatible outfits that are specifically designed for personalization based on historical clicks data.\nAll these approaches  [ref]6  [ref]5 are mainly designed for classification tasks (CP and FITB ) but do not address large-scale CIR. Outfit Complementary Item Retrieval. Although the compatibility prediction score in previous methods can be used for ranking items, it is impractical to do so in a large-scale setting. The framework must support indexing to avoid linearly scanning the entire database.\nLin et al.  propose a framework to retrieve complementary items that match the entire outfit. Their method retrieves items by considering compatibility between the target item and every\nitem in an outfit in a pairwise manner, and then aggregating the scores. However, they do not consider the outfit as a whole and only use attention at a paired-category level. In contrast, we use a transformer model to capture interactions between all the items in an outfit to learn a global outfit representation. Aso, our method has a much smaller indexing size than  which is important for practical applications (cf. Section 3.2.3).\nLorbert et al.  use a single layer self-attention based framework for outfit generation but do not explicitly model compatibility. Attention-Based Methods and Vision Transformers. There has been considerable research using transformers in a wide variety of computer vision tasks  0  2. Vision transformers  and related models  3  decompose each image into an ordered sequence of smaller patches to learn image representation of a single image for different tasks. On the contrary, we model outfits as an unordered set of different item images and use a transformer-based architecture to learn a global outfit representation that captures overall outfit compatibility. As discussed earlier in this section, attention mechanisms   [ref]5  have also been used in fashion recommendation systems.   use attention to understand complementary relationships in a pairwise manner. In contrast, we use a transformer model to learn interactions between all the items in an outfit, which attends to higher-order compatibility relationships  beyond pairwise [ref]25  .\nBoth POG [ref]5 and  use pretrained ImageNet embeddings, while we learn fashion-specific features by training in an end-to-end manner. Distance Metric Learning: CIR is a fundamentally different task from visual similarity search because the complementary item is from a different category and is visually dissimilar from the other items in the outfit. We specifically design a negative sampling strategy for CIR where we sample negatives from the same category as positive samples, unlike  .\nAlso, in contrast to  [ref]25 , which consider pair-wise compatibility between target items and each individual item in the outfit, we propose a set-wise outfit ranking loss that compares target items with a single embedding for the entire outfit."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "github.com/nmslib/hnswlib",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "github.com/facebookresearch/faiss",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 2,
      "title": "End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko"
    },
    {
      "index": 3,
      "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Chun-Fu Chen, Quanfu Fan, and Rameswar Panda",
      "orig_title": "Crossvit: Cross-attention multi-scale vision transformer for image classification",
      "paper_id": "2103.14899v2"
    },
    {
      "index": 4,
      "title": "Pog: Personalized outfit generation for fashion recommendation at alibaba ifashion",
      "abstract": "",
      "year": "2019",
      "venue": "SIGKDD",
      "authors": "Wen Feng Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler, Huan Zhao, and Binqiang Zhao"
    },
    {
      "index": 5,
      "title": "Context-Aware Visual Compatibility Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Guillem Cucurull, Perouz Taslakian, and David Vazquez",
      "orig_title": "Context-aware visual compatibility prediction",
      "paper_id": "1902.03646v2"
    },
    {
      "index": 6,
      "title": "Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "The World Wide Web Conference",
      "authors": "Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang",
      "orig_title": "Dressing as a whole: Outfit compatibility learning based on node-wise graph neural networks",
      "paper_id": "1902.08009v1"
    },
    {
      "index": 7,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 8,
      "title": "Learning fashion compatibility with bidirectional lstms",
      "abstract": "",
      "year": "2017",
      "venue": "ACM MM",
      "authors": "Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis"
    },
    {
      "index": 9,
      "title": "Learning attribute-driven disentangled representations for interactive fashion retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "The International Conference on Computer Vision (ICCV)",
      "authors": "Yuxin Hou, Eleonora Vig, Michael Donoser, and Loris Bazzani"
    },
    {
      "index": 10,
      "title": "Learning the Latent “Look”: Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Wei-Lin Hsiao and Kristen Grauman",
      "orig_title": "Learning the latent “look”: Unsupervised discovery of a style-coherent embedding from fashion images",
      "paper_id": "1707.03376v2"
    },
    {
      "index": 11,
      "title": "Creating Capsule Wardrobes from Fashion Images",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Wei-Lin Hsiao and Kristen Grauman",
      "orig_title": "Creating capsule wardrobes from fashion images",
      "paper_id": "1712.02662v2"
    },
    {
      "index": 12,
      "title": "Event Transformer",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh",
      "orig_title": "Set transformer",
      "paper_id": "2204.05172v2"
    },
    {
      "index": 13,
      "title": "Coherent and controllable outfit generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Kedan Li, Chen Liu, and David Forsyth"
    },
    {
      "index": 14,
      "title": "Focal Loss for Dense Object Detection",
      "abstract": "",
      "year": "2020",
      "venue": "TPAMI",
      "authors": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár",
      "orig_title": "Focal loss for dense object detection",
      "paper_id": "1708.02002v2"
    },
    {
      "index": 15,
      "title": "Fashion Outfit Complementary Item Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Yen-Liang Lin, S. Tran, and Larry Davis",
      "orig_title": "Fashion outfit complementary item retrieval",
      "paper_id": "1912.08967v2"
    },
    {
      "index": 16,
      "title": "Scalable and explainable outfit generation",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR Workshop",
      "authors": "Alexander Lorbert, David Neiman, Arik Poznanski, Eduard Oks, and Larry Davis"
    },
    {
      "index": 17,
      "title": "Stand-Alone Self-Attention in Vision Models",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens",
      "orig_title": "Stand-alone self-attention in vision models",
      "paper_id": "1906.05909v1"
    },
    {
      "index": 18,
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Nils Reimers and Iryna Gurevych",
      "orig_title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "paper_id": "1908.10084v1"
    },
    {
      "index": 19,
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Florian Schroff, Dmitry Kalenichenko, and James Philbin",
      "orig_title": "Facenet: A unified embedding for face recognition and clustering",
      "paper_id": "1503.03832v3"
    },
    {
      "index": 20,
      "title": "Deep Metric Learning via Lifted Structured Feature Embedding",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese",
      "orig_title": "Deep metric learning via lifted structured feature embedding",
      "paper_id": "1511.06452v1"
    },
    {
      "index": 21,
      "title": "Learning Similarity Conditions Without Explicit Supervision",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Reuben Tan, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer",
      "orig_title": "Learning similarity conditions without explicit supervision",
      "paper_id": "1908.08589v1"
    },
    {
      "index": 22,
      "title": "Personalized compatibility metric learning",
      "abstract": "",
      "year": "2021",
      "venue": "KDD Workshop",
      "authors": "Meet Taraviya, Anurag Beniwal, Yen-Liang Lin, , and Larry Davis"
    },
    {
      "index": 23,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou",
      "orig_title": "Training data-efficient image transformers; distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 24,
      "title": "Learning type-aware embeddings for fashion compatibility",
      "abstract": "",
      "year": "2018",
      "venue": "ICCV",
      "authors": "Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and David Forsyth"
    },
    {
      "index": 25,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 26,
      "title": "Conditional Similarity Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Andreas Veit, Serge Belongie, and Theofanis Karaletsos",
      "orig_title": "Conditional similarity networks",
      "paper_id": "1603.07810v3"
    },
    {
      "index": 27,
      "title": "Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie",
      "orig_title": "Learning visual clothing style with heterogeneous dyadic co-occurrences",
      "paper_id": "1509.07473v1"
    },
    {
      "index": 28,
      "title": "Sampling Matters in Deep Embedding Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krahenbuhl",
      "orig_title": "Sampling matters in deep embedding learning",
      "paper_id": "1706.07567v2"
    },
    {
      "index": 29,
      "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo",
      "orig_title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
      "paper_id": "2105.15203v3"
    },
    {
      "index": 30,
      "title": "Hard negative examples are hard, but useful",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless",
      "orig_title": "Hard negative examples are hard, but useful",
      "paper_id": "2007.12749v2"
    },
    {
      "index": 31,
      "title": "Learning Texture Transformer Network for Image Super-Resolution",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo",
      "orig_title": "Learning texture transformer network for image super-resolution",
      "paper_id": "2006.04139v2"
    },
    {
      "index": 32,
      "title": "Focal self-attention for local-global interactions in vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao"
    },
    {
      "index": 33,
      "title": "TransNFCM: Translation-Based Neural Fashion Compatibility Modeling",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "X. Yang, Yunshan Ma, Lizi Liao, M. Wang, and Tat-Seng Chua",
      "orig_title": "Transnfcm: Translation-based neural fashion compatibility modeling",
      "paper_id": "1812.10021v1"
    }
  ]
}