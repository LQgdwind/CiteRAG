{
  "paper_id": "2104.07454v3",
  "title": "Memory Capacity of Recurrent Neural Networks with Matrix Representation",
  "sections": {
    "introduction": "Several successful attempts have been made to explain learning and knowledge representation from the perspective of kernel methods  to dynamical systems viewpoint of recurrent neural networks (RNNs)  [ref]3 . A crucial element in knowledge representation studies of RNNs is that they deal only with finite dimensional vector representations. Thus implementing such neural networks in any architecture would eventually lead to flattening of the input data along any mode, given the input is multi-modal to begin with. It can thus be argued that such a flattening layer can cause loss of structural and spatial information in the input, especially in data arising in natural settings like tri-modal images/sequence of images in longitudinal analysis and higher modal datasets arising in the study of human behavior  . The introduction of richer inductive biases  in the past decade can be seen through recent deep learning architectures such as convolutional neural networks (CNNs) [ref]8, graph-structured representations [ref]9 0, and hierarchy of constituents 1.\nThese methods try to preserve the spatial information by providing fewer constraints on the structure of the representation that the model learns in comparison to a traditional neural network 2. A parallel branch of study, which aims to provide more generalized representations has been introduced by learning higher-order tensors (‚â•2absent2\\geq 2) for multi-modal data to preserve the number of dimensions and the modes at the output 3 4 5. This avoids the potential loss of the spatial coherence 6 in the input data, which can be incurred due to the flattening operation of the higher order tensors and instead processes the multi-modal higher-order tensor input data as it is. A particularly simple use of learning higher-order tensors as representations is in the field of symmetric positive definite (SPD) matrix (an order 2 tensor) learning, which arises largely in covariance estimation tasks like heteroscedastic multivariate regression 7. In this task, one wishes to learn fundamentally a matrix representation from the input data, thus several extensions of conventional neural networks with vector representations have been introduced to accept matrix inputs as well, such as learning the SPD representations throughout the layers via Mercer‚Äôs kernel 8 to incorporating Riemannian structure of the SPD matrices directly into the layers itself 9. The matrix representation in such proposals is generated through the bilinear map of the form, where ùëºùëº{\\bm{U}}, ùëΩùëΩ{\\bm{V}} and ùë©ùë©{\\bm{B}} are the parameters to be learned of appropriate shapes 0. Do et al.¬† 1, with the help of an aforementioned bilinear map, developed the notion of recurrent matrix networks (RMNs). One of the other benefits of RMNs is the reduction of the number of trainable parameters of the model, as the total parameters depend now linearly on any of the input dimensions. This is in contrast to vector-based conventional neural networks, where the number of parameters grows quadratically with the dimensions of the input vector.\nMatrix neural networks have been successfully used in the past for seemingly difficult real-life tasks like cyclone track prediction 2 and in high-frequency time series analysis 3, where the preservation of spatial data amongst the modes of input proved to be beneficiary as these networks gave better accuracy than their vector counterparts. Neural Turing machines (NTMs)4 are RNN models that combine the fuzzy pattern-matching capabilities of neural networks with the notion of programmable computers. NTMs feature a neural network controller coupled to external memory that interacts with attention mechanisms. NTMs that feature long short-term memory (LSTM) 5 for network controllers can infer simple algorithms that have copying, sorting, and associative recall tasks 4. With their potential benefits, it‚Äôs natural to investigate properties of the matrix representation as generated by the bilinear mapping (Equation 1). Out of the various possible ways in which this can be checked (such as convergence guarantee and training behavior etc.), we particularly focus on the asymptotic memory capacity of the matrix representation by introducing the dynamical systems viewpoint of generic matrix RNNs 1, where we study the evolution of state matrix over time and how much the state at a given time holds information about all previous states. In this paper, we study memory capacity using matrix representation as used by matrix RNNs and extend the notion to NTMs. We investigate if the matrix (second-order tensor) representation of data has a better memory capacity than the vector representation in conventional neural networks. Intuitively, one might argue that such matrix representations might always be better than vector representations in terms of memory capacity. However, we show theoretically that it isn‚Äôt the case under some constraints, even though in general, the capacity might be better than vector representations with a bound that is analogous to the one found for vector representations 6. Hence, it is vital to discover ways to increase the memory capacity, especially given the memory architectures that have been proposed in the past 4 7 8. In order to provide theoretical evaluation, we take into consideration a generic memory-augmented RNN which simply stores a finite amount of past states. We then use theoretical and simulation study to show that it has greater capacity than a simple matrix recurrent architecture without memory; hence, practically showing the increase in memory capacity that is to be expected by addition of a simple memory architecture. However, for more practical purposes, we extend the idea of memory networks 9 to also include the second order tensors, thus introducing the matrix representation stored neural memory for matrix NTM (MatNTM). We report the results of the simulation of the governing equations of memory capacity of matrix RNNs both with and without external memory and also show results of synthetic algorithmic experiments using MatNTMs. We present the rest of the paper as follows. In Section 2, we present various definitions for memory capacity in neural networks. In Section 3, the overarching goal is to capture the effect of external state memory dynamics. Motivated by previous discussions, Section 4 presents the MatNTM with the procedure of memory lookup. Section 5 presents a simulation study in order to quantitatively observe the theoretical properties obtained by the theorems in the prior sections. Section 6 presents experiments and results, which is followed by a discussion in Section 7."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "A. Jacot, F. Gabriel, C. Hongler",
      "orig_title": "Neural tangent kernel: Convergence and generalization in neural networks",
      "paper_id": "1806.07572v4"
    },
    {
      "index": 1,
      "title": "Training second-order recurrent neural networks using hints",
      "abstract": "",
      "year": "1992",
      "venue": "Ninth International Conference on Machine Learning",
      "authors": "C. W. Omlin, C. L. Giles"
    },
    {
      "index": 2,
      "title": "Fuzzy finite state automata can be deterministically encoded into recurrent neural networks",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE Trans. Fuzzy Syst.",
      "authors": "C. W. Omlin, K. K. Thornber, C. L. Giles"
    },
    {
      "index": 3,
      "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "B. Chang, M. Chen, E. Haber, E. H. Chi",
      "orig_title": "AntisymmetricRNN: A dynamical system view on recurrent neural networks",
      "paper_id": "1902.09689v1"
    },
    {
      "index": 4,
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "abstract": "",
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "authors": "C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, S. S. Narayanan"
    },
    {
      "index": 5,
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "abstract": "",
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)",
      "authors": "F. Ringeval, A. Sonderegger, J. Sauer, D. Lalanne"
    },
    {
      "index": 6,
      "title": "The need for biases in learning generalizations",
      "abstract": "",
      "year": "1980",
      "venue": "",
      "authors": "T. M. Mitchell"
    },
    {
      "index": 7,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Krizhevsky, I. Sutskever, G. E. Hinton"
    },
    {
      "index": 8,
      "title": "Graph-Structured Representations for Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "D. Teney, L. Liu, A. Van Den Hengel",
      "orig_title": "Graph-structured representations for visual question answering",
      "paper_id": "1609.05600v2"
    },
    {
      "index": 9,
      "title": "Contrastive Learning of Structured World Models",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "T. Kipf, E. van der Pol, M. Welling",
      "orig_title": "Contrastive learning of structured world models",
      "paper_id": "1911.12247v2"
    },
    {
      "index": 10,
      "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Y. Shen, S. Tan, A. Sordoni, A. Courville",
      "orig_title": "Ordered neurons: Integrating tree structures into recurrent neural networks",
      "paper_id": "1810.09536v6"
    },
    {
      "index": 11,
      "title": "Relational inductive biases, deep learning, and graph networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "P. W. Battaglia, J. B. Hamrick, et. al.",
      "orig_title": "Relational inductive biases, deep learning, and graph networks",
      "paper_id": "1806.01261v3"
    },
    {
      "index": 12,
      "title": "Tensorial neural networks and its application in longitudinal network data analysis",
      "abstract": "",
      "year": "2017",
      "venue": "Neural Information Processing",
      "authors": "M. Bai, B. Zhang, J. Gao"
    },
    {
      "index": 13,
      "title": "Tensorial Neural Networks: Generalization of Neural Networks and Application to Model Compression",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv e-prints",
      "authors": "J. Su, J. Li, B. Bhattacharjee, F. Huang",
      "orig_title": "Tensorial Neural Networks: Generalization of Neural Networks and Application to Model Compression",
      "paper_id": "1805.10352v3"
    },
    {
      "index": 14,
      "title": "Tensor-variate restricted boltzmann machines",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "T. Nguyen, T. Tran, D. Phung, S. Venkatesh"
    },
    {
      "index": 15,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Y. Bengio, A. Courville, P. Vincent"
    },
    {
      "index": 16,
      "title": "Estimation of heteroscedasticity in regression analysis",
      "abstract": "",
      "year": "1987",
      "venue": "The Annals of Statistics",
      "authors": "H.-G. Muller, U. Stadtmuller, et al."
    },
    {
      "index": 17,
      "title": "Constructing the matrix multilayer perceptron and its application to the vae",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "J. Taghia, M. B√•nkestad, F. Lindsten, T. B. Sch√∂n"
    },
    {
      "index": 18,
      "title": "A Riemannian Network for SPD Matrix Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Z. Huang, L. Van Gool",
      "orig_title": "A riemannian network for spd matrix learning",
      "paper_id": "1608.04233v2"
    },
    {
      "index": 19,
      "title": "Matrix Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Working Papers, University of Sydney Business School, Discipline of Business Analytics",
      "authors": "J. Gao, Y. Guo, Z. Wang",
      "orig_title": "Matrix neural networks",
      "paper_id": "1601.03805v2"
    },
    {
      "index": 20,
      "title": "Learning deep matrix representations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "K. Do, T. Tran, S. Venkatesh"
    },
    {
      "index": 21,
      "title": "Cyclone track prediction with matrix neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)",
      "authors": "Y. Zhang, R. Chandra, J. Gao"
    },
    {
      "index": 22,
      "title": "Temporal Attention augmented Bilinear Network for Financial Time-Series Data Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "D. T. Tran, A. Iosifidis, J. Kanniainen, M. Gabbouj",
      "orig_title": "Temporal attention-augmented bilinear network for financial time-series data analysis",
      "paper_id": "1712.00975v1"
    },
    {
      "index": 23,
      "title": "Neural turing machines",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "A. Graves, G. Wayne, I. Danihelka"
    },
    {
      "index": 24,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "S. Hochreiter, J. Schmidhuber"
    },
    {
      "index": 25,
      "title": "Memory traces in dynamical systems",
      "abstract": "",
      "year": "2008",
      "venue": "Proceedings of the National Academy of Sciences",
      "authors": "S. Ganguli, D. Huh, H. Sompolinsky"
    },
    {
      "index": 26,
      "title": "Hybrid computing using a neural network with dynamic external memory",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwi≈Ñska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, et al."
    },
    {
      "index": 27,
      "title": "Meta-learning with memory-augmented neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap"
    },
    {
      "index": 28,
      "title": "Memory Networks",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Weston, S. Chopra, A. Bordes",
      "orig_title": "Memory networks",
      "paper_id": "1410.3916v11"
    },
    {
      "index": 29,
      "title": "On neuronal capacity",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "P. Baldi, R. Vershynin"
    },
    {
      "index": 30,
      "title": "The Capacity of feedforward neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "P. Baldi, R. Vershynin",
      "orig_title": "The capacity of feedforward neural networks",
      "paper_id": "1901.00434v2"
    },
    {
      "index": 31,
      "title": "Memory capacity of neural networks with threshold and rectified linear unit activations",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Mathematics of Data Science",
      "authors": "R. Vershynin"
    },
    {
      "index": 32,
      "title": "Short-term memory in orthogonal neural networks",
      "abstract": "",
      "year": "2004",
      "venue": "Physical Review Letters",
      "authors": "O. L. White, D. D. Lee, H. Sompolinsky"
    },
    {
      "index": 33,
      "title": "Matrix Variate Distributions",
      "abstract": "",
      "year": "1999",
      "venue": "Monographs and Surveys in Pure and Applied Mathematics, Taylor & Francis",
      "authors": "A. Gupta, D. Nagar"
    },
    {
      "index": 34,
      "title": "Sequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Sutskever, O. Vinyals, Q. V. Le"
    },
    {
      "index": 35,
      "title": "On information and sufficiency",
      "abstract": "",
      "year": "1951",
      "venue": "The annals of mathematical statistics",
      "authors": "S. Kullback, R. A. Leibler"
    },
    {
      "index": 36,
      "title": "Information theory and statistics",
      "abstract": "",
      "year": "1997",
      "venue": "Courier Corporation",
      "authors": "S. Kullback"
    },
    {
      "index": 37,
      "title": "Information theory and statistics: A tutorial",
      "abstract": "",
      "year": "2004",
      "venue": "Foundations and Trends¬Æ in Communications and Information Theory",
      "authors": "I. Csisz√°r, P. C. Shields, et al."
    },
    {
      "index": 38,
      "title": "New Insights and Perspectives on the Natural Gradient Method",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "J. Martens",
      "orig_title": "New insights and perspectives on the natural gradient method",
      "paper_id": "1412.1193v11"
    },
    {
      "index": 39,
      "title": "Fisher memory of linear wigner echo state networks.",
      "abstract": "",
      "year": "2017",
      "venue": "ESANN",
      "authors": "P. Tino"
    },
    {
      "index": 40,
      "title": "Inverting modified matrices",
      "abstract": "",
      "year": "1950",
      "venue": "Memorandum report",
      "authors": "M. A. Woodbury"
    },
    {
      "index": 41,
      "title": "On the properties of neural machine translation: Encoder‚Äìdecoder approaches",
      "abstract": "",
      "year": "2014",
      "venue": "SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, Association for Computational Linguistics",
      "authors": "K. Cho, B. van Merri√´nboer, D. Bahdanau, Y. Bengio"
    },
    {
      "index": 42,
      "title": "Memory Augmented Neural Networks with Wormhole Connections",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "C. Gulcehre, S. Chandar, Y. Bengio",
      "orig_title": "Memory augmented neural networks with wormhole connections",
      "paper_id": "1701.08718v1"
    },
    {
      "index": 43,
      "title": "Neural Stored-program Memory",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "H. Le, T. Tran, S. Venkatesh",
      "orig_title": "Neural stored-program memory",
      "paper_id": "1906.08862v2"
    },
    {
      "index": 44,
      "title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
      "abstract": "",
      "year": "1992",
      "venue": "The Fourteenth Annual Conference of Cognitive Science Society. Indiana University",
      "authors": "S. Das, C. L. Giles, G.-Z. Sun"
    },
    {
      "index": 45,
      "title": "Graph Memory Networks for Molecular Activity Prediction",
      "abstract": "",
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)",
      "authors": "T. Pham, T. Tran, S. Venkatesh",
      "orig_title": "Graph memory networks for molecular activity prediction",
      "paper_id": "1801.02622v2"
    },
    {
      "index": 46,
      "title": "Memory-Based Graph Networks",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "A. H. Khasahmadi, K. Hassani, P. Moradi, L. Lee, Q. Morris",
      "orig_title": "Memory-based graph networks",
      "paper_id": "2002.09518v2"
    },
    {
      "index": 47,
      "title": "Lecture 6.5‚ÄîRmsProp: Divide the gradient by a running average of its recent magnitude",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "T. Tieleman, G. Hinton"
    },
    {
      "index": 48,
      "title": "On the difficulty of training recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Machine Learning Research, PMLR",
      "authors": "R. Pascanu, T. Mikolov, Y. Bengio"
    },
    {
      "index": 49,
      "title": "Higher Order Recurrent Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "R. Soltani, H. Jiang",
      "orig_title": "Higher order recurrent neural networks",
      "paper_id": "1605.00064v1"
    },
    {
      "index": 50,
      "title": "Matrix Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Networks-ISNN 2017: 14th International Symposium, ISNN 2017, Sapporo, Hakodate, and Muroran, Hokkaido, Japan, June 21‚Äì26, 2017, Proceedings, Part I 14, Springer",
      "authors": "J. Gao, Y. Guo, Z. Wang",
      "orig_title": "Matrix neural networks",
      "paper_id": "1601.03805v2"
    },
    {
      "index": 51,
      "title": "Langevin-gradient parallel tempering for bayesian neural learning",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "R. Chandra, K. Jain, R. V. Deo, S. Cripps"
    },
    {
      "index": 52,
      "title": "Surrogate-assisted parallel tempering for Bayesian neural learning",
      "abstract": "",
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence",
      "authors": "R. Chandra, K. Jain, A. Kapoor, A. Aman",
      "orig_title": "Surrogate-assisted parallel tempering for bayesian neural learning",
      "paper_id": "1811.08687v3"
    },
    {
      "index": 53,
      "title": "Weight uncertainty in neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning-Volume 37",
      "authors": "C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra"
    }
  ]
}