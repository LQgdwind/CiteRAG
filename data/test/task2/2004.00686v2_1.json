{
  "paper_id": "2004.00686v2",
  "title": "Bias in Machine Learning - What is it Good for?",
  "sections": {
    "assessing model bias": "The result from an inductive learning process, i.e. the function f‚àósuperscriptùëìf^{*} in Eq. 1), is often referred to as a ‚Äòmodel‚Äô. As described in the previous sections, bias may propagate from the biased world, through a biased data generation, to the learning step with its inevitable inductive bias and other biases. The observed bias of a resulting model is often simply denoted ‚Äòbias‚Äô¬†   .\nTo distinguish this from other types of bias discussed in this paper, we propose using the term model bias to refer to bias as it appears and is analyzed in the final model.\nAn alternative would be the existing term¬†algorithmic bias¬†. However, typical usage of that term usually refers to the societal effects of biased systems¬†, while our notion of bias is broader. Nevertheless, most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status .\nAs we will see in the following, classification rates may differ in very many respects, and a large number of bias types have been defined based on the condition that should hold for a model not to have that particular type of bias.\nFor a binary classifier we can for example require that the overall misclassification rate (OMR) is independent of a certain protected attribute Aùê¥A (that takes the values 0 or 1). The corresponding condition for a classifier not being biased in this respect is¬†: where Y^^ùëå\\widehat{Y} is the classifier output f‚Äã(x)ùëìùë•f(x) (see Eq.¬†2), and yùë¶y is the correct classification for input xùë•x. Both Y^^ùëå\\widehat{Y} and yùë¶y take the values 0 or 1.\nFor example, the fact that a person is female (A=0ùê¥0A=0) should not increase or decrease the risk of incorrectly being refused, or allowed, to borrow money at the bank.\nSeveral similar conditions can be defined to describe other types of unwanted bias in a classifier model¬†: false positive rate (FPR): false negative rate (FNR): false omission rate (FOR): false discovery rate (FDR): Each one of these equations focuses on that an incorrect (Y^‚â†y)^ùëåùë¶(\\widehat{Y}\\neq y) classification should be independent of Aùê¥A and a specific value of Y^^ùëå\\widehat{Y} or yùë¶y.\nThe advantageous classifier output (for example being accepted a loan) is here coded as 1.\nA classifier that does not satisfy one of these equations is said to be biased in the corresponding sense777In practise, the requirement is usually that the left and right hand side of the equation should be approximate equal..\nFor example, a classifier is biased with respect to FDR if the value of Aùê¥A affects the probability of incorrectly being allowed to borrow money. A related condition is the equalized odds, which appears in the literature with slightly different definitions (see¬† and¬†). In¬†, equalized odds is defined by the following two conditions (slightly modified notation): and Note that Eq.¬†8 is equivalent to FPR in Eq.¬†4, and Eq.¬†9 is equivalent to TPR in Eq.¬†5. Several other indicators of model bias have been proposed. Loftus et al.¬† define Calibration, Demographic Parity/Disparate Impact, and Individual Fairness.\nFor a binary classification Y^^ùëå\\widehat{Y}, and a binary protected group Aùê¥A, demographic parity is defined as follows: That is, Y^^ùëå\\widehat{Y} should be independent of Aùê¥A, such that the classifier in average gives the same predictions to different groups. If the equality does not hold, this is referred to as disparate impact.\nAn example is a software company that wants to reach a better gender balance among their, mainly male, programmers. By following the principle of demographic parity, when recruiting, the same proportion of female applicants as male applicants are hired. Taken all together we conclude that there is a large number of different types of model biases, each one with its own focus on unwanted behavior of a classifier. Furthermore, many of these biases are related, and it can also be shown that several of them are conflicting in the sense that they cannot be avoided simultaneously   .\nHence, it is problematic to talk about ‚Äòfair‚Äô or ‚Äòunbiased‚Äô classifiers, at least without clearly defining the meaning of the terms. It can also be argued that a proper notion of fairness must be task-specific [ref]16."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "False positives, false negatives, and false analyses: A rejoinder to ‚Äùmachine bias: There‚Äôs software used across the country to predict future criminals. and it‚Äôs biased against blacks‚Äù",
      "abstract": "",
      "year": "2016",
      "venue": "Federal Probation Journal",
      "authors": "Kristin Bechtel Anthony W. Flores and Christopher T. Lowenkamp"
    },
    {
      "index": 1,
      "title": "Learning de-biased representations with biased representations",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh"
    },
    {
      "index": 2,
      "title": "Big data‚Äôs disparate impact",
      "abstract": "",
      "year": "2014",
      "venue": "California law Review",
      "authors": "Solon Barocas and Andrew D. Selbst"
    },
    {
      "index": 3,
      "title": "Hyper-parameter optimization in deep learning and transfer learning : applications to medical imaging",
      "abstract": "",
      "year": "2019",
      "venue": "Theses, Universit√© Paris-Saclay",
      "authors": "Hadrien Bertrand"
    },
    {
      "index": 4,
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "30th Conference on Neural Information Processing Systems (NIPS 2016)",
      "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai"
    },
    {
      "index": 5,
      "title": "Understanding the Origins of Bias in Word Embeddings",
      "abstract": "",
      "year": "2019",
      "venue": "36th Int. Conf. on Machine Learning",
      "authors": "Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel",
      "orig_title": "Understanding the origins of bias inword embeddings",
      "paper_id": "1810.03611v2"
    },
    {
      "index": 6,
      "title": "Semantics derived automatically from language corpora necessarily contain human biases",
      "abstract": "",
      "year": "2017",
      "venue": "Science",
      "authors": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan"
    },
    {
      "index": 7,
      "title": "Ai now 2017 report",
      "abstract": "",
      "year": "2018",
      "venue": "AI Now 2017 Symposium and Workshop",
      "authors": "Alex Campolo, Madelyn Sanfilippo, Meredith Whittaker, and Kate Crawford"
    },
    {
      "index": 8,
      "title": "CRISP-DM 1.0: Step-by-step data mining guide",
      "abstract": "",
      "year": "2000",
      "venue": "SPSS",
      "authors": "Peter Chapman, Janet Clinton, Randy Kerber, Tom Khabaza, Thomas Reinartz, C. Russell H. Shearer, and Robert Wirth"
    },
    {
      "index": 9,
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "abstract": "",
      "year": "2016",
      "venue": "Big data",
      "authors": "Alexandra Chouldechova",
      "orig_title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "paper_id": "1703.00056v1"
    },
    {
      "index": 10,
      "title": "Algorithmic discrimination is an information problem",
      "abstract": "",
      "year": "2019",
      "venue": "Hastings Law Journal",
      "authors": "Ignacio N. Cofone"
    },
    {
      "index": 11,
      "title": "Think again: Big data",
      "abstract": "",
      "year": "2013",
      "venue": "Foreign Policy",
      "authors": "Kate Crawford"
    },
    {
      "index": 12,
      "title": "Algorithmic bias in autonomous systems",
      "abstract": "",
      "year": "2017",
      "venue": "26th International Joint Conference on Artificial Intelligence, IJCAI‚Äô17",
      "authors": "David Danks and Alex John London"
    },
    {
      "index": 13,
      "title": "Proxy non-discrimination in data-driven systems",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen"
    },
    {
      "index": 14,
      "title": "Chapter 6 research design - sampling bias",
      "abstract": "",
      "year": "",
      "venue": "Online Statistics Education: A Multimedia Course of Study",
      "authors": "Rice University David M. Lane"
    },
    {
      "index": 15,
      "title": "Fairness through awareness",
      "abstract": "",
      "year": "2012",
      "venue": "3rd Innovations in Theoretical Computer Science Conference, ITCS ‚Äô12",
      "authors": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel"
    },
    {
      "index": 16,
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "abstract": "",
      "year": "2019",
      "venue": "Int. Conf. on Learning Representations",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel",
      "orig_title": "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.",
      "paper_id": "1811.12231v3"
    },
    {
      "index": 17,
      "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Hila Gonen and Yoav Goldberg",
      "orig_title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "paper_id": "1903.03862v2"
    },
    {
      "index": 18,
      "title": "European Union regulations on algorithmic decision-making and a ‚Äúright to explanation‚Äù",
      "abstract": "",
      "year": "2017",
      "venue": "AI Magazine",
      "authors": "Bryce Goodman and Seth Flaxman",
      "orig_title": "European union regulations on algorithmic decision-making and a ‚Äúright to explanation‚Äù",
      "paper_id": "1606.08813v3"
    },
    {
      "index": 19,
      "title": "Truth and Method",
      "abstract": "",
      "year": "1975",
      "venue": "Continuum",
      "authors": "Gadamer Hans-Georg"
    },
    {
      "index": 20,
      "title": "Equality of Opportunity in Supervised Learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems 29",
      "authors": "Moritz Hardt, Eric Price, and Nati Srebro",
      "orig_title": "Equality of opportunity in supervised learning",
      "paper_id": "1610.02413v1"
    },
    {
      "index": 21,
      "title": "Handbook of Research on Machine Learning Innovations and Trends",
      "abstract": "",
      "year": "2017",
      "venue": "IGI Global, USA",
      "authors": "Aboul Ella Hassanien, Aboul Ella Hassanien, and Tarek Gaber"
    },
    {
      "index": 22,
      "title": "Privacy as protection of the incomputable self: From agnostic to agonistic machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "Theoretical Inquiries in Law",
      "authors": "Mireille Hildebrandt"
    },
    {
      "index": 23,
      "title": "Detecting biased statements in wikipedia",
      "abstract": "",
      "year": "2018",
      "venue": "WWW‚Äô18: Companion Proceedings of the The Web Conference 2018",
      "authors": "Christoph Hube and Besnik Fetahu"
    },
    {
      "index": 24,
      "title": "Towards bias detection in online text corpora",
      "abstract": "",
      "year": "2018",
      "venue": "International Workshop on Bias in Information, Algorithms, and Systems (BIAS)",
      "authors": "Christoph Hube, Besnik Fetahu, and Robert J√§schke"
    },
    {
      "index": 25,
      "title": "Machine bias: There‚Äôs software used across the country to predict future criminals. and it‚Äôs biased against blacks",
      "abstract": "",
      "year": "2016",
      "venue": "ProPublica",
      "authors": "Surya Mattu Julia Angwin, Jeff Larson and Lauren Kirchner"
    },
    {
      "index": 26,
      "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Svetlana Kiritchenko and Saif M. Mohammad",
      "orig_title": "Examining gender and race bias in two hundred sentiment analysis systems",
      "paper_id": "1805.04508v1"
    },
    {
      "index": 27,
      "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan",
      "orig_title": "Inherent trade-offs in the fair determination of risk scores",
      "paper_id": "1609.05807v2"
    },
    {
      "index": 28,
      "title": "Generalization versus discrimination in machine learning",
      "abstract": "",
      "year": "2012",
      "venue": "Encyclopedia of the Sciences of Learning",
      "authors": "Tim Kovacs and Andy J. Wills"
    },
    {
      "index": 29,
      "title": "Repair: Removing representation bias by dataset resampling",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Yi Li and Nuno Vasconcelos"
    },
    {
      "index": 30,
      "title": "Causal Reasoning for Algorithmic Fairness",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Joshua R. Loftus, Chris Russell, Matt J. Kusner, and Ricardo Silva",
      "orig_title": "Causal reasoning for algorithmic fairness",
      "paper_id": "1805.05859v1"
    },
    {
      "index": 31,
      "title": "Returns from investing in equity mutual funds 1971 to 1991",
      "abstract": "",
      "year": "1995",
      "venue": "The Journal of Finance",
      "authors": "Burton G. Malkiel"
    },
    {
      "index": 32,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "CoRR",
      "authors": "Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean"
    },
    {
      "index": 33,
      "title": "The need for biases in learning generalizations",
      "abstract": "",
      "year": "1980",
      "venue": "Technical report, Rutgers University, New Brunswick, NJ",
      "authors": "Tom M. Mitchell"
    },
    {
      "index": 34,
      "title": "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "abstract": "",
      "year": "2019",
      "venue": "Frontiers in Big Data",
      "authors": "Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kƒ±cƒ±man"
    },
    {
      "index": 35,
      "title": "Artificial intelligence and algorithmic bias: implications for health systems",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of global health",
      "authors": "Trishan Panch, Heather Mattie, and Rifat Atun"
    },
    {
      "index": 36,
      "title": "Discrimination-aware data mining",
      "abstract": "",
      "year": "2008",
      "venue": "14th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, KDD ‚Äô08",
      "authors": "Dino Pedreshi, Salvatore Ruggieri, and Franco Turini"
    },
    {
      "index": 37,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning"
    },
    {
      "index": 38,
      "title": "A Dictionary of Epidemiology",
      "abstract": "",
      "year": "2008",
      "venue": "Oxford University Press, USA",
      "authors": "Miquel Porta"
    },
    {
      "index": 39,
      "title": "Linguistic models for analyzing and detecting biased language",
      "abstract": "",
      "year": "2013",
      "venue": "51st Annual Meeting of the Association for Computational Linguistics",
      "authors": "Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky"
    },
    {
      "index": 40,
      "title": "Dirty data, bad predictions: How civil rights violations impact police data",
      "abstract": "",
      "year": "2019",
      "venue": "Predictive Policing Systems, and Justice",
      "authors": "Rashida Richardson, Jason Schultz, and Kate Crawford"
    },
    {
      "index": 41,
      "title": "Modern Epidemiology",
      "abstract": "",
      "year": "2015",
      "venue": "Wolters Kluwer Health/Lippincott Williams & Wilkins",
      "authors": "K.J. Rothman, S. Greenland, and T.L. Lash"
    },
    {
      "index": 42,
      "title": "The risk of racial bias in hate speech detection",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith"
    },
    {
      "index": 43,
      "title": "Mitigating gender bias in natural language processing: Literature review",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang"
    },
    {
      "index": 44,
      "title": "A framework for understanding unintended consequences of machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Harini Suresh and John V. Guttag"
    },
    {
      "index": 45,
      "title": "Unbiased look at dataset bias",
      "abstract": "",
      "year": "2011",
      "venue": "2011 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR ‚Äô11",
      "authors": "A. Torralba and A. A. Efros"
    },
    {
      "index": 46,
      "title": "It‚Äôs a Man‚Äôs Wikipedia? Assessing Gender Inequality in an Online Encyclopedia",
      "abstract": "",
      "year": "2015",
      "venue": "ICWSM",
      "authors": "Claudia Wagner, David Garc√≠a, Mohsen Jadidi, and Markus Strohmaier",
      "orig_title": "It‚Äôs a man‚Äôs wikipedia? assessing gender inequality in an online encyclopedia",
      "paper_id": "1501.06307v2"
    },
    {
      "index": 47,
      "title": "Disciplined convex-concave programming",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE 55th Conference on Decision and Control (CDC)",
      "authors": "Shen Xinyue, Diamond Steven, Gu Yuantao, and Boyd Stephen"
    },
    {
      "index": 48,
      "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment",
      "abstract": "",
      "year": "2017",
      "venue": "26th International Conference on World Wide Web, WWW 2017",
      "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi",
      "orig_title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "paper_id": "1610.08452v2"
    },
    {
      "index": 49,
      "title": "Equality of opportunity in classification: A causal approach",
      "abstract": "",
      "year": "2018",
      "venue": "32nd Conference on Neural Information Processing Systems (NIPS 2018)",
      "authors": "Junzhe Zhang and Elias Bareinboim"
    },
    {
      "index": 50,
      "title": "Deep Neural Network Hyperparameter Optimization with Orthogonal Array Tuning",
      "abstract": "",
      "year": "2019",
      "venue": "ICONIP",
      "authors": "Xu Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong",
      "orig_title": "Deep neural network hyperparameter optimization with orthogonal array tuning",
      "paper_id": "1907.13359v2"
    },
    {
      "index": 51,
      "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
      "abstract": "",
      "year": "2017",
      "venue": "2017 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang",
      "orig_title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "paper_id": "1707.09457v1"
    }
  ]
}