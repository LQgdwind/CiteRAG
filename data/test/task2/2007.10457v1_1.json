{
  "paper_id": "2007.10457v1",
  "title": "Multi-agent Reinforcement Learning in Bayesian Stackelberg Markov Games for Adaptive Moving Target Defense",
  "sections": {
    "discussion and related work": "Multi-agent Reinforcement Learning in Markov Games ‚ÄÉ\nA standard solution strategy in MGs, when œÑùúè\\tau and UùëàU are unknown but a simulator is available, is to adapt the Bellman‚Äôs update used in the single-agent Markov Decision Processes (MDPs) for multi-agent reinforcement learning to learn equilibrium policies . In the context of MARL, researchers have investigated different notions of equilibrium. The min-max Q-learning is meaningful when the game has a zero-sum reward structure . On the other hand, Nash Q-learning, introduced in , has been categorized into two types by ‚Äì Friend, where the game defined by the Q-values always allows for an optimal joint action profile, and Foe, where the game admits a saddle point solution. The convergence of these algorithms is mostly shown by the fact that Q-values of the states in self-play, given infinite exploration, approach the correct Q-values (i.e. calculated Q-values if all the parameters of the game were provided upfront). The convergence of Nash Q-learning in the context of general-sum games becomes difficult because of the existence of multiple equilibria and the lack of a common incentive or co-ordination amidst agents .\nIn correlated equilibrium (CE) Q-learning , authors assume the existence of a correlation device accessible to both players. However, the authors show that the learned strategies of the players converge to an uncorrelated equilibrium. On the other hand, in Stackelberg Q-learning , there exists a leader-follower paradigm among the players, i.e .the leader‚Äôs strategy can be observed by a follower before the latter commits to an action. Convergence guarantees in self-play, although popular, become less meaningful when action sets of the players are different (defense configuration vs. exploits) and game utilities have a general-sum reward structure. The Landscape of Existing Games ‚ÄÉ\nWe seek to answer two questions‚Äì where does our BSMG fit in and why it is useful. In Figure¬†5, we graphically situate BSMG in the landscape of existing work.\nBSMG, as seen in the experiments, can generalize Markov Games  (and therefore, MDP). Instead of assuming infinite reasoning capabilities required for Bayesian Nash equilibrium , Bayesian Markov Game considers scenarios where players have finite levels of belief about other players(s) . On the other hand, Markov Games with Imperfect Information (MGIIs) assumes a Markovian property over the state, the observations, and the joint set of actions; this results in reasoning over opponent types and allows them to decompose Partially Observable Stochastic Games (POSGs)  into a set of Bayesian stage-games . In BSMGs, the assumption of a pre-specified distribution over attacker types helps us (1) avoid reasoning over the nested belief space, and (2) can be interpreted as the private information of the opponent in MGIIs (and POSGs) as being provided upfront.\nThus, BSMG becomes a special case of MGII and BMG with the added semantics of leader-follower interaction.\nOur assumptions (about modeling imperfect information) helps our game be scalable while providing adequate expressive power in the context of MTDs. Leader-follower scenarios ‚ÄÉ Researchers have investigated solution concepts in the context of Stochastic games with multiple-followers, but do not model incomplete information about them.222Extending the uncertainty over follower-types in BSMGs to multiple followers results in scalability issues. The interaction is generally modeled as a Semi-Markov Decision Process  and improvements consider (1) multiple followers , (2) factored state spaces [ref]45, (3) methods based on deep reinforcement learning   etc. Reinforcement Learning in MTDs Works that are precursors to our BSS-Q learning approach are the min-max Q-learning  in the context of complete information MGs and the Bayesian Nash Q-learning  for dynamic placement of sensors. Recent works that model the multi-agent cyber-scenarios as an MDPs (RL in Flip-it games ) or POMDPs (RL for MTDs ), as shown above, can generate policies that can be exploited by a strategic adversary."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Moving target defense: creating asymmetric uncertainty for cyber threats, volume 54",
      "abstract": "",
      "year": "2011",
      "venue": "Springer Science & Business Media",
      "authors": "Sushil Jajodia, Anup K Ghosh, Vipin Swarup, Cliff Wang, and X Sean Wang"
    },
    {
      "index": 1,
      "title": "Moving target defense: a symbiotic framework for ai & security",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta"
    },
    {
      "index": 2,
      "title": "A Survey of Moving Target Defenses for Network Security",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Communications Surveys & Tutorials",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Abdulhakim Sabur, Adel Alshamrani, Dijiang Huang, and Subbarao Kambhampati",
      "orig_title": "A survey of moving target defenses for network security",
      "paper_id": "1905.00964v2"
    },
    {
      "index": 3,
      "title": "From physical security to cybersecurity",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Cybersecurity",
      "authors": "Arunesh Sinha, Thanh H Nguyen, Debarun Kar, Matthew Brown, Milind Tambe, and Albert Xin Jiang"
    },
    {
      "index": 4,
      "title": "Moving Target Defense for Web Applications using Bayesian Stackelberg Games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 International Conference on Autonomous Agents & Multiagent Systems",
      "authors": "Satya Gautam Vadlamudi, Sailik Sengupta, Marthony Taguinod, Ziming Zhao, Adam Doup√©, Gail-Joon Ahn, and Subbarao Kambhampati",
      "orig_title": "Moving target defense for web applications using bayesian stackelberg games",
      "paper_id": "1602.07024v3"
    },
    {
      "index": 5,
      "title": "A game theoretic approach to strategy generation for moving target defense in web applications",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta, Satya Gautam Vadlamudi, Subbarao Kambhampati, Adam Doup√©, Ziming Zhao, Marthony Taguinod, and Gail-Joon Ahn"
    },
    {
      "index": 6,
      "title": "Computing stackelberg equilibria in discounted stochastic games",
      "abstract": "",
      "year": "2012",
      "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence",
      "authors": "Yevgeniy Vorobeychik and Satinder Singh"
    },
    {
      "index": 7,
      "title": "Markov game modeling of moving target defense for strategic detection of threats in cloud networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.09660",
      "authors": "Ankur Chowdhary, Sailik Sengupta, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 8,
      "title": "Markov modeling of moving target defense games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 ACM Workshop on Moving Target Defense",
      "authors": "Hoda Maleki, Saeed Valizadeh, William Koch, Azer Bestavros, and Marten van Dijk"
    },
    {
      "index": 9,
      "title": "Game-theoretic approach to feedback-driven multi-stage moving target defense",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on decision and game theory for security",
      "authors": "Quanyan Zhu and Tamer Ba≈üar"
    },
    {
      "index": 10,
      "title": "Spatial-Temporal Moving Target Defense: A Markov Stackelberg Game Model",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.10390",
      "authors": "Henger Li, Wen Shen, and Zizhan Zheng",
      "orig_title": "Spatial-temporal moving target defense: A markov stackelberg game model",
      "paper_id": "2002.10390v1"
    },
    {
      "index": 11,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "7th international joint conference on Autonomous agents and multiagent systems-Volume 2",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 12,
      "title": "Markov security games: Learning in spatial security problems",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems (2016)",
      "authors": "Richard Klima, Karl Tuyls, and Frans Oliehoek"
    },
    {
      "index": 13,
      "title": "Dynamic ids configuration in the presence of intruder type uncertainty",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Global Communications Conference (GLOBECOM)",
      "authors": "Xiaofan He, Huaiyu Dai, Peng Ning, and Rudra Dutta"
    },
    {
      "index": 14,
      "title": "Playing adaptively against stealthy opponents: A reinforcement learning strategy for the flipit security game",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.11938",
      "authors": "Lisa Oakley and Alina Oprea"
    },
    {
      "index": 15,
      "title": "Deep reinforcement learning based adaptive moving target defense",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.11972",
      "authors": "Taha Eghtesad, Yevgeniy Vorobeychik, and Aron Laszka"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 17,
      "title": "Stochastic games",
      "abstract": "",
      "year": "1953",
      "venue": "Proceedings of the national academy of sciences",
      "authors": "Lloyd S Shapley"
    },
    {
      "index": 18,
      "title": "Solving security games on graphs via marginal probabilities",
      "abstract": "",
      "year": "2013",
      "venue": "Twenty-Seventh AAAI Conference on Artificial Intelligence",
      "authors": "Joshua Letchford and Vincent Conitzer"
    },
    {
      "index": 19,
      "title": "General sum markov games for strategic detection of advanced persistent threats using moving target defense in cloud networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 20,
      "title": "Dynamic policy-based ids configuration",
      "abstract": "",
      "year": "2009",
      "venue": "48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference",
      "authors": "Quanyan Zhu and Tamer Ba≈üar"
    },
    {
      "index": 21,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "AAMAS",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 22,
      "title": "On generalized stackelberg strategies",
      "abstract": "",
      "year": "1978",
      "venue": "Journal of Optimization Theory and Applications",
      "authors": "George Leitmann"
    },
    {
      "index": 23,
      "title": "Computing the optimal strategy to commit to",
      "abstract": "",
      "year": "2006",
      "venue": "7th ACM Conference on Electronic Commerce, EC ‚Äô06",
      "authors": "Vincent Conitzer and Tuomas Sandholm"
    },
    {
      "index": 24,
      "title": "Leadership with commitment to mixed strategies",
      "abstract": "",
      "year": "2004",
      "venue": "CDAM Research Report LSE-CDAM-2004-01",
      "authors": "BV Stengel and S Zamir"
    },
    {
      "index": 25,
      "title": "Lecture notes on non-cooperative game theory",
      "abstract": "",
      "year": "2010",
      "venue": "Game Theory Module of the Graduate Program in Network Mathematics",
      "authors": "Tamer Basar et al."
    },
    {
      "index": 26,
      "title": "National vulnerability database",
      "abstract": "",
      "year": "",
      "venue": "https://nvd.nist.gov",
      "authors": ""
    },
    {
      "index": 27,
      "title": "An initial study of targeted personality models in the flipit game",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Anjon Basak, Jakub ƒåern·ª≥, Marcus Gutierrez, Shelby Curtis, Charles Kamhoua, Daniel Jones, Branislav Bo≈°ansk·ª≥, and Christopher Kiekintveld"
    },
    {
      "index": 28,
      "title": "Automated generation and analysis of attack graphs",
      "abstract": "",
      "year": "2002",
      "venue": "2002 IEEE Symposium on Security and Privacy",
      "authors": "Oleg Sheyner, Joshua Haines, Somesh Jha, Richard Lippmann, and Jeannette M Wing"
    },
    {
      "index": 29,
      "title": "A game theoretic approach to strategy determination for dynamic platform defenses",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Kevin M Carter, James F Riordan, and Hamed Okhravi"
    },
    {
      "index": 30,
      "title": "Deceiving cyber adversaries: A game theoretic approach",
      "abstract": "",
      "year": "2018",
      "venue": "17th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Aaron Schlenker, Omkar Thakoor, Haifeng Xu, Fei Fang, Milind Tambe, Long Tran-Thanh, Phebe Vayanos, and Yevgeniy Vorobeychik"
    },
    {
      "index": 31,
      "title": "Asymmetric multiagent reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "Web Intelligence and Agent Systems: An international journal",
      "authors": "Ville K√∂n√∂nen"
    },
    {
      "index": 32,
      "title": "Towards a theory of moving target defense",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Rui Zhuang, Scott A DeLoach, and Xinming Ou"
    },
    {
      "index": 33,
      "title": "Multiagent reinforcement learning: theoretical framework and an algorithm",
      "abstract": "",
      "year": "1998",
      "venue": "ICML",
      "authors": "Junling Hu, Michael P Wellman, et al."
    },
    {
      "index": 34,
      "title": "Multi-agent reinforcement learning: a critical survey",
      "abstract": "",
      "year": "2003",
      "venue": "Web manuscript",
      "authors": "Yoav Shoham, Rob Powers, and Trond Grenager"
    },
    {
      "index": 35,
      "title": "Markov games as a framework for multi-agent reinforcement learning",
      "abstract": "",
      "year": "1994",
      "venue": "Machine learning proceedings 1994",
      "authors": "Michael L Littman"
    },
    {
      "index": 36,
      "title": "Friend-or-foe q-learning in general-sum games",
      "abstract": "",
      "year": "2001",
      "venue": "ICML",
      "authors": "Michael L Littman"
    },
    {
      "index": 37,
      "title": "Correlated q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "ICML",
      "authors": "Amy Greenwald, Keith Hall, and Roberto Serrano"
    },
    {
      "index": 38,
      "title": "Finite depth of reasoning and equilibrium play in games with incomplete information",
      "abstract": "",
      "year": "2013",
      "venue": "Discussion Paper, Center for Mathematical Studies in Economics and ...",
      "authors": "Willemien Kets"
    },
    {
      "index": 39,
      "title": "On markov games played by bayesian and boundedly-rational players",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Muthukumaran Chandrasekaran, Yingke Chen, and Prashant Doshi"
    },
    {
      "index": 40,
      "title": "Contributions to the Theory of Games, volume 2",
      "abstract": "",
      "year": "1953",
      "venue": "Princeton University Press",
      "authors": "Harold William Kuhn and Albert William Tucker"
    },
    {
      "index": 41,
      "title": "Markov games of incomplete information for multi-agent reinforcement learning",
      "abstract": "",
      "year": "2011",
      "venue": "Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence",
      "authors": "Liam MacDermed, Charles Isbell, and Lora Weiss"
    },
    {
      "index": 42,
      "title": "Leader-follower semi-markov decision problems: theoretical framework and approximate solution",
      "abstract": "",
      "year": "2007",
      "venue": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
      "authors": "Kurian Tharakunnel and Siddhartha Bhattacharyya"
    },
    {
      "index": 43,
      "title": "A multi-agent reinforcement learning algorithm based on stackelberg game",
      "abstract": "",
      "year": "2017",
      "venue": "2017 6th Data Driven Control and Learning Systems (DDCLS)",
      "authors": "Chi Cheng, Zhangqing Zhu, Bo Xin, and Chunlin Chen"
    },
    {
      "index": 44,
      "title": "Leader-follower mdp models with factored state space and many followers-followers abstraction, structured dynamics and state aggregation",
      "abstract": "",
      "year": "2016",
      "venue": "Twenty-second European Conference on Artificial Intelligence",
      "authors": "R√©gis Sabbadin and Anne-France Viet"
    },
    {
      "index": 45,
      "title": "M3RL: Mind-aware Multi-agent Management Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00147",
      "authors": "Tianmin Shu and Yuandong Tian",
      "orig_title": "M3rl: Mind-aware multi-agent management reinforcement learning",
      "paper_id": "1810.00147v3"
    },
    {
      "index": 46,
      "title": "Learning expensive coordination: An event-based deep rl approach",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, and Bo An"
    },
    {
      "index": 47,
      "title": "A unified analysis of value-function-based reinforcement-learning algorithms",
      "abstract": "",
      "year": "1999",
      "venue": "Neural computation",
      "authors": "Csaba Szepesv√°ri and Michael L Littman"
    },
    {
      "index": 48,
      "title": "Bayesian games for threat prediction and situation analysis",
      "abstract": "",
      "year": "2004",
      "venue": "International Conference on Information Fusion",
      "authors": "Joel Brynielsson and Stefan Arnborg"
    }
  ]
}