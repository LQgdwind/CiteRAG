{
  "paper_id": "2305.18381v4",
  "title": "Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection",
  "sections": {
    "related work": "Dataset Distillation is a process of compressing a large dataset into a smaller and more representative dataset while maintaining the performance.\nExisting approaches can be roughly classified into:\n1) Meta-Model Matching maintains the transferability of the synthetic data, by optimizing the empirical loss on the original dataset of models trained on the synthetic data.\nIn 6 the authors first proposed the task of data distillation and used the meta-model matching framework for optimization.\nIn 4 kernel ridge regression was exploited to facilitate its inner optimization loop and is further extended to infinite wide networks . In  the optimization of synthetic data/classifier and feature extractor was separated.\n2) Gradient Matching. The authors in 0 proposed to align the gradients of the synthetic and real datasets and are further improved in  to perform the same image augmentations on both the real and synthetic data.\n3) Distribution Matching, where  matches feature distributions of the synthetic and real data, which is simple but effective. In  layer-wise feature alignment and early exit conditions are used to promote DM. In  DM was further enhanced with regularizers and model pool.\n4) Trajectory Matching:\nIn  the authors proposed to match the training trajectory of the model parameters, by aligning the future parameters trained on real data and synthetic data. In  the memory consumption of MTT was reduced and label learning was used.\n5) Factorization of synthetic data can reduce the storage burden and share knowledge among instances. For example,\n uses a strategy of putting multiple images on one synthetic sample.\n decomposes the synthetic data to the linear combination of bases.\n uses a hallucination network to combine the bases.\n maintains a smaller base space to further reduce the storage.\n6) Bayesian Pseudocoreset is a family of algorithms that learn the synthetic data with Bayesian inference   2. Data Selection/Pruning reduces the training data without significantly affecting performance.\nClassic data selection often calculates a scalar utility score for each sample based on predefined criteria  0 3 and filters samples based on scores.\nSome data pruning methods also consider the interaction between samples. For example,\n7 examines generalization influence to reduce training data, which aims to identify the smallest subset to satisfy the expected generalization ability.\nIn comparison, data distillation 6 and data condensation 0\nsynthesize new and smaller data. The performance of data distillation with the same images per class (IPC) significantly outperforms data pruning."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Joao Carreira and Andrew Zisserman",
      "orig_title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "paper_id": "1705.07750v3"
    },
    {
      "index": 1,
      "title": "End-to-End Incremental Learning",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Francisco M. Castro, Manuel J. Marin-Jimenez, Nicolas Guil, Cordelia Schmid, and Karteek Alahari",
      "orig_title": "End-to-end incremental learning",
      "paper_id": "1807.09536v2"
    },
    {
      "index": 2,
      "title": "Dataset Distillation by Matching Training Trajectories",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu",
      "orig_title": "Dataset distillation by matching training trajectories",
      "paper_id": "2203.11932v1"
    },
    {
      "index": 3,
      "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.10586",
      "authors": "Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh",
      "orig_title": "Scaling up dataset distillation to imagenet-1k with constant memory",
      "paper_id": "2211.10586v4"
    },
    {
      "index": 4,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 5,
      "title": "Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2206.02916",
      "authors": "Zhiwei Deng and Olga Russakovsky",
      "orig_title": "Remember the past: Distilling datasets into addressable memories for neural networks",
      "paper_id": "2206.02916v2"
    },
    {
      "index": 6,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 7,
      "title": "Grad-Match: Gradient Matching based Data Subset Selection for Efficient Deep Model Training",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer",
      "orig_title": "Grad-match: Gradient matching based data subset selection for efficient deep model training",
      "paper_id": "2103.00123v2"
    },
    {
      "index": 8,
      "title": "Glister: Generalization based Data Subset Selection for Efficient and Robust Learning",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer",
      "orig_title": "Glister: Generalization based data subset selection for efficient and robust learning",
      "paper_id": "2012.10630v4"
    },
    {
      "index": 9,
      "title": "On Divergence Measures for Bayesian Pseudocoresets",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.06205",
      "authors": "Balhae Kim, Jungwon Choi, Seanie Lee, Yoonho Lee, Jung-Woo Ha, and Juho Lee",
      "orig_title": "On divergence measures for bayesian pseudocoresets",
      "paper_id": "2210.06205v1"
    },
    {
      "index": 10,
      "title": "Dataset condensation via efficient synthetic-data parameterization",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song"
    },
    {
      "index": 11,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Alex Krizhevsky, Geoffrey Hinton, et al."
    },
    {
      "index": 12,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Communications of the ACM",
      "authors": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton"
    },
    {
      "index": 13,
      "title": "Tiny imagenet visual recognition challenge",
      "abstract": "",
      "year": "2015",
      "venue": "CS 231N",
      "authors": "Ya Le and Xuan Yang"
    },
    {
      "index": 14,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner"
    },
    {
      "index": 15,
      "title": "Dataset Condensation with Latent Space Knowledge Factorization and Sharing",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.10494",
      "authors": "Hae Beom Lee, Dong Bok Lee, and Sung Ju Hwang",
      "orig_title": "Dataset condensation with latent space knowledge factorization and sharing",
      "paper_id": "2208.10494v1"
    },
    {
      "index": 16,
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.12597",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi",
      "orig_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "paper_id": "2301.12597v3"
    },
    {
      "index": 17,
      "title": "Dataset Distillation via Factorization",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.16774",
      "authors": "Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang",
      "orig_title": "Dataset distillation via factorization",
      "paper_id": "2210.16774v1"
    },
    {
      "index": 18,
      "title": "DREAM: Efficient Dataset Distillation by Representative Matching",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.14416",
      "authors": "Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You",
      "orig_title": "Dream: Efficient dataset distillation by representative matching",
      "paper_id": "2302.14416v3"
    },
    {
      "index": 19,
      "title": "Efficient dataset distillation using random feature approximation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.12067",
      "authors": "Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus"
    },
    {
      "index": 20,
      "title": "Bayesian pseudocoresets",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Dionysis Manousakas, Zuheng Xu, Cecilia Mascolo, and Trevor Campbell"
    },
    {
      "index": 21,
      "title": "Coresets for data-efficient training of machine learning models",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec"
    },
    {
      "index": 22,
      "title": "Reading digits in natural images with unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng"
    },
    {
      "index": 23,
      "title": "Dataset Meta-Learning from Kernel Ridge-Regression",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.00050",
      "authors": "Timothy Nguyen, Zhourong Chen, and Jaehoon Lee",
      "orig_title": "Dataset meta-learning from kernel ridge-regression",
      "paper_id": "2011.00050v3"
    },
    {
      "index": 24,
      "title": "Dataset Distillation with Infinitely Wide Convolutional Networks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee",
      "orig_title": "Dataset distillation with infinitely wide convolutional networks",
      "paper_id": "2107.13034v3"
    },
    {
      "index": 25,
      "title": "Iterative teaching by data hallucination",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.17467",
      "authors": "Zeju Qiu, Weiyang Liu, Tim Z Xiao, Zhen Liu, Umang Bhatt, Yucen Luo, Adrian Weller, and Bernhard Schölkopf"
    },
    {
      "index": 26,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 27,
      "title": "Black box variational inference",
      "abstract": "",
      "year": "2014",
      "venue": "AISTATS",
      "authors": "Rajesh Ranganath, Sean Gerrish, and David Blei"
    },
    {
      "index": 28,
      "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.02114",
      "authors": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki",
      "orig_title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
      "paper_id": "2111.02114v1"
    },
    {
      "index": 29,
      "title": "A geometric approach to active learning for convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv, abs/1708.00489",
      "authors": "Ozan Sener and Silvio Savarese"
    },
    {
      "index": 30,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 31,
      "title": "Constructing Bayesian Pseudo-Coresets using Contrastive Divergence",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.11278",
      "authors": "Piyush Tiwary, Kumar Shubham, Vivek Kashyap, et al.",
      "orig_title": "Constructing bayesian pseudo-coresets using contrastive divergence",
      "paper_id": "2303.11278v2"
    },
    {
      "index": 32,
      "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv, abs/1812.05159",
      "authors": "Mariya Toneva, Alessandro Sordoni, Rémi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon",
      "orig_title": "An empirical study of example forgetting during deep neural network learning",
      "paper_id": "1812.05159v3"
    },
    {
      "index": 33,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.13971",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 34,
      "title": "CAFE: Learning to Condense Dataset by Aligning Features",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You",
      "orig_title": "Cafe: Learning to condense dataset by aligning features",
      "paper_id": "2203.01531v2"
    },
    {
      "index": 35,
      "title": "Dataset Distillation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.10959",
      "authors": "Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros",
      "orig_title": "Dataset distillation",
      "paper_id": "1811.10959v3"
    },
    {
      "index": 36,
      "title": "Dataset Pruning: Reducing Training Data by Examining Generalization Influence",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/2205.09329",
      "authors": "Shuo Yang, Zeke Xie, Hanyu Peng, Minjing Xu, Mingming Sun, and P. Li",
      "orig_title": "Dataset pruning: Reducing training data by examining generalization influence",
      "paper_id": "2205.09329v2"
    },
    {
      "index": 37,
      "title": "Dataset Condensation with Differentiable Siamese Augmentation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Bo Zhao and Hakan Bilen",
      "orig_title": "Dataset condensation with differentiable siamese augmentation",
      "paper_id": "2102.08259v2"
    },
    {
      "index": 38,
      "title": "Dataset Condensation with Distribution Matching",
      "abstract": "",
      "year": "2023",
      "venue": "WACV",
      "authors": "Bo Zhao and Hakan Bilen",
      "orig_title": "Dataset condensation with distribution matching",
      "paper_id": "2110.04181v3"
    },
    {
      "index": 39,
      "title": "Dataset Condensation with Gradient Matching",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.05929",
      "authors": "Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen",
      "orig_title": "Dataset condensation with gradient matching",
      "paper_id": "2006.05929v3"
    },
    {
      "index": 40,
      "title": "Improved Distribution Matching for Dataset Condensation",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu",
      "orig_title": "Improved distribution matching for dataset condensation",
      "paper_id": "2307.09742v1"
    },
    {
      "index": 41,
      "title": "Dataset Distillation using Neural Feature Regression",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2206.00719",
      "authors": "Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba",
      "orig_title": "Dataset distillation using neural feature regression",
      "paper_id": "2206.00719v2"
    }
  ]
}