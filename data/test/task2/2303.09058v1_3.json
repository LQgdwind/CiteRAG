{
  "paper_id": "2303.09058v1",
  "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning",
  "sections": {
    "iv-b1 intrinsic reward design (ird)": "To encourage agents to explore in SARL tasks, prior works generally hope to visit novel states more frequently, i.e., those states visited less often .\nCount-based exploration methods provide an example of such bonuses, where the intrinsic reward is generally ri=1/n​(s)superscript𝑟𝑖1𝑛𝑠r^{i}=1/{n(s)}, where n𝑛n is the number of visits to the state [ref]45.\nAs the problem becomes more complex, the state space gradually increases, and almost every state can only be explored once.\nAn alternative is to calculate the prediction error associated with the agent’s transition .\nSince the environment only feeds back a team reward, there exists a credit assignment problem in MARL tasks.\nWe introduce an intrinsic reward based on prediction error, and calculate it for each agent’s local observations, so as to encourage the agent to explore.\nIn this way, we extend the intrinsic reward design from SARL to MARL [ref]15.\nAnd since each agent calculates its intrinsic reward based on its own local observations, the intrinsic reward can be used to represent the contribution of the agent to the whole, thus solving the problem of credit assignment to a certain extent. The IRD network is shown in Fig. 5.\nIt consists of a target network whose parameters are randomly initialized and fixed in the same way as in RND , and a predictor network trained with data collected by the agent.\nThe fixed parameters of the target network are intended to reduce the randomness of the target fitting function, so that prediction errors are all caused by the novelty of the observation.\nThus, the prediction error is large if similar observations are rarely seen, resulting in a higher intrinsic reward that encourages the agent to explore. The target and predictor networks accept the same input, which is the local observations of the agent, and output feature vectors of the same dimension: f^:𝒪→ℝk,g:𝒪→ℝk:^𝑓→𝒪superscriptℝ𝑘𝑔:→𝒪superscriptℝ𝑘\\hat{f}:\\mathcal{O}\\rightarrow\\mathbb{R}^{k},g:\\mathcal{O}\\rightarrow\\mathbb{R}^{k}.\nWe employ weight sharing across the network of agents.\nThe agent ID is included in the local observation to allow agents to have different intrinsic rewards.\nThe intrinsic reward and mean-square error (MSE) loss function are respectively defined as where ℬℬ\\mathcal{B} is the batch size sampled from the replay buffer, m𝑚m is a random time step as in DRQN [ref]46, and l𝑙l is the maximum length of an episode. We normalize the observations ot+1isuperscriptsubscript𝑜𝑡1𝑖o_{t+1}^{i} and intrinsic reward rt+1isuperscriptsubscript𝑟𝑡1𝑖r_{t+1}^{i} to keep the intrinsic reward at a consistent magnitude.\nA standard Gaussian distribution is set for observations o​b​s:𝐍​(0,1):𝑜𝑏𝑠𝐍01obs:\\mathbf{N}(0,1) and intrinsic rewards r​w:𝐍​(0,1):𝑟𝑤𝐍01rw:\\mathbf{N}(0,1), which regularly update the mean and standard deviation based on the network’s inputs and outputs:"
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Multitask learning and reinforcement learning for personalized dialog generation: An empirical study",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "M. Yang, W. Huang, W. Tu, Q. Qu, Y. Shen, and K. Lei"
    },
    {
      "index": 1,
      "title": "Learning automata-based multiagent reinforcement learning for optimization of cooperative tasks",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "Z. Zhang, D. Wang, and J. Gao"
    },
    {
      "index": 2,
      "title": "MASER: Multi-Agent Reinforcement Learning with Subgoals Generated from Experience Replay Buffer",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Jeon, W. Kim, W. Jung, and Y. Sung",
      "orig_title": "Maser: Multi-agent reinforcement learning with subgoals generated from experience replay buffer",
      "paper_id": "2206.10607v1"
    },
    {
      "index": 3,
      "title": "Intrinsic motivated multi-agent communication",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "C. Sun, B. Wu, R. Wang, X. Hu, X. Yang, and C. Cong"
    },
    {
      "index": 4,
      "title": "Minimax-optimal multi-agent rl in zero-sum markov games with a generative model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.10458",
      "authors": "G. Li, Y. Chi, Y. Wei, and Y. Chen"
    },
    {
      "index": 5,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 6,
      "title": "Learning Individually Inferred Communication for Multi-Agent Cooperation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Z. Ding, T. Huang, and Z. Lu",
      "orig_title": "Learning individually inferred communication for multi-agent cooperation",
      "paper_id": "2006.06455v2"
    },
    {
      "index": 7,
      "title": "Graph Convolutional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.09202",
      "authors": "J. Jiang, C. Dun, T. Huang, and Z. Lu",
      "orig_title": "Graph convolutional reinforcement learning",
      "paper_id": "1810.09202v5"
    },
    {
      "index": 8,
      "title": "Distributed multiagent reinforcement learning with action networks for dynamic economic dispatch",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "C. Hu, G. Wen, S. Wang, J. Fu, and W. Yu"
    },
    {
      "index": 9,
      "title": "Exploration with task information for meta reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "P. Jiang, S. Song, and G. Huang"
    },
    {
      "index": 10,
      "title": "Feudal latent space exploration for coordinated multi-agent reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "X. Liu and Y. Tan"
    },
    {
      "index": 11,
      "title": "Understanding via exploration: Discovery of interpretable features with deep reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. Wei, Z. Qiu, F. Wang, W. Lin, N. Gui, and W. Gui"
    },
    {
      "index": 12,
      "title": "Exploration in deep reinforcement learning: a comprehensive survey",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.06668",
      "authors": "T. Yang, H. Tang, C. Bai, J. Liu, J. Hao, Z. Meng, P. Liu, and Z. Wang"
    },
    {
      "index": 13,
      "title": "First return, then explore",
      "abstract": "",
      "year": "2021",
      "venue": "Nature",
      "authors": "A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune"
    },
    {
      "index": 14,
      "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multi-Agent Domain",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and Z. Wang",
      "orig_title": "Exploration in deep reinforcement learning: From single-agent to multiagent domain",
      "paper_id": "2109.06668v6"
    },
    {
      "index": 15,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al.",
      "orig_title": "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 16,
      "title": "SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.06591",
      "authors": "L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, and M. Michalski",
      "orig_title": "Seed rl: Scalable and efficient deep-rl with accelerated central inference",
      "paper_id": "1910.06591v2"
    },
    {
      "index": 17,
      "title": "Ray: A distributed framework for emerging {{\\{AI}}} applications",
      "abstract": "",
      "year": "2018",
      "venue": "13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)",
      "authors": "P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, et al."
    },
    {
      "index": 18,
      "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.05296",
      "authors": "P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.",
      "orig_title": "Value-decomposition networks for cooperative multi-agent learning",
      "paper_id": "1706.05296v1"
    },
    {
      "index": 19,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 20,
      "title": "Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "T. Rashid, G. Farquhar, B. Peng, and S. Whiteson",
      "orig_title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "2006.10800v2"
    },
    {
      "index": 21,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 22,
      "title": "MAVEN: Multi-Agent Variational Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson",
      "orig_title": "Maven: Multi-agent variational exploration",
      "paper_id": "1910.07483v2"
    },
    {
      "index": 23,
      "title": "Parallel data processing with mapreduce: a survey",
      "abstract": "",
      "year": "2012",
      "venue": "AcM sIGMoD record",
      "authors": "K.-H. Lee, Y.-J. Lee, H. Choi, Y. D. Chung, and B. Moon"
    },
    {
      "index": 24,
      "title": "Liir: Learning individual intrinsic reward in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Du, L. Han, M. Fang, J. Liu, T. Dai, and D. Tao"
    },
    {
      "index": 25,
      "title": "Exploration by Random Network Distillation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.12894",
      "authors": "Y. Burda, H. Edwards, A. Storkey, and O. Klimov",
      "orig_title": "Exploration by random network distillation",
      "paper_id": "1810.12894v1"
    },
    {
      "index": 26,
      "title": "Dynamic analysis of multiagent q-learning with ε𝜀\\varepsilon-greedy exploration",
      "abstract": "",
      "year": "2009",
      "venue": "26th annual international conference on machine learning",
      "authors": "E. Rodrigues Gomes and R. Kowalczyk"
    },
    {
      "index": 27,
      "title": "A survey and critique of multiagent deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Autonomous Agents and Multi-Agent Systems",
      "authors": "P. Hernandez-Leal, B. Kartal, and M. E. Taylor"
    },
    {
      "index": 28,
      "title": "Multi-agent reinforcement learning: Independent vs. cooperative agents",
      "abstract": "",
      "year": "1993",
      "venue": "tenth international conference on machine learning",
      "authors": "M. Tan"
    },
    {
      "index": 29,
      "title": "Multiagent cooperation and competition with deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "PloS one",
      "authors": "A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente"
    },
    {
      "index": 30,
      "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson",
      "orig_title": "Stabilising experience replay for deep multi-agent reinforcement learning",
      "paper_id": "1702.08887v3"
    },
    {
      "index": 31,
      "title": "Coordinated reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "ICML",
      "authors": "C. Guestrin, M. Lagoudakis, and R. Parr"
    },
    {
      "index": 32,
      "title": "Collaborative multiagent reinforcement learning by payoff propagation",
      "abstract": "",
      "year": "2006",
      "venue": "Journal of Machine Learning Research",
      "authors": "J. R. Kok and N. Vlassis"
    },
    {
      "index": 33,
      "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1702.03037",
      "authors": "J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel",
      "orig_title": "Multi-agent reinforcement learning in sequential social dilemmas",
      "paper_id": "1702.03037v1"
    },
    {
      "index": 34,
      "title": "TLeague: A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.12895",
      "authors": "P. Sun, J. Xiong, L. Han, X. Sun, S. Li, J. Xu, M. Fang, and Z. Zhang",
      "orig_title": "Tleague: A framework for competitive self-play based distributed multi-agent reinforcement learning",
      "paper_id": "2011.12895v2"
    },
    {
      "index": 35,
      "title": "Mava: A research framework for distributed multi-agent reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.01460",
      "authors": "A. Pretorius, K. ab Tessera, A. P. Smit, K. Eloff, C. Formanek, S. J. Grimbly, S. Danisa, L. Francis, J. Shock, H. Kamper, W. Brink, H. Engelbrecht, A. Laterre, and K. Beguir"
    },
    {
      "index": 36,
      "title": "Learning automata-based multiagent reinforcement learning for optimization of cooperative tasks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Z. Zhang, D. Wang, and J. Gao"
    },
    {
      "index": 37,
      "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.03939",
      "authors": "Y. Yang, J. Hao, B. Liao, K. Shao, G. Chen, W. Liu, and H. Tang",
      "orig_title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
      "paper_id": "2002.03939v2"
    },
    {
      "index": 38,
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.01062",
      "authors": "J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang",
      "orig_title": "Qplex: Duplex dueling multi-agent q-learning",
      "paper_id": "2008.01062v3"
    },
    {
      "index": 39,
      "title": "Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.02138",
      "authors": "W. Böhmer, T. Rashid, and S. Whiteson",
      "orig_title": "Exploration with unreliable intrinsic reward in multi-agent reinforcement learning",
      "paper_id": "1906.02138v1"
    },
    {
      "index": 40,
      "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.12127",
      "authors": "S. Iqbal and F. Sha",
      "orig_title": "Coordinated exploration via intrinsic rewards for multi-agent reinforcement learning",
      "paper_id": "1905.12127v3"
    },
    {
      "index": 41,
      "title": "A concise introduction to decentralized POMDPs",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "F. A. Oliehoek and C. Amato"
    },
    {
      "index": 42,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "nature",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al."
    },
    {
      "index": 43,
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell",
      "orig_title": "Curiosity-driven exploration by self-supervised prediction",
      "paper_id": "1705.05363v1"
    },
    {
      "index": 44,
      "title": "Count-Based Exploration with Neural Density Models",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos",
      "orig_title": "Count-based exploration with neural density models",
      "paper_id": "1703.01310v2"
    },
    {
      "index": 45,
      "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
      "abstract": "",
      "year": "2015",
      "venue": "2015 aaai fall symposium series",
      "authors": "M. Hausknecht and P. Stone",
      "orig_title": "Deep recurrent q-learning for partially observable mdps",
      "paper_id": "1507.06527v4"
    },
    {
      "index": 46,
      "title": "Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic Environments",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Control Systems Letters",
      "authors": "H. Chen and C. Liu",
      "orig_title": "Safe and sample-efficient reinforcement learning for clustered dynamic environments",
      "paper_id": "2303.14265v1"
    },
    {
      "index": 47,
      "title": "Self-adaptive priority correction for prioritized experience replay",
      "abstract": "",
      "year": "2020",
      "venue": "Applied Sciences",
      "authors": "H. Zhang, C. Qu, J. Zhang, and J. Li"
    },
    {
      "index": 48,
      "title": "The StarCraft Multi-Agent Challenge",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.04043",
      "authors": "M. Samvelyan, T. Rashid, C. S. De Witt, G. Farquhar, N. Nardelli, T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, and S. Whiteson",
      "orig_title": "The starcraft multi-agent challenge",
      "paper_id": "1902.04043v5"
    }
  ]
}