{
  "paper_id": "2111.05008v1",
  "title": "Misspecified Gaussian Process Bandit Optimization",
  "sections": {
    "introduction": "Bandit optimization has been successfully used in a great number of machine learning and real-world applications, e.g., in mobile health , environmental monitoring , economics , hyperparameter tuning , to name a few. To scale to large or continuous domains, modern bandit approaches try to model and exploit the problem structure that is often manifested as correlations in rewards of \"similar\" actions. Hence, the key idea of kernelized bandits is to consider only smooth reward functions of a low norm belonging to a chosen Reproducing Kernel Hilbert Space (RKHS) of functions. This permits the application of flexible nonparametric Gaussian process (GP) models and Bayesian optimization methods via a well-studied link between RKHS functions and GPs (see, e.g.,  for a concise review). A vast majority of previous works on nonparametric kernelized bandits have focused on designing algorithms and theoretical bounds on the standard notions of regret (see, e.g.,   ). However, they solely focus on the realizable (i.e., well-specified) case in which one assumes perfect knowledge of the true function class. For example, the analysis of the prominent GP-UCB  algorithm assumes the model to be well-specified and ignores potential misspecification issues. As the realizability assumption may be too restrictive in real applications, we focus on the case where it may only hold approximately. In practice, model misspecifications can arise due to various reasons, such as incorrect choice of kernel, consideration of an overly smooth function class, hyperparameter estimation errors, etc. Hence, an open question is to characterize the impact of model misspecification in the kernelized setting, and to design robust algorithms whose performance degrades optimally with the increasing level of misspecification. In this paper, we study the GP bandit problem with model misspecification in which the true unknown function might be ϵitalic-ϵ\\epsilon-far (as measured in the max norm) from a member of the learner’s assumed hypothesis class. We propose a novel GP bandit algorithm and regret bounds that depend on the misspecification error, time horizon, and underlying kernel. Specifically, we present an algorithm that is based on the classical uncertainty sampling approach that is frequently used in Bayesian optimization and experimental design. Importantly, our main presented algorithm assumes no knowledge of the misspecification error ϵitalic-ϵ\\epsilon and achieves standard regret rates in the realizable case. Related work on GP bandits. GP bandit algorithms have received significant attention in recent years (e.g.,    ). While the most popular approaches in the stochastic setting rely on upper confidence bound (UCB) and Thompson sampling strategies, a number of works also consider uncertainty sampling procedures (e.g.,   ). Beyond the standard setting, numerous works have also considered the contextual bandit setting (e.g.,   ), while the case of unknown kernel hyperparameters and misspecified smoothness has been studied in, e.g., [ref]46  . [ref]46  assume that the reward function is smooth as measured by the known (or partially known) kernel, while in our problem, we allow the unknown function to lie outside a given kernel’s reproducing space. Related corruption-robust GP bandits in which an adversary can additively perturb the observed rewards are recently considered in [ref]4.\nIn Section 2, we also discuss how the misspecified problem can be seen from this perspective, where the corruption function is fixed and the adversarial budget scales with the time horizon. While the focus of [ref]4 is on protecting against an adaptive adversary and thus designing randomized algorithms and regret bounds that depend on the adversarial budget, we propose deterministic algorithms and analyze the impact of misspecification. Apart from corruptions, several works have considered other robust aspects in GP bandits, such as designing robust strategies against the shift in uncontrollable covariates   1 8 . While they report robust regret guarantees, they still assume the realizable case only. Our goal of attaining small regret despite the wrong hypothesis class requires very different techniques from these previous works. Related work on misspecified linear bandits.\nRecently, works on reinforcement learning with misspecified linear features (e.g.,   ) have renewed interest in the related misspecified linear bandits (e.g., , , 0, 5) first introduced in . In , the authors show that standard algorithms must suffer Ω​(ϵ​T)Ωitalic-ϵ𝑇\\Omega(\\epsilon T) regret under an additive ϵitalic-ϵ\\epsilon–perturbation of the linear model. Recently,  propose a robust variant of OFUL  that requires knowing the misspecification parameter ϵitalic-ϵ\\epsilon. In particular, their algorithm obtains a high-probability O~​(d​T+ϵ​d​T)~𝑂𝑑𝑇italic-ϵ𝑑𝑇\\tilde{O}(d\\sqrt{T}+\\epsilon\\sqrt{d}T) regret bound. In , the authors propose another arm-elimination algorithm based on G-experimental design. Unlike the previous works, their algorithm is agnostic to the misspecification level, and its performance matches the lower bounds. As shown in , when the number of actions K𝐾K is large (K≫dmuch-greater-than𝐾𝑑K\\gg d), the “price” of ϵitalic-ϵ\\epsilon-misspecification must grow as Ω​(ϵ​d​T)Ωitalic-ϵ𝑑𝑇\\Omega(\\epsilon\\sqrt{d}T).111A result by 5 further shows that this result can be improved to O​(ϵ​K​T)𝑂italic-ϵ𝐾𝑇O(\\epsilon\\sqrt{K}T) in the small-K𝐾K regime. Our main algorithm is inspired by the proposed technique for the finite-arm misspecified linear bandit setting . It works in the more general kernelized setting, uses a simpler data acquisition rule often used in Bayesian optimization and experimental design, and recovers the same optimal guarantees when instantiated with linear kernel. Several works have recently considered the misspecified contextual linear bandit problem with unknown model misspecification ϵitalic-ϵ\\epsilon. 4 introduce a new family of algorithms that require access to an online oracle for square loss regression and address the case of adversarial contexts. Concurrent work of 3 solves the case when contexts / action sets are stochastic. Both works (4 and 3) leverage CORRAL-type aggregation  of contextual bandit algorithms and achieve the optimal O~​(d​T​ϵ+d​T)~𝑂𝑑𝑇italic-ϵ𝑑𝑇\\tilde{O}(\\sqrt{d}T\\epsilon+d\\sqrt{T}) regret bound. Finally, in 2, the authors present a practical master algorithm that plays base algorithms that come with a candidate regret bound that may not hold during all rounds. The master algorithm plays base algorithms in a balanced way and suitably eliminates algorithms whose regret bound is no longer valid. Similarly to the previous works that rely on the CORRAL-type master algorithms, we use the balancing master algorithm of 3 together with our GP-bandit base algorithm to provide contextual misspecified regret bounds. Around the time of the submission of this work, a related approach that among others also considers misspecified kernel bandits appeared online . [8, Theorem 3] contains the same regret scaling due to misspecification as we obtain in our results. The main difference between the two works is in the proposed algorithms and analysis techniques.\nOur approach does not require robust estimators and simply uses the standard ones (i.e., GP posterior/ kernelized ridge regression mean and variance estimators) that can be computed in the closed-form. It can also handle infinite action sets similarly to the classical Bayesian optimization algorithms; it utilizes a single acquisition function that, in practice, can be maximized via standard off-the-shelf global optimization solvers. We also present a complete treatment of the misspecified problem by showing the failure of standard UCB approaches, algorithms for known and unknown ϵitalic-ϵ\\epsilon, an impossibility result, and extensions to the contextual bandit setting. Finally, our main algorithm (Algorithm 2) demonstrates the use of a different acquisition function in comparison to   that relies on standard and non-robust estimators. Contributions. In this paper, we systematically handle model misspecification in GP bandits. Specifically, this paper makes the following contributions: We introduce a misspecified kernelized bandit problem, and for known misspecification error ϵitalic-ϵ\\epsilon, we present the EC-GP-UCB algorithm\nwith enlarged confidence bounds that achieves cumulative RT=O​(B​γT​T+γT​T+ϵ​T​γT)subscript𝑅𝑇𝑂𝐵subscript𝛾𝑇𝑇subscript𝛾𝑇𝑇italic-ϵ𝑇subscript𝛾𝑇R_{T}=O(B\\sqrt{\\gamma_{T}T}+\\gamma_{T}\\sqrt{T}+\\epsilon T\\sqrt{\\gamma_{T}}) regret. Our simple lower bound argument shows that Ω​(T​ϵ)Ω𝑇italic-ϵ\\Omega(T\\epsilon) regret is unavoidable in the general kernelized setting. For when ϵitalic-ϵ\\epsilon is unknown, we propose another algorithm based on uncertainty sampling and phased exploration that achieves (up to polylogpolylog\\operatorname{polylog} factors) the previous regret rates in the misspecified setting, and standard regret guarantees in the realizable case (when ϵ=0italic-ϵ0\\epsilon=0). Finally, we consider a misspecified contextual kernelized problem, and show that when action sets are stochastic, our EC-GP-UCB algorithm can be effectively combined with the regret bound balancing strategy from 3 to achieve previous regret bounds (up to some additive lower order terms)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Improved algorithms for linear stochastic bandits",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári"
    },
    {
      "index": 1,
      "title": "Corralling a band of bandit algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Learning Theory",
      "authors": "Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire"
    },
    {
      "index": 2,
      "title": "No-Regret Bayesian Optimization with Unknown Hyperparameters",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.03357",
      "authors": "Felix Berkenkamp, Angela P Schoellig, and Andreas Krause",
      "orig_title": "No-regret Bayesian optimization with unknown hyperparameters",
      "paper_id": "1901.03357v2"
    },
    {
      "index": 3,
      "title": "Corruption-tolerant Gaussian process bandit optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Artificial Intelligence and Statistics (AISTATS)",
      "authors": "Ilija Bogunovic, Andreas Krause, and Scarlett Jonathan"
    },
    {
      "index": 4,
      "title": "Adversarially Robust Optimization with Gaussian Processes",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher",
      "orig_title": "Adversarially robust optimization with Gaussian processes",
      "paper_id": "1810.10775v2"
    },
    {
      "index": 5,
      "title": "Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher",
      "orig_title": "Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation",
      "paper_id": "1610.07379v1"
    },
    {
      "index": 6,
      "title": "Bayesian optimization of risk measures",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.05554",
      "authors": "Sait Cakmak, Raul Astudillo, Peter Frazier, and Enlu Zhou"
    },
    {
      "index": 7,
      "title": "High-dimensional experimental design and kernel bandits",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Romain Camilleri, Kevin Jamieson, and Julian Katz-Samuels"
    },
    {
      "index": 8,
      "title": "On Kernelized Multi-armed Bandits",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Sayak Ray Chowdhury and Aditya Gopalan",
      "orig_title": "On kernelized multi-armed bandits",
      "paper_id": "1704.00445v2"
    },
    {
      "index": 9,
      "title": "Parallel Gaussian process optimization with upper confidence bound and pure exploration",
      "abstract": "",
      "year": "2013",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Emile Contal, David Buffoni, Alexandre Robicquet, and Nicolas Vayatis"
    },
    {
      "index": 10,
      "title": "Regret bounds for deterministic Gaussian process bandits",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1203.2177",
      "authors": "Nando de Freitas, Alex Smola, and Masrour Zoghi"
    },
    {
      "index": 11,
      "title": "Is a good representation sufficient for sample efficient reinforcement learning?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.03016",
      "authors": "Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang"
    },
    {
      "index": 12,
      "title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
      "abstract": "",
      "year": "2018",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Audrey Durand, Odalric-Ambrym Maillard, and Joelle Pineau",
      "orig_title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
      "paper_id": "1708.00768v1"
    },
    {
      "index": 13,
      "title": "Adapting to misspecification in contextual bandits",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert"
    },
    {
      "index": 14,
      "title": "Beyond UCB: Optimal and efficient contextual bandits with regression oracles",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.04926",
      "authors": "Dylan J Foster and Alexander Rakhlin"
    },
    {
      "index": 15,
      "title": "Misspecified Linear Bandits",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan",
      "orig_title": "Misspecified linear bandits",
      "paper_id": "1704.06880v1"
    },
    {
      "index": 16,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 17,
      "title": "Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.02582",
      "authors": "Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur",
      "orig_title": "Gaussian processes and kernel methods: A review on connections and equivalences",
      "paper_id": "1807.02582v1"
    },
    {
      "index": 18,
      "title": "The equivalence of two extremum problems",
      "abstract": "",
      "year": "1960",
      "venue": "Canadian Journal of Mathematics",
      "authors": "Jack Kiefer and Jacob Wolfowitz"
    },
    {
      "index": 19,
      "title": "Distributionally robust Bayesian optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.09038",
      "authors": "Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause"
    },
    {
      "index": 20,
      "title": "Stochastic Bandits with Context Distributions",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Johannes Kirschner and Andreas Krause",
      "orig_title": "Stochastic bandits with context distributions",
      "paper_id": "1906.02685v2"
    },
    {
      "index": 21,
      "title": "Contextual Gaussian process bandit optimization",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "Andreas Krause and Cheng S Ong"
    },
    {
      "index": 22,
      "title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research",
      "authors": "Andreas Krause, Ajit Singh, and Carlos Guestrin"
    },
    {
      "index": 23,
      "title": "Learning with good feature representations in bandits and in RL with a generative model",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Tor Lattimore, Csaba Szepesvari, and Gellert Weisz"
    },
    {
      "index": 24,
      "title": "A contextual-bandit approach to personalized news article recommendation",
      "abstract": "",
      "year": "2010",
      "venue": "International conference on World Wide Web",
      "authors": "Lihong Li, Wei Chu, John Langford, and Robert E Schapire"
    },
    {
      "index": 25,
      "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar",
      "orig_title": "Hyperband: A novel bandit-based approach to hyperparameter optimization",
      "paper_id": "1603.06560v4"
    },
    {
      "index": 26,
      "title": "Competing Bandits in Matching Markets",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Lydia T Liu, Horia Mania, and Michael Jordan",
      "orig_title": "Competing bandits in matching markets",
      "paper_id": "1906.05363v2"
    },
    {
      "index": 27,
      "title": "Universal kernels",
      "abstract": "",
      "year": "2006",
      "venue": "Journal of Machine Learning Research",
      "authors": "Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang"
    },
    {
      "index": 28,
      "title": "Uncertainty quantification using martingales for misspecified Gaussian processes",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.07368",
      "authors": "Willie Neiswanger and Aaditya Ramdas",
      "orig_title": "Uncertainty quantification using martingales for misspecified Gaussian processes",
      "paper_id": "2006.07368v2"
    },
    {
      "index": 29,
      "title": "Efficient and robust algorithms for adversarial linear contextual bandits",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.00287",
      "authors": "Gergely Neu and Julia Olkhovskaya",
      "orig_title": "Efficient and robust algorithms for adversarial linear contextual bandits",
      "paper_id": "2002.00287v3"
    },
    {
      "index": 30,
      "title": "Distributionally robust Bayesian quadrature optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.06814",
      "authors": "Thanh Tang Nguyen, Sunil Gupta, Huong Ha, Santu Rana, and Svetha Venkatesh"
    },
    {
      "index": 31,
      "title": "Regret Bound Balancing and Elimination for Model Selection in Bandits and RL",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.13045",
      "authors": "Aldo Pacchiano, Christoph Dann, Claudio Gentile, and Peter Bartlett",
      "orig_title": "Regret bound balancing and elimination for model selection in bandits and RL",
      "paper_id": "2012.13045v1"
    },
    {
      "index": 32,
      "title": "Model selection in contextual stochastic bandit problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.01704",
      "authors": "Aldo Pacchiano, My Phan, Yasin Abbasi-Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari"
    },
    {
      "index": 33,
      "title": "Gaussian processes for machine learning, volume 1",
      "abstract": "",
      "year": "2006",
      "venue": "MIT press Cambridge",
      "authors": "Carl Edward Rasmussen and Christopher KI Williams"
    },
    {
      "index": 34,
      "title": "Tight Regret Bounds for Bayesian Optimization in One Dimension",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Jonathan Scarlett",
      "orig_title": "Tight regret bounds for Bayesian optimization in one dimension",
      "paper_id": "1805.11792v3"
    },
    {
      "index": 35,
      "title": "Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Learning Theory (COLT)",
      "authors": "Jonathan Scarlett, Ilijia Bogunovic, and Volkan Cevher",
      "orig_title": "Lower bounds on regret for noisy Gaussian process bandit optimization",
      "paper_id": "1706.00090v3"
    },
    {
      "index": 36,
      "title": "Quantifying mismatch in Bayesian optimization",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS workshop on Bayesian optimization: Black-box optimization and beyond",
      "authors": "Eric Schulz, Maarten Speekenbrink, José Miguel Hernández-Lobato, Zoubin Ghahramani, and Samuel J Gershman"
    },
    {
      "index": 37,
      "title": "Mixed strategies for robust optimization of unknown objectives",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Artificial Intelligence and Statistics (AISTATS)",
      "authors": "Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, and Andreas Krause"
    },
    {
      "index": 38,
      "title": "Gaussian Process Bandits with Adaptive Discretization",
      "abstract": "",
      "year": "2018",
      "venue": "Electronic Journal of Statistics",
      "authors": "Shubhanshu Shekhar and Tara Javidi",
      "orig_title": "Gaussian process bandits with adaptive discretization",
      "paper_id": "1712.01447v2"
    },
    {
      "index": 39,
      "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger"
    },
    {
      "index": 40,
      "title": "Safe exploration for optimization with Gaussian processes",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause"
    },
    {
      "index": 41,
      "title": "From ads to interventions: Contextual bandits in mobile health",
      "abstract": "",
      "year": "2017",
      "venue": "Mobile Health",
      "authors": "Ambuj Tewari and Susan A Murphy"
    },
    {
      "index": 42,
      "title": "On information gain and regret bounds in Gaussian process bandits",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.06966",
      "authors": "Sattar Vakili, Kia Khezeli, and Victor Picheny"
    },
    {
      "index": 43,
      "title": "Finite-time analysis of kernelised contextual bandits",
      "abstract": "",
      "year": "2013",
      "venue": "Uncertainty In Artificial Intelligence (UAI)",
      "authors": "Michal Valko, Nathaniel Korda, Rémi Munos, Ilias Flaounas, and Nelo Cristianini"
    },
    {
      "index": 44,
      "title": "Comments on the du-kakade-wang-yang lower bounds",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.07910",
      "authors": "Benjamin Van Roy and Shi Dong"
    },
    {
      "index": 45,
      "title": "Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1406.7758",
      "authors": "Ziyu Wang and Nando de Freitas",
      "orig_title": "Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters",
      "paper_id": "1406.7758v1"
    },
    {
      "index": 46,
      "title": "Convergence Guarantees for Gaussian Process Means With Misspecified Likelihoods and Smoothness",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "George Wynne, Francois-Xavier Briol, and Mark Girolami",
      "orig_title": "Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness",
      "paper_id": "2001.10818v3"
    },
    {
      "index": 47,
      "title": "Learning Near Optimal Policies with Low Inherent Bellman Error",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.00153",
      "authors": "Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill",
      "orig_title": "Learning near optimal policies with low inherent Bellman error",
      "paper_id": "2003.00153v3"
    }
  ]
}