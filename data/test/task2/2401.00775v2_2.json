{
  "paper_id": "2401.00775v2",
  "title": "Recent Advances in Text Analysis",
  "sections": {
    "commonly used neural network architectures": "Some well-known network architectures for NLP include the convolutional neural networks (CNNs), recursive neural networks (RNNs), and transformers. CNNs and RNNs are more traditional,\nand transformers have become very popular in recent years. CNNs use structural layers (e.g., convolutional layers and pooling layers) to capture the spacial patterns in the input,\nand are extensively used in signal (speech, image, video) processing.\nIn processing a text document, sometimes it is not important whether certain words appear, but rather whether or not they appear in particular localities. Hence,\nCNNs are also useful for NLP tasks such as sentence modeling [ref]24 and sentiment analysis . RNNs are especially useful for sequence data with variable-lengths, making them suitable for text analysis. The long short-term memory (LSTM) network \nis the most popular variant of RNNs.\nIn the vanilla RNNs, information may be diluted with successive iterations, preventing the model to “remember” important information from the distant past. LSTMs add neurons (called “gates”) to retain, forget, or expose specific information, so it can better capture the dependence between two far-apart words in the sequence. The standard LSTMs are unidirectional (i.e., text is processed left-to-right). It is preferred to process text bidirectionally, as a word may depend on the words behind it. The bidirectional LSTMs combine outputs from left-to-right layers and right-to-left layers. The transformers  are\na type of architectures based on the attention mechanism [ref]3.\nIn a traditional encoder-decoder pair, the encoder maps the input sequence into a fixed-length vector, and the decoder has access to this vector only. The attention mechanism allows the encoder to pass all the hidden states (not just the final encoded vector) to the decoder, along with annotation vectors and attention weights to tell the decoder which part of information to “pay attention to”.\nThe attention mechanism was shown to be much more effective than RNNs in processing long documents.\n proposed a special architecture called transformer that uses self-attention within each of the encoder and decoder and cross-attention between them. The transformer\nhas become the most popular architecture in NLP. For example, the encoder part of the transformer is the building block of models like BERT (see below), and the decoder part of the transformer is the building block of models like GPT  for text generation."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "A practical algorithm for topic modeling with provable guarantees",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on machine learning",
      "authors": "Arora, S., R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu"
    },
    {
      "index": 1,
      "title": "Learning topic models–going beyond SVD",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE 53rd Annual Symposium on Foundations of Computer Science",
      "authors": "Arora, S., R. Ge, and A. Moitra"
    },
    {
      "index": 2,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.0473",
      "authors": "Bahdanau, D., K. Cho, and Y. Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 3,
      "title": "A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics.",
      "abstract": "",
      "year": "2020",
      "venue": "Bernoulli",
      "authors": "Bing, X., F. Bunea, and M. Wegkamp",
      "orig_title": "A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics",
      "paper_id": "1805.06837v3"
    },
    {
      "index": 4,
      "title": "Latent dirichlet allocation",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of Machine Learning Research",
      "authors": "Blei, D. M., A. Y. Ng, and M. I. Jordan"
    },
    {
      "index": 5,
      "title": "Testing high-dimensional multinomials with applications to text analysis",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.01381",
      "authors": "Cai, T. T., Z. T. Ke, and P. Turner"
    },
    {
      "index": 6,
      "title": "Indexing by latent semantic analysis",
      "abstract": "",
      "year": "1990",
      "venue": "Journal of the American Society for Information Science",
      "authors": "Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman"
    },
    {
      "index": 7,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 8,
      "title": "50 years of data science",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Computational and Graphical Statistics",
      "authors": "Donoho, D."
    },
    {
      "index": 9,
      "title": "Higher criticism for large-scale inference, especially for rare and weak effects",
      "abstract": "",
      "year": "2015",
      "venue": "Statistical science",
      "authors": "Donoho, D. and J. Jin"
    },
    {
      "index": 10,
      "title": "When does non-negative matrix factorization give a correct decomposition into parts?",
      "abstract": "",
      "year": "2003",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Donoho, D. and V. Stodden"
    },
    {
      "index": 11,
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th international conference on computational linguistics: technical papers",
      "authors": "Dos Santos, C. and M. Gatti"
    },
    {
      "index": 12,
      "title": "Experiments in automatic phrase indexing for document retrieval: A comparison of syntactic and nonsyntactic methods",
      "abstract": "",
      "year": "1988",
      "venue": "Cornell University",
      "authors": "Fagan, J. L."
    },
    {
      "index": 13,
      "title": "Fast and robust recursive algorithmsfor separable nonnegative matrix factorization",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Gillis, N. and S. A. Vavasis"
    },
    {
      "index": 14,
      "title": "The first text retrieval conference (TREC-1), Volume 500",
      "abstract": "",
      "year": "1993",
      "venue": "US Department of Commerce, National Institute of Standards and Technology",
      "authors": "Harman, D. K."
    },
    {
      "index": 15,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Hochreiter, S. and J. Schmidhuber"
    },
    {
      "index": 16,
      "title": "Probabilistic latent semantic indexing",
      "abstract": "",
      "year": "1999",
      "venue": "22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": "Hofmann, T."
    },
    {
      "index": 17,
      "title": "Matrix Analysis (2nd ed.)",
      "abstract": "",
      "year": "2013",
      "venue": "Cambridge University Press",
      "authors": "Horn, R. A. and C. R. Johnson"
    },
    {
      "index": 18,
      "title": "Co-citation and Co-authorship Networks of Statisticians",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of Business & Economic Statistics",
      "authors": "Ji, P., J. Jin, Z. T. Ke, and W. Li",
      "orig_title": "Co-citation and co-authorship networks of statisticians",
      "paper_id": "2204.11194v1"
    },
    {
      "index": 19,
      "title": "Fast community detection by SCORE",
      "abstract": "",
      "year": "2015",
      "venue": "The Annals of Statistics",
      "authors": "Jin, J."
    },
    {
      "index": 20,
      "title": "Network global testing by counting graphlets",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Jin, J., Z. Ke, and S. Luo"
    },
    {
      "index": 21,
      "title": "Optimal adaptivity of signed-polygon statistics for network testing",
      "abstract": "",
      "year": "2021",
      "venue": "The Annals of Statistics",
      "authors": "Jin, J., Z. T. Ke, and S. Luo"
    },
    {
      "index": 22,
      "title": "Mixed membership estimation for social networks",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Econometrics",
      "authors": "Jin, J., Z. T. Ke, and S. Luo"
    },
    {
      "index": 23,
      "title": "A convolutional neural network for modelling sentences",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1404.2188",
      "authors": "Kalchbrenner, N., E. Grefenstette, and P. Blunsom"
    },
    {
      "index": 24,
      "title": "Defining and Identifying Sleeping Beauties in Science",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the National Academy of Sciences",
      "authors": "Ke, Q., E. Ferrara, F. Radicchi, and A. Flammini",
      "orig_title": "Defining and identifying sleeping beauties in science",
      "paper_id": "1505.06454v1"
    },
    {
      "index": 25,
      "title": "Special invited paper: The SCORE normalization, especially for heterogeneous network and text data",
      "abstract": "",
      "year": "2023",
      "venue": "Stat",
      "authors": "Ke, Z. T. and J. Jin"
    },
    {
      "index": 26,
      "title": "Predicting returns with text data",
      "abstract": "",
      "year": "2019",
      "venue": "National Bureau of Economic Research",
      "authors": "Ke, Z. T., B. T. Kelly, and D. Xiu"
    },
    {
      "index": 27,
      "title": "Using SVD for topic modeling",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of the American Statistical Association",
      "authors": "Ke, Z. T. and M. Wang"
    },
    {
      "index": 28,
      "title": "Assigning topics to documents by successive projections",
      "abstract": "",
      "year": "2023",
      "venue": "The Annals of Statistics",
      "authors": "Klopp, O., M. Panov, S. Sigalla, and A. B. Tsybakov"
    },
    {
      "index": 29,
      "title": "Discussion of “Coauthorship and citation networks for statisticians”",
      "abstract": "",
      "year": "2016",
      "venue": "Annals of Applied Statistics",
      "authors": "Kolar, M. and M. Taddy"
    },
    {
      "index": 30,
      "title": "Learning the parts of objects by non-negative matrix factorization",
      "abstract": "",
      "year": "1999",
      "venue": "Nature",
      "authors": "Lee, D. D. and H. S. Seung"
    },
    {
      "index": 31,
      "title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining",
      "abstract": "",
      "year": "2020",
      "venue": "Bioinformatics",
      "authors": "Lee, J., W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang"
    },
    {
      "index": 32,
      "title": "Supervised topic models",
      "abstract": "",
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Mcauliffe, J. and D. Blei"
    },
    {
      "index": 33,
      "title": "A note on EM algorithm for probabilistic latent semantic analysis",
      "abstract": "",
      "year": "2001",
      "venue": "International Conference on Information and Knowledge Management, CIKM",
      "authors": "Mei, Q. and C. Zhai"
    },
    {
      "index": 34,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1301.3781",
      "authors": "Mikolov, T., K. Chen, G. Corrado, and J. Dean"
    },
    {
      "index": 35,
      "title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Otter, D. W., J. R. Medina, and J. K. Kalita",
      "orig_title": "A survey of the usages of deep learning for natural language processing",
      "paper_id": "1807.10854v3"
    },
    {
      "index": 36,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Radford, A., K. Narasimhan, T. Salimans, I. Sutskever, et al."
    },
    {
      "index": 37,
      "title": "End-to-end transformer-based models in textual-based NLP",
      "abstract": "",
      "year": "2023",
      "venue": "AI",
      "authors": "Rahali, A. and M. A. Akhloufi"
    },
    {
      "index": 38,
      "title": "Weaving the fabric of science: Dynamic network models of science’s unfolding structure",
      "abstract": "",
      "year": "2015",
      "venue": "Social Networks",
      "authors": "Shi, F., J. G. Foster, and J. A. Evans"
    },
    {
      "index": 39,
      "title": "Citation patterns in the journals of statistics and probability",
      "abstract": "",
      "year": "1994",
      "venue": "Statistical Science",
      "authors": "Stigler, S. M."
    },
    {
      "index": 40,
      "title": "On estimation and selection for topic models",
      "abstract": "",
      "year": "2012",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "Taddy, M."
    },
    {
      "index": 41,
      "title": "Statistical modeling of citation exchange between statistics journals (with discussions)",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of the Royal Statical Society: Series A",
      "authors": "Varin, C., M. Cattelan, and D. Firth"
    },
    {
      "index": 42,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 43,
      "title": "Topic modeling: beyond bag-of-words",
      "abstract": "",
      "year": "2006",
      "venue": "23rd international conference on Machine learning",
      "authors": "Wallach, H. M."
    },
    {
      "index": 44,
      "title": "Sparse topic modeling: Computational efficiency, near-optimal algorithms, and statistical inference",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of the American Statistical Association",
      "authors": "Wu, R., L. Zhang, and T. Tony Cai"
    },
    {
      "index": 45,
      "title": "A heuristic approach to determine an appropriate number of topics in topic modeling",
      "abstract": "",
      "year": "2015",
      "venue": "BMC bioinformatics",
      "authors": "Zhao, W., J. J. Chen, R. Perkins, Z. Liu, W. Ge, Y. Ding, and W. Zou"
    },
    {
      "index": 46,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler"
    }
  ]
}