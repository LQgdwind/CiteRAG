{
  "paper_id": "2008.01059v1",
  "title": "Improving One-stage Visual Grounding by Recursive Sub-query Construction",
  "sections": {
    "quantitative results": "Experiment settings.\nTable¬†1 reports visual grounding results on RefCOCO, RefCOCO+, RefCOCOg (the upper table), and ReferItGame, Flickr30K Entities (the lower table). The top part of each table contain results of the state-of-the-art two-stage visual grounding methods¬†[ref]29     [ref]27      . The ‚ÄúFeature‚Äù column lists the backbone and pretrained dataset used for proposal feature extraction. COCO-trained Faster-RCNN¬† detector is used for region proposal generation for the experiments on RefCOCO¬†, RefCOCO+¬† and RefCOCOg¬†[ref]29. We quote the two-stage methods‚Äô results on ReferItGame and Flickr30K Entities reported by SSG¬† and One-Stage-BERT¬†[ref]47 where Edgebox¬†3 is used for proposal generation. The bottom part of Table¬†1 compares the performance of our method to other state-of-the-art one-stage methods¬†  [ref]47. The ‚ÄúFeature‚Äù column shows the adopted visual backbone and its pretrained dataset, if any. For a fair comparison, we modify One-Stage-BERT¬†[ref]47 to have the exact same training details as ours, and observe a small accuracy improvement by the modification. Specifically, we 1). encode the query as the averaged BERT word embedding instead of the BERT sentence embedding at the first token‚Äôs position ([CLS]delimited-[]CLS\\left[\\textrm{CLS}\\right]), 2). remove the feature pyramid network, and 3). follow the implementation details in Section¬†4.2. We refer to the modified version ‚ÄúOne-Stage-BERT*.‚Äù Other than the state-of-the-art, we design and compare to additional alternatives to our methods such as ‚Äúsingle/ multi-head attention query modeling,‚Äù ‚Äúper-word sub-query,‚Äù etc., in Section¬†4.5 and Table¬†3. We obtain our main results by the method described in Section¬†3 and refer to it as ‚ÄúOurs-Base‚Äù in Table¬†1. Furthermore, we observe that a larger input image size of 512512512 and a ConvLSTM¬†  grounding module increase the accuracy, but meanwhile slightly slow the inference speed. We refer to the corresponding model as ‚ÄúOurs-Large‚Äù and analyze each modification in supplementary materials. Visual grounding results.\nOur proposed method outperforms the state-of-the-art one-stage grounding methods¬†[ref]47   by over 5%percent55\\% absolute accuracy on all experimented datasets. The two-stage methods¬† [ref]27     also show good performance on COCO-series datasets (i.e., RefCOCO, RefCOCO+, and RefCOCOg) by using the COCO-trained detector¬†. For example, we notice that MAttNet¬† achieves comparable performance with our best model in RefCOCO+, though our best model obviously surpasses MAttNet on RefCOCO, RefCOCOg, and the testB of RefCOCO+. However, in the ReferItGame dataset, as listed in the lower part of Table¬†1, MAttNet‚Äôs accuracy drops dramatically. The findings of previous one-stage work ¬†[ref]47  show that two-stage visual grounding methods rely highly on the region proposals quality.\nSince RefCOCO/ RefCOCO+/ RefCOCOg are subsets of COCO and have shared images and objects, the COCO-trained detector generates nearly perfect region proposals on COCO-series datasets. When used in other datasets, e.g., ReferItGame and Flickr30K Entities datasets, their proposal quality and grounding accuracy drop, such as the MAttNet‚Äôs degraded performance in ReferItGame. Nonetheless, our method performs stably across all datasets and, meanwhile being significantly faster. Inference time.\nThe real-time inference speed is one major advantage of the one-stage visual grounding method. We conduct all the experiments on a single NVIDIA 1080TI GPU. We observe our method achieves a real-time inference speed of 26‚Äãm‚Äãs26ùëöùë†26ms. The method is more than 101010 times faster than typical two-stage methods such as the MattNet¬† of 320‚Äãm‚Äãs320ùëöùë†320ms."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "G3raphground: Graph-based language grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Bajaj, M., Wang, L., Sigal, L."
    },
    {
      "index": 1,
      "title": "A fast and accurate dependency parser using neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Chen, D., Manning, C.D."
    },
    {
      "index": 2,
      "title": "Msrc: Multimodal spatial regression with semantic context for phrase grounding",
      "abstract": "",
      "year": "2017",
      "venue": "2017 ACM on International Conference on Multimedia Retrieval",
      "authors": "Chen, K., Kovvuri, R., Gao, J., Nevatia, R."
    },
    {
      "index": 3,
      "title": "Query-guided Regression Network with Context Policy for Phrase Grounding",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Chen, K., Kovvuri, R., Nevatia, R.",
      "orig_title": "Query-guided regression network with context policy for phrase grounding",
      "paper_id": "1708.01676v1"
    },
    {
      "index": 4,
      "title": "Real-time referring expression comprehension by single-stage grounding network",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.03426",
      "authors": "Chen, X., Ma, L., Chen, J., Jie, Z., Liu, W., Luo, J."
    },
    {
      "index": 5,
      "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S."
    },
    {
      "index": 6,
      "title": "Modulating early visual processing by language",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "De¬†Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., Courville, A.C.",
      "orig_title": "Modulating early visual processing by language",
      "paper_id": "1707.00683v3"
    },
    {
      "index": 7,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 8,
      "title": "Neural sequential phrase grounding (seqground)",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Dogan, P., Sigal, L., Gross, M."
    },
    {
      "index": 9,
      "title": "A Learned Representation for Artistic Style",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Dumoulin, V., Shlens, J., Kudlur, M.",
      "orig_title": "A learned representation for artistic style",
      "paper_id": "1610.07629v5"
    },
    {
      "index": 10,
      "title": "The segmented and annotated iapr tc-12 benchmark",
      "abstract": "",
      "year": "2010",
      "venue": "CVIU",
      "authors": "Escalante, H.J., Hern√°ndez, C.A., Gonzalez, J.A., L√≥pez-L√≥pez, A., Montes, M., Morales, E.F., Sucar, L.E., Villase√±or, L., Grubinger, M."
    },
    {
      "index": 11,
      "title": "LSTM: A Search Space Odyssey",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "Greff, K., Srivastava, R.K., Koutn√≠k, J., Steunebrink, B.R., Schmidhuber, J.",
      "orig_title": "Lstm: A search space odyssey",
      "paper_id": "1503.04069v2"
    },
    {
      "index": 12,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "He, K., Gkioxari, G., Doll√°r, P., Girshick, R.",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 13,
      "title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Hu, R., Rohrbach, M., Andreas, J., Darrell, T., Saenko, K.",
      "orig_title": "Modeling relationships in referential expressions with compositional modular networks",
      "paper_id": "1611.09978v1"
    },
    {
      "index": 14,
      "title": "Natural language object retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T."
    },
    {
      "index": 15,
      "title": "Compositional Attention Networks for Machine Reasoning",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Hudson, D.A., Manning, C.D.",
      "orig_title": "Compositional attention networks for machine reasoning",
      "paper_id": "1803.03067v2"
    },
    {
      "index": 16,
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T."
    },
    {
      "index": 17,
      "title": "Deep attribute-preserving metric learning for natural language object retrieval",
      "abstract": "",
      "year": "2017",
      "venue": "25th ACM international conference on Multimedia",
      "authors": "Li, J., Wei, Y., Liang, X., Zhao, F., Li, J., Xu, T., Feng, J."
    },
    {
      "index": 18,
      "title": "Person Search with Natural Language Description",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Li, S., Xiao, T., Li, H., Zhou, B., Yue, D., Wang, X.",
      "orig_title": "Person search with natural language description",
      "paper_id": "1702.05729v2"
    },
    {
      "index": 19,
      "title": "A real-time cross-modality correlation filtering method for referring expression comprehension",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Liao, Y., Liu, S., Li, G., Wang, F., Chen, Y., Qian, C., Li, B."
    },
    {
      "index": 20,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll√°r, P., Zitnick, C.L.",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 21,
      "title": "A Structured Self-attentive Sentence Embedding",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Lin, Z., Feng, M., Santos, C.N.d., Yu, M., Xiang, B., Zhou, B., Bengio, Y.",
      "orig_title": "A structured self-attentive sentence embedding",
      "paper_id": "1703.03130v1"
    },
    {
      "index": 22,
      "title": "Recurrent Multimodal Interaction for Referring Image Segmentation",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.",
      "orig_title": "Recurrent multimodal interaction for referring image segmentation",
      "paper_id": "1703.07939v2"
    },
    {
      "index": 23,
      "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Liu, D., Zhang, H., Wu, F., Zha, Z.J.",
      "orig_title": "Learning to assemble neural module tree networks for visual grounding",
      "paper_id": "1812.03299v3"
    },
    {
      "index": 24,
      "title": "Referring expression generation and comprehension via attributes",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Liu, J., Wang, L., Yang, M.H."
    },
    {
      "index": 25,
      "title": "SSD: Single Shot MultiBox Detector",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.",
      "orig_title": "Ssd: Single shot multibox detector",
      "paper_id": "1512.02325v5"
    },
    {
      "index": 26,
      "title": "Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Liu, X., Wang, Z., Shao, J., Wang, X., Li, H.",
      "orig_title": "Improving referring expression grounding with cross-modal attention-guided erasing",
      "paper_id": "1903.00839v2"
    },
    {
      "index": 27,
      "title": "Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Luo, G., Zhou, Y., Sun, X., Cao, L., Wu, C., Deng, C., Ji, R.",
      "orig_title": "Multi-task collaborative network for joint referring expression comprehension and segmentation",
      "paper_id": "2003.08813v1"
    },
    {
      "index": 28,
      "title": "Generation and comprehension of unambiguous object descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K."
    },
    {
      "index": 29,
      "title": "Modeling Context Between Objects for Referring Expression Understanding",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Nagaraja, V.K., Morariu, V.I., Davis, L.S.",
      "orig_title": "Modeling context between objects for referring expression understanding",
      "paper_id": "1608.00525v1"
    },
    {
      "index": 30,
      "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Perez, E., Strub, F., De¬†Vries, H., Dumoulin, V., Courville, A.",
      "orig_title": "Film: Visual reasoning with a general conditioning layer",
      "paper_id": "1709.07871v2"
    },
    {
      "index": 31,
      "title": "Conditional Image-Text Embedding Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Plummer, B.A., Kordas, P., Kiapour, M.H., Zheng, S., Piramuthu, R., Lazebnik, S.",
      "orig_title": "Conditional image-text embedding networks",
      "paper_id": "1711.08389v4"
    },
    {
      "index": 32,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2017",
      "venue": "International journal of computer vision",
      "authors": "Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 33,
      "title": "Yolov3: An incremental improvement",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.02767",
      "authors": "Redmon, J., Farhadi, A."
    },
    {
      "index": 34,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Ren, S., He, K., Girshick, R., Sun, J.",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 35,
      "title": "Grounding of textual phrases in images by reconstruction",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B."
    },
    {
      "index": 36,
      "title": "Zero-Shot Grounding of Objects from Natural Language Queries",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Sadhu, A., Chen, K., Nevatia, R.",
      "orig_title": "Zero-shot grounding of objects from natural language queries",
      "paper_id": "1908.07129v1"
    },
    {
      "index": 37,
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "abstract": "",
      "year": "2012",
      "venue": "COURSERA: Neural networks for machine learning",
      "authors": "Tieleman, T., Hinton, G."
    },
    {
      "index": 38,
      "title": "Selective search for object recognition",
      "abstract": "",
      "year": "2013",
      "venue": "International journal of computer vision",
      "authors": "Uijlings, J.R., Van De¬†Sande, K.E., Gevers, T., Smeulders, A.W."
    },
    {
      "index": 39,
      "title": "Learning two-branch neural networks for image-text matching tasks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Wang, L., Li, Y., Huang, J., Lazebnik, S."
    },
    {
      "index": 40,
      "title": "Learning deep structure-preserving image-text embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Wang, L., Li, Y., Lazebnik, S."
    },
    {
      "index": 41,
      "title": "Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Wang, P., Wu, Q., Cao, J., Shen, C., Gao, L., Hengel, A.v.d.",
      "orig_title": "Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks",
      "paper_id": "1812.04794v1"
    },
    {
      "index": 42,
      "title": "Huggingface‚Äôs transformers: State-of-the-art natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.03771",
      "authors": "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Brew, J."
    },
    {
      "index": 43,
      "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.",
      "orig_title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "paper_id": "1506.04214v2"
    },
    {
      "index": 44,
      "title": "Cross-modal relationship inference for grounding referring expressions",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Yang, S., Li, G., Yu, Y."
    },
    {
      "index": 45,
      "title": "Dynamic Graph Attention for Referring Expression Comprehension",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Yang, S., Li, G., Yu, Y.",
      "orig_title": "Dynamic graph attention for referring expression comprehension",
      "paper_id": "1909.08164v1"
    },
    {
      "index": 46,
      "title": "A Fast and Accurate One-Stage Approach to Visual Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.",
      "orig_title": "A fast and accurate one-stage approach to visual grounding",
      "paper_id": "1908.06354v1"
    },
    {
      "index": 47,
      "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.",
      "orig_title": "Mattnet: Modular attention network for referring expression comprehension",
      "paper_id": "1801.08186v3"
    },
    {
      "index": 48,
      "title": "Modeling context in referring expressions",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L."
    },
    {
      "index": 49,
      "title": "A joint speaker-listener-reinforcer model for referring expressions",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Yu, L., Tan, H., Bansal, M., Berg, T.L."
    },
    {
      "index": 50,
      "title": "Grounding Referring Expressions in Images by Variational Context",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhang, H., Niu, Y., Chang, S.F.",
      "orig_title": "Grounding referring expressions in images by variational context",
      "paper_id": "1712.01892v2"
    },
    {
      "index": 51,
      "title": "Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhuang, B., Wu, Q., Shen, C., Reid, I., van¬†den Hengel, A.",
      "orig_title": "Parallel attention: A unified framework for visual object discovery through dialogs and queries",
      "paper_id": "1711.06370v1"
    },
    {
      "index": 52,
      "title": "Edge boxes: Locating object proposals from edges",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Zitnick, C.L., Doll√°r, P."
    }
  ]
}