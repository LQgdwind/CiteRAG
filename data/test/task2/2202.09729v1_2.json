{
  "paper_id": "2202.09729v1",
  "title": "It’s Raw! Audio Generation with State-Space Models",
  "sections": {
    "introduction": "Generative modeling of raw audio waveforms is a challenging frontier for machine learning due to their high-dimensionality—waveforms contain tens of thousands of timesteps per second and exhibit long-range behavior at multiple timescales.\nA key problem is developing architectures for modeling waveforms with the following properties: Globally coherent generation, which requires modeling unbounded contexts with long-range dependencies. Computational efficiency through parallel training, and fast autoregressive and non-autoregressive inference. Sample efficiency through a model with inductive biases well suited to high-rate waveform data. Among the many training methods for waveform generation, autoregressive (AR) modeling is a fundamentally important approach.\nAR models learn the distribution of future variables conditioned on past observations, and are central to\nrecent advances in machine learning for\nlanguage and image generation  5 [ref]2.\nWith AR models, computing the exact likelihood is tractable, which makes them simple to train, and lends them to applications such as lossless compression  and posterior sampling .\nWhen generating, they can condition on arbitrary amounts of past context to sample sequences of unbounded length—potentially even longer than contexts observed during training.\nMoreover, architectural developments in AR waveform modeling can have a cascading effect on audio generation more broadly.\nFor example, WaveNet—the earliest such architecture 9—remains a central component of\nstate-of-the-art approaches for text-to-speech (TTS) ,\nunconditional generation ,\nand non-autoregressive (non-AR) generation . Despite notable progress in AR modeling of (relatively) short sequences found in domains such as natural language (e.g. 111K tokens),\nit is still an open challenge to develop architectures that are effective for the much longer sequence lengths of audio waveforms (e.g. 111M samples).\nPast attempts have tailored standard sequence modeling approaches like CNNs 9, RNNs , and Transformers  to fit the demands of AR waveform modeling,\nbut these approaches have limitations.\nFor example, RNNs lack computational efficiency because they cannot be parallelized during training, while CNNs cannot achieve global coherence because they are fundamentally constrained by the size of their receptive field. We introduce SaShiMi, a new architecture for modeling waveforms that yields state-of-the-art performance on unconditional audio generation benchmarks in both the AR and non-AR settings.\nSaShiMi is designed around recently developed deep state space models (SSM), specifically S4 .\nSSMs have a number of key features that make them ideal for modeling raw audio data. Concretely, S4: Incorporates a principled approach to modeling long range dependencies with strong results on long sequence modeling, including raw audio classification. Can be computed either as a CNN for efficient parallel training, or an RNN for fast autoregressive generation. Is implicitly a continuous-time model, making it well-suited to signals like waveforms. To realize these benefits of SSMs inside SaShiMi, we make 333 technical contributions.\nFirst, we observe that while stable to train, S4’s recurrent representation cannot be used for autoregressive generation due to numerical instability.\nWe identify the source of the instability using classical state space theory,\nwhich states that SSMs are stable when the state matrix is Hurwitz,\nwhich is not enforced by the S4 parameterization.\nWe provide a simple improvement to the S4 parameterization that theoretically ensures stability. Second, SaShiMi incorporates pooling layers between blocks of residual S4 layers to capture hierarchical information across multiple resolutions. This is a common technique in neural network architectures such as standard CNNs and multi-scale RNNs, and provides empirical improvements in both performance and computational efficiency over isotropic stacked S4 layers. Third, while S4 is a causal (unidirectional) model suitable for AR modeling, we provide a simple bidirectional relaxation to flexibly incorporate it in non-AR architectures.\nThis enables it to better take advantage of the available global context in non-AR settings. For AR modeling in audio domains with unbounded sequence lengths (e.g. music),\nSaShiMi can train on much longer contexts than existing methods including WaveNet (sequences of length 128128128K vs 444K),\nwhile simultaneously having better test likelihood, faster training and inference, and fewer parameters.\nSaShiMi outperforms existing AR methods in modeling the data (>0.15absent0.15>0.15 bits better negative log-likelihoods), with substantial improvements (+0.40.4+0.4 points) in the musicality of long generated samples (161616s) as measured by mean opinion scores.\nIn unconditional speech generation,\nSaShiMi achieves superior global coherence compared to previous AR models\non the difficult SC09 dataset both quantitatively (80%percent8080\\% higher inception score) and qualitatively (2×2\\times higher audio quality and digit intelligibility opinion scores by human evaluators). Finally, we validate that SaShiMi is a versatile backbone for non-AR architectures.\nReplacing the WaveNet backbone with SaShiMi in the state-of-the-art diffusion model DiffWave improves its quality, sample efficiency, and robustness to hyperparameters with no additional tuning. The central contribution of this paper is showing that deep neural networks using SSMs are a strong alternative to conventional architectures for modeling audio waveforms, with favorable tradeoffs in training speed, generation speed, sample efficiency, and audio quality. We technically improve the parameterization of S4, ensuring its stability when switching into recurrent mode at generation time. We introduce SaShiMi, an SSM-based architecture with high efficiency and performance for unconditional AR modeling of music and speech waveforms. We show that SaShiMi is easily incorporated into other deep generative models to improve their performance."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "High Fidelity Speech Synthesis with Adversarial Networks",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Mikołaj Bińkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C Cobo, and Karen Simonyan",
      "orig_title": "High fidelity speech synthesis with adversarial networks",
      "paper_id": "1909.11646v2"
    },
    {
      "index": 1,
      "title": "On the opportunities and risks of foundation models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2108.07258",
      "authors": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al."
    },
    {
      "index": 2,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.14165",
      "authors": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 3,
      "title": "Wavegrad: Estimating gradients for waveform generation",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan"
    },
    {
      "index": 4,
      "title": "Generating Long Sequences with Sparse Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.10509",
      "authors": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever",
      "orig_title": "Generating long sequences with sparse transformers",
      "paper_id": "1904.10509v1"
    },
    {
      "index": 5,
      "title": "Language Modeling with Gated Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier",
      "orig_title": "Language modeling with gated convolutional networks",
      "paper_id": "1612.08083v3"
    },
    {
      "index": 6,
      "title": "Samplernn",
      "abstract": "",
      "year": "2017",
      "venue": "https://github.com/deepsound-project/samplernn-pytorch",
      "authors": "DeepSound"
    },
    {
      "index": 7,
      "title": "Jukebox: A Generative Model for Music",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00341",
      "authors": "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever",
      "orig_title": "Jukebox: A generative model for music",
      "paper_id": "2005.00341v1"
    },
    {
      "index": 8,
      "title": "The challenge of realistic music generation: modelling raw audio at scale",
      "abstract": "",
      "year": "2018",
      "venue": "32nd International Conference on Neural Information Processing Systems",
      "authors": "Sander Dieleman, Aäron van den Oord, and Karen Simonyan",
      "orig_title": "The challenge of realistic music generation: modelling raw audio at scale",
      "paper_id": "1806.10474v1"
    },
    {
      "index": 9,
      "title": "Adversarial Audio Synthesis",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Chris Donahue, Julian McAuley, and Miller Puckette",
      "orig_title": "Adversarial audio synthesis",
      "paper_id": "1802.04208v3"
    },
    {
      "index": 10,
      "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan",
      "orig_title": "Neural audio synthesis of musical notes with wavenet autoencoders",
      "paper_id": "1704.01279v1"
    },
    {
      "index": 11,
      "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré",
      "orig_title": "Hippo: Recurrent memory with optimal polynomial projections",
      "paper_id": "2008.07669v2"
    },
    {
      "index": 12,
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Albert Gu, Karan Goel, and Christopher Ré",
      "orig_title": "Efficiently modeling long sequences with structured state spaces",
      "paper_id": "2111.00396v3"
    },
    {
      "index": 13,
      "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu",
      "orig_title": "Deligan: Generative adversarial networks for diverse and limited data",
      "paper_id": "1706.02071v1"
    },
    {
      "index": 14,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.08415",
      "authors": "Dan Hendrycks and Kevin Gimpel",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 15,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter",
      "orig_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 16,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber"
    },
    {
      "index": 17,
      "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al."
    },
    {
      "index": 18,
      "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics",
      "abstract": "",
      "year": "2021",
      "venue": "The International Conference on Machine Learning (ICML)",
      "authors": "Vivek Jayaram and John Thickstun",
      "orig_title": "Parallel and flexible sampling from autoregressive models via langevin dynamics",
      "paper_id": "2105.08164v2"
    },
    {
      "index": 19,
      "title": "Efficient Neural Audio Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aäron van den Oord, Sander Dieleman, and Koray Kavukcuoglu",
      "orig_title": "Efficient neural audio synthesis",
      "paper_id": "1802.08435v2"
    },
    {
      "index": 20,
      "title": "FloWaveNet : A Generative Flow for Raw Audio",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon",
      "orig_title": "Flowavenet: A generative flow for raw audio",
      "paper_id": "1811.02155v3"
    },
    {
      "index": 21,
      "title": "WAVENET BASED LOW RATE SPEECH CODING",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
      "authors": "W Bastiaan Kleijn, Felicia SC Lim, Alejandro Luebs, Jan Skoglund, Florian Stimberg, Quan Wang, and Thomas C Walters",
      "orig_title": "Wavenet based low rate speech coding",
      "paper_id": "1712.01120v1"
    },
    {
      "index": 22,
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro"
    },
    {
      "index": 23,
      "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brébisson, Yoshua Bengio, and Aaron C Courville",
      "orig_title": "Melgan: Generative adversarial networks for conditional waveform synthesis",
      "paper_id": "1910.06711v3"
    },
    {
      "index": 24,
      "title": "Generative spoken language modeling from raw audio",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.01192",
      "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, et al."
    },
    {
      "index": 25,
      "title": "Neural Speech Synthesis with Transformer Network",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu",
      "orig_title": "Neural speech synthesis with transformer network",
      "paper_id": "1809.08895v3"
    },
    {
      "index": 26,
      "title": "A ConvNet for the 2020s",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.03545",
      "authors": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie",
      "orig_title": "A convnet for the 2020s",
      "paper_id": "2201.03545v2"
    },
    {
      "index": 27,
      "title": "Samplernn: An unconditional end-to-end neural audio generation model",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio"
    },
    {
      "index": 28,
      "title": "Expediting TTS Synthesis with Adversarial Vocoding",
      "abstract": "",
      "year": "2019",
      "venue": "INTERSPEECH",
      "authors": "Paarth Neekhara, Chris Donahue, Miller Puckette, Shlomo Dubnov, and Julian McAuley",
      "orig_title": "Expediting tts synthesis with adversarial vocoding",
      "paper_id": "1904.07944v2"
    },
    {
      "index": 29,
      "title": "On the difficulty of training recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on machine learning",
      "authors": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio"
    },
    {
      "index": 30,
      "title": "Non-Autoregressive Neural Text-to-Speech",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Kainan Peng, Wei Ping, Zhao Song, and Kexin Zhao",
      "orig_title": "Non-autoregressive neural text-to-speech",
      "paper_id": "1905.08459v3"
    },
    {
      "index": 31,
      "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Wei Ping, Kainan Peng, and Jitong Chen",
      "orig_title": "Clarinet: Parallel wave generation in end-to-end text-to-speech",
      "paper_id": "1807.07281v3"
    },
    {
      "index": 32,
      "title": "WaveFlow: A Compact Flow-based Model for Raw Audio",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song",
      "orig_title": "Waveflow: A compact flow-based model for raw audio",
      "paper_id": "1912.01219v4"
    },
    {
      "index": 33,
      "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis",
      "abstract": "",
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "Ryan Prenger, Rafael Valle, and Bryan Catanzaro",
      "orig_title": "Waveglow: A flow-based generative network for speech synthesis",
      "paper_id": "1811.00002v1"
    },
    {
      "index": 34,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.12092",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 35,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen",
      "orig_title": "Improved techniques for training gans",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 36,
      "title": "Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma"
    },
    {
      "index": 37,
      "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al."
    },
    {
      "index": 38,
      "title": "WaveNet: A Generative Model for Raw Audio",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.03499",
      "authors": "Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu",
      "orig_title": "Wavenet: A generative model for raw audio",
      "paper_id": "1609.03499v2"
    },
    {
      "index": 39,
      "title": "Neural Discrete Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Aaron Van Den Oord, Oriol Vinyals, et al.",
      "orig_title": "Neural discrete representation learning",
      "paper_id": "1711.00937v2"
    },
    {
      "index": 40,
      "title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Aäron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al.",
      "orig_title": "Parallel wavenet: Fast high-fidelity speech synthesis",
      "paper_id": "1711.10433v1"
    },
    {
      "index": 41,
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv, abs/1804.03209",
      "authors": "Pete Warden"
    },
    {
      "index": 42,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 43,
      "title": "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "abstract": "",
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim",
      "orig_title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "paper_id": "1910.11480v2"
    },
    {
      "index": 44,
      "title": "Activation Maximization Generative Adversarial Nets",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu",
      "orig_title": "Activation maximization generative adversarial nets",
      "paper_id": "1703.02000v9"
    }
  ]
}