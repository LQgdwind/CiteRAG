{
  "paper_id": "2106.12566v2",
  "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding",
  "sections": {
    "baselines.": "In this task, we compare our proposed NPRF-Transformer with several Transformer-based models as well as several other strong baselines, including Image Transformer [ref]30, PRF-Transformer (called Performer in [ref]4); ScoreFlow , VDM [ref]20 and DenseFlow ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al.",
      "orig_title": "Unilmv2: Pseudo-masked language models for unified language model pre-training",
      "paper_id": "2002.12804v1"
    },
    {
      "index": 1,
      "title": "End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko"
    },
    {
      "index": 2,
      "title": "Generating Long Sequences with Sparse Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.10509",
      "authors": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever",
      "orig_title": "Generating long sequences with sparse transformers",
      "paper_id": "1904.10509v1"
    },
    {
      "index": 3,
      "title": "Rethinking attention with performers",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller"
    },
    {
      "index": 4,
      "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.08819",
      "authors": "Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter",
      "orig_title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "paper_id": "1707.08819v3"
    },
    {
      "index": 5,
      "title": "What Does BERT Look At? An Analysis of BERT’s Attention",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning",
      "orig_title": "What does bert look at? an analysis of bert’s attention",
      "paper_id": "1906.04341v1"
    },
    {
      "index": 6,
      "title": "SMYRF: Efficient Attention using Asymmetric Clustering",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis",
      "orig_title": "Smyrf-efficient attention using asymmetric clustering",
      "paper_id": "2010.05315v1"
    },
    {
      "index": 7,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 8,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 9,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 10,
      "title": "Toeplitz and circulant matrices: A review",
      "abstract": "",
      "year": "2006",
      "venue": "now publishers inc",
      "authors": "Robert M Gray"
    },
    {
      "index": 11,
      "title": "Densely connected normalizing flows",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.04627",
      "authors": "Matej Grcić, Ivan Grubišić, and Siniša Šegvić",
      "orig_title": "Densely connected normalizing flows",
      "paper_id": "2106.04627v3"
    },
    {
      "index": 12,
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "Proc. Interspeech",
      "authors": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al.",
      "orig_title": "Conformer: Convolution-augmented transformer for speech recognition",
      "paper_id": "2005.08100v1"
    },
    {
      "index": 13,
      "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen",
      "orig_title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "paper_id": "2006.03654v6"
    },
    {
      "index": 14,
      "title": "Axial Attention In Multidimensional Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans",
      "orig_title": "Axial attention in multidimensional transformers",
      "paper_id": "1912.12180v1"
    },
    {
      "index": 15,
      "title": "Finetuning Pretrained Transformers into RNNs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith",
      "orig_title": "Finetuning pretrained transformers into rnns",
      "paper_id": "2103.13076v2"
    },
    {
      "index": 16,
      "title": "Transformers are rnns: Fast autoregressive transformers with linear attention",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret"
    },
    {
      "index": 17,
      "title": "Rethinking Positional Encoding in Language Pre-training",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Guolin Ke, Di He, and Tie-Yan Liu",
      "orig_title": "Rethinking positional encoding in language pre-training",
      "paper_id": "2006.15595v4"
    },
    {
      "index": 18,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 19,
      "title": "Variational diffusion models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.00630",
      "authors": "Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho"
    },
    {
      "index": 20,
      "title": "Reformer: The Efficient Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya",
      "orig_title": "Reformer: The efficient transformer",
      "paper_id": "2001.04451v2"
    },
    {
      "index": 21,
      "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "authors": "Taku Kudo and John Richardson"
    },
    {
      "index": 22,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 23,
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.14030",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo"
    },
    {
      "index": 24,
      "title": "Fixing weight decay regularization in adam",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Ilya Loshchilov and Frank Hutter"
    },
    {
      "index": 25,
      "title": "Mixed Precision Training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu",
      "orig_title": "Mixed precision training",
      "paper_id": "1710.03740v3"
    },
    {
      "index": 26,
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.11972",
      "authors": "Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al.",
      "orig_title": "Do transformer modifications transfer across implementations and applications?",
      "paper_id": "2102.11972v2"
    },
    {
      "index": 27,
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli"
    },
    {
      "index": 28,
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "Association for Computational Linguistics",
      "authors": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu"
    },
    {
      "index": 29,
      "title": "Image Transformer",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran",
      "orig_title": "Image transformer",
      "paper_id": "1802.05751v3"
    },
    {
      "index": 30,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 31,
      "title": "Random Feature Attention",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong",
      "orig_title": "Random feature attention",
      "paper_id": "2103.02143v2"
    },
    {
      "index": 32,
      "title": "Blockwise Self-Attention for Long Document Understanding",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Empirical Methods in Natural Language Processing: Findings",
      "authors": "Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang",
      "orig_title": "Blockwise self-attention for long document understanding",
      "paper_id": "1911.02972v2"
    },
    {
      "index": 33,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"
    },
    {
      "index": 34,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 35,
      "title": "Neural machine translation of rare words with subword units",
      "abstract": "",
      "year": "2016",
      "venue": "Association for Computational Linguistics",
      "authors": "Rico Sennrich, Barry Haddow, and Alexandra Birch"
    },
    {
      "index": 36,
      "title": "Neural machine translation of rare words with subword units",
      "abstract": "",
      "year": "2016",
      "venue": "ACL",
      "authors": "Rico Sennrich, Barry Haddow, and Alexandra Birch"
    },
    {
      "index": 37,
      "title": "Self-Attention with Relative Position Representations",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.02155",
      "authors": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani",
      "orig_title": "Self-attention with relative position representations",
      "paper_id": "1803.02155v2"
    },
    {
      "index": 38,
      "title": "Maximum Likelihood Training of Score-Based Diffusion Models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv e-prints",
      "authors": "Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon",
      "orig_title": "Maximum likelihood training of score-based diffusion models",
      "paper_id": "2101.09258v4"
    },
    {
      "index": 39,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna"
    },
    {
      "index": 40,
      "title": "Efficient Transformers: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler",
      "orig_title": "Efficient transformers: A survey",
      "paper_id": "2009.06732v3"
    },
    {
      "index": 41,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 42,
      "title": "A Simple Method for Commonsense Reasoning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.02847",
      "authors": "Trieu H Trinh and Quoc V Le",
      "orig_title": "A simple method for commonsense reasoning",
      "paper_id": "1806.02847v2"
    },
    {
      "index": 43,
      "title": "Transformer Dissection: A Unified Understanding of Transformer’s Attention via the Lens of Kernel",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "authors": "Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov",
      "orig_title": "Transformer dissection: An unified understanding for transformer’s attention via the lens of kernel",
      "paper_id": "1908.11775v4"
    },
    {
      "index": 44,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 45,
      "title": "Fast Transformers with Clustered Attention",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Apoorv Vyas, Angelos Katharopoulos, and François Fleuret",
      "orig_title": "Fast transformers with clustered attention",
      "paper_id": "2007.04825v2"
    },
    {
      "index": 46,
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman",
      "orig_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "paper_id": "1804.07461v3"
    },
    {
      "index": 47,
      "title": "Linformer: Self-Attention with Linear Complexity",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma",
      "orig_title": "Linformer: Self-attention with linear complexity",
      "paper_id": "2006.04768v3"
    },
    {
      "index": 48,
      "title": "Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh",
      "orig_title": "Nyströmformer: A nyström-based algorithm for approximating self-attention",
      "paper_id": "2102.03902v3"
    },
    {
      "index": 49,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 50,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler"
    }
  ]
}