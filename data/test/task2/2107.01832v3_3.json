{
  "paper_id": "2107.01832v3",
  "title": "Provable Convergence of Nesterov‚Äôs Accelerated Gradient Method for Over-Parameterized Neural Networks",
  "sections": {
    "convergence analysis": "By recursively using¬†(22), it has Then applying Cauchy-Schwarz inequality on¬†(24), we have In order to prove the convergence of NAG, it needs to separately derive bounds for the two terms on the right-hand side of (25).\nThe first term is the norm of the product between the matrix power ùêåtsuperscriptùêåùë°\\mathbf{M}^{t} and the vector ùê≥0subscriptùê≥0\\mathbf{z}_{0}.\nWe provide its upper bound in the following lemma. Assume ùêá‚àà‚Ñùn√ónùêásuperscript‚Ñùùëõùëõ\\bm{H}\\in\\mathbb{R}^{n\\times n} is a symmetry positive definite matrix.\nLet ùêå=[(1+Œ≤)‚Äã(ùêàn‚àíŒ∑‚Äãùêá)Œ≤‚Äã(‚àíùêàn+Œ∑‚Äãùêá)ùêàn0n]‚àà‚Ñù2‚Äãn√ó2‚Äãnùêåmatrix1ùõΩsubscriptùêàùëõùúÇùêáùõΩsubscriptùêàùëõùúÇùêásubscriptùêàùëõsubscript0ùëõsuperscript‚Ñù2ùëõ2ùëõ\\mathbf{M}=\\begin{bmatrix}(1\\!+\\!\\beta)(\\mathbf{I}_{n}\\!-\\!\\eta\\bm{H})&\\beta(-\\mathbf{I}_{n}\\!+\\!\\eta\\bm{H})\\\\\n\\mathbf{I}_{n}&\\textbf{0}_{n}\\end{bmatrix}\\in\\mathbb{R}^{2n\\times 2n}.\nSuppose a sequence of iterates {ùêØi}subscriptùêØùëñ\\{\\mathbf{v}_{i}\\} satisfy ùêØt=ùêåùêØt‚àí1subscriptùêØùë°subscriptùêåùêØùë°1\\mathbf{v}_{t}=\\mathbf{M}\\mathbf{v}_{t-1} for any t‚â§Tùë°ùëát\\leq T.\nIf Œ≤ùõΩ\\beta and Œ∑ùúÇ\\eta are chosen that satisfy 1>Œ≤‚â•1‚àíŒ∑‚ÄãŒªm‚Äãi‚Äãn‚Äã(ùêá)1+Œ∑‚ÄãŒªm‚Äãi‚Äãn‚Äã(ùêá)1ùõΩ1ùúÇsubscriptùúÜùëöùëñùëõùêá1ùúÇsubscriptùúÜùëöùëñùëõùêá1>\\beta\\geq\\frac{1-\\sqrt{\\eta\\lambda_{min}(\\bm{H})}}{1+\\sqrt{\\eta\\lambda_{min}(\\bm{H})}} and 0<Œ∑‚â§1/Œªm‚Äãa‚Äãx‚Äã(ùêá)0ùúÇ1subscriptùúÜùëöùëéùë•ùêá0<\\eta\\leq 1/\\lambda_{max}(\\bm{H}), then it has the bound at any iteration k‚â§Tùëòùëák\\leq T as where C=2‚ÄãŒ≤‚Äã(1‚àíŒ∑‚ÄãŒªm‚Äãi‚Äãn‚Äã(ùêá))+2min‚Å°{g‚Äã(Œ≤,Œ∑‚ÄãŒªm‚Äãi‚Äãn‚Äã(ùêá)),g‚Äã(Œ≤,Œ∑‚ÄãŒªm‚Äãa‚Äãx‚Äã(ùêá))}ùê∂2ùõΩ1ùúÇsubscriptùúÜùëöùëñùëõùêá2ùëîùõΩùúÇsubscriptùúÜùëöùëñùëõùêáùëîùõΩùúÇsubscriptùúÜùëöùëéùë•ùêáC=\\frac{2\\beta(1-\\eta\\lambda_{min}(\\bm{H}))+2}{\\sqrt{\\min\\{g(\\beta,\\eta\\lambda_{min}(\\bm{H})),g(\\beta,\\eta\\lambda_{max}(\\bm{H}))\\}}} and the function gùëîg is defined as g‚Äã(x,y)=4‚Äãx‚Äã(1‚àíy)‚àí[(1+x)‚Äã(1‚àíy)]2ùëîùë•ùë¶4ùë•1ùë¶superscriptdelimited-[]1ùë•1ùë¶2g(x,y)=4x(1-y)-[(1+x)(1-y)]^{2}. The proof is provided in the B.\nGiven the ranges of the hyperparameters Œ∑ùúÇ\\eta and Œ≤ùõΩ\\beta, it is easy to observe that Œ≤‚Äã(1‚àíŒ∑‚ÄãŒªm‚Äãi‚Äãn‚Äã(H))<1ùõΩ1ùúÇsubscriptùúÜùëöùëñùëõùêª1\\sqrt{\\beta(1-\\eta\\lambda_{min}(H))}<1, which ensures the decline of ‚ÄñùêØk‚ÄñnormsubscriptùêØùëò\\|\\mathbf{v}_{k}\\| during evolution.\nFor further determining the upper bounds for C and the decay rate, we set Œ∑ùúÇ\\eta and Œ≤ùõΩ\\beta with the spectrum of ùëØùëØ\\bm{H}. Assume 0<Œª‚â§Œªm‚Äãi‚Äãn‚Äã(ùêá)‚â§Œªm‚Äãa‚Äãx‚Äã(ùêá)‚â§Œªm‚Äãa‚Äãx0ùúÜsubscriptùúÜùëöùëñùëõùêásubscriptùúÜùëöùëéùë•ùêásubscriptùúÜùëöùëéùë•0<\\lambda\\leq\\lambda_{min}(\\bm{H})\\leq\\lambda_{max}(\\bm{H})\\leq\\lambda_{max}.\nDenote Œ∫=Œªm‚Äãa‚Äãx/ŒªùúÖsubscriptùúÜùëöùëéùë•ùúÜ{\\kappa}=\\lambda_{max}/\\lambda.\nWith Œ∑=1/2‚ÄãŒªm‚Äãa‚ÄãxùúÇ12subscriptùúÜùëöùëéùë•\\eta=1/2\\lambda_{max} and Œ≤=3‚ÄãŒ∫‚àí23‚ÄãŒ∫+2ùõΩ3ùúÖ23ùúÖ2\\beta=\\frac{3\\sqrt{{\\kappa}}-2}{3\\sqrt{{\\kappa}}+2}, it has Furthermore, it should be noted that ùêåùêå\\mathbf{M} in (25) is composed by ùëØ0subscriptùëØ0\\bm{H}_{0}, which depends on the random initialization of ùêñ0subscriptùêñ0\\mathbf{W}_{0}.\nFor an over-parameterized neural network, the eigenvalues of the random matrix ùëØ0subscriptùëØ0\\bm{H}_{0} can be bounded by the spectrum of the deterministic NTK matrix ùëØ¬Ø¬ØùëØ\\bar{\\bm{H}}¬†[ref]23, thereby allowing us to determine the hyperparameters with specific values. (Lemma 13 in [ref]23)\n\nDenote Œª=Œªm‚Äãi‚Äãn‚Äã(ùêá¬Ø)ùúÜsubscriptùúÜùëöùëñùëõ¬Øùêá\\lambda=\\lambda_{min}(\\bar{\\bm{H}}). Set m=Œ©‚Äã(Œª‚àí2‚Äãn2‚Äãlog‚Å°(n/Œ¥))ùëöŒ©superscriptùúÜ2superscriptùëõ2ùëõùõøm=\\Omega(\\lambda^{-2}n^{2}\\log(n/\\delta)).\nAssume ùê∞0r‚àºùí©‚Äã(0,Id)similar-tosuperscriptsubscriptùê∞0ùëüùí©0subscriptùêºùëë\\mathbf{w}_{0}^{r}\\sim\\mathcal{N}(0,I_{d}) for all r‚àà[n]ùëüdelimited-[]ùëõr\\in[n].\nWith probability at least 1‚àíŒ¥1ùõø1-\\delta, it holds that As a result, the condition number of ùêá0subscriptùêá0\\bm{H}_{0} is bounded by Now we turn to analyzing the second term on the right-hand side of¬†(25), which is also composed by the product of the matrix power ùêåisuperscriptùêåùëñ\\mathbf{M}^{i} and a bounded vector.\nThen it only needs to bound the norm of ùùÅùùÅ\\bm{\\mu}.\nUsing the Cauchy-Schwarz inequality, it has ‚ÄñùùÅt‚Äñ‚â§‚Äñœït‚Äñ+‚Äñùùçt‚ÄñnormsubscriptùùÅùë°normsubscriptbold-italic-œïùë°normsubscriptùùçùë°\\|\\bm{\\mu}_{t}\\|\\leq\\|\\bm{\\phi}_{t}\\|+\\|\\bm{\\psi}_{t}\\|. From (21), we observe that the bound of |œït‚Äã[i]|subscriptbold-italic-œïùë°delimited-[]ùëñ|\\bm{\\phi}_{t}[i]| mainly depends on the term |Si‚üÇ|superscriptsubscriptùëÜùëñperpendicular-to|S_{i}^{\\perp}|, which describes how many neurons change their activation patterns on the iùëñi-th instance during training.\nAccording to recent studies¬†[ref]16 [ref]21,\n|Si‚üÇ|superscriptsubscriptùëÜùëñperpendicular-to|S_{i}^{\\perp}| has an upper bound 4‚Äãm‚ÄãR4ùëöùëÖ4mR, which is determined by the distance RùëÖR between ùê∞trsubscriptsuperscriptùê∞ùëüùë°\\mathbf{w}^{r}_{t} and its initialization for any r‚àà[m]ùëüdelimited-[]ùëör\\in[m] and t‚àà[T]ùë°delimited-[]ùëát\\in[T].\nIn¬†D, Lemma¬†5 presents the details.\nWhen RùëÖR is small enough, it has |Si‚üÇ|‚â™mmuch-less-thansuperscriptsubscriptùëÜùëñperpendicular-toùëö|S_{i}^{\\perp}|\\ll m. On the other hand, the bound of ‚Äñùùçt‚Äñnormsubscriptùùçùë°\\|\\bm{\\psi}_{t}\\| is closely related to the distance between ùëØtsubscriptùëØùë°\\bm{H}_{t} and ùëØ0subscriptùëØ0\\bm{H}_{0}.\nPrevious works¬†[ref]16 [ref]21 showed that the upper bound of ‚ÄñùëØt‚àíùëØ0‚ÄñnormsubscriptùëØùë°subscriptùëØ0\\|\\bm{H}_{t}-\\bm{H}_{0}\\| is also determined by the distance RùëÖR,\nwhere Lemma¬†6 in¬†D gives the details. In Theorem 1, we derive R=ùí™‚Äã(1/m)ùëÖùí™1ùëöR=\\mathcal{O}(1/\\sqrt{m}), which helps control the size of ‚Äñœït‚Äñnormsubscriptbold-italic-œïùë°\\|\\bm{\\phi}_{t}\\| and ‚Äñùùçt‚Äñnormsubscriptùùçùë°\\|\\bm{\\psi}_{t}\\| with an appropriate mùëöm.\nThe corresponding bounds for RùëÖR, ‚Äñœï‚Äñnormbold-italic-œï\\|\\bm{\\phi}\\| and ‚Äñùùç‚Äñnormùùç\\|\\bm{\\psi}\\| are given in the proof of Theorem 1.\nFinally, we introduce our main result on the convergence of NAG. Define Œª=3‚ÄãŒªm‚Äãi‚Äãn‚Äã(ùêá¬Ø)4ùúÜ3subscriptùúÜùëöùëñùëõ¬Øùêá4\\lambda=\\frac{3\\lambda_{min}(\\bar{\\bm{H}})}{4}, Œªm‚Äãa‚Äãx=Œªm‚Äãa‚Äãx‚Äã(ùêá¬Ø)+Œª4subscriptùúÜùëöùëéùë•subscriptùúÜùëöùëéùë•¬ØùêáùúÜ4\\lambda_{max}=\\lambda_{max}(\\bar{\\bm{H}})+\\frac{\\lambda}{4} and Œ∫=43‚ÄãŒ∫‚Äã(ùêá¬Ø)+13ùúÖ43ùúÖ¬Øùêá13{\\kappa}=\\frac{4}{3}\\kappa(\\bar{\\bm{H}})+\\frac{1}{3}.\nAssume ùê∞0r‚àºN‚Äã(0,Id)similar-tosuperscriptsubscriptùê∞0ùëüùëÅ0subscriptùêºùëë\\mathbf{w}_{0}^{r}\\sim N(0,I_{d}) and ar‚àºR‚Äãa‚Äãd‚Äãe‚Äãm‚Äãa‚Äãc‚Äãh‚Äãe‚Äãr‚Äã(1/2)similar-tosuperscriptùëéùëüùëÖùëéùëëùëíùëöùëéùëê‚Ñéùëíùëü12a^{r}\\sim Rademacher(1/2) for all r‚àà[m]ùëüdelimited-[]ùëör\\in[m].\nSuppose the number of the nodes in the hidden layer is m=Œ©‚Äã(Œª‚àí2‚Äãn4‚ÄãŒ∫2‚Äãl‚Äão‚Äãg3‚Äã(n/Œ¥))ùëöŒ©superscriptùúÜ2superscriptùëõ4superscriptùúÖ2ùëôùëúsuperscriptùëî3ùëõùõøm=\\Omega(\\lambda^{-2}n^{4}\\kappa^{2}log^{3}(n/\\delta)).\nIf the leaning rate Œ∑=1/(2‚ÄãŒªm‚Äãa‚Äãx)ùúÇ12subscriptùúÜùëöùëéùë•\\eta=1/(2\\lambda_{max}) and the momentum parameter Œ≤=3‚ÄãŒ∫‚àí23‚ÄãŒ∫+2ùõΩ3ùúÖ23ùúÖ2\\beta=\\frac{3\\sqrt{{\\kappa}}-2}{3\\sqrt{{\\kappa}}+2},\nwith probability at least 1‚àíŒ¥1ùõø1-\\delta over the random initialization, the residual error for NAG at any iteration tùë°t satisfies where Œ≥=12‚ÄãŒ∫ùõæ12ùúÖ\\gamma=12\\sqrt{{\\kappa}}. For every r‚àà[m]ùëüdelimited-[]ùëör\\in[m], we have Remark 1. With the initialization ùêñ‚àí1=ùêñ0subscriptùêñ1subscriptùêñ0\\mathbf{W}_{-1}=\\mathbf{W}_{0}, it has ùùÉ‚àí1=ùùÉ0subscriptùùÉ1subscriptùùÉ0\\bm{\\xi}_{-1}=\\bm{\\xi}_{0}.\nThus, according to Theorem 1, the training error ùùÉtsubscriptùùÉùë°\\bm{\\xi}_{t} of NAG converges linearly to zero at a (1‚àí12‚ÄãŒ∫)tsuperscript112ùúÖùë°(1-\\frac{1}{2\\sqrt{\\kappa}})^{t} rate after tùë°t iteration, which indicates NAG is able to achieve the global minimum as GD and HB. Remark 2. As shown in¬†[ref]16, GD converges at a rate (1‚àíŒ∑‚ÄãŒª2)tsuperscript1ùúÇùúÜ2ùë°(1-{\\frac{\\eta\\lambda}{2}})^{t}, but with a small learning rate Œ∑=ùí™‚Äã(Œªn2)ùúÇùí™ùúÜsuperscriptùëõ2\\eta=\\mathcal{O}(\\frac{\\lambda}{n^{2}}).\n further improved the bound of the learning rate to ùí™‚Äã(1‚ÄñùëØ¬Ø‚Äñ)ùí™1norm¬ØùëØ\\mathcal{O}(\\frac{1}{\\|{\\bar{\\bm{H}}}\\|}), where ‚ÄñùëØ¬Ø‚Äñ‚â§nnorm¬ØùëØùëõ\\|{\\bar{\\bm{H}}}\\|\\leq n and provides an ùí™‚Äã(Œª/n)ùí™ùúÜùëõ\\mathcal{O}(\\lambda/n) improvement.\nThis results in a faster convergence rate (1‚àíŒò‚Äã(1/Œ∫))tsuperscript1Œò1ùúÖùë°(1-\\Theta(1/\\kappa))^{t} for GD.\nAs shown in Theorem 1, NAG obtains a smaller convergence rate (1‚àíŒò‚Äã(1/Œ∫))tsuperscript1Œò1ùúÖùë°(1-\\Theta(1/\\sqrt{\\kappa}))^{t}, which validates its acceleration over GD.\nMoreover, compard to the convergence rate of HB as proved in¬†[ref]23,\nour results show that NAG obtains a comparable convergence rate. Remark 3. The initial residual error satisfies ‚ÄñùùÉ0‚Äñ2=ùí™‚Äã(n‚Äãl‚Äão‚Äãg‚Äã(m/Œ¥)‚Äãl‚Äão‚Äãg2‚Äã(n/Œ¥))superscriptnormsubscriptùùÉ02ùí™ùëõùëôùëúùëîùëöùõøùëôùëúsuperscriptùëî2ùëõùõø\\|\\bm{\\xi}_{0}\\|^{2}=\\mathcal{O}(nlog(m/\\delta)log^{2}(n/\\delta)) as shown in Lemma¬†7.\nTherefore, the upper bound RùëÖR of ‚Äñùê∞tr‚àíùê∞0r‚Äñnormsuperscriptsubscriptùê∞ùë°ùëüsuperscriptsubscriptùê∞0ùëü\\|\\mathbf{w}_{t}^{r}-\\mathbf{w}_{0}^{r}\\| scales as ùí™‚Äã(1/m)ùí™1ùëö\\mathcal{O}(1/\\sqrt{m}) for any r‚àà[m]ùëüdelimited-[]ùëör\\in[m] according to (30).\nThis is consistent with the NTK regime that the parameter is hardly changed when the neural network is over-parameterized.\nMoreover, the number of the changed activation patterns is bounded by |Si‚üÇ|=|‚àër=1mùïÄ{‚ü®ùê∞tr,ùê±i‚ü©}‚â†ùïÄ{‚ü®ùê∞0r,ùê±i‚ü©}|‚â§4mR|S_{i}^{\\perp}|=|\\sum_{r=1}^{m}\\mathbb{I}\\{\\langle\\mathbf{w}_{t}^{r},\\mathbf{x}_{i}\\rangle\\}\\neq\\mathbb{I}\\{\\langle\\mathbf{w}_{0}^{r},\\mathbf{x}_{i}\\rangle\\}|\\leq 4mR according to Lemma¬†5.\nAs a result, ‚àëi‚àà[n]|Si‚üÇ|/(m‚Äãn)subscriptùëñdelimited-[]ùëõsuperscriptsubscriptùëÜùëñperpendicular-toùëöùëõ\\sum_{i\\in[n]}|S_{i}^{\\perp}|/(mn) can be upper bounded with 4‚ÄãR4ùëÖ4R, which also scales with ùí™‚Äã(1/m)ùí™1ùëö\\mathcal{O}(1/\\sqrt{m}).\nOn the other hand, GD has R=ùí™‚Äã(nŒª‚Äãm‚Äã‚ÄñùùÉ0‚Äñ)ùëÖùí™ùëõùúÜùëönormsubscriptùùÉ0R=\\mathcal{O}(\\frac{\\sqrt{n}}{\\lambda\\sqrt{m}}\\|\\bm{\\xi}_{0}\\|) according to¬†[ref]16, which is smaller than NAG due to Œ∫>1ùúÖ1\\kappa>1.\nFurthermore, RùëÖR of HB scales as ùí™‚Äã(n‚ÄãŒ∫Œª‚Äãm‚Äã‚ÄñùùÉ0‚Äñ)ùí™ùëõùúÖùúÜùëönormsubscriptùùÉ0\\mathcal{O}(\\frac{\\sqrt{n\\kappa}}{\\lambda\\sqrt{m}}\\|\\bm{\\xi}_{0}\\|)¬†[ref]23, which is similar as NAG."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Accelerated optimization for machine learning: First-order algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "Z. Lin, H. Li, C. Fang"
    },
    {
      "index": 1,
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences",
      "authors": "A. Beck, M. Teboulle"
    },
    {
      "index": 2,
      "title": "Accelerated distributed nesterov gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "G. Qu, N. Li"
    },
    {
      "index": 3,
      "title": "Some methods of speeding up the convergence of iteration methods",
      "abstract": "",
      "year": "1964",
      "venue": "USSR Computational Mathematics and Mathematical Physics",
      "authors": "B. T. Polyak"
    },
    {
      "index": 4,
      "title": "A method for solving the convex programming problem with convergence rate o (1/k^ 2)",
      "abstract": "",
      "year": "1983",
      "venue": "Dokl. akad. nauk Sssr",
      "authors": "Y. E. Nesterov"
    },
    {
      "index": 5,
      "title": "On the importance of initialization and momentum in deep learning",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "I. Sutskever, J. Martens, G. Dahl, G. Hinton"
    },
    {
      "index": 6,
      "title": "Incorporating nesterov momentum into adam",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "T. Dozat"
    },
    {
      "index": 7,
      "title": "Quasi-hyperbolic momentum and Adam for deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Ma, D. Yarats",
      "orig_title": "Quasi-hyperbolic momentum and adam for deep learning",
      "paper_id": "1810.06801v4"
    },
    {
      "index": 8,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. P. Kingma, J. Ba"
    },
    {
      "index": 9,
      "title": "On the convergence of Adam and Beyond",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. J. Reddi, S. Kale, S. Kumar",
      "orig_title": "On the convergence of adam and beyond",
      "paper_id": "1904.09237v1"
    },
    {
      "index": 10,
      "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.09843",
      "authors": "L. Luo, Y. Xiong, Y. Liu, X. Sun",
      "orig_title": "Adaptive gradient methods with dynamic bound of learning rate",
      "paper_id": "1902.09843v1"
    },
    {
      "index": 11,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K√∂pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 12,
      "title": "Deep learning with Keras",
      "abstract": "",
      "year": "2017",
      "venue": "Packt Publishing Ltd",
      "authors": "A. Gulli, S. Pal"
    },
    {
      "index": 13,
      "title": "TensorFlow: A system for large-scale machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "Symposium on Operating Systems Design and Implementation",
      "authors": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X. Zheng",
      "orig_title": "Tensorflow: A system for large-scale machine learning",
      "paper_id": "1605.08695v2"
    },
    {
      "index": 14,
      "title": "Some np-complete problems in quadratic and nonlinear programming",
      "abstract": "",
      "year": "1987",
      "venue": "Mathematical Programming",
      "authors": "K. G. Murty, S. N. Kabadi"
    },
    {
      "index": 15,
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. S. Du, X. Zhai, B. Poczos, A. Singh",
      "orig_title": "Gradient descent provably optimizes over-parameterized neural networks",
      "paper_id": "1810.02054v2"
    },
    {
      "index": 16,
      "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Y. Li, Y. Liang",
      "orig_title": "Learning overparameterized neural networks via stochastic gradient descent on structured data",
      "paper_id": "1808.01204v3"
    },
    {
      "index": 17,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. S. Du, J. D. Lee, H. Li, L. Wang, X. Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 18,
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Z. Allen-Zhu, Y. Li, Z. Song",
      "orig_title": "A convergence theory for deep learning via over-parameterization",
      "paper_id": "1811.03962v5"
    },
    {
      "index": 19,
      "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Wang",
      "orig_title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
      "paper_id": "1901.08584v2"
    },
    {
      "index": 20,
      "title": "Quadratic suffices for over-parametrization via matrix chernoff bound",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.03593",
      "authors": "Z. Song, X. Yang"
    },
    {
      "index": 21,
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Jacot, F. Gabriel, C. Hongler",
      "orig_title": "Neural tangent kernel: Convergence and generalization in neural networks",
      "paper_id": "1806.07572v4"
    },
    {
      "index": 22,
      "title": "A Modular Analysis of Provable Acceleration via Polyak‚Äôs Momentum: Training a Wide ReLU Network and a Deep Linear Network",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Wang, C. Lin, J. D. Abernethy",
      "orig_title": "A modular analysis of provable acceleration via polyak‚Äôs momentum: Training a wide relu network and a deep linear network",
      "paper_id": "2010.01618v6"
    },
    {
      "index": 23,
      "title": "A Dynamical View on Optimization Algorithms of Overparameterized Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Z. Bu, S. Xu, K. Chen",
      "orig_title": "A dynamical view on optimization algorithms of overparameterized neural networks",
      "paper_id": "2010.13165v2"
    },
    {
      "index": 24,
      "title": "Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07111",
      "authors": "X. Wu, S. S. Du, R. Ward",
      "orig_title": "Global convergence of adaptive gradient methods for an over-parameterized neural network",
      "paper_id": "1902.07111v2"
    },
    {
      "index": 25,
      "title": "Descending through a crowded valley - benchmarking deep learning optimizers",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "R. M. Schmidt, F. Schneider, P. Hennig"
    },
    {
      "index": 26,
      "title": "Introductory lectures on convex optimization: A basic course",
      "abstract": "",
      "year": "2003",
      "venue": "Springer Science & Business Media",
      "authors": "Y. Nesterov"
    },
    {
      "index": 27,
      "title": "Normierte ringe",
      "abstract": "",
      "year": "1941",
      "venue": "Recueil Math√©matique",
      "authors": "I. Gelfand"
    },
    {
      "index": 28,
      "title": "The strength of nesterov‚Äôs extrapolation in the individual convergence of nonsmooth optimization",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "W. Tao, Z. Pan, G. Wu, Q. Tao"
    },
    {
      "index": 29,
      "title": "Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints",
      "abstract": "",
      "year": "2016",
      "venue": "SIAM Journal on Optimization",
      "authors": "L. Lessard, B. Recht, A. Packard",
      "orig_title": "Analysis and design of optimization algorithms via integral quadratic constraints",
      "paper_id": "1408.3595v7"
    },
    {
      "index": 30,
      "title": "A differential equation for modeling nesterov‚Äôs accelerated gradient method: Theory and insights",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "W. Su, S. Boyd, E. J. Cand√®s"
    },
    {
      "index": 31,
      "title": "Understanding the acceleration phenomenon via high-resolution differential equations",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematical Programming",
      "authors": "B. Shi, S. Du, M. Jordan, W. Su"
    },
    {
      "index": 32,
      "title": "Lower bounds for finding stationary points I",
      "abstract": "",
      "year": "2020",
      "venue": "Mathematical Programming",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 33,
      "title": "How to Escape Saddle Points Efficiently",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, M. I. Jordan",
      "orig_title": "How to escape saddle points efficiently",
      "paper_id": "1703.00887v1"
    },
    {
      "index": 34,
      "title": "Convex until proven guilty: Dimension-free acceleration of gradient descent on non-convex functions",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 35,
      "title": "Generalized momentum-based methods: A hamiltonian perspective",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Optimization",
      "authors": "J. Diakonikolas, M. I. Jordan"
    },
    {
      "index": 36,
      "title": "On exact computation with an infinitely wide neural net",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, R. Wang"
    },
    {
      "index": 37,
      "title": "Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? ‚Äî A Neural Tangent Kernel Perspective",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. Huang, Y. Wang, M. Tao, T. Zhao",
      "orig_title": "Why do deep residual networks generalize better than deep feedforward networks? - A neural tangent kernel perspective",
      "paper_id": "2002.06262v2"
    },
    {
      "index": 38,
      "title": "Graph neural tangent kernel: Fusing graph neural networks with graph kernels",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. S. Du, K. Hou, R. Salakhutdinov, B. P√≥czos, R. Wang, K. Xu"
    },
    {
      "index": 39,
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "National Academy of Sciences",
      "authors": "S. Mei, A. Montanari, P.-M. Nguyen",
      "orig_title": "A mean field view of the landscape of two-layers neural networks",
      "paper_id": "1804.06561v2"
    },
    {
      "index": 40,
      "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, F. Bach"
    },
    {
      "index": 41,
      "title": "Acceleration via symplectic discretization of high-resolution differential equations",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Shi, S. S. Du, W. Su, M. I. Jordan"
    },
    {
      "index": 42,
      "title": "Optimization Landscape and Expressivity of Deep CNNs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Q. Nguyen, M. Hein",
      "orig_title": "Optimization landscape and expressivity of deep cnns",
      "paper_id": "1710.10928v2"
    },
    {
      "index": 43,
      "title": "On Lazy Training in Differentiable Programming",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, E. Oyallon, F. Bach",
      "orig_title": "On lazy training in differentiable programming",
      "paper_id": "1812.07956v5"
    },
    {
      "index": 44,
      "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, J. Pennington"
    },
    {
      "index": 45,
      "title": "From averaging to acceleration, there is only a step-size",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "N. Flammarion, F. Bach"
    },
    {
      "index": 46,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.07747",
      "authors": "H. Xiao, K. Rasul, R. Vollgraf",
      "orig_title": "Fashion-mnist: A novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 47,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Y. LeCun, L. Bottou, Y. Bengio, P. Haffner"
    },
    {
      "index": 48,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "A. Krizhevsky, G. Hinton, et al."
    },
    {
      "index": 49,
      "title": "Finite Versus Infinite Neural Networks: an Empirical Study",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systemsl",
      "authors": "J. Lee, S. S. Schoenholz, J. Pennington, B. Adlam, L. Xiao, R. Novak, J. Sohl-Dickstein",
      "orig_title": "Finite versus infinite neural networks: An empirical study",
      "paper_id": "2007.15801v2"
    },
    {
      "index": 50,
      "title": "JAX: Composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, Q. Zhang"
    },
    {
      "index": 51,
      "title": "Position-transitional particle swarm optimization-incorporated latent factor analysis",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "X. Luo, Y. Yuan, S. Chen, N. Zeng, Z. Wang"
    },
    {
      "index": 52,
      "title": "A data-characteristic-aware latent factor model for web services qos prediction",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "D. Wu, X. Luo, M. Shang, Y. He, G. Wang, X. Wu"
    },
    {
      "index": 53,
      "title": "Fast and accurate non-negative latent factor analysis on high-dimensional and sparse matrices in recommender systems",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge & Data Engineering",
      "authors": "X. Luo, Y. Zhou, Z. Liu, M. Zhou"
    },
    {
      "index": 54,
      "title": "A novel approach to large-scale dynamically weighted directed network representation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "X. Luo, H. Wu, Z. Wang, J. Wang, D. Meng"
    }
  ]
}