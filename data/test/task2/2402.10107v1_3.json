{
  "paper_id": "2402.10107v1",
  "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
  "sections": {
    "ii-b controllable text generation": "Controllable text generation (CTG) is the task of generating text while adhering to a given controlled element [ref]20. In plug-and-play controllable generation, the LM is kept frozen, and its output is constrained using potential functions, such as classifiers [ref]8 . FUDGE [ref]3 focuses on the partial sequence and reweights the LM prediction at each token based on an estimate of control satisfaction. DExperts  achieves control by reweighting the LM prediction at each token using a smaller fine-tuned or trained LM for the specific control task. Diffusion-LM  fine-tunes the pre-trained weights while updating them based on a controllable text generation algorithm. In CTG, given a vocabulary V, the goal is to generate a target text X={x0,x1,…,xt,…,xT}𝑋subscript𝑥0subscript𝑥1…subscript𝑥𝑡…subscript𝑥𝑇X=\\left\\{x_{0},x_{1},\\ldots,x_{t},\\ldots,x_{T}\\right\\}, where xt∈Vsubscript𝑥𝑡Vx_{t}\\in\\textbf{\\em V}, while taking into account a control element denoted as w𝑤w. Formally, CTG can be expressed as follows: The specific expression of w𝑤w may vary in accordance with different tasks. For the sentence Y𝑌Y generated by the model, it is also expected to conform to the constraint conditions and the general natural language characteristics (e.g., fluency, rationality, readability, and control success). [ref]3 . To convert a continuous diffusion model into discrete text representation, we employ an embedding function EMB⁡(wi)EMBsubscript𝑤𝑖\\operatorname{EMB}\\left(w_{i}\\right), which maps each word’s embedding space to a corresponding vector in ℝdsuperscriptℝ𝑑\\mathbb{R}^{d}. The embedding vector is represented as EMB⁡(𝐰)=[EMB⁡(w1),…,EMB⁡(wn)]∈ℝn​dEMB𝐰EMBsubscript𝑤1…EMBsubscript𝑤𝑛superscriptℝ𝑛𝑑\\operatorname{EMB}(\\mathbf{w})=\\left[\\operatorname{EMB}\\left(w_{1}\\right),\\ldots,\\operatorname{EMB}\\left(w_{n}\\right)\\right]\\in\\mathbb{R}^{nd}, where n𝑛n is the length of the text. In the forward process, the diffusion model introduces a Markov transition from discrete words ww\\mathrm{w} to x0subscriptx0\\mathrm{x}_{0}, which is parameterized by qϕ​(X0∣𝐰)=𝒩​(EMB⁡(𝐰),σ0​I)subscript𝑞italic-ϕconditionalsubscriptX0𝐰𝒩EMB𝐰subscript𝜎0𝐼q_{\\phi}\\left(\\mathrm{X}_{0}\\mid\\mathbf{w}\\right)=\\mathcal{N}\\left(\\operatorname{EMB}(\\mathbf{w}),\\sigma_{0}I\\right). In the reverse process, a trainable rounding step is added, which is parameterized by pθ​(𝐰∣X0)=∏i=1npθ​(wi∣xi)subscript𝑝𝜃conditional𝐰subscriptX0superscriptsubscriptproduct𝑖1𝑛subscript𝑝𝜃conditionalsubscript𝑤𝑖subscript𝑥𝑖p_{\\theta}\\left(\\mathbf{w}\\mid\\mathrm{X}_{0}\\right)=\\prod_{i=1}^{n}p_{\\theta}\\left(w_{i}\\mid x_{i}\\right). This rounding step optimizes the connection between the continuous x0subscript𝑥0x_{0} and discrete text, where pθ​(wi∣xi)subscript𝑝𝜃conditionalsubscript𝑤𝑖subscript𝑥𝑖p_{\\theta}\\left(w_{i}\\mid x_{i}\\right) is modeled as a Softmax distribution. The training objective introduced above is now updated according to: Then we get We employ the same simplification which transforms ℒvlb →ℒsimple →subscriptℒvlb subscriptℒsimple \\mathcal{L}_{\\text{vlb }}\\rightarrow\\mathcal{L}_{\\text{simple }} to transform ℒvlb e2e→ℒsimple e2e→superscriptsubscriptℒvlb e2esuperscriptsubscriptℒsimple e2e\\mathcal{L}_{\\text{vlb }}^{\\mathrm{e}2\\mathrm{e}}\\rightarrow\\mathcal{L}_{\\text{simple }}^{\\mathrm{e}2\\mathrm{e}}, this result is consistent with  , that is: Accordingly, the training objective introduced above now becomes: We get the same formula in accordance to  and train the Transformer model to directly predict x0subscriptx0\\mathrm{x}_{0} via fθ​(xt,t)subscript𝑓𝜃subscriptx𝑡𝑡f_{\\theta}\\left(\\mathrm{x}_{t},t\\right) , and use the tractable Gaussian posterior q​(xt−1∣x0,xt)𝑞conditionalsubscriptx𝑡1subscriptx0subscriptx𝑡q\\left(\\mathrm{x}_{t-1}\\mid\\mathrm{x}_{0},\\mathrm{x}_{t}\\right) to compute the mean of xt−1subscriptx𝑡1\\mathrm{x}_{t-1}, conditioned on predicted x0subscriptx0\\mathrm{x}_{0} and observed xt:αt−1​βt1−α¯t​x0+αt​(1−α¯t−1)1−α¯t​xt:subscriptx𝑡subscript𝛼𝑡1subscript𝛽𝑡1subscript¯𝛼𝑡subscriptx0subscript𝛼𝑡1subscript¯𝛼𝑡11subscript¯𝛼𝑡subscriptx𝑡\\mathrm{x}_{t}:\\frac{\\sqrt{\\alpha_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}}\\mathrm{x}_{0}+\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}}\\mathrm{x}_{t}."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "S. Gehrmann et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2204.02311",
      "authors": "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 2,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2205.01068",
      "authors": "S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al.",
      "orig_title": "Opt: Open pre-trained transformer language models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 3,
      "title": "Fudge: Controlled Text Generation With Future Discriminators",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2104.05218",
      "authors": "K. Yang and D. Klein",
      "orig_title": "FUDGE: controlled text generation with future discriminators",
      "paper_id": "2104.05218v2"
    },
    {
      "index": 4,
      "title": "Diffusion-LM improves controllable text generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2205.14217",
      "authors": "X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B. Hashimoto"
    },
    {
      "index": 5,
      "title": "Lora: Low-rank adaptation of large language models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2106.09685",
      "authors": "E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen"
    },
    {
      "index": 6,
      "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2110.04366",
      "authors": "J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig",
      "orig_title": "Towards a unified view of parameter-efficient transfer learning",
      "paper_id": "2110.04366v3"
    },
    {
      "index": 7,
      "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1909.05858",
      "authors": "N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher",
      "orig_title": "CTRL: A conditional transformer language model for controllable generation",
      "paper_id": "1909.05858v2"
    },
    {
      "index": 8,
      "title": "A Plug-and-Play Method for Controlled Text Generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2109.09707",
      "authors": "D. Pascual, B. Egressy, C. Meister, R. Cotterell, and R. Wattenhofer",
      "orig_title": "A plug-and-play method for controlled text generation",
      "paper_id": "2109.09707v1"
    },
    {
      "index": 9,
      "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo",
      "orig_title": "Vector quantized diffusion model for text-to-image synthesis",
      "paper_id": "2111.14822v3"
    },
    {
      "index": 10,
      "title": "Discrete contrastive diffusion for cross-modal and conditional generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2206.07771",
      "authors": "Y. Zhu, Y. Wu, K. Olszewski, J. Ren, S. Tulyakov, and Y. Yan"
    },
    {
      "index": 11,
      "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2207.09983",
      "authors": "D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu",
      "orig_title": "Diffsound: Discrete diffusion model for text-to-sound generation",
      "paper_id": "2207.09983v2"
    },
    {
      "index": 12,
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2005.00247",
      "authors": "J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych",
      "orig_title": "Adapterfusion: Non-destructive task composition for transfer learning",
      "paper_id": "2005.00247v3"
    },
    {
      "index": 13,
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2101.00190",
      "authors": "X. L. Li and P. Liang",
      "orig_title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "paper_id": "2101.00190v1"
    },
    {
      "index": 14,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 15,
      "title": "Denoising Diffusion Probabilistic Models",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Ho, A. Jain, and P. Abbeel",
      "orig_title": "Denoising diffusion probabilistic models",
      "paper_id": "2006.11239v2"
    },
    {
      "index": 16,
      "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2105.03023",
      "authors": "A. Liu, M. Sap, X. Lu, S. Swayamdipta, C. Bhagavatula, N. A. Smith, and Y. Choi",
      "orig_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
      "paper_id": "2105.03023v2"
    },
    {
      "index": 17,
      "title": "Symbolic Music Generation with Diffusion Models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2103.16091",
      "authors": "G. Mittal, J. Engel, C. Hawthorne, and I. Simon",
      "orig_title": "Symbolic music generation with diffusion models",
      "paper_id": "2103.16091v2"
    },
    {
      "index": 18,
      "title": "Improved denoising diffusion probabilistic models",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Q. Nichol and P. Dhariwal"
    },
    {
      "index": 19,
      "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2211.15029",
      "authors": "Z. He, T. Sun, K. Wang, X. Huang, and X. Qiu",
      "orig_title": "Diffusionbert: Improving generative masked language models with diffusion models",
      "paper_id": "2211.15029v2"
    },
    {
      "index": 20,
      "title": "Exploring Controllable Text Generation Techniques",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2005.01822",
      "authors": "S. Prabhumoye, A. W. Black, and R. Salakhutdinov",
      "orig_title": "Exploring controllable text generation techniques",
      "paper_id": "2005.01822v2"
    },
    {
      "index": 21,
      "title": "Plug and Play Language Models: a Simple Approach to Controlled Text Generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1912.02164",
      "authors": "S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu",
      "orig_title": "Plug and play language models: A simple approach to controlled text generation",
      "paper_id": "1912.02164v4"
    },
    {
      "index": 22,
      "title": "Compressing Deep Convolutional Networks using Vector Quantization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv:1412.6115",
      "authors": "Y. Gong, L. Liu, M. Yang, and L. Bourdev",
      "orig_title": "Compressing deep convolutional networks using vector quantization",
      "paper_id": "1412.6115v1"
    },
    {
      "index": 23,
      "title": "Vector quantization based approximate spectral clustering of large datasets",
      "abstract": "",
      "year": "2012",
      "venue": "Pattern Recognition",
      "authors": "K. Taşdemir"
    },
    {
      "index": 24,
      "title": "Convergence of stochastic vector quantization and learning vector quantization with bregman divergences",
      "abstract": "",
      "year": "2020",
      "venue": "IFAC-PapersOnLine",
      "authors": "C. N. Mavridis and J. S. Baras"
    },
    {
      "index": 25,
      "title": "Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "S. Bond-Taylor, P. Hessey, H. Sasaki, T. P. Breckon, and C. G. Willcocks",
      "orig_title": "Unleashing transformers: parallel token prediction with discrete absorbing diffusion for fast high-resolution image generation from vector-quantized codes",
      "paper_id": "2111.12701v1"
    },
    {
      "index": 26,
      "title": "Vector quantized diffusion model with codeunet for text-to-sign pose sequences generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2208.09141",
      "authors": "P. Xie, Q. Zhang, Z. Li, H. Tang, Y. Du, and X. Hu"
    },
    {
      "index": 27,
      "title": "nuqmm: Quantized matmul for efficient inference of large-scale generative language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2206.09557",
      "authors": "G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee"
    },
    {
      "index": 28,
      "title": "Quantized neural networks: Training neural networks with low precision weights and activations",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio"
    },
    {
      "index": 29,
      "title": "Diffusion Models in Vision: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2209.04747",
      "authors": "F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah",
      "orig_title": "Diffusion models in vision: A survey",
      "paper_id": "2209.04747v6"
    },
    {
      "index": 30,
      "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "M. Courbariaux, Y. Bengio, and J.-P. David",
      "orig_title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
      "paper_id": "1511.00363v3"
    },
    {
      "index": 31,
      "title": "AdaBin: Improving Binary Neural Networks with Adaptive Binary Sets",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "Z. Tu, X. Chen, P. Ren, and Y. Wang",
      "orig_title": "AdaBin: improving binary neural networks with adaptive binary sets",
      "paper_id": "2208.08084v2"
    },
    {
      "index": 32,
      "title": "TernaryNet: Faster Deep Model Inference without GPUs for Medical 3D Segmentation using Sparse and Binary Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "International journal of computer assisted radiology and surgery",
      "authors": "M. P. Heinrich, M. Blendowski, and O. Oktay",
      "orig_title": "TernaryNet: faster deep model inference without gpus for medical 3D segmentation using sparse and binary convolutions",
      "paper_id": "1801.09449v1"
    },
    {
      "index": 33,
      "title": "Trq: Ternary neural networks with residual quantization",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Y. Li, W. Ding, C. Liu, B. Zhang, and G. Guo"
    },
    {
      "index": 34,
      "title": "U-Net Fixed-Point Quantization for Medical Image Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "Large-Scale Annotation of Biomedical Data and Expert Label Synthesis and Hardware Aware Learning for Medical Imaging and Computer Assisted Intervention",
      "authors": "M. AskariHemmat, S. Honari, L. Rouhier, C. S. Perone, J. Cohen-Adad, Y. Savaria, and J.-P. David",
      "orig_title": "U-Net fixed-point quantization for medical image segmentation",
      "paper_id": "1908.01073v2"
    },
    {
      "index": 35,
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Medical image computing and computer-assisted intervention",
      "authors": "O. Ronneberger, P. Fischer, and T. Brox",
      "orig_title": "U-net: Convolutional networks for biomedical image segmentation",
      "paper_id": "1505.04597v1"
    },
    {
      "index": 36,
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly",
      "orig_title": "Parameter-efficient transfer learning for nlp",
      "paper_id": "1902.00751v2"
    },
    {
      "index": 37,
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2104.08691",
      "authors": "B. Lester, R. Al-Rfou, and N. Constant",
      "orig_title": "The power of scale for parameter-efficient prompt tuning",
      "paper_id": "2104.08691v2"
    },
    {
      "index": 38,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 39,
      "title": "Vector-quantized Image Modeling with Improved VQGAN",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2110.04627",
      "authors": "J. Yu, X. Li, J. Y. Koh, H. Zhang, R. Pang, J. Qin, A. Ku, Y. Xu, J. Baldridge, and Y. Wu",
      "orig_title": "Vector-quantized image modeling with improved VQGAN",
      "paper_id": "2110.04627v3"
    },
    {
      "index": 40,
      "title": "Q8bert: Quantized 8bit bert",
      "abstract": "",
      "year": "2019",
      "venue": "2019 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)",
      "authors": "O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat"
    },
    {
      "index": 41,
      "title": "A comprehensive study on post-training quantization for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2303.08302",
      "authors": "Z. Yao, C. Li, X. Wu, S. Youn, and Y. He"
    },
    {
      "index": 42,
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2011.13456",
      "authors": "Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole",
      "orig_title": "Score-based generative modeling through stochastic differential equations",
      "paper_id": "2011.13456v2"
    },
    {
      "index": 43,
      "title": "The curious case of neural text degeneration",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1904.09751",
      "authors": "A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi"
    },
    {
      "index": 44,
      "title": "Minimum bayes-risk decoding for statistical machine translation",
      "abstract": "",
      "year": "2004",
      "venue": "JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP), Tech. Rep.",
      "authors": "S. Kumar and W. Byrne"
    },
    {
      "index": 45,
      "title": "Auto-encoding variational bayes",
      "abstract": "",
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings",
      "authors": "D. P. Kingma and M. Welling"
    },
    {
      "index": 46,
      "title": "Stochastic backpropagation and approximate inference in deep generative models",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "D. J. Rezende, S. Mohamed, and D. Wierstra"
    },
    {
      "index": 47,
      "title": "The E2E dataset: New challenges for end-to-end generation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv:1706.09254",
      "authors": "J. Novikova, O. Dušek, and V. Rieser"
    },
    {
      "index": 48,
      "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen"
    },
    {
      "index": 49,
      "title": "Pointer Sentinel Mixture Models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "S. Merity, C. Xiong, J. Bradbury, and R. Socher",
      "orig_title": "Pointer sentinel mixture models",
      "paper_id": "1609.07843v1"
    },
    {
      "index": 50,
      "title": "Adaptive subgradient methods for online learning and stochastic optimization.",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of machine learning research",
      "authors": "J. Duchi, E. Hazan, and Y. Singer"
    },
    {
      "index": 51,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of machine learning research",
      "authors": "L. Van der Maaten and G. Hinton",
      "orig_title": "Visualizing data using t-sne.",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 52,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "Technical report",
      "authors": "A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al."
    }
  ]
}