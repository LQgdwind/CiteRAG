{
  "paper_id": "2003.06709v5",
  "title": "FACMAC: Factored Multi-Agent Centralised Policy Gradients",
  "sections": {
    "introduction": "Significant progress has been made in cooperative multi-agent reinforcement learning (MARL) under the paradigm of centralised training with decentralised execution (CTDE)   in recent years, both in value-based    [ref]31   and actor-critic [ref]21 [ref]7   approaches. Most popular multi-agent actor-critic methods such as COMA [ref]7 and MADDPG [ref]21 learn a centralised critic with decentralised actors. The critic is centralised to make use of all available information (i.e., it can condition on the global state and the joint action) to estimate the joint action-value function Qt‚Äão‚ÄãtsubscriptùëÑùë°ùëúùë°Q_{tot}, unlike a decentralised critic that estimates the local action-value function QasubscriptùëÑùëéQ_{a} based only on individual observations and actions for each agent aùëéa.111COMA learns a single centralised critic for all cooperative agents due to parameter sharing. For each agent the critic has different inputs and can thus output different values for the same state and joint action. In MADDPG, each agents learns its own centralised critic, as it is designed for general multi-agent learning problems, including cooperative, competitive, and mixed settings.\nEven though the joint action-value function these actor-critic methods can represent is not restricted, in practice they significantly underperform value-based methods like QMIX  on the challenging StarCraft Multi-Agent Challenge (SMAC)  benchmark  [ref]31. In this paper, we propose a novel approach called FACtored Multi-Agent Centralised policy gradients (FACMAC), which works for both discrete and continuous cooperative multi-agent tasks.\nLike MADDPG, our approach uses deep deterministic policy gradients  to learn decentralised policies.\nHowever, FACMAC learns a single centralised but factored critic, which factors the joint action-value function Qt‚Äão‚ÄãtsubscriptùëÑùë°ùëúùë°Q_{tot} into per-agent utilities QasubscriptùëÑùëéQ_{a} that are combined via a non-linear monotonic function, as in the popular QùëÑQ-learning algorithm QMIX . While the critic used in COMA and MADDPG is also centralised, it is monolithic rather than factored.222We use ‚Äúcentralised and monolithic critic‚Äù and ‚Äúmonolithic critic‚Äù interchangeably to refer to the centralised critic used in COMA and MADDPG, and ‚Äúcentralised but factored critic‚Äù and ‚Äúfactored critic‚Äù interchangeably to refer to the critic used in our approach. Compared to learning a monolithic critic, our factored critic can potentially scale better to tasks with a larger number of agents and/or actions. In addition, in contrast to other value-based approaches such as QMIX, there are no inherent constraints on factoring the critic. This allows us to employ rich value factorisations, including nonmonotonic ones, that value-based methods cannot directly use without forfeiting decentralisability or introducing other significant algorithmic changes. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In MADDPG, a separate policy gradient is derived for each agent individually, which optimises its policy assuming all other agents‚Äô actions are fixed. This could cause the agents to converge to sub-optimal policies in which no single agent wishes to change its action unilaterally. In FACMAC, we use a new centralised gradient estimator that optimises over the entire joint action space, rather than optimising over each agent‚Äôs action space separately as in MADDPG. The agents‚Äô policies are thus trained as a single joint-action policy, which can enable learning of more coordinated behaviour, as well as the ability to escape sub-optimal solutions. The centralised gradient estimator fully reaps the benefits of learning a centralised critic, by not implicitly marginalising over the actions of the other agents in the policy-gradient update. The gradient estimator used in MADDPG is also known to be vulnerable to relative overgeneralisation . To overcome this issue, in our centralised gradient estimator, we sample all actions from all agents‚Äô current policies when evaluating the joint action-value function. We empirically show that MADDPG can quickly get stuck in local optima in a simple continuous matrix game, whereas our centralised gradient estimator finds the optimal policy. While Lyu et¬†al.  recently show that merely using a centralised critic (with per-agent gradients that optimise over each agent‚Äôs actions separately) does not necessarily lead to better coordination, our centralised gradient estimator re-establishes the value of using centralised critics. Most recent works on continuous MARL focus on evaluating their algorithms on the multi-agent particle environments [ref]21, which feature a simple two-dimensional world with some basic simulated physics. To demonstrate FACMAC‚Äôs scalability to more complex continuous domains and to stimulate more progress in continuous MARL, we introduce Multi-Agent MuJoCo (MAMuJoCo), a new, comprehensive benchmark suite that allows the study of decentralised continuous control.\nBased on the popular single-agent MuJoCo benchmark , MAMuJoCo features a wide variety of novel robotic control tasks in which multiple agents within a single robot have to solve a task cooperatively. We evaluate FACMAC on variants of the multi-agent particle environments [ref]21 and our novel MAMuJoCo benchmark, which both feature continuous action spaces, and the challenging SMAC benchmark , which features discrete action spaces. Empirical results demonstrate FACMAC‚Äôs superior performance over MADDPG and other baselines on all three domains. In particular, FACMAC scales better when the number of agents (and/or actions) and the complexity of the task increases. Results on SMAC show that FACMAC significantly outperforms stochastic DOP 3, which recently claimed to be the first multi-agent actor-critic method to outperform state-of-the-art valued-based methods on SMAC, in all scenarios we tested. Moreover, our ablations and additional experiments demonstrate the advantages of both factoring the critic and using our centralised gradient estimator. We show that, compared to learning a monolithic critic, learning a factored critic can: 1) better take advantage of the centralised gradient estimator to optimise the agent policies when the number of agents and/or actions is large, and 2) leverage a nonmonotonic factorisation to solve tasks that cannot be solved with monolithic or monotonically factored critics."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01465",
      "authors": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama",
      "orig_title": "Reducing overestimation bias in multi-agent domains using double centralized critics",
      "paper_id": "1910.01465v2"
    },
    {
      "index": 1,
      "title": "Input Invex Neural Network",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "Brandon Amos, Lei Xu, and J Zico Kolter",
      "orig_title": "Input convex neural networks",
      "paper_id": "2106.08748v4"
    },
    {
      "index": 2,
      "title": "Factorised critics in deep multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Master Thesis, University of Oxford",
      "authors": "Marilena Bescuca"
    },
    {
      "index": 3,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 4,
      "title": "A tutorial on the cross-entropy method",
      "abstract": "",
      "year": "2005",
      "venue": "Annals of operations research",
      "authors": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein"
    },
    {
      "index": 5,
      "title": "LIIR: Learning individual intrinsic reward in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao"
    },
    {
      "index": 6,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 7,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine",
      "orig_title": "Continuous deep q-learning with model-based acceleration",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 8,
      "title": "Cooperative multi-agent control using deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer"
    },
    {
      "index": 9,
      "title": "Hypernetworks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.09106",
      "authors": "David Ha, Andrew Dai, and Quoc V Le"
    },
    {
      "index": 10,
      "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Shariq Iqbal and Fei Sha",
      "orig_title": "Actor-attention-critic for multi-agent reinforcement learning",
      "paper_id": "1810.02912v2"
    },
    {
      "index": 11,
      "title": "Categorical reparameterization with gumbel-softmax",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.01144",
      "authors": "Eric Jang, Shixiang Gu, and Ben Poole"
    },
    {
      "index": 12,
      "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.10293",
      "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al.",
      "orig_title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
      "paper_id": "1806.10293v3"
    },
    {
      "index": 13,
      "title": "Robocup: A challenge problem for ai",
      "abstract": "",
      "year": "1997",
      "venue": "AI magazine",
      "authors": "Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, Eiichi Osawa, and Hitoshi Matsubara"
    },
    {
      "index": 14,
      "title": "Computing factored value functions for policies in structured mdps",
      "abstract": "",
      "year": "1999",
      "venue": "IJCAI",
      "authors": "Daphne Koller and Ronald Parr"
    },
    {
      "index": 15,
      "title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning",
      "abstract": "",
      "year": "2016",
      "venue": "Neurocomputing",
      "authors": "Landon Kraemer and Bikramjit Banerjee"
    },
    {
      "index": 16,
      "title": "Distributed self-reconfiguration of m-tran iii modular robotic system",
      "abstract": "",
      "year": "2008",
      "venue": "The International Journal of Robotics Research",
      "authors": "Haruhisa Kurokawa, Kohji Tomita, Akiya Kamimura, Shigeru Kokaji, Takashi Hasuo, and Satoshi Murata"
    },
    {
      "index": 17,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "4th International Conference on Learning Representations, ICLR",
      "authors": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra"
    },
    {
      "index": 18,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Long-Ji Lin"
    },
    {
      "index": 19,
      "title": "Emergent coordination through competition",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07151",
      "authors": "Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel"
    },
    {
      "index": 20,
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch",
      "orig_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
      "paper_id": "1706.02275v4"
    },
    {
      "index": 21,
      "title": "Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and Multi-Agent Systems",
      "authors": "Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato",
      "orig_title": "Contrasting centralized and decentralized critics in multi-agent reinforcement learning",
      "paper_id": "2102.04402v2"
    },
    {
      "index": 22,
      "title": "MAVEN: Multi-Agent Variational Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson",
      "orig_title": "Maven: Multi-agent variational exploration",
      "paper_id": "1910.07483v2"
    },
    {
      "index": 23,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al."
    },
    {
      "index": 24,
      "title": "Chainform: A linear integrated modular hardware system for shape changing interfaces",
      "abstract": "",
      "year": "2016",
      "venue": "29th Annual Symposium on User Interface Software and Technology",
      "authors": "Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A Paradiso, and Hiroshi Ishii"
    },
    {
      "index": 25,
      "title": "Optimal and approximate Q-value functions for decentralized pomdps",
      "abstract": "",
      "year": "2008",
      "venue": "JAIR",
      "authors": "Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos Vlassis"
    },
    {
      "index": 26,
      "title": "A concise introduction to decentralized POMDPs, volume 1",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Frans A Oliehoek, Christopher Amato, et al."
    },
    {
      "index": 27,
      "title": "openai/baselines, May 2020.",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 28,
      "title": "Softmax with regularization: Better value estimation in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.11883",
      "authors": "Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson"
    },
    {
      "index": 29,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 30,
      "title": "Weighted qmix: Expanding monotonic value function factorisation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson"
    },
    {
      "index": 31,
      "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "JMLR",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "2003.08839v2"
    },
    {
      "index": 32,
      "title": "Reinforcement learning for robot soccer",
      "abstract": "",
      "year": "2009",
      "venue": "Autonomous Robots",
      "authors": "Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange"
    },
    {
      "index": 33,
      "title": "The StarCraft Multi-Agent Challenge",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "The StarCraft Multi-Agent Challenge",
      "paper_id": "1902.04043v5"
    },
    {
      "index": 34,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.05408",
      "authors": "Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 35,
      "title": "Qtran++: Improved value transformation for cooperative multi-agent reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12010",
      "authors": "Kyunghwan Son, Sungsoo Ahn, Roben Delos Reyes, Jinwoo Shin, and Yung Yi"
    },
    {
      "index": 36,
      "title": "Scaling reinforcement learning toward RoboCup soccer",
      "abstract": "",
      "year": "2001",
      "venue": "Icml",
      "authors": "Peter Stone and Richard S. Sutton"
    },
    {
      "index": 37,
      "title": "Value-decomposition networks for cooperative multi-agent learning based on team reward",
      "abstract": "",
      "year": "2018",
      "venue": "AAMAS",
      "authors": "Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vin√≠cius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al."
    },
    {
      "index": 38,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Emanuel Todorov, Tom Erez, and Yuval Tassa"
    },
    {
      "index": 39,
      "title": "Towards understanding linear value decomposition in cooperative multi-agent q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.00587",
      "authors": "Jianhao Wang, Zhizhou Ren, Beining Han, and Chongjie Zhang"
    },
    {
      "index": 40,
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.01062",
      "authors": "Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang",
      "orig_title": "Qplex: Duplex dueling multi-agent q-learning",
      "paper_id": "2008.01062v3"
    },
    {
      "index": 41,
      "title": "NerveNet: learning structured policy with graph neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR",
      "authors": "Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler"
    },
    {
      "index": 42,
      "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
      "abstract": "",
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR",
      "authors": "Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang",
      "orig_title": "Dop: Off-policy multi-agent decomposed policy gradients",
      "paper_id": "2007.12322v2"
    },
    {
      "index": 43,
      "title": "Lenient learning in independent-learner stochastic cooperative games",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Ermo Wei and Sean Luke"
    },
    {
      "index": 44,
      "title": "Design and architecture of the unified modular snake robot",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE International Conference on Robotics and Automation",
      "authors": "Cornell Wright, Austin Buchan, Ben Brown, Jason Geist, Michael Schwerin, David Rollinson, Matthew Tesch, and Howie Choset"
    },
    {
      "index": 45,
      "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.03939",
      "authors": "Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang",
      "orig_title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
      "paper_id": "2002.03939v2"
    },
    {
      "index": 46,
      "title": "Modular robots",
      "abstract": "",
      "year": "2002",
      "venue": "IEEE Spectrum",
      "authors": "Mark Yim, Ying Zhang, and David Duff"
    },
    {
      "index": 47,
      "title": "Learning implicit credit assignment for multi-agent actor-critic",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung"
    }
  ]
}