{
  "paper_id": "2006.03217v3",
  "title": "Content and Context Features for Scene Image Representation",
  "sections": {
    "comparison with the state-of-the-art methods": "We compare the SVM classification performance of our three types of features - tag-based context features (T​F𝑇𝐹TF), deep content features (D​F=F​F+B​F𝐷𝐹𝐹𝐹𝐵𝐹DF=FF+BF) and fusion of context and content features (C​C​F𝐶𝐶𝐹CCF), with the state-of-the-art feature extraction methods including traditional CV-based methods, deep learning-based methods and tag-based methods. We present the accuracy numbers of all contending measures on MIT-67, Scene-15, and Event-8 in three separate Tables 2, 3 and 4, respectively. For existing methods, we simply present the accuracies reported in corresponding published papers. In Table 2, we present the classification accuracies of all contending methods on the MIT-67 dataset. We compare our features with different types of previous methods: 777 traditional CV-based methods, 999 deep learning-based methods and 666 tag-based methods. Our tag-based context features (T​F𝑇𝐹TF) outperform all existing traditional CV-based methods and existing tag-based features, by at least 27%percent2727\\% and 0.6%percent0.60.6\\%, respectively.\nOur features produce competitive results to many state-of-the-art deep content-based features. Our deep features (D​F=F​F+B​F𝐷𝐹𝐹𝐹𝐵𝐹DF=FF+BF) based on background and foreground information surpass all contending deep features by at least 1.5%percent1.51.5\\% except HDF method. The fusion of the two types of features (C​C​F𝐶𝐶𝐹CCF) further boost the performance by 5.3%percent5.35.3\\% aga t recent second best method (HDF [ref]42)\nand leads to the classification accuracy of 87.3%percent87.387.3\\%. Table 3 summarizes the classification accuracies on the Scene-15 dataset. For comparison with existing methods, we choose 666 traditional CV-based methods, 555 deep learning-based methods and 666 tag-based methods. Our context (T​F𝑇𝐹TF) features outperform all existing tag-based context features and some traditional CV-based methods, but they perform worse than other traditional CV-based and deep learning-based content features. Our deep features (D​F𝐷𝐹DF) surpass all existing tag-based, traditional CV-based and deep features except EISR  which is slightly higher than our D​F𝐷𝐹DF by 1%percent11\\%. As a result, the fusion of our context and content-based features (C​C​F𝐶𝐶𝐹CCF) exceed the best existing method (EISR ) by about 1%percent11\\%. In Table 4, we list the accuracies of contending feature extraction methods on Event-8. We select 444 traditional CV-based methods, 555 deep learning-based methods and 666 tag-based methods.\nThe results show that our context feature (T​F𝑇𝐹TF) outperforms all existing methods except ResNet1522, whereas our deep content features (D​F𝐷𝐹DF) outperform them all. The fusion of T​F𝑇𝐹TF and D​F𝐷𝐹DF (C​C​F𝐶𝐶𝐹CCF) generates the best result, which surpasses the best existing method, ResNet152 2 by more than 1%percent11\\%. To summarise, our context features (T​F𝑇𝐹TF) outperform existing context features on all three datasets, and our deep content features (D​F𝐷𝐹DF) produce generally better (with an exception of EISR  on Scene15). However, by simply fusing the two types (context and content) of features (C​C​F𝐶𝐶𝐹CCF), we achieve the best results in all datasets with the margin of 0.9%percent0.90.9\\% to 5.3%percent5.35.3\\% over the best existing methods. It shows that fusing context information with the content information can better differentiate images of different classes. It adds more value (more than 5%percent55\\% improvement in the classification accuracy) on the MIT-67 dataset, the largest dataset in our research involving lots of ambiguities between classes with inter-class similarity and intra-class dissimilarity. It is interesting to note that all three best existing methods – CNN-LSTM  (MIT-67), EISR  (Scene-15) and ResNet-152 2 (Event-8), have greater features sizes, which is one major problem in many existing methods as well. For instance, CNN-LSTM \nhas the feature size of 4,096-D on the MIT-67 dataset. Similarly, EISR  has an extremely higher dimensional features size, which is greater than 50×2,04850204850\\times 2,048-D after concatenation with the the tag-based features. Finally, ResNet-152 2 yields a feature size of 2,04820482,048-D, which is the average pooling layer of ResNet-152 and it is just the second best existing method on the Event-8 dataset.\nIn terms of the size of our features, we have the least features size than all other features. The size of our fused features (C​C​F𝐶𝐶𝐹CCF) on the MIT-67, Scene-15 and Event-8 are 1,53615361,536-D, <811absent811<811-D and <475absent475<475-D, respectively. The features size of images on the Scene-15 and Event-8 is not fixed because we used 101010 sets of train/test splits for each dataset and each set in the corresponding dataset has varied length of filter words, thus leading to varied sizes of tag-based context features. This impacts the final features size.\nAlso, the features size reduction through PCA to match the smallest size assists to decrease the final features size. Thus, our features size is significantly lower than existing methods’ features size. Despite this, our features can still enable better classification performance than existing methods, since our fused features account for both context and content information."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Coordinate cnns and lstms to categorize scene images with multi-views and multi-levels of abstraction",
      "abstract": "",
      "year": "2019",
      "venue": "Expert Systems with Applications",
      "authors": "S. Bai, H. Tang, and S. An"
    },
    {
      "index": 1,
      "title": "Fast algorithms for sorting and searching strings",
      "abstract": "",
      "year": "1997",
      "venue": "ACM-SIAM Symposium on Discrete Algorithms",
      "authors": "J. L Bentley and R. Sedgewick"
    },
    {
      "index": 2,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov",
      "orig_title": "Enriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 3,
      "title": "Keras",
      "abstract": "",
      "year": "2015",
      "venue": "https://github.com/fchollet/keras",
      "authors": "François Chollet et al."
    },
    {
      "index": 4,
      "title": "Histograms of oriented gradients for human detection",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "N. Dalal and B. Triggs"
    },
    {
      "index": 5,
      "title": "ImageNet: a large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei"
    },
    {
      "index": 6,
      "title": "A bayesian hierarchical model for learning natural scene categories",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. and Pattern Recognit. (CVPR)",
      "authors": "L. Fei-Fei and P. Perona"
    },
    {
      "index": 7,
      "title": "Iot security based on iris verification using multi-algorithm feature level fusion scheme",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Computer Applications & Information Security (ICCAIS)",
      "authors": "R. Gad, AA Abd El-Latif, S. Elseuofi, HM Ibrahim, M. Elmezain, and W. Said"
    },
    {
      "index": 8,
      "title": "Multi-Scale Orderless Pooling of Deep Convolutional Activation Features",
      "abstract": "",
      "year": "2014",
      "venue": "Eur. Conf. Comput. Vis. (ECCV)",
      "authors": "Y. Gong, L. Wang, R. Guo, and S. Lazebnik",
      "orig_title": "Multi-scale orderless pooling of deep convolutional activation features",
      "paper_id": "1403.1840v3"
    },
    {
      "index": 9,
      "title": "Bag of surrogate parts: one inherent feature of deep cnns",
      "abstract": "",
      "year": "2016",
      "venue": "BMVC",
      "authors": "Y. Guo and M. S. Lew"
    },
    {
      "index": 10,
      "title": "Bag of surrogate parts feature for visual recognition",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Trans. Multimedia",
      "authors": "Y. Guo, Y. Liu, S. Lao, E. M. Bakker, L. Bai, and M. S. Lew"
    },
    {
      "index": 11,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 12,
      "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Int. Conf. Multimedia",
      "authors": "Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell",
      "orig_title": "Caffe: Convolutional architecture for fast feature embedding",
      "paper_id": "1408.5093v1"
    },
    {
      "index": 13,
      "title": "Saliency detection based on integrated features",
      "abstract": "",
      "year": "2014",
      "venue": "Neurocomputing",
      "authors": "H. Jing, X. He, Q. Han, AA Abd El-Latif, and X. Niu"
    },
    {
      "index": 14,
      "title": "Blocks that shout: Distinctive parts for scene classification",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman"
    },
    {
      "index": 15,
      "title": "Keras-vgg16-places365",
      "abstract": "",
      "year": "2017",
      "venue": "https://github.com/GKalliatakis/Keras-VGG16-places365",
      "authors": "G. Kalliatakis"
    },
    {
      "index": 16,
      "title": "Convolutional Neural Networks for Sentence Classification",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1408.5882",
      "authors": "Y. Kim",
      "orig_title": "Convolutional neural networks for sentence classification",
      "paper_id": "1408.5882v2"
    },
    {
      "index": 17,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 18,
      "title": "When naive bayes nearest neighbors meet convolutional neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "I. Kuzborskij, F. Maria Carlucci, and B. Caputo"
    },
    {
      "index": 19,
      "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
      "authors": "S. Lazebnik, C. Schmid, and J. Ponce"
    },
    {
      "index": 20,
      "title": "What, where and who? classifying events by scene and object recognition",
      "abstract": "",
      "year": "2007",
      "venue": "ICCV",
      "authors": "L.-J. Li and F.-F. Li"
    },
    {
      "index": 21,
      "title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification",
      "abstract": "",
      "year": "2010",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing"
    },
    {
      "index": 22,
      "title": "Learning important spatial pooling regions for scene classification",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "D. Lin, C. Lu, R. Liao, and J. Jia"
    },
    {
      "index": 23,
      "title": "Otc: A novel local descriptor for scene classification",
      "abstract": "",
      "year": "2014",
      "venue": "Eur. Conf. Comput. Vis. (ECCV)",
      "authors": "R. Margolin, L. Zelnik-Manor, and A. Tal"
    },
    {
      "index": 24,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1301.3781",
      "authors": "T. Mikolov, K. Chen, G. Corrado, and J. Dean"
    },
    {
      "index": 25,
      "title": "Wordnet: a lexical database for english",
      "abstract": "",
      "year": "1995",
      "venue": "Commun. ACM",
      "authors": "G. A. Miller"
    },
    {
      "index": 26,
      "title": "Gist of the scene",
      "abstract": "",
      "year": "2005",
      "venue": "Neurobiology of Attention",
      "authors": "A. Oliva"
    },
    {
      "index": 27,
      "title": "Modeling the shape of the scene: a holistic representation of the spatial envelope",
      "abstract": "",
      "year": "2001",
      "venue": "Int. J. Comput. Vis.",
      "authors": "A. Oliva and A. Torralba"
    },
    {
      "index": 28,
      "title": "Reconfigurable models for scene recognition",
      "abstract": "",
      "year": "2012",
      "venue": "Comput. Vis. Pattern Recognit.(CVPR)",
      "authors": "N. Parizi, J. G. Oberlin, and P. F. Felzenszwalb"
    },
    {
      "index": 29,
      "title": "Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package",
      "abstract": "",
      "year": "2018",
      "venue": "Conf. on Empirical Methods in Natural Language Processing: System Demonstrations",
      "authors": "A. Patel, A. Sands, C. Callison-Burch, and M. Apidianaki",
      "orig_title": "Magnitude: A fast, efficient universal vector embedding utility package",
      "paper_id": "1810.11190v1"
    },
    {
      "index": 30,
      "title": "Linear discriminant multi-set canonical correlations analysis (ldmcca): an efficient approach for feature fusion of finger biometrics",
      "abstract": "",
      "year": "2015",
      "venue": "Multimedia Tools and Applications",
      "authors": "J. Peng, Q. Li, AA Abd El-Latif, and X. Niu"
    },
    {
      "index": 31,
      "title": "Finger-vein verification using gabor filter and sift feature matching",
      "abstract": "",
      "year": "2012",
      "venue": "Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing",
      "authors": "J. Peng, N. Wang, AA Abd El-Latif, Q. Li, and X. Niu"
    },
    {
      "index": 32,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "2014 Conf. on empirical methods in natural language processing (EMNLP)",
      "authors": "J. Pennington, R. Socher, and C. Manning"
    },
    {
      "index": 33,
      "title": "Improving the fisher kernel for large-scale image classification",
      "abstract": "",
      "year": "2010",
      "venue": "European Conference on Computer vision (ECCV)",
      "authors": "F. Perronnin, J. Sánchez, and T. Mensink"
    },
    {
      "index": 34,
      "title": "Recognizing indoor scenes",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "A. Quattoni and A. Torralba"
    },
    {
      "index": 35,
      "title": "Local features are not lonely–laplacian sparse coding for image classification",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "I.-H. ShenghuaGao and P. Liang-TienChia"
    },
    {
      "index": 36,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 37,
      "title": "Fusion of whole and part features for the classification of histopathological image of breast tissue",
      "abstract": "",
      "year": "2020",
      "venue": "Health Information Science and Systems",
      "authors": "C. Sitaula and S. Aryal"
    },
    {
      "index": 38,
      "title": "Attention-based vgg-16 model for covid-19 chest x-ray image classification",
      "abstract": "",
      "year": "2020",
      "venue": "Applied Intelligence",
      "authors": "C. Sitaula and MB Hossain"
    },
    {
      "index": 39,
      "title": "Unsupervised Deep Features for Privacy Image Classification",
      "abstract": "",
      "year": "2019",
      "venue": "Pacific-Rim Symposium on Image and Video Technology (PSIVT)",
      "authors": "C. Sitaula, Y. Xiang, S. Aryal, and X. Lu",
      "orig_title": "Unsupervised deep features for privacy image classification",
      "paper_id": "1909.10708v1"
    },
    {
      "index": 40,
      "title": "Tag-based semantic features for scene image classification",
      "abstract": "",
      "year": "2019",
      "venue": "Int. Conf. on Neural Inf. Process. (ICONIP)",
      "authors": "C. Sitaula, Y. Xiang, A. Basnet, S. Aryal, and X. Lu"
    },
    {
      "index": 41,
      "title": "HDF: Hybrid Deep Features for Scene Image Representation",
      "abstract": "",
      "year": "2020",
      "venue": "Int. Joint Conf. on Neural Networks (IJCNN)",
      "authors": "C. Sitaula, Y. Xiang, A. Basnet, S. Aryal, and X. Lu",
      "orig_title": "Hdf: Hybrid deep features for scene image representation",
      "paper_id": "2003.09773v1"
    },
    {
      "index": 42,
      "title": "Indoor Image Representation by High-Level Semantic Features",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "C. Sitaula, Y. Xiang, Y. Zhang, X. Lu, and S. Aryal",
      "orig_title": "Indoor image representation by high-level semantic features",
      "paper_id": "1906.04987v3"
    },
    {
      "index": 43,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 44,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit.",
      "authors": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna"
    },
    {
      "index": 45,
      "title": "G-MS2F: GoogLeNet based multi-stage feature fusion of deep CNN for scene recognition",
      "abstract": "",
      "year": "2017",
      "venue": "Neurocomputing",
      "authors": "P. Tang, H. Wang, and S. Kwong"
    },
    {
      "index": 46,
      "title": "Learning semantic text features for web text aided image classification",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Trans. Multimedia",
      "authors": "D. Wang and K. Mao"
    },
    {
      "index": 47,
      "title": "Task-generic semantic convolutional neural network for web text-aided image classification",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "D. Wang and K. Mao"
    },
    {
      "index": 48,
      "title": "CENTRIST: A visual descriptor for scene categorization",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "J. Wu and J. M. Rehg"
    },
    {
      "index": 49,
      "title": "mCENTRIST: a multi-channel feature generation mechanism for scene categorization",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Trans. Image Process.",
      "authors": "Y. Xiao, J. Wu, and J. Yuan"
    },
    {
      "index": 50,
      "title": "Feature fusion: parallel strategy vs. serial strategy",
      "abstract": "",
      "year": "2003",
      "venue": "Pattern recognition",
      "authors": "J. Yang, J.-y. Yang, D. Zhang, and J.-f. Lu"
    },
    {
      "index": 51,
      "title": "Sift descriptors modeling and application in texture image classification",
      "abstract": "",
      "year": "2016",
      "venue": "Int. Conf. Comput. Graphics, Imaging and Visualization (CGiV)",
      "authors": "O. Zeglazi, A. Amine, and M. Rziza"
    },
    {
      "index": 52,
      "title": "Image classification by search with explicitly and implicitly semantic representations",
      "abstract": "",
      "year": "2017",
      "venue": "Information Sciences",
      "authors": "C. Zhang, G. Zhu, Q. Huang, and Q. Tian"
    },
    {
      "index": 53,
      "title": "Places: An image database for deep scene understanding",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1610.02055",
      "authors": "B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva"
    },
    {
      "index": 54,
      "title": "Places: A 10 million image database for scene recognition",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba"
    },
    {
      "index": 55,
      "title": "Large margin learning of upstream scene understanding models",
      "abstract": "",
      "year": "2010",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "J. Zhu, L. Li, L. Fei-Fei, and EP. Xing"
    }
  ]
}