{
  "paper_id": "2401.06465v1",
  "title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test",
  "sections": {
    "experimental setup": "All experiments were implemented in python, using PyTorch  for deep learning. zennit 7 and captum  packages were used to compute explanations. The Quantus  package was utilised to for evaluations with MPRT and extended to implement sMPRT and eMPRT. The MetaQuantus library was employed for meta-evaluation [ref]27. As shown by  , MPRT can be sensitive to the choice of task. We therefore aim to cover a wide range of datasets in this work.\nWe use three image classification datasets in the experiments: ImageNet (ILSVRC2012 9), MNIST and fMNIST . For MNIST and fMNIST, we randomly sample 100010001000 test samples and for ImageNet we use the first 100010001000 samples of the validation set for experiments discussed in Figure 2 and randomly select 300300300 test samples for experiments showcased in Figure 3 and Figures 4-12. The experiments are performed using different neural network models, including architectures such as LeNets  and ResNets [ref]29 and VGGs 3,\nwhich contributes to the robustness of our findings. For MNIST and fMNIST, we train LeNets to an accuracy of 98.1498.1498.14% and 87.4487.4487.44% respectively. The training of all models is performed in a similar fashion; employing SGD optimisation with a standard cross-entropy loss, an initial learning rate of 0.0010.0010.001 and momentum of 0.90.90.9. All models are trained for 202020 epochs each. For ILSVRC-15 4 and ILSVRC2012 9, we use the ResNet-18 model [ref]29 and VGG-16 3 with pre-trained weights given the ImageNet dataset, accessible via PyTorch . Explanation Methods and Preprocessing. \nWe evaluated the following explanation methods using the MPRT variants:\nGradients  , Saliency , Input×\\timesGradient , GradCAM , GradientSHAP  with 555 samples, SmoothGrad  (with 20 noisy samples and a noise level of 0.1/(xmax−xmin)0.1subscript𝑥maxsubscript𝑥min0.1/(x_{\\text{max}}-x_{\\text{min}}) as implemented as default in zennit 7, Integrated Gradients  (with 20 iterations and a baseline of zero), Guided Backpropagation  and two distinct variations of Layer-wise Relevance Propagation (LRP)  : application of the LRP-ε𝜀\\varepsilon-rule  to all layers (called LRP-ε𝜀\\varepsilon) and application of the LRP-z+superscript𝑧{z^{+}}-rule  to all layers (called LRP-z+superscript𝑧z^{+}). For random attribution (as a control variant) we sampled from a uniform distribution, i.e., 𝒆^i∼𝒰​(1,0)similar-tosubscript^𝒆𝑖𝒰10\\hat{{\\bm{e}}}_{i}\\sim\\mathcal{U}(1,0). Since preprocessing can significantly affect the results of MPRT , we only take the absolute values for methods where the sign has no meaning in terms of feature importance (Saliency and SmoothGrad) and only consider the positive values for GradCAM, as per the original paper . While the histogram entropy we used for eMPRT already normalises inputs implicitly, the SSIM used to measure explanation difference in MPRT and sMPRT requires normalisation to make attributions at different scales before and after randomisation comparable. For this purpose, we utilised normalisation by the square root of the average second-moment estimate (cf. Supplement of ), as this normalisation does not impose as much additional variance as normalisation by, e.g., maximum value would. The normalisation is defined as follows: where 𝒆^h,wsubscript^𝒆ℎ𝑤\\hat{{\\bm{e}}}_{h,w} is the value of the explanation map at pixel location (hℎh, w𝑤w) and H𝐻H, W𝑊W denote the height and width, respectively333This normalisation technique guarantees that the mean squared distance of each attribution score to zero equals one. As the procedure does not constrain attributions to a fixed range, it is not designed for visual representation but aims to preserve a measure useful for comparing the distances between different explanation methods.. In the original paper , different similarity measures are used e.g., Structural Similarity Index (SSIM) , Spearman Rank Correlation and HOG. We employ SSIM , motivated by its widespread adoption in existing literature   . For the experiments with sMPRT, we employ SSIM  to measure explanation similarity. For generating noise (σ𝜎\\sigma), we used an adaptive standard deviation of 0.2/(xmax−xmin)0.2subscript𝑥maxsubscript𝑥min0.2/(x_{\\text{max}}-x_{\\text{min}}), following the original heuristic presented in . For the number of noise samples (N𝑁N) to average over, we investigated the following values: 1,20,50,300120503001,20,50,300. Evaluation for those values is shown in Supplement 6.3. While a larger N𝑁N generally improves the denoised explanation estimate, it also increases computation time significantly. For Section 3.1, we thus chose N=50𝑁50N=50 as our estimate with acceptable runtime. In our experiments, for Equation 4, we empirically set the bin count B=100𝐵100B=100, which demonstrated robust performance across diverse experiments and explanation methods. Although various statistical rules like Freedman-Diaconis’  and Scotts’  exist for optimising B𝐵B, initial tests indicated inconsistency in their performance, largely due to their assumption of data normality, which is generally not applicable to attribution data. To measure model complexity, we computed the Shannon entropy of the model output (post-softmax) probabilities as follows: To meta-evaluate the metrics, i.e., capture their metric performance characteristics in terms of statistical reliability, we employed the MetaQuantus library . For this, we measured both their resilience to noise (N​R𝑁𝑅NR) and their reactivity to adversary (A​R𝐴𝑅AR) by inducing controlled perturbations, where we applied minor and disruptive perturbations to the input- and model spaces, respectively444We applied i.i.d additive uniform noise such that 𝒙^i=𝒙+𝜹i​with​𝜹i∼𝒰​(α,β)subscript^𝒙𝑖𝒙subscript𝜹𝑖withsubscript𝜹𝑖similar-to𝒰𝛼𝛽\\hat{{\\bm{x}}}_{i}={\\bm{x}}+\\bm{\\delta}_{i}\\ \\text{with}\\ \\bm{\\delta}_{i}\\sim{\\mathcal{U}}(\\alpha,\\beta) for the Input Perturbation Test and applied multiplicative Gaussian noise to all weights of the network, i.e., 𝜽^i=𝜽⋅𝝂i​with​𝝂i∼𝒩​(𝝁,𝚺)subscript^𝜽𝑖⋅𝜽subscript𝝂𝑖withsubscript𝝂𝑖similar-to𝒩𝝁𝚺\\hat{\\bm{\\theta}}_{i}=\\bm{\\theta}\\cdot\\bm{\\nu}_{i}\\ \\text{with}\\ \\bm{\\nu}_{i}\\sim\\mathcal{N}(\\bm{\\mu},\\bm{\\Sigma}) for the Model Perturbation Test. The hyperparameters α,β,μ,Σ𝛼𝛽𝜇Σ\\alpha,\\beta,\\mu,\\Sigma were set according to the original publication [ref]27.. Here, we computed the intra- (𝐈𝐀𝐂𝐈𝐀𝐂\\mathbf{IAC}) and inter-consistency (𝐈𝐄𝐂𝐈𝐄𝐂\\mathbf{IEC}) scores, which include measuring the similarity in score distributions and ranking of different explanation methods (post-perturbation), respectively. Each metric received a summarising meta-consistency (MC) score with MC∈ MC01\\text{MC}\\in : with 𝒎∗=ℝ4superscript𝒎superscriptℝ4{\\bm{m}}^{*}=\\mathbb{R}^{4}\nrepresents an optimally performing quality estimator\nas defined by the all-one indicator vector. For exact definitions for the contents of the meta-evaluation vector 𝐦𝐦\\mathbf{m}, we refer to the original publication [ref]27. For empirical assessment, we utilised the pre-existing tests available in the MetaQuantus library  and their associated hyperparameters. The existing MetaQuatus tests are accessible via the GitHub repository555Code at: https://github.com/annahedstroem/MetaQuantus/.. All metrics have been implemented in Quantus . We executed these metrics over K=5𝐾5K=5 perturbations, spanning 333 iterations with the test configurations as outlined in the notebook666See hyperparameter settings at notebook: https://github.com/annahedstroem/MetaQuantus/blob/main/tutorials/Tutorial-Getting-Started-with-MetaQuantus.ipynb. To ensure a fair comparison among the metrics, we adhered to uniform hyperparameter settings, as specified by the normalisation formula in Equation 5. Given that the MetaQuantus library necessitates each metric to yield a single quality estimate q^∈ℝ^𝑞ℝ\\hat{q}\\in\\mathbb{R} per sample, we calculated the correlation coefficient for the fully randomised models in both MPRT and sMPRT, which is anticipated to enhance their performance."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Do backpropagation trained neural networks have normal weight distributions?",
      "abstract": "",
      "year": "1993",
      "venue": "ICANN ’93",
      "authors": "I. Bellido and E. Fiesler"
    },
    {
      "index": 1,
      "title": "Framework for Evaluating Faithfulness of Local Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Dasgupta, N. Frost, and M. Moshkovitz",
      "orig_title": "Framework for evaluating faithfulness of local explanations",
      "paper_id": "2202.00734v1"
    },
    {
      "index": 2,
      "title": "Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Hedström, L. Weber, D. Krakowczyk, D. Bareeva, F. Motzkus, W. Samek, S. Lapuschkin, and M. M.-C. Höhne",
      "orig_title": "Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations and beyond",
      "paper_id": "2202.06861v3"
    },
    {
      "index": 3,
      "title": "Methods for interpreting and understanding deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "Digit. Signal Process.",
      "authors": "G. Montavon, W. Samek, and K. Müller"
    },
    {
      "index": 4,
      "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "D. Alvarez-Melis and T. S. Jaakkola",
      "orig_title": "Towards robust interpretability with self-explaining neural networks",
      "paper_id": "1806.07538v2"
    },
    {
      "index": 5,
      "title": "On the (In)fidelity and Sensitivity of Explanations",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "C. Yeh, C. Hsieh, A. S. Suggala, D. I. Inouye, and P. Ravikumar",
      "orig_title": "On the (in)fidelity and sensitivity of explanations",
      "paper_id": "1901.09392v4"
    },
    {
      "index": 6,
      "title": "Evaluating and Aggregating Feature-based Model Explanations",
      "abstract": "",
      "year": "2020",
      "venue": "International Joint Conference on Artificial Intelligence, IJCAI 2020",
      "authors": "U. Bhatt, A. Weller, and J. M. F. Moura",
      "orig_title": "Evaluating and aggregating feature-based model explanations",
      "paper_id": "2005.00631v1"
    },
    {
      "index": 7,
      "title": "Rethinking Stability for Attribution-based Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "C. Agarwal, N. Johnson, M. Pawelczyk, S. Krishna, E. Saxena, M. Zitnik, and H. Lakkaraju",
      "orig_title": "Rethinking stability for attribution-based explanations",
      "paper_id": "2203.06877v1"
    },
    {
      "index": 8,
      "title": "On quantitative aspects of model interpretability",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "A. Nguyen and M. R. Martinez",
      "orig_title": "On quantitative aspects of model interpretability",
      "paper_id": "2007.07584v1"
    },
    {
      "index": 9,
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "abstract": "",
      "year": "2015",
      "venue": "PloS one",
      "authors": "S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek"
    },
    {
      "index": 10,
      "title": "Evaluating the visualization of what a Deep Neural Network has learned",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. Müller",
      "orig_title": "Evaluating the visualization of what a deep neural network has learned",
      "paper_id": "1509.06321v1"
    },
    {
      "index": 11,
      "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations, ICLR 2018",
      "authors": "M. Ancona, E. Ceolini, C. Öztireli, and M. Gross",
      "orig_title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "paper_id": "1711.06104v4"
    },
    {
      "index": 12,
      "title": "IROF: a low resource evaluation metric for explanation methods",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "L. Rieger and L. K. Hansen",
      "orig_title": "IROF: a low resource evaluation metric for explanation methods",
      "paper_id": "2003.08747v1"
    },
    {
      "index": 13,
      "title": "A Consistent and Efficient Evaluation Strategy for Attribution Methods",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning, ICML 2022",
      "authors": "Y. Rong, T. Leemann, V. Borisov, G. Kasneci, and E. Kasneci",
      "orig_title": "A consistent and efficient evaluation strategy for attribution methods",
      "paper_id": "2202.00449v2"
    },
    {
      "index": 14,
      "title": "Sanity checks for saliency maps",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31",
      "authors": "J. Adebayo, J. Gilmer, M. Muelly, I. J. Goodfellow, M. Hardt, and B. Kim"
    },
    {
      "index": 15,
      "title": "When Explanations Lie: Why Many Modified BP Attributions Fail",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning, ICML 2020",
      "authors": "L. Sixt, M. Granz, and T. Landgraf",
      "orig_title": "When explanations lie: Why many modified BP attributions fail",
      "paper_id": "1912.09818v7"
    },
    {
      "index": 16,
      "title": "A Note about: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "M. Sundararajan and A. Taly",
      "orig_title": "A note about: Local explanation methods for deep neural networks lack sensitivity to parameter values",
      "paper_id": "1806.04205v1"
    },
    {
      "index": 17,
      "title": "Investigating Sanity Checks for Saliency Maps with Image and Text Classification",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "N. Kokhlikyan, V. Miglani, B. Alsallakh, M. Martin, and O. Reblitz-Richardson",
      "orig_title": "Investigating sanity checks for saliency maps with image and text classification",
      "paper_id": "2106.07475v1"
    },
    {
      "index": 18,
      "title": "Revisiting Sanity Checks for Saliency Maps",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "G. Yona and D. Greenfeld",
      "orig_title": "Revisiting sanity checks for saliency maps",
      "paper_id": "2110.14297v1"
    },
    {
      "index": 19,
      "title": "Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023",
      "authors": "A. Binder, L. Weber, S. Lapuschkin, G. Montavon, K. Müller, and W. Samek",
      "orig_title": "Shortcomings of top-down randomization-based sanity checks for evaluations of deep neural network explanations",
      "paper_id": "2211.12486v1"
    },
    {
      "index": 20,
      "title": "SmoothGrad: removing noise by adding noise",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "D. Smilkov, N. Thorat, B. Kim, F. B. Viégas, and M. Wattenberg",
      "orig_title": "Smoothgrad: removing noise by adding noise",
      "paper_id": "1706.03825v1"
    },
    {
      "index": 21,
      "title": "Visualizing and understanding convolutional networks",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision - ECCV 2014",
      "authors": "M. D. Zeiler and R. Fergus"
    },
    {
      "index": 22,
      "title": "Axiomatic attribution for deep networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning, ICML 2017",
      "authors": "M. Sundararajan, A. Taly, and Q. Yan"
    },
    {
      "index": 23,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2020",
      "venue": "International Journal of Computer Vision",
      "authors": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 24,
      "title": "NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence, AAAI 2022",
      "authors": "K. Bykov, A. Hedström, S. Nakajima, and M. M. Höhne",
      "orig_title": "Noisegrad - enhancing explanations by introducing stochasticity to model weights",
      "paper_id": "2106.10185v3"
    },
    {
      "index": 25,
      "title": "Are artificial neural networks black boxes?",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. M. Benitez, J. L. Castro, and I. Requena"
    },
    {
      "index": 26,
      "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
      "abstract": "",
      "year": "2023",
      "venue": "Transactions on Machine Learning Research",
      "authors": "A. Hedström, P. Bommer, K. K. Wickstrøm, W. Samek, S. Lapuschkin, and M. M. C. Höhne",
      "orig_title": "The meta-evaluation problem in explainable ai: Identifying reliable estimators with metaquantus",
      "paper_id": "2302.07265v2"
    },
    {
      "index": 27,
      "title": "Concise Explanations of Neural Networks using Adversarial Training",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning, ICML 2020",
      "authors": "P. Chalasani, J. Chen, A. R. Chowdhury, X. Wu, and S. Jha",
      "orig_title": "Concise explanations of neural networks using adversarial training",
      "paper_id": "1810.06583v9"
    },
    {
      "index": 28,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 29,
      "title": "Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "T. Han, S. Srinivas, and H. Lakkaraju",
      "orig_title": "Which explanation should I choose? A function approximation perspective to characterizing post hoc explanations",
      "paper_id": "2206.01254v3"
    },
    {
      "index": 30,
      "title": "Finding the right XAI method — A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science",
      "abstract": "",
      "year": "2023",
      "venue": "CoRR",
      "authors": "P. Bommer, M. Kretschmer, A. Hedström, D. Bareeva, and M. M. Höhne",
      "orig_title": "Finding the right XAI method - A guide for the evaluation and ranking of explainable AI methods in climate science",
      "paper_id": "2303.00652v2"
    },
    {
      "index": 31,
      "title": "A mathematical theory of communication",
      "abstract": "",
      "year": "1948",
      "venue": "Bell Syst. Tech. J.",
      "authors": "C. E. Shannon"
    },
    {
      "index": 32,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations, ICLR 2015",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 33,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 34,
      "title": "Visualizing the impact of feature attribution baselines",
      "abstract": "",
      "year": "2020",
      "venue": "Distill",
      "authors": "P. Sturmfels, S. Lundberg, and S.-I. Lee"
    },
    {
      "index": 35,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 36,
      "title": "Software for dataset-wide xai: From local explanations to global insights with Zennit, CoRelAy, and ViRelAy",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "C. J. Anders, D. Neumann, W. Samek, K.-R. Müller, and S. Lapuschkin"
    },
    {
      "index": 37,
      "title": "Captum: A unified and generic model interpretability library for PyTorch",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh, J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan, and O. Reblitz-Richardson",
      "orig_title": "Captum: A unified and generic model interpretability library for pytorch",
      "paper_id": "2009.07896v1"
    },
    {
      "index": 38,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 39,
      "title": "Mnist handwritten digit database",
      "abstract": "",
      "year": "2010",
      "venue": "ATT Labs",
      "authors": "Y. LeCun, C. Cortes, and C. Burges"
    },
    {
      "index": 40,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "H. Xiao, K. Rasul, and R. Vollgraf",
      "orig_title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 41,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "Proc. IEEE",
      "authors": "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner"
    },
    {
      "index": 42,
      "title": "Visualization of neural networks using saliency maps",
      "abstract": "",
      "year": "1995",
      "venue": "International Conference on Neural Networks (ICNN’95)",
      "authors": "N. J. S. Morch, U. Kjems, L. K. Hansen, C. Svarer, I. Law, B. Lautrup, S. C. Strother, and K. Rehm"
    },
    {
      "index": 43,
      "title": "How to explain individual classification decisions",
      "abstract": "",
      "year": "2010",
      "venue": "J. Mach. Learn. Res.",
      "authors": "D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K. Müller"
    },
    {
      "index": 44,
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations, ICLR 2014",
      "authors": "K. Simonyan, A. Vedaldi, and A. Zisserman"
    },
    {
      "index": 45,
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "A. Shrikumar, P. Greenside, A. Shcherbina, and A. Kundaje"
    },
    {
      "index": 46,
      "title": "A Unified Approach to Interpreting Model Predictions",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30",
      "authors": "S. M. Lundberg and S. Lee",
      "orig_title": "A unified approach to interpreting model predictions",
      "paper_id": "1705.07874v2"
    },
    {
      "index": 47,
      "title": "Striving for Simplicity: The All Convolutional Net",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR (workshop track)",
      "authors": "J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller",
      "orig_title": "Striving for simplicity: The all convolutional net",
      "paper_id": "1412.6806v3"
    },
    {
      "index": 48,
      "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
      "abstract": "",
      "year": "2017",
      "venue": "Pattern Recognit.",
      "authors": "G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K. Müller",
      "orig_title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "paper_id": "1512.02479v1"
    },
    {
      "index": 49,
      "title": "Understanding SSIM",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "J. Nilsson and T. Akenine-Möller"
    },
    {
      "index": 50,
      "title": "On the histogram as a density estimator: L2 theory",
      "abstract": "",
      "year": "1981",
      "venue": "Z. Wahrscheinlichkeitstheorie verw Gebiete",
      "authors": "D. Freedman and P. Diaconis"
    },
    {
      "index": 51,
      "title": "On optimal and data-based histograms",
      "abstract": "",
      "year": "1979",
      "venue": "Biometrika",
      "authors": "D. W. Scott"
    }
  ]
}