{
  "paper_id": "2010.12871v1",
  "title": "Large Scale Legal Text Classification Using Transformer Models",
  "sections": {
    "ii related work": "In connection with the JRC-Acquis dataset, Steinberger et al.  present the “JRC EuroVoc Indexer JEX”,\nby the Joint Research Centre (JRC) of the European Commission.\nThe tool categorizes documents using the EuroVoc taxonomy by employing\na profile-based ranking task; the authors report an F-score between 0.44 and 0.54 depending on the document language.\nBoella et al.  manage to apply a support vector machine approach to the problem by transforming the multi-label classification problem into a single-label problem.\nLiu et al.  present a new family of Convolutional Neural Network (CNN) models tailored for multi-label text classification.\nThey compare their method to a large number of existing approaches on various datasets; for the EurLex/JRC dataset however, another method\n(SLEEC), provided the best results. SLEEC (Sparse Local Embeddings for Extreme Classification) , creates\nlocal distance preserving embeddings which are able to accurately predict infrequently occurring (tail) labels.\nThe results on precision for SLEEC applied in Liu et al.  are P@1: 0.78, P@3: 0.64 and P@5: 0.52 – however, they use a\nprevious version of the JRC-Acquis dataset with only 15.4K documents. Chalkidis et al.  recently published their work on the new EURLEX57K dataset.\nThe dataset will be described in more detail (incl. dataset statistics) in the next sections.\nChalkidis et al. also provide a strong baseline for LMTC on this dataset.\nAmong the tested neural architectures operating on the full documents, they have best results with BIGRUs with label-wise attention.\nAs input representation they use either GloVe 0 embeddings trained on domain text,\nor ELMO embeddings .\nThe authors investigated using only the first zones of the (long) documents for classification, and show that\nthe title and recitals part of each document leads to almost\nthe same performance as considering the full document .\nThis helps to alleviate BERT’s limitation of having a maximum of 512 tokens as input.\nUsing only the first 512 tokens of each document as input, BERT [ref]10 archives the best performance overall.\nThe work of Chalkidis et al. is inspired by You et al.  who experimented with RNN-based methods with self attention\non five LMTC datasets (RCV1, Amazon-13K, Wiki-30K, Wiki-500K, and EUR-Lex-4K).\nSimilar work has been done in the medical domain, Mullenbach et al.  investigate\nlabel-wise attention in LMTC for medical code prediction (on the MIMIC-II and MIMIC-III datasets). In this work, we experiment with BERT, RoBERTa, DistilBERT, XLNet,\nM-BERT and AWD-LSTM. We provide ablation studies to measure the impact of various training strategies and heuristics.\nMoreover, we provide new standardized datasets for further investigation by the research community,\nand leverage the EuroVoc term hierarchy to generate variants of the datasets."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Machine learning in automated text categorization",
      "abstract": "",
      "year": "2002",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "F. Sebastiani"
    },
    {
      "index": 1,
      "title": "Explainable Prediction of Medical Codes from Clinical Text",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.05695",
      "authors": "J. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, and J. Eisenstein",
      "orig_title": "Explainable prediction of medical codes from clinical text",
      "paper_id": "1802.05695v2"
    },
    {
      "index": 2,
      "title": "Few-shot and zero-shot multi-label learning for structured label spaces",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "A. Rios and R. Kavuluru"
    },
    {
      "index": 3,
      "title": "LSHTC: A Benchmark for Large-Scale Text Classification",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.08581",
      "authors": "P. Ioannis et al.",
      "orig_title": "Lshtc: A benchmark for large-scale text classification",
      "paper_id": "1503.08581v1"
    },
    {
      "index": 4,
      "title": "Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain",
      "abstract": "",
      "year": "2010",
      "venue": "Springer",
      "authors": "E. Loza Mencía and J. Fürnkranz"
    },
    {
      "index": 5,
      "title": "Large-Scale Multi-Label Text Classification on EU Legislation",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Meeting of the ACL",
      "authors": "I. Chalkidis, E. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos",
      "orig_title": "Large-scale multi-label text classification on EU legislation",
      "paper_id": "1906.02192v1"
    },
    {
      "index": 6,
      "title": "Eurepean Union Law Website",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 7,
      "title": "Transfer learning in natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "2019 NAACL: Tutorials",
      "authors": "S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf"
    },
    {
      "index": 8,
      "title": "The European Union’s multilingual and multidisciplinary thesaurus",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 9,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 NAACL: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 10,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 11,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01108",
      "authors": "V. Sanh, L. Debut, J. Chaumond, and T. Wolf",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 12,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 13,
      "title": "On the stratification of multi-label data",
      "abstract": "",
      "year": "2011",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "K. Sechidis, G. Tsoumakas, and I. Vlahavas"
    },
    {
      "index": 14,
      "title": "X-bert: extreme multi-label text classification using bidirectional encoder representations from transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.02331",
      "authors": "W.-C. Chang, H.-F. Yu, K. Zhong, Y. Yang, and I. Dhillon"
    },
    {
      "index": 15,
      "title": "Jrc eurovoc indexer jex-a freely available multi-label categorisation tool",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1309.5223",
      "authors": "R. Steinberger, M. Ebrahim, and M. Turchi"
    },
    {
      "index": 16,
      "title": "Linking legal open data: breaking the accessibility and language barrier in european legislation and case law",
      "abstract": "",
      "year": "2015",
      "venue": "15th International Conference on Artificial Intelligence and Law",
      "authors": "G. Boella et al."
    },
    {
      "index": 17,
      "title": "Deep learning for extreme multi-label text classification",
      "abstract": "",
      "year": "2017",
      "venue": "40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": "J. Liu, W.-C. Chang, Y. Wu, and Y. Yang"
    },
    {
      "index": 18,
      "title": "Sparse local embeddings for extreme multi-label classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain"
    },
    {
      "index": 19,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "J. Pennington, R. Socher, and C. D. Manning"
    },
    {
      "index": 20,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "2018 NAACL: Human Language Technologies, Volume 1 (Long Papers)",
      "authors": "M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer",
      "orig_title": "Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 21,
      "title": "Attentionxml: Extreme multi-label text classification with multi-label attention based recurrent neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.01727",
      "authors": "R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu"
    },
    {
      "index": 22,
      "title": "SKOS Simple Knowledge Organization System",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 23,
      "title": "Resource Description Framework",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 24,
      "title": "RDF 1.1 Turtle",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 25,
      "title": "Exploiting eurovoc’s hierarchical structure for classifying legal documents",
      "abstract": "",
      "year": "2019",
      "venue": "OTM Confederated International Conferences” On the Move to Meaningful Internet Systems",
      "authors": "E. Filtz, S. Kirrane, A. Polleres, and G. Wohlgenannt"
    },
    {
      "index": 26,
      "title": "JRC-Acquis",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 27,
      "title": "EURLEX57K dataset",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 28,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.06146",
      "authors": "J. Howard and S. Ruder",
      "orig_title": "Universal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 29,
      "title": "Fastai documentation",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 30,
      "title": "Huggingface transformers",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 31,
      "title": "Legal Documents, Large Multi-Label Text Classification",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 32,
      "title": "Regularizing and Optimizing LSTM Language Models",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.02182",
      "authors": "S. Merity, N. S. Keskar, and R. Socher",
      "orig_title": "Regularizing and optimizing lstm language models",
      "paper_id": "1708.02182v1"
    },
    {
      "index": 33,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 34,
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.02860",
      "authors": "Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov",
      "orig_title": "Transformer-xl: Attentive language models beyond a fixed-length context",
      "paper_id": "1901.02860v3"
    },
    {
      "index": 35,
      "title": "How many labels? determining the number of labels in multi-label text classification",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference of the Cross-Language Evaluation Forum for European Languages",
      "authors": "H. Azarbonyad and M. Marx"
    },
    {
      "index": 36,
      "title": "Multi-label data stratification",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 37,
      "title": "BERT, Multi-Lingual Model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "Huggingface BERT base uncased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 39,
      "title": "Huggingface RoBERTa base model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 40,
      "title": "Huggingface DistilBERT cased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "Huggingface XLNET cased model",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
      "abstract": "",
      "year": "2016",
      "venue": "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "authors": "H. Jain, Y. Prabhu, and M. Varma"
    },
    {
      "index": 43,
      "title": "Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning and its Application to Cross-Lingual Text Classification",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Transactions on Information Systems (TOIS)",
      "authors": "A. Esuli, A. Moreo, and F. Sebastiani",
      "orig_title": "Funnelling: A new ensemble method for heterogeneous transfer learning and its application to cross-lingual text classification",
      "paper_id": "1901.11459v2"
    },
    {
      "index": 44,
      "title": "MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network",
      "abstract": "",
      "year": "2020",
      "venue": "12th Int. Conf. on Agents and Artificial Intelligence - Volume 2: ICAART, INSTICC",
      "authors": "A. Pal, M. Selvakumar, and M. Sankarasubbu",
      "orig_title": "Magnet: Multi-label text classification using attention-based graph neural network",
      "paper_id": "2003.11644v1"
    }
  ]
}