{
  "paper_id": "2006.03217v3",
  "title": "Content and Context Features for Scene Image Representation",
  "sections": {
    "introduction": "Image representation in a machine readable form is an emerging field due to the wide use of camera technology in our daily life [ref]43. For automatic analysis, images are often represented as a set of features. Most prior works in image feature extraction are focused on the content of images     . Features are extracted based on the visual information of images such as pixels, colors, objects, scenes, and so on. Compared to the existing traditional vision-based content features      , content features extracted from the pre-trained deep learning methods [ref]12 4 3  [ref]11  are found to be more effective. Though these content features are shown to work reasonably well in many image processing tasks   , the information they capture may be insufficient to discriminate complex and ambiguous images with inter-class similarities and intra-class dissimilarities such as scene images. Fig.¬†1 shows two images which look very similar but they belong to two different categories (hospital room and bedroom). In such a case, the contextual information of images can provide rich discriminating information. Contextual information about an image can be usually obtained from its annotations or descriptions. Though it is impossible to have descriptions or annotations for all images, such information can be searched from the web for certain types of images like objects and scenes. A very few studies in the literature considered the contextual information in scene image classification 3 . They extracted tag-based features from description/annotations of similar images. Despite that tag-based features produce better results than traditional vision-based content features and are comparable to some deep content features 3 , they still have some limitations. In general, they have a large feature size and low classification accuracy.\nThey use task-generic filter banks, which may miss the relevant tags in most cases. Furthermore, they also suffer from the repetition of semantically similar words such as plane and planes. These issues deteriorate the classification performance of scene images. In this paper, we introduce a new technique to extract tag features based on the pre-trained word embedding vectors   , which are used to define the associations between words/tags semantically. For content-based features, we adopt features extracted from pre-trained deep learning models. In most state-of-the-art methods, deep features are extracted\nfrom the intermediate layers of VGG16 model 7, GoogleNet , ResNet-152 [ref]12, etc.\nNevertheless, normally these features from such pre-trained models are extracted based on the fixed input image size of 224√ó224224224224\\times 224 pixels.  This is because pre-trained VGG-16 models have been trained with images of 224√ó224224224224\\times 224 pixels size during training.\nExisting methods have a limited capability to deal with varied sizes of images and may not yield stable performance in the case of multi-scale (or also called multi-size) images.\n\nAlso, existing deep content features based on ImageNet and Places pre-trained deep learning models impart foreground and background information, respectively. However, both information are equally important for scene image representation.\nAs such, the multi-scale deep features based on foreground and background information are needed to achieve good classification performance.\n In literature, either content-based or context-based features have been mostly used for image classification. Since these two sets of features capture different types of information about images, which may be complementary in many cases, we suppose that their fusion will boost the classification accuracy. Therefore, we propose to combine the two sets of features to produce Content and Context Features (abbreviated as C‚ÄãC‚ÄãFùê∂ùê∂ùêπCCF). The main contributions of this paper are summarized as follows. We propose a novel approach to extract context features, which is tag-based features, of an image from tags in annotations/descriptions of its top kùëòk similar images available on the web.\nFor this, we first design a novel filter bank (codebook) with a size significantly smaller than the number of raw tags by eliminating redundant and outlier tags. Note that before achieving our codebook, we also devise a novel algorithm to perform the stemming of tokens during pre-processing step.\nLast, the tag-based context features (T‚ÄãFùëáùêπTF) of the image are extracted as the histogram based on the codebook, leading to features with a noticeably smaller size compared to existing tag-based context features. For content features, we design multi-scale deep features based on background and foreground information in images. Our deep features are multi-scale\nand have a smaller size compared to existing deep content features. We propose to fuse context and content features (C‚ÄãC‚ÄãFùê∂ùê∂ùêπCCF), to enable the ability of capturing different information in discriminating scene images of different classes. To validate the proposed tag-based (context) features (T‚ÄãFùëáùêπTF), deep (content) features (D‚ÄãFùê∑ùêπDF) and the fusion of the two types of features (C‚ÄãC‚ÄãF=T‚ÄãF+D‚ÄãFùê∂ùê∂ùêπùëáùêπùê∑ùêπCCF=TF+DF), we evaluate their performances against state-of-the-art features in the scene image classification task using Support Vector Machine (SVM) on three commonly used scene image datasets (MIT-67¬†5, Scene-15¬† and Event-8¬†). The results show that our proposed T‚ÄãFùëáùêπTF and D‚ÄãFùê∑ùêπDF produce better results than many existing context and content features, respectively and the combined C‚ÄãC‚ÄãFùê∂ùê∂ùêπCCF produce significantly better results than numerous existing tag-based and deep learning-based features. The remainder of the paper is organized as follows. Section 2 reviews related works in terms of content and context features extraction. Section 3 discusses the proposed methods to extract context-based, content-based and fused features. Section 4 details the implementation and results of our experiments in scene image classification.\nFinally, we conclude the paper and point out potential future work in the last section."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Coordinate cnns and lstms to categorize scene images with multi-views and multi-levels of abstraction",
      "abstract": "",
      "year": "2019",
      "venue": "Expert Systems with Applications",
      "authors": "S. Bai, H. Tang, and S. An"
    },
    {
      "index": 1,
      "title": "Fast algorithms for sorting and searching strings",
      "abstract": "",
      "year": "1997",
      "venue": "ACM-SIAM Symposium on Discrete Algorithms",
      "authors": "J. L Bentley and R. Sedgewick"
    },
    {
      "index": 2,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov",
      "orig_title": "Enriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 3,
      "title": "Keras",
      "abstract": "",
      "year": "2015",
      "venue": "https://github.com/fchollet/keras",
      "authors": "Fran√ßois Chollet et al."
    },
    {
      "index": 4,
      "title": "Histograms of oriented gradients for human detection",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "N. Dalal and B. Triggs"
    },
    {
      "index": 5,
      "title": "ImageNet: a large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei"
    },
    {
      "index": 6,
      "title": "A bayesian hierarchical model for learning natural scene categories",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. and Pattern Recognit. (CVPR)",
      "authors": "L. Fei-Fei and P. Perona"
    },
    {
      "index": 7,
      "title": "Iot security based on iris verification using multi-algorithm feature level fusion scheme",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Computer Applications & Information Security (ICCAIS)",
      "authors": "R. Gad, AA Abd El-Latif, S. Elseuofi, HM Ibrahim, M. Elmezain, and W. Said"
    },
    {
      "index": 8,
      "title": "Multi-Scale Orderless Pooling of Deep Convolutional Activation Features",
      "abstract": "",
      "year": "2014",
      "venue": "Eur. Conf. Comput. Vis. (ECCV)",
      "authors": "Y. Gong, L. Wang, R. Guo, and S. Lazebnik",
      "orig_title": "Multi-scale orderless pooling of deep convolutional activation features",
      "paper_id": "1403.1840v3"
    },
    {
      "index": 9,
      "title": "Bag of surrogate parts: one inherent feature of deep cnns",
      "abstract": "",
      "year": "2016",
      "venue": "BMVC",
      "authors": "Y. Guo and M. S. Lew"
    },
    {
      "index": 10,
      "title": "Bag of surrogate parts feature for visual recognition",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Trans. Multimedia",
      "authors": "Y. Guo, Y. Liu, S. Lao, E. M. Bakker, L. Bai, and M. S. Lew"
    },
    {
      "index": 11,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 12,
      "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Int. Conf. Multimedia",
      "authors": "Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell",
      "orig_title": "Caffe: Convolutional architecture for fast feature embedding",
      "paper_id": "1408.5093v1"
    },
    {
      "index": 13,
      "title": "Saliency detection based on integrated features",
      "abstract": "",
      "year": "2014",
      "venue": "Neurocomputing",
      "authors": "H. Jing, X. He, Q. Han, AA Abd El-Latif, and X. Niu"
    },
    {
      "index": 14,
      "title": "Blocks that shout: Distinctive parts for scene classification",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman"
    },
    {
      "index": 15,
      "title": "Keras-vgg16-places365",
      "abstract": "",
      "year": "2017",
      "venue": "https://github.com/GKalliatakis/Keras-VGG16-places365",
      "authors": "G. Kalliatakis"
    },
    {
      "index": 16,
      "title": "Convolutional Neural Networks for Sentence Classification",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1408.5882",
      "authors": "Y. Kim",
      "orig_title": "Convolutional neural networks for sentence classification",
      "paper_id": "1408.5882v2"
    },
    {
      "index": 17,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 18,
      "title": "When naive bayes nearest neighbors meet convolutional neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "I. Kuzborskij, F. Maria Carlucci, and B. Caputo"
    },
    {
      "index": 19,
      "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
      "authors": "S. Lazebnik, C. Schmid, and J. Ponce"
    },
    {
      "index": 20,
      "title": "What, where and who? classifying events by scene and object recognition",
      "abstract": "",
      "year": "2007",
      "venue": "ICCV",
      "authors": "L.-J. Li and F.-F. Li"
    },
    {
      "index": 21,
      "title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification",
      "abstract": "",
      "year": "2010",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing"
    },
    {
      "index": 22,
      "title": "Learning important spatial pooling regions for scene classification",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "D. Lin, C. Lu, R. Liao, and J. Jia"
    },
    {
      "index": 23,
      "title": "Otc: A novel local descriptor for scene classification",
      "abstract": "",
      "year": "2014",
      "venue": "Eur. Conf. Comput. Vis. (ECCV)",
      "authors": "R. Margolin, L. Zelnik-Manor, and A. Tal"
    },
    {
      "index": 24,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1301.3781",
      "authors": "T. Mikolov, K. Chen, G. Corrado, and J. Dean"
    },
    {
      "index": 25,
      "title": "Wordnet: a lexical database for english",
      "abstract": "",
      "year": "1995",
      "venue": "Commun. ACM",
      "authors": "G. A. Miller"
    },
    {
      "index": 26,
      "title": "Gist of the scene",
      "abstract": "",
      "year": "2005",
      "venue": "Neurobiology of Attention",
      "authors": "A. Oliva"
    },
    {
      "index": 27,
      "title": "Modeling the shape of the scene: a holistic representation of the spatial envelope",
      "abstract": "",
      "year": "2001",
      "venue": "Int. J. Comput. Vis.",
      "authors": "A. Oliva and A. Torralba"
    },
    {
      "index": 28,
      "title": "Reconfigurable models for scene recognition",
      "abstract": "",
      "year": "2012",
      "venue": "Comput. Vis. Pattern Recognit.(CVPR)",
      "authors": "N. Parizi, J. G. Oberlin, and P. F. Felzenszwalb"
    },
    {
      "index": 29,
      "title": "Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package",
      "abstract": "",
      "year": "2018",
      "venue": "Conf. on Empirical Methods in Natural Language Processing: System Demonstrations",
      "authors": "A. Patel, A. Sands, C. Callison-Burch, and M. Apidianaki",
      "orig_title": "Magnitude: A fast, efficient universal vector embedding utility package",
      "paper_id": "1810.11190v1"
    },
    {
      "index": 30,
      "title": "Linear discriminant multi-set canonical correlations analysis (ldmcca): an efficient approach for feature fusion of finger biometrics",
      "abstract": "",
      "year": "2015",
      "venue": "Multimedia Tools and Applications",
      "authors": "J. Peng, Q. Li, AA Abd El-Latif, and X. Niu"
    },
    {
      "index": 31,
      "title": "Finger-vein verification using gabor filter and sift feature matching",
      "abstract": "",
      "year": "2012",
      "venue": "Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing",
      "authors": "J. Peng, N. Wang, AA Abd El-Latif, Q. Li, and X. Niu"
    },
    {
      "index": 32,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "2014 Conf. on empirical methods in natural language processing (EMNLP)",
      "authors": "J. Pennington, R. Socher, and C. Manning"
    },
    {
      "index": 33,
      "title": "Improving the fisher kernel for large-scale image classification",
      "abstract": "",
      "year": "2010",
      "venue": "European Conference on Computer vision (ECCV)",
      "authors": "F. Perronnin, J. S√°nchez, and T. Mensink"
    },
    {
      "index": 34,
      "title": "Recognizing indoor scenes",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "A. Quattoni and A. Torralba"
    },
    {
      "index": 35,
      "title": "Local features are not lonely‚Äìlaplacian sparse coding for image classification",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "I.-H. ShenghuaGao and P. Liang-TienChia"
    },
    {
      "index": 36,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 37,
      "title": "Fusion of whole and part features for the classification of histopathological image of breast tissue",
      "abstract": "",
      "year": "2020",
      "venue": "Health Information Science and Systems",
      "authors": "C. Sitaula and S. Aryal"
    },
    {
      "index": 38,
      "title": "Attention-based vgg-16 model for covid-19 chest x-ray image classification",
      "abstract": "",
      "year": "2020",
      "venue": "Applied Intelligence",
      "authors": "C. Sitaula and MB Hossain"
    },
    {
      "index": 39,
      "title": "Unsupervised Deep Features for Privacy Image Classification",
      "abstract": "",
      "year": "2019",
      "venue": "Pacific-Rim Symposium on Image and Video Technology (PSIVT)",
      "authors": "C. Sitaula, Y. Xiang, S. Aryal, and X. Lu",
      "orig_title": "Unsupervised deep features for privacy image classification",
      "paper_id": "1909.10708v1"
    },
    {
      "index": 40,
      "title": "Tag-based semantic features for scene image classification",
      "abstract": "",
      "year": "2019",
      "venue": "Int. Conf. on Neural Inf. Process. (ICONIP)",
      "authors": "C. Sitaula, Y. Xiang, A. Basnet, S. Aryal, and X. Lu"
    },
    {
      "index": 41,
      "title": "HDF: Hybrid Deep Features for Scene Image Representation",
      "abstract": "",
      "year": "2020",
      "venue": "Int. Joint Conf. on Neural Networks (IJCNN)",
      "authors": "C. Sitaula, Y. Xiang, A. Basnet, S. Aryal, and X. Lu",
      "orig_title": "Hdf: Hybrid deep features for scene image representation",
      "paper_id": "2003.09773v1"
    },
    {
      "index": 42,
      "title": "Indoor Image Representation by High-Level Semantic Features",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "C. Sitaula, Y. Xiang, Y. Zhang, X. Lu, and S. Aryal",
      "orig_title": "Indoor image representation by high-level semantic features",
      "paper_id": "1906.04987v3"
    },
    {
      "index": 43,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)",
      "authors": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 44,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recognit.",
      "authors": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna"
    },
    {
      "index": 45,
      "title": "G-MS2F: GoogLeNet based multi-stage feature fusion of deep CNN for scene recognition",
      "abstract": "",
      "year": "2017",
      "venue": "Neurocomputing",
      "authors": "P. Tang, H. Wang, and S. Kwong"
    },
    {
      "index": 46,
      "title": "Learning semantic text features for web text aided image classification",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Trans. Multimedia",
      "authors": "D. Wang and K. Mao"
    },
    {
      "index": 47,
      "title": "Task-generic semantic convolutional neural network for web text-aided image classification",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "D. Wang and K. Mao"
    },
    {
      "index": 48,
      "title": "CENTRIST: A visual descriptor for scene categorization",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "J. Wu and J. M. Rehg"
    },
    {
      "index": 49,
      "title": "mCENTRIST: a multi-channel feature generation mechanism for scene categorization",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Trans. Image Process.",
      "authors": "Y. Xiao, J. Wu, and J. Yuan"
    },
    {
      "index": 50,
      "title": "Feature fusion: parallel strategy vs. serial strategy",
      "abstract": "",
      "year": "2003",
      "venue": "Pattern recognition",
      "authors": "J. Yang, J.-y. Yang, D. Zhang, and J.-f. Lu"
    },
    {
      "index": 51,
      "title": "Sift descriptors modeling and application in texture image classification",
      "abstract": "",
      "year": "2016",
      "venue": "Int. Conf. Comput. Graphics, Imaging and Visualization (CGiV)",
      "authors": "O. Zeglazi, A. Amine, and M. Rziza"
    },
    {
      "index": 52,
      "title": "Image classification by search with explicitly and implicitly semantic representations",
      "abstract": "",
      "year": "2017",
      "venue": "Information Sciences",
      "authors": "C. Zhang, G. Zhu, Q. Huang, and Q. Tian"
    },
    {
      "index": 53,
      "title": "Places: An image database for deep scene understanding",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1610.02055",
      "authors": "B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva"
    },
    {
      "index": 54,
      "title": "Places: A 10 million image database for scene recognition",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba"
    },
    {
      "index": 55,
      "title": "Large margin learning of upstream scene understanding models",
      "abstract": "",
      "year": "2010",
      "venue": "Adv. Neural Inf. Process. Syst. (NIPS)",
      "authors": "J. Zhu, L. Li, L. Fei-Fei, and EP. Xing"
    }
  ]
}