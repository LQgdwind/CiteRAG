{
  "paper_id": "2010.15622v1",
  "title": "Low-Variance Policy Gradient Estimation with World Models",
  "sections": {
    "policy gradient methods": "Gradient-based policy search defines a policy function πθsubscript𝜋𝜃\\pi_{\\theta} that is differentiable wrt. to its parameters θ𝜃\\theta . Then, θ𝜃\\theta is optimized such that the expected value of starting states is maximized, with expectation taken wrt. to the starting states. Assuming a single starting state, the gradient of the objective can be expressed as ([ref]33; [ref]24): By applying the log-derivative trick, the inner sum can be rewritten as expectation, such that policy gradient at state s∗superscript𝑠s^{*} becomes equal to: In most practical applications, agent can perform only one action before being transitioned to some further state. As a result, the above expectation is often evaluated with single-sample MC, such that ∇θJ^​(θ,s∗)=Qπθ​(s∗,a)​∇θlog⁡πθ​(a|s∗)subscript∇𝜃^𝐽𝜃superscript𝑠superscript𝑄subscript𝜋𝜃superscript𝑠𝑎subscript∇𝜃subscript𝜋𝜃conditional𝑎superscript𝑠\\nabla_{\\theta}\\hat{J}(\\theta,s^{*})=Q^{\\pi_{\\theta}}(s^{*},a)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s^{*}) with a∼πθsimilar-to𝑎subscript𝜋𝜃a\\sim\\pi_{\\theta} (; [ref]24; [ref]18). Given single-sample estimation with some action aisubscript𝑎𝑖a_{i}, the sign of J^​(θ,s∗)^𝐽𝜃superscript𝑠\\hat{J}(\\theta,s^{*}) depends solely on the sign of Qπθ​(s∗,ai)superscript𝑄subscript𝜋𝜃superscript𝑠subscript𝑎𝑖Q^{\\pi_{\\theta}}(s^{*},a_{i}). Furthermore, Qπθ​(s∗,a)superscript𝑄subscript𝜋𝜃superscript𝑠𝑎Q^{\\pi_{\\theta}}(s^{*},a) might itself be unknown. If so, Q^πθ​(s∗,a)superscript^𝑄subscript𝜋𝜃superscript𝑠𝑎\\hat{Q}^{\\pi_{\\theta}}(s^{*},a) can be calculated simultaneously with ∇θJ^​(θ,s∗)subscript∇𝜃^𝐽𝜃superscript𝑠\\nabla_{\\theta}\\hat{J}(\\theta,s^{*}) using value approximation techniques like MC policy rollout, TD(n𝑛n) or TD(λ𝜆\\lambda) (; ; ). Given some method of Q-value approximation, the non-zero variance of ∇θJ^​(θ,s∗)subscript∇𝜃^𝐽𝜃superscript𝑠\\nabla_{\\theta}\\hat{J}(\\theta,s^{*}) can be reduced with an additive control variate b​(s)𝑏𝑠b(s). Then, the term (Q^πθ​(s∗,a)−b​(s))superscript^𝑄subscript𝜋𝜃superscript𝑠𝑎𝑏𝑠(\\hat{Q}^{\\pi_{\\theta}}(s^{*},a)-b(s)) is referred to as advantage. Having calculated ∇θJ^​(θ,s)subscript∇𝜃^𝐽𝜃𝑠\\nabla_{\\theta}\\hat{J}(\\theta,s) for every state in batch Dssubscript𝐷𝑠D_{s}, the scalar loss is calculated with the batch average ∇θJ^​(θ)=1|Ds|​∑s∈Ds∇θJ^​(θ,s)subscript∇𝜃^𝐽𝜃1subscript𝐷𝑠subscript𝑠subscript𝐷𝑠subscript∇𝜃^𝐽𝜃𝑠\\nabla_{\\theta}\\hat{J}(\\theta)=\\frac{1}{|D_{s}|}\\sum_{s\\in D_{s}}\\nabla_{\\theta}\\hat{J}(\\theta,s)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Mean actor critic",
      "abstract": "",
      "year": "2017",
      "venue": "stat",
      "authors": "Asadi, K., Allen, C., Roderick, M., Mohamed, A.-r., Konidaris, G., Littman, M., and Amazon, B. U."
    },
    {
      "index": 1,
      "title": "CoPhy: Counterfactual Learning of Physical Dynamics",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Baradel, F., Neverova, N., Mille, J., Mori, G., and Wolf, C.",
      "orig_title": "Cophy: Counterfactual learning of physical dynamics",
      "paper_id": "1909.12000v2"
    },
    {
      "index": 2,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M."
    },
    {
      "index": 3,
      "title": "Learning Discrete State Abstractions With Deep Variational Inference∗",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.04300",
      "authors": "Biza, O., Platt, R., van de Meent, J.-W., and Wong, L. L.",
      "orig_title": "Learning discrete state abstractions with deep variational inference",
      "paper_id": "2003.04300v3"
    },
    {
      "index": 4,
      "title": "Model-based value estimation for efficient model-free reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.00101",
      "authors": "Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S."
    },
    {
      "index": 5,
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M. G.",
      "orig_title": "Deepmdp: Learning continuous latent space models for representation learning",
      "paper_id": "1906.02736v1"
    },
    {
      "index": 6,
      "title": "Recurrent World Models Facilitate Policy Evolution",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ha, D. and Schmidhuber, J.",
      "orig_title": "Recurrent world models facilitate policy evolution",
      "paper_id": "1809.01999v1"
    },
    {
      "index": 7,
      "title": "Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "UC Berkeley",
      "authors": "Haarnoja, T."
    },
    {
      "index": 8,
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.",
      "orig_title": "Dream to control: Learning behaviors by latent imagination",
      "paper_id": "1912.01603v3"
    },
    {
      "index": 9,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Hafner, D., Lillicrap, T. P., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J.",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 10,
      "title": "Advances in importance sampling",
      "abstract": "",
      "year": "1988",
      "venue": "Citeseer",
      "authors": "Hesterberg, T. C."
    },
    {
      "index": 11,
      "title": "A generalization of sampling without replacement from a finite universe",
      "abstract": "",
      "year": "1952",
      "venue": "Journal of the American statistical Association",
      "authors": "Horvitz, D. G. and Thompson, D. J."
    },
    {
      "index": 12,
      "title": "Deep Variational Reinforcement Learning for POMDPs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Igl, M., Zintgraf, L., Le, T. A., Wood, F., and Whiteson, S.",
      "orig_title": "Deep variational reinforcement learning for pomdps",
      "paper_id": "1806.02426v1"
    },
    {
      "index": 13,
      "title": "Model Based Reinforcement Learning for Atari",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Kaiser, Ł., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al.",
      "orig_title": "Model based reinforcement learning for atari",
      "paper_id": "1903.00374v5"
    },
    {
      "index": 14,
      "title": "A natural policy gradient",
      "abstract": "",
      "year": "2002",
      "venue": "Advances in neural information processing systems",
      "authors": "Kakade, S. M."
    },
    {
      "index": 15,
      "title": "Auto-encoding variational bayes",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "Kingma, D. P. and Welling, M."
    },
    {
      "index": 16,
      "title": "Contrastive Learning of Structured World Models",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Kipf, T., van der Pol, E., and Welling, M.",
      "orig_title": "Contrastive learning of structured world models",
      "paper_id": "1911.12247v2"
    },
    {
      "index": 17,
      "title": "Buy 4 reinforce samples, get a baseline for free!",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Kool, W., van Hoof, H., and Welling, M."
    },
    {
      "index": 18,
      "title": "Stochastic Beams and Where to Find Them: The Gumbel-Top-𝑘 Trick for Sampling Sequences Without Replacement",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Kool, W., Van Hoof, H., and Welling, M.",
      "orig_title": "Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement",
      "paper_id": "1903.06059v2"
    },
    {
      "index": 19,
      "title": "Bisimulation through probabilistic testing",
      "abstract": "",
      "year": "1991",
      "venue": "Information and computation",
      "authors": "Larsen, K. G. and Skou, A."
    },
    {
      "index": 20,
      "title": "Towards a unified theory of state abstraction for mdps",
      "abstract": "",
      "year": "2006",
      "venue": "ISAIM",
      "authors": "Li, L., Walsh, T. J., and Littman, M. L."
    },
    {
      "index": 21,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Lin, L.-J."
    },
    {
      "index": 22,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al."
    },
    {
      "index": 23,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K."
    },
    {
      "index": 24,
      "title": "Value Prediction Network",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Oh, J., Singh, S., and Lee, H.",
      "orig_title": "Value prediction network",
      "paper_id": "1707.03497v2"
    },
    {
      "index": 25,
      "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.08265",
      "authors": "Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al.",
      "orig_title": "Mastering atari, go, chess and shogi by planning with a learned model",
      "paper_id": "1911.08265v2"
    },
    {
      "index": 26,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 27,
      "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P.",
      "orig_title": "High-dimensional continuous control using generalized advantage estimation",
      "paper_id": "1506.02438v6"
    },
    {
      "index": 28,
      "title": "Planning to explore via self-supervised world models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.05960",
      "authors": "Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D."
    },
    {
      "index": 29,
      "title": "Without-replacement sampling for particle methods on finite state spaces",
      "abstract": "",
      "year": "2018",
      "venue": "Statistics and Computing",
      "authors": "Shah, R. and Kroese, D. P."
    },
    {
      "index": 30,
      "title": "Incremental sampling without replacement for sequence models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.09067",
      "authors": "Shi, K., Bieber, D., and Sutton, C."
    },
    {
      "index": 31,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "nature",
      "authors": "Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al."
    },
    {
      "index": 32,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y."
    },
    {
      "index": 33,
      "title": "Plannable Approximations to MDP Homomorphisms: Equivariance under Actions",
      "abstract": "",
      "year": "2020",
      "venue": "19th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "van der Pol, E., Kipf, T., Oliehoek, F. A., and Welling, M.",
      "orig_title": "Plannable approximations to mdp homomorphisms: Equivariance under actions",
      "paper_id": "2002.11963v1"
    },
    {
      "index": 34,
      "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Nature",
      "authors": "Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al."
    },
    {
      "index": 35,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Williams, R. J."
    },
    {
      "index": 36,
      "title": "Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba, J."
    },
    {
      "index": 37,
      "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M. J., and Levine, S.",
      "orig_title": "Solar: Deep structured representations for model-based reinforcement learning",
      "paper_id": "1808.09105v4"
    }
  ]
}