{
  "paper_id": "2110.11721v2",
  "title": "Projection-Free Stochastic Bi-level Optimization",
  "sections": {
    "i introduction": "We consider the two-level hierarchical optimization problem Here, the outer problem involves minimizing the objective function Q‚Äã(ùê±)ùëÑùê±Q({\\mathbf{x}}) with respect to ùê±ùê±{\\mathbf{x}} over the convex compact constraint set ùí≥‚äÇ‚Ñùmùí≥superscript‚Ñùùëö\\mathcal{X}\\subset\\mathbb{R}^{m}. The objective function is of the form Q‚Äã(ùê±):=F‚Äã(ùê±,ùê≤‚ãÜ‚Äã(ùê±))assignùëÑùê±ùêπùê±superscriptùê≤‚ãÜùê±Q({\\mathbf{x}}):=F({\\mathbf{x}},{\\mathbf{y}}^{\\star}({\\mathbf{x}})), where ùê≤‚ãÜ‚Äã(ùê±)superscriptùê≤‚ãÜùê±{\\mathbf{y}}^{\\star}({\\mathbf{x}}) is a solution of the inner optimization problem, which for a given ùê±ùê±{\\mathbf{x}}, entails minimizing the strongly convex function G‚Äã(ùê≤,ùê±)ùê∫ùê≤ùê±G({\\mathbf{y}},{\\mathbf{x}}) with respect to optimization variable ùê≤ùê≤{\\mathbf{y}}. Observe that for bilevel problems of type ùí´1subscriptùí´1\\mathcal{P}_{1}, the inner and outer problems are inter-dependent and cannot be solved in isolation. Yet, these problems arise in a number of areas, such as meta-learning [ref]1, continual learning , reinforcement learning [ref]3, and hyper-parameter optimization  . Of particular interest are the large-scale or stochastic settings, where the functions FùêπF and Gùê∫G are expectations of random functions with unknown distributions, and are accessible only through their samples. First-order stochastic approximation algorithms have been recently proposed    . The main idea behind these algorithms is to run one gradient descent step in order to solve the inner optimization problem, and subsequently, utilize the updated variable ùê≤ùê≤{\\mathbf{y}} to run another gradient descent step on the outer minimization problem. In some works, such as 0 1, the constraint set ùí≥ùí≥\\mathcal{X} in the outer optimization problem is taken to be ùí≥=‚Ñùmùí≥superscript‚Ñùùëö\\mathcal{X}=\\mathbb{R}^{m}, resulting in a simpler unconstrained outer optimization problem. However, in applications such as meta-learning [ref]1, personalized federated learning 2, and corsets , the constraint set ùí≥ùí≥\\mathcal{X} is a strict subset of ùí≥‚äÇ‚Ñùmùí≥superscript‚Ñùùëö\\mathcal{X}\\subset\\mathbb{R}^{m}. The standard approach to dealing with such constraint sets is to project the updates of the outer optimization problem onto ùí≥ùí≥\\mathcal{X} at every iteration. Though popular and widely used, the projected gradient approaches may not necessarily be practical, for instance, in cases where the projection sub-problem is too expensive to be solved at every iteration. The difficulties surrounding projection-based methods have motivated the development of projection-free algorithms 3, that make use of the Frank-Wolfe (FW) updates 4. These FW-based algorithms only require solving a linear program over ùí≥ùí≥\\mathcal{X}, which could be significantly cheaper than solving a non-linear projection problem, as in the case of ‚Ñì1subscript‚Ñì1\\ell_{1}-norm or nuclear norm ball constraints. Projection-free algorithms for single-level stochastic optimization algorithms are well-known and state-of-the-art algorithms achieve a sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2})   5. These algorithms rely on a recursive gradient tracking approach that allows the samples to be processed sequentially and achieves variance reduction without the use of checkpoints or large batches. Motivated from these developments, we ask the following question: ‚ÄúIs it possible to develop efficient projection-free algorithms for bi-level stochastic optimization problems?‚Äù This work puts forth the Stochastic Bi-level Frank-Wolfe (SBFW) algorithm, which is the first projection-free algorithm for bi-level problems. Further, we also focus on the special class of problems called stochastic compositional problems where the inner optimization problem in (1) is solvable in closed-form 1 0. Although the results developed for bi-level problems are applicable to compositional problems, we provide improved convergence rates for the stochastic compositional problems using a modified analysis (cf. Sec. III-B). It is important to note that, while bilevel optimization problems are more challenging to solve, they offer additional flexibility in formulation, and tend to outperform their single-level counterparts. Of particular interests are sparse bilevel problems, such as sparse MAML (6 7), which involve ‚Ñì1subscript‚Ñì1\\ell_{1} or nuclear norm constraints. Other important applications where projection-free bilevel optimization algorithms apply include image reconstruction 8 and neural network architecture search 6.\nBefore proceeding, we\nprovide an practical example and discuss various challenges\nassociated with solving it efficiently."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Meta-learning with implicit gradients",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.04630",
      "authors": "A. Rajeswaran, C. Finn, S. Kakade, and S. Levine"
    },
    {
      "index": 1,
      "title": "Coresets via Bilevel Optimization for Continual Learning and Streaming",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.03875",
      "authors": "Z. Borsos, M. Mutn·ª≥, and A. Krause",
      "orig_title": "Coresets via bilevel optimization for continual learning and streaming",
      "paper_id": "2006.03875v2"
    },
    {
      "index": 2,
      "title": "Bi-level actor-critic for multi-agent coordination",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "H. Zhang, W. Chen, Z. Huang, M. Li, Y. Yang, W. Zhang, and J. Wang"
    },
    {
      "index": 3,
      "title": "On the Iteration Complexity of Hypergradient Computation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo",
      "orig_title": "On the iteration complexity of hypergradient computation",
      "paper_id": "2006.16218v2"
    },
    {
      "index": 4,
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil",
      "orig_title": "Bilevel programming for hyperparameter optimization and meta-learning",
      "paper_id": "1806.04910v2"
    },
    {
      "index": 5,
      "title": "Bilevel Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "S. Dempe and A. Zemkoho"
    },
    {
      "index": 6,
      "title": "Approximation methods for bilevel programming",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.02246",
      "authors": "S. Ghadimi and M. Wang"
    },
    {
      "index": 7,
      "title": "Provably Faster Algorithms for Bilevel Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.04692",
      "authors": "J. Yang, K. Ji, and Y. Liang",
      "orig_title": "Provably faster algorithms for bilevel optimization",
      "paper_id": "2106.04692v2"
    },
    {
      "index": 8,
      "title": "A single-timescale stochastic bilevel optimization method",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.04671",
      "authors": "T. Chen, Y. Sun, and W. Yin"
    },
    {
      "index": 9,
      "title": "Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Mokhtari, H. Hassani, and A. Karbasi",
      "orig_title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
      "paper_id": "1804.09554v2"
    },
    {
      "index": 10,
      "title": "Efficient projection-free online methods with stochastic recursive gradient.",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "J. Xie, Z. Shen, C. Zhang, B. Wang, and H. Qian"
    },
    {
      "index": 11,
      "title": "One Sample Stochastic Frank-Wolfe",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "M. Zhang, Z. Shen, A. Mokhtari, H. Hassani, and A. Karbasi",
      "orig_title": "One sample stochastic frank-wolfe",
      "paper_id": "1910.04322v1"
    },
    {
      "index": 12,
      "title": "Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions",
      "abstract": "",
      "year": "2017",
      "venue": "Mathematical Programming",
      "authors": "M. Wang, E. X. Fang, and H. Liu"
    },
    {
      "index": 13,
      "title": "Accelerating stochastic composition optimization",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Machine Learning Research",
      "authors": "M. Wang, J. Liu, and E. X. Fang"
    },
    {
      "index": 14,
      "title": "A single timescale stochastic approximation method for nested stochastic optimization",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Optimization",
      "authors": "S. Ghadimi, A. Ruszczynski, and M. Wang"
    },
    {
      "index": 15,
      "title": "Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.10847",
      "authors": "T. Chen, Y. Sun, and W. Yin"
    },
    {
      "index": 16,
      "title": "Bilevel optimization: Nonasymptotic analysis and faster algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.07962",
      "authors": "K. Ji, J. Yang, and Y. Liang"
    },
    {
      "index": 17,
      "title": "A momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv e-prints",
      "authors": "P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang"
    },
    {
      "index": 18,
      "title": "A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.05170",
      "authors": "M. Hong, H.-T. Wai, Z. Wang, and Z. Yang"
    },
    {
      "index": 19,
      "title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.07367",
      "authors": "P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang",
      "orig_title": "A near-optimal algorithm for stochastic bilevel optimization via double-momentum",
      "paper_id": "2102.07367v3"
    },
    {
      "index": 20,
      "title": "Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.13781",
      "authors": "T. Chen, Y. Sun, and W. Yin",
      "orig_title": "Tighter analysis of alternating stochastic gradient method for stochastic nested problems",
      "paper_id": "2106.13781v1"
    },
    {
      "index": 21,
      "title": "Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Fallah, A. Mokhtari, and A. Ozdaglar"
    },
    {
      "index": 22,
      "title": "Revisiting frank-wolfe: Projection-free sparse convex optimization",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "M. Jaggi"
    },
    {
      "index": 23,
      "title": "An algorithm for quadratic programming",
      "abstract": "",
      "year": "1956",
      "venue": "Naval research logistics quarterly",
      "authors": "M. Frank, P. Wolfe et al."
    },
    {
      "index": 24,
      "title": "Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Z. Akhtar and K. Rajawat",
      "orig_title": "Zeroth and first order stochastic frank-wolfe algorithms for constrained optimization",
      "paper_id": "2107.06534v2"
    },
    {
      "index": 25,
      "title": "Enhanced Bilevel Optimization via Bregman Distance",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.12301",
      "authors": "F. Huang and H. Huang",
      "orig_title": "Enhanced bilevel optimization via bregman distance",
      "paper_id": "2107.12301v3"
    },
    {
      "index": 26,
      "title": "Sparse model-agnostic meta-learning algorithm for few-shot learning",
      "abstract": "",
      "year": "2019",
      "venue": "2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)",
      "authors": "S. Gai and D. Wang"
    },
    {
      "index": 27,
      "title": "Bilevel methods for image reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.09610",
      "authors": "C. Crockett and J. A. Fessler"
    },
    {
      "index": 28,
      "title": "Low-rank matrix completion using alternating minimization",
      "abstract": "",
      "year": "2013",
      "venue": "ACM symposium on Theory of computing",
      "authors": "P. Jain, P. Netrapalli, and S. Sanghavi"
    },
    {
      "index": 29,
      "title": "Low-rank quaternion approximation for color image processing",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Y. Chen, X. Xiao, and Y. Zhou"
    },
    {
      "index": 30,
      "title": "Multi-task feature learning",
      "abstract": "",
      "year": "2007",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Evgeniou and M. Pontil"
    },
    {
      "index": 31,
      "title": "Transfer learning for collaborative filtering via a rating-matrix generative model",
      "abstract": "",
      "year": "2009",
      "venue": "26th annual international conference on machine learning",
      "authors": "B. Li, Q. Yang, and X. Xue"
    },
    {
      "index": 32,
      "title": "Simultaneous visual data completion and denoising based on tensor rank and total variation minimization and its primal-dual splitting algorithm",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "T. Yokota and H. Hontani"
    },
    {
      "index": 33,
      "title": "Low-rank matrix completion and denoising under Poisson noise",
      "abstract": "",
      "year": "2021",
      "venue": "Information and Inference: A Journal of the IMA",
      "authors": "A. D. McRae and M. A. Davenport",
      "orig_title": "Low-rank matrix completion and denoising under poisson noise",
      "paper_id": "1907.05325v2"
    },
    {
      "index": 34,
      "title": "Robust video denoising using low rank matrix completion",
      "abstract": "",
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "H. Ji, C. Liu, Z. Shen, and Y. Xu"
    },
    {
      "index": 35,
      "title": "Matrix completion from noisy entries",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Keshavan, A. Montanari, and S. Oh"
    },
    {
      "index": 36,
      "title": "Supervised learning of sparsity-promoting regularizers for denoising",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.05521",
      "authors": "M. T. McCann and S. Ravishankar"
    },
    {
      "index": 37,
      "title": "Motivating bilevel approaches to filter learning: A case study",
      "abstract": "",
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing (ICIP)",
      "authors": "C. Crockett and J. A. Fessler"
    },
    {
      "index": 38,
      "title": "The theory of the market economy",
      "abstract": "",
      "year": "1952",
      "venue": "Oxford University Press",
      "authors": "H. Von Stackelberg and S. H. Von"
    },
    {
      "index": 39,
      "title": "Bilevel optimization and machine learning",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE World Congress on Computational Intelligence",
      "authors": "K. P. Bennett, G. Kunapuli, J. Hu, and J.-S. Pang"
    },
    {
      "index": 40,
      "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.11396",
      "authors": "F. Huang and H. Huang",
      "orig_title": "Biadam: Fast adaptive bilevel optimization methods",
      "paper_id": "2106.11396v4"
    },
    {
      "index": 41,
      "title": "Momentum-Based Variance Reduction in Non-Convex SGD",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.10018",
      "authors": "A. Cutkosky and F. Orabona",
      "orig_title": "Momentum-based variance reduction in non-convex sgd",
      "paper_id": "1905.10018v3"
    },
    {
      "index": 42,
      "title": "Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.13515",
      "authors": "H. Yuan, X. Lian, and J. Liu"
    },
    {
      "index": 43,
      "title": "Stochastic recursive momentum method for non-convex compositional optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.01688",
      "authors": "J. Yang and W. Hu"
    },
    {
      "index": 44,
      "title": "Stochastic Compositional Gradient Descent under Compositional Csonstraints",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.09400",
      "authors": "S. T. Thomdapu, K. Rajawat et al.",
      "orig_title": "Stochastic compositional gradient descent under compositional constraints",
      "paper_id": "2012.09400v4"
    },
    {
      "index": 45,
      "title": "Variance reduced methods for non-convex composition optimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.04416",
      "authors": "L. Liu, J. Liu, and D. Tao"
    },
    {
      "index": 46,
      "title": "Accelerated Method for Stochastic Composition Optimization with Nonsmooth Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Z. Huo, B. Gu, J. Liu, and H. Huang",
      "orig_title": "Accelerated method for stochastic composition optimization with nonsmooth regularization",
      "paper_id": "1711.03937v2"
    },
    {
      "index": 47,
      "title": "Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "H. Yuan, X. Lian, C. J. Li, and J. Liu"
    },
    {
      "index": 48,
      "title": "Robust stochastic approximation approach to stochastic programming",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on optimization",
      "authors": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro"
    },
    {
      "index": 49,
      "title": "Stochastic proximal gradient descent with acceleration techniques",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Nitanda"
    },
    {
      "index": 50,
      "title": "An optimal method for stochastic composite optimization",
      "abstract": "",
      "year": "2012",
      "venue": "Mathematical Programming",
      "authors": "G. Lan"
    },
    {
      "index": 51,
      "title": "Variance-Reduced and Projection-Free Stochastic Optimization",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "E. Hazan and H. Luo",
      "orig_title": "Variance-reduced and projection-free stochastic optimization",
      "paper_id": "1602.02101v2"
    },
    {
      "index": 52,
      "title": "Stochastic frank-wolfe methods for nonconvex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",
      "authors": "S. J. Reddi, S. Sra, B. P√≥czos, and A. Smola"
    },
    {
      "index": 53,
      "title": "Conservative Stochastic Optimization with Expectation Constraints",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Z. Akhtar, A. S. Bedi, and K. Rajawat",
      "orig_title": "Conservative stochastic optimization with expectation constraints",
      "paper_id": "2008.05758v2"
    },
    {
      "index": 54,
      "title": "Stochastic conditional gradient++:(non) convex minimization and continuous submodular maximization",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Optimization",
      "authors": "H. Hassani, A. Karbasi, A. Mokhtari, and Z. Shen"
    },
    {
      "index": 55,
      "title": "A stochastic compositional gradient method using markov samples",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Winter Simulation Conference (WSC)",
      "authors": "M. Wang and J. Liu"
    },
    {
      "index": 56,
      "title": "First-order methods in optimization",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM",
      "authors": "A. Beck"
    },
    {
      "index": 57,
      "title": "Convergence Rate of Frank-Wolfe for Non-Convex Objectives",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1607.00345",
      "authors": "S. Lacoste-Julien",
      "orig_title": "Convergence rate of frank-wolfe for non-convex objectives",
      "paper_id": "1607.00345v1"
    },
    {
      "index": 58,
      "title": "A stochastic composite gradient method with incremental variance reduction",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.10186",
      "authors": "J. Zhang and L. Xiao"
    },
    {
      "index": 59,
      "title": "Investigating Practical Linear Temporal Difference Learning ‚Äã‚Äã‚Äã",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1602.08771",
      "authors": "A. White and M. White",
      "orig_title": "Investigating practical linear temporal difference learning",
      "paper_id": "1602.08771v2"
    }
  ]
}