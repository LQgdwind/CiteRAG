{
  "paper_id": "2204.00049v1",
  "title": "AKF-SR: Adaptive Kalman Filtering-based Successor Representation",
  "sections": {
    "algorithmic complexity": "As explained in the previous sections, the proposed AKF-SR framework consists of four main modules: (i) RBF-based Feature Vector Construction, which maps each state ğ’”ksubscriptğ’”ğ‘˜\\bm{s}_{k} to NRBFsubscriptğ‘RBFN_{\\text{RBF}}-dimensional state feature vector Ï•â€‹(ğ’”k)bold-italic-Ï•subscriptğ’”ğ‘˜\\bm{\\phi}(\\bm{s}_{k}) consisting of basis functions achieved from Eq.Â (14). The state-action feature vector ğâ€‹(ğ’”k,ak)ğsubscriptğ’”ğ‘˜subscriptğ‘ğ‘˜\\bm{\\psi}(\\bm{s}_{k},a_{k}) is then generated by assigning Ï•â€‹(ğ’”k)bold-italic-Ï•subscriptğ’”ğ‘˜\\bm{\\phi}(\\bm{s}_{k}) to the corresponding spot for action aksubscriptğ‘ğ‘˜a_{k} while the feature vector values for the remainder of the actions are set to zero. The whole construction process of state-action feature vector ğâ€‹(ğ’”k,ak)ğsubscriptğ’”ğ‘˜subscriptğ‘ğ‘˜\\bm{\\psi}(\\bm{s}_{k},a_{k}) has a computational complexity of Oâ€‹(NRBFÃ—D3)ğ‘‚subscriptğ‘RBFsuperscriptğ·3O(N_{\\text{RBF}}\\times D^{3}) per iteration, (ii) Reward Learning, which first, estimates the reward weight vector ğœ½ksubscriptğœ½ğ‘˜\\bm{\\theta}_{k} via implementation of NRBFsubscriptğ‘RBFN_{\\text{RBF}} parallel KFs. The global computational complexity (per iteration) of MMAE is Oâ€‹(NKFÃ—L3)ğ‘‚subscriptğ‘KFsuperscriptğ¿3O(N_{\\text{KF}}\\times L^{3}). The measurement mapping function of the reward weight vector is then adapted by updating the means and covariances of basis functions through Eqs.Â (38) andÂ Â (39). Since at each time step, only the mean or covariance is updated, this process results in memory complexity of Oâ€‹(L+D32)ğ‘‚ğ¿superscriptğ·32O(\\frac{L+D^{3}}{2}), (iii) KTD-based SR Learning, which estimates the SR weight matrix ğ‘¾ksubscriptğ‘¾ğ‘˜\\bm{W}_{k} via KTD algorithm. This estimation procedure can be computed in Oâ€‹(L4)ğ‘‚superscriptğ¿4O(L^{4})Â  per iteration, and; (iv) Active Learning Scheme, which selects the action leading to the largest\ndecrease in uncertainty of the state-action value function in Oâ€‹(LÃ—Nactions)ğ‘‚ğ¿subscriptğ‘actionsO(L\\times N_{\\text{actions}}). The global computational complexity (per iteration) of the proposed AKF-SR scheme is, therefore, in Oâ€‹(NRBFÃ—D3+NKFÃ—L3+L4)ğ‘‚subscriptğ‘RBFsuperscriptğ·3subscriptğ‘KFsuperscriptğ¿3superscriptğ¿4O(N_{\\text{RBF}}\\times D^{3}+N_{\\text{KF}}\\times L^{3}+L^{4}). It should be noted that the main focus of this paper is on the improvement of the performance of RL agents by considering their uncertainties/beliefs for choosing different actions. As it has been shown in the manuscript, this uncertainty can be achieved by using KF for the reward function learning and KTD for the SR learning. The proposed MMAE and RGD improve the performances of the used KF by adaptation of its two most important parameters: measurement noise covariance and measurement mapping function. The computational complexities of DQNÂ [ref]37, USRÂ  and SSRÂ  are not specified; however, based on our experiments and since the proposed AKF-SR has much less trainable parameters than the DQNÂ [ref]37, USRÂ , and SSRÂ  frameworks, which all use DNNs with large number of parameters for the learning process, AKF-SR has less computational cost compared to its DNN-based counterparts. As an example, DQN with only one hidden layer of size 646464 has 111401114011140 parameters resulting in high computational complexity and memory requirement. Training with a mini-batch of 323232 on a typical performance GPU shows that it needs over 3.53.53.5 GB of local DRAM."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "An Efficient Hardware Implementation of Reinforcement Learning: The Q-Learning Algorithm",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "S. SpanÃ², G.C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, M. Matta, A. Nannarelli, and M. Re"
    },
    {
      "index": 1,
      "title": "Universal Successor Representations for Transfer Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "C. Ma, J. Wen, and Y. Bengio",
      "orig_title": "Universal successor representations for transfer reinforcement learning",
      "paper_id": "1804.03758v1"
    },
    {
      "index": 2,
      "title": "Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "M. Seo, L.F. Vecchietti, S. Lee, and D. Har"
    },
    {
      "index": 3,
      "title": "MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "P. Malekzadeh, M. Salimibeni, A. Mohammadi, A. Assa and K. N. Plataniotis",
      "orig_title": "MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning",
      "paper_id": "2006.00195v1"
    },
    {
      "index": 4,
      "title": "Modeling behavior of Computer Generated Forces with Machine Learning Techniques, the NATO Task Group approach",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Int. Con. Systems, Man, and Cyb. (SMC)",
      "authors": "A. Toubman et al."
    },
    {
      "index": 5,
      "title": "Machine Learning Techniques for Autonomous Agents in Military Simulations - Multum in Parvo",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Int. Con. Systems, Man, and Cyb. (SMC)",
      "authors": "J. J. Roessingh et al."
    },
    {
      "index": 6,
      "title": "Recovering Robustness in Model-Free Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "American Control Conference (ACC)",
      "authors": "H. K. Venkataraman, and P. J. Seiler",
      "orig_title": "Recovering Robustness in Model-Free Reinforcement Learning",
      "paper_id": "1810.09337v3"
    },
    {
      "index": 7,
      "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "H. Hu, S. Song and C. L. P. Chen"
    },
    {
      "index": 8,
      "title": "Kalman meets Bellman: Improving Policy Evaluation through Value Tracking",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "S.D.C. Shashua, and S. Mannor",
      "orig_title": "Kalman meets bellman: Improving policy evaluation through value tracking",
      "paper_id": "2002.07171v1"
    },
    {
      "index": 9,
      "title": "Information Theoretic MPC for Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou"
    },
    {
      "index": 10,
      "title": "Agnostic System Identification for Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv",
      "authors": "S. Ross and J. A. Bagnell"
    },
    {
      "index": 11,
      "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Ayoub, Z. Jia, C. Szepesvari, M. Wang, M., and L. Yang",
      "orig_title": "Model-based reinforcement learning with value-targeted regression",
      "paper_id": "2006.01107v1"
    },
    {
      "index": 12,
      "title": "A neurally plausible model learns successor representations in partially observable environments",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "E. VÃ©rtes,and M. Sahani",
      "orig_title": "A neurally plausible model learns successor representations in partially observable environments",
      "paper_id": "1906.09480v1"
    },
    {
      "index": 13,
      "title": "A Complementary Learning Systems Approach to Temporal Difference Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Networks",
      "authors": "S. Blakeman, and D. Mareschal",
      "orig_title": "A complementary learning systems approach to temporal difference learning",
      "paper_id": "1905.02636v1"
    },
    {
      "index": 14,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation",
      "authors": "P. Dayan"
    },
    {
      "index": 15,
      "title": "The Successor Representation as a model of behavioural flexibility",
      "abstract": "",
      "year": "2017",
      "venue": "JournÃ©es Francophones sur la Planification, la DÃ©cision et lâ€™Apprentissage pour la conduite de systÃ¨mes (JFPDA)",
      "authors": "A. Ducarouge, O. Sigaud"
    },
    {
      "index": 16,
      "title": "Deep Successor Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint",
      "authors": "T.D. Kulkarni, A. Saeedi, S. Gautam, and S.J. Gershman",
      "orig_title": "Deep successor reinforcement learning",
      "paper_id": "1606.02396v1"
    },
    {
      "index": 17,
      "title": "Neural Fitted Q Iteration-first Experiences with a Data Efficient Neural Reinforcement Learning Method",
      "abstract": "",
      "year": "2005",
      "venue": "European Conference on Machine Learning",
      "authors": "M. Riedmiller"
    },
    {
      "index": 18,
      "title": "Flow Splitter: A Deep Reinforcement Learning-Based Flow Scheduler for Hybrid Optical-Electrical Data Center Network",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "Y. Tang, H. Guo,T. Yuan, X. Gao, X. Hong, Y. Li, J. Qiu, Y. Zuo, and J. Wu"
    },
    {
      "index": 19,
      "title": "Unexpected Collision Avoidance Driving Strategy Using Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "M. Kim, S. Lee, J. Lim, J. Choi, and S.G. Kang"
    },
    {
      "index": 20,
      "title": "Deep reinforcement learning with optimized reward functions for robotic trajectory planning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "J. Xie, Z. Shao, Y. Li, Y. Guan, and J. Tan"
    },
    {
      "index": 21,
      "title": "An analysis of temporal-difference learning with function approximation",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "J. N. Tsitsiklis and B. Van Roy"
    },
    {
      "index": 22,
      "title": "Improved temporal difference methods with linear function approximation",
      "abstract": "",
      "year": "2004",
      "venue": "Learning and Approximate Dynamic Programming",
      "authors": "D.P. Bertsekas, V.S. Borkar, and A. Nedic"
    },
    {
      "index": 23,
      "title": "Cmas: An Associative Neural Network Alternative to Backpropagation",
      "abstract": "",
      "year": "1990",
      "venue": "IEEE",
      "authors": "W. T. Miller, F. H. Glanz, and L. G. Kraft"
    },
    {
      "index": 24,
      "title": "Neural Networks: A Comprehensive Foundation",
      "abstract": "",
      "year": "1994",
      "venue": "Prentice Hall PTR",
      "authors": "S. Haykin"
    },
    {
      "index": 25,
      "title": "Basis Function Adaptation in Temporal Difference Reinforcement Learning",
      "abstract": "",
      "year": "2005",
      "venue": "Annals of Operations Research",
      "authors": "I. Menache, S. Mannor, and N. Shimkin"
    },
    {
      "index": 26,
      "title": "Restricted Gradient-descent Algorithm for Value-function Approximation in Reinforcement Learning",
      "abstract": "",
      "year": "2008",
      "venue": "Artificial Intelligence",
      "authors": "A. d. M. S. Barreto and C. W. Anderson"
    },
    {
      "index": 27,
      "title": "Meta-cognitive neural network for classification problems in a sequential learning framework",
      "abstract": "",
      "year": "2012",
      "venue": "Neurocomputing",
      "authors": "G.S. Babu, S. and Suresh"
    },
    {
      "index": 28,
      "title": "Comparison of CMACs and radial basis functions for local function approximators in reinforcement learning",
      "abstract": "",
      "year": "1997",
      "venue": "International Conference on Neural Networks (ICNN)",
      "authors": "R. M. Kretchmar and C. W. Anderson"
    },
    {
      "index": 29,
      "title": "The successor representation and temporal context",
      "abstract": "",
      "year": "2012",
      "venue": "Neural Computation",
      "authors": "S.J. Gershman, C.D. Moore, M.T. Todd, K.A. Norman, and P.B. Sederberg"
    },
    {
      "index": 30,
      "title": "The successor representation in human reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Nature Human Behaviour",
      "authors": "I. Momennejad, E.M. Russek, J.H. Cheong, M.M. Botvinick, N.D. Daw, and S.J. Gershman"
    },
    {
      "index": 31,
      "title": "Predictive representations can link model-based reinforcement learning to model-free mechanisms",
      "abstract": "",
      "year": "2017",
      "venue": "PLoS computational biology",
      "authors": "E.M. Russek, I. Momennejad, M.M. Botvinick, S.J. Gershman, and N.D. Daw"
    },
    {
      "index": 32,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT Press",
      "authors": "R. S. Sutton, A. G. Barto, F. Bach et al."
    },
    {
      "index": 33,
      "title": "Kalman Temporal Differences",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "M. Geist and O. Pietquin"
    },
    {
      "index": 34,
      "title": "Successor features combine elements of model-free and model-based reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "L. Lehnert, M.L. and Littman"
    },
    {
      "index": 35,
      "title": "Count-Based Exploration with the Successor Representation",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "M.C. Machado, M.G. Bellemare, and M. Bowling",
      "orig_title": "Count-based exploration with the successor representation",
      "paper_id": "1807.11622v4"
    },
    {
      "index": 36,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, et al."
    },
    {
      "index": 37,
      "title": "Measuring the Reliability of Reinforcement Learning Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "S.C. Chan, S. Fishman, J. Canny, et al.",
      "orig_title": "Measuring the reliability of reinforcement learning algorithms",
      "paper_id": "1912.05663v2"
    },
    {
      "index": 38,
      "title": "Probabilistic successor representations with Kalman temporal differences",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "J.P., Geerts, K.L. Stachenfeld, and N. Burgess"
    },
    {
      "index": 39,
      "title": "Makf-Sr: Multi-Agent Adaptive Kalman Filtering-Based Successor Representations",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "M. Salimibeni, P. Malekzadeh, A. Mohammadi, P. Spachos and K. N. Plataniotis"
    },
    {
      "index": 40,
      "title": "Multiple Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2002",
      "venue": "Neural Computation",
      "authors": "K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato"
    },
    {
      "index": 41,
      "title": "Model Selection based on Kalman Temporal Differences Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Collaboration and Internet Computing (CIC)",
      "authors": "T. Kitao, M. Shirai, and T. Miura"
    },
    {
      "index": 42,
      "title": "Adaptive adjustment of noise covariance in Kalman filter for dynamic state estimation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Power & Energy Society General Meeting",
      "authors": "S. Akhlaghi, N. Zhou and Z. Huang"
    },
    {
      "index": 43,
      "title": "On the Identification of Variances and Adaptive Kalman Filtering",
      "abstract": "",
      "year": "1970",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "R. Mehra"
    },
    {
      "index": 44,
      "title": "Partitioning: A Unifying Framework for Adaptive Systems, i: Estimation",
      "abstract": "",
      "year": "1976",
      "venue": "IEEE",
      "authors": "D. G. Lainiotis"
    },
    {
      "index": 45,
      "title": "Similarity-based Multiple Model Adaptive Estimation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "A. Assa and K. N. Plataniotis"
    },
    {
      "index": 46,
      "title": "Adaptive Ïµitalic-Ïµ\\epsilon-greedy exploration in reinforcement learning based on value differences",
      "abstract": "",
      "year": "2010",
      "venue": "Annual Conference on Artificial Intelligence",
      "authors": "T. Michel"
    },
    {
      "index": 47,
      "title": "Temporal Difference Updating without a Learning Rate",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Hutter and S. Legg"
    },
    {
      "index": 48,
      "title": "Reinforcement Learning Based Stochastic Shortest Path Finding in Wireless Sensor Networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "W. Xia, C. Di, H. Guo, and S. Li"
    },
    {
      "index": 49,
      "title": "Successor features for transfer in reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Barreto, R. Dabney, R. Munos, J.J. Hunt, T. Schaul, H.V. Hasselt, and D. Silver"
    },
    {
      "index": 50,
      "title": "STUPEFY: Set-Valued Box Particle Filtering for Bluetooth Low Energy-Based Indoor Localization",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Signal Processing Letters",
      "authors": "P. Malekzadeh, A. Mohammadi, M. Barbulescu and K. N. Plataniotis"
    },
    {
      "index": 51,
      "title": "Event-Based Estimation With Information-Based Triggering and Adaptive Update",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 52,
      "title": "Improper Complex-Valued Bhattacharyya Distance",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 53,
      "title": "Distributed Widely Linear Multiple-Model Adaptive Estimation",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Trans. Signal & Information Processing over Networks",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 54,
      "title": "A General and Adaptive Robust Loss Function",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "J. T. Barron",
      "orig_title": "A General and Adaptive Robust Loss Function",
      "paper_id": "1701.03077v10"
    },
    {
      "index": 55,
      "title": "Kalman filtering for matrix estimation",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Transactions on Aerospace and Electronic Systems",
      "authors": "D. Choukroun, H. Weiss, I. Y. Bar-Itzhack and Y. Oshman"
    },
    {
      "index": 56,
      "title": "A note on the variance of a matrix",
      "abstract": "",
      "year": "1968",
      "venue": "Econometrica",
      "authors": "D.H. Nissen"
    }
  ]
}