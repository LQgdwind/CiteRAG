{
  "paper_id": "2204.00049v1",
  "title": "AKF-SR: Adaptive Kalman Filtering-based Successor Representation",
  "sections": {
    "algorithmic complexity": "As explained in the previous sections, the proposed AKF-SR framework consists of four main modules: (i) RBF-based Feature Vector Construction, which maps each state 𝒔ksubscript𝒔𝑘\\bm{s}_{k} to NRBFsubscript𝑁RBFN_{\\text{RBF}}-dimensional state feature vector ϕ​(𝒔k)bold-italic-ϕsubscript𝒔𝑘\\bm{\\phi}(\\bm{s}_{k}) consisting of basis functions achieved from Eq. (14). The state-action feature vector 𝝍​(𝒔k,ak)𝝍subscript𝒔𝑘subscript𝑎𝑘\\bm{\\psi}(\\bm{s}_{k},a_{k}) is then generated by assigning ϕ​(𝒔k)bold-italic-ϕsubscript𝒔𝑘\\bm{\\phi}(\\bm{s}_{k}) to the corresponding spot for action aksubscript𝑎𝑘a_{k} while the feature vector values for the remainder of the actions are set to zero. The whole construction process of state-action feature vector 𝝍​(𝒔k,ak)𝝍subscript𝒔𝑘subscript𝑎𝑘\\bm{\\psi}(\\bm{s}_{k},a_{k}) has a computational complexity of O​(NRBF×D3)𝑂subscript𝑁RBFsuperscript𝐷3O(N_{\\text{RBF}}\\times D^{3}) per iteration, (ii) Reward Learning, which first, estimates the reward weight vector 𝜽ksubscript𝜽𝑘\\bm{\\theta}_{k} via implementation of NRBFsubscript𝑁RBFN_{\\text{RBF}} parallel KFs. The global computational complexity (per iteration) of MMAE is O​(NKF×L3)𝑂subscript𝑁KFsuperscript𝐿3O(N_{\\text{KF}}\\times L^{3}). The measurement mapping function of the reward weight vector is then adapted by updating the means and covariances of basis functions through Eqs. (38) and  (39). Since at each time step, only the mean or covariance is updated, this process results in memory complexity of O​(L+D32)𝑂𝐿superscript𝐷32O(\\frac{L+D^{3}}{2}), (iii) KTD-based SR Learning, which estimates the SR weight matrix 𝑾ksubscript𝑾𝑘\\bm{W}_{k} via KTD algorithm. This estimation procedure can be computed in O​(L4)𝑂superscript𝐿4O(L^{4})  per iteration, and; (iv) Active Learning Scheme, which selects the action leading to the largest\ndecrease in uncertainty of the state-action value function in O​(L×Nactions)𝑂𝐿subscript𝑁actionsO(L\\times N_{\\text{actions}}). The global computational complexity (per iteration) of the proposed AKF-SR scheme is, therefore, in O​(NRBF×D3+NKF×L3+L4)𝑂subscript𝑁RBFsuperscript𝐷3subscript𝑁KFsuperscript𝐿3superscript𝐿4O(N_{\\text{RBF}}\\times D^{3}+N_{\\text{KF}}\\times L^{3}+L^{4}). It should be noted that the main focus of this paper is on the improvement of the performance of RL agents by considering their uncertainties/beliefs for choosing different actions. As it has been shown in the manuscript, this uncertainty can be achieved by using KF for the reward function learning and KTD for the SR learning. The proposed MMAE and RGD improve the performances of the used KF by adaptation of its two most important parameters: measurement noise covariance and measurement mapping function. The computational complexities of DQN [ref]37, USR  and SSR  are not specified; however, based on our experiments and since the proposed AKF-SR has much less trainable parameters than the DQN [ref]37, USR , and SSR  frameworks, which all use DNNs with large number of parameters for the learning process, AKF-SR has less computational cost compared to its DNN-based counterparts. As an example, DQN with only one hidden layer of size 646464 has 111401114011140 parameters resulting in high computational complexity and memory requirement. Training with a mini-batch of 323232 on a typical performance GPU shows that it needs over 3.53.53.5 GB of local DRAM."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "An Efficient Hardware Implementation of Reinforcement Learning: The Q-Learning Algorithm",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "S. Spanò, G.C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, M. Matta, A. Nannarelli, and M. Re"
    },
    {
      "index": 1,
      "title": "Universal Successor Representations for Transfer Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "C. Ma, J. Wen, and Y. Bengio",
      "orig_title": "Universal successor representations for transfer reinforcement learning",
      "paper_id": "1804.03758v1"
    },
    {
      "index": 2,
      "title": "Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "M. Seo, L.F. Vecchietti, S. Lee, and D. Har"
    },
    {
      "index": 3,
      "title": "MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "P. Malekzadeh, M. Salimibeni, A. Mohammadi, A. Assa and K. N. Plataniotis",
      "orig_title": "MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning",
      "paper_id": "2006.00195v1"
    },
    {
      "index": 4,
      "title": "Modeling behavior of Computer Generated Forces with Machine Learning Techniques, the NATO Task Group approach",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Int. Con. Systems, Man, and Cyb. (SMC)",
      "authors": "A. Toubman et al."
    },
    {
      "index": 5,
      "title": "Machine Learning Techniques for Autonomous Agents in Military Simulations - Multum in Parvo",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Int. Con. Systems, Man, and Cyb. (SMC)",
      "authors": "J. J. Roessingh et al."
    },
    {
      "index": 6,
      "title": "Recovering Robustness in Model-Free Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "American Control Conference (ACC)",
      "authors": "H. K. Venkataraman, and P. J. Seiler",
      "orig_title": "Recovering Robustness in Model-Free Reinforcement Learning",
      "paper_id": "1810.09337v3"
    },
    {
      "index": 7,
      "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "H. Hu, S. Song and C. L. P. Chen"
    },
    {
      "index": 8,
      "title": "Kalman meets Bellman: Improving Policy Evaluation through Value Tracking",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "S.D.C. Shashua, and S. Mannor",
      "orig_title": "Kalman meets bellman: Improving policy evaluation through value tracking",
      "paper_id": "2002.07171v1"
    },
    {
      "index": 9,
      "title": "Information Theoretic MPC for Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou"
    },
    {
      "index": 10,
      "title": "Agnostic System Identification for Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv",
      "authors": "S. Ross and J. A. Bagnell"
    },
    {
      "index": 11,
      "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Ayoub, Z. Jia, C. Szepesvari, M. Wang, M., and L. Yang",
      "orig_title": "Model-based reinforcement learning with value-targeted regression",
      "paper_id": "2006.01107v1"
    },
    {
      "index": 12,
      "title": "A neurally plausible model learns successor representations in partially observable environments",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "E. Vértes,and M. Sahani",
      "orig_title": "A neurally plausible model learns successor representations in partially observable environments",
      "paper_id": "1906.09480v1"
    },
    {
      "index": 13,
      "title": "A Complementary Learning Systems Approach to Temporal Difference Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Networks",
      "authors": "S. Blakeman, and D. Mareschal",
      "orig_title": "A complementary learning systems approach to temporal difference learning",
      "paper_id": "1905.02636v1"
    },
    {
      "index": 14,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation",
      "authors": "P. Dayan"
    },
    {
      "index": 15,
      "title": "The Successor Representation as a model of behavioural flexibility",
      "abstract": "",
      "year": "2017",
      "venue": "Journées Francophones sur la Planification, la Décision et l’Apprentissage pour la conduite de systèmes (JFPDA)",
      "authors": "A. Ducarouge, O. Sigaud"
    },
    {
      "index": 16,
      "title": "Deep Successor Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint",
      "authors": "T.D. Kulkarni, A. Saeedi, S. Gautam, and S.J. Gershman",
      "orig_title": "Deep successor reinforcement learning",
      "paper_id": "1606.02396v1"
    },
    {
      "index": 17,
      "title": "Neural Fitted Q Iteration-first Experiences with a Data Efficient Neural Reinforcement Learning Method",
      "abstract": "",
      "year": "2005",
      "venue": "European Conference on Machine Learning",
      "authors": "M. Riedmiller"
    },
    {
      "index": 18,
      "title": "Flow Splitter: A Deep Reinforcement Learning-Based Flow Scheduler for Hybrid Optical-Electrical Data Center Network",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "Y. Tang, H. Guo,T. Yuan, X. Gao, X. Hong, Y. Li, J. Qiu, Y. Zuo, and J. Wu"
    },
    {
      "index": 19,
      "title": "Unexpected Collision Avoidance Driving Strategy Using Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "M. Kim, S. Lee, J. Lim, J. Choi, and S.G. Kang"
    },
    {
      "index": 20,
      "title": "Deep reinforcement learning with optimized reward functions for robotic trajectory planning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "J. Xie, Z. Shao, Y. Li, Y. Guan, and J. Tan"
    },
    {
      "index": 21,
      "title": "An analysis of temporal-difference learning with function approximation",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "J. N. Tsitsiklis and B. Van Roy"
    },
    {
      "index": 22,
      "title": "Improved temporal difference methods with linear function approximation",
      "abstract": "",
      "year": "2004",
      "venue": "Learning and Approximate Dynamic Programming",
      "authors": "D.P. Bertsekas, V.S. Borkar, and A. Nedic"
    },
    {
      "index": 23,
      "title": "Cmas: An Associative Neural Network Alternative to Backpropagation",
      "abstract": "",
      "year": "1990",
      "venue": "IEEE",
      "authors": "W. T. Miller, F. H. Glanz, and L. G. Kraft"
    },
    {
      "index": 24,
      "title": "Neural Networks: A Comprehensive Foundation",
      "abstract": "",
      "year": "1994",
      "venue": "Prentice Hall PTR",
      "authors": "S. Haykin"
    },
    {
      "index": 25,
      "title": "Basis Function Adaptation in Temporal Difference Reinforcement Learning",
      "abstract": "",
      "year": "2005",
      "venue": "Annals of Operations Research",
      "authors": "I. Menache, S. Mannor, and N. Shimkin"
    },
    {
      "index": 26,
      "title": "Restricted Gradient-descent Algorithm for Value-function Approximation in Reinforcement Learning",
      "abstract": "",
      "year": "2008",
      "venue": "Artificial Intelligence",
      "authors": "A. d. M. S. Barreto and C. W. Anderson"
    },
    {
      "index": 27,
      "title": "Meta-cognitive neural network for classification problems in a sequential learning framework",
      "abstract": "",
      "year": "2012",
      "venue": "Neurocomputing",
      "authors": "G.S. Babu, S. and Suresh"
    },
    {
      "index": 28,
      "title": "Comparison of CMACs and radial basis functions for local function approximators in reinforcement learning",
      "abstract": "",
      "year": "1997",
      "venue": "International Conference on Neural Networks (ICNN)",
      "authors": "R. M. Kretchmar and C. W. Anderson"
    },
    {
      "index": 29,
      "title": "The successor representation and temporal context",
      "abstract": "",
      "year": "2012",
      "venue": "Neural Computation",
      "authors": "S.J. Gershman, C.D. Moore, M.T. Todd, K.A. Norman, and P.B. Sederberg"
    },
    {
      "index": 30,
      "title": "The successor representation in human reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Nature Human Behaviour",
      "authors": "I. Momennejad, E.M. Russek, J.H. Cheong, M.M. Botvinick, N.D. Daw, and S.J. Gershman"
    },
    {
      "index": 31,
      "title": "Predictive representations can link model-based reinforcement learning to model-free mechanisms",
      "abstract": "",
      "year": "2017",
      "venue": "PLoS computational biology",
      "authors": "E.M. Russek, I. Momennejad, M.M. Botvinick, S.J. Gershman, and N.D. Daw"
    },
    {
      "index": 32,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT Press",
      "authors": "R. S. Sutton, A. G. Barto, F. Bach et al."
    },
    {
      "index": 33,
      "title": "Kalman Temporal Differences",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "M. Geist and O. Pietquin"
    },
    {
      "index": 34,
      "title": "Successor features combine elements of model-free and model-based reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "L. Lehnert, M.L. and Littman"
    },
    {
      "index": 35,
      "title": "Count-Based Exploration with the Successor Representation",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "M.C. Machado, M.G. Bellemare, and M. Bowling",
      "orig_title": "Count-based exploration with the successor representation",
      "paper_id": "1807.11622v4"
    },
    {
      "index": 36,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, et al."
    },
    {
      "index": 37,
      "title": "Measuring the Reliability of Reinforcement Learning Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "S.C. Chan, S. Fishman, J. Canny, et al.",
      "orig_title": "Measuring the reliability of reinforcement learning algorithms",
      "paper_id": "1912.05663v2"
    },
    {
      "index": 38,
      "title": "Probabilistic successor representations with Kalman temporal differences",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "J.P., Geerts, K.L. Stachenfeld, and N. Burgess"
    },
    {
      "index": 39,
      "title": "Makf-Sr: Multi-Agent Adaptive Kalman Filtering-Based Successor Representations",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "M. Salimibeni, P. Malekzadeh, A. Mohammadi, P. Spachos and K. N. Plataniotis"
    },
    {
      "index": 40,
      "title": "Multiple Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2002",
      "venue": "Neural Computation",
      "authors": "K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato"
    },
    {
      "index": 41,
      "title": "Model Selection based on Kalman Temporal Differences Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Collaboration and Internet Computing (CIC)",
      "authors": "T. Kitao, M. Shirai, and T. Miura"
    },
    {
      "index": 42,
      "title": "Adaptive adjustment of noise covariance in Kalman filter for dynamic state estimation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Power & Energy Society General Meeting",
      "authors": "S. Akhlaghi, N. Zhou and Z. Huang"
    },
    {
      "index": 43,
      "title": "On the Identification of Variances and Adaptive Kalman Filtering",
      "abstract": "",
      "year": "1970",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "R. Mehra"
    },
    {
      "index": 44,
      "title": "Partitioning: A Unifying Framework for Adaptive Systems, i: Estimation",
      "abstract": "",
      "year": "1976",
      "venue": "IEEE",
      "authors": "D. G. Lainiotis"
    },
    {
      "index": 45,
      "title": "Similarity-based Multiple Model Adaptive Estimation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "A. Assa and K. N. Plataniotis"
    },
    {
      "index": 46,
      "title": "Adaptive ϵitalic-ϵ\\epsilon-greedy exploration in reinforcement learning based on value differences",
      "abstract": "",
      "year": "2010",
      "venue": "Annual Conference on Artificial Intelligence",
      "authors": "T. Michel"
    },
    {
      "index": 47,
      "title": "Temporal Difference Updating without a Learning Rate",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Hutter and S. Legg"
    },
    {
      "index": 48,
      "title": "Reinforcement Learning Based Stochastic Shortest Path Finding in Wireless Sensor Networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "W. Xia, C. Di, H. Guo, and S. Li"
    },
    {
      "index": 49,
      "title": "Successor features for transfer in reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Barreto, R. Dabney, R. Munos, J.J. Hunt, T. Schaul, H.V. Hasselt, and D. Silver"
    },
    {
      "index": 50,
      "title": "STUPEFY: Set-Valued Box Particle Filtering for Bluetooth Low Energy-Based Indoor Localization",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Signal Processing Letters",
      "authors": "P. Malekzadeh, A. Mohammadi, M. Barbulescu and K. N. Plataniotis"
    },
    {
      "index": 51,
      "title": "Event-Based Estimation With Information-Based Triggering and Adaptive Update",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 52,
      "title": "Improper Complex-Valued Bhattacharyya Distance",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 53,
      "title": "Distributed Widely Linear Multiple-Model Adaptive Estimation",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Trans. Signal & Information Processing over Networks",
      "authors": "A. Mohammadi and K. N. Plataniotis"
    },
    {
      "index": 54,
      "title": "A General and Adaptive Robust Loss Function",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "J. T. Barron",
      "orig_title": "A General and Adaptive Robust Loss Function",
      "paper_id": "1701.03077v10"
    },
    {
      "index": 55,
      "title": "Kalman filtering for matrix estimation",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Transactions on Aerospace and Electronic Systems",
      "authors": "D. Choukroun, H. Weiss, I. Y. Bar-Itzhack and Y. Oshman"
    },
    {
      "index": 56,
      "title": "A note on the variance of a matrix",
      "abstract": "",
      "year": "1968",
      "venue": "Econometrica",
      "authors": "D.H. Nissen"
    }
  ]
}