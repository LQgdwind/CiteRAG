{
  "paper_id": "2003.06709v5",
  "title": "FACMAC: Factored Multi-Agent Centralised Policy Gradients",
  "sections": {
    "experimental results": "In this section we present our experimental results on our cooperative variants of the continuous simple tag environment introduced by Lowe et al. [ref]21 (we refer to this environment as Continuous Predator-Prey), our novel continuous benchmark MAMuJoCo, and the challenging SMAC444We utilise SC2.4.10., which is used by the latest PyMARL framework. The original results reported in Samvelyan et al. [ref]34 and Rashid et al.  use SC2.4.6. Performance is not always comparable across versions. [ref]34 benchmark with discrete action spaces.\nIn discrete cooperative tasks, we compare with state-of-the-art multi-agent actor-critic algorithms MADDPG [ref]21, COMA [ref]7, CentralV [ref]7, DOP , and value-based methods QMIX  and QPLEX .\nIn continuous cooperative tasks, we compare with MADDPG [ref]21 and independent DDPG (IDDPG), as well as COVDN and COMIX, two novel baselines described below. We also explore different forms of critic factorisation. More details about the environments, experimental setup, and training details are included in Appendix C and D."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01465",
      "authors": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama",
      "orig_title": "Reducing overestimation bias in multi-agent domains using double centralized critics",
      "paper_id": "1910.01465v2"
    },
    {
      "index": 1,
      "title": "Input Invex Neural Network",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "Brandon Amos, Lei Xu, and J Zico Kolter",
      "orig_title": "Input convex neural networks",
      "paper_id": "2106.08748v4"
    },
    {
      "index": 2,
      "title": "Factorised critics in deep multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Master Thesis, University of Oxford",
      "authors": "Marilena Bescuca"
    },
    {
      "index": 3,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 4,
      "title": "A tutorial on the cross-entropy method",
      "abstract": "",
      "year": "2005",
      "venue": "Annals of operations research",
      "authors": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein"
    },
    {
      "index": 5,
      "title": "LIIR: Learning individual intrinsic reward in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao"
    },
    {
      "index": 6,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 7,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine",
      "orig_title": "Continuous deep q-learning with model-based acceleration",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 8,
      "title": "Cooperative multi-agent control using deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer"
    },
    {
      "index": 9,
      "title": "Hypernetworks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.09106",
      "authors": "David Ha, Andrew Dai, and Quoc V Le"
    },
    {
      "index": 10,
      "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Shariq Iqbal and Fei Sha",
      "orig_title": "Actor-attention-critic for multi-agent reinforcement learning",
      "paper_id": "1810.02912v2"
    },
    {
      "index": 11,
      "title": "Categorical reparameterization with gumbel-softmax",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.01144",
      "authors": "Eric Jang, Shixiang Gu, and Ben Poole"
    },
    {
      "index": 12,
      "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.10293",
      "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al.",
      "orig_title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
      "paper_id": "1806.10293v3"
    },
    {
      "index": 13,
      "title": "Robocup: A challenge problem for ai",
      "abstract": "",
      "year": "1997",
      "venue": "AI magazine",
      "authors": "Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, Eiichi Osawa, and Hitoshi Matsubara"
    },
    {
      "index": 14,
      "title": "Computing factored value functions for policies in structured mdps",
      "abstract": "",
      "year": "1999",
      "venue": "IJCAI",
      "authors": "Daphne Koller and Ronald Parr"
    },
    {
      "index": 15,
      "title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning",
      "abstract": "",
      "year": "2016",
      "venue": "Neurocomputing",
      "authors": "Landon Kraemer and Bikramjit Banerjee"
    },
    {
      "index": 16,
      "title": "Distributed self-reconfiguration of m-tran iii modular robotic system",
      "abstract": "",
      "year": "2008",
      "venue": "The International Journal of Robotics Research",
      "authors": "Haruhisa Kurokawa, Kohji Tomita, Akiya Kamimura, Shigeru Kokaji, Takashi Hasuo, and Satoshi Murata"
    },
    {
      "index": 17,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "4th International Conference on Learning Representations, ICLR",
      "authors": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra"
    },
    {
      "index": 18,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Long-Ji Lin"
    },
    {
      "index": 19,
      "title": "Emergent coordination through competition",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07151",
      "authors": "Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel"
    },
    {
      "index": 20,
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch",
      "orig_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
      "paper_id": "1706.02275v4"
    },
    {
      "index": 21,
      "title": "Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and Multi-Agent Systems",
      "authors": "Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato",
      "orig_title": "Contrasting centralized and decentralized critics in multi-agent reinforcement learning",
      "paper_id": "2102.04402v2"
    },
    {
      "index": 22,
      "title": "MAVEN: Multi-Agent Variational Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson",
      "orig_title": "Maven: Multi-agent variational exploration",
      "paper_id": "1910.07483v2"
    },
    {
      "index": 23,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al."
    },
    {
      "index": 24,
      "title": "Chainform: A linear integrated modular hardware system for shape changing interfaces",
      "abstract": "",
      "year": "2016",
      "venue": "29th Annual Symposium on User Interface Software and Technology",
      "authors": "Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A Paradiso, and Hiroshi Ishii"
    },
    {
      "index": 25,
      "title": "Optimal and approximate Q-value functions for decentralized pomdps",
      "abstract": "",
      "year": "2008",
      "venue": "JAIR",
      "authors": "Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos Vlassis"
    },
    {
      "index": 26,
      "title": "A concise introduction to decentralized POMDPs, volume 1",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Frans A Oliehoek, Christopher Amato, et al."
    },
    {
      "index": 27,
      "title": "openai/baselines, May 2020.",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 28,
      "title": "Softmax with regularization: Better value estimation in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.11883",
      "authors": "Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson"
    },
    {
      "index": 29,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 30,
      "title": "Weighted qmix: Expanding monotonic value function factorisation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson"
    },
    {
      "index": 31,
      "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "JMLR",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "2003.08839v2"
    },
    {
      "index": 32,
      "title": "Reinforcement learning for robot soccer",
      "abstract": "",
      "year": "2009",
      "venue": "Autonomous Robots",
      "authors": "Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange"
    },
    {
      "index": 33,
      "title": "The StarCraft Multi-Agent Challenge",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "The StarCraft Multi-Agent Challenge",
      "paper_id": "1902.04043v5"
    },
    {
      "index": 34,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.05408",
      "authors": "Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 35,
      "title": "Qtran++: Improved value transformation for cooperative multi-agent reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12010",
      "authors": "Kyunghwan Son, Sungsoo Ahn, Roben Delos Reyes, Jinwoo Shin, and Yung Yi"
    },
    {
      "index": 36,
      "title": "Scaling reinforcement learning toward RoboCup soccer",
      "abstract": "",
      "year": "2001",
      "venue": "Icml",
      "authors": "Peter Stone and Richard S. Sutton"
    },
    {
      "index": 37,
      "title": "Value-decomposition networks for cooperative multi-agent learning based on team reward",
      "abstract": "",
      "year": "2018",
      "venue": "AAMAS",
      "authors": "Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al."
    },
    {
      "index": 38,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Emanuel Todorov, Tom Erez, and Yuval Tassa"
    },
    {
      "index": 39,
      "title": "Towards understanding linear value decomposition in cooperative multi-agent q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.00587",
      "authors": "Jianhao Wang, Zhizhou Ren, Beining Han, and Chongjie Zhang"
    },
    {
      "index": 40,
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.01062",
      "authors": "Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang",
      "orig_title": "Qplex: Duplex dueling multi-agent q-learning",
      "paper_id": "2008.01062v3"
    },
    {
      "index": 41,
      "title": "NerveNet: learning structured policy with graph neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR",
      "authors": "Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler"
    },
    {
      "index": 42,
      "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
      "abstract": "",
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR",
      "authors": "Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang",
      "orig_title": "Dop: Off-policy multi-agent decomposed policy gradients",
      "paper_id": "2007.12322v2"
    },
    {
      "index": 43,
      "title": "Lenient learning in independent-learner stochastic cooperative games",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Ermo Wei and Sean Luke"
    },
    {
      "index": 44,
      "title": "Design and architecture of the unified modular snake robot",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE International Conference on Robotics and Automation",
      "authors": "Cornell Wright, Austin Buchan, Ben Brown, Jason Geist, Michael Schwerin, David Rollinson, Matthew Tesch, and Howie Choset"
    },
    {
      "index": 45,
      "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.03939",
      "authors": "Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang",
      "orig_title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
      "paper_id": "2002.03939v2"
    },
    {
      "index": 46,
      "title": "Modular robots",
      "abstract": "",
      "year": "2002",
      "venue": "IEEE Spectrum",
      "authors": "Mark Yim, Ying Zhang, and David Duff"
    },
    {
      "index": 47,
      "title": "Learning implicit credit assignment for multi-agent actor-critic",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung"
    }
  ]
}