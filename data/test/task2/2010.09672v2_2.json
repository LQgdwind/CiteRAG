{
  "paper_id": "2010.09672v2",
  "title": "Multi-Stage Fusion for One-Click Segmentation",
  "sections": {
    "interactive image segmentation": "Approaches in the literature [ref]2  [ref]4 [ref]5 are typically evaluated by (1) the average number of clicks needed to reach the desired level of segmentation (@​85%@percent85@85\\% mIoU for PASCAL VOC 201220122012, MS COCO, @​90%@percent90@90\\% mIoU for the less challenging Grabcut and Berkeley) and (2) the average mIoU vs the number of clicks. The first criterion is primarily geared towards annotation tasks  [ref]4 where high-quality segments are desired for each instance in the scene; the fewer the number of clicks, the lower the annotation effort. In this work, we are concerned primarily with achieving high-quality segments for the object of interest given only a single click. Accordingly, given a single user click, we report the average mIoU across all instances for the GrabCut, Berkeley and the PASCAL VOC 201220122012 val dataset. For MS COCO object instances, following [ref]2, we split the dataset into the 202020 PASCAL VOC 201220122012 categories and the 606060 additional categories, and randomly sample 101010 images per category for evaluation. We also report the average mIoU across the sampled 800800800 MS COCO instances [ref]5. For training [ref]2 [ref]4 [ref]5, we use the ground truth masks of object instances from PASCAL VOC 201220122012  train set with additional masks from Semantic Boundaries Dataset (SBD)  resulting in 105821058210582 images. Note that unlike , we do not use the training instances from MS COCO. \n Ablation Study. We perform extensive ablation studies to thoroughly analyze the effectiveness of the individual components of our one-click segmentation framework. First, to validate our choice of guidance maps, we consider the user click transformations commonly used in existing interactive segmentation algorithms - Euclidean distance maps [ref]2 [ref]5, Gaussian distance maps  and disk . Fig. 4 shows examples of such guidance maps. For each kind of guidance map, we train separate networks to understand the impact of different user click transformations. For evaluation, we report the average mIoU over all instances in the dataset, given a single click (see Table 2). Next, we study the impact of our proposed late-fusion module (denoted by -multi in Table. 2); we observe an average mIoU improvement of around 1.8%percent1.81.8\\% across different datasets. \n One-click segmentation. We compare the segmentation performance of our method with existing interactive instance segmentation approaches (see Table 3). The approaches are grouped separately into 333 different categories - pre-deep learning approaches, deep learning-based interactive instance segmentation approaches and tap-and-shoot segmentation approaches. From Table. 3, we observe that our approach outperforms the classical interactive segmentation works by a significant margin reporting 40%percent4040\\% absolute improvement in average mIoU. We also outperform existing state-of-the-art interactive instance segmentation approaches [ref]4  by a considerable margin (>3%absentpercent3>3\\%). Additionally, we report an absolute mIoU improvement of 7.2%percent7.27.2\\% and 17%percent1717\\% on Grabcut and Berkeley over the tap-and-shoot segmentation framework of . We show qualitative results to demonstrate the effectiveness of our proposed algorithm (see Fig. 5). The resulting segmentations demonstrate that our approach is highly effective for the one-click segmentation paradigm."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Tap and shoot segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, and Long-Wen Chang"
    },
    {
      "index": 1,
      "title": "Deep interactive object selection",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang"
    },
    {
      "index": 2,
      "title": "Iteratively Trained Interactive Segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "BMVC",
      "authors": "Sabarinath Mahadevan, Paul Voigtlaender, and Bastian Leibe",
      "orig_title": "Iteratively trained interactive segmentation",
      "paper_id": "1805.04398v1"
    },
    {
      "index": 3,
      "title": "Content-aware multi-level guidance for interactive instance segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Soumajit Majumder and Angela Yao"
    },
    {
      "index": 4,
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "Yang Hu, Andrea Soltoggio, Russell Lock, and Steve Carter",
      "orig_title": "A fully convolutional two-stream fusion network for interactive image segmentation",
      "paper_id": "1807.02480v2"
    },
    {
      "index": 5,
      "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu",
      "orig_title": "Semantic image synthesis with spatially-adaptive normalization",
      "paper_id": "1903.07291v2"
    },
    {
      "index": 6,
      "title": "Few-Shot Segmentation Propagation with Guided Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07373",
      "authors": "Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A Efros, and Sergey Levine",
      "orig_title": "Few-shot segmentation propagation with guided networks",
      "paper_id": "1806.07373v1"
    },
    {
      "index": 7,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 8,
      "title": "Intelligent scissors for image composition",
      "abstract": "",
      "year": "1995",
      "venue": "SIGGRAPH",
      "authors": "Eric. N. Mortensen and William. A. Barrett"
    },
    {
      "index": 9,
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Jonathan Long, Evan Shelhamer, and Trevor Darrell",
      "orig_title": "Fully convolutional networks for semantic segmentation",
      "paper_id": "1605.06211v1"
    },
    {
      "index": 10,
      "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam",
      "orig_title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
      "paper_id": "1802.02611v3"
    },
    {
      "index": 11,
      "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
      "abstract": "",
      "year": "2018",
      "venue": "TPAMI",
      "authors": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille"
    },
    {
      "index": 12,
      "title": "Interactive graph cuts for optimal boundary & region segmentation of objects in nd images",
      "abstract": "",
      "year": "2001",
      "venue": "ICCV",
      "authors": "Yuri Y Boykov and M-P Jolly"
    },
    {
      "index": 13,
      "title": "Grabcut: Interactive foreground extraction using iterated graph cuts",
      "abstract": "",
      "year": "2004",
      "venue": "ACM transactions on graphics (TOG)",
      "authors": "Carsten Rother, Vladimir Kolmogorov, and Andrew Blake"
    },
    {
      "index": 14,
      "title": "Geodesic star convexity for interactive image segmentation",
      "abstract": "",
      "year": "2010",
      "venue": "CVPR",
      "authors": "Varun Gulshan, Carsten Rother, Antonio Criminisi, Andrew Blake, and Andrew Zisserman"
    },
    {
      "index": 15,
      "title": "Hierarchical image saliency detection on extended cssd",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE TPAMI",
      "authors": "Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia"
    },
    {
      "index": 16,
      "title": "Global contrast based salient region detection",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE TPAMI",
      "authors": "Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu"
    },
    {
      "index": 17,
      "title": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild",
      "abstract": "",
      "year": "2017",
      "venue": "19th ACM International Conference on Multimodal Interaction",
      "authors": "Valentin Vielzeuf, Stéphane Pateux, and Frédéric Jurie",
      "orig_title": "Temporal multimodal fusion for video emotion classification in the wild",
      "paper_id": "1709.07200v1"
    },
    {
      "index": 18,
      "title": "A late fusion cnn for digital matting",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu"
    },
    {
      "index": 19,
      "title": "Nuclei segmentation via a deep panoptic model with semantic feature fusion",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Dongnan Liu, Donghao Zhang, Yang Song, Chaoyi Zhang, Fan Zhang, Lauren O’Donnell, and Weidong Cai"
    },
    {
      "index": 20,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 21,
      "title": "Pyramid Scene Parsing Network",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia",
      "orig_title": "Pyramid scene parsing network",
      "paper_id": "1612.01105v2"
    },
    {
      "index": 22,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Jie Hu, Li Shen, and Gang Sun",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 23,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 24,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft COCO: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 25,
      "title": "Large-scale interactive object segmentation with human annotators",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari",
      "orig_title": "Large-scale interactive object segmentation with human annotators",
      "paper_id": "1903.10830v2"
    },
    {
      "index": 26,
      "title": "A comparative evaluation of interactive segmentation algorithms",
      "abstract": "",
      "year": "2010",
      "venue": "Pattern Recognition",
      "authors": "Kevin McGuinness and Noel E O’connor"
    },
    {
      "index": 27,
      "title": "The pascal visual object classes (voc) challenge",
      "abstract": "",
      "year": "2010",
      "venue": "IJCV",
      "authors": "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman"
    },
    {
      "index": 28,
      "title": "Semantic contours from inverse detectors",
      "abstract": "",
      "year": "2011",
      "venue": "ICCV",
      "authors": "Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik"
    },
    {
      "index": 29,
      "title": "Geodesic matting: A framework for fast interactive image and video segmentation and matting",
      "abstract": "",
      "year": "2009",
      "venue": "IJCV",
      "authors": "Xue Bai and Guillermo Sapiro"
    }
  ]
}