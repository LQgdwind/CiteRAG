{
  "paper_id": "2011.06631v2",
  "title": "Steady State Analysis of Episodic Reinforcement Learning",
  "sections": {
    "preliminaries": "Markov chain.\nIn this paper, a markov chain is a homogeneous discrete-time stochastic process with countable (finite or infinite) state space. The state-transition probabilities are written into a transition matrix Mğ‘€M, where Mâ€‹(s,sâ€²)ğ‘€ğ‘ superscriptğ‘ â€²M(s,s^{\\prime}) is the entry of row sğ‘ s and column sâ€²superscriptğ‘ â€²s^{\\prime} which specifies â„™[st+1=sâ€²|st=s]â„™subscriptğ‘ ğ‘¡1conditionalsuperscriptğ‘ â€²subscriptğ‘ ğ‘¡ğ‘ \\operatorname*{\\mathbb{P}}[s_{t+1}=s^{\\prime}|s_{t}=s]. A rollout of Mğ‘€M generates a trajectory Î¶=(s0,s1,s2,â€¦)ğœsubscriptğ‘ 0subscriptğ‘ 1subscriptğ‘ 2â€¦\\zeta=(s_{0},s_{1},s_{2},\\dots) of infinite length. Let row vector Ï(t)superscriptğœŒğ‘¡\\rho^{\\scriptscriptstyle(t)} denote the marginal distribution of the state at time tğ‘¡t, so stâˆ¼Ï(t)=Ï(tâˆ’1)â€‹Msimilar-tosubscriptğ‘ ğ‘¡superscriptğœŒğ‘¡superscriptğœŒğ‘¡1ğ‘€s_{t}\\sim\\rho^{\\scriptscriptstyle(t)}=\\rho^{\\scriptscriptstyle(t-1)}M. The limiting distribution of stsubscriptğ‘ ğ‘¡s_{t}, if exists, is the marginal distribution at infinite time, i.e. limtâ†’âˆÏ(t)subscriptâ†’ğ‘¡superscriptğœŒğ‘¡\\lim_{t\\rightarrow\\infty}\\rho^{\\scriptscriptstyle(t)}, and the stationary (or steady-state) distribution is defined as the fixed-point distribution with Ï=Ïâ€‹MğœŒğœŒğ‘€\\rho=\\rho M. The existence and uniqueness of stationary and limiting distributions are characterized by the following well-known concepts and conditions: Given a markov chain with transition matrix Mğ‘€M, a state sğ‘ s is reachable from a state sÂ¯Â¯ğ‘ \\bar{s} if â„™[st=s|s0=sÂ¯]>0â„™subscriptğ‘ ğ‘¡conditionalğ‘ subscriptğ‘ 0Â¯ğ‘ 0\\operatorname*{\\mathbb{P}}[s_{t}=s|s_{0}=\\bar{s}]>0 for some tâ‰¥1ğ‘¡1t\\geq 1, and the markov chain Mğ‘€M is said irreducible if every state is reachable from any state in Mğ‘€M. The mean recurrence time of a state sğ‘ s is the expected number of steps for sğ‘ s to reach itself, denoted as ğ”¼[Ts]â‰âˆ‘t=1âˆtâ‹…â„™[st=sâ€‹Â andÂ â€‹sâˆ‰{s1:tâˆ’1}|s0=s]approaches-limitsubscriptğ”¼absentsubscriptğ‘‡ğ‘ superscriptsubscriptğ‘¡1â‹…ğ‘¡â„™subscriptğ‘ ğ‘¡ğ‘ Â andÂ ğ‘ conditionalsubscriptğ‘ :1ğ‘¡1subscriptğ‘ 0ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}]\\doteq\\sum_{t=1}^{\\infty}t\\cdot\\operatorname*{\\mathbb{P}}[s_{t}=s\\texttt{~{}and~{}}s\\not\\in\\{s_{1:t-1}\\}|s_{0}=s], and state sğ‘ s is said positive recurrent if ğ”¼[Ts]<âˆsubscriptğ”¼absentsubscriptğ‘‡ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}]<\\infty. The markov chain Mğ‘€M is positive recurrent if every state in Mğ‘€M is positive recurrent. Finally, a state sğ‘ s is aperiodic if sğ‘ s can reach itself in two trajectories with co-prime lengths, i.e. if gcdâ¡{t>0:â„™[st=s|s0=s]>0}=1:ğ‘¡0â„™subscriptğ‘ ğ‘¡conditionalğ‘ subscriptğ‘ 0ğ‘ 01\\gcd\\{t>0:\\operatorname*{\\mathbb{P}}[s_{t}=s|s_{0}=s]>0\\}=1. An irreducible markov chain Mğ‘€M has a unique stationary distribution ÏM=1/ğ”¼[Ts]subscriptğœŒğ‘€1subscriptğ”¼absentsubscriptğ‘‡ğ‘ \\rho_{\\scriptscriptstyle M}=1/\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}] if and only if Mğ‘€M is positive recurrent.\n(, Theorem 54) An irreducible and positive-recurrent markov chain Mğ‘€M has a limiting distribution limtâ†’âˆÏ(t)=ÏMsubscriptâ†’ğ‘¡superscriptğœŒğ‘¡subscriptğœŒğ‘€\\lim\\limits_{t\\rightarrow\\infty}\\rho^{\\scriptscriptstyle(t)}=\\rho_{\\scriptscriptstyle M} if and only if there exists one aperiodic state in Mğ‘€M.\n(, Theorem 59) A markov chain satisfying the condition in Proposition 2 is called an ergodic markov chain. Markov Decision Process (MDP).\nAn MDP â„³=(SS,ğ’œ,R,P,Ï0)â„³SSğ’œğ‘…ğ‘ƒsubscriptğœŒ0\\mathcal{M}=(\\SS,\\mathcal{A},R,P,\\rho_{0}) is a sequential decision making model where SSSS\\SS is a countable state space, ğ’œğ’œ\\mathcal{A} a countable action space, Râ€‹(s)âˆˆ[rmâ€‹iâ€‹n,rmâ€‹aâ€‹x]ğ‘…ğ‘ subscriptğ‘Ÿğ‘šğ‘–ğ‘›subscriptğ‘Ÿğ‘šğ‘ğ‘¥R(s)\\in[r_{min},r_{max}] is a reward assigned to each state sâˆˆSSğ‘ SSs\\in\\SS, Pâ€‹(sâ€²|s,a)â‰â„™[st+1=sâ€²|st=s,at=a]approaches-limitğ‘ƒconditionalsuperscriptğ‘ â€²ğ‘ ğ‘â„™subscriptğ‘ ğ‘¡1conditionalsuperscriptğ‘ â€²subscriptğ‘ ğ‘¡ğ‘ subscriptğ‘ğ‘¡ğ‘P(s^{\\prime}|s,a)\\doteq\\operatorname*{\\mathbb{P}}[s_{t+1}=s^{\\prime}|s_{t}=s,a_{t}=a] specifies action-conditioned transition probabilities between states, and Ï0subscriptğœŒ0\\rho_{0} is the initial distribution with s0âˆ¼Ï0similar-tosubscriptğ‘ 0subscriptğœŒ0s_{0}\\sim\\rho_{0}. A policy function Ï€:SSÃ—ğ’œâ†’ :ğœ‹â†’SSğ’œ01\\pi:\\SS\\times\\mathcal{A}\\to  prescribes the probability to take action under each state. With â„™[at=a|st=s]=Ï€â€‹(s,a)â„™subscriptğ‘ğ‘¡conditionalğ‘subscriptğ‘ ğ‘¡ğ‘ ğœ‹ğ‘ ğ‘\\operatorname*{\\mathbb{P}}[a_{t}=a|s_{t}=s]=\\pi(s,a), every policy Ï€ğœ‹\\pi induces a markov chain with transition matrix MÏ€â€‹(s,sâ€²)=âˆ‘aÏ€â€‹(s,a)â€‹Pâ€‹(sâ€²|s,a)subscriptğ‘€ğœ‹ğ‘ superscriptğ‘ â€²subscriptğ‘ğœ‹ğ‘ ğ‘ğ‘ƒconditionalsuperscriptğ‘ â€²ğ‘ ğ‘M_{\\pi}(s,s^{\\prime})=\\sum_{a}\\pi(s,a)P(s^{\\prime}|s,a). Accordingly, row vectors ÏÏ€(t)subscriptsuperscriptğœŒğ‘¡ğœ‹\\rho^{\\scriptscriptstyle(t)}_{\\pi} and ÏÏ€subscriptğœŒğœ‹\\rho_{\\pi} denote the marginal and stationary distributions of stsubscriptğ‘ ğ‘¡s_{t} under policy Ï€ğœ‹\\pi, respectively, so ÏÏ€(t+1)=ÏÏ€(t)â€‹MÏ€subscriptsuperscriptğœŒğ‘¡1ğœ‹subscriptsuperscriptğœŒğ‘¡ğœ‹subscriptğ‘€ğœ‹\\rho^{\\scriptscriptstyle(t+1)}_{\\pi}=\\rho^{\\scriptscriptstyle(t)}_{\\pi}M_{\\pi}, and ÏÏ€=ÏÏ€â€‹MÏ€subscriptğœŒğœ‹subscriptğœŒğœ‹subscriptğ‘€ğœ‹\\rho_{\\pi}=\\rho_{\\pi}M_{\\pi}. An MDP is ergodic if for every policy Ï€ğœ‹\\pi the induced markov chain MÏ€subscriptğ‘€ğœ‹M_{\\pi} is ergodic, in which case the steady-state performanceÂ  of a policy Ï€ğœ‹\\pi is Jaâ€‹vâ€‹gâ€‹(Ï€)â‰limTâ†’âˆ1Tâ€‹âˆ‘t=1TRâ€‹(st)=ğ”¼sâˆ¼ÏÏ€[Râ€‹(s)]approaches-limitsubscriptğ½ğ‘ğ‘£ğ‘”ğœ‹subscriptâ†’ğ‘‡1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘…subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğœ‹ğ‘…ğ‘ J_{avg}(\\pi)\\doteq\\lim\\limits_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}R(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\pi}\\end{subarray}}[R(s)].\nWhen a set of terminal states SSâŸ‚âŠ‚SSsubscriptSSperpendicular-toSS\\SS_{\\perp}\\subset\\SS is identified, a rollout trajectory Î¶ğœ\\zeta is said terminated when it reaches a terminal state. We use Tâ€‹(Î¶)â‰inf{tâ‰¥1:stâˆˆSSâŸ‚}approaches-limitğ‘‡ğœinfimumconditional-setğ‘¡1subscriptğ‘ ğ‘¡subscriptSSperpendicular-toT(\\zeta)\\doteq\\inf\\{t\\geq 1:s_{t}\\in\\SS_{\\perp}\\} to denote the termination time of Î¶ğœ\\zeta,\nand the episode-wise performance of a policy Ï€ğœ‹\\pi is defined as Jeâ€‹pâ€‹iâ€‹(Ï€)â‰ğ”¼Î¶âˆ¼MÏ€[âˆ‘t=1Tâ€‹(Î¶)Râ€‹(st)]approaches-limitsubscriptğ½ğ‘’ğ‘ğ‘–ğœ‹subscriptğ”¼similar-toğœsubscriptğ‘€ğœ‹superscriptsubscriptğ‘¡1ğ‘‡ğœğ‘…subscriptğ‘ ğ‘¡J_{epi}(\\pi)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\zeta\\sim M_{\\pi}\\end{subarray}}[\\sum_{t=1}^{T(\\zeta)}R(s_{t})]. A value function Q:SSÃ—ğ’œâ†’â„:ğ‘„â†’SSğ’œâ„Q:\\SS\\times\\mathcal{A}\\to\\mathbb{R} prescribes the â€œvalueâ€ of taking an action under a state.\nIn this paper, we consider the following product-form family of value functionsÂ        : in which Pâ€‹(s,a)ğ‘ƒğ‘ ğ‘P(s,a) and Ï€â€‹(sâ€²)ğœ‹superscriptğ‘ â€²\\pi(s^{\\prime}) are short-hands for the conditional distributions P(â‹…|s,a)P(\\cdot|s,a) and Ï€(â‹…|sâ€²)\\pi(\\cdot|s^{\\prime}), and Î³:SSâ†’ :ğ›¾â†’SS01\\gamma:\\SS\\to  is called the discounting function. Besides entailing the Bellman equation (2), the definition of QÏ€subscriptğ‘„ğœ‹Q_{\\pi}, i.e. (1), also induces the state-value function VÏ€â€‹(s)â‰ğ”¼aâˆ¼Ï€â€‹(s)[QÏ€â€‹(s,a)]approaches-limitsubscriptğ‘‰ğœ‹ğ‘ subscriptğ”¼similar-toğ‘ğœ‹ğ‘ subscriptğ‘„ğœ‹ğ‘ ğ‘V_{\\pi}(s)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}a\\sim\\pi(s)\\end{subarray}}[Q_{\\pi}(s,a)]. With Î³â€‹(s)=Î³cğ›¾ğ‘ subscriptğ›¾ğ‘\\gamma(s)=\\gamma_{c} for constant Î³câˆˆ[0,1)subscriptğ›¾ğ‘01\\gamma_{c}\\in[0,1), the product-form value functions QÏ€subscriptğ‘„ğœ‹Q_{\\pi} and VÏ€subscriptğ‘‰ğœ‹V_{\\pi} subsume the classic value functions that have been underlying much of the existing RL literature, and we will use QÏ€Î³csubscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœ‹Q^{\\scriptscriptstyle\\gamma_{c}}_{\\pi} and VÏ€Î³csubscriptsuperscriptğ‘‰subscriptğ›¾ğ‘ğœ‹V^{\\scriptscriptstyle\\gamma_{c}}_{\\pi} to refer to this particular form of value functions with constant discounting function.\nOn the other hand, with Î³â€‹(s)=ğŸ™â€‹(sâˆ‰SSâŸ‚)ğ›¾ğ‘ 1ğ‘ subscriptSSperpendicular-to\\gamma(s)=\\mathds{1}(s\\not\\in\\SS_{\\perp}), the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} can be recast as the product-form value of the initial stateÂ : Jeâ€‹pâ€‹iâ€‹(Ï€)=VÏ€â€‹(s0)subscriptğ½ğ‘’ğ‘ğ‘–ğœ‹subscriptğ‘‰ğœ‹subscriptğ‘ 0J_{epi}(\\pi)=V_{\\pi}(s_{0}) if Î³â€‹(s)=1ğ›¾ğ‘ 1\\gamma(s)=1 for sâˆ‰SSâŸ‚ğ‘ subscriptSSperpendicular-tos\\not\\in\\SS_{\\perp} and Î³â€‹(s)=0ğ›¾ğ‘ 0\\gamma(s)=0 for sâˆˆSSâŸ‚ğ‘ subscriptSSperpendicular-tos\\in\\SS_{\\perp}. We will call the function Î³â€‹(s)=ğŸ™â€‹(sâˆ‰SSâŸ‚)ğ›¾ğ‘ 1ğ‘ subscriptSSperpendicular-to\\gamma(s)=\\mathds{1}(s\\not\\in\\SS_{\\perp}), the episodic discounting function. See Appendix A for more discussions on the formulation choices behind the MDP model. Reinforcement Learning (RL).\nIn continual RL, we are given a decision task modeled by an ergodic MDP â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D}, and the goal is to find good policy with respect to the steady-state performance Jaâ€‹vâ€‹gsubscriptğ½ğ‘ğ‘£ğ‘”J_{avg} through a single continual rollout of â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D}. In episodic RL, the decision task â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D} is a finite-horizon MDP with terminal states, and the goal is to optimize the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} through repeatedly rolling out â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D} from the beginning. In this case, a special resetting procedure will intervene the learning process to start a new decision episode upon the end of the lastÂ . A common and basic idea, for both continual and episodic RL algorithms, is to use the rollout data to optimize a parameterized policy function Ï€â€‹(s,a;Î¸)ğœ‹ğ‘ ğ‘ğœƒ\\pi(s,a;\\theta) with respect to a surrogate objective J~â€‹(Î¸)~ğ½ğœƒ\\tilde{J}(\\theta) via the stochastic gradient method. At a update time tğ‘¡t, the gradient âˆ‡Î¸J~subscriptâˆ‡ğœƒ~ğ½\\nabla_{\\theta}\\tilde{J} is approximately computed as a sample mean of some computable function Fğ¹F over a â€œmini-batchâ€ ğ’Ÿtsubscriptğ’Ÿğ‘¡\\mathcal{D}_{t}, with\nâˆ‡Î¸J~â€‹(Î¸t)â‰ˆ1|ğ’Ÿt|â€‹âˆ‘(s,a)âˆˆğ’ŸtFâ€‹(s,a,Î¸t)subscriptâˆ‡ğœƒ~ğ½subscriptğœƒğ‘¡1subscriptğ’Ÿğ‘¡subscriptğ‘ ğ‘subscriptğ’Ÿğ‘¡ğ¹ğ‘ ğ‘subscriptğœƒğ‘¡\\nabla_{\\theta}\\tilde{J}(\\theta_{t})\\approx\\frac{1}{|\\mathcal{D}_{t}|}\\sum_{(s,a)\\in\\mathcal{D}_{t}}F(s,a,\\theta_{t}), where ğ’Ÿtsubscriptğ’Ÿğ‘¡\\mathcal{D}_{t} is a selected subset of all the rollout data up to time tğ‘¡t. For example, policy gradient algorithms, as a family of widely used RL algorithms, work on policy functions Ï€â€‹(s,a;Î¸)ğœ‹ğ‘ ğ‘ğœƒ\\pi(s,a;\\theta) that are directly differentiable, and typically choose J~pâ€‹gâ€‹(Î¸)â‰ğ”¼s0âˆ¼Ï0[VÏ€â€‹(Î¸)Î³câ€‹(s0)]approaches-limitsubscript~ğ½ğ‘ğ‘”ğœƒsubscriptğ”¼similar-tosubscriptğ‘ 0subscriptğœŒ0subscriptsuperscriptğ‘‰subscriptğ›¾ğ‘ğœ‹ğœƒsubscriptğ‘ 0\\tilde{J}_{pg}(\\theta)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s_{0}\\sim\\rho_{0}\\end{subarray}}[V^{\\scriptscriptstyle\\gamma_{c}}_{\\scriptscriptstyle\\pi(\\theta)}(s_{0})]\nÂ 111\nTo simplify notations we will write ÏÎ¸subscriptğœŒğœƒ\\rho_{\\theta}, QÎ¸subscriptğ‘„ğœƒQ_{\\theta}, VÎ¸subscriptğ‘‰ğœƒV_{\\theta}, Jâ€‹(Î¸)ğ½ğœƒJ(\\theta) for ÏÏ€â€‹(Î¸)subscriptğœŒğœ‹ğœƒ\\rho_{\\pi(\\theta)}, QÏ€â€‹(Î¸)subscriptğ‘„ğœ‹ğœƒQ_{\\pi(\\theta)}, VÏ€â€‹(Î¸)subscriptğ‘‰ğœ‹ğœƒV_{\\pi(\\theta)}, Jâ€‹(Ï€â€‹(Î¸))ğ½ğœ‹ğœƒJ(\\pi(\\theta)) from now on.\n\nin episodic tasks as the surrogate objective, whose gradient can be estimated by Fpâ€‹gâ€‹(s,a,Î¸)â‰QÎ¸Î³c^â€‹(s,a)â€‹âˆ‡logâ¡Ï€â€‹(s,a;Î¸)approaches-limitsubscriptğ¹ğ‘ğ‘”ğ‘ ğ‘ğœƒ^subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘âˆ‡ğœ‹ğ‘ ğ‘ğœƒF_{pg}(s,a,\\theta)\\doteq\\widehat{Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}}(s,a)\\nabla\\log\\pi(s,a;\\theta), where QÎ¸Î³c^â€‹(s,a)^subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘\\widehat{Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}}(s,a) is some practical estimation of QÎ¸Î³câ€‹(s,a)subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}(s,a) for the specific (s,a)âˆˆğ’Ÿtğ‘ ğ‘subscriptğ’Ÿğ‘¡(s,a)\\in\\mathcal{D}_{t}.\nSubstituting J~pâ€‹gsubscript~ğ½ğ‘ğ‘”\\tilde{J}_{pg} and Fpâ€‹gsubscriptğ¹ğ‘ğ‘”F_{pg} to the general RL algorithmic framework above, yields where the equality part in (3) is known as the classic policy gradient theoremÂ . Policy gradient algorithms illustrate a general disparity observed in episodic RL between the â€œdesiredâ€ data distribution and the data actually collected. Specifically, according to (3), the policy gradient should be estimated by rolling out an episode using Ï€â€‹(Î¸)ğœ‹ğœƒ\\pi(\\theta) then summing over (Î³c)Ï„â‹…Fpâ€‹gâ‹…superscriptsubscriptğ›¾ğ‘ğœsubscriptğ¹ğ‘ğ‘”(\\gamma_{c})^{\\tau}\\cdot F_{pg} across all steps Ï„ğœ\\tau in the episode, which collectively provide one sample point to the right-hand side of (3). Many policy gradient algorithms indeed work this wayÂ [ref]5 6. However, some modern policy gradient algorithms, such as A3CÂ 4 or the variant of REINFORCE as described in , compute the policy gradient based on data only from a small time window, much smaller than the average episode length. Such an â€œonlineâ€ approachÂ [ref]5 has witnessed remarkable empirical success in practiceÂ 4. A popular explanation for the disparity above is to re-arrange the sum âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹âˆ‘sÏ„ÏÎ¸(Ï„)â€‹(sÏ„)superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsubscriptğ‘ ğœsubscriptsuperscriptğœŒğœğœƒsubscriptğ‘ ğœ\\sum_{\\tau=0}^{\\infty}~{}(\\gamma_{c})^{\\tau}\\sum_{s_{\\tau}}\\rho^{\\scriptscriptstyle(\\tau)}_{\\theta}(s_{\\tau}) in (3) into âˆ‘s(âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹ÏÎ¸(Ï„)â€‹(s))subscriptğ‘ superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsuperscriptğœŒğœğœƒğ‘ \\sum_{s}\\big{(}\\sum_{\\tau=0}^{\\infty}(\\gamma_{c})^{\\tau}\\rho^{\\scriptscriptstyle(\\tau)}_{\\theta}(s)\\big{)}, which is a weighted sum over states, thus can be interpreted as an expectation over a state distribution that we will call it the episode-wise visitation distribution Î¼Ï€Î³câ€‹(s)â‰âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹ÏÏ€(Ï„)â€‹(s)/Zapproaches-limitsuperscriptsubscriptğœ‡ğœ‹subscriptğ›¾ğ‘ğ‘ superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsuperscriptğœŒğœğœ‹ğ‘ ğ‘\\mu_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s)\\doteq\\sum_{\\tau=0}^{\\infty}(\\gamma_{c})^{\\tau}\\rho^{\\scriptscriptstyle(\\tau)}_{\\pi}(s)/ZÂ  6 , where Zğ‘Z is a normalizing term. However, the visitation-distribution interpretation cannot explain why a single rollout data point stsubscriptğ‘ ğ‘¡s_{t} can serve as a faithful sample of Î¼Ï€Î³csubscriptsuperscriptğœ‡subscriptğ›¾ğ‘ğœ‹\\mu^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}. For Î³c<1subscriptğ›¾ğ‘1\\gamma_{c}<1, we know that stsubscriptğ‘ ğ‘¡s_{t} is just biased to Î¼Ï€Î³csubscriptsuperscriptğœ‡subscriptğ›¾ğ‘ğœ‹\\mu^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}Â ; for Î³c=1subscriptğ›¾ğ‘1\\gamma_{c}=1, we have Î¼Ï€1âˆğ”¼Ï€[âˆ‘tğŸ™â€‹(st=s)]proportional-tosubscriptsuperscriptğœ‡1ğœ‹subscriptğ”¼ğœ‹subscriptğ‘¡1subscriptğ‘ ğ‘¡ğ‘ \\mu^{1}_{\\pi}\\propto\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\pi\\end{subarray}}[\\sum_{t}\\mathds{1}(s_{t}=s)], which justifies the practice of taking long-run average across multiple episodesÂ [ref]5 6, but still cannot explain why a single step can represent this long-run averageÂ 4 . Importantly, we remark that the aforementioned disparity between desired data and actual data is observed not only in policy gradient algorithms, but also in other policy-based algorithms like PPO8, and in value-based and off-policy algorithms like Q-Learning2  too. The disparity between theory and practice in data collection is currently a general issue in the RL framework thus discussed (which subsumes all these algorithms, see Appendix B for more details)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Pybullet gymperium.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Dynamic programming and optimal control, volume 1.",
      "abstract": "",
      "year": "1995",
      "venue": "Athena scientific Belmont, MA",
      "authors": "D. P. Bertsekas."
    },
    {
      "index": 2,
      "title": "Openai gym.",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba."
    },
    {
      "index": 3,
      "title": "Off-policy actor-critic.",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1205.4839",
      "authors": "T. Degris, M. White, and R. S. Sutton."
    },
    {
      "index": 4,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel.",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control.",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 5,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.09477",
      "authors": "S. Fujimoto, H. Van Hoof, and D. Meger.",
      "orig_title": "Addressing function approximation error in actor-critic methods.",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 6,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Gu, T. Lillicrap, I. Sutskever, and S. Levine.",
      "orig_title": "Continuous deep q-learning with model-based acceleration.",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 7,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine.",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 8,
      "title": "A natural policy gradient.",
      "abstract": "",
      "year": "2002",
      "venue": "Advances in neural information processing systems",
      "authors": "S. M. Kakade."
    },
    {
      "index": 9,
      "title": "Continuous control with deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1509.02971",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra."
    },
    {
      "index": 10,
      "title": "Simulation-based optimization of markov reward processes.",
      "abstract": "",
      "year": "2001",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "P. Marbach and J. N. Tsitsiklis."
    },
    {
      "index": 11,
      "title": "Markov chains and stochastic stability.",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "S. P. Meyn and R. L. Tweedie."
    },
    {
      "index": 12,
      "title": "Human-level control through deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al."
    },
    {
      "index": 13,
      "title": "Asynchronous methods for deep reinforcement learning.",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu."
    },
    {
      "index": 14,
      "title": "Time Limits in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.00378",
      "authors": "F. Pardo, A. Tavakoli, V. Levdik, and P. Kormushev.",
      "orig_title": "Time limits in reinforcement learning.",
      "paper_id": "1712.00378v4"
    },
    {
      "index": 15,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz.",
      "orig_title": "Trust region policy optimization.",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 16,
      "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
      "abstract": "",
      "year": "2017a",
      "venue": "arXiv preprint arXiv:1704.06440",
      "authors": "J. Schulman, X. Chen, and P. Abbeel.",
      "orig_title": "Equivalence between policy gradients and soft q-learning.",
      "paper_id": "1704.06440v4"
    },
    {
      "index": 17,
      "title": "Proximal policy optimization algorithms.",
      "abstract": "",
      "year": "2017b",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov."
    },
    {
      "index": 18,
      "title": "Basics of applied stochastic processes.",
      "abstract": "",
      "year": "2009",
      "venue": "Springer Science & Business Media",
      "authors": "R. Serfozo."
    },
    {
      "index": 19,
      "title": "Deterministic policy gradient algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller."
    },
    {
      "index": 20,
      "title": "The predictron: End-to-end learning and planning.",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "D. Silver, H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley, G. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, et al."
    },
    {
      "index": 21,
      "title": "A new q (lambda) with interim forward view and monte carlo equivalence.",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "R. Sutton, A. R. Mahmood, D. Precup, and H. Hasselt."
    },
    {
      "index": 22,
      "title": "Td models: modeling the world at a mixture of time scales.",
      "abstract": "",
      "year": "1995",
      "venue": "Twelfth International Conference on Machine Learning",
      "authors": "R. S. Sutton."
    },
    {
      "index": 23,
      "title": "Reinforcement learning: An introduction.",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto."
    },
    {
      "index": 24,
      "title": "Policy gradient methods for reinforcement learning with function approximation.",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour."
    },
    {
      "index": 25,
      "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.",
      "abstract": "",
      "year": "2011",
      "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup."
    },
    {
      "index": 26,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "R. S. Sutton, A. R. Mahmood, and M. White.",
      "orig_title": "An emphatic approach to the problem of off-policy temporal-difference learning.",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 27,
      "title": "Bias in natural actor-critic algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "P. Thomas."
    },
    {
      "index": 28,
      "title": "On average versus discounted reward temporal-difference learning.",
      "abstract": "",
      "year": "2002",
      "venue": "Machine Learning",
      "authors": "J. N. Tsitsiklis and B. Van Roy."
    },
    {
      "index": 29,
      "title": "Deep Reinforcement Learning with Double Q-learning",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI",
      "authors": "H. Van Hasselt, A. Guez, and D. Silver.",
      "orig_title": "Deep reinforcement learning with double q-learning.",
      "paper_id": "1509.06461v3"
    },
    {
      "index": 30,
      "title": "Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.",
      "abstract": "",
      "year": "2011",
      "venue": "Utrecht University",
      "authors": "H. P. van Hasselt."
    },
    {
      "index": 31,
      "title": "Learning from delayed rewards.",
      "abstract": "",
      "year": "1989",
      "venue": "Kingâ€™s College, University of Cambridge",
      "authors": "C. Watkins."
    },
    {
      "index": 32,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "M. White.",
      "orig_title": "Unifying task specification in reinforcement learning.",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 33,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "R. J. Williams."
    },
    {
      "index": 34,
      "title": "On convergence of emphatic temporal-difference learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "H. Yu."
    },
    {
      "index": 35,
      "title": "Generalized Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Zhang, W. Boehmer, and S. Whiteson.",
      "orig_title": "Generalized off-policy actor-critic.",
      "paper_id": "1903.11329v8"
    }
  ]
}