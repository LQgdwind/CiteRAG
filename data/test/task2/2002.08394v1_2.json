{
  "paper_id": "2002.08394v1",
  "title": "MonoLayout: Amodal scene layout from a single image",
  "sections": {
    "datasets": "We present our results on two different datasets - KITTI0 and Argoverse. The latter contains a high-resolution semantic occupancy grid in bird’s eye view, which facilitates the evaluation of amodal scene layout estimation. The KITTI dataset, however, has no such provision. For a semblance of ground-truth, we register depth and semantic segmentation of lidar scans in bird’s eye view. To the best of our knowledge, there is no published prior work that reasons jointly about road and vehicle occupancies. However, there exist approaches for road layout estimation [ref]34 , and a separate set of approaches for vehicle detection  [ref]6. Furthermore, each of these approaches evaluate over different datasets (and in cases, different train/validation splits). To ensure fair comparision with all such approaches, we organize our results into the following categories. Baseline Comparison: For a fair comparison with state-of-the-art road layout estimation techniques, we evaluate performance on the KITTI RAW split used in [ref]34 (101561015610156 training images, 507450745074 validation images). For a fair comparision with state-of-the-art 3D vehicle detection approaches we evaluate performance on the KITTI 3D object detection split of Chen et al.[ref]6 (371237123712 training images, 376937693769 validation images). Amodal Layout Estimation: To evaluate layout estimation on both static and dynamic scene attributes (road, vehicles), we use the KITTI Tracking 0 and Argoverse  datasets. We annotate sequences from the KITTI Tracking dataset for evaluation (577357735773 training images, 223522352235 validation images). Argoverse provides HD maps as well as vehicle detections in bird’s eye view (672367236723 training images, 241824182418 validation images). Temporal sensor fusion for supervision: We then present results using our data generation approach (c.f. Sec. 3.3) on the KITTI Odometry dataset. This also uses the dense semantic segmentation labels from the Semantic KITTI dataset ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall",
      "orig_title": "Semantickitti: A dataset for semantic scene understanding of lidar sequences",
      "paper_id": "1904.01416v3"
    },
    {
      "index": 1,
      "title": "BirdNet: a 3D Object Detection Framework from LiDAR information",
      "abstract": "",
      "year": "2018",
      "venue": "ITSC",
      "authors": "J. Beltrán, C. Guindel, F. M. Moreno, D. Cruzado, F. Garcia, and A. De La Escalera",
      "orig_title": "Birdnet: a 3d object detection framework from lidar information",
      "paper_id": "1805.01195v1"
    },
    {
      "index": 2,
      "title": "High-speed tracking-by-detection without using image information",
      "abstract": "",
      "year": "2017",
      "venue": "International Workshop on Traffic and Street Surveillance for Safety and Security at IEEE AVSS 2017",
      "authors": "E. Bochinski, V. Eiselein, and T. Sikora"
    },
    {
      "index": 3,
      "title": "Pyramid Stereo Matching Network",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "J.-R. Chang and Y.-S. Chen",
      "orig_title": "Pyramid stereo matching network",
      "paper_id": "1803.08669v1"
    },
    {
      "index": 4,
      "title": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang, P. Carr, S. Lucey, D. Ramanan, et al.",
      "orig_title": "Argoverse: 3d tracking and forecasting with rich maps",
      "paper_id": "1911.02620v1"
    },
    {
      "index": 5,
      "title": "Monocular 3d object detection for autonomous driving",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun"
    },
    {
      "index": 6,
      "title": "Multi-View 3D Object Detection Network for Autonomous Driving",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "X. Chen, H. Ma, J. Wan, B. Li, and T. Xia",
      "orig_title": "Multi-view 3d object detection network for autonomous driving",
      "paper_id": "1611.07759v3"
    },
    {
      "index": 7,
      "title": "Multi-column deep neural networks for image classification",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint",
      "authors": "D. Cireşan, U. Meier, and J. Schmidhuber"
    },
    {
      "index": 8,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei"
    },
    {
      "index": 9,
      "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
      "abstract": "",
      "year": "2012",
      "venue": "CVPR",
      "authors": "A. Geiger, P. Lenz, and R. Urtasun"
    },
    {
      "index": 10,
      "title": "Fast R-CNN",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "R. Girshick",
      "orig_title": "Fast r-cnn",
      "paper_id": "1504.08083v2"
    },
    {
      "index": 11,
      "title": "Digging Into Self-Supervised Monocular Depth Estimation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "C. Godard, O. Mac Aodha, M. Firman, and G. Brostow",
      "orig_title": "Digging into self-supervised monocular depth estimation",
      "paper_id": "1806.01260v4"
    },
    {
      "index": 12,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems 27",
      "authors": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio"
    },
    {
      "index": 13,
      "title": "Probabilistic Object Detection: Definition and Evaluation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.10800",
      "authors": "D. Hall, F. Dayoub, J. Skinner, H. Zhang, D. Miller, P. Corke, G. Carneiro, A. Angelova, and N. Sünderhauf",
      "orig_title": "Probabilistic object detection: Definition and evaluation",
      "paper_id": "1811.10800v4"
    },
    {
      "index": 14,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE international conference on computer vision",
      "authors": "K. He, G. Gkioxari, P. Dollár, and R. Girshick",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 15,
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun"
    },
    {
      "index": 16,
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros",
      "orig_title": "Image-to-image translation with conditional adversarial networks",
      "paper_id": "1611.07004v3"
    },
    {
      "index": 17,
      "title": "Image-to-Image Translation with Conditional Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros",
      "orig_title": "Image-to-image translation with conditional adversarial networks",
      "paper_id": "1611.07004v3"
    },
    {
      "index": 18,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representatiosn (ICLR)",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 19,
      "title": "Joint 3D Proposal Generation and Object Detection from View Aggregation",
      "abstract": "",
      "year": "2018",
      "venue": "IROS",
      "authors": "J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander",
      "orig_title": "Joint 3d proposal generation and object detection from view aggregation",
      "paper_id": "1712.02294v4"
    },
    {
      "index": 20,
      "title": "GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "B. Li, W. Ouyang, L. Sheng, X. Zeng, and X. Wang",
      "orig_title": "Gs3d: An efficient 3d object detection framework for autonomous driving",
      "paper_id": "1903.10955v2"
    },
    {
      "index": 21,
      "title": "Deep Continuous Fusion for Multi-Sensor 3D Object Detection",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "M. Liang, B. Yang, S. Wang, and R. Urtasun",
      "orig_title": "Deep continuous fusion for multi-sensor 3d object detection",
      "paper_id": "2012.10992v1"
    },
    {
      "index": 22,
      "title": "Wise-ale: Wide sample estimator for aggregate latent embedding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "S. Lin, R. Clark, R. Birke, N. Trigoni, and S. Roberts"
    },
    {
      "index": 23,
      "title": "Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters",
      "authors": "C. Lu, M. J. G. van de Molengraft, and G. Dubbelman",
      "orig_title": "Monocular semantic occupancy grid mapping with convolutional variational encoder–decoder networks",
      "paper_id": "1804.02176v3"
    },
    {
      "index": 24,
      "title": "3d bounding box estimation using deep learning and geometry",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka"
    },
    {
      "index": 25,
      "title": "Planet dump retrieved from https://planet.osm.org",
      "abstract": "",
      "year": "2017",
      "venue": "https://www.openstreetmap.org",
      "authors": "OpenStreetMap contributors"
    },
    {
      "index": 26,
      "title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.02147",
      "authors": "A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello",
      "orig_title": "Enet: A deep neural network architecture for real-time semantic segmentation",
      "paper_id": "1606.02147v1"
    },
    {
      "index": 27,
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06434",
      "authors": "A. Radford, L. Metz, and S. Chintala",
      "orig_title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "paper_id": "1511.06434v2"
    },
    {
      "index": 28,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "S. Ren, K. He, R. Girshick, and J. Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 29,
      "title": "Orthographic Feature Transform for Monocular 3D Object Detection",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "T. Roddick, A. Kendall, and R. Cipolla",
      "orig_title": "Orthographic feature transform for monocular 3d object detection",
      "paper_id": "1811.08188v1"
    },
    {
      "index": 30,
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Medical image computing and computer-assisted intervention",
      "authors": "O. Ronneberger, P. Fischer, and T. Brox",
      "orig_title": "U-net: Convolutional networks for biomedical image segmentation",
      "paper_id": "1505.04597v1"
    },
    {
      "index": 31,
      "title": "In-Place Activated BatchNorm for Memory-Optimized Training of DNNs",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "S. Rota Bulò, L. Porzi, and P. Kontschieder",
      "orig_title": "In-place activated batchnorm for memory-optimized training of dnns",
      "paper_id": "1712.02616v3"
    },
    {
      "index": 32,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen",
      "orig_title": "Improved techniques for training gans",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 33,
      "title": "Learning to Look around Objects for Top-View Representations of Outdoor Scenes",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "S. Schulter, M. Zhai, N. Jacobs, and M. Chandraker",
      "orig_title": "Learning to look around objects for top-view representations of outdoor scenes",
      "paper_id": "1803.10870v1"
    },
    {
      "index": 34,
      "title": "PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "S. Shi, X. Wang, and H. Li",
      "orig_title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
      "paper_id": "1812.04244v2"
    },
    {
      "index": 35,
      "title": "Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "S. Srivastava, F. Jurie, and G. Sharma",
      "orig_title": "Learning 2d to 3d lifting for object detection in 3d for autonomous vehicles",
      "paper_id": "1904.08494v2"
    },
    {
      "index": 36,
      "title": "Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger",
      "orig_title": "Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving",
      "paper_id": "1812.07179v6"
    },
    {
      "index": 37,
      "title": "A Parametric Top-View Representation of Complex Road Scenes",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Z. Wang, B. Liu, S. Schulter, and M. Chandraker",
      "orig_title": "A parametric top-view representation of complex road scenes",
      "paper_id": "1812.06152v2"
    },
    {
      "index": 38,
      "title": "PIXOR: Real-time 3D Object Detection from Point Clouds",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "B. Yang, W. Luo, and R. Urtasun",
      "orig_title": "Pixor: Real-time 3d object detection from point clouds",
      "paper_id": "1902.06326v3"
    },
    {
      "index": 39,
      "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Y. You, Y. Wang, W.-L. Chao, D. Garg, G. Pleiss, B. Hariharan, M. Campbell, and K. Q. Weinberger",
      "orig_title": "Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving",
      "paper_id": "1906.06310v3"
    },
    {
      "index": 40,
      "title": "Sketch-a-net: A deep neural network that beats humans",
      "abstract": "",
      "year": "2017",
      "venue": "International journal of computer vision",
      "authors": "Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M. Hospedales"
    }
  ]
}