{
  "paper_id": "2107.01832v3",
  "title": "Provable Convergence of Nesterov‚Äôs Accelerated Gradient Method for Over-Parameterized Neural Networks",
  "sections": {
    "related work": "First-order methods.\nWith the growing demands for handling large-scale machine learning problems,\nfirst-order methods that only access the objective values and gradients have become popular due to their efficiency and effectiveness. For convex problems, GD is the most well-known first-order method, which achieves ùí™‚Äã(1/t)ùí™1ùë°\\mathcal{O}(1/t) convergence rate with tùë°t iterations¬†.\nMomentum methods make a further step by exploiting the history of gradients.\nAmong first-order methods, NAG obtains the optimal rate ùí™‚Äã(1/t2)ùí™1superscriptùë°2\\mathcal{O}(1/t^{2}) for convex problem with Lipschitz gradient¬†.\nFocusing on non-smooth convex problems, Tao et al.¬†[ref]29 proved that NAG improves the convergence rate of stochastic gradient descent by a factor log‚Å°(t)ùë°\\log(t).\nIn contrast, Lessard et al.¬† found a counterexample that HB may fail to find the global optimum for some strongly convex problems.\nOn the other hand, several researches established a connection between the discrete-time methods and the ODE models.\nIn the limit of infinitesimally learning rate, Su et al.¬† formulated a second-order ODE associated with NAG.\nThe convergence of NAG is then linked to the analysis of the related ODE solution.\nShi et al.¬† further developed a more accurate high-resolution ODE that helps distinguish between HB and NAG. For non-convex problems, it is intractable to find a global optimum.\nAs an alternative, current researches consider the convergence to the stationary point or local minimum as a criterion for evaluation¬†[ref]33   .\nIn contrast to previous work, we show a non-asymptotic convergence result for NAG to arrive at a global minimum for a non-convex and non-smooth problem. Convergence theory of over-parameterized neural networks.\nDu et al.¬†[ref]16 was the first to prove the convergence rate of GD for training a randomly initialized two-layer ReLU neural network.\nTheir results showed that GD can linearly converge to a global optimum when the width of the hidden layer is large enough.\nBased on the same neural network architecture, Li and Liang¬† investigated the convergence of stochastic gradient descent on structured data.\nWu et al.¬† improved the upper bound of the learning rate in¬†[ref]16, which results in a faster convergence rate for GD.\nOn the other hand, Jacot¬†et al.¬† introduced the NTK theory, which establishes a link between the over-parameterized neural network and the neural tangent kernel.\nTheir result was further extended to investigate the convergence of GD for training different architectures of neural networks, including convolutional¬†, residual¬† and graph neural network¬†.\nWhile these results are mostly concerned with GD, there are few theoretical guarantees for momentum methods. Recently, some researchers have drawn attention to analyzing the convergence of momentum methods with NTK theory.\nWang et al.¬† studied the convergence of HB using a similar setting as¬†[ref]16.\nThey proved that, as compared to GD, HB converges linearly to the global optimum at a faster rate.\nBu et al.¬† established the convergence results of HB and NAG by considering their limiting ODE from a continuous perspective.\nNonetheless, their analysis is asymptotic and far from practice because they use the infinitesimal learning rate and the approximation of Dirac delta function.\nIn contrast, our analysis focuses on the discrete-time situation and yields a non-asymptotic convergence rate of NAG with a finite learning rate, which is close to the reality. Furthermore, some researchers applied optimal transport theory to analyze the training dynamics of neural networks in the mean field setting¬† , where the evolution of the parameter can be approximated by a distributional dynamics.\nHowever, their results are limited to (stochastic) GD."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Accelerated optimization for machine learning: First-order algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "Z. Lin, H. Li, C. Fang"
    },
    {
      "index": 1,
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences",
      "authors": "A. Beck, M. Teboulle"
    },
    {
      "index": 2,
      "title": "Accelerated distributed nesterov gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "G. Qu, N. Li"
    },
    {
      "index": 3,
      "title": "Some methods of speeding up the convergence of iteration methods",
      "abstract": "",
      "year": "1964",
      "venue": "USSR Computational Mathematics and Mathematical Physics",
      "authors": "B. T. Polyak"
    },
    {
      "index": 4,
      "title": "A method for solving the convex programming problem with convergence rate o (1/k^ 2)",
      "abstract": "",
      "year": "1983",
      "venue": "Dokl. akad. nauk Sssr",
      "authors": "Y. E. Nesterov"
    },
    {
      "index": 5,
      "title": "On the importance of initialization and momentum in deep learning",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "I. Sutskever, J. Martens, G. Dahl, G. Hinton"
    },
    {
      "index": 6,
      "title": "Incorporating nesterov momentum into adam",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "T. Dozat"
    },
    {
      "index": 7,
      "title": "Quasi-hyperbolic momentum and Adam for deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Ma, D. Yarats",
      "orig_title": "Quasi-hyperbolic momentum and adam for deep learning",
      "paper_id": "1810.06801v4"
    },
    {
      "index": 8,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. P. Kingma, J. Ba"
    },
    {
      "index": 9,
      "title": "On the convergence of Adam and Beyond",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. J. Reddi, S. Kale, S. Kumar",
      "orig_title": "On the convergence of adam and beyond",
      "paper_id": "1904.09237v1"
    },
    {
      "index": 10,
      "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.09843",
      "authors": "L. Luo, Y. Xiong, Y. Liu, X. Sun",
      "orig_title": "Adaptive gradient methods with dynamic bound of learning rate",
      "paper_id": "1902.09843v1"
    },
    {
      "index": 11,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K√∂pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 12,
      "title": "Deep learning with Keras",
      "abstract": "",
      "year": "2017",
      "venue": "Packt Publishing Ltd",
      "authors": "A. Gulli, S. Pal"
    },
    {
      "index": 13,
      "title": "TensorFlow: A system for large-scale machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "Symposium on Operating Systems Design and Implementation",
      "authors": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X. Zheng",
      "orig_title": "Tensorflow: A system for large-scale machine learning",
      "paper_id": "1605.08695v2"
    },
    {
      "index": 14,
      "title": "Some np-complete problems in quadratic and nonlinear programming",
      "abstract": "",
      "year": "1987",
      "venue": "Mathematical Programming",
      "authors": "K. G. Murty, S. N. Kabadi"
    },
    {
      "index": 15,
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "S. S. Du, X. Zhai, B. Poczos, A. Singh",
      "orig_title": "Gradient descent provably optimizes over-parameterized neural networks",
      "paper_id": "1810.02054v2"
    },
    {
      "index": 16,
      "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Y. Li, Y. Liang",
      "orig_title": "Learning overparameterized neural networks via stochastic gradient descent on structured data",
      "paper_id": "1808.01204v3"
    },
    {
      "index": 17,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. S. Du, J. D. Lee, H. Li, L. Wang, X. Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 18,
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Z. Allen-Zhu, Y. Li, Z. Song",
      "orig_title": "A convergence theory for deep learning via over-parameterization",
      "paper_id": "1811.03962v5"
    },
    {
      "index": 19,
      "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Wang",
      "orig_title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
      "paper_id": "1901.08584v2"
    },
    {
      "index": 20,
      "title": "Quadratic suffices for over-parametrization via matrix chernoff bound",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.03593",
      "authors": "Z. Song, X. Yang"
    },
    {
      "index": 21,
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Jacot, F. Gabriel, C. Hongler",
      "orig_title": "Neural tangent kernel: Convergence and generalization in neural networks",
      "paper_id": "1806.07572v4"
    },
    {
      "index": 22,
      "title": "A Modular Analysis of Provable Acceleration via Polyak‚Äôs Momentum: Training a Wide ReLU Network and a Deep Linear Network",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Wang, C. Lin, J. D. Abernethy",
      "orig_title": "A modular analysis of provable acceleration via polyak‚Äôs momentum: Training a wide relu network and a deep linear network",
      "paper_id": "2010.01618v6"
    },
    {
      "index": 23,
      "title": "A Dynamical View on Optimization Algorithms of Overparameterized Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Z. Bu, S. Xu, K. Chen",
      "orig_title": "A dynamical view on optimization algorithms of overparameterized neural networks",
      "paper_id": "2010.13165v2"
    },
    {
      "index": 24,
      "title": "Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07111",
      "authors": "X. Wu, S. S. Du, R. Ward",
      "orig_title": "Global convergence of adaptive gradient methods for an over-parameterized neural network",
      "paper_id": "1902.07111v2"
    },
    {
      "index": 25,
      "title": "Descending through a crowded valley - benchmarking deep learning optimizers",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "R. M. Schmidt, F. Schneider, P. Hennig"
    },
    {
      "index": 26,
      "title": "Introductory lectures on convex optimization: A basic course",
      "abstract": "",
      "year": "2003",
      "venue": "Springer Science & Business Media",
      "authors": "Y. Nesterov"
    },
    {
      "index": 27,
      "title": "Normierte ringe",
      "abstract": "",
      "year": "1941",
      "venue": "Recueil Math√©matique",
      "authors": "I. Gelfand"
    },
    {
      "index": 28,
      "title": "The strength of nesterov‚Äôs extrapolation in the individual convergence of nonsmooth optimization",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "W. Tao, Z. Pan, G. Wu, Q. Tao"
    },
    {
      "index": 29,
      "title": "Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints",
      "abstract": "",
      "year": "2016",
      "venue": "SIAM Journal on Optimization",
      "authors": "L. Lessard, B. Recht, A. Packard",
      "orig_title": "Analysis and design of optimization algorithms via integral quadratic constraints",
      "paper_id": "1408.3595v7"
    },
    {
      "index": 30,
      "title": "A differential equation for modeling nesterov‚Äôs accelerated gradient method: Theory and insights",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "W. Su, S. Boyd, E. J. Cand√®s"
    },
    {
      "index": 31,
      "title": "Understanding the acceleration phenomenon via high-resolution differential equations",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematical Programming",
      "authors": "B. Shi, S. Du, M. Jordan, W. Su"
    },
    {
      "index": 32,
      "title": "Lower bounds for finding stationary points I",
      "abstract": "",
      "year": "2020",
      "venue": "Mathematical Programming",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 33,
      "title": "How to Escape Saddle Points Efficiently",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, M. I. Jordan",
      "orig_title": "How to escape saddle points efficiently",
      "paper_id": "1703.00887v1"
    },
    {
      "index": 34,
      "title": "Convex until proven guilty: Dimension-free acceleration of gradient descent on non-convex functions",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Carmon, J. C. Duchi, O. Hinder, A. Sidford"
    },
    {
      "index": 35,
      "title": "Generalized momentum-based methods: A hamiltonian perspective",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Optimization",
      "authors": "J. Diakonikolas, M. I. Jordan"
    },
    {
      "index": 36,
      "title": "On exact computation with an infinitely wide neural net",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, R. Wang"
    },
    {
      "index": 37,
      "title": "Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? ‚Äî A Neural Tangent Kernel Perspective",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "K. Huang, Y. Wang, M. Tao, T. Zhao",
      "orig_title": "Why do deep residual networks generalize better than deep feedforward networks? - A neural tangent kernel perspective",
      "paper_id": "2002.06262v2"
    },
    {
      "index": 38,
      "title": "Graph neural tangent kernel: Fusing graph neural networks with graph kernels",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. S. Du, K. Hou, R. Salakhutdinov, B. P√≥czos, R. Wang, K. Xu"
    },
    {
      "index": 39,
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "National Academy of Sciences",
      "authors": "S. Mei, A. Montanari, P.-M. Nguyen",
      "orig_title": "A mean field view of the landscape of two-layers neural networks",
      "paper_id": "1804.06561v2"
    },
    {
      "index": 40,
      "title": "On the global convergence of gradient descent for over-parameterized models using optimal transport",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, F. Bach"
    },
    {
      "index": 41,
      "title": "Acceleration via symplectic discretization of high-resolution differential equations",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Shi, S. S. Du, W. Su, M. I. Jordan"
    },
    {
      "index": 42,
      "title": "Optimization Landscape and Expressivity of Deep CNNs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Q. Nguyen, M. Hein",
      "orig_title": "Optimization landscape and expressivity of deep cnns",
      "paper_id": "1710.10928v2"
    },
    {
      "index": 43,
      "title": "On Lazy Training in Differentiable Programming",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Chizat, E. Oyallon, F. Bach",
      "orig_title": "On lazy training in differentiable programming",
      "paper_id": "1812.07956v5"
    },
    {
      "index": 44,
      "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, J. Pennington"
    },
    {
      "index": 45,
      "title": "From averaging to acceleration, there is only a step-size",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "N. Flammarion, F. Bach"
    },
    {
      "index": 46,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.07747",
      "authors": "H. Xiao, K. Rasul, R. Vollgraf",
      "orig_title": "Fashion-mnist: A novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 47,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Y. LeCun, L. Bottou, Y. Bengio, P. Haffner"
    },
    {
      "index": 48,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "A. Krizhevsky, G. Hinton, et al."
    },
    {
      "index": 49,
      "title": "Finite Versus Infinite Neural Networks: an Empirical Study",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systemsl",
      "authors": "J. Lee, S. S. Schoenholz, J. Pennington, B. Adlam, L. Xiao, R. Novak, J. Sohl-Dickstein",
      "orig_title": "Finite versus infinite neural networks: An empirical study",
      "paper_id": "2007.15801v2"
    },
    {
      "index": 50,
      "title": "JAX: Composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, Q. Zhang"
    },
    {
      "index": 51,
      "title": "Position-transitional particle swarm optimization-incorporated latent factor analysis",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "X. Luo, Y. Yuan, S. Chen, N. Zeng, Z. Wang"
    },
    {
      "index": 52,
      "title": "A data-characteristic-aware latent factor model for web services qos prediction",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "D. Wu, X. Luo, M. Shang, Y. He, G. Wang, X. Wu"
    },
    {
      "index": 53,
      "title": "Fast and accurate non-negative latent factor analysis on high-dimensional and sparse matrices in recommender systems",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge & Data Engineering",
      "authors": "X. Luo, Y. Zhou, Z. Liu, M. Zhou"
    },
    {
      "index": 54,
      "title": "A novel approach to large-scale dynamically weighted directed network representation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "X. Luo, H. Wu, Z. Wang, J. Wang, D. Meng"
    }
  ]
}