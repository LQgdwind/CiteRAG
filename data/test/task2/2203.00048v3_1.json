{
  "paper_id": "2203.00048v3",
  "title": "Multi-modal Alignment using Representation Codebook",
  "sections": {
    "introduction": "Vision language (V&L) representation learning is the problem of learning a unified feature embedding using both image and text signals. Pretrained V&L models have a great diversity of applications in various downstream tasks across different settings, e.g. via transfer learning  [ref]28 . The main tasks in V&L pretraining include aligning the feature spaces of different modalities (multi-modal alignment    [ref]28 [ref]25) and capturing the interaction across modalities (cross-modal fusion,   ). Late fusion approaches such as CLIP and ALIGN [ref]21 focused on the first task, while early fusion approaches such as OSCAR [ref]28, VinVL  and VilLT focused on the second one. In this work, we adopt a hybrid approach similar to ALBEF [ref]25, where features from image and text modalities were first aligned and then fused using a transformer encoder. The main focus of our work is on the feature alignment stage, which is challenging due to the fact that image and text inputs have very different characteristics. Existing approaches such as CLIP  and ALIGN [ref]21 have to rely on large training resources and on massive amount of data to obtain good alignments (400M and 1.8B image-text pairs respectively). In this work, we propose a more efficient alignment strategy by using a codebook that quantizes the common text-image feature space into codewords. These codewords or cluster centers provide a more stable means for contrastive reasoning compared to individual text or visual features. We took the inspiration from SwAV, which was developed for self-supervised visual representation learning. In , two augmented versions (views) of the same input image were passed through a deep network for feature extraction. Visual embedding was learned by optimizing an objective function that enforces the consistency between the feature from one view and the assigned cluster from the other view. SwAV achieved impressive performance in various transfer tasks (see ). Here, we carried out contrastive reasoning across modalities (image-text) instead of cross image views. Details are in Section 3.1, but in a nutshell, we use a learnable codebook for both image and text modalities and train our model to predict the codeword assignment using either text or visual information. Effectively, visual and text features are lined up via aligning with the common codewords during training. See Figure 1 for an illustration. The codebook can be considered as a quantized sample of the underlying output feature distribution. It is end-to-end learnable together with the model parameters. To avoid abrupt changes during training, we further employ momentum distillation, which has been widely used in previous self-supervised learning works such as BYOL , DINO , MoCo. In brief, similar to ALBEF [ref]25, for each of the image, text and fusion encoders, there is a corresponding encoder that is updated through moving average without gradient back propagation. These momentum encoders serve as teachers to guide the self-supervised learning process. Different from ALBEF [ref]25, we use the teachers to guide codebook learning as well as for the cross-modal and intra-modal alignment. The above two components are wired up to support the stable update of the codebook which, in turn, provides an efficient regularization mean for cross modality alignment. Experiment results (Section 4) show that our approach is competitive with state of the art across various benchmarks even when comparing with approach that use massive amount of data such as CLIP  and ALIGN [ref]21.\nIn summary, our main contributions are as follows,\n We propose a codebook-based approach for efficient vision-language alignment learning. It is an extension from self-supervised vision representation learning (SSL) to the multimodal setting. We introduce a new distillation algorithm that helps unimodal and crossmodal contrastive optimization as well as helps stablize codebook learning. The rest of the paper is organized as follows. We introduce related work to ours in Section 2. In Section 3, we describe our framework, called Codebook Learning with Distillation (CODIS), and its two components, multimodal codebook learning and teacher-student distillation. Experimental results are presented in Section 4. Section 5 concludes the paper."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Gradient flows: in metric spaces and in the space of probability measures",
      "abstract": "",
      "year": "2008",
      "venue": "Springer Science & Business Media",
      "authors": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré"
    },
    {
      "index": 1,
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.08254",
      "authors": "Hangbo Bao, Li Dong, and Furu Wei",
      "orig_title": "Beit: Bert pre-training of image transformers",
      "paper_id": "2106.08254v2"
    },
    {
      "index": 2,
      "title": "Deep clustering for unsupervised learning of visual features",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze"
    },
    {
      "index": 3,
      "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.09882",
      "authors": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin",
      "orig_title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "paper_id": "2006.09882v5"
    },
    {
      "index": 4,
      "title": "Emerging properties in self-supervised vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.14294",
      "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin"
    },
    {
      "index": 5,
      "title": "Graph Optimal Transport for Cross-Domain Alignment",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu",
      "orig_title": "Graph optimal transport for cross-domain alignment",
      "paper_id": "2006.14744v3"
    },
    {
      "index": 6,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 7,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu",
      "orig_title": "Uniter: Universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 8,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le"
    },
    {
      "index": 9,
      "title": "An optimal transport approach to robust reconstruction and simplification of 2d shapes",
      "abstract": "",
      "year": "2011",
      "venue": "Computer Graphics Forum",
      "authors": "Fernando De Goes et al."
    },
    {
      "index": 10,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 11,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.11929",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 12,
      "title": "SLADE: A Self-Training Framework For Distance Metric Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiali Duan, Yen-Liang Lin, Son Tran, Larry S Davis, and C-C Jay Kuo",
      "orig_title": "Slade: A self-training framework for distance metric learning",
      "paper_id": "2011.10269v2"
    },
    {
      "index": 13,
      "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.06195",
      "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu",
      "orig_title": "Large-scale adversarial training for vision-and-language representation learning",
      "paper_id": "2006.06195v2"
    },
    {
      "index": 14,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 15,
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.07733",
      "authors": "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al.",
      "orig_title": "Bootstrap your own latent: A new approach to self-supervised learning",
      "paper_id": "2006.07733v3"
    },
    {
      "index": 16,
      "title": "Dimensionality reduction by learning an invariant mapping",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)",
      "authors": "Raia Hadsell, Sumit Chopra, and Yann LeCun"
    },
    {
      "index": 17,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 18,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 19,
      "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu",
      "orig_title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
      "paper_id": "2104.03135v2"
    },
    {
      "index": 20,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.05918",
      "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig"
    },
    {
      "index": 21,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03334",
      "authors": "Wonjae Kim, Bokyung Son, and Ildoo Kim",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 22,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International Journal of Computer Vision",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al."
    },
    {
      "index": 23,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 24,
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.07651",
      "authors": "Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi",
      "orig_title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "paper_id": "2107.07651v2"
    },
    {
      "index": 25,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 26,
      "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.15409",
      "authors": "Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang"
    },
    {
      "index": 27,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al.",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 28,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "European Conference on Computer Vision",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 29,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.05101",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 30,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.02265",
      "authors": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee"
    },
    {
      "index": 31,
      "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee",
      "orig_title": "12-in-1: Multi-task vision and language representation learning",
      "paper_id": "1912.02315v2"
    },
    {
      "index": 32,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.03748",
      "authors": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 33,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg"
    },
    {
      "index": 34,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 35,
      "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.07966",
      "authors": "Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti",
      "orig_title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data",
      "paper_id": "2001.07966v2"
    },
    {
      "index": 36,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.00020",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 37,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever"
    },
    {
      "index": 38,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 39,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut"
    },
    {
      "index": 40,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.08530",
      "authors": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai"
    },
    {
      "index": 41,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.00491",
      "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 42,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.07490",
      "authors": "Hao Tan and Mohit Bansal",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 43,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 44,
      "title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin",
      "orig_title": "Unsupervised feature learning via non-parametric instance discrimination",
      "paper_id": "1805.01978v1"
    },
    {
      "index": 45,
      "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.06706",
      "authors": "Ning Xie, Farley Lai, Derek Doran, and Asim Kadav",
      "orig_title": "Visual entailment: A novel task for fine-grained image understanding",
      "paper_id": "1901.06706v1"
    },
    {
      "index": 46,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le"
    },
    {
      "index": 47,
      "title": "Vision-Language Pre-Training with Triple Contrastive Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang",
      "orig_title": "Vision-language pre-training with triple contrastive learning",
      "paper_id": "2202.10401v4"
    },
    {
      "index": 48,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao",
      "orig_title": "Vinvl: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    }
  ]
}