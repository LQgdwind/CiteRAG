{
  "paper_id": "2007.10457v1",
  "title": "Multi-agent Reinforcement Learning in Bayesian Stackelberg Markov Games for Adaptive Moving Target Defense",
  "sections": {
    "bayesian stackelberg markov games (bsmgs)": "Markov Games (MGs)  are used to model multi-agent interactions in sequential planning problems. Under this framework, a player can reason about the behavior of other agents (co-operative or adversarial) and come up with policies that adhere to some notion of equilibrium (where no agent can gain by deviating away from the action or strategy profile). While MGs have been widely used to model adversarial scenarios, they suffer from two major shortcomingsâ€“ (1) they do not consider incomplete information about the adversary     and/or (2) they consider weak threat models where the attacker has no information about the defenderâ€™s policy  . On the other hand, Bayesian Stackelberg Games   is a single-stage game-theoretic formalism that addresses both of these concerns but cannot be trivially generalized to sequential settings. To overcome these challenges of expressiveness required for Moving Target Defenses (MTDs) while ensuring scalability, we introduce the formalism of Bayesian Stackelberg Markov Games (BSMGs). BSMGs extends Bayesian Stackelberg Games (BSGs) to multi-stage sequential games. While one can consider using existing formalism in Markov Games that capture incomplete information, they face severe scalability issues and have thus been unpopular in cyber-security domains (we discuss how BSMG is situated in this landscape of works in sectionÂ 5). In the context of Moving Target Defense (MTD), BSMG acts as a unifying framework helping us characterize optimal movement policies against strategic adversaries, capture transition dynamics and costs of the underlying cyber-system, aid in reasoning about stronger threat models, and consider incomplete information about strategic adversaries. Formally, a BSMG can be represented by the tuple (P,S,Î˜,A,Ï„,U,Î³ğ’Ÿ,Î³ğ’œ)ğ‘ƒğ‘†Î˜ğ´ğœğ‘ˆsuperscriptğ›¾ğ’Ÿsuperscriptğ›¾ğ’œ(P,S,\\Theta,A,\\tau,U,\\gamma^{\\mathcal{D}},\\gamma^{\\mathcal{A}}) where, P={ğ’Ÿ,ğ’œ={ğ’œ1,ğ’œ2,â€¦â€‹ğ’œt}}ğ‘ƒğ’Ÿğ’œsubscriptğ’œ1subscriptğ’œ2â€¦subscriptğ’œğ‘¡P=\\{\\mathcal{D},\\mathcal{A}=\\{\\mathcal{A}_{1},\\mathcal{A}_{2},\\dots\\mathcal{A}_{t}\\}\\} where ğ’Ÿğ’Ÿ\\mathcal{D} denotes the leader (defender) and ğ’œğ’œ\\mathcal{A} denotes the follower (attacker). In our model, only the second player has tğ‘¡t types. S={s1,s2,â€¦,sk}ğ‘†subscriptğ‘ 1subscriptğ‘ 2â€¦subscriptğ‘ ğ‘˜S=\\{s_{1},s_{2},\\dots,s_{k}\\} are kğ‘˜k (finite) states of the game, Î˜={Î¸1,Î¸2,â€¦â€‹Î¸k}Î˜subscriptğœƒ1subscriptğœƒ2â€¦subscriptğœƒğ‘˜\\Theta=\\{\\theta_{1},\\theta_{2},\\dots\\theta_{k}\\} denotes kğ‘˜k probability distributions (for kğ‘˜k states) over the tğ‘¡t attackers and Î¸iâ€‹(s)subscriptğœƒğ‘–ğ‘ \\theta_{i}(s) denotes the probability of iğ‘–i-th attacker type in state sğ‘ s A={Ağ’Ÿ,Ağ’œ1,â€¦â€‹Ağ’œt}ğ´superscriptğ´ğ’Ÿsuperscriptğ´subscriptğ’œ1â€¦superscriptğ´subscriptğ’œğ‘¡A=\\{A^{\\mathcal{D}},A^{\\mathcal{A}_{1}},\\dots A^{\\mathcal{A}_{t}}\\} denotes the action set of the player and Aiâ€‹(s)superscriptğ´ğ‘–ğ‘ A^{i}(s) represents the set of actions/pure strategies available to player iğ‘–i in state sğ‘ s. Ï„iâ€‹(s,ağ’Ÿ,ağ’œi,sâ€²)superscriptğœğ‘–ğ‘ superscriptğ‘ğ’Ÿsuperscriptğ‘subscriptğ’œğ‘–superscriptğ‘ â€²\\tau^{i}(s,a^{\\mathcal{D}},a^{\\mathcal{A}_{i}},s^{\\prime}) represents the probability of reaching a state sâ€²âˆˆSsuperscriptğ‘ â€²ğ‘†s^{\\prime}\\in S from the state sâˆˆSğ‘ ğ‘†s\\in S when the ğ’Ÿğ’Ÿ\\mathcal{D} chooses ağ’Ÿsuperscriptğ‘ğ’Ÿa^{\\mathcal{D}} and attacker type iğ‘–i choose the action ağ’œisuperscriptğ‘subscriptğ’œğ‘–a^{\\mathcal{A}_{i}}, U={Uğ’Ÿ,Uğ’œ1,â€¦,Uğ’œt}ğ‘ˆsuperscriptğ‘ˆğ’Ÿsuperscriptğ‘ˆsubscriptğ’œ1â€¦superscriptğ‘ˆsubscriptğ’œğ‘¡U=\\{U^{\\mathcal{D}},U^{\\mathcal{A}_{1}},\\dots,U^{\\mathcal{A}_{t}}\\} where Uğ’Ÿâ€‹(s,ağ’Ÿ,ağ’œi)superscriptğ‘ˆğ’Ÿğ‘ superscriptğ‘ğ’Ÿsuperscriptğ‘subscriptğ’œğ‘–U^{\\mathcal{D}}(s,a^{\\mathcal{D}},a^{\\mathcal{A}_{i}}) and Uiâ€‹(s,ağ’Ÿ,ağ’œi)superscriptğ‘ˆğ‘–ğ‘ superscriptğ‘ğ’Ÿsuperscriptğ‘subscriptğ’œğ‘–U^{i}(s,a^{\\mathcal{D}},a^{\\mathcal{A}_{i}}) represents the reward/utility of ğ’Ÿğ’Ÿ\\mathcal{D} and an attacker type ğ’œisubscriptğ’œğ‘–\\mathcal{A}_{i} respectively if, in state sğ‘ s, actions ağ’Ÿsuperscriptğ‘ğ’Ÿa^{\\mathcal{D}} and ağ’œisuperscriptğ‘subscriptğ’œğ‘–a^{\\mathcal{A}_{i}} are chosen by the players, Î³iâ†¦[0,1)maps-tosuperscriptğ›¾ğ‘–01\\gamma^{i}\\mapsto[0,1) is the discount factor for player iğ‘–i. We will assume that Î³ğ’Ÿ=Î³ğ’œğ’¾=Î³superscriptğ›¾ğ’Ÿsuperscriptğ›¾subscriptğ’œğ’¾ğ›¾\\gamma^{\\mathcal{D}}=\\gamma^{\\mathcal{A_{i}}}=\\gamma. In BSMGs the individual stage games constitute normal-form Bayesian games with a distribution over attacker types; this is in contrast to the unit probability over a single adversary type in MGs. Both in physical  and cyber-security , defenders are known to have knowledge about follower types, a classic case of known-unknowns. BSMGs provide the expressive power to represent this information; precisely Î¸ssubscriptğœƒğ‘ \\theta_{s} represents the probability estimate with which a defender believes a certain kind of adversary is encountered in a particular state sğ‘ s of the game. Note that a defender ğ’Ÿğ’Ÿ\\mathcal{D} is expected to deploy a system first. Thus, a strong threat model assumes that all the attacker types ğ’œisubscriptğ’œğ‘–\\mathcal{A}_{i} know the defenderâ€™s policy, making the Bayesian notion of Stackelberg Equilibrium an appropriate solution concept for such games.\nFor a normal-form game, let a defenderâ€™s mixed policy be denoted as xğ‘¥x and let us denote an attacker type ğ’œisubscriptğ’œğ‘–\\mathcal{A}_{i}â€™s response set (i.e. a set of best responses to xğ‘¥x) as Riâ€‹(x)superscriptğ‘…ğ‘–ğ‘¥R^{i}(x). If the response set for all adversary types is singleton, then the action profile (x,R1â€‹(x),â€¦â€‹Rtâ€‹(x))ğ‘¥superscriptğ‘…1ğ‘¥â€¦superscriptğ‘…ğ‘¡ğ‘¥(x,R^{1}(x),\\dots R^{t}(x)) constitutes a Stackelberg Equilibrium of the normal-form game . When the response set contains more than one action, the final response chosen can yield different rewards for ğ’Ÿğ’Ÿ\\mathcal{D}. In such cases, a popular assumption made in general-sum games is to consider the response that results in the optimal rewards for ğ’Ÿğ’Ÿ\\mathcal{D}; this is termed as the Strong Stackelberg Equilibrium (SSE)   [ref]4 . In contrast to the notion of Weak Stackelberg Equilibrium, which considers the pessimistic case, an SSE is guaranteed to exist and yields a unique game value to the defender regardless of the particular SSE chosen  . Thus, we consider SSEs as the solution concept in BSMGs and highlight a few properties about player strategies at equilibrium for BSMGs (the proofs are deferred to the supplementary material). Lemma 1. For a given policy of the leader/defender in BSMG, every follower/attacker type will have a deterministic policy in all states sâˆˆSğ‘ ğ‘†s\\in S that is an optimal response. Corollary 1. For an SSE policy of the defender, denoted as xğ‘¥x, each attacker type Aisubscriptğ´ğ‘–A_{i} has a deterministic best policy qisubscriptğ‘ğ‘–q_{i}. The action profile (x,q1,â€¦â€‹qt)ğ‘¥subscriptğ‘1â€¦subscriptğ‘ğ‘¡(x,q_{1},\\dots q_{t}) denotes the SSE of the BSMG. Lemma 2. If an action profile (x,q1,â€¦,qt)ğ‘¥subscriptğ‘1â€¦subscriptğ‘ğ‘¡(x,q_{1},\\dots,q_{t}) yields the equilibrium values Vx,qğ’Ÿsubscriptsuperscriptğ‘‰ğ’Ÿğ‘¥ğ‘V^{\\mathcal{D}}_{x,q} and Vx,qğ’œisubscriptsuperscriptğ‘‰subscriptğ’œğ‘–ğ‘¥ğ‘V^{\\mathcal{A}_{i}}_{x,q} to the players and is an SSE of BSMG, then âˆ€sâˆˆSfor-allğ‘ ğ‘†\\forall s\\in S (xâ€‹(s),q1â€‹(s),â€¦,qtâ€‹(s))ğ‘¥ğ‘ subscriptğ‘1ğ‘ â€¦subscriptğ‘ğ‘¡ğ‘ (x(s),q_{1}(s),\\dots,q_{t}(s)) is an SSE of the bi-matrix Bayesian game represented by the Q-values Qx,qiğ’Ÿ,iâ€‹(s),Qx,qiğ’œiâ€‹(s)â€‹âˆ€iâˆˆ{1,â€¦,t}subscriptsuperscriptğ‘„ğ’Ÿğ‘–ğ‘¥subscriptğ‘ğ‘–ğ‘ subscriptsuperscriptğ‘„subscriptğ’œğ‘–ğ‘¥subscriptğ‘ğ‘–ğ‘ for-allğ‘–1â€¦ğ‘¡Q^{\\mathcal{D},i}_{x,q_{i}}(s),Q^{\\mathcal{A}_{i}}_{x,q_{i}}(s)~{}\\forall i\\in\\{1,\\dots,t\\}. When the parameters of a game are provided up-front, an approach similar to calculating Strong Stackelberg Equilibrium in Bayesian Games  alongside Mixed-Integer Non-Linear Programming approaches  or Bellman-style approaches for Markov Games  can be leveraged to find the defenderâ€™s policy. In contrast, when game-parameters are difficult to provide upfront but interaction with an environment is considered possible, we can resort to reinforcement learning techniques.\nBefore proposing our model-free multi-agent reinforcement learning method in the next section, we briefly discuss how the various MTDs, used later in the experiments, are modeled as BSMGs."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Moving target defense: creating asymmetric uncertainty for cyber threats, volume 54",
      "abstract": "",
      "year": "2011",
      "venue": "Springer Science & Business Media",
      "authors": "Sushil Jajodia, Anup K Ghosh, Vipin Swarup, Cliff Wang, and X Sean Wang"
    },
    {
      "index": 1,
      "title": "Moving target defense: a symbiotic framework for ai & security",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta"
    },
    {
      "index": 2,
      "title": "A Survey of Moving Target Defenses for Network Security",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Communications Surveys & Tutorials",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Abdulhakim Sabur, Adel Alshamrani, Dijiang Huang, and Subbarao Kambhampati",
      "orig_title": "A survey of moving target defenses for network security",
      "paper_id": "1905.00964v2"
    },
    {
      "index": 3,
      "title": "From physical security to cybersecurity",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Cybersecurity",
      "authors": "Arunesh Sinha, Thanh H Nguyen, Debarun Kar, Matthew Brown, Milind Tambe, and Albert Xin Jiang"
    },
    {
      "index": 4,
      "title": "Moving Target Defense for Web Applications using Bayesian Stackelberg Games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 International Conference on Autonomous Agents & Multiagent Systems",
      "authors": "Satya Gautam Vadlamudi, Sailik Sengupta, Marthony Taguinod, Ziming Zhao, Adam DoupÃ©, Gail-Joon Ahn, and Subbarao Kambhampati",
      "orig_title": "Moving target defense for web applications using bayesian stackelberg games",
      "paper_id": "1602.07024v3"
    },
    {
      "index": 5,
      "title": "A game theoretic approach to strategy generation for moving target defense in web applications",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Sailik Sengupta, Satya Gautam Vadlamudi, Subbarao Kambhampati, Adam DoupÃ©, Ziming Zhao, Marthony Taguinod, and Gail-Joon Ahn"
    },
    {
      "index": 6,
      "title": "Computing stackelberg equilibria in discounted stochastic games",
      "abstract": "",
      "year": "2012",
      "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence",
      "authors": "Yevgeniy Vorobeychik and Satinder Singh"
    },
    {
      "index": 7,
      "title": "Markov game modeling of moving target defense for strategic detection of threats in cloud networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.09660",
      "authors": "Ankur Chowdhary, Sailik Sengupta, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 8,
      "title": "Markov modeling of moving target defense games",
      "abstract": "",
      "year": "2016",
      "venue": "2016 ACM Workshop on Moving Target Defense",
      "authors": "Hoda Maleki, Saeed Valizadeh, William Koch, Azer Bestavros, and Marten van Dijk"
    },
    {
      "index": 9,
      "title": "Game-theoretic approach to feedback-driven multi-stage moving target defense",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on decision and game theory for security",
      "authors": "Quanyan Zhu and Tamer BaÅŸar"
    },
    {
      "index": 10,
      "title": "Spatial-Temporal Moving Target Defense: A Markov Stackelberg Game Model",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.10390",
      "authors": "Henger Li, Wen Shen, and Zizhan Zheng",
      "orig_title": "Spatial-temporal moving target defense: A markov stackelberg game model",
      "paper_id": "2002.10390v1"
    },
    {
      "index": 11,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "7th international joint conference on Autonomous agents and multiagent systems-Volume 2",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 12,
      "title": "Markov security games: Learning in spatial security problems",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems (2016)",
      "authors": "Richard Klima, Karl Tuyls, and Frans Oliehoek"
    },
    {
      "index": 13,
      "title": "Dynamic ids configuration in the presence of intruder type uncertainty",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Global Communications Conference (GLOBECOM)",
      "authors": "Xiaofan He, Huaiyu Dai, Peng Ning, and Rudra Dutta"
    },
    {
      "index": 14,
      "title": "Playing adaptively against stealthy opponents: A reinforcement learning strategy for the flipit security game",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.11938",
      "authors": "Lisa Oakley and Alina Oprea"
    },
    {
      "index": 15,
      "title": "Deep reinforcement learning based adaptive moving target defense",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.11972",
      "authors": "Taha Eghtesad, Yevgeniy Vorobeychik, and Aron Laszka"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 17,
      "title": "Stochastic games",
      "abstract": "",
      "year": "1953",
      "venue": "Proceedings of the national academy of sciences",
      "authors": "Lloyd S Shapley"
    },
    {
      "index": 18,
      "title": "Solving security games on graphs via marginal probabilities",
      "abstract": "",
      "year": "2013",
      "venue": "Twenty-Seventh AAAI Conference on Artificial Intelligence",
      "authors": "Joshua Letchford and Vincent Conitzer"
    },
    {
      "index": 19,
      "title": "General sum markov games for strategic detection of advanced persistent threats using moving target defense in cloud networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Sailik Sengupta, Ankur Chowdhary, Dijiang Huang, and Subbarao Kambhampati"
    },
    {
      "index": 20,
      "title": "Dynamic policy-based ids configuration",
      "abstract": "",
      "year": "2009",
      "venue": "48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference",
      "authors": "Quanyan Zhu and Tamer BaÅŸar"
    },
    {
      "index": 21,
      "title": "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "abstract": "",
      "year": "2008",
      "venue": "AAMAS",
      "authors": "Praveen Paruchuri, Jonathan P Pearce, Janusz Marecki, Milind Tambe, Fernando Ordonez, and Sarit Kraus"
    },
    {
      "index": 22,
      "title": "On generalized stackelberg strategies",
      "abstract": "",
      "year": "1978",
      "venue": "Journal of Optimization Theory and Applications",
      "authors": "George Leitmann"
    },
    {
      "index": 23,
      "title": "Computing the optimal strategy to commit to",
      "abstract": "",
      "year": "2006",
      "venue": "7th ACM Conference on Electronic Commerce, EC â€™06",
      "authors": "Vincent Conitzer and Tuomas Sandholm"
    },
    {
      "index": 24,
      "title": "Leadership with commitment to mixed strategies",
      "abstract": "",
      "year": "2004",
      "venue": "CDAM Research Report LSE-CDAM-2004-01",
      "authors": "BV Stengel and S Zamir"
    },
    {
      "index": 25,
      "title": "Lecture notes on non-cooperative game theory",
      "abstract": "",
      "year": "2010",
      "venue": "Game Theory Module of the Graduate Program in Network Mathematics",
      "authors": "Tamer Basar et al."
    },
    {
      "index": 26,
      "title": "National vulnerability database",
      "abstract": "",
      "year": "",
      "venue": "https://nvd.nist.gov",
      "authors": ""
    },
    {
      "index": 27,
      "title": "An initial study of targeted personality models in the flipit game",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Decision and Game Theory for Security",
      "authors": "Anjon Basak, Jakub ÄŒerná»³, Marcus Gutierrez, Shelby Curtis, Charles Kamhoua, Daniel Jones, Branislav BoÅ¡anská»³, and Christopher Kiekintveld"
    },
    {
      "index": 28,
      "title": "Automated generation and analysis of attack graphs",
      "abstract": "",
      "year": "2002",
      "venue": "2002 IEEE Symposium on Security and Privacy",
      "authors": "Oleg Sheyner, Joshua Haines, Somesh Jha, Richard Lippmann, and Jeannette M Wing"
    },
    {
      "index": 29,
      "title": "A game theoretic approach to strategy determination for dynamic platform defenses",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Kevin M Carter, James F Riordan, and Hamed Okhravi"
    },
    {
      "index": 30,
      "title": "Deceiving cyber adversaries: A game theoretic approach",
      "abstract": "",
      "year": "2018",
      "venue": "17th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Aaron Schlenker, Omkar Thakoor, Haifeng Xu, Fei Fang, Milind Tambe, Long Tran-Thanh, Phebe Vayanos, and Yevgeniy Vorobeychik"
    },
    {
      "index": 31,
      "title": "Asymmetric multiagent reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "Web Intelligence and Agent Systems: An international journal",
      "authors": "Ville KÃ¶nÃ¶nen"
    },
    {
      "index": 32,
      "title": "Towards a theory of moving target defense",
      "abstract": "",
      "year": "2014",
      "venue": "First ACM Workshop on Moving Target Defense",
      "authors": "Rui Zhuang, Scott A DeLoach, and Xinming Ou"
    },
    {
      "index": 33,
      "title": "Multiagent reinforcement learning: theoretical framework and an algorithm",
      "abstract": "",
      "year": "1998",
      "venue": "ICML",
      "authors": "Junling Hu, Michael P Wellman, et al."
    },
    {
      "index": 34,
      "title": "Multi-agent reinforcement learning: a critical survey",
      "abstract": "",
      "year": "2003",
      "venue": "Web manuscript",
      "authors": "Yoav Shoham, Rob Powers, and Trond Grenager"
    },
    {
      "index": 35,
      "title": "Markov games as a framework for multi-agent reinforcement learning",
      "abstract": "",
      "year": "1994",
      "venue": "Machine learning proceedings 1994",
      "authors": "Michael L Littman"
    },
    {
      "index": 36,
      "title": "Friend-or-foe q-learning in general-sum games",
      "abstract": "",
      "year": "2001",
      "venue": "ICML",
      "authors": "Michael L Littman"
    },
    {
      "index": 37,
      "title": "Correlated q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "ICML",
      "authors": "Amy Greenwald, Keith Hall, and Roberto Serrano"
    },
    {
      "index": 38,
      "title": "Finite depth of reasoning and equilibrium play in games with incomplete information",
      "abstract": "",
      "year": "2013",
      "venue": "Discussion Paper, Center for Mathematical Studies in Economics and ...",
      "authors": "Willemien Kets"
    },
    {
      "index": 39,
      "title": "On markov games played by bayesian and boundedly-rational players",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Muthukumaran Chandrasekaran, Yingke Chen, and Prashant Doshi"
    },
    {
      "index": 40,
      "title": "Contributions to the Theory of Games, volume 2",
      "abstract": "",
      "year": "1953",
      "venue": "Princeton University Press",
      "authors": "Harold William Kuhn and Albert William Tucker"
    },
    {
      "index": 41,
      "title": "Markov games of incomplete information for multi-agent reinforcement learning",
      "abstract": "",
      "year": "2011",
      "venue": "Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence",
      "authors": "Liam MacDermed, Charles Isbell, and Lora Weiss"
    },
    {
      "index": 42,
      "title": "Leader-follower semi-markov decision problems: theoretical framework and approximate solution",
      "abstract": "",
      "year": "2007",
      "venue": "2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning",
      "authors": "Kurian Tharakunnel and Siddhartha Bhattacharyya"
    },
    {
      "index": 43,
      "title": "A multi-agent reinforcement learning algorithm based on stackelberg game",
      "abstract": "",
      "year": "2017",
      "venue": "2017 6th Data Driven Control and Learning Systems (DDCLS)",
      "authors": "Chi Cheng, Zhangqing Zhu, Bo Xin, and Chunlin Chen"
    },
    {
      "index": 44,
      "title": "Leader-follower mdp models with factored state space and many followers-followers abstraction, structured dynamics and state aggregation",
      "abstract": "",
      "year": "2016",
      "venue": "Twenty-second European Conference on Artificial Intelligence",
      "authors": "RÃ©gis Sabbadin and Anne-France Viet"
    },
    {
      "index": 45,
      "title": "M3RL: Mind-aware Multi-agent Management Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00147",
      "authors": "Tianmin Shu and Yuandong Tian",
      "orig_title": "M3rl: Mind-aware multi-agent management reinforcement learning",
      "paper_id": "1810.00147v3"
    },
    {
      "index": 46,
      "title": "Learning expensive coordination: An event-based deep rl approach",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, and Bo An"
    },
    {
      "index": 47,
      "title": "A unified analysis of value-function-based reinforcement-learning algorithms",
      "abstract": "",
      "year": "1999",
      "venue": "Neural computation",
      "authors": "Csaba SzepesvÃ¡ri and Michael L Littman"
    },
    {
      "index": 48,
      "title": "Bayesian games for threat prediction and situation analysis",
      "abstract": "",
      "year": "2004",
      "venue": "International Conference on Information Fusion",
      "authors": "Joel Brynielsson and Stefan Arnborg"
    }
  ]
}