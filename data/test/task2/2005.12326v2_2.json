{
  "paper_id": "2005.12326v2",
  "title": "Towards Efficient Scheduling of Federated Mobile Devices under Computational and Statistical Heterogeneity",
  "sections": {
    "introduction": "The tremendous success of machine learning stimulates a new wave of smart applications. Despite the great convenience, these applications consume massive personal data, at the expense of our privacy. The growing concerns of privacy become one of the major impetus to shift computation from the centralized cloud to users‚Äô end devices such as mobile, edge and IoTs. The current solution supports running on-device inference from a pre-trained model in near real-time¬† , whereas their capability to adapt to the new data and learn from each other is still limited. Federated Learning (FL) emerges as a practical and cost-effective solution to mitigate the risk of privacy leakage¬†      0 1 2, which enables collaboration on an abstracted level rather than the raw data itself. Originated from distributed learning¬†, it learns a centralized model where the training data is held privately at the end users. Local models are computed in parallel and the updates are aggregated towards a centralized parameter server. The server takes the mean of the parameters from the users, pushes the averaged model back as the initial point for the next iteration. Previous research mainly focuses on the prominent problems left from distributed learning such as improving communication efficiency¬†  0 1 2 3, security robustness¬†9 0, or the learning algorithms¬†2 to address new challenges in FL. Most of these works only contain proof-of-concept implementations on cloud/edge servers with stable, external power and proprietary GPUs. Though pioneering efforts to improve FL at the algorithm level, they still leave a gap to the system-level implementations at the mobile data source, where FL was originally targeting at. Meanwhile, the dramatic increase of mobile processing power has enabled not only on-device inference, but also moderate training (backpropagation)¬†  7, thus providing a basis to launch FL on battery-powered mobile devices. Unfortunately, the vast heterogeneity of mobile processing power has yet to be addressed. Even worse, the high variance among user data adds another layer of statistical heterogeneity¬†2 and makes the selection of participants a nontrivial problem. An inappropriate selection would adversely cause gradient divergence and diminish every effort to reduce computation time. From an empirical study, we first validate that the bottleneck has actually shifted from communication back to computation on consumer mobile devices. The runtime behavior depends on a complex combination of vendor-specific software implementations and the underlying hardware architecture, i.e., the System on a Chip (SoC), power management policies and the input computation intensity of the neural networks. These challenges are magnified in practices when users behave differently. For example, in activity recognition, some users may perform only a few actions (e.g., sitting for a long time), thus leading to highly skewed local distributions, which breaks the independent and identically distributed (IID) assumptions held as a basis for distributed learning. When averaged into the global model, these skewed gradients may have a damaging impact on the collaborative model. Thus, efficient scheduling of FL tasks entails an optimal selection of participants relying on both computation and data distribution. To tackle these challenges, we propose an optimization framework to schedule training using the workloads (amount of training data) as a tuning knob and achieve near-optimal staleness in synchronous gradient aggregation. We build a performance profiler to characterize the relations between training time and data size/model parameters using a multiple linear regressor. Taking the profiles as inputs, we start with the basic case when data is IID (class-balanced), and formulate the problem into a min-max optimization problem to find optimal partitioning of data that achieves the minimum makespan. We propose an efficient ùí™‚Äã(n2‚Äãlog‚Å°n)ùí™superscriptùëõ2ùëõ\\mathcal{O}(n^{2}\\log n) algorithm¬† with ùí™‚Äã(n)ùí™ùëõ\\mathcal{O}(n) analytical solution when the cost function is linear (nùëõn is the number of users). For non-IID data, we introduce a new accuracy cost and a quantitative derivation from the analysis of gradient diversity¬†. Then we re-formulate the problem into a min average cost problem and develop a greedy ùí™‚Äã(m‚Äãn)ùí™ùëöùëõ\\mathcal{O}(mn)-algorithm to assign workloads with the minimum average cost in each step using a variation of bin packing¬†0 (mùëöm is the number of data shards). The observation suggests a nontrivial trade-off between staleness and convergence. The proposed algorithm aims to leverage users‚Äô class distributions to adaptively include/exclude an outlier in order to improve model generalization and convergence speed without sacrificing the epoch-wise training time too much. Finally, the proposed algorithms are evaluated on MNIST and CIFAR10 datasets with a mobile testbed of various smartphone models. The main contributions are summarized below. First, we motivate the design by a series of empirical studies of launching backpropagation on Android. This expands the current research of FL with new discoveries of the fundamental cause of mobile stragglers, as well as offering an explanation to the subtlety in non-IID outliers through the lens of gradient diversity. Second, we formulate the problem to find the optimal scheduling with both IID and non-IID data, and propose polynomial-time algorithms with analytical solutions when the cost profile is linear. Finally, we conduct extensive evaluations on MNIST and CIFAR10 datasets under a mobile testbed with 6 types of device combinations (up to 20 devices). Compared to the benchmarks, the results show 2-100√ó\\times speedups for both IID/non-IID data while boosting the accuracy by 2-7% on MNIST/CIFAR10 for non-IID data. The algorithms demonstrate advantages of avoiding worst-case stragglers and better utilization of the parallelled resources. In contrast to the existing works that decouple learning from system-level implementations, to the best of our knowledge, this is the first work that not only connects them, but also optimizes the overall system performance. The rest of the paper is organized as follows. Section 2 discusses the related works and background. Section 3 and 4 motivate this work with a series of empirical studies. Sections 5 and 6 optimizes training time for IID and non-IID data. Section 7 describes the profiler. Section 8 evaluates the framework on the mobile testbed and dataset. Section 9 discusses the limitation and Section 10 concludes this work."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Large scale distributed deep networks",
      "abstract": "",
      "year": "2012",
      "venue": "NIPS",
      "authors": "J. Dean, et. al."
    },
    {
      "index": 1,
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "abstract": "",
      "year": "2017",
      "venue": "AISTATS",
      "authors": "B. McMahan, E. Moore, D. Ramage, S. Hampson, B. Arcas"
    },
    {
      "index": 2,
      "title": "On the Convergence of FedAvg on Non-IID Data",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "X. Li, K. Huang, W. Yang, S. Wang and Z. Zhang",
      "orig_title": "On the convergence of FedAvg on Non-IID Data",
      "paper_id": "1907.02189v4"
    },
    {
      "index": 3,
      "title": "Federated Learning with Non-IID Data",
      "abstract": "",
      "year": "",
      "venue": "arXiv:1806.00582",
      "authors": "Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin and V. Chandra",
      "orig_title": "Federated Learning with Non-IID Data",
      "paper_id": "1806.00582v2"
    },
    {
      "index": 4,
      "title": "Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS-MLPCD",
      "authors": "E. Jeong, S. Oh, H. Kim, S. Kim, J. Park and M. Bennis",
      "orig_title": "Communication-efficient on-device machine learning: federated distillation and augmentation under non-IID private data",
      "paper_id": "1811.11479v2"
    },
    {
      "index": 5,
      "title": "Towards federated learning at scale: system design",
      "abstract": "",
      "year": "2019",
      "venue": "SysML Conference",
      "authors": "K. Bonawitz, et. al."
    },
    {
      "index": 6,
      "title": "Exploring the Capabilities of Mobile Devices in Supporting Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ACM/IEEE Symposium on Edge Computing",
      "authors": "Y. Chen, S. Biookaghazadeh, and M. Zhao"
    },
    {
      "index": 7,
      "title": "Federated Learning: Strategies for Improving Communication Efficiency",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS",
      "authors": "J. Konecny, et. al.",
      "orig_title": "Federated learning: strategies for improving communication efficiency",
      "paper_id": "1610.05492v2"
    },
    {
      "index": 8,
      "title": "CMFL: Mitigating communication overhead for federated learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE ICDCS",
      "authors": "L. Wang, W. Wang and B. Li"
    },
    {
      "index": 9,
      "title": "Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "X. Lian, et. al."
    },
    {
      "index": 10,
      "title": "Multi-objective Evolutionary Federated Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.07478",
      "authors": "H. Zhu, Y. Jin",
      "orig_title": "Multi-objective evolutionary federated learning",
      "paper_id": "1812.07478v2"
    },
    {
      "index": 11,
      "title": "Federated Multi-Task Learning",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "V. Smith, C. Chiang, M. Sanjabi and A. Talwalkar",
      "orig_title": "Federated multi-task learning",
      "paper_id": "1705.10467v2"
    },
    {
      "index": 12,
      "title": "Client-Edge-Cloud Hierarchical Federated Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv: 1905.06641",
      "authors": "L. Liu, J. Zhang, S. H. Song, K. Letaief",
      "orig_title": "Client-Edge-Cloud hierarchical federated learning",
      "paper_id": "1905.06641v2"
    },
    {
      "index": 13,
      "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "S. Zheng, Q. Meng, T. Wang, W. Chen, N. Yu, Z. Ma and T. Liu",
      "orig_title": "Asynchronous stochastic gradient descent with delay compensation",
      "paper_id": "1609.08326v6"
    },
    {
      "index": 14,
      "title": "More effective distributed ML via a stale synchronous parallel parameter server",
      "abstract": "",
      "year": "2013",
      "venue": "NIPS",
      "authors": "Q. Ho, J. Cipar, H. Cui, J. Kim, S. Lee, P. Gibbons, G. Gibson, G. Ganger and E. Xing"
    },
    {
      "index": 15,
      "title": "Game of Threads: Enabling Asynchronous Poisoning Attacks",
      "abstract": "",
      "year": "2020",
      "venue": "ACM ASPLOS",
      "authors": "J. Vicarte, B. Schriber, R. Paccagnella and C. Fletcher"
    },
    {
      "index": 16,
      "title": "Gradient coding",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS",
      "authors": "R. Tandon, Q. Lei, A. Dimakis and N. Karampatziakis"
    },
    {
      "index": 17,
      "title": "Federated Learning: Collaborative Machine Learning without Centralized Training Data",
      "abstract": "",
      "year": "2017",
      "venue": "Google AI Blog",
      "authors": "B. McMahan and D. Ramage"
    },
    {
      "index": 18,
      "title": "Practical secure aggregation for privacy-preserving machine learning",
      "abstract": "",
      "year": "2017",
      "venue": "ACM CCS",
      "authors": "K. Bonawitz, et. al."
    },
    {
      "index": 19,
      "title": "Machine learning with adversaries: byzantine tolerant gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "P. Blanchard, E. Mhamdi, R. Guerraoui and J. Stainer"
    },
    {
      "index": 20,
      "title": "Apple A13 Bionic chip",
      "abstract": "",
      "year": "",
      "venue": "macworld.com",
      "authors": ""
    },
    {
      "index": 21,
      "title": "Huawei kirin 980 AI chip",
      "abstract": "",
      "year": "",
      "venue": "consumer.huawei.com",
      "authors": ""
    },
    {
      "index": 22,
      "title": "MobileDeepPill: A small-footprint mobile deep learning system for recognizing unconstrained pill images",
      "abstract": "",
      "year": "2017",
      "venue": "ACM Mobisys",
      "authors": "X. Zeng X, K. Cao, M. Zhang"
    },
    {
      "index": 23,
      "title": "Deepeye: Resource efficient local execution of multiple deep vision models using wearable commodity hardware",
      "abstract": "",
      "year": "2017",
      "venue": "ACM Mobisys",
      "authors": "A. Mathur, N. Lane, D. Bhattacharya, S. Boran, A. Forlivesi, C. Kawsar"
    },
    {
      "index": 24,
      "title": "Deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "S. Han, H. Mao and W. J. Dally"
    },
    {
      "index": 25,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "J. Frankle and M. Carbin",
      "orig_title": "The lottery ticket hypothesis: finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 26,
      "title": "Close the gap between deep learning and mobile intelligence by incorporating training in the loop",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Multimedia",
      "authors": "C. Wang, Y. Xiao, X. Gao, L. Li and J. Wang"
    },
    {
      "index": 27,
      "title": "ARM‚Äôs big.LITTLE",
      "abstract": "",
      "year": "",
      "venue": "arm.com",
      "authors": ""
    },
    {
      "index": 28,
      "title": "Android large heap",
      "abstract": "",
      "year": "",
      "venue": "developer.android.com",
      "authors": ""
    },
    {
      "index": 29,
      "title": "Quantifying inductive bias: AI learning algorithms and Valiant‚Äôs learning framework",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Artif. Intell.",
      "authors": "D. Haussler"
    },
    {
      "index": 30,
      "title": "Approximation Schemes for Scheduling on Uniformly Related and Identical Parallel Machines",
      "abstract": "",
      "year": "1999",
      "venue": "ESA",
      "authors": "L Epstein and J. Sgall"
    },
    {
      "index": 31,
      "title": "Optimus: an efficient dynamic resource scheduler for deep learning clusters",
      "abstract": "",
      "year": "2018",
      "venue": "EuroSys",
      "authors": "Y. Peng, Y. Bao, Y. Chen, C. Wu and C. Guo"
    },
    {
      "index": 32,
      "title": "Online Job Scheduling in Distributed Machine Learning Clusters",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE INFOCOM",
      "authors": "Y. Bao, Y. Peng, C. Wu and Z. Li",
      "orig_title": "Online Job Scheduling in Distributed Machine Learning Clusters",
      "paper_id": "1801.00936v1"
    },
    {
      "index": 33,
      "title": "Gradient Diversity: a Key Ingredient for Scalable Distributed Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AISTATS",
      "authors": "D. Yin, A. Pananjady, M. Lam, D. Papailiopoulos, K. Ramchandran and P. Bartlett",
      "orig_title": "Gradient diversity: a key ingredient for scalable distributed learning",
      "paper_id": "1706.05699v3"
    },
    {
      "index": 34,
      "title": "In-depth with the Snapdragon 810‚Äôs heat problems",
      "abstract": "",
      "year": "",
      "venue": "arstechnica.com",
      "authors": ""
    },
    {
      "index": 35,
      "title": "Assignment problems",
      "abstract": "",
      "year": "2012",
      "venue": "SIAM",
      "authors": "R. Burkard, M. Dell‚ÄôAmico and S. Martello"
    },
    {
      "index": 36,
      "title": "Bottleneck generalized assignment problems",
      "abstract": "",
      "year": "1988",
      "venue": "Engineering Costs and Production Economics",
      "authors": "J. B. Mazzola and A. W. Neebe"
    },
    {
      "index": 37,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Y. Lecun, L. Bottou, Y. Bengio and P. Haffner"
    },
    {
      "index": 38,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 39,
      "title": "Bin packing with fragmentable items: Presentation and approximations",
      "abstract": "",
      "year": "2015",
      "venue": "Theo. Comp. Sci.",
      "authors": "B. LeCun, T. Mautor, F. Quessette and M. Weisser"
    },
    {
      "index": 40,
      "title": "MNIST dataset",
      "abstract": "",
      "year": "",
      "venue": "yann.lecun.com",
      "authors": ""
    },
    {
      "index": 41,
      "title": "CIFAR10 dataset",
      "abstract": "",
      "year": "",
      "venue": "cs.toronto.edu",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Deep Learning for Java",
      "abstract": "",
      "year": "",
      "venue": "deeplearning4j.org",
      "authors": ""
    },
    {
      "index": 43,
      "title": "Optimize scheduling of federated learning on battery-powered mobile devices",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE IPDPS",
      "authors": "C. Wang, X. Wei and P. Zhou"
    },
    {
      "index": 44,
      "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "F. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. Dally and Kurt Keutzer",
      "orig_title": "SqueezeNet: AlexNet-level accuracy with 50√ó√ó fewer parameters and <<0.5MB model size",
      "paper_id": "1602.07360v4"
    },
    {
      "index": 45,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE CVPR",
      "authors": "K. He, X. Zhang, S. Ren and J. Sun",
      "orig_title": "Deep Residual Learning for Image Recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 46,
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "C. Szegedy, S. Ioffe, V. Vanhoucke and A. A. Alemi",
      "orig_title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "paper_id": "1602.07261v2"
    },
    {
      "index": 47,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "S. Hochreiter and J. Schmidhuber"
    },
    {
      "index": 48,
      "title": "Restructuring Batch Normalization to Accelerate CNN Training",
      "abstract": "",
      "year": "2019",
      "venue": "SysML",
      "authors": "W. Jung, D. Jung, B. Kim, S. Lee, W. Rhee and J. H. Ahn"
    },
    {
      "index": 49,
      "title": "Machine Learning at Facebook: Understanding Inference at the Edge",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Symposium on High Performance Computer Architecture (HPCA)",
      "authors": "C. Wu, et. al"
    },
    {
      "index": 50,
      "title": "Physionet Challenge",
      "abstract": "",
      "year": "",
      "venue": "physionet.org",
      "authors": ""
    }
  ]
}