{
  "paper_id": "2010.02068v4",
  "title": "Deep Reinforcement Learning for the Electric Vehicle Routing Problem with Time Windows",
  "sections": {
    "i introduction": "Electric vehicles (EV) have been playing an increasingly important role in urban transportation and logistics systems for their capability of reducing greenhouse gas emission, promoting renewable energy and introducing sustainable transportation system  .\nTo model the operations of logistic companies using EVs for service provision, Schneider et al. proposed the electric vehicle routing problem with time windows (EVRPTW) .\nIn the context of EVRPTW, a fleet of capacitated EVs is responsible for serving customers located in a specific region;\neach customer is associated with a demand that must be satisfied during a time window;\nall the EVs are fully charged at the start of the planning horizon and could visit charging stations anytime to fully charge their batteries.\nThe objective is to find routes for the EVs such that total distance travelled by the fleet is minimized. As an NP-hard combinatorial optimization problem (CO), solving the EVRPTW is computationally expensive.\nSchineider et al.  developed a variable neighborhood search and tabu search hybrid meta-heuristic (VNS/TS) that is able to effectively solve benchmark instances.\nIn a later paper , Desaulniers et al. proposed exact branch-and-price-and-cut algorithms for four variants of the EVRPTW according to the number of and the type of recharges.\nBoth algorithms are able to provide high-quality solutions to the EVRPTW benchmark instances introduced in , yet the solution quality and efficiency decrease as the instance size increases.\nIn addition, both algorithms have components that rely on the structure and assumptions of EVRPTW, making it difficult to generalize these algorithms to other EVRPTW variants . This research is motivated by an emerging group of literature on utilizing machine learning (ML) to solve CO. For example, ML could be incorporated into the solution processes to improve primal heuristic [ref]6 , make branching decisions  and generate cuts  in a branch-and-bound framework.\nOn improving primal heuristic, previous studies present two typical paradigms: supervised learning and reinforcement learning (RL).\nSupervised learning models, such as the ones presented in 0 1 2 and 3, are trained with solutions provided by existing algorithms.\nAlthough they could generate near-optimal solutions to the problems they are trained on 0 and could generalize to instances from different distributions 2 and of larger sizes than the ones they have seen during training 1, supervised approaches are not applicable to most CO problems as obtaining optimal labels for CO instances is computationally expensive 4. On the other hand, RL models, such as the ones presented in 4 5 6 7 [ref]6  and 8, could learn to tackle CO even without optimal labels.\nThey consider solving problems through taking a sequence of actions similar to Markov decision process (MDP).\nSome reward schemes are designed to inform the model about the quality of the actions it made based on which model parameters are adjusted to enhance the solution quality.\nRL has already been successfully applied to various COs such as the travelling salesman problem (TSP), vehicle routing problem (VRP), minimum vertex cover (MVC), maximum cut (MAXCUT) etc.\nDespite the difficulty in training deep RL models, it is currently accepted as a very promising research direction to pursue. The main objective of this research is to develop an RL model to solve EVRPTW. In particular, based on the framework proposed by Nazari et al.  for VRP and TSP, we re-define the system state, rewarding schemes as well as the masking policy for EVRPTW.\nThe original framework in  only considers representation of vertex information and does not take into account graph structure as well as global information which is very important in EVRPTW.\nTo this end, we incorporate the model with a graph embedding component put forward by Dai et al. 9 to synthesize local and global information of the graph on which the problem is defined.\nThe model is then trained using the REINFORCE gradient estimator with greedy rollout baseline 8. The proposed model is able to efficiently generate good feasible solutions to EVRPTW instances of very large sizes that are unsolvable with any existing methods.\nIt, therefore, could be implemented to support large-scale real-time EV fleet operations.\nMoreover, the RL model could be incorporated with other solution algorithms as an initialization for meta-heuristics or as a primal heuristic in mixed integer programming (MIP) solvers, which may assist to enhance solution efficiency and quality.\nFurthermore, the model has potential to generalize to other variants of EVRPTW through tailoring the rewarding scheme and masking policy. The remainder of the paper is structured as follows.\nWe review previous related literature in Section II, and formally introduce the problem formulation in Section III.\nWe then describe the reinforcement learning framework for EVRPTW in Section IV and provide detailed illustration on our methodology in Section V.\nComputational results and analysis about the proposed approach are presented in Section VI. Finally, we conclude the paper and suggest possible extensions of the proposed method in Section VII."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Vehicle electrification: Status and issues",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE",
      "authors": "A. G. Boulanger, A. C. Chu, S. Maxx, and D. L. Waltz"
    },
    {
      "index": 1,
      "title": "Plug-in hybrid electric vehicles and smart grids: Investigations based on a microsimulation",
      "abstract": "",
      "year": "2013",
      "venue": "Transportation Research Part C: Emerging Technologies",
      "authors": "R. A. Waraich, M. D. Galus, C. Dobler, M. Balmer, G. Andersson, and K. W. Axhausen"
    },
    {
      "index": 2,
      "title": "The electric vehicle-routing problem with time windows and recharging stations",
      "abstract": "",
      "year": "2014",
      "venue": "Transportation Science",
      "authors": "M. Schneider, A. Stenger, and D. Goeke"
    },
    {
      "index": 3,
      "title": "Exact algorithms for electric vehicle-routing problems with time windows",
      "abstract": "",
      "year": "2016",
      "venue": "Operations Research",
      "authors": "G. Desaulniers, F. Errico, S. Irnich, and M. Schneider"
    },
    {
      "index": 4,
      "title": "Electric vehicle routing with charging/discharging under time-variant electricity prices",
      "abstract": "",
      "year": "2021",
      "venue": "Transportation Research Part C: Emerging Technologies",
      "authors": "B. Lin, B. Ghaddar, and J. Nathwani"
    },
    {
      "index": 5,
      "title": "Learning Combinatorial Optimization Algorithms over Graphs",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song",
      "orig_title": "Learning combinatorial optimization algorithms over graphs",
      "paper_id": "1704.01665v4"
    },
    {
      "index": 6,
      "title": "Reinforcement Learning for Solving the Vehicle Routing Problem",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Nazari, A. Oroojlooy, L. Snyder, and M. Takác",
      "orig_title": "Reinforcement learning for solving the vehicle routing problem",
      "paper_id": "1802.04240v2"
    },
    {
      "index": 7,
      "title": "Learning to branch in mixed integer programming",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "E. Khalil, P. Le Bodic, L. Song, G. Nemhauser, and B. Dilkina"
    },
    {
      "index": 8,
      "title": "Reinforcement Learning for Integer Programming: Learning to Cut",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Tang, S. Agrawal, and Y. Faenza",
      "orig_title": "Reinforcement learning for integer programming: Learning to cut",
      "paper_id": "1906.04859v3"
    },
    {
      "index": 9,
      "title": "Pointer networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "O. Vinyals, M. Fortunato, and N. Jaitly"
    },
    {
      "index": 10,
      "title": "Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Z. Li, Q. Chen, and V. Koltun",
      "orig_title": "Combinatorial optimization with graph convolutional networks and guided tree search",
      "paper_id": "1810.10659v1"
    },
    {
      "index": 11,
      "title": "Learning a SAT solver from single-bit supervision",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Selsam, M. Lamm, B. Bünz, P. Liang, L. de Moura, and D. L. Dill"
    },
    {
      "index": 12,
      "title": "Learning to Solve NP-Complete Problems: A Graph Neural Network for Decision TSP",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "M. Prates, P. H. Avelar, H. Lemos, L. C. Lamb, and M. Y. Vardi",
      "orig_title": "Learning to solve np-complete problems: A graph neural network for decision tsp",
      "paper_id": "1809.02721v3"
    },
    {
      "index": 13,
      "title": "Neural Combinatorial Optimization with Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations Workshop",
      "authors": "I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio",
      "orig_title": "Neural combinatorial optimization with reinforcement learning",
      "paper_id": "1611.09940v3"
    },
    {
      "index": 14,
      "title": "A reinforcement learning approach to job-shop scheduling",
      "abstract": "",
      "year": "1995",
      "venue": "IJCAI",
      "authors": "W. Zhang and T. G. Dietterich"
    },
    {
      "index": 15,
      "title": "Learning heuristics over large graphs via deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "S. Manchanda, A. Mittal, A. Dhawan, S. Medya, S. Ranu, and A. K. Singh"
    },
    {
      "index": 16,
      "title": "Exploratory combinatorial optimization with reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI conference",
      "authors": "T. D. Barrett, W. R. Clements, J. N. Foerster, and A. I. Lvovsky"
    },
    {
      "index": 17,
      "title": "Attention, Learn to Solve Routing Problems!",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "W. Kool, H. van Hoof, and M. Welling",
      "orig_title": "Attention, learn to solve routing problems!",
      "paper_id": "1803.08475v3"
    },
    {
      "index": 18,
      "title": "Discriminative embeddings of latent variable models for structured data",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "H. Dai, B. Dai, and L. Song"
    },
    {
      "index": 19,
      "title": "Machine learning for combinatorial optimization: a methodological tour d’horizon",
      "abstract": "",
      "year": "2020",
      "venue": "European Journal of Operational Research",
      "authors": "Y. Bengio, A. Lodi, and A. Prouvost"
    },
    {
      "index": 20,
      "title": "“neural” computation of decisions in optimization problems",
      "abstract": "",
      "year": "1985",
      "venue": "Biological cybernetics",
      "authors": "J. J. Hopfield and D. W. Tank"
    },
    {
      "index": 21,
      "title": "On the stability of the travelling salesman problem algorithm of hopfield and tank",
      "abstract": "",
      "year": "1988",
      "venue": "Biological Cybernetics",
      "authors": "G. Wilson and G. Pawley"
    },
    {
      "index": 22,
      "title": "Sequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Sutskever, O. Vinyals, and Q. V. Le"
    },
    {
      "index": 23,
      "title": "Online vehicle routing with neural combinatorial optimization and deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "authors": "J. James, W. Yu, and J. Gu"
    },
    {
      "index": 24,
      "title": "A hybrid of deep reinforcement learning and local search for the vehicle routing problems",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "authors": "J. Zhao, M. Mao, X. Zhao, and J. Zou"
    },
    {
      "index": 25,
      "title": "Operating electric vehicle fleet for ride-hailing services with reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "authors": "J. Shi, Y. Gao, W. Wang, N. Yu, and P. A. Ioannou"
    },
    {
      "index": 26,
      "title": "Reinforcement-learning-based cooperative adaptive cruise control of buses in the lincoln tunnel corridor with time-varying topology",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "authors": "W. Gao, J. Gao, K. Ozbay, and Z.-P. Jiang"
    },
    {
      "index": 27,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. Bahdanau, K. Cho, and Y. Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 28,
      "title": "Neural machine translation and sequence-to-sequence models: A tutorial",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.01619",
      "authors": "G. Neubig"
    },
    {
      "index": 29,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 30,
      "title": "Algorithms for the vehicle routing and scheduling problems with time window constraints",
      "abstract": "",
      "year": "1987",
      "venue": "Operations research",
      "authors": "M. M. Solomon"
    },
    {
      "index": 31,
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "abstract": "",
      "year": "2010",
      "venue": "International conference on artificial intelligence and statistics",
      "authors": "X. Glorot and Y. Bengio"
    }
  ]
}