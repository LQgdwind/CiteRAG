{
  "paper_id": "2401.00775v2",
  "title": "Recent Advances in Text Analysis",
  "sections": {
    "deep neural network approaches to natural language processing": "The deep neural network approaches to natural language processing\n(DNN-NLP)\nhave become very popular recently, with successes observed\nin a variety of NLP tasks such as text classification, question answering, machine translation, among others . In statistics, a “model” is a generative model with\nsome unknown parameters we need to estimate. In DNN-NLP,\nresearchers use the term “model” slightly differently:\na neural language model usually refers to a pre-trained neural network equipped with estimated parameters.\nA neural language model usually consists of three components as follows. A neural network architecture. This is the core of a neural language model. It specifies how an input text is processed to generate the desirable output. The encoder-decoder structure is commonly used: the encoder is a neural network that maps the input text into a numeric vector (a.k.a., the encoder state), and the decoder converts the encoder state to the targeted output (e.g., a variable-length sequence of tokens).\nMany neural network models were inspired by new architectures proposed in the literature. The NLP tasks used to train the neural networks. A neural language model\nusually targets on one specific task (e.g., machine translation) or several specific NLP tasks (e.g., the BERT model  outputs document embeddings, which can be used in various downstream tasks).\nIn either case, pre-training the neural networks (i.e., estimating the parameters) must use specific NLP tasks to define the objective function. Hence, the same architecture may lead to different neural language models if they are pre-trained using different NLP tasks. The text corpora and domain knowledge used in training.\nEven with the same architecture and the same NLP tasks in training, the resulting neural language model still varies with the training corpora. One strategy is selecting training corpora to obtain a domain-specific language model. For example, BERT has variants such as BioBERT  trained using publications in biomedicine. Besides domain-specific corpora, other knowledge such as a domain-specific vocabulary can also be employed. The research on DNN-NLP has multiple goals, including but not limited to (a) Prediction of the next word given the previous words in a sentence (e.g., GPT family ), (b) Extraction of numeric features from text (e.g., BERT family ), and (c) modeling the (synatic and semantic) relationships of words (e.g., word2vec [ref]35).\nDNN-NLP is a fast-developing area, which is hard to review comprehensively (especially as our focus is on the topic modeling approaches and the\nMADStat data). For these reasons, we select a few interesting topics in DNN-NLP to review, focusing on\n(a) popular DNN architectures for NLP, (b) BERT, a powerful feature extraction tool developed by Google Inc. We also discuss word embedding and how to apply a neural language model (e.g., BERT) to a text corpus in our own research (see Remarks 1-2)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "A practical algorithm for topic modeling with provable guarantees",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on machine learning",
      "authors": "Arora, S., R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu"
    },
    {
      "index": 1,
      "title": "Learning topic models–going beyond SVD",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE 53rd Annual Symposium on Foundations of Computer Science",
      "authors": "Arora, S., R. Ge, and A. Moitra"
    },
    {
      "index": 2,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.0473",
      "authors": "Bahdanau, D., K. Cho, and Y. Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 3,
      "title": "A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics.",
      "abstract": "",
      "year": "2020",
      "venue": "Bernoulli",
      "authors": "Bing, X., F. Bunea, and M. Wegkamp",
      "orig_title": "A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics",
      "paper_id": "1805.06837v3"
    },
    {
      "index": 4,
      "title": "Latent dirichlet allocation",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of Machine Learning Research",
      "authors": "Blei, D. M., A. Y. Ng, and M. I. Jordan"
    },
    {
      "index": 5,
      "title": "Testing high-dimensional multinomials with applications to text analysis",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.01381",
      "authors": "Cai, T. T., Z. T. Ke, and P. Turner"
    },
    {
      "index": 6,
      "title": "Indexing by latent semantic analysis",
      "abstract": "",
      "year": "1990",
      "venue": "Journal of the American Society for Information Science",
      "authors": "Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman"
    },
    {
      "index": 7,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 8,
      "title": "50 years of data science",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Computational and Graphical Statistics",
      "authors": "Donoho, D."
    },
    {
      "index": 9,
      "title": "Higher criticism for large-scale inference, especially for rare and weak effects",
      "abstract": "",
      "year": "2015",
      "venue": "Statistical science",
      "authors": "Donoho, D. and J. Jin"
    },
    {
      "index": 10,
      "title": "When does non-negative matrix factorization give a correct decomposition into parts?",
      "abstract": "",
      "year": "2003",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Donoho, D. and V. Stodden"
    },
    {
      "index": 11,
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th international conference on computational linguistics: technical papers",
      "authors": "Dos Santos, C. and M. Gatti"
    },
    {
      "index": 12,
      "title": "Experiments in automatic phrase indexing for document retrieval: A comparison of syntactic and nonsyntactic methods",
      "abstract": "",
      "year": "1988",
      "venue": "Cornell University",
      "authors": "Fagan, J. L."
    },
    {
      "index": 13,
      "title": "Fast and robust recursive algorithmsfor separable nonnegative matrix factorization",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Gillis, N. and S. A. Vavasis"
    },
    {
      "index": 14,
      "title": "The first text retrieval conference (TREC-1), Volume 500",
      "abstract": "",
      "year": "1993",
      "venue": "US Department of Commerce, National Institute of Standards and Technology",
      "authors": "Harman, D. K."
    },
    {
      "index": 15,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Hochreiter, S. and J. Schmidhuber"
    },
    {
      "index": 16,
      "title": "Probabilistic latent semantic indexing",
      "abstract": "",
      "year": "1999",
      "venue": "22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": "Hofmann, T."
    },
    {
      "index": 17,
      "title": "Matrix Analysis (2nd ed.)",
      "abstract": "",
      "year": "2013",
      "venue": "Cambridge University Press",
      "authors": "Horn, R. A. and C. R. Johnson"
    },
    {
      "index": 18,
      "title": "Co-citation and Co-authorship Networks of Statisticians",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of Business & Economic Statistics",
      "authors": "Ji, P., J. Jin, Z. T. Ke, and W. Li",
      "orig_title": "Co-citation and co-authorship networks of statisticians",
      "paper_id": "2204.11194v1"
    },
    {
      "index": 19,
      "title": "Fast community detection by SCORE",
      "abstract": "",
      "year": "2015",
      "venue": "The Annals of Statistics",
      "authors": "Jin, J."
    },
    {
      "index": 20,
      "title": "Network global testing by counting graphlets",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Jin, J., Z. Ke, and S. Luo"
    },
    {
      "index": 21,
      "title": "Optimal adaptivity of signed-polygon statistics for network testing",
      "abstract": "",
      "year": "2021",
      "venue": "The Annals of Statistics",
      "authors": "Jin, J., Z. T. Ke, and S. Luo"
    },
    {
      "index": 22,
      "title": "Mixed membership estimation for social networks",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Econometrics",
      "authors": "Jin, J., Z. T. Ke, and S. Luo"
    },
    {
      "index": 23,
      "title": "A convolutional neural network for modelling sentences",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1404.2188",
      "authors": "Kalchbrenner, N., E. Grefenstette, and P. Blunsom"
    },
    {
      "index": 24,
      "title": "Defining and Identifying Sleeping Beauties in Science",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the National Academy of Sciences",
      "authors": "Ke, Q., E. Ferrara, F. Radicchi, and A. Flammini",
      "orig_title": "Defining and identifying sleeping beauties in science",
      "paper_id": "1505.06454v1"
    },
    {
      "index": 25,
      "title": "Special invited paper: The SCORE normalization, especially for heterogeneous network and text data",
      "abstract": "",
      "year": "2023",
      "venue": "Stat",
      "authors": "Ke, Z. T. and J. Jin"
    },
    {
      "index": 26,
      "title": "Predicting returns with text data",
      "abstract": "",
      "year": "2019",
      "venue": "National Bureau of Economic Research",
      "authors": "Ke, Z. T., B. T. Kelly, and D. Xiu"
    },
    {
      "index": 27,
      "title": "Using SVD for topic modeling",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of the American Statistical Association",
      "authors": "Ke, Z. T. and M. Wang"
    },
    {
      "index": 28,
      "title": "Assigning topics to documents by successive projections",
      "abstract": "",
      "year": "2023",
      "venue": "The Annals of Statistics",
      "authors": "Klopp, O., M. Panov, S. Sigalla, and A. B. Tsybakov"
    },
    {
      "index": 29,
      "title": "Discussion of “Coauthorship and citation networks for statisticians”",
      "abstract": "",
      "year": "2016",
      "venue": "Annals of Applied Statistics",
      "authors": "Kolar, M. and M. Taddy"
    },
    {
      "index": 30,
      "title": "Learning the parts of objects by non-negative matrix factorization",
      "abstract": "",
      "year": "1999",
      "venue": "Nature",
      "authors": "Lee, D. D. and H. S. Seung"
    },
    {
      "index": 31,
      "title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining",
      "abstract": "",
      "year": "2020",
      "venue": "Bioinformatics",
      "authors": "Lee, J., W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang"
    },
    {
      "index": 32,
      "title": "Supervised topic models",
      "abstract": "",
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Mcauliffe, J. and D. Blei"
    },
    {
      "index": 33,
      "title": "A note on EM algorithm for probabilistic latent semantic analysis",
      "abstract": "",
      "year": "2001",
      "venue": "International Conference on Information and Knowledge Management, CIKM",
      "authors": "Mei, Q. and C. Zhai"
    },
    {
      "index": 34,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1301.3781",
      "authors": "Mikolov, T., K. Chen, G. Corrado, and J. Dean"
    },
    {
      "index": 35,
      "title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Otter, D. W., J. R. Medina, and J. K. Kalita",
      "orig_title": "A survey of the usages of deep learning for natural language processing",
      "paper_id": "1807.10854v3"
    },
    {
      "index": 36,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Radford, A., K. Narasimhan, T. Salimans, I. Sutskever, et al."
    },
    {
      "index": 37,
      "title": "End-to-end transformer-based models in textual-based NLP",
      "abstract": "",
      "year": "2023",
      "venue": "AI",
      "authors": "Rahali, A. and M. A. Akhloufi"
    },
    {
      "index": 38,
      "title": "Weaving the fabric of science: Dynamic network models of science’s unfolding structure",
      "abstract": "",
      "year": "2015",
      "venue": "Social Networks",
      "authors": "Shi, F., J. G. Foster, and J. A. Evans"
    },
    {
      "index": 39,
      "title": "Citation patterns in the journals of statistics and probability",
      "abstract": "",
      "year": "1994",
      "venue": "Statistical Science",
      "authors": "Stigler, S. M."
    },
    {
      "index": 40,
      "title": "On estimation and selection for topic models",
      "abstract": "",
      "year": "2012",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "Taddy, M."
    },
    {
      "index": 41,
      "title": "Statistical modeling of citation exchange between statistics journals (with discussions)",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of the Royal Statical Society: Series A",
      "authors": "Varin, C., M. Cattelan, and D. Firth"
    },
    {
      "index": 42,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 43,
      "title": "Topic modeling: beyond bag-of-words",
      "abstract": "",
      "year": "2006",
      "venue": "23rd international conference on Machine learning",
      "authors": "Wallach, H. M."
    },
    {
      "index": 44,
      "title": "Sparse topic modeling: Computational efficiency, near-optimal algorithms, and statistical inference",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of the American Statistical Association",
      "authors": "Wu, R., L. Zhang, and T. Tony Cai"
    },
    {
      "index": 45,
      "title": "A heuristic approach to determine an appropriate number of topics in topic modeling",
      "abstract": "",
      "year": "2015",
      "venue": "BMC bioinformatics",
      "authors": "Zhao, W., J. J. Chen, R. Perkins, Z. Liu, W. Ge, Y. Ding, and W. Zou"
    },
    {
      "index": 46,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler"
    }
  ]
}