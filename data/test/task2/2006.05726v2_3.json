{
  "paper_id": "2006.05726v2",
  "title": "Estimating semantic structure for the VQA answer space",
  "sections": {
    "results": "Model-agnosticity — \nTable 1 shows the effectiveness of our semantic loss on VQAv2-CP\nwith three models. The proposed approach improves accuracy by +2.02.0+2.0, +0.90.9+0.9 and\n+0.50.5+0.5 points on, respectively, MCAN, BAN and UpDn models when using the Glove semantic space. The gain is in general even higher when using Co-oc semantic space, leading to improvements of of +2.92.9+2.9 (MCAN), 0.80.80.8 (BAN) and +0.10.1+0.1 (UpDn).\nThe on-par performance of Co-oc with respect to Glove (or arguably superior), despite the estimation of Glove from large-scale datasets, illustrates the strength of the goal-directed estimation strategy of Co-oc and the thus learning signal directly derived from human annotations. We observe that the impact of the semantic loss on the UpDn architecture is less significant than on BAN and MCAN, both for the Co-oc and Glove semantic spaces.\nWe conjecture that this is due to the higher dependency of UpDn on the question bias.\nTo further investigate this, we performed experiments combining the semantic loss with a state-of-the-art bias reduction method. Complementarity of gains with bias-reduction methods — \nWe combine the semantic loss with RUBi [ref]5,\na state-of-the-art method designed to reduce language biases in VQA models. RUBi combines standard VQA models with a second question-only branch, whose objective is the explicit estimation of language biases. During training time, the prediction of the question-only branch is used as a mask combined with the VQA branch by element-wise multiplication, which drives the VQA model to overcome the inherent language bias. The masking is removed during testing. As shown in Table 2, the semantic loss (in the using Glove variant) improves upon the combination of UpDn model architecture + RUBi training with a margin of +3.33.3+3.3 points and reaches an accuracy of 47.5%percent47.547.5\\% on the VQAv2-CP test split444We observe an instability when using UpDn+RUBi which occasionally prevents the model from converging. As a consequence, we provide the average accuracy over four converged models with random seeds along with the standard deviation..\nThis indicates that the proposed loss is complementary to existing bias-reduction approaches and improves the generalization and reasoning abilities of the model. Impact on VQAv2 dataset —  As discussed before, the VQAv2-CP dataset has been proposed by   with the goal to evaluate the performance of VQA models in a condition where they cannot fully rely on question biases.\nIndeed, as shown in , the original VQAv2  dataset contains numerous question biases (e.g. the question \\saywhat color is the banana in the picture can be correctly answered as \\sayyellow without even analyzing the picture in VQAv2).\nAt the same time, it is very important to verify that our semantic loss, which is effective for training VQA models on the unbiased VQAv2-CP dataset, does not hinder the model’s performances when the training is done on the biased VQAv2 dataset. Therefore, Table 3 analyzes the impact of our semantic loss on the VQAv2 dataset and compares it with recent approaches designed to remove the language bias in VQA.\nMore precisely, we compare the accuracies on the VQAv2 validation dataset of baseline VQA models (the original baselines from the respective works are taken) with and without one of the SOTA approaches aiming to reduce the question biases.\nFor a fair comparison, we only compare with methods which does not rely on extra annotated supervision such as HINT  and SCR(VQA-X) [ref]17.\nThe impacts of the compared approaches on the respective baselines are highlighted in the \\sayΔΔ\\Delta column of Table 3. When combining the semantic loss with MCAN  in the Co-oc variant, we observe a marginal drop of −0.10.1-0.1 in accuracy.\nOn the contrary, SCR (QA) [ref]17 and RUBi [ref]5 cause significant drops of performance of respectively −1.21.2-1.2 and −2.62.6-2.6 points.\nThe drop of the recent DLR  method is even more impressive reaching almost −55-5 accuracy points.\nAll in all, contrary to other SOTA methods, our semantic loss allows to reduce the dependency on question biases (cf. results presented in Table 1) without sacrificing accuracy on the biased VQAv2 dataset. Comparison with the state-of-the-art — \nWe compare our method with SOTA approaches on the VQAv2-CP dataset in Table 4. For a fair comparison, we divide Table 4 into two groups: methods based on the UpDn [ref]7 architecture and the others. When combining the proposed loss along with another bias reduction method – namely RUBi [ref]5 – using the UpDn architecture, we achieve a SOTA-level accuracy of 47.5%percent47.547.5\\%.\nNote that, contrary to HINT  or SCR(VQA-X) [ref]17, our approach does not require any additional annotations.\nDLR  achieves a high accuracy (48.5%percent48.548.5\\%) on VQAv2-CP. However, unlike our approach, DLR causes a significant drop in accuracy (of −4.94.9-4.9 points) on the biased VQAv2 dataset as highlighted in Table 3."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Vqa: Visual question answering",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh"
    },
    {
      "index": 1,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 2,
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Drew A Hudson and Christopher D Manning"
    },
    {
      "index": 3,
      "title": "Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi",
      "orig_title": "Don’t just assume; look and answer: Overcoming priors for visual question answering",
      "paper_id": "1712.00377v2"
    },
    {
      "index": 4,
      "title": "Rubi: Reducing unimodal biases for visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al."
    },
    {
      "index": 5,
      "title": "Stacked Attention Networks for Image Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola",
      "orig_title": "Stacked attention networks for image question answering",
      "paper_id": "1511.02274v2"
    },
    {
      "index": 6,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 7,
      "title": "Bilinear Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang",
      "orig_title": "Bilinear attention networks",
      "paper_id": "1805.07932v2"
    },
    {
      "index": 8,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 9,
      "title": "Deep Modular Co-Attention Networks for Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian",
      "orig_title": "Deep modular co-attention networks for visual question answering",
      "paper_id": "1906.10770v1"
    },
    {
      "index": 10,
      "title": "Analyzing the behavior of visual question answering models",
      "abstract": "",
      "year": "2016",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "Aishwarya Agrawal, Dhruv Batra, and Devi Parikh"
    },
    {
      "index": 11,
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick",
      "orig_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "paper_id": "1612.06890v1"
    },
    {
      "index": 12,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2016",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra",
      "orig_title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "paper_id": "1606.03556v2"
    },
    {
      "index": 13,
      "title": "Women also Snowboard: Overcoming Bias in Captioning Models",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach",
      "orig_title": "Women also snowboard: Overcoming bias in captioning models",
      "paper_id": "1803.09797v4"
    },
    {
      "index": 14,
      "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee",
      "orig_title": "Overcoming language priors in visual question answering with adversarial regularization",
      "paper_id": "1810.03649v2"
    },
    {
      "index": 15,
      "title": "Taking a hint: Leveraging explanations to make vision and language models more grounded",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh"
    },
    {
      "index": 16,
      "title": "Self-critical reasoning for robust visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jialin Wu and Raymond Mooney"
    },
    {
      "index": 17,
      "title": "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach",
      "orig_title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
      "paper_id": "1802.08129v1"
    },
    {
      "index": 18,
      "title": "Overcoming language priors in vqa via decomposed linguistic representations",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, and Qi Wu"
    },
    {
      "index": 19,
      "title": "Facial age estimation by learning from label distributions",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Xin Geng, Chao Yin, and Zhi-Hua Zhou"
    },
    {
      "index": 20,
      "title": "Label distribution learning",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "Xin Geng"
    },
    {
      "index": 21,
      "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "Mateusz Malinowski and Mario Fritz",
      "orig_title": "A multi-world approach to question answering about real-world scenes based on uncertain input",
      "paper_id": "1410.0210v4"
    },
    {
      "index": 22,
      "title": "Verbs semantics and lexical selection",
      "abstract": "",
      "year": "1994",
      "venue": "Association for Computational Linguistics",
      "authors": "Zhibiao Wu and Martha Palmer"
    },
    {
      "index": 23,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D Manning"
    },
    {
      "index": 24,
      "title": "Distributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in neural information processing systems",
      "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean"
    },
    {
      "index": 25,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Laurens van der Maaten and Geoffrey Hinton",
      "orig_title": "Visualizing data using t-sne",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 26,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yazhe Li Aaron van den Oord and Oriol Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 27,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 28,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 29,
      "title": "Learning by Abstraction: The Neural State Machine",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Drew Hudson and Christopher D Manning",
      "orig_title": "Learning by abstraction: The neural state machine",
      "paper_id": "1907.03950v4"
    }
  ]
}