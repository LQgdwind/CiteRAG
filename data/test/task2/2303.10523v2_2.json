{
  "paper_id": "2303.10523v2",
  "title": "Unsupervised Interpretable Basis Extraction for Concept â€“ Based Visual Explanations",
  "sections": {
    "experimental results": "Overall Evaluation Approach\nTo the best of our knowledge, the proposed method is the first unsupervised method to suggest an interpretable basis. In addition to this, and once again to the best of our knowledge, except from [ref]11 which performs this in a statistical manner, the proposed method is the first unsupervised method to also provide an estimate for the position of the hyperplane that separates each conceptâ€™s representations from the representations of other concepts. Therefore, we quantitatively evaluate the interpretability of the bases extracted with the proposed method against the interpretability of the natural feature space basis (baseline). Apart from this, we quantitatively evaluate the bases extracted with our method with the bases extracted via the supervised approach of [ref]8 and thus setting a baseline for future unsupervised works. An exhaustive search and ablation study over all hyper-parameters is difficult, due to the sheer number of parameters, combinations and computational resource constraints. Nevertheless, the results presented below show that by making simple and intuitive hyper-parameter choices, one may obtain a basis that is more interpretable than the natural. In [ref]11, Bau et. al proved experimentally that the natural feature space basis is more interpretable than other random bases. In this work, we build on the previous findings of [ref]11 and show that the proposed method is able to suggest a basis which is more interpretable than the natural and consequently more interpretable than most other random bases. In short, the main advantage of the proposed method is that it can provide an improvement over the interpretability of the natural basis, and do so, without annotations. Moreover, future, more exhaustive work on fine-tuning strategies has the potential to further improve interpretability. In all of our experiments we used the Broden [ref]11 concept dataset to probe the networks and obtain intermediate layer feature representations. Except for comparison with the supervised approach (Section 7.3), where we only used the object and part categories of the dataset, on all other experiments we used the complete set of concept categories, namely {scene, object, part, texture, material, color}. In all experiments, we used post ReLU activations of the considered networkâ€™s last-layer. A networkâ€™s last-layer refers to the latest convolutional or max-pooling layer where the representation remains spatial, before the flattening to the latest fully-connected one. To learn an interpretable basis with the proposed method, we used the training split of the concept dataset. Next, we used the same training split to label the basis using [ref]11, and finally, we calculated the basis interpretability scores (eq. (9), (10)) using the validation split of the same dataset. Annotation labels were only used to label the bases and perform quantitative evaluation, and were not used in any way to learn the aforementioned bases. Regarding the evaluation of the natural basis (baseline), we used ğ°i=ğisubscriptğ°ğ‘–subscriptğğ‘–\\mathbf{w}_{i}=\\mathbf{e}_{i}, ği=[0,â€¦,0âŸiâ€‹Â times,1,0,â€¦,0âŸDâˆ’iâˆ’1â€‹times]Tsubscriptğğ‘–superscriptsubscriptâŸ0â€¦0ğ‘–Â times1subscriptâŸ0â€¦0ğ·ğ‘–1timesğ‘‡\\mathbf{e}_{i}=[\\underbrace{0,...,0}_{i\\text{ times}},1,\\underbrace{0,...,0}_{D-i-1\\,\\text{times}}]^{T} and we chose the thresholds bisubscriptğ‘ğ‘–b_{i} according to the top 0.005 â€“ quantile among the population of projected representations, as suggested by [ref]11. The rest of the evaluation pipeline was the same as before. Finally, to establish comparisons, we also used the same interpretability score functions of Section 6, in order to evaluate the bases extracted with the supervised approach of [ref]8. In that case, the bases were learned in a supervised way using the training split of the concept dataset. Given the a-priory known concept labels of the basis vectors, evaluation was performed on the validation split of the dataset, ommiting the basis labeling procedure which is not required. The overall evaluation pipeline is depicted in Fig. 8. Basis Learning Details To learn each one of the basis, we used the Adam [ref]29 optimizer with the default beta parameters (0.9,â€‰0.9990.90.9990.9,\\,0.999) provided by the PyTorch  implementation. We fixed the learning rate to 0.0010.0010.001 and did not employ any form of learning rate scheduling. In all cases, basis learning lasted for 300300300 epochs. Batch size was a variable that varied across our experiments and its value was based solely on the available GPU memory resources. The values we used, approximately lied in the interval â‰ˆ[800âˆ’3600]absentdelimited-[]8003600\\approx[800-3600]. Hyper-parameters We kept most of the hyper-parameters of our method fixed to the same values across all the presented experiments, except for the parameters we wanted to ablate. We linearly combined the loss terms with the weights given in Table 1. Empirical evaluation showed that Î»mâ€‹asuperscriptğœ†ğ‘šğ‘\\lambda^{ma} should have higher weight than Î»ssuperscriptğœ†ğ‘ \\lambda^{s} due to the fact that even if the entropy sparsity criterion is fulfilled, the basis may still be not meaningful (Fig. 6). The choice for the rest of the weights was guided by intuition for the relative importance across loss terms. In all of our experiments we used I=Dğ¼ğ·I=D, while extensive study for cases where I<Dğ¼ğ·I<D is left for future work. Parameter Initialization In all of our experiments we initialize the basis vectors with the vectors of the natural feature space basis (i.e ğ°i=ğisubscriptğ°ğ‘–subscriptğğ‘–\\mathbf{w}_{i}=\\mathbf{e}_{i}). We also initialize tğ‘¡t and bğ‘b with t=0.5ğ‘¡0.5t=0.5 and b=0.5ğ‘0.5b=0.5."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 1,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 2,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv:1409.4842 [cs]",
      "authors": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 3,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 4,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6199",
      "authors": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus"
    },
    {
      "index": 5,
      "title": "Understanding intermediate layers using linear classifier probes",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1610.01644",
      "authors": "G. Alain and Y. Bengio",
      "orig_title": "Understanding intermediate layers using linear classifier probes",
      "paper_id": "1610.01644v4"
    },
    {
      "index": 6,
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al."
    },
    {
      "index": 7,
      "title": "Interpretable basis decomposition for visual explanation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "B. Zhou, Y. Sun, D. Bau, and A. Torralba"
    },
    {
      "index": 8,
      "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "R. Fong and A. Vedaldi",
      "orig_title": "Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks",
      "paper_id": "1801.03454v2"
    },
    {
      "index": 9,
      "title": "Robust Semantic Interpretability: Revisiting Concept Activation Vectors",
      "abstract": "",
      "year": "2020",
      "venue": "Fifth Annual Workshop on Human Interpretability in Machine Learning (WHI), ICML 2020",
      "authors": "J. Pfau, A. T. Young, J. Wei, M. L. Wei, and M. J. Keiser",
      "orig_title": "Robust semantic interpretability: Revisiting concept activation vectors",
      "paper_id": "2104.02768v1"
    },
    {
      "index": 10,
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba",
      "orig_title": "Network dissection: Quantifying interpretability of deep visual representations",
      "paper_id": "1704.05796v1"
    },
    {
      "index": 11,
      "title": "Compositional Explanations of Neurons",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Mu and J. Andreas",
      "orig_title": "Compositional explanations of neurons",
      "paper_id": "2006.14032v2"
    },
    {
      "index": 12,
      "title": "Towards Automatic Concept-based Explanations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.03129",
      "authors": "A. Ghorbani, J. Wexler, J. Zou, and B. Kim",
      "orig_title": "Towards automatic concept-based explanations",
      "paper_id": "1902.03129v3"
    },
    {
      "index": 13,
      "title": "Interpretable Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Q. Zhang, Y. N. Wu, and S.-C. Zhu",
      "orig_title": "Interpretable convolutional neural networks",
      "paper_id": "1710.00935v4"
    },
    {
      "index": 14,
      "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "H. Liang, Z. Ouyang, Y. Zeng, H. Su, Z. He, S.-T. Xia, J. Zhu, and B. Zhang",
      "orig_title": "Training interpretable convolutional neural networks by differentiating class-specific filters",
      "paper_id": "2007.08194v3"
    },
    {
      "index": 15,
      "title": "Semantic bottlenecks: Quantifying and improving inspectability of deep representations",
      "abstract": "",
      "year": "2021",
      "venue": "International Journal of Computer Vision",
      "authors": "M. Losch, M. Fritz, and B. Schiele"
    },
    {
      "index": 16,
      "title": "Unified Perceptual Parsing for Scene Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun",
      "orig_title": "Unified perceptual parsing for scene understanding",
      "paper_id": "1807.10221v1"
    },
    {
      "index": 17,
      "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele",
      "orig_title": "The cityscapes dataset for semantic urban scene understanding",
      "paper_id": "1604.01685v2"
    },
    {
      "index": 18,
      "title": "Towards a Definition of Disentangled Representations",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.02230",
      "authors": "I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. Rezende, and A. Lerchner",
      "orig_title": "Towards a definition of disentangled representations",
      "paper_id": "1812.02230v1"
    },
    {
      "index": 19,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "S. Ioffe and C. Szegedy"
    },
    {
      "index": 20,
      "title": "Trivializations for Gradient-Based Optimization on Manifolds",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Lezcano-Casado",
      "orig_title": "Trivializations for gradient-based optimization on manifolds",
      "paper_id": "1909.09501v2"
    },
    {
      "index": 21,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "MIT Press",
      "authors": "I. Goodfellow, Y. Bengio, and A. Courville",
      "orig_title": "Deep Learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 22,
      "title": "Support-vector networks",
      "abstract": "",
      "year": "1995",
      "venue": "Machine learning",
      "authors": "C. Cortes and V. Vapnik"
    },
    {
      "index": 23,
      "title": "Unique arrangements of points on a sphere",
      "abstract": "",
      "year": "1952",
      "venue": "The American Mathematical Monthly",
      "authors": "L. Whyte"
    },
    {
      "index": 24,
      "title": "On the origin of number and arrangement of the places of exit on the surface of pollen-grains",
      "abstract": "",
      "year": "1930",
      "venue": "Recueil des travaux botaniques nÃ©erlandais",
      "authors": "P. M. L. Tammes"
    },
    {
      "index": 25,
      "title": "The densest packing of equal circles on a sphere",
      "abstract": "",
      "year": "1991",
      "venue": "Acta Crystallographica Section A: Foundations of Crystallography",
      "authors": "D. Kottwitz"
    },
    {
      "index": 26,
      "title": "Finding and investigating exact spherical codes",
      "abstract": "",
      "year": "2009",
      "venue": "Experimental Mathematics",
      "authors": "J. Wang"
    },
    {
      "index": 27,
      "title": "Interpreting Deep Visual Representations via Network Dissection",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE TPAMI",
      "authors": "B. Zhou, D. Bau, A. Oliva, and A. Torralba",
      "orig_title": "Interpreting deep visual representations via network dissection",
      "paper_id": "1711.05611v2"
    },
    {
      "index": 28,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 29,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 30,
      "title": "Places: A 10 million image database for scene recognition",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba"
    },
    {
      "index": 31,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition",
      "authors": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei"
    }
  ]
}