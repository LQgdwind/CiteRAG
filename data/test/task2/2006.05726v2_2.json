{
  "paper_id": "2006.05726v2",
  "title": "Estimating semantic structure for the VQA answer space",
  "sections": {
    "experiments": "We evaluate our contributions on the following two VQA corpuses: has been proposed in  and\ncontains open-ended questions in natural language about real images. The\ncorpus gathers 265K images with at least 3 questions for each. Each question is annotated with 10 ground-truth answers. has been introduced in ¬†. It has been constructed by reorganizing the training and validation splits of the VQAv2¬† dataset in order to explicitly make the distribution of answers different between the training and test.\nIn other words, the VQAv2-CP dataset has been designed to measure the sensitivity of a VQA model to the language bias, and therefore is a test for measuring the ability of a model to generalize to unseen situations. The evaluation of our contribution on VQAv2-CP is a particularly interesting setup, as obtaining good results there requires an agent to reason beyond exploiting biases. This is important in the context of our contribution. While classical auxiliary losses add additional difficulty to a learning task, as for instance self-supervised contrastive losses 7, the proposed semantic loss is a different case as it is inherently still based on classification of the answer classes, albeit on a restructured output space. Testing this auxiliary loss in a setting, where the training distribution equals the test distribution would make the evaluation unfavorable by introducing a difference between the minimized objective and the evaluation metric. We claim that the new semantic loss increases reasoning and decreases the dependence on biases, which is better evaluated on datasets with shifts in distributions such as VQAv2-CP. We nevertheless also include comparisons on VQAv2. To demonstrate that the semantic loss is model-agnostic, we test it on three different standard VQA architectures: [ref]7, is a strong baseline architecture for VQA. It introduced the use of a bottom-up (from pixels to visual objects) visual attention to the standard top-down mechanism. In particular, UpDn uses an object detector ‚Äî R-CNN¬†8 ‚Äî to extract bounding boxes along with dense visual features for each object in the image. Thereby, the question attention is computed over a set of objects rather than over standard grid features as has been done before in the literature. [ref]8 adds a bilinear attention operator on top of the bottom-up top-down mechanism introduced in [ref]7. Moreover, this model allows multi-hop reasoning by stacking multiple bilinear attention layers with residual connections.  is a Transformer-based¬†\nmulti-modal architecture aiming at modeling both the interactions inside one modality (between words or visual objects) and the interactions between the two modalities (between words and objects) using self-attention mechanisms. Like BAN¬†, this architecture contains several stacked self-attention blocks in order to perform complex multi-hop reasoning. We complement our evaluation by showing the complementarity of our method with RUBi¬†, a\nSOTA approach focusing on the reduction of language bias. It consists of a training procedure adding a\nquestion-only branch with a masking mechanism to the base VQA model during the training. The RUBi module adapts the prediction of the base model in order to prevent it from fully exploiting a language-only bias. At test time, the question-only branch is removed. Training details ‚Äî \nWe train all our models during 40 epochs.\nWe use the 6-layers version of MCAN¬†. At the beginning of the training we linearly increase the learning rate from 1‚Äãe‚àí41superscriptùëí41e^{-4} to 2‚Äãe‚àí12superscriptùëí12e^{-1} during 8 epochs, followed by a decay by a factor of 0.20.20.2 at epochs 10 and 20. We set the batch size to 64.\nFor UpDn¬†[ref]7 and BAN¬†[ref]8 we set the batch size to 512 and increase the learning rate from 2‚Äãe‚àí32superscriptùëí32e^{-3} to 2‚Äãe‚àí12superscriptùëí12e^{-1} during the first 8 epochs, followed by a decay by a factor of 0.20.20.2 at epochs 10 and 20. We use the 4-layers implementation of BAN.\nWe use binary cross entropy along with the Adam optimizer¬†9 for MCAN¬† and Adamax¬†9 for BAN¬†[ref]8 and UpDn¬†For the baseline models, we use publicly available implementations at https://github.com/MILVLG/openvqa. All of our experiments are run on two NVIDIA P100 GPUs with half precision training using the apex library.333https://github.com/NVIDIA/apex\nNote that all of our models are trained on the training split only, without the help of any external dataset such as in [ref]8 and .\nWe set the two hyper-parameters Œª=10.0ùúÜ10.0\\lambda=10.0 and k=10ùëò10k=10 using grid search."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Vqa: Visual question answering",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh"
    },
    {
      "index": 1,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 2,
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Drew A Hudson and Christopher D Manning"
    },
    {
      "index": 3,
      "title": "Don‚Äôt Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi",
      "orig_title": "Don‚Äôt just assume; look and answer: Overcoming priors for visual question answering",
      "paper_id": "1712.00377v2"
    },
    {
      "index": 4,
      "title": "Rubi: Reducing unimodal biases for visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al."
    },
    {
      "index": 5,
      "title": "Stacked Attention Networks for Image Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola",
      "orig_title": "Stacked attention networks for image question answering",
      "paper_id": "1511.02274v2"
    },
    {
      "index": 6,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 7,
      "title": "Bilinear Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang",
      "orig_title": "Bilinear attention networks",
      "paper_id": "1805.07932v2"
    },
    {
      "index": 8,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 9,
      "title": "Deep Modular Co-Attention Networks for Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian",
      "orig_title": "Deep modular co-attention networks for visual question answering",
      "paper_id": "1906.10770v1"
    },
    {
      "index": 10,
      "title": "Analyzing the behavior of visual question answering models",
      "abstract": "",
      "year": "2016",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "Aishwarya Agrawal, Dhruv Batra, and Devi Parikh"
    },
    {
      "index": 11,
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick",
      "orig_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "paper_id": "1612.06890v1"
    },
    {
      "index": 12,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2016",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra",
      "orig_title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "paper_id": "1606.03556v2"
    },
    {
      "index": 13,
      "title": "Women also Snowboard: Overcoming Bias in Captioning Models",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach",
      "orig_title": "Women also snowboard: Overcoming bias in captioning models",
      "paper_id": "1803.09797v4"
    },
    {
      "index": 14,
      "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee",
      "orig_title": "Overcoming language priors in visual question answering with adversarial regularization",
      "paper_id": "1810.03649v2"
    },
    {
      "index": 15,
      "title": "Taking a hint: Leveraging explanations to make vision and language models more grounded",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh"
    },
    {
      "index": 16,
      "title": "Self-critical reasoning for robust visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jialin Wu and Raymond Mooney"
    },
    {
      "index": 17,
      "title": "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach",
      "orig_title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
      "paper_id": "1802.08129v1"
    },
    {
      "index": 18,
      "title": "Overcoming language priors in vqa via decomposed linguistic representations",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, and Qi Wu"
    },
    {
      "index": 19,
      "title": "Facial age estimation by learning from label distributions",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Xin Geng, Chao Yin, and Zhi-Hua Zhou"
    },
    {
      "index": 20,
      "title": "Label distribution learning",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "Xin Geng"
    },
    {
      "index": 21,
      "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "Mateusz Malinowski and Mario Fritz",
      "orig_title": "A multi-world approach to question answering about real-world scenes based on uncertain input",
      "paper_id": "1410.0210v4"
    },
    {
      "index": 22,
      "title": "Verbs semantics and lexical selection",
      "abstract": "",
      "year": "1994",
      "venue": "Association for Computational Linguistics",
      "authors": "Zhibiao Wu and Martha Palmer"
    },
    {
      "index": 23,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D Manning"
    },
    {
      "index": 24,
      "title": "Distributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in neural information processing systems",
      "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean"
    },
    {
      "index": 25,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Laurens van der Maaten and Geoffrey Hinton",
      "orig_title": "Visualizing data using t-sne",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 26,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yazhe Li Aaron van den Oord and Oriol Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 27,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 28,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 29,
      "title": "Learning by Abstraction: The Neural State Machine",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Drew Hudson and Christopher D Manning",
      "orig_title": "Learning by abstraction: The neural state machine",
      "paper_id": "1907.03950v4"
    }
  ]
}