{
  "paper_id": "2004.14973v2",
  "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
  "sections": {
    "related work": "Path Selection in VLN.\nIn VLN , an agent is required to follow a navigation instruction from a start location to a goal. While most existing works focus on the setting in which the test environments are previously unseen, many also consider the scenario in which the test environment is previously explored and stored in memory (i.e., fully observable). In this setting, a high-probability path is typically generated by performing beam search through the environment and ranking paths according to either: (1) their probability under a ‘follower’ model  [ref]8 0    , as in Figure 1 (left), or (2) by how well they explain the instruction according to a ‘speaker’ (instruction generation) model [ref]8 [ref]28, as in Figure 1 (center). In contrast, we use beam search with an existing agent model [ref]28 to generate a set of candidate paths, which we then evaluate using our discriminative path-instruction compatibility model, as in Figure 1 (right). Data Augmentation and Auxiliary Tasks in VLN.\nTo compensate for the small size of existing VLN datasets, previous works have investigated various data augmentation strategies and auxiliary tasks. Many papers report results trained on augmented data including instructions synthesized by a speaker model [ref]8   [ref]28. Tan et al. [ref]28 use environmental dropout to mimic additional training environments to improve generalization. Li et al.  incorporate language-only pretraining using a BERT model. Several existing papers 0  and one concurrent hitherto-unpublished work  consider path-instruction compatibility as an auxiliary loss function or reward for VLN agents. We focus on path-instruction compatibility in the context of transfer learning from large-scale internet data, which has not been previously explored. Vision-and-Language Pretraining.\nThere has been significant recent progress towards learning transferable joint representations of images and text 2     . Using BERT-like  self-supervised objectives and Transformer  architectures, these models have achieved state-of-the-art results on multiple vision-and-language tasks by pretraining on aligned image-and-text data collected from the web  and transferring the base architecture to other tasks such as VQA , referring expressions , and caption-based image retrieval . However, these tasks are all based on single images. The extent to which these pretrained models can generalize from human-composed and curated internet images to embodied AI tasks has not been investigated. In this work we propose a training curriculum to handle potential domain-shift and augment a previous model architecture to process panoramic image sequences, extending the progress in vision-and-language to vision-and-language navigation (VLN)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "On evaluation of embodied navigation agents",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.06757",
      "authors": "Anderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et al."
    },
    {
      "index": 1,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 2,
      "title": "Chasing ghosts: Instruction following as bayesian state tracking",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Anderson, P., Shrivastava, A., Parikh, D., Batra, D., Lee, S."
    },
    {
      "index": 3,
      "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., van den Hengel, A.",
      "orig_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
      "paper_id": "1711.07280v3"
    },
    {
      "index": 4,
      "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on 3D Vision (3DV)",
      "authors": "Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.",
      "orig_title": "Matterport3d: Learning from rgb-d data in indoor environments",
      "paper_id": "1709.06158v1"
    },
    {
      "index": 5,
      "title": "Microsoft coco captions: Data collection and evaluation server",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1504.00325",
      "authors": "Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L."
    },
    {
      "index": 6,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 7,
      "title": "Speaker-Follower Models for Vision-and-Language Navigation",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.P., Berg-Kirkpatrick, T., Saenko, K., Klein, D., Darrell, T.",
      "orig_title": "Speaker-follower models for vision-and-language navigation",
      "paper_id": "1806.02724v2"
    },
    {
      "index": 8,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.",
      "orig_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 9,
      "title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.10638",
      "authors": "Hao, W., Li, C., Li, X., Carin, L., Gao, J."
    },
    {
      "index": 10,
      "title": "Multi-modal Discriminative Model for Vision-and-Language Navigation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.13358",
      "authors": "Huang, H., Jain, V., Mehta, H., Baldridge, J., Ie, E.",
      "orig_title": "Multi-modal discriminative model for vision-and-language navigation",
      "paper_id": "1905.13358v1"
    },
    {
      "index": 11,
      "title": "Referit game: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L."
    },
    {
      "index": 12,
      "title": "Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Ke, L., Li, X., Bisk, Y., Holtzman, A., Gan, Z., Liu, J., Gao, J., Choi, Y., Srinivasa, S.",
      "orig_title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation",
      "paper_id": "1903.02547v2"
    },
    {
      "index": 13,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1602.07332",
      "authors": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L."
    },
    {
      "index": 14,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.06066",
      "authors": "Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 15,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 16,
      "title": "Robust navigation with language pretraining and stochastic sampling",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.02244",
      "authors": "Li, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N., Choi, Y."
    },
    {
      "index": 17,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Lu, J., Batra, D., Parikh, D., Lee, S."
    },
    {
      "index": 18,
      "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Lu, J., Yang, J., Batra, D., Parikh, D.",
      "orig_title": "Hierarchical question-image co-attention for visual question answering",
      "paper_id": "1606.00061v5"
    },
    {
      "index": 19,
      "title": "Self-monitoring navigation agent via auxiliary progress estimation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.03035",
      "authors": "Ma, C.Y., Lu, J., Wu, Z., AlRegib, G., Kira, Z., Socher, R., Xiong, C."
    },
    {
      "index": 20,
      "title": "The regretful agent: Heuristic-aided navigation through progress estimation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Ma, C.Y., Wu, Z., AlRegib, G., Xiong, C., Kira, Z."
    },
    {
      "index": 21,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NIPS)",
      "authors": "Ren, S., He, K., Girshick, R., Sun, J.",
      "orig_title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 22,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International journal of computer vision",
      "authors": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 23,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Sharma, P., Ding, N., Goodman, S., Soricut, R."
    },
    {
      "index": 24,
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6034",
      "authors": "Simonyan, K., Vedaldi, A., Zisserman, A."
    },
    {
      "index": 25,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.08530",
      "authors": "Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J."
    },
    {
      "index": 26,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Tan, H., Bansal, M.",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 27,
      "title": "Learning to navigate unseen environments: Back translation with environmental dropout",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.04195",
      "authors": "Tan, H., Yu, L., Bansal, M."
    },
    {
      "index": 28,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 29,
      "title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L."
    },
    {
      "index": 30,
      "title": "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Wang, X., Xiong, W., Wang, H., Yang Wang, W.",
      "orig_title": "Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation",
      "paper_id": "1803.07729v2"
    },
    {
      "index": 31,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.11059",
      "authors": "Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 32,
      "title": "Vision-language navigation with self-supervised auxiliary reasoning tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.07883",
      "authors": "Zhu, F., Zhu, Y., Chang, X., Liang, X."
    },
    {
      "index": 33,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S."
    }
  ]
}