{
  "paper_id": "2307.01610v1",
  "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
  "sections": {
    "vi related work": "Membership inference attacks.\nDepending on the adversary capabilities, MIAs can be divided into black-box 7    8 [ref]8  7 and white-box attacks   0.\nThe former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference.\nBlack-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies    (and in HAMP).\nSuch attacks can be mounted by either shadow-training 7   or computing statistical metrics based on the partial knowledge of the private dataset 8 [ref]8 7.\nMany of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label.\nThis motivates a new class of attacks called, label-only attacks, which can be launched either with [ref]8 or without 7 partial knowledge of the membership information.\nCarlini et al.  introduce the LiRA attack that can succeed in inferring membership when controlled at low false positive or false negative, through a well-calibrated Gaussian likelihood estimate. In addition to supervised classification, MIAs have also been explored in other domains, including contrastive learning , generative models  , federated learning 0, graph neural networks , and recommender systems . Defenses against membership inference attacks.\nThese defenses can be divided into provable and practical defenses.\nThe former can provide rigorous privacy guarantee, such as DP-SGD , PATE 2.\nNevertheless, these defenses often incur severe accuracy drop when used with acceptable provable bounds 5 3.\nAnother line of practical defenses aim to achieve empirical privacy without severely degrading accuracy.\nCommon regularization techniques such as dropout 9, weight decay 4 are shown to be able to reduce privacy leakage, but with limited effectiveness 7 6.\nOther defenses enforce specific optimization constraint during training to mitigate MIAs  6, or perform output obfuscation  .\nKnowledge distillation is used by different techniques to mitigate MIAs, including PATE 2, DMP 6, SELENA  and KCD .\nHowever, existing defenses are often biased towards either privacy or utility. In contrast, HAMP both achieves strong membership privacy and high accuracy, which offers a much better privacy-utility trade off. Other privacy attacks.\nIn addition to membership privacy, common ML models are found to leak different private properties      .\nModel extraction attacks can duplicate the functionality of a proprietary model  .\nModel inversion attacks are capable of inferring critical information in the input features such as genomic information  .\nProperty inference attacks are constructed to infer sensitive properties of the training dataset ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Pytorch opacus",
      "abstract": "",
      "year": "",
      "venue": "https://github.com/pytorch/opacus",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Deep Learning with Differential Privacy",
      "abstract": "",
      "year": "2016",
      "venue": "ACM SIGSAC conference on computer and communications security",
      "authors": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang",
      "orig_title": "Deep learning with differential privacy",
      "paper_id": "1607.00133v2"
    },
    {
      "index": 2,
      "title": "Membership Inference Attacks From First Principles",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer",
      "orig_title": "Membership inference attacks from first principles",
      "paper_id": "2112.03570v2"
    },
    {
      "index": 3,
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "USENIX Security Symposium (USENIX Security 21)",
      "authors": "N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson et al.",
      "orig_title": "Extracting training data from large language models",
      "paper_id": "2012.07805v2"
    },
    {
      "index": 4,
      "title": "Towards evaluating the robustness of neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE symposium on security and privacy (sp)",
      "authors": "N. Carlini and D. Wagner"
    },
    {
      "index": 5,
      "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
      "abstract": "",
      "year": "2001",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Caruana, S. Lawrence, and L. Giles"
    },
    {
      "index": 6,
      "title": "Gan-leaks: A taxonomy of membership inference attacks against generative models",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGSAC conference on computer and communications security",
      "authors": "D. Chen, N. Yu, Y. Zhang, and M. Fritz"
    },
    {
      "index": 7,
      "title": "Label-Only Membership Inference Attacks",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot",
      "orig_title": "Label-only membership inference attacks",
      "paper_id": "2007.14321v3"
    },
    {
      "index": 8,
      "title": "Knowledge Cross-Distillation for Membership Privacy",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.01363",
      "authors": "R. Chourasia, B. Enkhtaivan, K. Ito, J. Mori, I. Teranishi, and H. Tsuchida",
      "orig_title": "Knowledge cross-distillation for membership privacy",
      "paper_id": "2111.01363v3"
    },
    {
      "index": 9,
      "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
      "abstract": "",
      "year": "2015",
      "venue": "ACM SIGSAC conference on computer and communications security",
      "authors": "M. Fredrikson, S. Jha, and T. Ristenpart"
    },
    {
      "index": 10,
      "title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing",
      "abstract": "",
      "year": "2014",
      "venue": "USENIX Security Symposium (USENIX Security 14)",
      "authors": "M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart"
    },
    {
      "index": 11,
      "title": "Property inference attacks on fully connected neural networks using permutation invariant representations",
      "abstract": "",
      "year": "2018",
      "venue": "ACM SIGSAC conference on computer and communications security",
      "authors": "K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov"
    },
    {
      "index": 12,
      "title": "Logan: Membership inference attacks against generative models",
      "abstract": "",
      "year": "2019",
      "venue": "Privacy Enhancing Technologies (PoPETs)",
      "authors": "J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro"
    },
    {
      "index": 13,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 14,
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.04861",
      "authors": "A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam"
    },
    {
      "index": 15,
      "title": "Densely connected convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger"
    },
    {
      "index": 16,
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.01341",
      "authors": "B. Hui, Y. Yang, H. Yuan, P. Burlina, N. Z. Gong, and Y. Cao",
      "orig_title": "Practical blind membership inference attack via differential comparisons",
      "paper_id": "2101.01341v2"
    },
    {
      "index": 17,
      "title": "Revisiting Membership Inference Under Realistic Assumptions",
      "abstract": "",
      "year": "2021",
      "venue": "Privacy Enhancing Technologies",
      "authors": "B. Jayaraman, L. Wang, K. Knipmeyer, Q. Gu, and D. Evans",
      "orig_title": "Revisiting membership inference under realistic assumptions",
      "paper_id": "2005.10881v5"
    },
    {
      "index": 18,
      "title": "Memguard: Defending against black-box membership inference attacks via adversarial examples",
      "abstract": "",
      "year": "2019",
      "venue": "ACM SIGSAC conference on computer and communications security",
      "authors": "J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong"
    },
    {
      "index": 19,
      "title": "When does data augmentation help with membership inference attacks?",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Kaya and T. Dumitras"
    },
    {
      "index": 20,
      "title": "The megaface benchmark: 1 million faces for recognition at scale",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard"
    },
    {
      "index": 21,
      "title": "Machine learning applications in cancer prognosis and prediction",
      "abstract": "",
      "year": "2015",
      "venue": "Computational and structural biotechnology journal",
      "authors": "K. Kourou, T. P. Exarchos, K. P. Exarchos, M. V. Karamouzis, and D. I. Fotiadis"
    },
    {
      "index": 22,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "A. Krizhevsky, G. Hinton et al."
    },
    {
      "index": 23,
      "title": "A simple weight decay can improve generalization",
      "abstract": "",
      "year": "1992",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Krogh and J. A. Hertz"
    },
    {
      "index": 24,
      "title": "Stolen memories: Leveraging model memorization for calibrated white-box membership inference",
      "abstract": "",
      "year": "2020",
      "venue": "USENIX Security Symposium (USENIX Security 20)",
      "authors": "K. Leino and M. Fredrikson"
    },
    {
      "index": 25,
      "title": "Membership Inference Attacks and Defenses in Classification Models",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Conference on Data and Application Security and Privacy",
      "authors": "J. Li, N. Li, and B. Ribeiro",
      "orig_title": "Membership inference attacks and defenses in classification models",
      "paper_id": "2002.12062v3"
    },
    {
      "index": 26,
      "title": "Membership Leakage in Label-Only Exposures",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "Z. Li and Y. Zhang",
      "orig_title": "Membership leakage in label-only exposures",
      "paper_id": "2007.15528v3"
    },
    {
      "index": 27,
      "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "H. Liu, J. Jia, W. Qu, and N. Z. Gong",
      "orig_title": "Encodermi: Membership inference against pre-trained encoders in contrastive learning",
      "paper_id": "2108.11023v1"
    },
    {
      "index": 28,
      "title": "Machine Learning with Membership Privacy using Adversarial Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "M. Nasr, R. Shokri, and A. Houmansadr",
      "orig_title": "Machine learning with membership privacy using adversarial regularization",
      "paper_id": "1807.05852v1"
    },
    {
      "index": 29,
      "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE symposium on security and privacy (SP)",
      "authors": "M. Nasr, R. Shokri, and A. Houmansadr",
      "orig_title": "Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning",
      "paper_id": "1812.00910v2"
    },
    {
      "index": 30,
      "title": "The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature",
      "abstract": "",
      "year": "2011",
      "venue": "Decision support systems",
      "authors": "E. W. Ngai, Y. Hu, Y. H. Wong, Y. Chen, and X. Sun"
    },
    {
      "index": 31,
      "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1610.05755",
      "authors": "N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar",
      "orig_title": "Semi-supervised knowledge transfer for deep learning from private training data",
      "paper_id": "1610.05755v4"
    },
    {
      "index": 32,
      "title": "Tempered Sigmoid Activations for Deep Learning with Differential Privacy",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "N. Papernot, A. Thakurta, S. Song, S. Chien, and Ú. Erlingsson",
      "orig_title": "Tempered sigmoid activations for deep learning with differential privacy",
      "paper_id": "2007.14191v1"
    },
    {
      "index": 33,
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06434",
      "authors": "A. Radford, L. Metz, and S. Chintala",
      "orig_title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "paper_id": "1511.06434v2"
    },
    {
      "index": 34,
      "title": "Membership inference attack against differentially private deep learning model.",
      "abstract": "",
      "year": "2018",
      "venue": "Trans. Data Priv.",
      "authors": "M. A. Rahman, T. Rahman, R. Laganière, N. Mohammed, and Y. Wang"
    },
    {
      "index": 35,
      "title": "Membership Privacy for Machine Learning Models Through Knowledge Transfer",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "V. Shejwalkar and A. Houmansadr",
      "orig_title": "Membership privacy for machine learning models through knowledge transfer",
      "paper_id": "1906.06589v3"
    },
    {
      "index": 36,
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "R. Shokri, M. Stronati, C. Song, and V. Shmatikov",
      "orig_title": "Membership inference attacks against machine learning models",
      "paper_id": "1610.05820v2"
    },
    {
      "index": 37,
      "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
      "abstract": "",
      "year": "2021",
      "venue": "USENIX Security Symposium (USENIX Security 21)",
      "authors": "L. Song and P. Mittal",
      "orig_title": "Systematic evaluation of privacy risks of machine learning models",
      "paper_id": "2003.10595v2"
    },
    {
      "index": 38,
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "abstract": "",
      "year": "2014",
      "venue": "The journal of machine learning research",
      "authors": "N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov"
    },
    {
      "index": 39,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna"
    },
    {
      "index": 40,
      "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
      "abstract": "",
      "year": "2022",
      "venue": "USENIX Security Symposium (USENIX Security 22)",
      "authors": "X. Tang, S. Mahloujifar, L. Song, V. Shejwalkar, M. Nasr, A. Houmansadr, and P. Mittal",
      "orig_title": "Mitigating membership inference attacks by Self-Distillation through a novel ensemble architecture",
      "paper_id": "2110.08324v1"
    },
    {
      "index": 41,
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "",
      "year": "2022",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "F. Tramèr, R. Shokri, A. San Joaquin, H. Le, M. Jagielski, S. Hong, and N. Carlini",
      "orig_title": "Truth serum: Poisoning machine learning models to reveal their secrets",
      "paper_id": "2204.00032v2"
    },
    {
      "index": 42,
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "",
      "year": "2016",
      "venue": "USENIX Security Symposium (USENIX Security 16)",
      "authors": "F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart",
      "orig_title": "Stealing machine learning models via prediction apis",
      "paper_id": "1609.02943v2"
    },
    {
      "index": 43,
      "title": "Data-free model extraction",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "J.-B. Truong, P. Maini, R. J. Walls, and N. Papernot"
    },
    {
      "index": 44,
      "title": "Modeling tabular data using conditional gan",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Information Processing Systems",
      "authors": "L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni"
    },
    {
      "index": 45,
      "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.03915",
      "authors": "Z. Yang, B. Shao, B. Xuan, E.-C. Chang, and F. Zhang",
      "orig_title": "Defending model inversion and membership inference attacks via prediction purification",
      "paper_id": "2005.03915v2"
    },
    {
      "index": 46,
      "title": "Enhanced membership inference attacks against machine learning models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.09679",
      "authors": "J. Ye, A. Maddi, S. K. Murakonda, V. Bindschaedler, and R. Shokri"
    },
    {
      "index": 47,
      "title": "Privacy risk in machine learning: Analyzing the connection to overfitting",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE 31st Computer Security Foundations Symposium (CSF)",
      "authors": "S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha"
    },
    {
      "index": 48,
      "title": "Membership inference attacks against recommender systems",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "M. Zhang, Z. Ren, Z. Wang, P. Ren, Z. Chen, P. Hu, and Y. Zhang"
    },
    {
      "index": 49,
      "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "X. Zhang, X. Zhou, M. Lin, and J. Sun",
      "orig_title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "paper_id": "1707.01083v2"
    },
    {
      "index": 50,
      "title": "Inference Attacks Against Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "USENIX Security Symposium (USENIX Security)",
      "authors": "Z. Zhang, M. Chen, M. Backes, Y. Shen, and Y. Zhang",
      "orig_title": "Inference attacks against graph neural networks",
      "paper_id": "2110.02631v1"
    }
  ]
}