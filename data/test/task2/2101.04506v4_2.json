{
  "paper_id": "2101.04506v4",
  "title": "UFA-FUSE: A novel deep supervised and hybrid model for multi-focus image fusion",
  "sections": {
    "iv-a experimental settings": "First of all, we introduce the training details which are important for the training of fusion models. The proposed fusion model is trained by using the generated dataset, which consists of 84424 images. In each training batch, we randomly extract 8 images with the sizes of 256×\\times256. We implement our proposed fusion model with Pytorch framework and update it with Adam optimizer . The learning rate is initialized to 1e-4 for all layers and divided into ten for every 200 epochs. The proposed fusion framework is trained for a total of 300 epochs, and the total training time is 60h. The proposed model is trained and tested on an eight-core PC with an Intel 9700KF 3.5 GHz CPU (with 16 GB RAM) and a GTX 2070Super GPU (with 8GB memory). To objectively verify the superiority of our proposed fusion method over other fusion methods, we have compared it with nineteen representative fusion algorithms. What’s more, the comparison algorithms we chose are diverse and not limited to those deep learning-based methods. Specifically, the comparison algorithms contain such as sparse representation (SR) , curvelet transform (CVT) , discrete wavelet transform (DWT), dual-tree complex wavelet transform (DTCWT), non-subsampled contourlet transform (NSCT)  these transform domain-based fusion algorithms; dense SIFT (DSIFT) , guiding filter fusion (GFF) , matting fusion (IMF), multi-scale weighted gradient fusion (MWGF)  and spatial frequency (SF)  these spatial domain-based algorithms; convolutional neural network (CNN) , DeepFuse [ref]23, SESF, DenseFuse  (including elementwise-add and L1subscript𝐿1L_{1} fusion strategy), DRPL, MFF and IFCNN  (including elementwise-maximum fusion strategy) these deep learning-based algorithms. Compared with these traditional comparison algorithms (including transform domain-based and spatial domain-based algorithms), the proposed algorithm has stronger feature representation capability and can effectively extract more informative image features. Moreover, compared with these deep learning-based comparison algorithms, the proposed algorithm is an end-to-end network structure and does not require the generation of an intermediate decision map to achieve image fusion. In addition, the proposed algorithm designs the novel attention mechanism-based fusion strategies, which can provide additional flexibility for fusing different types of image features.\n What’s more, we have discriminated against the performance of different fusion models through qualitative and quantitative evaluation methods. From the perspective of qualitative evaluation methods, whether the visual effects of the fusion image differ from the source images is the main discriminating method. Specifically, the fused image should contain as many distinct and sharp features from source images in multi-focus image fusion, and does not exist additional artifacts. Since only evaluating the performance of fusion methods from the qualitative perspective is not comprehensive. Consequently, we further use seven metrics to evaluate the performance of the fusion algorithms on the multi-focus image fusion. The seven metrics are respectively the visual information fidelity (VIFF) , standard deviation (STD) , average gradient (AVG) , gradient-based fusion performance (Qa​b​fsuperscript𝑄𝑎𝑏𝑓Q^{abf}) , shannon entropy (SEN) , Qcsuperscript𝑄𝑐Q^{c}  and Qmsuperscript𝑄𝑚Q^{m} . \nVIFF, which is an evaluation metric based on human visual perception, measures the visual information fidelity between the fusion image and source images. STD and AVG, which are two image features-based evaluation metrics, measure the textural information of the fusion image from a statistical point of view. Qa​b​fsuperscript𝑄𝑎𝑏𝑓Q^{abf} is an evaluation metric based on source images and the fusion image, and Qa​b​fsuperscript𝑄𝑎𝑏𝑓Q^{abf} evaluates the performance of fusion methods based on gradient. SEN is an evaluation metric based on information theory. Qmsuperscript𝑄𝑚Q^{m} retrieves the edge information from the high and band-pass components of the decomposition by using the two-level Haar wavelet. Qcsuperscript𝑄𝑐Q^{c} employs the major features in human visual system model to evaluate the perceptual quality of image fusion. There is no doubt that these seven metrics cover different types of fusion evaluation metrics. So, it is enough to evaluate the fusion results of different fusion models through these seven metrics. These seven metrics can objectively reflect the abilities of fusion methods on merging the clear image features and on integrating the texture information. It is certainly appropriate to choose these seven metrics to evaluate the performance of different fusion algorithms. In addition, it should be noted that qualitative evaluation and quantitative evaluation perform on the ‘Lytro’ multi-focus image dataset , as shown in Fig. 5, the dataset contains 20 pairs of multi-focus images."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Image fusion using adjustable non-subsampled shearlet transform",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "A. Vishwakarma and M. K. Bhuyan"
    },
    {
      "index": 1,
      "title": "Drf: Disentangled representation for visible and infrared image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "H. Xu, X. Wang, and J. Ma"
    },
    {
      "index": 2,
      "title": "Sedrfuse: A symmetric encoder–decoder with residual block network for infrared and visible image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "L. Jian, X. Yang, Z. Liu, G. Jeon, M. Gao, and D. Chisholm"
    },
    {
      "index": 3,
      "title": "Multimodal medical image fusion based on weighted local energy matching measurement and improved spatial frequency",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "Y. Yang, S. Cao, S. Huang, and W. Wan"
    },
    {
      "index": 4,
      "title": "Medical image fusion with parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform domain",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "M. Yin, X. Liu, Y. Liu, and X. Chen"
    },
    {
      "index": 5,
      "title": "Efficient Misalignment-Robust Multi-Focus Microscopical Images Fusion",
      "abstract": "",
      "year": "2019",
      "venue": "Signal Processing",
      "authors": "Y. Liang, Y. Mao, Z. Tang, M. Yan, Y. Zhao, and J. Liu",
      "orig_title": "Efficient misalignment-robust multi-focus microscopical images fusion",
      "paper_id": "1812.08915v1"
    },
    {
      "index": 6,
      "title": "Structural characterization and measurement of nonwoven fabrics based on multi-focus image fusion",
      "abstract": "",
      "year": "2019",
      "venue": "Measurement",
      "authors": "Yang, Chen, Na, Deng, Bin-Jie, Xin, Wen-Yu, Xing, and Zheng-Ye"
    },
    {
      "index": 7,
      "title": "Region-based multifocus image fusion for the precise acquisition of pap smear images",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Biomedical Optics",
      "authors": "S. Tello-Mijares and J. Bescós"
    },
    {
      "index": 8,
      "title": "Image fusion: algorithms and applications",
      "abstract": "",
      "year": "2011",
      "venue": "Elsevier",
      "authors": "T. Stathaki"
    },
    {
      "index": 9,
      "title": "A morphological pyramidal image decomposition",
      "abstract": "",
      "year": "1989",
      "venue": "Pattern Recognition Letters",
      "authors": "A. Toet"
    },
    {
      "index": 10,
      "title": "Multisensor image fusion using the wavelet transform",
      "abstract": "",
      "year": "1995",
      "venue": "Graphical Models and Image Processing",
      "authors": "H. Li, B. Manjunath, and S. K. Mitra"
    },
    {
      "index": 11,
      "title": "A novel image decomposition-based hybrid technique with super-resolution method for multi-focus image fusion",
      "abstract": "",
      "year": "2018",
      "venue": "Information Fusion",
      "authors": "S. Aymaz and C. Kse"
    },
    {
      "index": 12,
      "title": "Pixel- and region-based image fusion with complex wavelets",
      "abstract": "",
      "year": "2007",
      "venue": "Information Fusion",
      "authors": "J. J. Lewis, R. J. O’Callaghan, S. G. Nikolov, D. R. Bull, and N. Canagarajah"
    },
    {
      "index": 13,
      "title": "Multifocus image fusion using the nonsubsampled contourlet transform",
      "abstract": "",
      "year": "2009",
      "venue": "Signal Processing",
      "authors": "Q. Zhang and B.-l. Guo"
    },
    {
      "index": 14,
      "title": "Quadtree-based multi-focus image fusion using a weighted focus-measure",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "X. Bai, Y. Zhang, F. Zhou, and B. Xue"
    },
    {
      "index": 15,
      "title": "Image fusion with guided filtering",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Image processing",
      "authors": "S. Li, X. Kang, and J. Hu"
    },
    {
      "index": 16,
      "title": "Multifocus image fusion using region segmentation and spatial frequency",
      "abstract": "",
      "year": "2008",
      "venue": "Image and Vision Computing",
      "authors": "S. Li and B. Yang"
    },
    {
      "index": 17,
      "title": "Multi-scale weighted gradient-based fusion for multi-focus images",
      "abstract": "",
      "year": "2014",
      "venue": "Information Fusion",
      "authors": "Z. Zhou, S. Li, and B. Wang"
    },
    {
      "index": 18,
      "title": "Multi-focus image fusion with dense sift",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "Y. Liu, S. Liu, and Z. Wang"
    },
    {
      "index": 19,
      "title": "Multi-focus image fusion with a deep convolutional neural network",
      "abstract": "",
      "year": "2017",
      "venue": "Information Fusion",
      "authors": "Y. Liu, X. Chen, H. Peng, and Z. Wang"
    },
    {
      "index": 20,
      "title": "Pixel convolutional neural network for multi-focus image fusion",
      "abstract": "",
      "year": "2018",
      "venue": "Information Sciences",
      "authors": "H. Tang, B. Xiao, W. Li, and G. Wang"
    },
    {
      "index": 21,
      "title": "Sesf-fuse: An unsupervised deep model for multi-focus image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "authors": "B. Ma, Y. Zhu, X. Yin, X. Ban, H. Huang, and M. Mukeshimana"
    },
    {
      "index": 22,
      "title": "DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "authors": "K. R. Prabhakar, V. S. Srikar, and R. V. Babu",
      "orig_title": "Deepfuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs",
      "paper_id": "1712.07384v1"
    },
    {
      "index": 23,
      "title": "DenseFuse: A Fusion Approach to Infrared and Visible Images",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "H. Li and X. Wu",
      "orig_title": "Densefuse: A fusion approach to infrared and visible images",
      "paper_id": "1804.08361v9"
    },
    {
      "index": 24,
      "title": "Ifcnn: A general image fusion framework based on convolutional neural network",
      "abstract": "",
      "year": "2020",
      "venue": "Information Fusion",
      "authors": "Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, and L. Zhang"
    },
    {
      "index": 25,
      "title": "Remote sensing image fusion using the curvelet transform",
      "abstract": "",
      "year": "2007",
      "venue": "Information Fusion",
      "authors": "F. Nencini, A. Garzelli, S. Baronti, and L. Alparone"
    },
    {
      "index": 26,
      "title": "Robust multi-focus image fusion using edge model and multi-matting",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Y. Chen, J. Guan, and W.-K. Cham"
    },
    {
      "index": 27,
      "title": "Drpl: Deep regression pair learning for multi-focus image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "J. Li, X. Guo, G. Lu, B. Zhang, and D. Zhang"
    },
    {
      "index": 28,
      "title": "Mff-gan: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "Information Fusion",
      "authors": "H. Zhang, Z. Le, Z. Shao, H. Xu, and J. Ma"
    },
    {
      "index": 29,
      "title": "Brain medical image fusion based on dual-branch cnns in nsst domain",
      "abstract": "",
      "year": "2020",
      "venue": "BioMed Research International",
      "authors": "Z. Ding, D. Zhou, R. Nie, R. Hou, and Y. Liu"
    },
    {
      "index": 30,
      "title": "Deepanf: A deep attentive neural framework with distributed representation for chromatin accessibility prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Neurocomputing",
      "authors": "Y. Guo, D. Zhou, R. Nie, X. Ruan, and W. Li"
    },
    {
      "index": 31,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Hu, L. Shen, and G. Sun",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 32,
      "title": "Multigrained attention network for infrared and visible image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "J. Li, H. Huo, C. Li, R. Wang, C. Sui, and Z. Liu"
    },
    {
      "index": 33,
      "title": "NestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel Attention Models",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "H. Li, X. J. Wu, and T. Durrani",
      "orig_title": "Nestfuse: An infrared and visible image fusion architecture based on nest connection and spatial/channel attention models",
      "paper_id": "2007.00328v2"
    },
    {
      "index": 34,
      "title": "Global-feature encoding u-net (geu-net) for multi-focus image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "B. Xiao, B. Xu, X. Bi, and W. Li"
    },
    {
      "index": 35,
      "title": "CBAM: Convolutional Block Attention Module",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "S. Woo, J. Park, J. Lee, and I. So Kweon",
      "orig_title": "Cbam: Convolutional block attention module",
      "paper_id": "1807.06521v2"
    },
    {
      "index": 36,
      "title": "Loss Functions for Image Restoration with Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Computational Imaging",
      "authors": "H. Zhao, O. Gallo, I. Frosio, and J. Kautz",
      "orig_title": "Loss functions for image restoration with neural networks",
      "paper_id": "1511.08861v3"
    },
    {
      "index": 37,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "European Conference on Computer Vision",
      "authors": "T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 38,
      "title": "Image quality assessment: from error visibility to structural similarity",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli"
    },
    {
      "index": 39,
      "title": "Learning to detect salient objects with image-level supervision",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan"
    },
    {
      "index": 40,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 41,
      "title": "Multifocus image fusion and restoration with sparse representation",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "B. Yang and S. Li"
    },
    {
      "index": 42,
      "title": "A new image fusion performance metric based on visual information fidelity",
      "abstract": "",
      "year": "2013",
      "venue": "Information Fusion",
      "authors": "Y. Han, Y. Cai, Y. Cao, and X. Xu"
    },
    {
      "index": 43,
      "title": "An image fusion method based on directional contrast and area-based standard deviation",
      "abstract": "",
      "year": "2005",
      "venue": "Electronic Imaging and Multimedia Technology IV",
      "authors": "G. Liu, W. Chen, and W. Ling"
    },
    {
      "index": 44,
      "title": "Fusion of multispectral and panchromatic satellite images based on contourlet transform and local average gradient",
      "abstract": "",
      "year": "2007",
      "venue": "Optical Engineering",
      "authors": "H. Song, S. Yu, L. Song, and X. Yang"
    },
    {
      "index": 45,
      "title": "Gradient-based/evolutionary relay hybrid for computing pareto front approximations maximizing the s-metric",
      "abstract": "",
      "year": "2007",
      "venue": "International Workshop on Hybrid Metaheuristics",
      "authors": "M. Emmerich, A. Deutz, and N. Beume"
    },
    {
      "index": 46,
      "title": "Improved bounds on the local mean-square error and the bias of parameter estimators (corresp.)",
      "abstract": "",
      "year": "1977",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "M. Wax and J. Ziv"
    },
    {
      "index": 47,
      "title": "A new automated quality assessment algorithm for image fusion",
      "abstract": "",
      "year": "2009",
      "venue": "Image and Vision Computing",
      "authors": "Y. Chen and R. S. Blum"
    },
    {
      "index": 48,
      "title": "A novel image fusion metric based on multi-scale analysis",
      "abstract": "",
      "year": "2008",
      "venue": "2008 9th International Conference on Signal Processing",
      "authors": "P. Wang and B. Liu"
    },
    {
      "index": 49,
      "title": "Multi-focus image fusion using dictionary-based sparse representation",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "M. Nejati, S. Samavi, and S. Shirani"
    },
    {
      "index": 50,
      "title": "Infrared and visible image fusion via gradient transfer and total variation minimization",
      "abstract": "",
      "year": "2016",
      "venue": "Information Fusion",
      "authors": "J. Ma, C. Chen, C. Li, and J. Huang"
    },
    {
      "index": 51,
      "title": "Perceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with gaussian and bilateral filters",
      "abstract": "",
      "year": "2016",
      "venue": "Information Fusion",
      "authors": "Z. Zhou, B. Wang, S. Li, and M. Dong"
    },
    {
      "index": 52,
      "title": "Vif-net: an unsupervised framework for infrared and visible image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Computational Imaging",
      "authors": "R. Hou, D. Zhou, R. Nie, D. Liu, L. Xiong, Y. Guo, and C. Yu"
    },
    {
      "index": 53,
      "title": "A medical image fusion method based on convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "2017 20th International Conference on Information Fusion (Fusion)",
      "authors": "Y. Liu, X. Chen, J. Cheng, and H. Peng"
    },
    {
      "index": 54,
      "title": "Multimodal medical image fusion using hybrid layer decomposition with cnn-based feature mapping and structural clustering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "S. Singh and R. Anand"
    },
    {
      "index": 55,
      "title": "Laplacian re-decomposition for multimodal medical image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "X. Li, X. Guo, P. Han, X. Wang, H. Li, and T. Luo"
    },
    {
      "index": 56,
      "title": "On the use of a joint spatial-frequency representation for the fusion of multi-focus images",
      "abstract": "",
      "year": "2005",
      "venue": "Pattern Recognition Letters",
      "authors": "S. Gabarda and G. Cristobal"
    },
    {
      "index": 57,
      "title": "Multifocus image fusion using the log-gabor transform and a multisize windows technique",
      "abstract": "",
      "year": "2009",
      "venue": "Information Fusion",
      "authors": "R. Redondo, F. Sroubek, S. Fischer, and G. Cristóbal"
    },
    {
      "index": 58,
      "title": "Multi-focus- ing algorithm for microscopy imagery in assembly line using low-cost camera",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Biomedical Optics",
      "authors": "V. R. L. Juocas, R. D. R. Maskeliunas, and M. Wozniak"
    }
  ]
}