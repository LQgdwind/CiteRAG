{
  "paper_id": "2308.03572v5",
  "title": "Provably Efficient Learning in Partially Observable Contextual Bandit",
  "sections": {
    "task 3: transfer from partially observable contextual bandit to partially observable contextual bandit": "We will now consider a more challenging scenario in which knowledge is transferred between two partially observable contextual bandit agents.\nIn this task, the expert only provides partial model knowledge F^â€‹(a,y,w)^ğ¹ğ‘ğ‘¦ğ‘¤\\hat{F}(a,y,w) to the agent,\nwhile the agent can have prior information F^â€‹(u)^ğ¹ğ‘¢\\hat{F}(u).\nIn this setting, the causal effect ğ”¼â€‹[y|dâ€‹oâ€‹(a),w]ğ”¼delimited-[]conditionalğ‘¦ğ‘‘ğ‘œğ‘ğ‘¤\\mathbb{E}[y|do(a),w] is partially identified. Similarly, we need to solve the following optimization problem: and discrete it as SectionÂ 3.1 does.\nThe object is AlgorithmÂ 3 provides causal bounds for several discrete points,\nwhich can be generalized to the entire space ğ’´Ã—ğ’²ğ’´ğ’²\\mathcal{Y}\\times\\mathcal{W} through interpolation. Contextual bandit.\nWe will begin by discussing discrete contexts and then move on to more general function approximation problems.\nThe objective is to minimize the regret as shown in (2).\nSuppose that the true causal effect ğ”¼â€‹[Y|dâ€‹oâ€‹(a),w]ğ”¼delimited-[]conditionalğ‘Œğ‘‘ğ‘œğ‘ğ‘¤\\mathbb{E}[Y|do(a),w] falls within the interval [lâ€‹(w,a),hâ€‹(w,a)]ğ‘™ğ‘¤ğ‘â„ğ‘¤ğ‘[l(w,a),h(w,a)].\nWe define the set ğ’œâˆ—â€‹(x)superscriptğ’œğ‘¥\\mathcal{A}^{*}(x) as follows: This set eliminates suboptimal context-action pairs (w,a)ğ‘¤ğ‘(w,a) through causal bounds.\nRecall that the context-dependent optimal expected reward is denoted Î¼wâˆ—=maxaâˆˆğ’œâ¡ğ”¼â€‹[Y|dâ€‹oâ€‹(a),w]superscriptsubscriptğœ‡ğ‘¤subscriptğ‘ğ’œğ”¼delimited-[]conditionalğ‘Œğ‘‘ğ‘œğ‘ğ‘¤\\mu_{w}^{*}=\\max_{a\\in\\mathcal{A}}\\mathbb{E}[Y|do(a),w]. Consider a contextual bandit problem with |ğ’œ|<âˆğ’œ|\\mathcal{A}|<\\infty and |ğ’²|<âˆğ’²|\\mathcal{W}|<\\infty.\nFor each arm aâˆˆğ’œğ‘ğ’œa\\in\\mathcal{A} and expected conditional reward Î¼w,asubscriptğœ‡ğ‘¤ğ‘\\mu_{w,a} bounded by [lâ€‹(w,a),hâ€‹(w,a)]ğ‘™ğ‘¤ğ‘â„ğ‘¤ğ‘[l(w,a),h(w,a)].\nFor any given context wâˆˆğ’²ğ‘¤ğ’²w\\in\\mathcal{W}, suppose wğ‘¤w occurs for Twsubscriptğ‘‡ğ‘¤T_{w} times.\nThen in the AlgorithmÂ 2, the conditional number of draws ğ”¼â€‹[Naâ€‹(Tw)]ğ”¼delimited-[]subscriptğ‘ğ‘subscriptğ‘‡ğ‘¤\\mathbb{E}[N_{a}(T_{w})] for any sub-optimal arm is upper bounded as: In this scenario, the classical agent minimizes regrets for |ğ’²|ğ’²|\\mathcal{W}| independent bandit instances.\nThe above theorem (TheoremÂ 3.3) shows how causal bounds improve the performance of classical bandit algorithms by controlling the number of times each arm is pulled.\nNote that TheoremÂ 3.2 is a special case when |W|=1ğ‘Š1|W|=1. Compared with ,\nauthors provide gap-dependent regret bounds for tabular reinforcement learning\nwhich removes state-action pairs that are not reached by an optimal policy, i.e., This result shows similarity in our transfer learning setting as\nwe can model each context as a state and apply the generic RL methods.\nHowever, our method still outperforms generic RL algorithms\nas the contexts can not be affected by the actions.\nHence, the goal of RL algorithms for optimal state-action (context-action) pair is not realizable,\nso the define the regret (2) with respect to a context-dependent optimal action. Consider a contextual bandit problem with |ğ’œ|<âˆğ’œ|\\mathcal{A}|<\\infty and |ğ’²|<âˆğ’²|\\mathcal{W}|<\\infty.\nDenote Then the regret of AlgorithmÂ 4 satisfies The result does not scale with |ğ’œâˆ—â€‹(w)|superscriptğ’œğ‘¤\\sqrt{|\\mathcal{A}^{*}(w)|} but with |ğ’œâˆ—~â€‹(w)|~superscriptğ’œğ‘¤\\sqrt{|\\widetilde{\\mathcal{A}^{*}}(w)|}.\nAlthough our AlgorithmÂ 4 does not explicitly remove suboptimal arms in ğ’œâˆ—~â€‹(w)âˆ’ğ’œâˆ—â€‹(w)~superscriptğ’œğ‘¤superscriptğ’œğ‘¤\\widetilde{\\mathcal{A}^{*}}(w)-\\mathcal{A}^{*}(w),\nthe property of UCB can still control the number of such suboptimal arms due to truncation by causal upper bounds. Using Cauchy-Schwarz inequality, we have This inequality shows clearly that any elimination of suboptimal arms will improve transfer learning algorithms.\nIt also implies that our regret result outperform that of  in terms of minimax versions,\nas the minimax regret in  scales with the right hand side. Denote the contextual bandit instances with prior knowledge lâ€‹(w,a)ğ‘™ğ‘¤ğ‘l(w,a) and hâ€‹(w,a)â„ğ‘¤ğ‘h(w,a) as Suppose |ğ’œ|<âˆğ’œ|\\mathcal{A}|<\\infty and |ğ’²|<âˆğ’²|\\mathcal{W}|<\\infty. Then for any algorithm ğ– ğ– \\mathsf{A}, there exists an absolute constant c>0ğ‘0c>0 such that The lower bound result indicates that our AlgorithmÂ 3 is near-optimal up to logarithmic terms in Tğ‘‡T. Function approximation.\nNext, we consider continuous and general context distributions.\nSince contexts are infinite, it is impossible to learn an optimal policy,\nexcept when the reward has specific function structures, such as linear.\nTherefore, we consider the following function approximation setting under the realizability assumption.\nOnce again, we assume that the true causal effect falls within [lâ€‹(w,a),hâ€‹(w,a)]ğ‘™ğ‘¤ğ‘â„ğ‘¤ğ‘[l(w,a),h(w,a)]. We eliminates the functions that cannot be the true reward function using causal bound: and modify the best arm candidate set as following Our algorithm (AlgorithmÂ 5) is based on the inverse gap weighting technique (IGW)   0 [ref]34.\nIn previous literature concerning IGW, this strategy is employed using a fixed action set size |ğ’œ|ğ’œ|\\mathcal{A}|,\nand the learning rate Î³ğ›¾\\gamma is typically chosen to follow fixed non-adaptive schedule  [ref]34.\nHowever, our algorithm differs from these existing approaches in three aspects.\nFirstly, we only apply the IGW scheme to functions and actions that are not eliminated by causal bounds.\nSecondly, we select the learning rate Î³tsubscriptğ›¾ğ‘¡\\gamma_{t} to be context-dependent, capturing the influence of causal bounds on the arm set.\nThis adaptive approach enhances the efficiency of our algorithm in the online learning process,\nresulting in improved performance in the transfer learning task.\nThirdly, in comparison with 0, our regret order in Tğ‘‡T is ğ’ªâ€‹(Tâ€‹logâ¡(Î´âˆ’1â€‹logâ¡T))ğ’ªğ‘‡superscriptğ›¿1ğ‘‡\\mathcal{O}(\\sqrt{T\\log(\\delta^{-1}\\log T)}) instead of ğ’ªâ€‹(Tâ€‹logâ¡(Î´âˆ’1â€‹T2)â€‹logâ¡T)ğ’ªğ‘‡superscriptğ›¿1superscriptğ‘‡2ğ‘‡\\mathcal{O}(\\sqrt{T\\log(\\delta^{-1}T^{2})}\\log T).\nBy eliminating suboptimal functions using certain causal bounds rather than relying on data-driven upper confidence bounds on the policy space,\nwe are able to save additional logâ¡Tğ‘‡\\log T terms. In AlgorithmÂ 5, one needs to compute â„±âˆ—superscriptâ„±\\mathcal{F}^{*} and ğ’œâˆ—â€‹(w)superscriptğ’œğ‘¤\\mathcal{A}^{*}(w).\nA naive way costs ğ’ªâ€‹(|â„±|)ğ’ªâ„±\\mathcal{O}(|\\mathcal{F}|) time complexity,\nwhich becomes inefficient for large |â„±|â„±|\\mathcal{F}| and infeasible for infinite |â„±|â„±|\\mathcal{F}|.\nActually, we can implicitly compute â„±âˆ—superscriptâ„±\\mathcal{F}^{*} by clipping, i.e., using as the estimator at the epoch mğ‘šm.\nAs f^msubscript^ğ‘“ğ‘š\\hat{f}_{m} gets closer to the true reward function fâˆ—superscriptğ‘“f^{*},\nwhich is within the causal bounds,\nthe causal bounds gradually lose their constraint effect.\nFor computing ğ’œâˆ—â€‹(w)superscriptğ’œğ‘¤\\mathcal{A}^{*}(w), we refer readers to the section 4 of 0,\nwhere a systematic method for computing ğ’œâˆ—â€‹(w)superscriptğ’œğ‘¤\\mathcal{A}^{*}(w) within a given accuracy is provided. Another option to implement AlgorithmÂ 5 is to compute ğ”¼Wâ€‹[|ğ’œâˆ—â€‹(W)|]subscriptğ”¼ğ‘Šdelimited-[]superscriptğ’œğ‘Š\\mathbb{E}_{W}[|\\mathcal{A}^{*}(W)|] using expert knowledge Fâ€‹(a,y,w)ğ¹ğ‘ğ‘¦ğ‘¤F(a,y,w).\nWe can set Î³t=Î·â€‹ğ”¼Wâ€‹[|ğ’œâˆ—â€‹(W)|]â€‹Ï„mâˆ’1logâ¡(2â€‹Î´âˆ’1â€‹|â„±âˆ—|â€‹logâ¡T)subscriptğ›¾ğ‘¡ğœ‚subscriptğ”¼ğ‘Šdelimited-[]superscriptğ’œğ‘Šsubscriptğœğ‘š12superscriptğ›¿1superscriptâ„±ğ‘‡\\gamma_{t}=\\sqrt{\\frac{\\eta\\mathbb{E}_{W}[|\\mathcal{A}^{*}(W)|]\\tau_{m-1}}{\\log(2\\delta^{-1}|\\mathcal{F}^{*}|\\log T)}}, so that Î³tsubscriptğ›¾ğ‘¡\\gamma_{t} remains constant within an epoch.\nOur proof still holds for this option, and the regret order is the same as in TheoremÂ 3.6.\nIntuitively, |ğ’œâ€‹(wt)|ğ’œsubscriptğ‘¤ğ‘¡|\\mathcal{A}(w_{t})| is a sample from an induced distribution with a mean of ğ”¼Wâ€‹[|ğ’œâˆ—â€‹(W)|]subscriptğ”¼ğ‘Šdelimited-[]superscriptğ’œğ‘Š\\mathbb{E}_{W}[|\\mathcal{A}^{*}(W)|],\nso on average, the regrets of both options are of the same order. Consider a contextual bandit problem with |ğ’œ|<âˆğ’œ|\\mathcal{A}|<\\infty and |â„±|<âˆâ„±|\\mathcal{F}|<\\infty.\nSuppose that the realizability 2.1 holds.\nThen with probability at least 1âˆ’Î´1ğ›¿1-\\delta, the expected regret ğ”¼â€‹[Râ€‹eâ€‹gâ€‹(T)]ğ”¼delimited-[]ğ‘…ğ‘’ğ‘”ğ‘‡\\mathbb{E}[Reg(T)] of AlgorithmÂ 5 is upper bounded by As pointed out by 0, the gap-dependent regret of general contextual bandits is not possible, so we only consider minimax regrets.\nPrevious works such as [ref]41 and  have also explored transfer learning in general contextual bandits,\nbut the method in [ref]41 relies on instrumental variables,\nand their regrets scale with |Î |Î \\sqrt{|\\Pi|} instead of the more desirable logâ¡|Î |Î \\sqrt{\\log|\\Pi|}.\nHowever, finding instrumental variables is still an open problem in academia.\nAdditionally, the authors of [ref]41 treat each basis policy as an independent arm in bandit problems,\nwhich is often invalid as similar policies can provide information to each other.\nFor example, exploring the optimal policy can also provide information about the second optimal policy,\nsince these two policies typically yield similar actions and only differ in a small amount of contexts.\nThis insight helps explain why their regret depends on |Î |Î \\sqrt{|\\Pi|} (as |Î |=|â„±|Î â„±|\\Pi|=|\\mathcal{F}|) rather than more desirable logâ¡|Î |Î \\sqrt{\\log|\\Pi|}. We also prove that our upper bound in TheoremÂ 3.6 matches the lower bound in transfer learning.\nDenote the contextual bandit instances with prior knowledge lâ€‹(w,a)ğ‘™ğ‘¤ğ‘l(w,a) and hâ€‹(w,a)â„ğ‘¤ğ‘h(w,a) as Consider a contextual bandit problem with |ğ’œ|<âˆğ’œ|\\mathcal{A}|<\\infty and |â„±|<âˆâ„±|\\mathcal{F}|<\\infty.\nAssume that the agent has access to the function space â„±â„±\\mathcal{F} and the realizability assumption holds.\nThen for any algorithm ğ– ğ– \\mathsf{A}, we have Infinite function classes.\nIt is worth noting that AlgorithmÂ 5 and TheoremÂ 3.6 can be easily\nextended to handle infinite â„±â„±\\mathcal{F} using standard learning-theoretic tools such as metric entropy.\nSuppose â„±â„±\\mathcal{F} is equipped with a maximum norm âˆ¥â‹…âˆ¥\\|\\cdot\\|.\nWe can consider an Ïµitalic-Ïµ\\epsilon-covering â„±Ïµâˆ—subscriptsuperscriptâ„±italic-Ïµ\\mathcal{F}^{*}_{\\epsilon} of â„±âˆ—superscriptâ„±\\mathcal{F}^{*} under maximum norm.\nSince |â„±Ïµâˆ—|subscriptsuperscriptâ„±italic-Ïµ|\\mathcal{F}^{*}_{\\epsilon}| is finite, we can directly replace â„±âˆ—superscriptâ„±\\mathcal{F}^{*} with â„±Ïµâˆ—subscriptsuperscriptâ„±italic-Ïµ\\mathcal{F}^{*}_{\\epsilon}\nand do not change any algorithmic procedure.\nThanks to the property of Ïµitalic-Ïµ\\epsilon-covering,\nthere exists a function fÏµâˆ—âˆˆâ„±Ïµâˆ—superscriptsubscriptğ‘“italic-Ïµsubscriptsuperscriptâ„±italic-Ïµf_{\\epsilon}^{*}\\in\\mathcal{F}^{*}_{\\epsilon} such that\nâ€–fÏµâˆ—âˆ’fâˆ—â€–â‰¤Ïµ.normsuperscriptsubscriptğ‘“italic-Ïµsuperscriptğ‘“italic-Ïµ\\|f_{\\epsilon}^{*}-f^{*}\\|\\leq\\epsilon.\nHence, the regret can be bounded by By replacing the dependence on logâ¡|â„±âˆ—|superscriptâ„±\\log|\\mathcal{F}^{*}| in the algorithmâ€™s parameters with logâ¡|â„±Ïµâˆ—|subscriptsuperscriptâ„±italic-Ïµ\\log|\\mathcal{F}^{*}_{\\epsilon}| and setting Ïµ=1Titalic-Ïµ1ğ‘‡\\epsilon=\\frac{1}{T},\nwe obtain a similar result as TheoremÂ 3.6 up to an additive constant of 111. Let (â„±,âˆ¥â‹…âˆ¥)(\\mathcal{F},\\|\\cdot\\|) be a normed space. The set {f1,â‹¯,fN}subscriptğ‘“1â‹¯subscriptğ‘“ğ‘\\{f_{1},\\cdots,f_{N}\\} is an Ïµitalic-Ïµ\\epsilon-covering of â„±â„±\\mathcal{F} if\nâˆ€fâˆˆâ„±for-allğ‘“â„±\\forall f\\in\\mathcal{F}, there exists iâˆˆ[N]ğ‘–delimited-[]ğ‘i\\in[N] such that â€–fâˆ’fiâ€–â‰¤Ïµnormğ‘“subscriptğ‘“ğ‘–italic-Ïµ\\|f-f_{i}\\|\\leq\\epsilon.\nThe covering number N(â„±,âˆ¥â‹…âˆ¥,Ïµ)N(\\mathcal{F},\\|\\cdot\\|,\\epsilon) is defined as the minimum cardinality Nğ‘N of the covering set over all Ïµitalic-Ïµ\\epsilon-coverings of â„±â„±\\mathcal{F}. It clear that Râ€‹eâ€‹gâ€‹(T)ğ‘…ğ‘’ğ‘”ğ‘‡Reg(T) scales with logN(â„±âˆ—,âˆ¥â‹…âˆ¥,Ïµ)\\sqrt{\\log N(\\mathcal{F}^{*},\\|\\cdot\\|,\\epsilon)}.\nNote that N(â„±âˆ—,âˆ¥â‹…âˆ¥,Ïµ)â‰¤N(â„±,âˆ¥â‹…âˆ¥,Ïµ)N(\\mathcal{F}^{*},\\|\\cdot\\|,\\epsilon)\\leq N(\\mathcal{F},\\|\\cdot\\|,\\epsilon) as â„±âˆ—âŠ‚â„±superscriptâ„±â„±\\mathcal{F}^{*}\\subset\\mathcal{F}.\nThe covering number shows clearly how extra causal bounds help improve the algorithm performance by shrinking the search space.\nLet m=infw,alâ€‹(w,a)ğ‘šsubscriptinfimumğ‘¤ğ‘ğ‘™ğ‘¤ğ‘m=\\inf_{w,a}l(w,a) and M=supw,ahâ€‹(w,a)ğ‘€subscriptsupremumğ‘¤ğ‘â„ğ‘¤ğ‘M=\\sup_{w,a}h(w,a).\nThese bounds chip away the surface of the unit sphere and scoop out the concentric sphere of radius mğ‘šm.\nTherefore, the transfer learning algorithm only needs to search within a spherical shell with a thickness of at most Mâˆ’mğ‘€ğ‘šM-m."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Contextual bandit learning with predictable rewards",
      "abstract": "",
      "year": "2012",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "A. Agarwal, M. DudÃ­k, S. Kale, J. Langford, and R. Schapire"
    },
    {
      "index": 1,
      "title": "Bandits with unobserved confounders: A causal approach",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "E. Bareinboim, A. Forney, and J. Pearl"
    },
    {
      "index": 2,
      "title": "Transfer Learning for Contextual Multi-armed Bandits",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.12612",
      "authors": "C. Cai, T. T. Cai, and H. Li",
      "orig_title": "Transfer learning for contextual multi-armed bandits",
      "paper_id": "2211.12612v2"
    },
    {
      "index": 3,
      "title": "Contextual bandits with linear payoff functions",
      "abstract": "",
      "year": "2011",
      "venue": "Fourteenth International Conference on Artificial Intelligence and Statistics",
      "authors": "W. Chu, L. Li, L. Reyzin, and R. Schapire"
    },
    {
      "index": 4,
      "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "C. Dann, T. V. Marinov, M. Mohri, and J. Zimmert",
      "orig_title": "Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning",
      "paper_id": "2107.01264v2"
    },
    {
      "index": 5,
      "title": "An automated approach to causal inference in discrete settings",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of the American Statistical Association",
      "authors": "G. Duarte, N. Finkelstein, D. Knox, J. Mummolo, and I. Shpitser"
    },
    {
      "index": 6,
      "title": "Minimax theorems",
      "abstract": "",
      "year": "1953",
      "venue": "National Academy of Sciences of the United States of America",
      "authors": "K. Fan"
    },
    {
      "index": 7,
      "title": "Beyond ucb: Optimal and efficient contextual bandits with regression oracles",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "D. Foster and A. Rakhlin"
    },
    {
      "index": 8,
      "title": "Practical Contextual Bandits with Regression Oracles",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "D. Foster, A. Agarwal, M. DudÃ­k, H. Luo, and R. Schapire",
      "orig_title": "Practical contextual bandits with regression oracles",
      "paper_id": "1803.01088v1"
    },
    {
      "index": 9,
      "title": "Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.03104",
      "authors": "D. J. Foster, A. Rakhlin, D. Simchi-Levi, and Y. Xu"
    },
    {
      "index": 10,
      "title": "Dual Instrumental Method for Confounded Kernelized Bandits",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.03224",
      "authors": "X. Gong and J. Zhang",
      "orig_title": "Dual instrumental method for confounded kernelized bandits",
      "paper_id": "2209.03224v1"
    },
    {
      "index": 11,
      "title": "Provably efficient offline reinforcement learning for partially observable Markov decision processes",
      "abstract": "",
      "year": "2022",
      "venue": "39th International Conference on Machine Learning",
      "authors": "H. Guo, Q. Cai, Y. Zhang, Z. Yang, and Z. Wang"
    },
    {
      "index": 12,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "C. Jin, Z. Yang, Z. Wang, and M. I. Jordan",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 13,
      "title": "Instrument-armed bandits",
      "abstract": "",
      "year": "2018",
      "venue": "Algorithmic Learning Theory",
      "authors": "N. Kallus"
    },
    {
      "index": 14,
      "title": "Causal Bandits: Learning Good Interventions via Causal Inference",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "F. Lattimore, T. Lattimore, and M. D. Reid",
      "orig_title": "Causal bandits: Learning good interventions via causal inference",
      "paper_id": "1606.03203v1"
    },
    {
      "index": 15,
      "title": "Bandit algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Cambridge University Press",
      "authors": "T. Lattimore and C. SzepesvÃ¡ri"
    },
    {
      "index": 16,
      "title": "Sequential transfer in multi-armed bandit with finite set of models",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Lazaric, E. Brunskill, et al."
    },
    {
      "index": 17,
      "title": "Probabilities of causation with nonbinary treatment and effect",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.09568",
      "authors": "A. Li and J. Pearl"
    },
    {
      "index": 18,
      "title": "Bounds on Causal Effects and Application to High Dimensional Data",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "A. Li and J. Pearl",
      "orig_title": "Bounds on causal effects and application to high dimensional data",
      "paper_id": "2106.12121v1"
    },
    {
      "index": 19,
      "title": "Unit Selection with Nonbinary Treatment and Effect",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.09569",
      "authors": "A. Li and J. Pearl",
      "orig_title": "Unit selection with nonbinary treatment and effect",
      "paper_id": "2208.09569v1"
    },
    {
      "index": 20,
      "title": "Learning probabilities of causation from finite population data",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.08453",
      "authors": "A. Li, S. Jiang, Y. Sun, and J. Pearl"
    },
    {
      "index": 21,
      "title": "Epsilon-identifiability of causal quantities",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.12022",
      "authors": "A. Li, S. Mueller, and J. Pearl"
    },
    {
      "index": 22,
      "title": "A contextual-bandit approach to personalized news article recommendation",
      "abstract": "",
      "year": "2010",
      "venue": "19th international conference on World wide web",
      "authors": "L. Li, W. Chu, J. Langford, and R. E. Schapire"
    },
    {
      "index": 23,
      "title": "Transferable contextual bandit for cross-domain recommendation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "B. Liu, Y. Wei, Y. Zhang, Z. Yan, and Q. Yang"
    },
    {
      "index": 24,
      "title": "Learning without knowing: Unobserved context in continuous transfer reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "Learning for Dynamics and Control",
      "authors": "C. Liu, Y. Zhang, Y. Shen, and M. M. Zavlanos"
    },
    {
      "index": 25,
      "title": "Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "M. Lu, Y. Min, Z. Wang, and Z. Yang",
      "orig_title": "Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes",
      "paper_id": "2205.13589v3"
    },
    {
      "index": 26,
      "title": "On some useful â€œinefficientâ€ statistics",
      "abstract": "",
      "year": "2006",
      "venue": "Springer",
      "authors": "F. Mosteller"
    },
    {
      "index": 27,
      "title": "A regret bound for greedy partially observed stochastic contextual bandits",
      "abstract": "",
      "year": "2022",
      "venue": "Decision Awareness in Reinforcement Learning Workshop at ICML",
      "authors": "H. Park and M. K. S. Faradonbeh"
    },
    {
      "index": 28,
      "title": "Analysis of Thompson Sampling for Partially Observable Contextual Multi-Armed Bandits",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Control Systems Letters",
      "authors": "H. Park and M. K. S. Faradonbeh",
      "orig_title": "Analysis of thompson sampling for partially observable contextual multi-armed bandits",
      "paper_id": "2110.12175v2"
    },
    {
      "index": 29,
      "title": "Causal inference in statistics: An overview",
      "abstract": "",
      "year": "2009",
      "venue": "Statistics surveys",
      "authors": "J. Pearl"
    },
    {
      "index": 30,
      "title": "The book of why",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of MultiDisciplinary Evaluation",
      "authors": "S. Powell"
    },
    {
      "index": 31,
      "title": "A Minimax Learning Approach to Off-Policy Evaluation in Confounded Partially Observable Markov Decision Processes",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "C. Shi, M. Uehara, J. Huang, and N. Jiang",
      "orig_title": "A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes",
      "paper_id": "2111.06784v4"
    },
    {
      "index": 32,
      "title": "Scalable Computation of Causal Bounds",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "M. Shridharan and G. Iyengar",
      "orig_title": "Scalable computation of causal bounds",
      "paper_id": "2308.02709v1"
    },
    {
      "index": 33,
      "title": "Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematics of Operations Research",
      "authors": "D. Simchi-Levi and Y. Xu"
    },
    {
      "index": 34,
      "title": "Bandits with Partially Observable Confounded Data",
      "abstract": "",
      "year": "2021",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "G. Tennenholtz, U. Shalit, S. Mannor, and Y. Efroni",
      "orig_title": "Bandits with partially observable confounded data",
      "paper_id": "2006.06731v2"
    },
    {
      "index": 35,
      "title": "A general identification condition for causal effects",
      "abstract": "",
      "year": "2002",
      "venue": "Aaai/iaai",
      "authors": "J. Tian and J. Pearl"
    },
    {
      "index": 36,
      "title": "Provably efficient reinforcement learning in partially observable dynamical systems",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun"
    },
    {
      "index": 37,
      "title": "Provably Efficient Causal Reinforcement Learning with Confounded Observational Data",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Wang, Z. Yang, and Z. Wang",
      "orig_title": "Provably efficient causal reinforcement learning with confounded observational data",
      "paper_id": "2006.12311v1"
    },
    {
      "index": 38,
      "title": "Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Xu, H. Kanagawa, and A. Gretton",
      "orig_title": "Deep proxy causal learning and its application to confounded bandit policy evaluation",
      "paper_id": "2106.03907v5"
    },
    {
      "index": 39,
      "title": "Transfer learning in multi-armed bandit: a causal approach",
      "abstract": "",
      "year": "2017",
      "venue": "16th Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "J. Zhang and E. Bareinboim"
    },
    {
      "index": 40,
      "title": "Bounding causal effects on continuous outcome",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "J. Zhang and E. Bareinboim"
    },
    {
      "index": 41,
      "title": "Partial Counterfactual Identification from Observational and Experimental Data",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Zhang, J. Tian, and E. Bareinboim",
      "orig_title": "Partial counterfactual identification from observational and experimental data",
      "paper_id": "2110.05690v1"
    },
    {
      "index": 42,
      "title": "A Comprehensive Survey on Transfer Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE",
      "authors": "F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He",
      "orig_title": "A comprehensive survey on transfer learning",
      "paper_id": "1911.02685v3"
    }
  ]
}