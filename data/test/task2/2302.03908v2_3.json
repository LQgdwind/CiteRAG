{
  "paper_id": "2302.03908v2",
  "title": "Syntax and Domain Aware Model for Unsupervised Program Translation",
  "sections": {
    "vii-b threats to validity": "Threats to internal validity include the influence of the model hyper-parameter settings for both our model and the reproduced baseline model TransCoder*.\nTo ensure a fair comparison between TransCoder and SDA-Trans, we use the same setting with the strong baseline TransCoder [ref]21 in our backbone model. For our proposed components, i.e., GAT layers and heads, we chose the hyper-parameters through a small-scale grid search and manual selection.\nFor TransCoder*, as the authors of [ref]21 offered the artifact and running scripts to re-train their model with new training data 999https://github.com/facebookresearch/TransCoder#train-a-new-model. We follow their scripts to train TranCoder* with our data. During the training, we tuned the mini-batch size and initial learning rate according to our GPU memory and the model’s initial performance. Then we use the same configuration in SDA-Trans. We have tried our best to make sure the training process is stable and finish training until the model is converged. However, it cannot be denied that when the datasets changes, the original model’s performance could be affected. We have tried our best to make TransCoder* approximate its optimal performance, and make the comparison between TransCoder* and SDA-Trans fair. Therefore, there is a minor threat to the hyper-parameter tuning. Threats to external validity include the quality of the datasets. We use the Python and Java programs of the CodeSearchNet dataset  to pre-train our model. CodeSearchNet contains function-document pairs collected from GitHub repositories, covering 6 kinds of languages. It is widely used in code pre-train models [ref]10 , and these model has achieved great performance in various downstream tasks. Thus, most of the programs can be viewed as high-quality. For evaluation, we use the dataset built in TransCoder [ref]21. The evaluation set is composed of hundreds of parallel functions in 3 languages, and some of the programs have unit tests that can be used to evaluate the correctness of generated translations. Nerveless, the evaluation scale is not big and comprehensive enough, and further evaluation on big data and other programming languages is needed.\nBesides, a further threat is related to the baseline choice. TransCoder-ST  is state-of-the-art for unsupervised code translation, which outperforms TransCoder [ref]21 substantially. It is an incremental work on TransCoder and relies on large-scale pre-trained models as its backbone.\nSpecifically, TransCoder-ST improves TransCoder by filtering out invalid translations through an automated unit-testing system during back-translation, which reduces the noise and further boosts translation performance. The essential question of high training costs and poor cross-lingual transfer is not solved. We believe their filtering technology is orthogonal to SDA-Trans, which can also be used to improve the back-translation performance in our model. Thus, we don’t consider it as a baseline in this paper. Threats to construct validity relate to the property of the evaluation measure. We adopted BLEU-4 score and exact match accuracy to measure the quality of the translated programs by comparing the predicted programs with the ground truth, and these two metrics are generally used in the previous program translation work. Besides, we further introduce computational accuracy to evaluate the semantic correctness of the generated translations."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "A Transformer-based Approach for Source Code Summarization",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang",
      "orig_title": "A transformer-based approach for source code summarization",
      "paper_id": "2005.00653v1"
    },
    {
      "index": 1,
      "title": "Unified Pre-training for Program Understanding and Generation",
      "abstract": "",
      "year": "2021",
      "venue": "NAACL-HLT",
      "authors": "Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang",
      "orig_title": "Unified pre-training for program understanding and generation",
      "paper_id": "2103.06333v2"
    },
    {
      "index": 2,
      "title": "Syntax-augmented multilingual BERT for cross-lingual transfer",
      "abstract": "",
      "year": "2021",
      "venue": "ACL/IJCNLP (1)",
      "authors": "Wasi Uddin Ahmad, Haoran Li, Kai-Wei Chang, and Yashar Mehdad"
    },
    {
      "index": 3,
      "title": "Gate: graph attention transformer encoder for cross-lingual relation and event extraction",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Wasi Uddin Ahmad, Nanyun Peng, and Kai-Wei Chang"
    },
    {
      "index": 4,
      "title": "Unsupervised Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR (Poster)",
      "authors": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho",
      "orig_title": "Unsupervised neural machine translation",
      "paper_id": "1710.11041v2"
    },
    {
      "index": 5,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 6,
      "title": "When Deep Learning Met Code Search",
      "abstract": "",
      "year": "2019",
      "venue": "ESEC/SIGSOFT FSE",
      "authors": "José Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra",
      "orig_title": "When deep learning met code search",
      "paper_id": "1905.03813v4"
    },
    {
      "index": 7,
      "title": "Tree-to-tree Neural Networks for Program Translation",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Xinyun Chen, Chang Liu, and Dawn Song",
      "orig_title": "Tree-to-tree neural networks for program translation",
      "paper_id": "1802.03691v3"
    },
    {
      "index": 8,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT (1)",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 9,
      "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP (Findings)",
      "authors": "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou",
      "orig_title": "Codebert: A pre-trained model for programming and natural languages",
      "paper_id": "2002.08155v4"
    },
    {
      "index": 10,
      "title": "Domain-Adversarial Training of Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of machine learning research",
      "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky",
      "orig_title": "Domain-adversarial training of neural networks",
      "paper_id": "1505.07818v4"
    },
    {
      "index": 11,
      "title": "Universal Neural Machine Translation for Extremely Low Resource Languages",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li",
      "orig_title": "Universal neural machine translation for extremely low resource languages",
      "paper_id": "1802.05368v2"
    },
    {
      "index": 12,
      "title": "DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim",
      "orig_title": "Deepam: Migrate apis with multi-modal sequence to sequence learning",
      "paper_id": "1704.07734v1"
    },
    {
      "index": 13,
      "title": "Deep code search",
      "abstract": "",
      "year": "2018",
      "venue": "ICSE",
      "authors": "Xiaodong Gu, Hongyu Zhang, and Sunghun Kim"
    },
    {
      "index": 14,
      "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou",
      "orig_title": "Graphcodebert: Pre-training code representations with data flow",
      "paper_id": "2009.08366v4"
    },
    {
      "index": 15,
      "title": "Dual Learning for Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma",
      "orig_title": "Dual learning for machine translation",
      "paper_id": "1611.00179v1"
    },
    {
      "index": 16,
      "title": "Deep code comment generation",
      "abstract": "",
      "year": "2018",
      "venue": "ICPC",
      "authors": "Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin"
    },
    {
      "index": 17,
      "title": "CodeSearchNet Challenge Evaluating the State of Semantic Code Search",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.09436",
      "authors": "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt",
      "orig_title": "Codesearchnet challenge: Evaluating the state of semantic code search",
      "paper_id": "1909.09436v3"
    },
    {
      "index": 18,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 19,
      "title": "Statistical Machine Translation",
      "abstract": "",
      "year": "2010",
      "venue": "Cambridge University Press",
      "authors": "Philipp Koehn"
    },
    {
      "index": 20,
      "title": "Unsupervised Translation of Programming Languages",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.03511",
      "authors": "Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample",
      "orig_title": "Unsupervised translation of programming languages",
      "paper_id": "2006.03511v3"
    },
    {
      "index": 21,
      "title": "DOBF: A Deobfuscation Pre-Training Objective for Programming Languages",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec, and Guillaume Lample",
      "orig_title": "DOBF: A deobfuscation pre-training objective for programming languages",
      "paper_id": "2102.07492v3"
    },
    {
      "index": 22,
      "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR (Poster)",
      "authors": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato",
      "orig_title": "Unsupervised machine translation using monolingual corpora only",
      "paper_id": "1711.00043v2"
    },
    {
      "index": 23,
      "title": "Phrase-Based & Neural Unsupervised Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato",
      "orig_title": "Phrase-based & neural unsupervised machine translation",
      "paper_id": "1804.07755v2"
    },
    {
      "index": 24,
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer",
      "orig_title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "paper_id": "1910.13461v1"
    },
    {
      "index": 25,
      "title": "Multi-task learning based pre-trained language model for code completion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/ACM International Conference on Automated Software Engineering",
      "authors": "Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin"
    },
    {
      "index": 26,
      "title": "Lexical statistical machine translation for language migration",
      "abstract": "",
      "year": "2013",
      "venue": "Foundations of Software Engineering",
      "authors": "Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen"
    },
    {
      "index": 27,
      "title": "Divide-and-conquer approach for multi-phase statistical migration for source code (t)",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE/ACM International Conference on Automated Software Engineering (ASE)",
      "authors": "Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen"
    },
    {
      "index": 28,
      "title": "Integrating tree path in transformer for code representation",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin"
    },
    {
      "index": 29,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al."
    },
    {
      "index": 30,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 31,
      "title": "Leveraging Automated Unit Tests for Unsupervised Code Translation",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Baptiste Rozière, Jie Zhang, François Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample",
      "orig_title": "Leveraging automated unit tests for unsupervised code translation",
      "paper_id": "2110.06773v2"
    },
    {
      "index": 32,
      "title": "Improving neural machine translation models with monolingual data",
      "abstract": "",
      "year": "2016",
      "venue": "ACL (1)",
      "authors": "Rico Sennrich, Barry Haddow, and Alexandra Birch"
    },
    {
      "index": 33,
      "title": "Pythia: AI-assisted Code Completion System",
      "abstract": "",
      "year": "2019",
      "venue": "ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan",
      "orig_title": "Pythia: Ai-assisted code completion system",
      "paper_id": "1912.00742v1"
    },
    {
      "index": 34,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of machine learning research",
      "authors": "Laurens Van der Maaten and Geoffrey Hinton",
      "orig_title": "Visualizing data using t-sne",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 35,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 36,
      "title": "Graph Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR (Poster)",
      "authors": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio",
      "orig_title": "Graph attention networks",
      "paper_id": "1710.10903v3"
    },
    {
      "index": 37,
      "title": "Dynamic Neural Program Embeddings for Program Repair",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR (Poster)",
      "authors": "Ke Wang, Rishabh Singh, and Zhendong Su",
      "orig_title": "Dynamic neural program embeddings for program repair",
      "paper_id": "1711.07163v4"
    },
    {
      "index": 38,
      "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
      "abstract": "",
      "year": "2021",
      "venue": "EMNLP (1)",
      "authors": "Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi",
      "orig_title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
      "paper_id": "2109.00859v1"
    },
    {
      "index": 39,
      "title": "AURA: a hybrid approach to identify framework evolution",
      "abstract": "",
      "year": "2010",
      "venue": "ICSE (1)",
      "authors": "Wei Wu, Yann-Gaël Guéhéneuc, Giuliano Antoniol, and Miryung Kim"
    },
    {
      "index": 40,
      "title": "A contextual alignment enhanced cross graph attention network for cross-lingual entity alignment",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Computational Linguistics",
      "authors": "Zhiwen Xie, Runjie Zhu, Kunsong Zhao, Jin Liu, Guangyou Zhou, and Xiangji Huang"
    },
    {
      "index": 41,
      "title": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Michihiro Yasunaga and Percy Liang",
      "orig_title": "Graph-based, self-supervised program repair from diagnostic feedback",
      "paper_id": "2005.10636v2"
    },
    {
      "index": 42,
      "title": "Mining API mapping for language migration",
      "abstract": "",
      "year": "2010",
      "venue": "ICSE (1)",
      "authors": "Hao Zhong, Suresh Thummalapenta, Tao Xie, Lu Zhang, and Qing Wang"
    },
    {
      "index": 43,
      "title": "Language-Agnostic Representation Learning of Source Code from Structure and Context",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann",
      "orig_title": "Language-agnostic representation learning of source code from structure and context",
      "paper_id": "2103.11318v1"
    }
  ]
}