{
  "paper_id": "2301.13867v2",
  "title": "Mathematical Capabilities of ChatGPT",
  "sections": {
    "subdatasets": "The subdatasets that make up our GHOSTS dataset are summarized in Table 1. In the following, we describe each subdataset in more detail. This subdataset consists of a collection of books    that are widely used in universities to teach upper undergraduate or first-year graduate courses in a degree in mathematics. We have used most of the exercises from these books’ first and second chapters, except for the book , where we only used exercises from the first chapter, which was longer than the other books’ chapters. This subdataset consists of a number of proofs sourced from math.stackexchange.com, a collection of books  , and the MATH dataset [ref]11, where parts of the proofs were intentionally deleted and the LLM was prompted to fill in the gaps: This was done either by (1) using a MISSING token, (2) finishing the proof early and prompting the LLM to complete it, or (3) explicitly asking for certain conditions or results. This subdataset consists of a selection of exercises from the book Problem-Solving Strategies , that is often used to prepare for mathematical competitions. We selected and graded the LLM outputs on one hundred exercises drawn from all chapters. This subdataset consists of random samples of integrals from the test set of . There are three ways in which integrals are generated in : Forward generation (FWD), Backward generation (BWD), and Backward generation with integration by parts (IBP). We sample 212121 integrals from FWD test set, 202020 integrals from the BWD test set, and 595959 integrals from the IBP test set.\nAs these integrals are given in Polish/prefix notation, a natural-language prompt conversion of them is unlikely to be witnessed in the training dataset of (Chat)GPT.\nThe assessment was done by verifying the correctness of the output both by using Mathematica, as well as making use of the provided solutions (in Polish notation), which  generated using SymPy.\nIn particular, we notice that all integrals in this dataset have solutions that can be expressed using elementary functions. This subdataset consists of a random sample of problems from the MATH dataset [ref]11.\nThe latter dataset attaches a level of difficulty to each problem.\nWe focused on two domains, Algebra and Probability Theory, and sampled an equal number of problems at each level of difficulty. This subdataset consists of problems that were not sampled from a particular source but generated by a human expert in the field. In the file Named Theorem Proof Completion, we focused on prompting the LLM to provide proof outlines of various theorems that are sufficiently well-known within Functional Analysis to have names. In the Definition Retrieval file, we asked the LLM to correctly state various definitions centered around Functional Analysis and Topology. In contrast, in the Reverse Definition Retrieval file, we verified whether the LLM was able to deduce the name of a mathematical object by describing its properties. Because input to (Chat)GPT is purely textual (at the time of writing), certain types of questions that might be stated and solved in a non-text-based fashion (e.g., questions involving graphical diagrams, without text explaining the diagram555See, e.g., Exercise 15 in [38, Chapter 2], which asked the reader to inspect a figure on which the problem is based., as occasionally occur in ), have been excluded. Our subdatasets can be categorized along the following dimensions (see Appendix B.1 for more details): Mathematical difficulty (ascending): (M1) Elementary arithmetic problems, (M2) Symbolic problems, (M3) (Under)graduate-level exercises, (M4) Mathematical olympiad problems. Question type: (Q1) Stating mathematical facts, (Q2) Overview-type review questions, (Q3) Computational questions, (Q4) Theorem proofs or puzzle solutions, (Q5) Proof-completion questions. Types of high out-of-distribution likelihood: (D1) Nontrivial problem encoding, (D2) Succinct solution, (D3) Spoken dialogue. The existing datasets of natural-language mathematics are far from covering all possible combinations across these dimensions. In our well-crafted GHOSTS datasets, we have striven to cover each of these aspects individually, as can be seen in Table 1. The next section specifies the format of our dataset and the methodology for analyzing (Chat)GPT’s output."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Das Ende von Google, wie wir es kannten",
      "abstract": "",
      "year": "2023",
      "venue": "Der Spiegel",
      "authors": "Sascha Lobo"
    },
    {
      "index": 1,
      "title": "The ChatGPT bot is causing panic now – but it’ll soon be as mundane a tool as Excel",
      "abstract": "",
      "year": "2023",
      "venue": "The Guardian",
      "authors": "John Naughton"
    },
    {
      "index": 2,
      "title": "The Brilliance and Weirdness of ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "The New York Times",
      "authors": "Kevon Roose"
    },
    {
      "index": 3,
      "title": "I made ChatGPT take a full SAT test. Here’s how it did: [Image attached] [Tweet]",
      "abstract": "",
      "year": "2023",
      "venue": "Twitter",
      "authors": "teddy [@teddynpc]"
    },
    {
      "index": 4,
      "title": "It’s amusing when ChatGPT makes ridiculous mathematical mistakes. But of course, it’s more interesting to find out what it can do well. Here’s one example that wasn’t bad: I gave it a very rough outline of a proof and asked it to fill in the details [Tweet]",
      "abstract": "",
      "year": "2023",
      "venue": "Twitter",
      "authors": "Timothy Gowers [@wtgowers]"
    },
    {
      "index": 5,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.08774",
      "authors": "OpenAI (2023)",
      "orig_title": "GPT-4 technical report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 6,
      "title": "Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "medRxiv",
      "authors": "Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, and Lorie De Leon et al."
    },
    {
      "index": 7,
      "title": "What is the IQ of ChatGPT?",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "David Rozado"
    },
    {
      "index": 8,
      "title": "Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Performance in the Operations Management Course",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Christian Terwiesch"
    },
    {
      "index": 9,
      "title": "ChatGPT – Release Notes",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Natalie"
    },
    {
      "index": 10,
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.03874",
      "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, and Steven Basart et al.",
      "orig_title": "Measuring mathematical problem solving with the MATH dataset",
      "paper_id": "2103.03874v2"
    },
    {
      "index": 11,
      "title": "Deep learning for symbolic mathematics",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01412",
      "authors": "Guillaume Lample and François Charton",
      "orig_title": "Deep learning for symbolic mathematics",
      "paper_id": "1912.01412v1"
    },
    {
      "index": 12,
      "title": "Some studies in machine learning using the game of checkers",
      "abstract": "",
      "year": "1959",
      "venue": "IBM Journal of Research and Development",
      "authors": "A. L. Samuel"
    },
    {
      "index": 13,
      "title": "Learning from previous proof experience: A survey",
      "abstract": "",
      "year": "1999",
      "venue": "TU München",
      "authors": "Jörg Denzinger, Matthias Fuchs, Christoph Goller, and Stephan Schulz"
    },
    {
      "index": 14,
      "title": "History of interactive theorem proving",
      "abstract": "",
      "year": "2014",
      "venue": "Computational Logic",
      "authors": "John Harrison, Josef Urban, and Freek Wiedijk"
    },
    {
      "index": 15,
      "title": "Machine learning class numbers of real quadratic fields",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.09283",
      "authors": "Malik Amir, Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, and Eldar Sultanow",
      "orig_title": "Machine Learning Class Numbers of Real Quadratic Fields",
      "paper_id": "2209.09283v1"
    },
    {
      "index": 16,
      "title": "Advancing mathematics by guiding human intuition with AI",
      "abstract": "",
      "year": "2021",
      "venue": "Nature",
      "authors": "Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, and Daniel Zheng et al."
    },
    {
      "index": 17,
      "title": "Machine-learning the string landscape",
      "abstract": "",
      "year": "2017",
      "venue": "Physics Letters B",
      "authors": "Yang-Hui He"
    },
    {
      "index": 18,
      "title": "Solving Quantitative Reasoning Problems with Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, and Henryk Michalewski et al.",
      "orig_title": "Solving quantitative reasoning problems with language models",
      "paper_id": "2206.14858v2"
    },
    {
      "index": 19,
      "title": "Learning advanced mathematical computations from examples",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Francois Charton, Amaury Hayat, and Guillaume Lample",
      "orig_title": "Learning advanced mathematical computations from examples",
      "paper_id": "2006.06462v2"
    },
    {
      "index": 20,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 21,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.02311",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, and Gaurav Mishra et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 22,
      "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
      "abstract": "",
      "year": "2019",
      "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Aida Amini, Saadia Gabriel, Shanchuan Lin, and Rik Koncel-Kedziorski et al.",
      "orig_title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
      "paper_id": "1905.13319v1"
    },
    {
      "index": 23,
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.14168",
      "authors": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, and Heewoo Jun et al.",
      "orig_title": "Training verifiers to solve math word problems",
      "paper_id": "2110.14168v2"
    },
    {
      "index": 24,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, and Jared D Kaplan et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 25,
      "title": "Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning",
      "abstract": "",
      "year": "2021",
      "venue": "Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
      "authors": "Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski",
      "orig_title": "Measuring and improving BERT’s mathematical abilities by predicting the order of reasoning",
      "paper_id": "2106.03921v1"
    },
    {
      "index": 26,
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "abstract": "",
      "year": "2017",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom",
      "orig_title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "paper_id": "1705.04146v3"
    },
    {
      "index": 27,
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.05100",
      "authors": "Teven Le Scao and Angela Fan et al.",
      "orig_title": "Bloom: A 176b-parameter open-access multilingual language model",
      "paper_id": "2211.05100v4"
    },
    {
      "index": 28,
      "title": "LaMDA: Language Models for Dialog Applications",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.08239",
      "authors": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, and Apoorv Kulshreshtha et al.",
      "orig_title": "Lamda: Language models for dialog applications",
      "paper_id": "2201.08239v3"
    },
    {
      "index": 29,
      "title": "A Survey of Deep Learning for Mathematical Reasoning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.10535",
      "authors": "Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang",
      "orig_title": "A survey of deep learning for mathematical reasoning",
      "paper_id": "2212.10535v2"
    },
    {
      "index": 30,
      "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.01112",
      "authors": "Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho",
      "orig_title": "Naturalproofs: Mathematical theorem proving in natural language",
      "paper_id": "2104.01112v2"
    },
    {
      "index": 31,
      "title": "NaturalProver: Grounded Mathematical Proof Generation with Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.12910",
      "authors": "Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi",
      "orig_title": "Naturalprover: Grounded mathematical proof generation with language models",
      "paper_id": "2205.12910v2"
    },
    {
      "index": 32,
      "title": "Probability: Theory and Examples",
      "abstract": "",
      "year": "2019",
      "venue": "Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press",
      "authors": "Rick Durrett"
    },
    {
      "index": 33,
      "title": "Topology",
      "abstract": "",
      "year": "2000",
      "venue": "Prentice-Hall",
      "authors": "James R. Munkres"
    },
    {
      "index": 34,
      "title": "Functional analysis",
      "abstract": "",
      "year": "1991",
      "venue": "McgGraw-Hill",
      "authors": "Walter Rudin"
    },
    {
      "index": 35,
      "title": "Linear algebra done right",
      "abstract": "",
      "year": "2015",
      "venue": "Springer",
      "authors": "Sheldon Axler"
    },
    {
      "index": 36,
      "title": "Principles of Mathematical Analysis",
      "abstract": "",
      "year": "1976",
      "venue": "International series in pure and applied mathematics. McGraw-Hill",
      "authors": "W. Rudin"
    },
    {
      "index": 37,
      "title": "Problem-Solving Strategies",
      "abstract": "",
      "year": "1998",
      "venue": "Springer",
      "authors": "Arthur Engel"
    },
    {
      "index": 38,
      "title": "Does ChatGPT code LaTeX and write proofs? Youtube",
      "abstract": "",
      "year": "2023",
      "venue": "Youtube",
      "authors": "Tranquil Sea Of Math"
    },
    {
      "index": 39,
      "title": "Huh. ChatGPT confidently gives the right kind of reasoning to solve this math problem, but whiffs on the algebra in the middle and gets the answer wrong [Tweet]",
      "abstract": "",
      "year": "2023",
      "venue": "Twitter",
      "authors": "Richard Van Noorden @richvn@mastodon.social [@Richvn]"
    },
    {
      "index": 40,
      "title": "ChatGPT Usage and Limitations",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Amos Azaria"
    },
    {
      "index": 41,
      "title": "Mathematics, word problems, common sense, and artificial intelligence",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.09723",
      "authors": "Ernest Davis",
      "orig_title": "Mathematics, word problems, common sense, and artificial intelligence",
      "paper_id": "2301.09723v2"
    },
    {
      "index": 42,
      "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.12712",
      "authors": "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al."
    },
    {
      "index": 43,
      "title": "Measuring Massive Multitask Language Understanding",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.03300",
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt",
      "orig_title": "Measuring massive multitask language understanding",
      "paper_id": "2009.03300v3"
    },
    {
      "index": 44,
      "title": "The Lean mathematical library",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGPLAN International Conference on Certified Programs and Proofs. ACM",
      "authors": "The mathlib Community"
    },
    {
      "index": 45,
      "title": "Mathematical Reasoning via Self-supervised Skip-tree Training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.04757v3",
      "authors": "Markus N. Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy",
      "orig_title": "Mathematical reasoning via self-supervised skip-tree training",
      "paper_id": "2006.04757v3"
    },
    {
      "index": 46,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.02155",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, and Carroll L. Wainwright et al."
    },
    {
      "index": 47,
      "title": "InstructGPT: Training Language Models to Follow Instructions with Human Feedback",
      "abstract": "",
      "year": "2023",
      "venue": "GitHub repository",
      "authors": "Carroll Wainwright and Ryan Lowe"
    },
    {
      "index": 48,
      "title": "If text-davinci-001 is a rough approximate to the model reported in the NeurIPS 2020 paper, and text-davinci-002 is  InstructGPT in the 2022 preprint, then what is just \"davinci\"? Trying to reproduce results from a time before this naming existed [Tweet]",
      "abstract": "",
      "year": "2023",
      "venue": "Twitter",
      "authors": "Sarah Wiegreffe (sigmoid.social/@sarah) [@sarahwiegreffe]"
    },
    {
      "index": 49,
      "title": "GPT-4 API waitlist",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 50,
      "title": "Documentation - Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 51,
      "title": "OpenAI API Reference - Chat Completion Endpoint",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 52,
      "title": "Evaluating Large Language Models Trained on Code",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.03374",
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and Henrique Ponde de Oliveira Pinto et al.",
      "orig_title": "Evaluating large language models trained on code",
      "paper_id": "2107.03374v2"
    },
    {
      "index": 53,
      "title": "Datasheets for datasets",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM",
      "authors": "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford"
    }
  ]
}