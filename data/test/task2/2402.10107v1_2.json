{
  "paper_id": "2402.10107v1",
  "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
  "sections": {
    "ii-a diffusion models for text generation": "Diffusion generative models were initially proposed in [ref]14 and have shown state-of-the-art sample quality in image and audio domains, readers of interest can be referred to [ref]15 [ref]16  . In the language domain, DiffusionBERT explores training BERT to learn the reverse process of a discrete diffusion process with an absorbing state . Recent work on continuous diffusion and controllable text generation  has demonstrated that language models can successfully control simple sentence attributes. In general, a diffusion model [ref]15  is a latent variable model that represents data x0âˆˆâ„dsubscriptğ‘¥0superscriptâ„ğ‘‘x_{0}\\in\\mathbb{R}^{d} as a Markov chain xTâ€‹â€¦â€‹x0subscriptğ‘¥ğ‘‡â€¦subscriptğ‘¥0x_{T}...x_{0}, with the respective variable in â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d}, and xTsubscriptğ‘¥ğ‘‡x_{T} following a Gaussian distribution. The diffusion model aims to denoise the sequence of latent variables xT:1subscriptğ‘¥:ğ‘‡1x_{T:1} to approximate target samples generated from the target data distribution (see Figure 2). The initial state pÎ¸subscriptğ‘ğœƒp_{\\theta} is approximated as ğ’©â€‹(0,I)ğ’©0ğ¼\\mathcal{N}(0,I), and the denoising transition xtâ†’xtâˆ’1â†’subscriptğ‘¥ğ‘¡subscriptğ‘¥ğ‘¡1x_{t}\\rightarrow x_{t-1} is parameterized by the model pÎ¸â€‹(xt|xtâˆ’1)=ğ’©â€‹(xtâˆ’1;Î¼Î¸â€‹(xt,t),ÏƒÎ¸â€‹(xt,t))subscriptğ‘ğœƒconditionalsubscriptğ‘¥ğ‘¡subscriptğ‘¥ğ‘¡1ğ’©subscriptğ‘¥ğ‘¡1subscriptğœ‡ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡subscriptğœğœƒsubscriptğ‘¥ğ‘¡ğ‘¡p_{\\theta}(x_{t}|x_{t-1})=\\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t},t),\\sigma_{\\theta}(x_{t},t)). For instance, Î¼Î¸subscriptğœ‡ğœƒ\\mu_{\\theta} and ÏƒÎ¸subscriptğœğœƒ\\sigma_{\\theta} can be determined using a Transformer . To train the diffusion model, we use a forward process to generate intermediate latent variables x1:Tsubscriptğ‘¥:1ğ‘‡x_{1:T}. The process starts by incrementally adding Gaussian noise to the initial data point x0subscriptğ‘¥0x_{0}. As the diffusion progresses through step Tğ‘‡T, the samples xTsubscriptğ‘¥ğ‘‡x_{T} become approximately Gaussian. Each transition xtâˆ’1â†’xtâ†’subscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡x_{t-1}\\rightarrow x_{t} is parameterized by qâ€‹(xt|xtâˆ’1)=ğ’©â€‹(xt;1âˆ’Î²tâ€‹xtâˆ’1,Î²tâ€‹I)ğ‘conditionalsubscriptğ‘¥ğ‘¡subscriptğ‘¥ğ‘¡1ğ’©subscriptğ‘¥ğ‘¡1subscriptğ›½ğ‘¡subscriptğ‘¥ğ‘¡1subscriptğ›½ğ‘¡ğ¼q(x_{t}|x_{t-1})=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}I), where Î²tsubscriptğ›½ğ‘¡\\beta_{t} is a hyperparameter representing the amount of noise added at diffusion step tğ‘¡t. The forward process qğ‘q does not contain any trainable parameters. The training objective of the diffusion model is to maximize the marginal likelihood of the data, which is formulated as ğ”¼x0âˆ¼pdâ€‹aâ€‹tâ€‹aâ€‹{logâ¡[pÎ¸â€‹(x0)]}subscriptğ”¼similar-tosubscriptğ‘¥0subscriptğ‘ğ‘‘ğ‘ğ‘¡ğ‘subscriptğ‘ğœƒsubscriptğ‘¥0\\mathbb{E}_{x_{0}\\sim p_{data}\\{\\log[p_{\\theta}(x_{0})]\\}}. To achieve this, we utilize the variational lower bound of logâ¡[pÎ¸â€‹(x0)]subscriptğ‘ğœƒsubscriptğ‘¥0\\log[p_{\\theta}(x_{0})] [ref]14. The model is trained to reverse the diffusion process and accurately reconstruct the original data. However, achieving this goal can be challenging and may necessitate the use of several optimization techniques to stabilize it . To tackle this issue, [ref]15 proposed a straightforward surrogate objective. They extended and reweighted each KL-divergence term in LVâ€‹Lâ€‹Bsubscriptğ¿ğ‘‰ğ¿ğµL_{VLB} to create a mean-squared error loss, which is also referred to as : where Î¼^â€‹(xt,x0)^ğœ‡subscriptxğ‘¡subscriptx0\\hat{\\mu}\\left(\\mathrm{x}_{t},\\mathrm{x}_{0}\\right) is the mean of the posterior qâ€‹(xtâˆ’1|x0,xt)ğ‘conditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥0subscriptğ‘¥ğ‘¡q(x_{t-1}|x_{0},x_{t}) which is a closed form Gaussian, and Î¼Î¸â€‹(xt,t)subscriptğœ‡ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡\\mu_{\\theta}(x_{t},t) is the predicted mean of pÎ¸â€‹(xtâˆ’1|xt)subscriptğ‘ğœƒconditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡p_{\\theta}(x_{t-1}|x_{t}) computed by a neural network. We also make use of similar simplifications in DLM  to stabilize training and improve sample quality."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "S. Gehrmann et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2204.02311",
      "authors": "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 2,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2205.01068",
      "authors": "S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al.",
      "orig_title": "Opt: Open pre-trained transformer language models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 3,
      "title": "Fudge: Controlled Text Generation With Future Discriminators",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2104.05218",
      "authors": "K. Yang and D. Klein",
      "orig_title": "FUDGE: controlled text generation with future discriminators",
      "paper_id": "2104.05218v2"
    },
    {
      "index": 4,
      "title": "Diffusion-LM improves controllable text generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2205.14217",
      "authors": "X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B. Hashimoto"
    },
    {
      "index": 5,
      "title": "Lora: Low-rank adaptation of large language models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2106.09685",
      "authors": "E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen"
    },
    {
      "index": 6,
      "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2110.04366",
      "authors": "J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig",
      "orig_title": "Towards a unified view of parameter-efficient transfer learning",
      "paper_id": "2110.04366v3"
    },
    {
      "index": 7,
      "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1909.05858",
      "authors": "N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher",
      "orig_title": "CTRL: A conditional transformer language model for controllable generation",
      "paper_id": "1909.05858v2"
    },
    {
      "index": 8,
      "title": "A Plug-and-Play Method for Controlled Text Generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2109.09707",
      "authors": "D. Pascual, B. Egressy, C. Meister, R. Cotterell, and R. Wattenhofer",
      "orig_title": "A plug-and-play method for controlled text generation",
      "paper_id": "2109.09707v1"
    },
    {
      "index": 9,
      "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo",
      "orig_title": "Vector quantized diffusion model for text-to-image synthesis",
      "paper_id": "2111.14822v3"
    },
    {
      "index": 10,
      "title": "Discrete contrastive diffusion for cross-modal and conditional generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2206.07771",
      "authors": "Y. Zhu, Y. Wu, K. Olszewski, J. Ren, S. Tulyakov, and Y. Yan"
    },
    {
      "index": 11,
      "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2207.09983",
      "authors": "D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu",
      "orig_title": "Diffsound: Discrete diffusion model for text-to-sound generation",
      "paper_id": "2207.09983v2"
    },
    {
      "index": 12,
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2005.00247",
      "authors": "J. Pfeiffer, A. Kamath, A. RÃ¼cklÃ©, K. Cho, and I. Gurevych",
      "orig_title": "Adapterfusion: Non-destructive task composition for transfer learning",
      "paper_id": "2005.00247v3"
    },
    {
      "index": 13,
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2101.00190",
      "authors": "X. L. Li and P. Liang",
      "orig_title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "paper_id": "2101.00190v1"
    },
    {
      "index": 14,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 15,
      "title": "Denoising Diffusion Probabilistic Models",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Ho, A. Jain, and P. Abbeel",
      "orig_title": "Denoising diffusion probabilistic models",
      "paper_id": "2006.11239v2"
    },
    {
      "index": 16,
      "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2105.03023",
      "authors": "A. Liu, M. Sap, X. Lu, S. Swayamdipta, C. Bhagavatula, N. A. Smith, and Y. Choi",
      "orig_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
      "paper_id": "2105.03023v2"
    },
    {
      "index": 17,
      "title": "Symbolic Music Generation with Diffusion Models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2103.16091",
      "authors": "G. Mittal, J. Engel, C. Hawthorne, and I. Simon",
      "orig_title": "Symbolic music generation with diffusion models",
      "paper_id": "2103.16091v2"
    },
    {
      "index": 18,
      "title": "Improved denoising diffusion probabilistic models",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Q. Nichol and P. Dhariwal"
    },
    {
      "index": 19,
      "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2211.15029",
      "authors": "Z. He, T. Sun, K. Wang, X. Huang, and X. Qiu",
      "orig_title": "Diffusionbert: Improving generative masked language models with diffusion models",
      "paper_id": "2211.15029v2"
    },
    {
      "index": 20,
      "title": "Exploring Controllable Text Generation Techniques",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2005.01822",
      "authors": "S. Prabhumoye, A. W. Black, and R. Salakhutdinov",
      "orig_title": "Exploring controllable text generation techniques",
      "paper_id": "2005.01822v2"
    },
    {
      "index": 21,
      "title": "Plug and Play Language Models: a Simple Approach to Controlled Text Generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1912.02164",
      "authors": "S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu",
      "orig_title": "Plug and play language models: A simple approach to controlled text generation",
      "paper_id": "1912.02164v4"
    },
    {
      "index": 22,
      "title": "Compressing Deep Convolutional Networks using Vector Quantization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv:1412.6115",
      "authors": "Y. Gong, L. Liu, M. Yang, and L. Bourdev",
      "orig_title": "Compressing deep convolutional networks using vector quantization",
      "paper_id": "1412.6115v1"
    },
    {
      "index": 23,
      "title": "Vector quantization based approximate spectral clustering of large datasets",
      "abstract": "",
      "year": "2012",
      "venue": "Pattern Recognition",
      "authors": "K. TaÅŸdemir"
    },
    {
      "index": 24,
      "title": "Convergence of stochastic vector quantization and learning vector quantization with bregman divergences",
      "abstract": "",
      "year": "2020",
      "venue": "IFAC-PapersOnLine",
      "authors": "C. N. Mavridis and J. S. Baras"
    },
    {
      "index": 25,
      "title": "Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "S. Bond-Taylor, P. Hessey, H. Sasaki, T. P. Breckon, and C. G. Willcocks",
      "orig_title": "Unleashing transformers: parallel token prediction with discrete absorbing diffusion for fast high-resolution image generation from vector-quantized codes",
      "paper_id": "2111.12701v1"
    },
    {
      "index": 26,
      "title": "Vector quantized diffusion model with codeunet for text-to-sign pose sequences generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2208.09141",
      "authors": "P. Xie, Q. Zhang, Z. Li, H. Tang, Y. Du, and X. Hu"
    },
    {
      "index": 27,
      "title": "nuqmm: Quantized matmul for efficient inference of large-scale generative language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2206.09557",
      "authors": "G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee"
    },
    {
      "index": 28,
      "title": "Quantized neural networks: Training neural networks with low precision weights and activations",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio"
    },
    {
      "index": 29,
      "title": "Diffusion Models in Vision: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2209.04747",
      "authors": "F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah",
      "orig_title": "Diffusion models in vision: A survey",
      "paper_id": "2209.04747v6"
    },
    {
      "index": 30,
      "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "M. Courbariaux, Y. Bengio, and J.-P. David",
      "orig_title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
      "paper_id": "1511.00363v3"
    },
    {
      "index": 31,
      "title": "AdaBin: Improving Binary Neural Networks with Adaptive Binary Sets",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "Z. Tu, X. Chen, P. Ren, and Y. Wang",
      "orig_title": "AdaBin: improving binary neural networks with adaptive binary sets",
      "paper_id": "2208.08084v2"
    },
    {
      "index": 32,
      "title": "TernaryNet: Faster Deep Model Inference without GPUs for Medical 3D Segmentation using Sparse and Binary Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "International journal of computer assisted radiology and surgery",
      "authors": "M. P. Heinrich, M. Blendowski, and O. Oktay",
      "orig_title": "TernaryNet: faster deep model inference without gpus for medical 3D segmentation using sparse and binary convolutions",
      "paper_id": "1801.09449v1"
    },
    {
      "index": 33,
      "title": "Trq: Ternary neural networks with residual quantization",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Y. Li, W. Ding, C. Liu, B. Zhang, and G. Guo"
    },
    {
      "index": 34,
      "title": "U-Net Fixed-Point Quantization for Medical Image Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "Large-Scale Annotation of Biomedical Data and Expert Label Synthesis and Hardware Aware Learning for Medical Imaging and Computer Assisted Intervention",
      "authors": "M. AskariHemmat, S. Honari, L. Rouhier, C. S. Perone, J. Cohen-Adad, Y. Savaria, and J.-P. David",
      "orig_title": "U-Net fixed-point quantization for medical image segmentation",
      "paper_id": "1908.01073v2"
    },
    {
      "index": 35,
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Medical image computing and computer-assisted intervention",
      "authors": "O. Ronneberger, P. Fischer, and T. Brox",
      "orig_title": "U-net: Convolutional networks for biomedical image segmentation",
      "paper_id": "1505.04597v1"
    },
    {
      "index": 36,
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly",
      "orig_title": "Parameter-efficient transfer learning for nlp",
      "paper_id": "1902.00751v2"
    },
    {
      "index": 37,
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2104.08691",
      "authors": "B. Lester, R. Al-Rfou, and N. Constant",
      "orig_title": "The power of scale for parameter-efficient prompt tuning",
      "paper_id": "2104.08691v2"
    },
    {
      "index": 38,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 39,
      "title": "Vector-quantized Image Modeling with Improved VQGAN",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2110.04627",
      "authors": "J. Yu, X. Li, J. Y. Koh, H. Zhang, R. Pang, J. Qin, A. Ku, Y. Xu, J. Baldridge, and Y. Wu",
      "orig_title": "Vector-quantized image modeling with improved VQGAN",
      "paper_id": "2110.04627v3"
    },
    {
      "index": 40,
      "title": "Q8bert: Quantized 8bit bert",
      "abstract": "",
      "year": "2019",
      "venue": "2019 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)",
      "authors": "O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat"
    },
    {
      "index": 41,
      "title": "A comprehensive study on post-training quantization for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2303.08302",
      "authors": "Z. Yao, C. Li, X. Wu, S. Youn, and Y. He"
    },
    {
      "index": 42,
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2011.13456",
      "authors": "Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole",
      "orig_title": "Score-based generative modeling through stochastic differential equations",
      "paper_id": "2011.13456v2"
    },
    {
      "index": 43,
      "title": "The curious case of neural text degeneration",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1904.09751",
      "authors": "A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi"
    },
    {
      "index": 44,
      "title": "Minimum bayes-risk decoding for statistical machine translation",
      "abstract": "",
      "year": "2004",
      "venue": "JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP), Tech. Rep.",
      "authors": "S. Kumar and W. Byrne"
    },
    {
      "index": 45,
      "title": "Auto-encoding variational bayes",
      "abstract": "",
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings",
      "authors": "D. P. Kingma and M. Welling"
    },
    {
      "index": 46,
      "title": "Stochastic backpropagation and approximate inference in deep generative models",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "D. J. Rezende, S. Mohamed, and D. Wierstra"
    },
    {
      "index": 47,
      "title": "The E2E dataset: New challenges for end-to-end generation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv:1706.09254",
      "authors": "J. Novikova, O. DuÅ¡ek, and V. Rieser"
    },
    {
      "index": 48,
      "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen"
    },
    {
      "index": 49,
      "title": "Pointer Sentinel Mixture Models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "S. Merity, C. Xiong, J. Bradbury, and R. Socher",
      "orig_title": "Pointer sentinel mixture models",
      "paper_id": "1609.07843v1"
    },
    {
      "index": 50,
      "title": "Adaptive subgradient methods for online learning and stochastic optimization.",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of machine learning research",
      "authors": "J. Duchi, E. Hazan, and Y. Singer"
    },
    {
      "index": 51,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of machine learning research",
      "authors": "L. Van der Maaten and G. Hinton",
      "orig_title": "Visualizing data using t-sne.",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 52,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "Technical report",
      "authors": "A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al."
    }
  ]
}