{
  "paper_id": "2101.10292v3",
  "title": "Transferable Interactiveness Knowledge for Human-Object Interaction Detection",
  "sections": {
    "related works": "Visual Relationship Detection. Visual relationship detection  [ref]2 [ref]1 [ref]17 aims to detect the objects and classify their relationships simultaneously. In  [ref]2, Lu et al. proposed a relationship dataset VRD and an approach combined with language priors. Predicates within relationship triplet ⟨s​u​b​j​e​c​t,p​r​e​d​i​c​a​t​e,o​b​j​e​c​t⟩𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒𝑜𝑏𝑗𝑒𝑐𝑡\\langle subject,predicate,object\\rangle include actions, verbs, spatial and preposition vocabularies. Such vocabulary setting and severe long-tail issue within the dataset make this task quite difficult. Large-scale dataset Visual Genome [ref]1 is then proposed to promote the studies in this direction. Recent works      put attention on more effective and efficient visual feature extraction and try to exploit semantic information to refine the relationship detection. Human-Object Interaction Detection. Human-Object Interaction    is essential to understand human-centric interaction with objects. Several large-scale datasets, such as V-COCO , HICO-DET , HCVRD , HAKE  were proposed for the exploration of HOI detection.\nDifferent from HOI recognition      which is an image-level problem, HOI detection needs to detect interactive human-object pairs and classify their interactions at the instance-level.\nWith the assistance of DNNs and large datasets, recent methods have made significant progress         . Chao et al.  proposed a multi-stream model combining visual features, spatial locations to help tackle this problem.\nTo address the long-tail issue, Shen et al.  studied zero-shot learning and predicted the verb and object separately.\nIn InteractNet , an action-specific density map estimation method is introduced to locate interacted objects.\nIn , Qi et al. proposed GPNN incorporating DNN and graph model, which uses message parsing to iteratively update states and classifies all possible pairs/edges.\nGao et al.  exploited an instance-centric attention module to enhance the information from interest regions and facilitate HOI classification.\nPeyre et al.   learned a unified space combining visual and semantic language features and detected unseen interactions through entity analogy. Generally, these methods infer HOI in one-stage and may suffer from severe non-interactive pair domination problems. To address this issue, we utilize interactiveness to explicitly discriminate non-interactive pairs and suppress them before HOI classification. Part-based Action Recognition. Part-level human feature takes further insight into human-object interaction.\nBased on the whole person and part bounding boxes, Gkioxari et al.  developed a part-based model to make fine-grained action recognition.\nIn , Fang et al. proposed a pairwise body part attention model that can learn to focus on crucial parts and their correlations. An attention-based feature selection method and a pairwise parts representation learning scheme are introduced.\nIn our work, we utilize the body part features and whole body feature to learn hierarchical interactivenesses. And the unique consistency between the two levels is fully explored to guide the learning."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "IJCV",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei"
    },
    {
      "index": 1,
      "title": "Visual Relationship Detection with Language Priors",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Cewu Lu, Ranjay Krishna, Michael Bernstein, and Fei Fei Li",
      "orig_title": "Visual relationship detection with language priors",
      "paper_id": "1608.00187v1"
    },
    {
      "index": 2,
      "title": "Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, and Cewu Lu",
      "orig_title": "Weakly and semi-supervised human body part parsing via pose-guided knowledge transfer",
      "paper_id": "1805.04310v1"
    },
    {
      "index": 3,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 4,
      "title": "Beyond holistic object recognition: Enriching image understanding with part states",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Cewu Lu, Hao Su, Yonglu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, and Leonidas J Guibas"
    },
    {
      "index": 5,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 6,
      "title": "Activitynet: A large-scale video benchmark for human activity understanding",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles"
    },
    {
      "index": 7,
      "title": "A survey of robot learning from demonstration",
      "abstract": "",
      "year": "2009",
      "venue": "Robotics and autonomous systems",
      "authors": "Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning"
    },
    {
      "index": 8,
      "title": "Learning to detect human-object interactions",
      "abstract": "",
      "year": "2018",
      "venue": "WACV",
      "authors": "Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng"
    },
    {
      "index": 9,
      "title": "Detecting and recognizing human-object interactions",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He"
    },
    {
      "index": 10,
      "title": "Learning Human-Object Interactions by Graph Parsing Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu",
      "orig_title": "Learning human-object interactions by graph parsing neural networks",
      "paper_id": "1808.07962v1"
    },
    {
      "index": 11,
      "title": "iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.10437",
      "authors": "Chen Gao, Yuliang Zou, and Jia-Bin Huang",
      "orig_title": "ican: Instance-centric attention network for human-object interaction detection",
      "paper_id": "1808.10437v1"
    },
    {
      "index": 12,
      "title": "Visual Semantic Role Labeling",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1505.04474",
      "authors": "Saurabh Gupta and Jitendra Malik",
      "orig_title": "Visual semantic role labeling",
      "paper_id": "1505.04474v1"
    },
    {
      "index": 13,
      "title": "A framework for multiple-instance learning",
      "abstract": "",
      "year": "1998",
      "venue": "NIPS",
      "authors": "Oded Maron and Tomás Lozano-Pérez"
    },
    {
      "index": 14,
      "title": "PaStaNet: Toward Human Activity Knowledge Engine",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, and Cewu Lu",
      "orig_title": "PaStaNet: Toward Human Activity Knowledge Engine",
      "paper_id": "2004.00945v2"
    },
    {
      "index": 15,
      "title": "Recognition using visual phrases",
      "abstract": "",
      "year": "2012",
      "venue": "CVPR",
      "authors": "M. A. Sadeghi and A. Farhadi"
    },
    {
      "index": 16,
      "title": "Situation recognition: Visual semantic role labeling for image understanding",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "M. Yatskar, L. Zettlemoyer, and A. Farhadi"
    },
    {
      "index": 17,
      "title": "Scene Graph Generation by Iterative Message Passing",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei",
      "orig_title": "Scene graph generation by iterative message passing",
      "paper_id": "1701.02426v2"
    },
    {
      "index": 18,
      "title": "Visual Translation Embedding Network for Visual Relation Detection",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua",
      "orig_title": "Visual translation embedding network for visual relation detection",
      "paper_id": "1702.08319v1"
    },
    {
      "index": 19,
      "title": "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.04979",
      "authors": "G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, J. Shao, and C. C. Loy",
      "orig_title": "Zoom-net: Mining deep feature interactions for visual relationship recognition",
      "paper_id": "1807.04979v1"
    },
    {
      "index": 20,
      "title": "Graph R-CNN for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "L. Yang, L. Lu, S. Lee. D. Batra and D. Parikh",
      "orig_title": "Graph r-cnn for scene graph generation",
      "paper_id": "1808.00191v1"
    },
    {
      "index": 21,
      "title": "Unsupervised discovery of action classes",
      "abstract": "",
      "year": "2006",
      "venue": "CVPR",
      "authors": "Y. Wang, H. Jiang, Mark. S. Drew, Z.-N. Li, and G. Mori"
    },
    {
      "index": 22,
      "title": "Recognizing human actions from still images with latent poses",
      "abstract": "",
      "year": "2010",
      "venue": "CVPR",
      "authors": "W. Yang, Y. Wang, and G. Mori"
    },
    {
      "index": 23,
      "title": "Recognizing actions from still images",
      "abstract": "",
      "year": "2008",
      "venue": "ICPR",
      "authors": "N. Ikizler, R. G. Cinbis, S. Pehlivan, and P. Duygulu"
    },
    {
      "index": 24,
      "title": "Care about you: towards large-scale human-centric visual relationship detection",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.09892",
      "authors": "B. Zhuang, Q. Wu, C. Shen, I. Reid, and A. v. d. Hengel",
      "orig_title": "Care about you: towards large-scale human-centric visual relationship detection",
      "paper_id": "1705.09892v1"
    },
    {
      "index": 25,
      "title": "Pairwise Body-Part Attention for Recognizing Human-Object Interactions",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "H.-S. Fang, J. Cao, Y.-W. Tai, and C. Lu",
      "orig_title": "Pairwise body-part attention for recognizing human-object interactions",
      "paper_id": "1807.10889v1"
    },
    {
      "index": 26,
      "title": "Recognizing human actions in still images: a study of bag-of-features and part-based representations",
      "abstract": "",
      "year": "2010",
      "venue": "BMVC",
      "authors": "V. Delaitre, I. Laptev, and J. Sivic"
    },
    {
      "index": 27,
      "title": "Hico: A benchmark for recognizing human-object interactions in images",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Y. W. Chao, Z. Wang, Y. He, J. Wang, and J. Deng"
    },
    {
      "index": 28,
      "title": "Predicting the location of “interactees” in novel human-object interactions",
      "abstract": "",
      "year": "2014",
      "venue": "ACCV",
      "authors": "C.-Y. Chen and K. Grauman"
    },
    {
      "index": 29,
      "title": "Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "A. Mallya and S. Lazebnik",
      "orig_title": "Learning models for actions and person-object interactions with transfer to question answering",
      "paper_id": "1604.04808v2"
    },
    {
      "index": 30,
      "title": "Detailed 2D-3D Joint Representation for Human-Object Interaction",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, and Cewu Lu",
      "orig_title": "Detailed 2D-3D Joint Representation for Human-Object Interaction",
      "paper_id": "2004.08154v2"
    },
    {
      "index": 31,
      "title": "Detecting Human-Object Interactions with Action Co-occurrence Priors",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "D.-J. Kim, X. Sun, J. Choi, S. Lin, and I. S. Kweon",
      "orig_title": "Detecting human-object interactions with action co-occurrence priors",
      "paper_id": "2007.08728v2"
    },
    {
      "index": 32,
      "title": "Visual Compositional Learning for Human-Object Interaction Detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Z. Hou, X. Peng, Y. Qiao, and D. Tao",
      "orig_title": "Visual compositional learning for human-object interaction detection",
      "paper_id": "2007.12407v2"
    },
    {
      "index": 33,
      "title": "Polysemy deciphering network for human-object interaction detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "X. Zhong, C. Ding, X. Qu, and D. Tao"
    },
    {
      "index": 34,
      "title": "Contextual heterogeneous graph network for human-object interaction detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "H. Wang, W.-s. Zheng, and L. Yingbiao"
    },
    {
      "index": 35,
      "title": "UnionDet: Union-Level Detector Towards Real-Time Human-Object Interaction Detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "B. Kim, T. Choi, J. Kang, and H. J. Kim",
      "orig_title": "Uniondet: Union-level detector towards real-time human-object interaction detection",
      "paper_id": "2312.12664v1"
    },
    {
      "index": 36,
      "title": "DRG: Dual Relation Graph for Human-Object Interaction Detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "C. Gao, J. Xu, Y. Zou, and J.-B. Huang",
      "orig_title": "Drg: Dual relation graph for human-object interaction detection",
      "paper_id": "2008.11714v1"
    },
    {
      "index": 37,
      "title": "Amplifying key cues for human-object-interaction detection",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Y. Liu, Q. Chen, and A. Zisserman"
    },
    {
      "index": 38,
      "title": "HOI Analysis: Integrating and Decomposing Human-Object Interaction",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Y.-L. Li, X. Liu, X. Wu, Y. Li, and C. Lu",
      "orig_title": "Hoi analysis: Integrating and decomposing human-object interaction",
      "paper_id": "2010.16219v2"
    },
    {
      "index": 39,
      "title": "Scaling human-object interaction recognition through zero-shot learning",
      "abstract": "",
      "year": "2018",
      "venue": "WACV",
      "authors": "Liyue Shen, Serena Yeung, Judy Hoffman, Greg Mori, and Li Fei Fei"
    },
    {
      "index": 40,
      "title": "Detecting rare visual relations using analogies",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Julia Peyre, Ivan Laptev, Cordelia Schmid, and Josef Sivic"
    },
    {
      "index": 41,
      "title": "Actions and Attributes from Wholes and Parts",
      "abstract": "",
      "year": "2014",
      "venue": "ICCV",
      "authors": "G. Gkioxari, R. Girshick, and J. Malik",
      "orig_title": "DActions and attributes from wholes and parts",
      "paper_id": "1412.2604v2"
    },
    {
      "index": 42,
      "title": "Detectron",
      "abstract": "",
      "year": "2018",
      "venue": "https://github.com/facebookresearch/detectron",
      "authors": "R. Girshick, I. Radosavovic, G. Gkioxari, P. Dollár, and K. He"
    },
    {
      "index": 43,
      "title": "Feature Pyramid Networks for Object Detection",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "T.-Y. Lin, P. Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie",
      "orig_title": "Feature pyramid networks for object detection",
      "paper_id": "1612.03144v2"
    },
    {
      "index": 44,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 45,
      "title": "RMPE: Regional multi-person pose estimation",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu"
    },
    {
      "index": 46,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 47,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 48,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.02907",
      "authors": "T. N. Kipf and M. Welling",
      "orig_title": "Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    }
  ]
}