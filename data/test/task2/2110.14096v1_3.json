{
  "paper_id": "2110.14096v1",
  "title": "Towards Robust Bisimulation Metric Learning",
  "sections": {
    "preliminaries": "In this work, we consider a discounted Markov Decision Process (MDP) given by a tuple, âŸ¨ğ’®,ğ’œ,ğ’«,R,Ï0âŸ©ğ’®ğ’œğ’«ğ‘…subscriptğœŒ0\\langle\\mathcal{S},\\mathcal{A},\\mathcal{P},R,\\rho_{0}\\rangle. At the beginning of each episode, an initial state, ğ¬0âˆˆğ’®subscriptğ¬0ğ’®\\mathbf{s}_{0}\\in\\mathcal{S}, is sampled from the initial-state distribution Ï0subscriptğœŒ0\\rho_{0} over the state space ğ’®ğ’®\\mathcal{S}. Then, at each discrete time-step tâ‰¥0ğ‘¡0t\\geq 0, an agent takes an action, ğštâˆˆğ’œsubscriptğšğ‘¡ğ’œ\\mathbf{a}_{t}\\in\\mathcal{A}, according to a policy Ï€â€‹(ğšt|ğ¬t)ğœ‹conditionalsubscriptğšğ‘¡subscriptğ¬ğ‘¡\\pi(\\mathbf{a}_{t}|\\mathbf{s}_{t}). As a result, the MDP transitions to the next state according to a transition distribution ğ’«â€‹(ğ¬t+1|ğ¬t,ğšt)ğ’«conditionalsubscriptğ¬ğ‘¡1subscriptğ¬ğ‘¡subscriptğšğ‘¡\\mathcal{P}(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t}). The agent collects a scalar reward, rt=Râ€‹(ğ¬t,ğšt)subscriptğ‘Ÿğ‘¡ğ‘…subscriptğ¬ğ‘¡subscriptğšğ‘¡r_{t}=R(\\mathbf{s}_{t},\\mathbf{a}_{t}), from the environment according to a bounded reward function, R:ğ’®Ã—ğ’œâ†’[Rmin,Rmax]:ğ‘…â†’ğ’®ğ’œsubscriptğ‘…minsubscriptğ‘…maxR:\\mathcal{S}\\times\\mathcal{A}\\rightarrow[R_{\\mathrm{min}},R_{\\mathrm{max}}]. In infinite- and long-horizon settings, a discount factor, Î³âˆˆ[0,1)ğ›¾01\\gamma\\in[0,1), is used to calculate the agentâ€™s discounted return in a given episode, G=âˆ‘tâ‰¥0Î³tâ€‹rtğºsubscriptğ‘¡0superscriptğ›¾ğ‘¡subscriptğ‘Ÿğ‘¡G=\\sum_{t\\geq 0}\\gamma^{t}r_{t}. RL algorithms aim to find an optimal policy, Ï€âˆ—â‰”argâ¡maxÏ€âˆˆÎ â¡ğ”¼â€‹[G]â‰”superscriptğœ‹subscriptğœ‹Î ğ”¼delimited-[]ğº\\pi^{*}\\coloneqq\\operatorname*{\\arg\\!\\max}_{\\pi\\in\\Pi}\\mathbb{E}[G], for a class of stationary policies Î Î \\Pi. In high-dimensional, continuous state (or observation) spaces, this learning problem is rendered tractable via a state encoder (e.g., a neural network), Ï•:ğ’®â†’â„n:italic-Ï•â†’ğ’®superscriptâ„ğ‘›\\phi:\\mathcal{S}\\rightarrow\\mathbb{R}^{n},\nwhich is used to\nlearn a policy of the form Ï€â€‹(ğš|Ï•â€‹(ğ¬))ğœ‹conditionalğšitalic-Ï•ğ¬\\pi(\\mathbf{a}|\\phi(\\mathbf{s})). The following (pseudo222Pseudo-metrics are a generalization of metrics that allow distinct points to have zero distance. For simplicity, we broadly use the term â€metricâ€ in the remainder of this paper at the cost of imprecision.)-metric, based on the Wasserstein metric (see Appendix A for a review), is of particular relevance to this work. Distances are assigned to state pairs, (ğ¬i,ğ¬j)âˆˆğ’®Ã—ğ’®subscriptğ¬ğ‘–subscriptğ¬ğ‘—ğ’®ğ’®(\\mathbf{s}_{i},\\mathbf{s}_{j})\\in\\mathcal{S}\\times\\mathcal{S}, according to a pessimistic measure  of how much the rewards collected in each state and the respective transition distributions differ. A distance of zero for a pair implies state aggregation, or bisimilarity. The following metric exists and is unique, given R:ğ’®Ã—ğ’œâ†’[ref]0 [ref]1:ğ‘…â†’ğ’®ğ’œ01R:\\mathcal{S}\\times\\mathcal{A}\\rightarrow[ref]0 [ref]1 and câˆˆ(0,1)ğ‘01c\\in(0,1) for continuous MDPs: An earlier version of this metric for finite MDPs used separate weighting constants cR,cTâ‰¥0subscriptğ‘ğ‘…subscriptğ‘ğ‘‡0c_{R},c_{T}\\geq 0 for the first and second terms respectively, and required that cR+cTâ‰¤1subscriptğ‘ğ‘…subscriptğ‘ğ‘‡1c_{R}+c_{T}\\leq 1 . Here, when the weighting constant cTsubscriptğ‘ğ‘‡c_{T} of the W1â€‹(d)subscriptğ‘Š1ğ‘‘W_{1}(d) term is in [0,1)01[0,1), the RHS is a contraction mapping, â„±â€‹(d):ğ”ªâ€‹ğ”¢â€‹ğ”±â†’ğ”ªâ€‹ğ”¢â€‹ğ”±:â„±ğ‘‘â†’ğ”ªğ”¢ğ”±ğ”ªğ”¢ğ”±\\mathcal{F}(d):\\mathfrak{met}\\rightarrow\\mathfrak{met}, in the space of metrics. Then, the Banach fixed-point theorem can be applied to ensure the existence of a unique metric, which also ensures convergence via fixed-point iteration for finite MDPs. For more details, we refer the reader to   and the proof of Remark 1 in Appendix B.\nNotice that cğ‘c (or cTsubscriptğ‘ğ‘‡c_{T}) determines a timescale for the bisimulation metric, weighting the importance of current versus future rewards, analogously to the discount factor Î³ğ›¾\\gamma.\nMore recently, an on-policy bisimulation metric (also called Ï€ğœ‹\\pi-bisimulation) was proposed to circumvent the intractibility introduced by taking the max\\max operation over high-dimensional action spaces (e.g., continuous control), as well as the inherent pessimism of the policy-independent form . Given a fixed policy Ï€ğœ‹\\pi, the following on-policy bisimulation metric exists and is unique: where riÏ€â‰”ğ”¼ğšâˆ¼Ï€â€‹[Râ€‹(ğ¬i,ğš)]â‰”superscriptsubscriptğ‘Ÿğ‘–ğœ‹subscriptğ”¼similar-toğšğœ‹delimited-[]ğ‘…subscriptğ¬ğ‘–ğšr_{i}^{\\pi}\\coloneqq\\mathbb{E}_{\\mathbf{a}\\sim\\pi}[R(\\mathbf{s}_{i},\\mathbf{a})] and ğ’«Ï€(â‹…|ğ¬i)â‰”ğ”¼ğšâˆ¼Ï€[ğ’«(â‹…|ğ¬i,ğš)]\\mathcal{P}^{\\pi}(\\cdot|\\mathbf{s}_{i})\\coloneqq\\mathbb{E}_{\\mathbf{a}\\sim\\pi}[\\mathcal{P}(\\cdot|\\mathbf{s}_{i},\\mathbf{a})]. Zhang et al. [ref]53 proposed to learn a similar on-policy bisimulation metric directly in the embedding space via an MSE objective. They proposed an algorithm for jointly learning a policy Ï€â€‹(ğš|Ï•â€‹(ğ¬))ğœ‹conditionalğšitalic-Ï•ğ¬\\pi(\\mathbf{a}|\\phi(\\mathbf{s})) with an on-policy bisimulation metric. Below, we define a generalized variant of their objective: where d^Ï€,Ï•â€‹(ğ¬i,ğ¬j)â‰”âˆ¥Ï•â€‹(ğ¬i)âˆ’Ï•â€‹(ğ¬j)âˆ¥q2â‰”subscript^ğ‘‘ğœ‹italic-Ï•subscriptğ¬ğ‘–subscriptğ¬ğ‘—subscriptdelimited-âˆ¥âˆ¥italic-Ï•subscriptğ¬ğ‘–italic-Ï•subscriptğ¬ğ‘—subscriptğ‘2\\widehat{d}_{\\pi,\\phi}(\\mathbf{s}_{i},\\mathbf{s}_{j})\\coloneqq\\lVert\\phi(\\mathbf{s}_{i})-\\phi(\\mathbf{s}_{j})\\rVert_{q_{2}} and they used (q1=2,q2=1)formulae-sequencesubscriptğ‘12subscriptğ‘21(q_{1}=2,q_{2}=1). Notice that the recursion induced by this objective is different from prior metrics in three ways; (i) a W2subscriptğ‘Š2W_{2} metric was used instead of a W1subscriptğ‘Š1W_{1} metric since W2subscriptğ‘Š2W_{2} has a convenient closed form for Gaussian distributions when q1=2subscriptğ‘12q_{1}=2, (ii) the distance function used for the bisimulation metric and the Wasserstein metric are different (L1subscriptğ¿1L_{1} and L2subscriptğ¿2L_{2} respectively), and (iii) a forward dynamics model is used instead of the ground truth dynamics. While they may introduce practical benefits, these differences violate the conditions under which the existence of a unique bisimulation metric has been proven    . Thus, we will (i) assume a unique metric exists for all Wasserstein metrics Wpsubscriptğ‘Šğ‘W_{p} when necessary (see Assumption 1), (ii) study losses that use a matching metric, i.e., q1=q2=qsubscriptğ‘1subscriptğ‘2ğ‘q_{1}=q_{2}=q, and (iii) introduce a constraint on forward models in Sec. 3.2.2. Next, we note that they recommend the use of stop gradients for the W2subscriptğ‘Š2W_{2} term. The resulting gradient updates may be considered as approximate fixed-point iteration in the space of metrics: where Î±nsubscriptğ›¼ğ‘›\\alpha_{n} is a learning rate. However, in practice, Ï€ğœ‹\\pi and ğ’«^^ğ’«\\widehat{\\mathcal{P}} may also be updated in training, which is of particular relevance when they have form Ï€â€‹(ğš|Ï•â€‹(ğ¬))ğœ‹conditionalğšitalic-Ï•ğ¬\\pi(\\mathbf{a}|\\phi(\\mathbf{s})) and ğ’«^â€‹(Ï•â€‹(ğ¬â€²)|Ï•â€‹(ğ¬),ğš)^ğ’«conditionalitalic-Ï•superscriptğ¬â€²italic-Ï•ğ¬ğš\\widehat{\\mathcal{P}}(\\phi(\\mathbf{s}^{\\prime})|\\phi(\\mathbf{s}),\\mathbf{a}) as in [ref]53. In the next section, we will discuss conditions under which joint updates to a policy Ï€ğœ‹\\pi and a metric defined by state encodings Ï•italic-Ï•\\phi may or may not converge in practical settings: where dÏ€âˆ—subscriptğ‘‘superscriptğœ‹d_{\\pi^{*}} is the on-policy bisimulation metric for the optimal policy Ï€âˆ—superscriptğœ‹\\pi^{*}."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.01732",
      "authors": "Joshua Achiam and Shankar Sastry",
      "orig_title": "Surprise-based intrinsic motivation for deep reinforcement learning",
      "paper_id": "1703.01732v1"
    },
    {
      "index": 1,
      "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Rishabh Agarwal, MarlosÂ C. Machado, PabloÂ Samuel Castro, and MarcÂ G. Bellemare",
      "orig_title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning",
      "paper_id": "2101.05265v2"
    },
    {
      "index": 2,
      "title": "Learning to poke by poking: experiential learning of intuitive physics",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.07419",
      "authors": "Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine"
    },
    {
      "index": 3,
      "title": "A survey on intrinsic motivation in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.06976",
      "authors": "Arthur Aubret, Laetitia Matignon, and Salima Hassas"
    },
    {
      "index": 4,
      "title": "Unifying count-based exploration and intrinsic motivation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01868",
      "authors": "MarcÂ G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos"
    },
    {
      "index": 5,
      "title": "Online Abstraction with MDP Homomorphisms for Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "18th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Ondrej Biza and Robert Platt",
      "orig_title": "Online abstraction with MDP homomorphisms for deep learning",
      "paper_id": "1811.12929v2"
    },
    {
      "index": 6,
      "title": "OpenAI gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 7,
      "title": "Large-Scale Study of Curiosity-Driven Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.04355",
      "authors": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and AlexeiÂ A Efros",
      "orig_title": "Large-scale study of curiosity-driven learning",
      "paper_id": "1808.04355v1"
    },
    {
      "index": 8,
      "title": "Using bisimulation for policy transfer in MDPs",
      "abstract": "",
      "year": "2010",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Pablo Castro and Doina Precup"
    },
    {
      "index": 9,
      "title": "Scalable methods for computing state similarity in deterministic Markov Decision Processes",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "PabloÂ Samuel Castro",
      "orig_title": "Scalable methods for computing state similarity in deterministic Markov decision processes",
      "paper_id": "1911.09291v1"
    },
    {
      "index": 10,
      "title": "An elementary proof of the triangle inequality for the Wasserstein metric",
      "abstract": "",
      "year": "2008",
      "venue": "American Mathematical Society",
      "authors": "Philippe Clement and Wolfgang Desch"
    },
    {
      "index": 11,
      "title": "Efficient Modelâ€“Based Deep Reinforcement Learning with Variational State Tabulation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Dane Corneil, Wulfram Gerstner, and Johanni Brea",
      "orig_title": "Efficient model-based deep reinforcement learning with variational state tabulation",
      "paper_id": "1802.04325v2"
    },
    {
      "index": 12,
      "title": "Metrics for finite Markov decision processes",
      "abstract": "",
      "year": "2004",
      "venue": "20th Conference on Uncertainty in Artificial Intelligence, UAI â€™04",
      "authors": "Norm Ferns, Prakash Panangaden, and Doina Precup"
    },
    {
      "index": 13,
      "title": "Bisimulation metrics for continuous Markov decision processes",
      "abstract": "",
      "year": "2011",
      "venue": "SIAM J. Comput.",
      "authors": "Norm Ferns, Prakash Panangaden, and Doina Precup"
    },
    {
      "index": 14,
      "title": "Bisimulation metrics are optimal value functions",
      "abstract": "",
      "year": "2014",
      "venue": "Thirtieth Conference on Uncertainty in Artificial Intelligence, UAIâ€™14",
      "authors": "Norm Ferns and Doina Precup"
    },
    {
      "index": 15,
      "title": "Combined Reinforcement Learning via Abstract Representations",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Vincent FranÃ§ois-Lavet, Yoshua Bengio, Doina Precup, and Joelle Pineau",
      "orig_title": "Combined reinforcement learning via abstract representations",
      "paper_id": "1809.04506v2"
    },
    {
      "index": 16,
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and MarcÂ G Bellemare",
      "orig_title": "DeepMDP: Learning continuous latent space models for representation learning",
      "paper_id": "1906.02736v1"
    },
    {
      "index": 17,
      "title": "Learning Actionable Representations with Goal-Conditioned Policies",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.07819",
      "authors": "Dibya Ghosh, Abhishek Gupta, and Sergey Levine",
      "orig_title": "Learning actionable representations with goal-conditioned policies",
      "paper_id": "1811.07819v2"
    },
    {
      "index": 18,
      "title": "Equivalence notions and model minimization in markov decision processes",
      "abstract": "",
      "year": "2003",
      "venue": "Artificial Intelligence",
      "authors": "Robert Givan, Thomas Dean, and Matthew Greig"
    },
    {
      "index": 19,
      "title": "Variational Intrinsic Control",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.07507",
      "authors": "Karol Gregor, DaniloÂ Jimenez Rezende, and Daan Wierstra",
      "orig_title": "Variational intrinsic control",
      "paper_id": "1611.07507v1"
    },
    {
      "index": 20,
      "title": "The Value Equivalence Principle for Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Christopher Grimm, Andre Barreto, Satinder Singh, and David Silver",
      "orig_title": "The value equivalence principle for model-based reinforcement learning",
      "paper_id": "2011.03506v1"
    },
    {
      "index": 21,
      "title": "World Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.10122",
      "authors": "David Ha and JÃ¼rgen Schmidhuber",
      "orig_title": "World models",
      "paper_id": "1803.10122v4"
    },
    {
      "index": 22,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 23,
      "title": "Soft Actor-Critic Algorithms and Applications",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.05905",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, etÂ al.",
      "orig_title": "Soft actor-critic algorithms and applications",
      "paper_id": "1812.05905v2"
    },
    {
      "index": 24,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 25,
      "title": "In defense of the triplet loss for person re-identification",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.07737",
      "authors": "Alexander Hermans, Lucas Beyer, and Bastian Leibe"
    },
    {
      "index": 26,
      "title": "Learning Plannable Representations with Causal InfoGAN",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.09341",
      "authors": "Thanard Kurutach, Aviv Tamar, GeÂ Yang, Stuart Russell, and Pieter Abbeel",
      "orig_title": "Learning plannable representations with causal InfoGAN",
      "paper_id": "1807.09341v1"
    },
    {
      "index": 27,
      "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Michael Laskin, Aravind Srinivas, and Pieter Abbeel",
      "orig_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
      "paper_id": "2004.04136v4"
    },
    {
      "index": 28,
      "title": "Metrics and continuity in reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Charline LeÂ Lan, MarcÂ G Bellemare, and PabloÂ Samuel Castro",
      "orig_title": "Metrics and continuity in reinforcement learning",
      "paper_id": "2102.01514v1"
    },
    {
      "index": 29,
      "title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.00953",
      "authors": "AlexÂ X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine"
    },
    {
      "index": 30,
      "title": "Towards a unified theory of state abstraction for MDPs",
      "abstract": "",
      "year": "2006",
      "venue": "Ninth International Symposium on Artificial Intelligence and Mathematics",
      "authors": "Lihong Li, ThomasÂ J. Walsh, and MichaelÂ L. Littman"
    },
    {
      "index": 31,
      "title": "Efficient memory-based learning for robot control",
      "abstract": "",
      "year": "1990",
      "venue": "",
      "authors": "AndrewÂ William Moore"
    },
    {
      "index": 32,
      "title": "Visual Reinforcement Learning with Imagined Goals",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.04742",
      "authors": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine",
      "orig_title": "Visual reinforcement learning with imagined goals",
      "paper_id": "1807.04742v2"
    },
    {
      "index": 33,
      "title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.14535",
      "authors": "Masashi Okada and Tadahiro Taniguchi",
      "orig_title": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction",
      "paper_id": "2007.14535v2"
    },
    {
      "index": 34,
      "title": "The distance between two random vectors with given dispersion matrices",
      "abstract": "",
      "year": "1982",
      "venue": "Linear Algebra and its Applications",
      "authors": "Ingram Olkin and Friedrich Pukelsheim"
    },
    {
      "index": 35,
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Deepak Pathak, Pulkit Agrawal, AlexeiÂ A Efros, and Trevor Darrell",
      "orig_title": "Curiosity-driven exploration by self-supervised prediction",
      "paper_id": "1705.05363v1"
    },
    {
      "index": 36,
      "title": "Biasing approximate dynamic programming with a lower discount factor",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Marek Petrik and Bruno Scherrer"
    },
    {
      "index": 37,
      "title": "An algebraic approach to abstraction in reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "Balaraman Ravindran"
    },
    {
      "index": 38,
      "title": "Approximate homomorphisms: A framework for non-exact minimization in Markov decision processes",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "Balaraman Ravindran and AndrewÂ G Barto"
    },
    {
      "index": 39,
      "title": "Empowermentâ€“an introduction",
      "abstract": "",
      "year": "2014",
      "venue": "Guided Self-Organization: Inception",
      "authors": "Christoph Salge, Cornelius Glackin, and Daniel Polani"
    },
    {
      "index": 40,
      "title": "Optimal transport for applied mathematicians",
      "abstract": "",
      "year": "2015",
      "venue": "BirkÃ¤user, NY",
      "authors": "Filippo Santambrogio"
    },
    {
      "index": 41,
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Florian Schroff, Dmitry Kalenichenko, and James Philbin",
      "orig_title": "FaceNet: A unified embedding for face recognition and clustering",
      "paper_id": "1503.03832v3"
    },
    {
      "index": 42,
      "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1507.00814",
      "authors": "BradlyÂ C Stadie, Sergey Levine, and Pieter Abbeel",
      "orig_title": "Incentivizing exploration in reinforcement learning with deep predictive models",
      "paper_id": "1507.00814v3"
    },
    {
      "index": 43,
      "title": "Decoupling Representation Learning from Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.08319",
      "authors": "Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin",
      "orig_title": "Decoupling representation learning from reinforcement learning",
      "paper_id": "2009.08319v3"
    },
    {
      "index": 44,
      "title": "DeepMind Control Suite",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.00690",
      "authors": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego deÂ Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, etÂ al.",
      "orig_title": "DeepMind control suite",
      "paper_id": "1801.00690v1"
    },
    {
      "index": 45,
      "title": "Bounding performance loss in approximate MDP homomorphisms",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jonathan Taylor, Doina Precup, and Prakash Panagaden"
    },
    {
      "index": 46,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Emanuel Todorov, Tom Erez, and Yuval Tassa"
    },
    {
      "index": 47,
      "title": "Plannable Approximations to MDP Homomorphisms: Equivariance under Actions",
      "abstract": "",
      "year": "2020",
      "venue": "19th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Elise vanÂ der Pol, Thomas Kipf, FransÂ A Oliehoek, and Max Welling",
      "orig_title": "Plannable approximations to MDP homomorphisms: Equivariance under actions",
      "paper_id": "2002.11963v1"
    },
    {
      "index": 48,
      "title": "MDP homomorphic networks: Group symmetries in reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Elise vanÂ der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling"
    },
    {
      "index": 49,
      "title": "Optimal transport: old and new, volume 338",
      "abstract": "",
      "year": "2008",
      "venue": "Springer Science & Business Media",
      "authors": "CÃ©dric Villani"
    },
    {
      "index": 50,
      "title": "Embed to control: a locally linear latent dynamics model for control from raw images",
      "abstract": "",
      "year": "2015",
      "venue": "28th International Conference on Neural Information Processing Systems",
      "authors": "Manuel Watter, JostÂ Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller"
    },
    {
      "index": 51,
      "title": "Sampling Matters in Deep Embedding Learning",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "authors": "Chao-Yuan Wu, R.Â Manmatha, AlexanderÂ J. Smola, and Philipp KrÃ¤henbÃ¼hl",
      "orig_title": "Sampling matters in deep embedding learning",
      "paper_id": "1706.07567v2"
    },
    {
      "index": 52,
      "title": "Invariant representations for reinforcement learning without reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Amy Zhang, RowanÂ Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine"
    },
    {
      "index": 53,
      "title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine",
      "orig_title": "Solar: Deep structured representations for model-based reinforcement learning",
      "paper_id": "1808.09105v4"
    },
    {
      "index": 54,
      "title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "BrianÂ D Ziebart"
    }
  ]
}