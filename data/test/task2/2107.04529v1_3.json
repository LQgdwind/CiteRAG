{
  "paper_id": "2107.04529v1",
  "title": "Entropy, Information, and the Updating of Probabilities",
  "sections": {
    "what is information?": "The term ‚Äòinformation‚Äô is used with a wide variety of different meanings\n. There is the\nShannon notion of information that is meant to measure an amount of\ninformation and is quite divorced from semantics. There is also an algorithmic\nnotion of information that captures a notion of complexity and originates in\nthe work of Solomonov, Kolmogorov, and Chaitin , and\nthere is a related notion of entropy as a Minimum Description Length\n. Furthermore, in the general context of the\nthermodynamics of computation it is said that ‚Äúinformation is\nphysical‚Äù¬†because systems ‚Äúcarry‚Äù¬†or ‚Äúcontain‚Äù¬†information about their own physical state¬† (see also 3). Here we follow a different path . We\nseek an epistemic notion of information that is closer to the everyday\ncolloquial use of the term ‚Äî roughly, information is what we request when we\nask a question. In a Bayesian framework this requires an explicit account of\nthe relation between information and the beliefs of ideally rational agents\n4. It is implicit in the recognition that most of our beliefs are held on the\nbasis of incomplete information that not all probability assignments are\nequally good; some beliefs are preferable to others in the very pragmatic\nsense that they enhance our chances to successfully navigate this world. Thus\na theory of probability demands a theory of updating probabilities in order to\nimprove our beliefs. We are now ready to address the question: What, after all, is ‚Äòinformation‚Äô?\nThe answer is pragmatic. Information is what information does.\nInformation is defined by its effects: (a) it restricts our options as to what\nwe are honestly and rationally allowed to believe; and (b) it induces us to\nupdate from prior beliefs to posterior beliefs. This, I propose, is a defining\ncharacteristic of information: Information is that which induces a change from one state of\nrational belief to another. One aspect of this notion is that for a rational agent, the\nidentification of what constitutes information ‚Äî as opposed to mere noise\n‚Äî already involves a judgement, an evaluation. Another aspect is that the\nnotion that information is directly related to changing our minds does not\ninvolve any talk about amounts of information but, it nevertheless\nallows precise quantitative calculations. Indeed, constraints on the\nacceptable posterior probabilities are precisely the kind of information the\nmethod of maximum entropy is designed to handle. In short, Information constrains probability distributions. The\nconstraints are the information. To the extent that the probabilities are Bayesian, this definition\ncaptures the Bayesian notion that information is directly related to changing\nour minds; that it is the driving force behind the process of learning. It\nalso incorporates an important feature of rationality: being rational means\naccepting that ‚Äúnot everything goes‚Äù, that\nour beliefs must be constrained in very specific ways. But the indiscriminate\nacceptance of any arbitrary constraint does not qualify as rational behavior.\nTo be rational an agent must exercise some judgement before accepting a\nparticular piece of information as a reliable basis for the revision of its\nbeliefs which raises questions about what judgements might be considered\nsound. Furthermore, there is no implication that the information must be\ntrue; only that we accept it as true. False information is information\ntoo, at least to the extent that we are prepared to accept it and allow it to\naffect our beliefs. The paramount virtue of the definition above is that it is useful; it allows\nprecise quantitative calculations. The constraints that constitute information\ncan take a wide variety of forms. They can be expressed in terms of expected\nvalues, or they can specify the functional form of a distribution, or be\nimposed through various geometrical relations. Examples are given in Section 5\nand in 5. Concerning the act of updating it may be worthwhile to point out an analogy\nwith dynamics. In Newtonian mechanics the state of motion of a system is\ndescribed in terms of momentum and the change from one state to another is\nsaid to be ‚Äúcaused‚Äù¬†by an applied force or\nimpulse. Bayesian inference is analogous in that a state of belief is\ndescribed in terms of probabilities and the change from one state to another\nis ‚Äúcaused‚Äù¬†by information. Just as a force\nis that which induces a change from one state of motion to another, so\ninformation is that which induces a change from one state of belief to\nanother. Updating is a form of dynamics. In 6 the analogy\nis taken seriously: the logic is reversed and quantum mechanics is derived as\nan example of the entropic updating of probabilities."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Relative Entropy and Inductive Inference",
      "abstract": "",
      "year": "2004",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 1,
      "title": "Updating Probabilities",
      "abstract": "",
      "year": "2006",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha and A. Giffin"
    },
    {
      "index": 2,
      "title": "Information and Entropy",
      "abstract": "",
      "year": "2007",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 3,
      "title": "Towards an Informational Pragmatic Realism",
      "abstract": "",
      "year": "2014",
      "venue": "Mind and Machines",
      "authors": "A. Caticha"
    },
    {
      "index": 4,
      "title": "Entropic Updating of Probabilities and Density Matrices",
      "abstract": "",
      "year": "2017",
      "venue": "Entropy",
      "authors": "K. Vanslette"
    },
    {
      "index": 5,
      "title": "Lectures on Probability, Entropy, and Statistical Physics",
      "abstract": "",
      "year": "2008",
      "venue": "MaxEnt 2008, S√£o Paulo, Brazil",
      "authors": "A. Caticha"
    },
    {
      "index": 6,
      "title": "Entropic Inference and the Foundations of Physics",
      "abstract": "",
      "year": "2012",
      "venue": "EBEB 2012, S√£o Paulo, Brazil",
      "authors": "A. Caticha"
    },
    {
      "index": 7,
      "title": "Entropic Physics: Probability, Entropy, and the Foundations of Physics",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "A. Caticha"
    },
    {
      "index": 8,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "1957",
      "venue": "Phys. Rev.",
      "authors": "E. T. Jaynes",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 9,
      "title": "Papers on Probability, Statistics and Statistical Physics",
      "abstract": "",
      "year": "1983",
      "venue": "",
      "authors": "E. T. Jaynes"
    },
    {
      "index": 10,
      "title": "Can the Maximum Entropy Principle be explained as a consistency requirement?",
      "abstract": "",
      "year": "1995",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "J. Uffink"
    },
    {
      "index": 11,
      "title": "On measures of entropy and information",
      "abstract": "",
      "year": "1961",
      "venue": "Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability",
      "authors": "A. Renyi"
    },
    {
      "index": 12,
      "title": "On Measures of Information and their Characterizations",
      "abstract": "",
      "year": "1975",
      "venue": "",
      "authors": "J. Acz√©l and Z. Dar√≥czy"
    },
    {
      "index": 13,
      "title": "Possible Generalization of Boltzmann-Gibbs Statistics",
      "abstract": "",
      "year": "1988",
      "venue": "J. Stat. Phys.",
      "authors": "C. Tsallis"
    },
    {
      "index": 14,
      "title": "Axiomatic derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy",
      "abstract": "",
      "year": "1980",
      "venue": "IEEE Trans. Inf. Theory",
      "authors": "J. E. Shore and R. W. Johnson"
    },
    {
      "index": 15,
      "title": "The Axioms of Maximum Entropy",
      "abstract": "",
      "year": "1988",
      "venue": "Maximum-Entropy and Bayesian Methods in Science and Engineering",
      "authors": "J. Skilling"
    },
    {
      "index": 16,
      "title": "Classic Maximum Entropy",
      "abstract": "",
      "year": "1989",
      "venue": "Maximum Entropy and Bayesian Methods",
      "authors": "J. Skilling"
    },
    {
      "index": 17,
      "title": "On the axiomatic approach to the maximum entropy principle of inference",
      "abstract": "",
      "year": "1986",
      "venue": "Pramana ‚Äì J. Phys.",
      "authors": "S. N. Karbelkar"
    },
    {
      "index": 18,
      "title": "Conceptual Inadequacy of the Shore and Johnson Axioms for Wide Classes of Complex systems",
      "abstract": "",
      "year": "2015",
      "venue": "Entropy",
      "authors": "C. Tsallis"
    },
    {
      "index": 19,
      "title": "Maximum Entropy Principle in Statistical Inference: Case for Non-Shannonian Entropies",
      "abstract": "",
      "year": "2019",
      "venue": "Phys. Rev. Lett.",
      "authors": "P. Jizba and J. Korbel"
    },
    {
      "index": 20,
      "title": "Nonadditive Entropies Yield Probability Distributions with Biases not Warranted by the Data",
      "abstract": "",
      "year": "2013",
      "venue": "Phys. Rev. Lett.",
      "authors": "S. Press√©, K. Ghosh, J. Lee, and K. A. Dill"
    },
    {
      "index": 21,
      "title": "Reply to Tsallis‚Äô ‚ÄúConceptual inadequacy of the Shore and Johnson axioms for wide classes of complex systems",
      "abstract": "",
      "year": "2015",
      "venue": "Entropy",
      "authors": "S. Press√©, K. Ghosh, J. Lee, and K. A. Dill"
    },
    {
      "index": 22,
      "title": "Bayesian Conditionalization and the Principle of Minimum Relative Information",
      "abstract": "",
      "year": "1980",
      "venue": "Brit. J. Phil. Sci.",
      "authors": "P. M. Williams"
    },
    {
      "index": 23,
      "title": "Entropic dynamics and the quantum measurement problem",
      "abstract": "",
      "year": "2012",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "D. T. Johnson and A. Caticha"
    },
    {
      "index": 24,
      "title": "Quantum measurement and weak values in entropic quantum dynamics",
      "abstract": "",
      "year": "2017",
      "venue": "AIP Conf. Proc.",
      "authors": "K. Vanslette and A. Caticha"
    },
    {
      "index": 25,
      "title": "Elements of Information Theory",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": "T. Cover and J. Thomas"
    },
    {
      "index": 26,
      "title": "Foundations of Info-Metrics: Modeling, Inference, and Imperfect Information",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "A. Golan"
    },
    {
      "index": 27,
      "title": "Modeling by shortest data description",
      "abstract": "",
      "year": "1978",
      "venue": "Automatica",
      "authors": "J. Rissanen"
    },
    {
      "index": 28,
      "title": "Information is Physical",
      "abstract": "",
      "year": "1991",
      "venue": "Physics Today",
      "authors": "R. Landauer"
    },
    {
      "index": 29,
      "title": "The thermodynamics of computation‚ÄîA review",
      "abstract": "",
      "year": "1982",
      "venue": "Int. J. Th. Phys.",
      "authors": "C. Bennett"
    },
    {
      "index": 30,
      "title": "Notes on Landauer‚Äôs principle, reversible computation, and Maxwell‚Äôs demon",
      "abstract": "",
      "year": "2003",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "C. Bennett"
    },
    {
      "index": 31,
      "title": "Waiting for Landauer",
      "abstract": "",
      "year": "2011",
      "venue": "Studies in History and Philosophy of Modern Physics",
      "authors": "J. D. Norton"
    },
    {
      "index": 32,
      "title": "The End of the Thermodynamics of Computation:¬†A No-Go Result",
      "abstract": "",
      "year": "2013",
      "venue": "Philosophy of Science",
      "authors": "J. D. Norton"
    },
    {
      "index": 33,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 34,
      "title": "Entropic dynamics on Gibbs statistical manifolds",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "P. Pessoa, F. X. Costa, and A. Caticha"
    },
    {
      "index": 35,
      "title": "The Entropic Dynamics approach to Quantum Mechanics",
      "abstract": "",
      "year": "2019",
      "venue": "Entropy",
      "authors": "A. Caticha"
    },
    {
      "index": 36,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 37,
      "title": "Lattice duality: The origin of probability and entropy",
      "abstract": "",
      "year": "2005",
      "venue": "Neurocomputing",
      "authors": "K. H. Knuth"
    },
    {
      "index": 38,
      "title": "Foundations of Inference",
      "abstract": "",
      "year": "2012",
      "venue": "Axioms",
      "authors": "K. H. Knuth, J. Skilling"
    },
    {
      "index": 39,
      "title": "Updating Probabilities with Data and Moments",
      "abstract": "",
      "year": "2007",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Giffin and A. Caticha"
    },
    {
      "index": 40,
      "title": "Differential-Geometrical Methods in Statistics",
      "abstract": "",
      "year": "1985",
      "venue": "",
      "authors": "S. Amari"
    },
    {
      "index": 41,
      "title": "Maximum entropy and Bayesian data analysis: entropic prior distributions",
      "abstract": "",
      "year": "2004",
      "venue": "Phys. Rev. E",
      "authors": "A. Caticha and R. Preuss"
    },
    {
      "index": 42,
      "title": "Maximum Probability and Maximum Entropy Methods: Bayesian interpretation",
      "abstract": "",
      "year": "2004",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "M. Grendar, Jr. and M. Grendar"
    },
    {
      "index": 43,
      "title": "Tsallis maximum entropy principle and the law of large numbers",
      "abstract": "",
      "year": "2000",
      "venue": "Phys. Rev E",
      "authors": "B.R. La Cour and W. C. Schieve"
    },
    {
      "index": 44,
      "title": "Critique of qùëûq-entropy for thermal statistics",
      "abstract": "",
      "year": "2003",
      "venue": "Phys. Rev E",
      "authors": "M. Nauenberg"
    },
    {
      "index": 45,
      "title": "From Gibbs microcanonical ensemble to Tsallis generalized canonical distribution",
      "abstract": "",
      "year": "1994",
      "venue": "Phys. Lett. A",
      "authors": "A. R. Plastino and A. Plastino"
    },
    {
      "index": 46,
      "title": "Dynamical Foundations of nonextensive Statistical Mechanics",
      "abstract": "",
      "year": "2001",
      "venue": "Phys. Rev. Lett.",
      "authors": "C. Beck"
    },
    {
      "index": 47,
      "title": "Superstatistics",
      "abstract": "",
      "year": "2003",
      "venue": "Physica A",
      "authors": "C. Beck and E.G.D. Cohen"
    },
    {
      "index": 48,
      "title": "Beyond Boltzmann-Gibbs statistics: Maximum entropy hyperensembles out of equilibrium",
      "abstract": "",
      "year": "2007",
      "venue": "Phys. Rev. E",
      "authors": "G. E. Crooks"
    },
    {
      "index": 49,
      "title": "Entropic inference: some pitfalls and paradoxes we can avoid",
      "abstract": "",
      "year": "2013",
      "venue": "Bayesian Inference and Maximum Entropy Methods in Science and Engineering",
      "authors": "A. Caticha"
    },
    {
      "index": 50,
      "title": "The Undivided Universe: an ontological interpretation on quantum theory",
      "abstract": "",
      "year": "1993",
      "venue": "",
      "authors": "D. Bohm and B. J. Hiley"
    },
    {
      "index": 51,
      "title": "On the foundations of decision theory",
      "abstract": "",
      "year": "2017",
      "venue": "Homo Oecon",
      "authors": "K. Binmore"
    },
    {
      "index": 52,
      "title": "Information Theory for Agents in Artificial Intelligence, Psychology, and Economics",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "M.S. Harre"
    },
    {
      "index": 53,
      "title": "A Maximum Entropy Model of Bounded Rational Decision-Making with Prior Beliefs and Market Feedback",
      "abstract": "",
      "year": "2021",
      "venue": "Entropy",
      "authors": "B. P. Evans, M. Prokopenko"
    },
    {
      "index": 54,
      "title": "An Entropic framework for Modeling Economies",
      "abstract": "",
      "year": "2014",
      "venue": "Physica A",
      "authors": "A. Caticha and A. Golan"
    }
  ]
}