{
  "paper_id": "2210.05271v1",
  "title": "GAN you hear me? Reclaiming unconditional speech synthesis from diffusion models",
  "sections": {
    "introduction": "Unconditional speech synthesis is the task of generating coherent speech without any conditioning inputs such as text or speaker labels [ref]1.\nAs\nin\nimage synthesis [ref]2, a well-performing unconditional speech synthesis model would have several useful applications:\nfrom latent interpolations between utterances and fine-grained tuning of different aspects of the generated speech, to audio compression and better\nprobability density estimation of speech. Spurred on by recent improvements in diffusion models  for images   , there has been a substantial improvement in\nunconditional speech synthesis\nin the last few years.\nThe\ncurrent best-performing approaches are all trained as\ndiffusion models  .\nBefore this, most studies\nused generative adversarial networks (GANs) \nthat map\na latent vector to a sequence of speech features with a single forward pass through the model.\nHowever,\nperformance was limited [ref]1 , leading to GANs falling out of favour for this task. Motivated by the StyleGAN literature   [ref]13 for image synthesis, we aim to reinvigorate GANs for unconditional speech synthesis.\nTo this end, we propose AudioStyleGAN (ASGAN): a convolutional GAN which maps a single latent vector to a sequence of audio features,\nand is designed to have a\ndisentangled latent space.\nThe model is based in large part on StyleGAN3 [ref]13, which we adapt for audio synthesis.\nConcretely, we\nadapt\nthe style layers to\nremove signal\naliasing caused by the non-linearities in the network.\nThis is accomplished with anti-aliasing filters to ensure that the Nyquist−\\mathchar 45\\relaxShannon sampling limits are met in each layer.\nWe also propose a modification to adaptive discriminator augmentation  to stabilize training by randomly dropping discriminator updates based on a guiding signal. Using objective metrics to measure the quality and diversity of generated samples  [ref]2 , we show that ASGAN sets a new state-of-the-art in unconditional speech synthesis on\nthe Google Speech Commands digits dataset .\nIt not only outperforms the best existing models, but is also faster to train and faster in inference.\nMean opinion scores (MOS)\nalso indicate\nthat ASGAN’s generated\nutterances sound\nmore natural (MOS: 3.68) than the existing best model (SaShiMi , MOS: 3.33). Through ASGAN’s design, the model’s latent space is disentangled during training, enabling the model – without any additional training\n– to also perform voice conversion and\nspeech editing\nin a zero-shot fashion.\nObjective metrics that measure latent space disentanglement indicate that ASGAN has smoother latent representations\ncompared to\nexisting diffusion models."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Adversarial Audio Synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Chris Donahue, Julian McAuley, and Miller Puckette",
      "orig_title": "Adversarial audio synthesis",
      "paper_id": "1802.04208v3"
    },
    {
      "index": 1,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter",
      "orig_title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 2,
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli",
      "orig_title": "Deep unsupervised learning using nonequilibrium thermodynamics",
      "paper_id": "1503.03585v8"
    },
    {
      "index": 3,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, et al.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 4,
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.06125",
      "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen",
      "orig_title": "Hierarchical text-conditional image generation with CLIP latents",
      "paper_id": "2204.06125v1"
    },
    {
      "index": 5,
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.11487",
      "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, et al.",
      "orig_title": "Photorealistic text-to-image diffusion models with deep language understanding",
      "paper_id": "2205.11487v1"
    },
    {
      "index": 6,
      "title": "It’s Raw! Audio Generation with State-Space Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.09729",
      "authors": "Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré",
      "orig_title": "It’s raw! Audio generation with state-space models",
      "paper_id": "2202.09729v1"
    },
    {
      "index": 7,
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.09761",
      "authors": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro"
    },
    {
      "index": 8,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, et al."
    },
    {
      "index": 9,
      "title": "Generative Adversarial Phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Frontiers in artificial intelligence",
      "authors": "Gašper Beguš",
      "orig_title": "Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks",
      "paper_id": "2006.03965v1"
    },
    {
      "index": 10,
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, and Timo Aila",
      "orig_title": "A style-based generator architecture for generative adversarial networks",
      "paper_id": "1812.04948v3"
    },
    {
      "index": 11,
      "title": "Analyzing and Improving the Image Quality of StyleGAN",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, et al.",
      "orig_title": "Analyzing and improving the image quality of StyleGAN",
      "paper_id": "1912.04958v2"
    },
    {
      "index": 12,
      "title": "Alias-Free Generative Adversarial Networks",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, et al.",
      "orig_title": "Alias-free generative adversarial networks",
      "paper_id": "2106.12423v4"
    },
    {
      "index": 13,
      "title": "Training Generative Adversarial Networks with Limited Data",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, et al.",
      "orig_title": "Training generative adversarial networks with limited data",
      "paper_id": "2006.06676v2"
    },
    {
      "index": 14,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, et al.",
      "orig_title": "Improved techniques for training GANs",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 15,
      "title": "Activation Maximization Generative Adversarial Nets",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, et al.",
      "orig_title": "Activation maximization generative adversarial nets",
      "paper_id": "1703.02000v9"
    },
    {
      "index": 16,
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.03209",
      "authors": "Pete Warden"
    },
    {
      "index": 17,
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.07447",
      "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, et al.",
      "orig_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "paper_id": "2106.07447v1"
    },
    {
      "index": 18,
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "abstract": "",
      "year": "2021",
      "venue": "Interspeech",
      "authors": "Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux"
    },
    {
      "index": 19,
      "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, et al.",
      "orig_title": "Text-free prosody-aware generative spoken language modeling",
      "paper_id": "2109.03264v2"
    },
    {
      "index": 20,
      "title": "Generative spoken language modeling from raw audio",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.01192",
      "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mohamed, et al."
    },
    {
      "index": 21,
      "title": "WaveNet: A Generative Model for Raw Audio",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.03499",
      "authors": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, et al.",
      "orig_title": "WaveNet: A generative model for raw audio",
      "paper_id": "1609.03499v2"
    },
    {
      "index": 22,
      "title": "Which training methods for GANs do actually converge?",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin"
    },
    {
      "index": 23,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 24,
      "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Computer Speech & Language",
      "authors": "Gašper Beguš",
      "orig_title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks",
      "paper_id": "2009.12711v2"
    },
    {
      "index": 25,
      "title": "Generating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms",
      "abstract": "",
      "year": "2022",
      "venue": "ICML ExVo Generate",
      "authors": "Marco Jiralerspong and Gauthier Gidel",
      "orig_title": "Generating diverse vocal bursts with StyleGAN2 and mel-spectrograms",
      "paper_id": "2206.12563v1"
    },
    {
      "index": 26,
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae",
      "orig_title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "paper_id": "2010.05646v2"
    },
    {
      "index": 27,
      "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, et al.",
      "orig_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
      "paper_id": "2006.10739v1"
    },
    {
      "index": 28,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Saining Xie, Ross B Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 29,
      "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu",
      "orig_title": "DeLiGAN: Generative adversarial networks for diverse and limited data",
      "paper_id": "1706.02071v1"
    },
    {
      "index": 30,
      "title": "The VoiceMOS Challenge 2022",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11389",
      "authors": "Wen-Chin Huang, Erica Cooper, Yu Tsao, Hsin-Min Wang, Tomoki Toda, et al.",
      "orig_title": "The VoiceMOS challenge 2022",
      "paper_id": "2203.11389v3"
    },
    {
      "index": 31,
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "abstract": "",
      "year": "2015",
      "venue": "ICASSP",
      "authors": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur"
    },
    {
      "index": 32,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Diederik P. Kingma and Jimmy Ba"
    },
    {
      "index": 33,
      "title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Song Tao and J Wang",
      "orig_title": "Alleviation of gradient exploding in GANs: Fake can be real",
      "paper_id": "1912.12485v2"
    },
    {
      "index": 34,
      "title": "Digital filters",
      "abstract": "",
      "year": "1966",
      "venue": "System analysis by digital computer",
      "authors": "James F Kaiser"
    }
  ]
}