{
  "paper_id": "2008.11804v2",
  "title": "Deep Inverse Reinforcement Learning for Structural Evolution of Small Molecules",
  "sections": {
    "inverse reinforcement learning": "IRL is the problem of learning the reward function of an observed agent, given its policy or behavior, thereby avoiding the manual specification of a reward function . The IRL class of solutions assumes the following MDP∖REsubscript𝑅𝐸\\setminus R_{E}: A set of states 𝒮𝒮\\mathcal{S}, with a distribution over starting states p​(s0)𝑝subscript𝑠0p(s_{0}). A set of actions 𝒜𝒜\\mathcal{A}. State transition dynamics function 𝒯​(st+1|st,at)𝒯conditionalsubscript𝑠𝑡1subscript𝑠𝑡subscript𝑎𝑡\\mathcal{T}(s_{t+1}|s_{t},a_{t}) that maps a state-action pair at a time step t𝑡t to a distribution of states at time step t+1𝑡1t+1. A set of demonstrated trajectories \n𝒟={⟨(s0i,a0i),…,(sT−1i,aT−1i)⟩i=1N}𝒟superscriptsubscriptsuperscriptsubscript𝑠0𝑖superscriptsubscript𝑎0𝑖…superscriptsubscript𝑠𝑇1𝑖superscriptsubscript𝑎𝑇1𝑖𝑖1𝑁\\mathcal{D}=\\left\\{\\left<(s_{0}^{i},a_{0}^{i}),...,(s_{T-1}^{i},a_{T-1}^{i})\\right>_{i=1}^{N}\\right\\} from the observed agent or expert. A discount factor γ∈[ref]0 𝛾01\\gamma\\in[ref]0  may be used to discount future rewards. The goal of IRL then is to learn the reward function REsubscript𝑅𝐸R_{E} that best explains the expert demonstrations. It is assumed that the demonstrations are perfectly observed and that the expert follows an optimal policy. While learning reward functions from data is appealing, the IRL problem is ill-posed since there are many reward functions under which the observed expert behavior is optimal  . An instance is a reward function that assigns 0 (or any constant value) for all selected actions; in such a case, any policy is optimal. Other main challenges are accurate inference, generalizability, the correctness of prior knowledge, and computational cost with an increase in problem complexity. To this end, several IRL proposals exist in the literature to mitigate the IRL challenges mentioned above . In recent times, the entropy optimization class of IRL methods has been widely used by researchers due to the maximum entropy’s goal to obtain an unbiased distribution of potential reward [ref]33   [ref]24. The intuition is that the solution that maximizes entropy violates the optimization constraints least and hence, least wrong. In the Maximum Entropy (MaxEnt) formulation, the probability of the expert’s trajectory is proportional to the exponential of the total reward [ref]33, where R​(τ)=∑(s,a)∈τr​(s,a)𝑅𝜏subscript𝑠𝑎𝜏𝑟𝑠𝑎R(\\tau)=\\sum_{(s,a)\\in\\tau}r(s,a) and r​(s,a)𝑟𝑠𝑎r(s,a) gives the reward for taking action a𝑎a in state s𝑠s."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Molecular De-Novo Design through Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Cheminformatics",
      "authors": "Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen",
      "orig_title": "Molecular de-novo design through deep reinforcement learning",
      "paper_id": "1704.07555v2"
    },
    {
      "index": 1,
      "title": "Small-molecule inhibitors of hepatitis c virus (hcv) non-structural protein 5a (ns5a): a patent review (2010-2015)",
      "abstract": "",
      "year": "2017",
      "venue": "Expert Opinion on Therapeutic Patents",
      "authors": "Yan A. Ivanenkov, Vladimir A. Aladinskiy, Nikolay A. Bushkov, Andrey A. Ayginin, Alexander G. Majouga, and Alexandre V. Ivachtchenko"
    },
    {
      "index": 2,
      "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ACS Central Science",
      "authors": "Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller",
      "orig_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
      "paper_id": "1701.01329v1"
    },
    {
      "index": 3,
      "title": "Estimation of the size of drug-like chemical space based on GDB-17 data",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Computer-Aided Molecular Design",
      "authors": "P. G. Polishchuk, T. I. Madzhidov, and A. Varnek"
    },
    {
      "index": 4,
      "title": "Improving the odds of drug development success through human genomics: modelling study",
      "abstract": "",
      "year": "2019",
      "venue": "Scientific Reports",
      "authors": "Aroon D Hingorani, Valerie Kuan, Chris Finan, Felix A Kruger, Anna Gaulton, Sandesh Chopade, Reecha Sofat, Raymond J. MacAllister, John P Overington, Harry Hemingway, Spiros Denaxas, David Prieto, and Juan Pablo Casas"
    },
    {
      "index": 5,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arxiv e-prints",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 6,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2018",
      "venue": "OpenAI Technical Report",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"
    },
    {
      "index": 7,
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio"
    },
    {
      "index": 8,
      "title": "Protein-Ligand Scoring with Convolutional Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and David Ryan Koes"
    },
    {
      "index": 9,
      "title": "Multi-View Self-Attention for Interpretable Drug-Target Interaction Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Biomedical Informatics",
      "authors": "Brighter Agyemang, Wei-Ping Wu, Michael Yelpengne Kpiebaareh, Zhihua Lei, Ebenezer Nanor, and Lei Chen",
      "orig_title": "Multi-view self-attention for interpretable drug–target interaction prediction",
      "paper_id": "2005.00397v2"
    },
    {
      "index": 10,
      "title": "Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences",
      "abstract": "",
      "year": "2019",
      "venue": "Bioinformatics",
      "authors": "Masashi Tsubaki, Kentaro Tomii, and Jun Sese"
    },
    {
      "index": 11,
      "title": "Modelling Chemical Reasoning to Predict and Invent Reactions",
      "abstract": "",
      "year": "2017",
      "venue": "Chemistry - A European Journal",
      "authors": "Marwin H.S. Segler and Mark P. Waller"
    },
    {
      "index": 12,
      "title": "SMILES, a Chemical Language and Information System: 1: Introduction to Methodology and Encoding Rules",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Chemical Information and Computer Sciences",
      "authors": "David Weininger"
    },
    {
      "index": 13,
      "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu"
    },
    {
      "index": 14,
      "title": "Deep reinforcement learning for de novo drug design",
      "abstract": "",
      "year": "2018",
      "venue": "Science Advances",
      "authors": "Mariya Popova, Olexandr Isayev, and Alexander Tropsha"
    },
    {
      "index": 15,
      "title": "Simple statistical gradient-following methods for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "R J Williams"
    },
    {
      "index": 16,
      "title": "Reinforced molecule generation with heterogeneous states",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Data Mining (ICDM)",
      "authors": "Fangzhou Shi, Shan You, and Chang Xu"
    },
    {
      "index": 17,
      "title": "Generalization in Generation: A closer look at Exposure Bias",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Florian Schmidt"
    },
    {
      "index": 18,
      "title": "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC)",
      "abstract": "",
      "year": "2017",
      "venue": "ChemRxiv",
      "authors": "Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alán Aspuru-Guzik"
    },
    {
      "index": 19,
      "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Pedro Luis Cunha Farias, and Alán Aspuru-Guzik",
      "orig_title": "Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models",
      "paper_id": "1705.10843v3"
    },
    {
      "index": 20,
      "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Mostapha Benhenda",
      "orig_title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?",
      "paper_id": "1708.08227v3"
    },
    {
      "index": 21,
      "title": "Reinforced Adversarial Neural Computer for de Novo Molecular Design",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Evgeny Putin, Arip Asadulaev, Yan Ivanenkov, Vladimir Aladinskiy, Benjamin Sanchez-Lengeling, Alán Aspuru-Guzik, and Alex Zhavoronkov"
    },
    {
      "index": 22,
      "title": "Towards Principled Methods for Training Generative Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Martín Arjovsky and Léon Bottou",
      "orig_title": "Towards principled methods for training generative adversarial networks",
      "paper_id": "1701.04862v1"
    },
    {
      "index": 23,
      "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Chelsea Finn, Sergey Levine, and Pieter Abbeel",
      "orig_title": "Guided cost learning: Deep inverse optimal control via policy optimization",
      "paper_id": "1603.00448v3"
    },
    {
      "index": 24,
      "title": "A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine",
      "orig_title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models",
      "paper_id": "1611.03852v3"
    },
    {
      "index": 25,
      "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel",
      "orig_title": "High-dimensional continuous control using generalized advantage estimation",
      "paper_id": "1506.02438v6"
    },
    {
      "index": 26,
      "title": "A Brief Survey of Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath"
    },
    {
      "index": 27,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov"
    },
    {
      "index": 28,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 29,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel",
      "orig_title": "Trust Region Policy Optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 30,
      "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv e-prints",
      "authors": "Saurabh Arora and Prashant Doshi",
      "orig_title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
      "paper_id": "1806.06877v3"
    },
    {
      "index": 31,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "Seventeenth International Conference on Machine Learning",
      "authors": "Andrew Ng and Stuart Russell"
    },
    {
      "index": 32,
      "title": "Maximum Entropy Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey"
    },
    {
      "index": 33,
      "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Markus Wulfmeier and Peter Ondr",
      "orig_title": "Maximum Entropy Deep Inverse Reinforcement Learning",
      "paper_id": "1507.04888v3"
    },
    {
      "index": 34,
      "title": "Generative adversarial imitation learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jonathan Ho and Stefano Ermon"
    },
    {
      "index": 35,
      "title": "DrugBank 3.0: A comprehensive resource for ’Omics’ research on drugs",
      "abstract": "",
      "year": "2011",
      "venue": "Nucleic Acids Research",
      "authors": "Craig Knox, Vivian Law, Timothy Jewison, Philip Liu, Son Ly, Alex Frolkis, Allison Pon, Kelly Banco, Christine Mak, Vanessa Neveu, Yannick Djoumbou, Roman Eisner, An Chi Guo, and David S. Wishart"
    },
    {
      "index": 36,
      "title": "KEGG for integration and interpretation of large-scale molecular data sets",
      "abstract": "",
      "year": "2012",
      "venue": "Nucleic Acids Research",
      "authors": "Minoru Kanehisa, Susumu Goto, Yoko Sato, Miho Furumichi, and Mao Tanabe"
    },
    {
      "index": 37,
      "title": "STITCH 5: Augmenting protein-chemical interaction networks with tissue and affinity data",
      "abstract": "",
      "year": "2016",
      "venue": "Nucleic Acids Research",
      "authors": "Damian Szklarczyk, Alberto Santos, Christian Von Mering, Lars Juhl Jensen, Peer Bork, and Michael Kuhn"
    },
    {
      "index": 38,
      "title": "The ChEMBL bioactivity database: An update",
      "abstract": "",
      "year": "2014",
      "venue": "Nucleic Acids Research",
      "authors": "A. Patrícia Bento, Anna Gaulton, Anne Hersey, Louisa J. Bellis, Jon Chambers, Mark Davies, Felix A. Krüger, Yvonne Light, Lora Mak, Shaun McGlinchey, Michal Nowotka, George Papadatos, Rita Santos, and John P. Overington"
    },
    {
      "index": 39,
      "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Armand Joulin and Tomas Mikolov",
      "orig_title": "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "paper_id": "1503.01007v4"
    },
    {
      "index": 40,
      "title": "ExCAPE-DB: An integrated large scale dataset facilitating Big Data analysis in chemogenomics",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Cheminformatics",
      "authors": "Jiangming Sun, Nina Jeliazkova, Vladimir Chupakin, Jose Felipe Golib-Dzib, Ola Engkvist, Lars Carlsson, Jörg Wegner, Hugo Ceulemans, Ivan Georgiev, Vedrin Jeliazkov, Nikolay Kochev, Thomas J. Ashby, and Hongming Chen"
    },
    {
      "index": 41,
      "title": "Naples: A natural products likeness scorer—web application and database",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Cheminformatics",
      "authors": "Maria Sorokina and Christoph Steinbeck"
    },
    {
      "index": 42,
      "title": "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Cheminformatics",
      "authors": "Peter Ertl and Ansgar Schuffenhauer"
    },
    {
      "index": 43,
      "title": "Stabilizing Transformers for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell",
      "orig_title": "Stabilizing Transformers for Reinforcement Learning",
      "paper_id": "1910.06764v1"
    }
  ]
}