{
  "paper_id": "2008.11804v2",
  "title": "Deep Inverse Reinforcement Learning for Structural Evolution of Small Molecules",
  "sections": {
    "inverse reinforcement learning": "IRL is the problem of learning the reward function of an observed agent, given its policy or behavior, thereby avoiding the manual specification of a reward functionÂ . The IRL class of solutions assumes the following MDPâˆ–REsubscriptğ‘…ğ¸\\setminus R_{E}: A set of states ğ’®ğ’®\\mathcal{S}, with a distribution over starting states pâ€‹(s0)ğ‘subscriptğ‘ 0p(s_{0}). A set of actions ğ’œğ’œ\\mathcal{A}. State transition dynamics function ğ’¯â€‹(st+1|st,at)ğ’¯conditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\\mathcal{T}(s_{t+1}|s_{t},a_{t}) that maps a state-action pair at a time step tğ‘¡t to a distribution of states at time step t+1ğ‘¡1t+1. A set of demonstrated trajectories \nğ’Ÿ={âŸ¨(s0i,a0i),â€¦,(sTâˆ’1i,aTâˆ’1i)âŸ©i=1N}ğ’Ÿsuperscriptsubscriptsuperscriptsubscriptğ‘ 0ğ‘–superscriptsubscriptğ‘0ğ‘–â€¦superscriptsubscriptğ‘ ğ‘‡1ğ‘–superscriptsubscriptğ‘ğ‘‡1ğ‘–ğ‘–1ğ‘\\mathcal{D}=\\left\\{\\left<(s_{0}^{i},a_{0}^{i}),...,(s_{T-1}^{i},a_{T-1}^{i})\\right>_{i=1}^{N}\\right\\} from the observed agent or expert. A discount factor Î³âˆˆ[ref]0 ğ›¾01\\gamma\\in[ref]0  may be used to discount future rewards. The goal of IRL then is to learn the reward function REsubscriptğ‘…ğ¸R_{E} that best explains the expert demonstrations. It is assumed that the demonstrations are perfectly observed and that the expert follows an optimal policy. While learning reward functions from data is appealing, the IRL problem is ill-posed since there are many reward functions under which the observed expert behavior is optimalÂ  . An instance is a reward function that assigns 0 (or any constant value) for all selected actions; in such a case, any policy is optimal. Other main challenges are accurate inference, generalizability, the correctness of prior knowledge, and computational cost with an increase in problem complexity. To this end, several IRL proposals exist in the literature to mitigate the IRL challenges mentioned aboveÂ . In recent times, the entropy optimization class of IRL methods has been widely used by researchers due to the maximum entropyâ€™s goal to obtain an unbiased distribution of potential rewardÂ [ref]33   [ref]24. The intuition is that the solution that maximizes entropy violates the optimization constraints least and hence, least wrong. In the Maximum Entropy (MaxEnt) formulation, the probability of the expertâ€™s trajectory is proportional to the exponential of the total rewardÂ [ref]33, where Râ€‹(Ï„)=âˆ‘(s,a)âˆˆÏ„râ€‹(s,a)ğ‘…ğœsubscriptğ‘ ğ‘ğœğ‘Ÿğ‘ ğ‘R(\\tau)=\\sum_{(s,a)\\in\\tau}r(s,a) and râ€‹(s,a)ğ‘Ÿğ‘ ğ‘r(s,a) gives the reward for taking action ağ‘a in state sğ‘ s."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Molecular De-Novo Design through Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Cheminformatics",
      "authors": "Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen",
      "orig_title": "Molecular de-novo design through deep reinforcement learning",
      "paper_id": "1704.07555v2"
    },
    {
      "index": 1,
      "title": "Small-molecule inhibitors of hepatitis c virus (hcv) non-structural protein 5a (ns5a): a patent review (2010-2015)",
      "abstract": "",
      "year": "2017",
      "venue": "Expert Opinion on Therapeutic Patents",
      "authors": "YanÂ A. Ivanenkov, VladimirÂ A. Aladinskiy, NikolayÂ A. Bushkov, AndreyÂ A. Ayginin, AlexanderÂ G. Majouga, and AlexandreÂ V. Ivachtchenko"
    },
    {
      "index": 2,
      "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ACS Central Science",
      "authors": "Marwin H.Â S. Segler, Thierry Kogej, Christian Tyrchan, and MarkÂ P. Waller",
      "orig_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
      "paper_id": "1701.01329v1"
    },
    {
      "index": 3,
      "title": "Estimation of the size of drug-like chemical space based on GDB-17 data",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Computer-Aided Molecular Design",
      "authors": "P.Â G. Polishchuk, T.Â I. Madzhidov, and A.Â Varnek"
    },
    {
      "index": 4,
      "title": "Improving the odds of drug development success through human genomics: modelling study",
      "abstract": "",
      "year": "2019",
      "venue": "Scientific Reports",
      "authors": "AroonÂ D Hingorani, Valerie Kuan, Chris Finan, FelixÂ A Kruger, Anna Gaulton, Sandesh Chopade, Reecha Sofat, RaymondÂ J. MacAllister, JohnÂ P Overington, Harry Hemingway, Spiros Denaxas, David Prieto, and JuanÂ Pablo Casas"
    },
    {
      "index": 5,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arxiv e-prints",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 6,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2018",
      "venue": "OpenAI Technical Report",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"
    },
    {
      "index": 7,
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio"
    },
    {
      "index": 8,
      "title": "Protein-Ligand Scoring with Convolutional Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and DavidÂ Ryan Koes"
    },
    {
      "index": 9,
      "title": "Multi-View Self-Attention for Interpretable Drug-Target Interaction Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Biomedical Informatics",
      "authors": "Brighter Agyemang, Wei-Ping Wu, MichaelÂ Yelpengne Kpiebaareh, Zhihua Lei, Ebenezer Nanor, and Lei Chen",
      "orig_title": "Multi-view self-attention for interpretable drugâ€“target interaction prediction",
      "paper_id": "2005.00397v2"
    },
    {
      "index": 10,
      "title": "Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences",
      "abstract": "",
      "year": "2019",
      "venue": "Bioinformatics",
      "authors": "Masashi Tsubaki, Kentaro Tomii, and Jun Sese"
    },
    {
      "index": 11,
      "title": "Modelling Chemical Reasoning to Predict and Invent Reactions",
      "abstract": "",
      "year": "2017",
      "venue": "Chemistry - A European Journal",
      "authors": "MarwinÂ H.S. Segler and MarkÂ P. Waller"
    },
    {
      "index": 12,
      "title": "SMILES, a Chemical Language and Information System: 1: Introduction to Methodology and Encoding Rules",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Chemical Information and Computer Sciences",
      "authors": "David Weininger"
    },
    {
      "index": 13,
      "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu"
    },
    {
      "index": 14,
      "title": "Deep reinforcement learning for de novo drug design",
      "abstract": "",
      "year": "2018",
      "venue": "Science Advances",
      "authors": "Mariya Popova, Olexandr Isayev, and Alexander Tropsha"
    },
    {
      "index": 15,
      "title": "Simple statistical gradient-following methods for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "RÂ J Williams"
    },
    {
      "index": 16,
      "title": "Reinforced molecule generation with heterogeneous states",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Data Mining (ICDM)",
      "authors": "Fangzhou Shi, Shan You, and Chang Xu"
    },
    {
      "index": 17,
      "title": "Generalization in Generation: A closer look at Exposure Bias",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Florian Schmidt"
    },
    {
      "index": 18,
      "title": "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry (ORGANIC)",
      "abstract": "",
      "year": "2017",
      "venue": "ChemRxiv",
      "authors": "Benjamin Sanchez-Lengeling, Carlos Outeiral, GabrielÂ L Guimaraes, and AlÃ¡n Aspuru-Guzik"
    },
    {
      "index": 19,
      "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "GabrielÂ Lima Guimaraes, Benjamin Sanchez-Lengeling, Pedro LuisÂ Cunha Farias, and AlÃ¡n Aspuru-Guzik",
      "orig_title": "Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models",
      "paper_id": "1705.10843v3"
    },
    {
      "index": 20,
      "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Mostapha Benhenda",
      "orig_title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?",
      "paper_id": "1708.08227v3"
    },
    {
      "index": 21,
      "title": "Reinforced Adversarial Neural Computer for de Novo Molecular Design",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Chemical Information and Modeling",
      "authors": "Evgeny Putin, Arip Asadulaev, Yan Ivanenkov, Vladimir Aladinskiy, Benjamin Sanchez-Lengeling, AlÃ¡n Aspuru-Guzik, and Alex Zhavoronkov"
    },
    {
      "index": 22,
      "title": "Towards Principled Methods for Training Generative Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "MartÃ­n Arjovsky and LÃ©on Bottou",
      "orig_title": "Towards principled methods for training generative adversarial networks",
      "paper_id": "1701.04862v1"
    },
    {
      "index": 23,
      "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Chelsea Finn, Sergey Levine, and Pieter Abbeel",
      "orig_title": "Guided cost learning: Deep inverse optimal control via policy optimization",
      "paper_id": "1603.00448v3"
    },
    {
      "index": 24,
      "title": "A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Chelsea Finn, PaulÂ F. Christiano, Pieter Abbeel, and Sergey Levine",
      "orig_title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models",
      "paper_id": "1611.03852v3"
    },
    {
      "index": 25,
      "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel",
      "orig_title": "High-dimensional continuous control using generalized advantage estimation",
      "paper_id": "1506.02438v6"
    },
    {
      "index": 26,
      "title": "A Brief Survey of Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Kai Arulkumaran, MarcÂ Peter Deisenroth, Miles Brundage, and AnilÂ Anthony Bharath"
    },
    {
      "index": 27,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov"
    },
    {
      "index": 28,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, AndreiÂ a Rusu, Joel Veness, MarcÂ G Bellemare, Alex Graves, Martin Riedmiller, AndreasÂ K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 29,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "John Schulman, Sergey Levine, Philipp Moritz, MichaelÂ I. Jordan, and Pieter Abbeel",
      "orig_title": "Trust Region Policy Optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 30,
      "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv e-prints",
      "authors": "Saurabh Arora and Prashant Doshi",
      "orig_title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
      "paper_id": "1806.06877v3"
    },
    {
      "index": 31,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "Seventeenth International Conference on Machine Learning",
      "authors": "Andrew Ng and Stuart Russell"
    },
    {
      "index": 32,
      "title": "Maximum Entropy Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "BrianÂ D Ziebart, Andrew Maas, JÂ Andrew Bagnell, and AnindÂ K Dey"
    },
    {
      "index": 33,
      "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints",
      "authors": "Markus Wulfmeier and Peter Ondr",
      "orig_title": "Maximum Entropy Deep Inverse Reinforcement Learning",
      "paper_id": "1507.04888v3"
    },
    {
      "index": 34,
      "title": "Generative adversarial imitation learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jonathan Ho and Stefano Ermon"
    },
    {
      "index": 35,
      "title": "DrugBank 3.0: A comprehensive resource for â€™Omicsâ€™ research on drugs",
      "abstract": "",
      "year": "2011",
      "venue": "Nucleic Acids Research",
      "authors": "Craig Knox, Vivian Law, Timothy Jewison, Philip Liu, Son Ly, Alex Frolkis, Allison Pon, Kelly Banco, Christine Mak, Vanessa Neveu, Yannick Djoumbou, Roman Eisner, AnÂ Chi Guo, and DavidÂ S. Wishart"
    },
    {
      "index": 36,
      "title": "KEGG for integration and interpretation of large-scale molecular data sets",
      "abstract": "",
      "year": "2012",
      "venue": "Nucleic Acids Research",
      "authors": "Minoru Kanehisa, Susumu Goto, Yoko Sato, Miho Furumichi, and Mao Tanabe"
    },
    {
      "index": 37,
      "title": "STITCH 5: Augmenting protein-chemical interaction networks with tissue and affinity data",
      "abstract": "",
      "year": "2016",
      "venue": "Nucleic Acids Research",
      "authors": "Damian Szklarczyk, Alberto Santos, Christian Von Mering, LarsÂ Juhl Jensen, Peer Bork, and Michael Kuhn"
    },
    {
      "index": 38,
      "title": "The ChEMBL bioactivity database: An update",
      "abstract": "",
      "year": "2014",
      "venue": "Nucleic Acids Research",
      "authors": "A.Â PatrÃ­cia Bento, Anna Gaulton, Anne Hersey, LouisaÂ J. Bellis, Jon Chambers, Mark Davies, FelixÂ A. KrÃ¼ger, Yvonne Light, Lora Mak, Shaun McGlinchey, Michal Nowotka, George Papadatos, Rita Santos, and JohnÂ P. Overington"
    },
    {
      "index": 39,
      "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Armand Joulin and Tomas Mikolov",
      "orig_title": "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "paper_id": "1503.01007v4"
    },
    {
      "index": 40,
      "title": "ExCAPE-DB: An integrated large scale dataset facilitating Big Data analysis in chemogenomics",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Cheminformatics",
      "authors": "Jiangming Sun, Nina Jeliazkova, Vladimir Chupakin, JoseÂ Felipe Golib-Dzib, Ola Engkvist, Lars Carlsson, JÃ¶rg Wegner, Hugo Ceulemans, Ivan Georgiev, Vedrin Jeliazkov, Nikolay Kochev, ThomasÂ J. Ashby, and Hongming Chen"
    },
    {
      "index": 41,
      "title": "Naples: A natural products likeness scorerâ€”web application and database",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Cheminformatics",
      "authors": "Maria Sorokina and Christoph Steinbeck"
    },
    {
      "index": 42,
      "title": "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Cheminformatics",
      "authors": "Peter Ertl and Ansgar Schuffenhauer"
    },
    {
      "index": 43,
      "title": "Stabilizing Transformers for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Emilio Parisotto, H.Â Francis Song, JackÂ W. Rae, Razvan Pascanu, Caglar Gulcehre, SiddhantÂ M. Jayakumar, Max Jaderberg, RaphaelÂ Lopez Kaufman, Aidan Clark, Seb Noury, MatthewÂ M. Botvinick, Nicolas Heess, and Raia Hadsell",
      "orig_title": "Stabilizing Transformers for Reinforcement Learning",
      "paper_id": "1910.06764v1"
    }
  ]
}