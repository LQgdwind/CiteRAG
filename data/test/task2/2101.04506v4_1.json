{
  "paper_id": "2101.04506v4",
  "title": "UFA-FUSE: A novel deep supervised and hybrid model for multi-focus image fusion",
  "sections": {
    "i introduction": "Image fusion is an effective approach to extend the measurement of optical lenses to integrate two or more input images to produce a more informative fused image. Image fusion includes several fusion subareas such as multi-focus image fusion, infrared and visible image fusion, and medical image fusion,[ref]5. Specifically, multi-focus image fusion aims to get clear images that obtain all objects of a scene in focus. Multi-focus image fusion has become more closely related to our daily lives, while playing a crucial role in biological, industrial and medical fields. For example, multi-focus image fusion is used to fuse microscopic cervical cell images for disease diagnosis, and can also be applied to structural measurement of nonwoven fabrics. Also, multi-focus image fusion is a guarantee for the precise acquisition of pap smear images. Generally speaking, there exist three categories of multi-focus image fusion techniques according to imaging principles. The fusion techniques are broadly divided into transform domain techniques , spatial domain techniques and deep learning-based methods. The transform domain techniques are prevailing in the first phase of image fusion. The image algorithms based on transform domain share a universal three main steps to fuse image, which are generally summarized as decomposition, fusion and reconstruction. Some representative examples include pyramid-based and wavelet-based methods, such as the morphological pyramid-based method 0, the discrete wavelet transform-based method1, the stationary wavelet transform-based method2, the dual-tree complex wavelet transform-based method 3, the adjustable non-subsampled shearlet transform  and the non-subsampled contourlet transform-based method 4. The spatial domain methods do not require the source image to be converted to another feature domain like the transform domain methods. Spatial domain-based methods operate on pixels or regions directly and use linear or nonlinear methods to fuse the multi-focus images. Spatial domain-based methods can be roughly divided into three categories 5, which often include pixel-based 6, block-based 5 and region-based 7 algorithms. The block-based algorithms decompose the input images into blocks with equal size according to block-based strategy. Then the decomposed blocks are fused by the designed activity level measurement. However, the selected block-size has a great impact on the quality of the fused image, the unsuitable size of block restricts the performance of these block-based algorithms. The region-based algorithms firstly use image segmentation to split up the input images into regions. Then, the clarities of corresponding regions are measured and image fusion is performed by integrating the sharply focused regions. Clearly, the accuracy of image segmentation greatly affects the efficiency of the region-based algorithms. Different from the fusion algorithms mentioned above, pixel-based algorithms directly use activity level measurement strategy to generate the decision map for fusion. Some novel pixel-based spatial domain methods have been proposed, such as multi-scale weighted gradient (MWGF) 8, guided filtering (GFF) 6 and dense SIFT 9. Although the traditional fusion methods have indicated some significant advantages, these fusion methods still exist some weaknesses as follows: (i) The fusion methods rely highly on activity level measurement strategy and transform domain, which leads to poor robustness of these fusion methods. (ii) These fusion methods have poor fusion performance when the input images are complex. To ameliorate these shortcomings, a series of deep learning-based fusion algorithms have been proposed and obtained good results. For example, Liu et al. 0 proposed convolutional neural network (CNN)-based fusion model which can recognize whether the image area is in-focus to generate the decision map for image fusion. Based on this, Tang et al. 1 optimized the structure of CNN from the perspective of pixel to obtain the decision map. Besides, Ma et al. 2 proposed an unsupervised deep learning fusion framework, SESF, to generate a well decision map for image fusion. It is no doubt that these fusion models reached a better performance compared to traditional methods because of the strong and stabilized representation capability. However, the fusion image generated by these methods heavily rely on the decision map refined by some post-processing procedures, which is hard to obtain the fusion image with high quality. Therefore, other fusion models, such as DeepFuse 3, DenseFuse 4 and IFCNN 5 directly fused the deep image features rather than generating the decision map to reconstruct the fusion image. Although the above methods have achieved relatively good performance in image fusion, the new problems which are lacking of rich image details yielded with simple fusion strategy remain unsolved in these image fusion methods. The conclusion of the existing SOTA methods is shown in Table I.\n To address these problems mentioned above, in this paper, A novel fusion model is proposed for multi-focus images. The fusion model can be trained in end-to-end manner without any post-processing procedures, and directly obtain the fusion image without generating the intermediate decision map. To effectively train the fusion model, we have created a large-scale multi-focus image dataset with ground-truth fusion image. Moreover, to obtain a high quality of fusion image, we design a novel fusion strategy based on unity fusion attention mechanism. Specifically, the source images were firstly extracted in feature extraction module by seven stacked convolutional blocks. Then the extracted image features are fused by the proposed fusion strategy. Finally, the fused image features were reconstructed by four stacked convolutional blocks to generate the fusion image. Furthermore, we conducted extensive experiments to evaluate the performance of our proposed fusion model. The results of qualitative evaluation and quantitative evaluation show that our fusion model achieves the remarkable performance, which is superior to the state-of-the-art multi-focus image fusion algorithms. Overall, the main contributions of our work for multi-focus image fusion are fourfold: A large-scale multi-focus image dataset is generated for multi-focus image fusion. The generated multi-focus images in data sets are partially-focused and have different levels of blur, which is suitable for training the fusion model. Moreover, we introduced how to generate the large-scale multi-focus image dataset, which is of great reference significance for making the dataset. A novel and appropriate fusion strategy is designed. The proposed fusion strategy based on proposed unity fusion attention can effectively integrate the extracted image features and meanwhile is more flexible than other simple fusion strategies. An end-to-end fusion framework without any post-processing procedures is proposed. So, all the parameters in the fusion model can be jointly optimized and the fusion network can directly input the fusion image without generating the intermediate decision map. In addition, the fusion framework is very efficient and fast in processing images, substantially ahead of other comparison algorithms. Some expanded experiments on the infrared and visible image dataset and medical image dataset are conducted. Furtherly, the potential applications of our proposed fusion model for other fusion tasks are presented. The remainder of our paper is structured as follows: we briefly review related works about fusion algorithms based on deep learning in Section II. Then Section III describes the proposed fusion framework and introduces the generated large-scale multi-focus image dataset in detail. In Section IV, we present the extensive experimental results and the expanded experiments. We also discuss the application of the proposed framework and the prospects for future research work in Section V. Finally, the conclusion is drawn in Section V. The codes of our proposed UFA-Fusion are accessible on https://github.com/ReckonerInheritor/UFA-Fusion."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Image fusion using adjustable non-subsampled shearlet transform",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "A. Vishwakarma and M. K. Bhuyan"
    },
    {
      "index": 1,
      "title": "Drf: Disentangled representation for visible and infrared image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "H. Xu, X. Wang, and J. Ma"
    },
    {
      "index": 2,
      "title": "Sedrfuse: A symmetric encoderâ€“decoder with residual block network for infrared and visible image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "L. Jian, X. Yang, Z. Liu, G. Jeon, M. Gao, and D. Chisholm"
    },
    {
      "index": 3,
      "title": "Multimodal medical image fusion based on weighted local energy matching measurement and improved spatial frequency",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "Y. Yang, S. Cao, S. Huang, and W. Wan"
    },
    {
      "index": 4,
      "title": "Medical image fusion with parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform domain",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "M. Yin, X. Liu, Y. Liu, and X. Chen"
    },
    {
      "index": 5,
      "title": "Efficient Misalignment-Robust Multi-Focus Microscopical Images Fusion",
      "abstract": "",
      "year": "2019",
      "venue": "Signal Processing",
      "authors": "Y. Liang, Y. Mao, Z. Tang, M. Yan, Y. Zhao, and J. Liu",
      "orig_title": "Efficient misalignment-robust multi-focus microscopical images fusion",
      "paper_id": "1812.08915v1"
    },
    {
      "index": 6,
      "title": "Structural characterization and measurement of nonwoven fabrics based on multi-focus image fusion",
      "abstract": "",
      "year": "2019",
      "venue": "Measurement",
      "authors": "Yang, Chen, Na, Deng, Bin-Jie, Xin, Wen-Yu, Xing, and Zheng-Ye"
    },
    {
      "index": 7,
      "title": "Region-based multifocus image fusion for the precise acquisition of pap smear images",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Biomedical Optics",
      "authors": "S. Tello-Mijares and J. BescÃ³s"
    },
    {
      "index": 8,
      "title": "Image fusion: algorithms and applications",
      "abstract": "",
      "year": "2011",
      "venue": "Elsevier",
      "authors": "T. Stathaki"
    },
    {
      "index": 9,
      "title": "A morphological pyramidal image decomposition",
      "abstract": "",
      "year": "1989",
      "venue": "Pattern Recognition Letters",
      "authors": "A. Toet"
    },
    {
      "index": 10,
      "title": "Multisensor image fusion using the wavelet transform",
      "abstract": "",
      "year": "1995",
      "venue": "Graphical Models and Image Processing",
      "authors": "H. Li, B. Manjunath, and S. K. Mitra"
    },
    {
      "index": 11,
      "title": "A novel image decomposition-based hybrid technique with super-resolution method for multi-focus image fusion",
      "abstract": "",
      "year": "2018",
      "venue": "Information Fusion",
      "authors": "S. Aymaz and C. Kse"
    },
    {
      "index": 12,
      "title": "Pixel- and region-based image fusion with complex wavelets",
      "abstract": "",
      "year": "2007",
      "venue": "Information Fusion",
      "authors": "J. J. Lewis, R. J. Oâ€™Callaghan, S. G. Nikolov, D. R. Bull, and N. Canagarajah"
    },
    {
      "index": 13,
      "title": "Multifocus image fusion using the nonsubsampled contourlet transform",
      "abstract": "",
      "year": "2009",
      "venue": "Signal Processing",
      "authors": "Q. Zhang and B.-l. Guo"
    },
    {
      "index": 14,
      "title": "Quadtree-based multi-focus image fusion using a weighted focus-measure",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "X. Bai, Y. Zhang, F. Zhou, and B. Xue"
    },
    {
      "index": 15,
      "title": "Image fusion with guided filtering",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Image processing",
      "authors": "S. Li, X. Kang, and J. Hu"
    },
    {
      "index": 16,
      "title": "Multifocus image fusion using region segmentation and spatial frequency",
      "abstract": "",
      "year": "2008",
      "venue": "Image and Vision Computing",
      "authors": "S. Li and B. Yang"
    },
    {
      "index": 17,
      "title": "Multi-scale weighted gradient-based fusion for multi-focus images",
      "abstract": "",
      "year": "2014",
      "venue": "Information Fusion",
      "authors": "Z. Zhou, S. Li, and B. Wang"
    },
    {
      "index": 18,
      "title": "Multi-focus image fusion with dense sift",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "Y. Liu, S. Liu, and Z. Wang"
    },
    {
      "index": 19,
      "title": "Multi-focus image fusion with a deep convolutional neural network",
      "abstract": "",
      "year": "2017",
      "venue": "Information Fusion",
      "authors": "Y. Liu, X. Chen, H. Peng, and Z. Wang"
    },
    {
      "index": 20,
      "title": "Pixel convolutional neural network for multi-focus image fusion",
      "abstract": "",
      "year": "2018",
      "venue": "Information Sciences",
      "authors": "H. Tang, B. Xiao, W. Li, and G. Wang"
    },
    {
      "index": 21,
      "title": "Sesf-fuse: An unsupervised deep model for multi-focus image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "authors": "B. Ma, Y. Zhu, X. Yin, X. Ban, H. Huang, and M. Mukeshimana"
    },
    {
      "index": 22,
      "title": "DeepFuse: A Deep Unsupervised Approach for Exposure Fusion with Extreme Exposure Image Pairs",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "authors": "K. R. Prabhakar, V. S. Srikar, and R. V. Babu",
      "orig_title": "Deepfuse: A deep unsupervised approach for exposure fusion with extreme exposure image pairs",
      "paper_id": "1712.07384v1"
    },
    {
      "index": 23,
      "title": "DenseFuse: A Fusion Approach to Infrared and Visible Images",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "H. Li and X. Wu",
      "orig_title": "Densefuse: A fusion approach to infrared and visible images",
      "paper_id": "1804.08361v9"
    },
    {
      "index": 24,
      "title": "Ifcnn: A general image fusion framework based on convolutional neural network",
      "abstract": "",
      "year": "2020",
      "venue": "Information Fusion",
      "authors": "Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, and L. Zhang"
    },
    {
      "index": 25,
      "title": "Remote sensing image fusion using the curvelet transform",
      "abstract": "",
      "year": "2007",
      "venue": "Information Fusion",
      "authors": "F. Nencini, A. Garzelli, S. Baronti, and L. Alparone"
    },
    {
      "index": 26,
      "title": "Robust multi-focus image fusion using edge model and multi-matting",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Y. Chen, J. Guan, and W.-K. Cham"
    },
    {
      "index": 27,
      "title": "Drpl: Deep regression pair learning for multi-focus image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "J. Li, X. Guo, G. Lu, B. Zhang, and D. Zhang"
    },
    {
      "index": 28,
      "title": "Mff-gan: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "Information Fusion",
      "authors": "H. Zhang, Z. Le, Z. Shao, H. Xu, and J. Ma"
    },
    {
      "index": 29,
      "title": "Brain medical image fusion based on dual-branch cnns in nsst domain",
      "abstract": "",
      "year": "2020",
      "venue": "BioMed Research International",
      "authors": "Z. Ding, D. Zhou, R. Nie, R. Hou, and Y. Liu"
    },
    {
      "index": 30,
      "title": "Deepanf: A deep attentive neural framework with distributed representation for chromatin accessibility prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Neurocomputing",
      "authors": "Y. Guo, D. Zhou, R. Nie, X. Ruan, and W. Li"
    },
    {
      "index": 31,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Hu, L. Shen, and G. Sun",
      "orig_title": "Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 32,
      "title": "Multigrained attention network for infrared and visible image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "J. Li, H. Huo, C. Li, R. Wang, C. Sui, and Z. Liu"
    },
    {
      "index": 33,
      "title": "NestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel Attention Models",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "H. Li, X. J. Wu, and T. Durrani",
      "orig_title": "Nestfuse: An infrared and visible image fusion architecture based on nest connection and spatial/channel attention models",
      "paper_id": "2007.00328v2"
    },
    {
      "index": 34,
      "title": "Global-feature encoding u-net (geu-net) for multi-focus image fusion",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "B. Xiao, B. Xu, X. Bi, and W. Li"
    },
    {
      "index": 35,
      "title": "CBAM: Convolutional Block Attention Module",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "S. Woo, J. Park, J. Lee, and I. So Kweon",
      "orig_title": "Cbam: Convolutional block attention module",
      "paper_id": "1807.06521v2"
    },
    {
      "index": 36,
      "title": "Loss Functions for Image Restoration with Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Computational Imaging",
      "authors": "H. Zhao, O. Gallo, I. Frosio, and J. Kautz",
      "orig_title": "Loss functions for image restoration with neural networks",
      "paper_id": "1511.08861v3"
    },
    {
      "index": 37,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "European Conference on Computer Vision",
      "authors": "T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. DollÃ¡r, and C. L. Zitnick",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 38,
      "title": "Image quality assessment: from error visibility to structural similarity",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli"
    },
    {
      "index": 39,
      "title": "Learning to detect salient objects with image-level supervision",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan"
    },
    {
      "index": 40,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 41,
      "title": "Multifocus image fusion and restoration with sparse representation",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "B. Yang and S. Li"
    },
    {
      "index": 42,
      "title": "A new image fusion performance metric based on visual information fidelity",
      "abstract": "",
      "year": "2013",
      "venue": "Information Fusion",
      "authors": "Y. Han, Y. Cai, Y. Cao, and X. Xu"
    },
    {
      "index": 43,
      "title": "An image fusion method based on directional contrast and area-based standard deviation",
      "abstract": "",
      "year": "2005",
      "venue": "Electronic Imaging and Multimedia Technology IV",
      "authors": "G. Liu, W. Chen, and W. Ling"
    },
    {
      "index": 44,
      "title": "Fusion of multispectral and panchromatic satellite images based on contourlet transform and local average gradient",
      "abstract": "",
      "year": "2007",
      "venue": "Optical Engineering",
      "authors": "H. Song, S. Yu, L. Song, and X. Yang"
    },
    {
      "index": 45,
      "title": "Gradient-based/evolutionary relay hybrid for computing pareto front approximations maximizing the s-metric",
      "abstract": "",
      "year": "2007",
      "venue": "International Workshop on Hybrid Metaheuristics",
      "authors": "M. Emmerich, A. Deutz, and N. Beume"
    },
    {
      "index": 46,
      "title": "Improved bounds on the local mean-square error and the bias of parameter estimators (corresp.)",
      "abstract": "",
      "year": "1977",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "M. Wax and J. Ziv"
    },
    {
      "index": 47,
      "title": "A new automated quality assessment algorithm for image fusion",
      "abstract": "",
      "year": "2009",
      "venue": "Image and Vision Computing",
      "authors": "Y. Chen and R. S. Blum"
    },
    {
      "index": 48,
      "title": "A novel image fusion metric based on multi-scale analysis",
      "abstract": "",
      "year": "2008",
      "venue": "2008 9th International Conference on Signal Processing",
      "authors": "P. Wang and B. Liu"
    },
    {
      "index": 49,
      "title": "Multi-focus image fusion using dictionary-based sparse representation",
      "abstract": "",
      "year": "2015",
      "venue": "Information Fusion",
      "authors": "M. Nejati, S. Samavi, and S. Shirani"
    },
    {
      "index": 50,
      "title": "Infrared and visible image fusion via gradient transfer and total variation minimization",
      "abstract": "",
      "year": "2016",
      "venue": "Information Fusion",
      "authors": "J. Ma, C. Chen, C. Li, and J. Huang"
    },
    {
      "index": 51,
      "title": "Perceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with gaussian and bilateral filters",
      "abstract": "",
      "year": "2016",
      "venue": "Information Fusion",
      "authors": "Z. Zhou, B. Wang, S. Li, and M. Dong"
    },
    {
      "index": 52,
      "title": "Vif-net: an unsupervised framework for infrared and visible image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Computational Imaging",
      "authors": "R. Hou, D. Zhou, R. Nie, D. Liu, L. Xiong, Y. Guo, and C. Yu"
    },
    {
      "index": 53,
      "title": "A medical image fusion method based on convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "2017 20th International Conference on Information Fusion (Fusion)",
      "authors": "Y. Liu, X. Chen, J. Cheng, and H. Peng"
    },
    {
      "index": 54,
      "title": "Multimodal medical image fusion using hybrid layer decomposition with cnn-based feature mapping and structural clustering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "S. Singh and R. Anand"
    },
    {
      "index": 55,
      "title": "Laplacian re-decomposition for multimodal medical image fusion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "authors": "X. Li, X. Guo, P. Han, X. Wang, H. Li, and T. Luo"
    },
    {
      "index": 56,
      "title": "On the use of a joint spatial-frequency representation for the fusion of multi-focus images",
      "abstract": "",
      "year": "2005",
      "venue": "Pattern Recognition Letters",
      "authors": "S. Gabarda and G. Cristobal"
    },
    {
      "index": 57,
      "title": "Multifocus image fusion using the log-gabor transform and a multisize windows technique",
      "abstract": "",
      "year": "2009",
      "venue": "Information Fusion",
      "authors": "R. Redondo, F. Sroubek, S. Fischer, and G. CristÃ³bal"
    },
    {
      "index": 58,
      "title": "Multi-focus- ing algorithm for microscopy imagery in assembly line using low-cost camera",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Biomedical Optics",
      "authors": "V. R. L. Juocas, R. D. R. Maskeliunas, and M. Wozniak"
    }
  ]
}