{
  "paper_id": "2009.04965v3",
  "title": "Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations",
  "sections": {
    "iv-d quantitative results on vrd dataset": "\n\n\n\nModel\nRecall@50\nRecall@100\n\n\n\nVisual Phrase \n0.97\n1.91\n\nJoint CNN \n1.47\n2.03\n\nVTransE \n44.76\n44.76\n\nPPR-FCN 0\n47.43\n47.43\n\nLanguage Priors \n47.87\n47.87\n\nZoom-Net 1\n50.69\n50.69\n\nTFR 2\n52.30\n52.30\n\nWeakly (+ Language) [ref]8\n52.60\n52.60\n\nLK Distillation \n55.16\n55.16\n\nJung et al. 3\n55.16\n55.16\n\nUVTransE 4\n55.46\n55.46\n\nMF-URLN 6\n58.20\n58.20\n\nHGAT 8\n59.54\n59.54\n\nRVL-BERT\n55.55\n55.55\n\n \n\n\n\nModel\nOverall\nabove\nbehind\nin\nin front of\nnext to\non\nto the left of\nto the right of\nunder\n\n\n\nL-baseline 9\n60.1\n60.4\n62.0\n54.4\n55.1\n56.8\n63.2\n51.7\n54.1\n70.3\n\nPPR-FCN 0\n66.3\n61.5\n65.2\n70.4\n64.2\n53.4\n72.0\n69.1\n71.9\n59.3\n\nViP-CNN \n67.2\n55.6\n68.1\n66.0\n62.7\n62.3\n72.5\n69.7\n73.3\n66.6\n\nWeakly [ref]8\n67.5\n59.0\n67.1\n69.8\n57.8\n65.7\n75.6\n56.7\n69.2\n66.2\n\nS-baseline 9\n68.8\n58.0\n66.9\n70.7\n63.1\n62.0\n76.0\n66.3\n74.7\n67.9\n\nVTransE \n69.4\n61.5\n69.7\n67.8\n64.9\n57.7\n76.2\n64.6\n68.5\n76.9\n\nL+S-baseline 9\n71.1\n61.1\n67.5\n69.2\n66.2\n64.8\n77.9\n69.7\n74.7\n77.2\n\nDR-Net \n71.3\n62.8\n72.2\n69.8\n66.9\n59.9\n79.4\n63.5\n66.4\n75.9\n\nDSRR††\\dagger 7\n72.7\n61.5\n71.3\n71.3\n67.8\n65.1\n79.8\n67.4\n75.3\n78.6\n\nRVL-BERT\n72.3\n62.5\n70.3\n71.9\n70.2\n65.1\n78.5\n68.0\n74.0\n75.5\n\nHuman Perf. 9\n94.6\n90.0\n96.3\n95.0\n95.8\n94.5\n95.7\n88.8\n93.2\n94.1\n\n We conduct experiments on VRD dataset to compare our method with existing approaches.\nVisual Phrase  represents visual relationships as visual phrases and learns appearance vectors for each category for classification.\nJoint CNN  classifies the objects and predicates using only visual features from bounding boxes.\nVTransE  projects objects and predicates into a low-dimensional space and models visual relationships as a vector translation.\nPPR-FCN 0 uses fully convolutional layers to perform relationship detection.\nLanguage Priors  utilizes individual detectors for objects and predicates and combines the results for classification.\nZoom-Net 1 introduces new RoI Pooling cells to perform message passing between local objects and global predicate features.\nTFR 2 performs a factorization process on the training data and derives relational priors to be used in VRD.\nWeakly [ref]8 adopts a weakly-supervised clustering model to learn relations from image-level labels.\nLK Distillation  introduced external knowledge with a teacher-student knowledge distillation framework.\nJung et al. 3 propose a new spatial vector with element-wise feature combination to improve the performance.\nUVTransE 4 extends the idea of vector translation in VTransE with the contextual information of the bounding boxes.\nMF-URLN 6 uses external linguistic knowledge and internal statistics to explore undetermined relationships.\nHGAT 8 proposes a TransE-based multi-head attention approach performed on a fully-connected graph. Table III shows the performance comparison on the VRD dataset.555Note that for the results other than Visual Phrases and Joint CNN, Recall@50 is equivalent to Recall@100 (also observed in  ) because the number of ground truth subject-object pairs is less than 50.\nIt can be seen that our RVL-BERT achieves competitive Recall@50/100 (53.07/55.55) compared to most of the existing methods, while lags behind the latest state-of-the-art, such as MF-URLN and HGAT. We note that the use of an additional graph attention network in HGAT and a confidence-weighting module in MF-URLN can be possibly incorporated into our design, while we leave for future work."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Recognition using visual phrases",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. A. Sadeghi and A. Farhadi"
    },
    {
      "index": 1,
      "title": "Understanding kin relationships in a photo",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "S. Xia, M. Shao, J. Luo, and Y. Fu"
    },
    {
      "index": 2,
      "title": "Visual Relationship Detection with Language Priors",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Computer Vision",
      "authors": "C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei",
      "orig_title": "Visual relationship detection with language priors",
      "paper_id": "1608.00187v1"
    },
    {
      "index": 3,
      "title": "Vip-cnn: Visual phrase guided convolutional neural network",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Y. Li, W. Ouyang, X. Wang, and X. Tang"
    },
    {
      "index": 4,
      "title": "Towards context-aware interaction recognition for visual relationship detection",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "B. Zhuang, L. Liu, C. Shen, and I. Reid"
    },
    {
      "index": 5,
      "title": "Detecting Visual Relationships with Deep Relational Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "B. Dai, Y. Zhang, and D. Lin",
      "orig_title": "Detecting visual relationships with deep relational networks",
      "paper_id": "1704.03114v2"
    },
    {
      "index": 6,
      "title": "Visual Translation Embedding Network for Visual Relation Detection",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua",
      "orig_title": "Visual translation embedding network for visual relation detection",
      "paper_id": "1702.08319v1"
    },
    {
      "index": 7,
      "title": "Weakly-supervised learning of visual relations",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "J. Peyre, J. Sivic, I. Laptev, and C. Schmid"
    },
    {
      "index": 8,
      "title": "Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "R. Yu, A. Li, V. I. Morariu, and L. S. Davis",
      "orig_title": "Visual relationship detection with internal and external linguistic knowledge distillation",
      "paper_id": "1707.09423v2"
    },
    {
      "index": 9,
      "title": "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang",
      "orig_title": "Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn",
      "paper_id": "1708.01956v1"
    },
    {
      "index": 10,
      "title": "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, J. Shao, and C. Change Loy",
      "orig_title": "Zoom-net: Mining deep feature interactions for visual relationship recognition",
      "paper_id": "1807.04979v1"
    },
    {
      "index": 11,
      "title": "Tensorize, factorize and regularize: Robust visual relationship learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Jae Hwang, S. N. Ravi, Z. Tao, H. J. Kim, M. D. Collins, and V. Singh"
    },
    {
      "index": 12,
      "title": "Visual relationship detection with language prior and softmax",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE International Conference on Image Processing, Applications and Systems (IPAS)",
      "authors": "J. Jung and J. Park"
    },
    {
      "index": 13,
      "title": "Union visual translation embedding for visual relationship detection and scene graph generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.11624",
      "authors": "Z.-S. Hung, A. Mallya, and S. Lazebnik"
    },
    {
      "index": 14,
      "title": "Scene graph generation with external knowledge and image reconstruction",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Gu, H. Zhao, Z. Lin, S. Li, J. Cai, and M. Ling"
    },
    {
      "index": 15,
      "title": "On Exploring Undetermined Relationships for Visual Relationship Detection",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Y. Zhan, J. Yu, T. Yu, and D. Tao",
      "orig_title": "On exploring undetermined relationships for visual relationship detection",
      "paper_id": "1905.01595v1"
    },
    {
      "index": 16,
      "title": "Exploring depth information for spatial relation recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)",
      "authors": "X. Ding, Y. Li, Y. Pan, D. Zeng, and T. Yao"
    },
    {
      "index": 17,
      "title": "Hierarchical graph attention network for visual relationship detection",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "L. Mi and Z. Chen"
    },
    {
      "index": 18,
      "title": "Scene Graph Generation by Iterative Message Passing",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei",
      "orig_title": "Scene graph generation by iterative message passing",
      "paper_id": "1701.02426v2"
    },
    {
      "index": 19,
      "title": "Scene Graph Generation from Objects, Phrases and Region Captions",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Y. Li, W. Ouyang, B. Zhou, K. Wang, and X. Wang",
      "orig_title": "Scene graph generation from objects, phrases and region captions",
      "paper_id": "1707.09700v2"
    },
    {
      "index": 20,
      "title": "Neural Motifs: Scene Graph Parsing with Global Context",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "R. Zellers, M. Yatskar, S. Thomson, and Y. Choi",
      "orig_title": "Neural motifs: Scene graph parsing with global context",
      "paper_id": "1711.06640v2"
    },
    {
      "index": 21,
      "title": "Graph R-CNN for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh",
      "orig_title": "Graph r-cnn for scene graph generation",
      "paper_id": "1808.00191v1"
    },
    {
      "index": 22,
      "title": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang",
      "orig_title": "Factorizable net: an efficient subgraph-based framework for scene graph generation",
      "paper_id": "1806.11538v2"
    },
    {
      "index": 23,
      "title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "R. Herzig, M. Raboh, G. Chechik, J. Berant, and A. Globerson",
      "orig_title": "Mapping images to scene graphs with permutation-invariant structured prediction",
      "paper_id": "1802.05451v4"
    },
    {
      "index": 24,
      "title": "Knowledge-Embedded Routing Network for Scene Graph Generation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "T. Chen, W. Yu, R. Chen, and L. Lin",
      "orig_title": "Knowledge-embedded routing network for scene graph generation",
      "paper_id": "1903.03326v1"
    },
    {
      "index": 25,
      "title": "Graphical Contrastive Losses for Scene Graph Parsing",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Zhang, K. J. Shih, A. Elgammal, A. Tao, and B. Catanzaro",
      "orig_title": "Graphical contrastive losses for scene graph parsing",
      "paper_id": "1903.02728v5"
    },
    {
      "index": 26,
      "title": "Attentive Relational Networks for Mapping Images to Scene Graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. Qi, W. Li, Z. Yang, Y. Wang, and J. Luo",
      "orig_title": "Attentive relational networks for mapping images to scene graphs",
      "paper_id": "1811.10696v2"
    },
    {
      "index": 27,
      "title": "Bridging knowledge graphs to generate scene graphs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.02314",
      "authors": "A. Zareian, S. Karaman, and S.-F. Chang"
    },
    {
      "index": 28,
      "title": "Unbiased Scene Graph Generation from Biased Training",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "K. Tang, Y. Niu, J. Huang, J. Shi, and H. Zhang",
      "orig_title": "Unbiased scene graph generation from biased training",
      "paper_id": "2002.11949v3"
    },
    {
      "index": 29,
      "title": "Know more say less: Image captioning based on scene graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "X. Li and S. Jiang"
    },
    {
      "index": 30,
      "title": "Visual relationship embedding network for image paragraph generation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "W. Che, X. Fan, R. Xiong, and D. Zhao"
    },
    {
      "index": 31,
      "title": "Graph-Structured Representations for Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "D. Teney, L. Liu, and A. van Den Hengel",
      "orig_title": "Graph-structured representations for visual question answering",
      "paper_id": "1609.05600v2"
    },
    {
      "index": 32,
      "title": "Explainable and Explicit Visual Reasoning over Scene Graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Shi, H. Zhang, and J. Li",
      "orig_title": "Explainable and explicit visual reasoning over scene graphs",
      "paper_id": "1812.01855v2"
    },
    {
      "index": 33,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 34,
      "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "J. Lu, D. Batra, D. Parikh, and S. Lee"
    },
    {
      "index": 35,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "P. Sharma, N. Ding, S. Goodman, and R. Soricut"
    },
    {
      "index": 36,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai"
    },
    {
      "index": 37,
      "title": "Interact as You Intend: Intention-Driven Human-Object Interaction Detection",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "B. Xu, J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli",
      "orig_title": "Interact as you intend: Intention-driven human-object interaction detection",
      "paper_id": "1808.09796v2"
    },
    {
      "index": 38,
      "title": "Context-associative hierarchical memory model for human activity recognition and prediction",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "L. Wang, X. Zhao, Y. Si, L. Cao, and Y. Liu"
    },
    {
      "index": 39,
      "title": "Image retrieval using scene graphs",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein, and L. Fei-Fei"
    },
    {
      "index": 40,
      "title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval",
      "abstract": "",
      "year": "2015",
      "venue": "fourth Workshop on Vision and Language",
      "authors": "S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning"
    },
    {
      "index": 41,
      "title": "Ask me anything: Dynamic memory networks for natural language processing",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani, V. Zhong, R. Paulus, and R. Socher"
    },
    {
      "index": 42,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 43,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
      "authors": "M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer",
      "orig_title": "Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 44,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.14165",
      "authors": "T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 45,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler"
    },
    {
      "index": 46,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 47,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.08415",
      "authors": "D. Hendrycks and K. Gimpel",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 48,
      "title": "SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "K. Yang, O. Russakovsky, and J. Deng",
      "orig_title": "Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition",
      "paper_id": "1908.02660v2"
    },
    {
      "index": 49,
      "title": "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.08144",
      "authors": "Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al.",
      "orig_title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 50,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "K. He, G. Gkioxari, P. Dollár, and R. Girshick",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 51,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International Journal of Computer Vision",
      "authors": "R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al."
    },
    {
      "index": 52,
      "title": "Fast R-CNN",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "R. Girshick",
      "orig_title": "Fast r-cnn",
      "paper_id": "1504.08083v2"
    },
    {
      "index": 53,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "S. Ren, K. He, R. Girshick, and J. Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 54,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 55,
      "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "K. Tang, H. Zhang, B. Wu, W. Luo, and W. Liu",
      "orig_title": "Learning to compose dynamic tree structures for visual contexts",
      "paper_id": "1812.01880v1"
    }
  ]
}