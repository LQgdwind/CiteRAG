{
  "paper_id": "2006.07214v3",
  "title": "Sparse and Continuous Attention Mechanisms",
  "sections": {
    "appendix c relation to the tsallis maxent principle": "We discuss here the relation between the (2âˆ’Î±)2ğ›¼(2-\\alpha)-exponential family of distributions as presented in Prop.Â 2.3 and the distributions arising from the Tsallis maxent principle [ref]13. We put in perspective the related work in statistical physics  , information geometry  , and the discrete case presented in the machine learning literature [ref]7 [ref]8. We start by noting that our Î±ğ›¼\\alpha parameter matches the Î±ğ›¼\\alpha used in prior machine learning literature related to the â€œÎ±ğ›¼\\alpha-entmax transformationâ€ [ref]7 [ref]8. In the definition of Tsallis entropies (6), our Î±ğ›¼\\alpha corresponds to the entropic index qğ‘q defined by Tsallis [ref]13.\nHowever, our (2âˆ’Î±)2ğ›¼(2-\\alpha)-exponential families correspond to the qğ‘q-exponential families as defined by Naudts , and to the tğ‘¡t-exponential families described by Ding and Vishwanathan  (which include the tğ‘¡t-Student distribution). The family of Amariâ€™s Î±ğ›¼\\alpha-divergences relates to this qğ‘q as Î±=2â€‹qâˆ’1ğ›¼2ğ‘1\\alpha=2q-1 [18, Â§4.3]. These differences in notation have historical reasons, and they are explained by the different ways in which Tsallis entropies relate to qğ‘q-exponential families.\nIn fact, the physics literature has defined qğ‘q-exponential distributions in two distinct ways, as we next describe. Note first that the Î©Î©\\Omega-RPM in our Def.Â 2.1 is a generalization of the free energy variational principle, if we see âˆ’fÎ¸â€‹(t)=âˆ’Î¸âŠ¤â€‹Ï•â€‹(t)subscriptğ‘“ğœƒğ‘¡superscriptğœƒtopitalic-Ï•ğ‘¡-f_{\\theta}(t)=-\\theta^{\\top}\\phi(t) as an energy function and Î©Î©\\Omega the entropy scaled by a temperature.\nLet Î©=Î©Î±Î©subscriptÎ©ğ›¼\\Omega=\\Omega_{\\alpha} be the Tsallis Î±ğ›¼\\alpha-entropy.\nAn equivalent constrained version of this problem is the maximum entropy (maxent) principle : The solution of this problem corresponds to a distribution in the (2âˆ’Î±)2ğ›¼(2-\\alpha)-exponential family (7): for some Lagrange multiplier Î¸ğœƒ\\theta. However, this construction differs from the one by Tsallis [ref]13 and others, who use escort distributions (Eq.Â 5) in the expectation constraints. Namely, instead of (27), they consider the problem: The solution of (29) is of the form where Î¸ğœƒ\\theta is again a Lagrange multiplier. This is derived, for example, in [50, Eq.Â 15].\nThere are two main differences between (28) and (30): While (28) involves the (2âˆ’Î±)2ğ›¼(2-\\alpha)-exponential, (30) involves the Î±ğ›¼\\alpha-exponential. In (28), the normalizing term AÎ±â€‹(Î¸)subscriptğ´ğ›¼ğœƒA_{\\alpha}(\\theta) is inside the (2âˆ’Î±)2ğ›¼(2-\\alpha)-exponential. In (30), there is an normalizing factor BÎ±â€‹(Î¸)subscriptğµğ›¼ğœƒB_{\\alpha}(\\theta) outside the Î±ğ›¼\\alpha-exponential. Naturally, when Î±=1ğ›¼1\\alpha=1, these two problems become equivalent, since an additive term inside the exponential is equivalent to a multiplicative term outside. However, this does not happen with Î²ğ›½\\beta-exponentials (expÎ²â¡(u+v)â‰ expÎ²â¡(u)â€‹expÎ²â¡(v)subscriptğ›½ğ‘¢ğ‘£subscriptğ›½ğ‘¢subscriptğ›½ğ‘£\\exp_{\\beta}(u+v)\\neq\\exp_{\\beta}(u)\\exp_{\\beta}(v) in general, for Î²â‰ 1ğ›½1\\beta\\neq 1), and therefore these two alternative paths lead to two different definitions of qğ‘q-exponential families. Unfortunately, both have been considered in the physics literature, under the same name, and\nthis has been subject of debate. Quoting Naudts [10, Â§1]: â€œAn important question is then whether in the modification the normalization should stand in front of the deformed exponential function, or whether it should be included as lnâ¡Zâ€‹(Î²)ğ‘ğ›½\\ln Z(\\beta) inside. From the general formalism mentioned above it follows\nthat the latter is the right way to go.â€ Throughout our paper, we use the definition of  , equivalent to the maxent problem (27)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory",
      "abstract": "",
      "year": "1986",
      "venue": "Institute of Mathematical Statistics",
      "authors": "Lawrence D Brown"
    },
    {
      "index": 1,
      "title": "Information and Exponential Families in Statistical Theory",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Ole Barndorff-Nielsen"
    },
    {
      "index": 2,
      "title": "Sufficient statistics and intrinsic accuracy",
      "abstract": "",
      "year": "1936",
      "venue": "Mathematical Proceedings of the Cambridge Philosophical Society",
      "authors": "Edwin James George Pitman"
    },
    {
      "index": 3,
      "title": "Sur les lois de probabilitÃ©a estimation exhaustive",
      "abstract": "",
      "year": "1935",
      "venue": "CR Acad. Sci. Paris",
      "authors": "Georges Darmois"
    },
    {
      "index": 4,
      "title": "On distributions admitting a sufficient statistic",
      "abstract": "",
      "year": "1936",
      "venue": "Transactions of the American Mathematical society",
      "authors": "Bernard Osgood Koopman"
    },
    {
      "index": 5,
      "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "AndrÃ© FT Martins and RamÃ³n Fernandez Astudillo"
    },
    {
      "index": 6,
      "title": "Learning with Fenchel-Young Losses",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "Mathieu Blondel, AndrÃ© FT Martins, and Vlad Niculae",
      "orig_title": "Learning with fenchel-young losses",
      "paper_id": "1901.02324v2"
    },
    {
      "index": 7,
      "title": "Sparse Sequence-to-Sequence Models",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Ben Peters, Vlad Niculae, and AndrÃ© F.T. Martins",
      "orig_title": "Sparse sequence-to-sequence models",
      "paper_id": "1905.05702v2"
    },
    {
      "index": 8,
      "title": "Adaptively Sparse Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "authors": "GonÃ§alo M Correia, Vlad Niculae, and AndrÃ© FT Martins",
      "orig_title": "Adaptively sparse transformers",
      "paper_id": "1909.00015v2"
    },
    {
      "index": 9,
      "title": "The q-exponential family in statistical physics",
      "abstract": "",
      "year": "2009",
      "venue": "Central European Journal of Physics",
      "authors": "Jan Naudts"
    },
    {
      "index": 10,
      "title": "Generalized Maximum Entropy, Convexity and Machine Learning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Timothy Sears"
    },
    {
      "index": 11,
      "title": "t-logistic regression",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nan Ding and S.V.N. Vishwanathan"
    },
    {
      "index": 12,
      "title": "Possible generalization of Boltzmann-Gibbs statistics",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Statistical Physics",
      "authors": "Constantino Tsallis"
    },
    {
      "index": 13,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 14,
      "title": "Measure Theory, volume 18",
      "abstract": "",
      "year": "2013",
      "venue": "Springer",
      "authors": "Paul R Halmos"
    },
    {
      "index": 15,
      "title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition",
      "abstract": "",
      "year": "1990",
      "venue": "Neurocomputing",
      "authors": "John S. Bridle"
    },
    {
      "index": 16,
      "title": "Elements of Information Theory",
      "abstract": "",
      "year": "2012",
      "venue": "John Wiley & Sons",
      "authors": "Thomas M Cover and Joy A Thomas"
    },
    {
      "index": 17,
      "title": "Information geometry and its applications, volume 194",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Shun-ichi Amari"
    },
    {
      "index": 18,
      "title": "Quantification method of classification processes. concept of structural ağ‘a-entropy",
      "abstract": "",
      "year": "1967",
      "venue": "Kybernetika",
      "authors": "Jan Havrda and FrantiÅ¡ek CharvÃ¡t"
    },
    {
      "index": 19,
      "title": "Entropy and diversity",
      "abstract": "",
      "year": "2006",
      "venue": "Oikos",
      "authors": "L. Jost"
    },
    {
      "index": 20,
      "title": "Gini-Simpson index of diversity: a characterization, generalization, and applications",
      "abstract": "",
      "year": "1982",
      "venue": "Utilitas Mathematics",
      "authors": "R.A. Rao"
    },
    {
      "index": 21,
      "title": "Adaptive sparseness using Jeffreys prior",
      "abstract": "",
      "year": "2001",
      "venue": "NeurIPS",
      "authors": "M. Figueiredo"
    },
    {
      "index": 22,
      "title": "Sparse Bayesian learning and the relevance vector machine",
      "abstract": "",
      "year": "2001",
      "venue": "Journal of Machine Learning Research",
      "authors": "M. Tipping"
    },
    {
      "index": 23,
      "title": "Geometry for q-exponential families",
      "abstract": "",
      "year": "2012",
      "venue": "Recent Progress in Differential Geometry and its Related Fields",
      "authors": "Hiroshi Matsuzoe and Atsumi Ohara"
    },
    {
      "index": 24,
      "title": "Geometry of q-exponential family of probability distributions",
      "abstract": "",
      "year": "2011",
      "venue": "Entropy",
      "authors": "Shun-ichi Amari and Atsumi Ohara"
    },
    {
      "index": 25,
      "title": "Non-parametric estimation of a multivariate probability density",
      "abstract": "",
      "year": "1969",
      "venue": "Theory of Probability & Its Applications",
      "authors": "Vassiliy A Epanechnikov"
    },
    {
      "index": 26,
      "title": "End-to-end memory networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al."
    },
    {
      "index": 27,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 28,
      "title": "Learning word vectors for sentiment analysis",
      "abstract": "",
      "year": "2011",
      "venue": "NAACL-HLT",
      "authors": "Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts"
    },
    {
      "index": 29,
      "title": "Overview of the IWSLT 2017 evaluation campaign",
      "abstract": "",
      "year": "2017",
      "venue": "IWSLT",
      "authors": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, StÃ¼ker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian"
    },
    {
      "index": 30,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "International Journal of Computer Vision",
      "authors": "Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 31,
      "title": "Deep Modular Co-Attention Networks for Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian",
      "orig_title": "Deep modular co-attention networks for visual question answering",
      "paper_id": "1906.10770v1"
    },
    {
      "index": 32,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 33,
      "title": "Deep Parametric Continuous Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun",
      "orig_title": "Deep parametric continuous convolutional neural networks",
      "paper_id": "2101.06742v1"
    },
    {
      "index": 34,
      "title": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Kristof SchÃ¼tt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert MÃ¼ller"
    },
    {
      "index": 35,
      "title": "Approximation of dynamical systems by continuous time recurrent neural networks",
      "abstract": "",
      "year": "1993",
      "venue": "Neural networks",
      "authors": "Ken-ichi Funahashi and Yuichi Nakamura"
    },
    {
      "index": 36,
      "title": "Latent ordinary differential equations for irregularly-sampled time series",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yulia Rubanova, Tian Qi Chen, and David K Duvenaud"
    },
    {
      "index": 37,
      "title": "Neural Ordinary Differential Equations",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud",
      "orig_title": "Neural ordinary differential equations",
      "paper_id": "1806.07366v5"
    },
    {
      "index": 38,
      "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Sachin Kumar and Yulia Tsvetkov",
      "orig_title": "Von mises-fisher loss for training sequence to sequence models with continuous outputs",
      "paper_id": "1812.04616v3"
    },
    {
      "index": 39,
      "title": "On the Relationship between Self-Attention and Convolutional Layers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi",
      "orig_title": "On the relationship between self-attention and convolutional layers",
      "paper_id": "1911.03584v2"
    },
    {
      "index": 40,
      "title": "Hard-Coded Gaussian Attention for Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Weiqiu You, Simeng Sun, and Mohit Iyyer",
      "orig_title": "Hard-coded Gaussian attention for neural machine translation",
      "paper_id": "2005.00742v1"
    },
    {
      "index": 41,
      "title": "DRAW: A Recurrent Neural Network For Image Generation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra",
      "orig_title": "Draw: A recurrent neural network for image generation",
      "paper_id": "1502.04623v2"
    },
    {
      "index": 42,
      "title": "Attention is not explanation",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Sarthak Jain and Byron C Wallace"
    },
    {
      "index": 43,
      "title": "Is Attention Interpretable?",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Sofia Serrano and Noah A Smith",
      "orig_title": "Is attention interpretable?",
      "paper_id": "1906.03731v1"
    },
    {
      "index": 44,
      "title": "Attention is not not explanation",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "Sarah Wiegreffe and Yuval Pinter"
    },
    {
      "index": 45,
      "title": "Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez",
      "orig_title": "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
      "paper_id": "1811.08489v4"
    },
    {
      "index": 46,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Emma Strubell, Ananya Ganesh, and Andrew McCallum",
      "orig_title": "Energy and policy considerations for deep learning in nlp",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 47,
      "title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "abstract": "",
      "year": "2011",
      "venue": "Springer",
      "authors": "Heinz Bauschke and Patrick Combettes"
    },
    {
      "index": 48,
      "title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming",
      "abstract": "",
      "year": "1967",
      "venue": "USSR Computational Mathematics and Mathematical Physics",
      "authors": "Lev M Bregman"
    },
    {
      "index": 49,
      "title": "Geometry of escort distributions",
      "abstract": "",
      "year": "2003",
      "venue": "Physical Review E",
      "authors": "Sumiyoshi Abe"
    },
    {
      "index": 50,
      "title": "Information theory and statistical mechanics",
      "abstract": "",
      "year": "1957",
      "venue": "Physical review",
      "authors": "Edwin T Jaynes"
    },
    {
      "index": 51,
      "title": "Neural machine translation of rare words with subword units",
      "abstract": "",
      "year": "2016",
      "venue": "ACL",
      "authors": "Rico Sennrich, Barry Haddow, and Alexandra Birch"
    },
    {
      "index": 52,
      "title": "Joey nmt: A minimalist nmt toolkit for novices",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP: System Demonstrations",
      "authors": "Julia Kreutzer, Joost Bastings, and Stefan Riezler"
    },
    {
      "index": 53,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 54,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei",
      "orig_title": "ImageNet Large Scale Visual Recognition Challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 55,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang",
      "orig_title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 56,
      "title": "Glove: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning"
    }
  ]
}