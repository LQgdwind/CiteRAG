{
  "paper_id": "2003.06709v5",
  "title": "FACMAC: Factored Multi-Agent Centralised Policy Gradients",
  "sections": {
    "related work": "Value function factorisation  has been widely employed in value-based MARL algorithms.\nVDN  and QMIX  factor the joint action-value function into per-agent utilities that are combined via a simple summation or a monotonic mixing function respectively, to ensure consistency between the arg​maxargmax\\operatorname*{arg\\,max} of the centralised joint-action value function and the arg​maxargmax\\operatorname*{arg\\,max} of the decentralised polices. This monotonicity constraint, however, prevents them from representing joint action-value functions that are characterised as nonmonotonic , i.e., an agent’s ordering over its own actions depends on other agents’ actions.\nA large number of recent works   1   thus focus on developing new value-based MARL algorithms that address this representational limitation, in order to learn a richer class of action-value functions. QTRAN  learns an\nunrestricted joint action-value function and aims to solve a constrained optimisation problem in order to decentralise it, but has been shown to scale poorly to more complex tasks such as SMAC .\nQPLEX  takes advantage of the dueling network architecture to factor the joint action-value function in a manner that does not restrict the representational capacity, whilst also remaining easily decentralisable, but can still fail to solve simple tasks with nonmonotonic value functions 1. Weighted QMIX 1 introduces a weighting scheme to place more importance on the better joint actions to learn a richer class of joint action-value functions. QTRAN++  addresses the gap between the empirical performance and theoretical guarantees of QTRAN. Our multi-agent actor-critic framework with decentralised actors and a centralised but factored critic, by contrast, provides a more direct and simpler way of coping with nonmonotonic tasks as one can simply factor the centralised critic in any manner without constraints. Additionally, our framework can be readily applied to tasks with continuous action spaces, whereas these value-based algorithms require additional algorithmic changes. Most state-of-the-art multi-agent actor-critic methods [ref]21 [ref]7   learn a centralised and monolithic critic conditioning on the global state and the joint action to stabilise learning. Even though the joint action-value function they can represent is not restricted, in practice they significantly underperform value-based methods like QMIX on the challenging SMAC benchmark 1 . In contrast, FACMAC utilises a centralised but factored critic to allow it to scale to the more complex tasks in SMAC, and follows the centralised policy gradient instead of per-agent policy gradients. Lyu et al.  recently provide some interesting insights about the pros and cons of centralised and decentralised critics for on-policy actor-critic algorithms. One important issue that they highlight is that merely utilising a centralised critic does not necessarily lead to the learning of more coordinated behaviours.\nThis is because the use of a per-agent policy gradient can lead to the agents getting stuck in sub-optimal solutions in which no one agents wishes to change their policy, as discussed in 3.2. Our centralised policy gradient resolves this issue by taking full advantage of the centralised training paradigm to optimise over the joint-action policy, which allows us to reap the benefits of a centralised critic. Since FACMAC is off-policy, we also benefit immensely from utilising a centralised critic over a decentralised one since we avoid the issues of non-stationarity when training on older data. Zhou et al.  propose to use a single centralised critic for MADDPG, whose weights are generated by hypernetworks that condition on the state, similarly to QMIX’s mixing network without the monotonicity constraints. FACMAC also uses a single centralised critic, but factorises it similarly to QMIX (not just using the mixing network) which allows for more efficient learning on more complex tasks. Of existing work, the deterministic decomposed policy gradients (DOP) algorithm proposed by Wang et al.  is perhaps most similar to our own approach. Deterministic DOP is off-policy and factors the centralised critic as a weighted linear sum of individual agent utilities and a state bias. It is limited to only considering linearly factored critics, which have limited representational capacity, whilst we are free to choose any method of factorisation in FACMAC to allow for the learning of a richer class of action-value functions.\nWhile they claim to be the first to introduce the idea of value function factorisation into the multi-agent actor-critic framework, it is actually first explored by Bescuca , where a monotonically factored critic is learned for COMA [ref]7. However, their performance improvement on SMAC is limited since COMA requires on-policy learning and it is not straightforward to extend COMA to continuous action spaces.\nFurthermore, both works only consider monotonically factored critics, whilst we employ a nonmonotonic factorisation and demonstrate its benefits.\nWe also investigate the benefits of learning a centralised but factored critic more thoroughly, providing a better understanding about the type of tasks that can benefit more from a factored critic. Furthermore, both deterministic DOP and LICA  use a naive adaptation of the deterministic policy gradient used by MADDPG and suffer from the same problems as discussed in Section 3.2, while our centralised policy gradients allow for better coordination across agents in certain tasks."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01465",
      "authors": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama",
      "orig_title": "Reducing overestimation bias in multi-agent domains using double centralized critics",
      "paper_id": "1910.01465v2"
    },
    {
      "index": 1,
      "title": "Input Invex Neural Network",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "Brandon Amos, Lei Xu, and J Zico Kolter",
      "orig_title": "Input convex neural networks",
      "paper_id": "2106.08748v4"
    },
    {
      "index": 2,
      "title": "Factorised critics in deep multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Master Thesis, University of Oxford",
      "authors": "Marilena Bescuca"
    },
    {
      "index": 3,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 4,
      "title": "A tutorial on the cross-entropy method",
      "abstract": "",
      "year": "2005",
      "venue": "Annals of operations research",
      "authors": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein"
    },
    {
      "index": 5,
      "title": "LIIR: Learning individual intrinsic reward in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao"
    },
    {
      "index": 6,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 7,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine",
      "orig_title": "Continuous deep q-learning with model-based acceleration",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 8,
      "title": "Cooperative multi-agent control using deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer"
    },
    {
      "index": 9,
      "title": "Hypernetworks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.09106",
      "authors": "David Ha, Andrew Dai, and Quoc V Le"
    },
    {
      "index": 10,
      "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Shariq Iqbal and Fei Sha",
      "orig_title": "Actor-attention-critic for multi-agent reinforcement learning",
      "paper_id": "1810.02912v2"
    },
    {
      "index": 11,
      "title": "Categorical reparameterization with gumbel-softmax",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.01144",
      "authors": "Eric Jang, Shixiang Gu, and Ben Poole"
    },
    {
      "index": 12,
      "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.10293",
      "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al.",
      "orig_title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
      "paper_id": "1806.10293v3"
    },
    {
      "index": 13,
      "title": "Robocup: A challenge problem for ai",
      "abstract": "",
      "year": "1997",
      "venue": "AI magazine",
      "authors": "Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, Eiichi Osawa, and Hitoshi Matsubara"
    },
    {
      "index": 14,
      "title": "Computing factored value functions for policies in structured mdps",
      "abstract": "",
      "year": "1999",
      "venue": "IJCAI",
      "authors": "Daphne Koller and Ronald Parr"
    },
    {
      "index": 15,
      "title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning",
      "abstract": "",
      "year": "2016",
      "venue": "Neurocomputing",
      "authors": "Landon Kraemer and Bikramjit Banerjee"
    },
    {
      "index": 16,
      "title": "Distributed self-reconfiguration of m-tran iii modular robotic system",
      "abstract": "",
      "year": "2008",
      "venue": "The International Journal of Robotics Research",
      "authors": "Haruhisa Kurokawa, Kohji Tomita, Akiya Kamimura, Shigeru Kokaji, Takashi Hasuo, and Satoshi Murata"
    },
    {
      "index": 17,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "4th International Conference on Learning Representations, ICLR",
      "authors": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra"
    },
    {
      "index": 18,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Long-Ji Lin"
    },
    {
      "index": 19,
      "title": "Emergent coordination through competition",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.07151",
      "authors": "Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel"
    },
    {
      "index": 20,
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch",
      "orig_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
      "paper_id": "1706.02275v4"
    },
    {
      "index": 21,
      "title": "Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and Multi-Agent Systems",
      "authors": "Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato",
      "orig_title": "Contrasting centralized and decentralized critics in multi-agent reinforcement learning",
      "paper_id": "2102.04402v2"
    },
    {
      "index": 22,
      "title": "MAVEN: Multi-Agent Variational Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson",
      "orig_title": "Maven: Multi-agent variational exploration",
      "paper_id": "1910.07483v2"
    },
    {
      "index": 23,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al."
    },
    {
      "index": 24,
      "title": "Chainform: A linear integrated modular hardware system for shape changing interfaces",
      "abstract": "",
      "year": "2016",
      "venue": "29th Annual Symposium on User Interface Software and Technology",
      "authors": "Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A Paradiso, and Hiroshi Ishii"
    },
    {
      "index": 25,
      "title": "Optimal and approximate Q-value functions for decentralized pomdps",
      "abstract": "",
      "year": "2008",
      "venue": "JAIR",
      "authors": "Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos Vlassis"
    },
    {
      "index": 26,
      "title": "A concise introduction to decentralized POMDPs, volume 1",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Frans A Oliehoek, Christopher Amato, et al."
    },
    {
      "index": 27,
      "title": "openai/baselines, May 2020.",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 28,
      "title": "Softmax with regularization: Better value estimation in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.11883",
      "authors": "Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson"
    },
    {
      "index": 29,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 30,
      "title": "Weighted qmix: Expanding monotonic value function factorisation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson"
    },
    {
      "index": 31,
      "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "JMLR",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "2003.08839v2"
    },
    {
      "index": 32,
      "title": "Reinforcement learning for robot soccer",
      "abstract": "",
      "year": "2009",
      "venue": "Autonomous Robots",
      "authors": "Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange"
    },
    {
      "index": 33,
      "title": "The StarCraft Multi-Agent Challenge",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson",
      "orig_title": "The StarCraft Multi-Agent Challenge",
      "paper_id": "1902.04043v5"
    },
    {
      "index": 34,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.05408",
      "authors": "Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 35,
      "title": "Qtran++: Improved value transformation for cooperative multi-agent reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12010",
      "authors": "Kyunghwan Son, Sungsoo Ahn, Roben Delos Reyes, Jinwoo Shin, and Yung Yi"
    },
    {
      "index": 36,
      "title": "Scaling reinforcement learning toward RoboCup soccer",
      "abstract": "",
      "year": "2001",
      "venue": "Icml",
      "authors": "Peter Stone and Richard S. Sutton"
    },
    {
      "index": 37,
      "title": "Value-decomposition networks for cooperative multi-agent learning based on team reward",
      "abstract": "",
      "year": "2018",
      "venue": "AAMAS",
      "authors": "Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al."
    },
    {
      "index": 38,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Emanuel Todorov, Tom Erez, and Yuval Tassa"
    },
    {
      "index": 39,
      "title": "Towards understanding linear value decomposition in cooperative multi-agent q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.00587",
      "authors": "Jianhao Wang, Zhizhou Ren, Beining Han, and Chongjie Zhang"
    },
    {
      "index": 40,
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.01062",
      "authors": "Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang",
      "orig_title": "Qplex: Duplex dueling multi-agent q-learning",
      "paper_id": "2008.01062v3"
    },
    {
      "index": 41,
      "title": "NerveNet: learning structured policy with graph neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR",
      "authors": "Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler"
    },
    {
      "index": 42,
      "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
      "abstract": "",
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR",
      "authors": "Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang",
      "orig_title": "Dop: Off-policy multi-agent decomposed policy gradients",
      "paper_id": "2007.12322v2"
    },
    {
      "index": 43,
      "title": "Lenient learning in independent-learner stochastic cooperative games",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Ermo Wei and Sean Luke"
    },
    {
      "index": 44,
      "title": "Design and architecture of the unified modular snake robot",
      "abstract": "",
      "year": "2012",
      "venue": "2012 IEEE International Conference on Robotics and Automation",
      "authors": "Cornell Wright, Austin Buchan, Ben Brown, Jason Geist, Michael Schwerin, David Rollinson, Matthew Tesch, and Howie Choset"
    },
    {
      "index": 45,
      "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.03939",
      "authors": "Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang",
      "orig_title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
      "paper_id": "2002.03939v2"
    },
    {
      "index": 46,
      "title": "Modular robots",
      "abstract": "",
      "year": "2002",
      "venue": "IEEE Spectrum",
      "authors": "Mark Yim, Ying Zhang, and David Duff"
    },
    {
      "index": 47,
      "title": "Learning implicit credit assignment for multi-agent actor-critic",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung"
    }
  ]
}