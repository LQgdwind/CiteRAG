{
  "paper_id": "2410.14759v4",
  "title": "Universal approximation results for neural networks with non-polynomial activation function over non-compact domains",
  "sections": {
    "introduction": "Inspired by the functionality of human brains, (artificial) neural networks have been discovered in the seminal work of McCulloch and Pitts (see ). Fundamentally, a neural network consists of nodes arranged in hierarchical layers, where the connections between adjacent layers transmit the data through the network and the nodes transform this information. In mathematical terms, a neural network can therefore be described as a concatenation of affine and non-affine functions. Nowadays, neural networks are successfully applied in the fields of image classification (see e.g.¬†), speech recognition (see e.g.¬†) and computer games (see e.g.¬†), and provide as a supervised machine learning technique an algorithmic approach for the quest of artificial intelligence (see 3 ). The universal approximation property of neural networks was first proven by Cybenko and Hornik et al.¬†in their seminal works   , which establishes in universal approximation theorems (UATs) the denseness of the set of neural networks within a given function space. For example,   showed an UAT for neural networks with sigmoidal activation function within the space of continuous function over a compact subset of a Euclidean space, which was extended in  to bounded and non-constant activation functions, and in  [ref]9  to non-polynomial activation functions. Moreover,  proved an UAT for neural networks with sigmoidal activation within LpsuperscriptùêøùëùL^{p}-spaces whose measure is compactly supported, which was generalized in  to bounded and non-constant activation function, and in [31, Proposition¬†2] to non-polynomial activation functions. In addition,   included the approximation of the derivatives and showed UATs within Cksuperscriptùê∂ùëòC^{k}-spaces and Sobolev spaces over compact domains. In this paper, we extend these universal approximation theorems (UATs) to more general activation functions and more general function spaces over non-compact domains. More precisely, we show UATs for neural networks with non-polynomial activation function within function spaces that are obtained as completions of the space of bounded and kùëòk-times differentiable functions with bounded derivatives over a possibly non-compact domain with respect to a weighted norm. This allows us to obtain UATs for weighted spaces, LpsuperscriptùêøùëùL^{p}-spaces, and (weighted) Sobolev spaces over unbounded domains, where the latter includes the approximation of the (weak) derivatives. To this end, we combine the Hahn-Banach separation argument with a Riesz representation theorem (see [11, Theorem¬†1] and [14, Theorem¬†2.4]) and follow Korevaar‚Äôs distributional extension (see ) of Wiener‚Äôs Tauberian theorem (see ). This approach also generalizes the UATs in   for neural networks within (weighted) function spaces over non-compact domains by including the approximation of the derivatives. Furthermore, we prove dimension-independent rates to approximate a given function by a single-hidden-layer neural network in a (weighted) Sobolev space. To this end, we apply the reconstruction formula in  (see also ) and use the concept of Rademacher averages. This extends the approximation rates for neural networks with sigmoidal activation in LpsuperscriptùêøùëùL^{p}-spaces (see    ), with periodic activation function in C0superscriptùê∂0C^{0}-spaces and LpsuperscriptùêøùëùL^{p}-spaces (see  ), and with linear combination of polynomially decaying activation functions in Wk,2superscriptùëäùëò2W^{k,2}-Sobolev spaces (see 0) to more general weighted Sobolev spaces and more general activation functions. For a more detailed review of the literature, we refer to   0."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Handbook of mathematical functions with formulas, graphs, and mathematical tables",
      "abstract": "",
      "year": "1970",
      "venue": "Applied mathematics series / National Bureau of Standards 55",
      "authors": "Milton Abramowitz and Irene¬†Ann Stegun"
    },
    {
      "index": 1,
      "title": "Sobolev Spaces",
      "abstract": "",
      "year": "1975",
      "venue": "Pure and applied mathematics. Academic Press",
      "authors": "Robert¬†A. Adams"
    },
    {
      "index": 2,
      "title": "Topics in Banach space theory",
      "abstract": "",
      "year": "2006",
      "venue": "Graduate Texts in Mathematics 233. Springer, New York",
      "authors": "Fernando Albiac and Nigel¬†J. Kalton"
    },
    {
      "index": 3,
      "title": "Neural net approximation",
      "abstract": "",
      "year": "1992",
      "venue": "7th Yale workshop on adaptive and learning systems",
      "authors": "Andrew¬†R. Barron"
    },
    {
      "index": 4,
      "title": "Universal approximation bounds for superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1993",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Andrew¬†R. Barron"
    },
    {
      "index": 5,
      "title": "Measure Theory",
      "abstract": "",
      "year": "2007",
      "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg",
      "authors": "Vladimir¬†I. Bogachev"
    },
    {
      "index": 6,
      "title": "Optimal Approximation with Sparsely Connected Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Mathematics of Data Science",
      "authors": "Helmut B√∂lcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen",
      "orig_title": "Optimal approximation with sparsely connected deep neural networks",
      "paper_id": "1705.01714v4"
    },
    {
      "index": 7,
      "title": "Ridgelets: Theory and Applications",
      "abstract": "",
      "year": "1998",
      "venue": "PhD thesis, Stanford University",
      "authors": "Emmanuel¬†J. Cand√®s"
    },
    {
      "index": 8,
      "title": "Approximation capability to functions of several variables, nonlinear functionals, and operators by radial basis function neural networks",
      "abstract": "",
      "year": "1995",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "Tianping Chen and Hong Chen"
    },
    {
      "index": 9,
      "title": "Global universal approximation of functional input maps on weighted spaces",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv e-prints 2306.03303",
      "authors": "Christa Cuchiero, Philipp Schmocker, and Josef Teichmann",
      "orig_title": "Global universal approximation of functional input maps on weighted spaces",
      "paper_id": "2306.03303v4"
    },
    {
      "index": 10,
      "title": "Approximation by superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of Control, Signals and Systems",
      "authors": "George Cybenko"
    },
    {
      "index": 11,
      "title": "Rate of approximation results motivated by robust neural network learning",
      "abstract": "",
      "year": "1993",
      "venue": "Sixth Annual Conference on Computational Learning Theory, COLT ‚Äô93",
      "authors": "Christian Darken, James¬†M. Donahue, Leonid Gurvits, and Eduardo¬†D. Sontag"
    },
    {
      "index": 12,
      "title": "Measure theory",
      "abstract": "",
      "year": "1994",
      "venue": "Graduate texts in mathematics; 143. Springer Science+Business Media, LLC, New York",
      "authors": "Joseph¬†L. Doob"
    },
    {
      "index": 13,
      "title": "A semigroup point of view on splitting schemes for stochastic (partial) differential equations",
      "abstract": "",
      "year": "2010",
      "venue": "arXiv e-prints 1011.2651",
      "authors": "Philipp D√∂rsek and Josef Teichmann"
    },
    {
      "index": 14,
      "title": "The Barron Space and the Flow-induced Function Spaces for Neural Network Models",
      "abstract": "",
      "year": "2022",
      "venue": "Constructive Approximation",
      "authors": "Weinan E, Chao Ma, and Lei Wu",
      "orig_title": "The Barron space and the flow-induced function spaces for neural network models",
      "paper_id": "1906.08039v2"
    },
    {
      "index": 15,
      "title": "Partial differential equations, volume¬†19 of Graduate studies in mathematics",
      "abstract": "",
      "year": "2010",
      "venue": "American Mathematical Society, Providence, Rhode Island",
      "authors": "Lawrence¬†C. Evans"
    },
    {
      "index": 16,
      "title": "Fourier analysis and its applications",
      "abstract": "",
      "year": "1992",
      "venue": "Brooks/Cole Publishing Company, Belmont, California",
      "authors": "Gerald¬†B. Folland"
    },
    {
      "index": 17,
      "title": "Uniform error estimates for artificial neural network approximations for heat equations",
      "abstract": "",
      "year": "2021",
      "venue": "IMA Journal of Numerical Analysis",
      "authors": "Lukas Gonon, Philipp Grohs, Arnulf Jentzen, David Kofler, and David ≈†i≈°ka",
      "orig_title": "Uniform error estimates for artificial neural network approximations for heat equations",
      "paper_id": "1911.09647v3"
    },
    {
      "index": 18,
      "title": "Modern Fourier Analysis",
      "abstract": "",
      "year": "2014",
      "venue": "Graduate Texts in Mathematics, 250. Springer New York, New York, NY",
      "authors": "Loukas Grafakos"
    },
    {
      "index": 19,
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Signal Processing Magazine",
      "authors": "Geoffrey Hinton, Li¬†Deng, Dong Yu, George¬†E. Dahl, Abdel-Rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara¬†N. Sainath, and Brian Kingsbury"
    },
    {
      "index": 20,
      "title": "Approximation capabilities of multilayer feedforward networks",
      "abstract": "",
      "year": "1991",
      "venue": "Neural Networks",
      "authors": "Kurt Hornik"
    },
    {
      "index": 21,
      "title": "Multilayer feedforward networks are universal approximators",
      "abstract": "",
      "year": "1989",
      "venue": "Neural Networks",
      "authors": "Kurt Hornik, Maxwell Stinchcombe, and Halbert White"
    },
    {
      "index": 22,
      "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks",
      "abstract": "",
      "year": "1990",
      "venue": "Neural Networks",
      "authors": "Kurt Hornik, Maxwell Stinchcombe, and Halbert White"
    },
    {
      "index": 23,
      "title": "Analysis in Banach Spaces, volume¬†63 of Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge",
      "abstract": "",
      "year": "2016",
      "venue": "Springer, Cham",
      "authors": "Tuomas Hyt√∂nen, Jan van Neerven, Mark Veraar, and Lutz Weis"
    },
    {
      "index": 24,
      "title": "Risk bounds for high-dimensional ridge function combinations including neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv e-prints 1607.01434",
      "authors": "Jason¬†M. Klusowski and Andrew¬†R. Barron"
    },
    {
      "index": 25,
      "title": "Distribution proof of Wiener‚Äôs Tauberian theorem",
      "abstract": "",
      "year": "1965",
      "venue": "American Mathematical Society",
      "authors": "Jacob Korevaar"
    },
    {
      "index": 26,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey¬†E Hinton"
    },
    {
      "index": 27,
      "title": "Complexity estimates based on integral transforms induced by computational units",
      "abstract": "",
      "year": "2012",
      "venue": "Neural Networks",
      "authors": "Vƒõra K≈Ørkov√°"
    },
    {
      "index": 28,
      "title": "Weighted Sobolev spaces",
      "abstract": "",
      "year": "1980",
      "venue": "Teubner-Texte zur Mathematik Bd.¬†31. B.G. Teubner, Leipzig",
      "authors": "Alois Kufner"
    },
    {
      "index": 29,
      "title": "Probability in Banach spaces: Isoperimetry and Processes",
      "abstract": "",
      "year": "1991",
      "venue": "Ergebnisse der Mathematik und ihrer Grenzgebiete. Folge 3 Bd. 23. Springer, Berlin",
      "authors": "Michel Ledoux and Michel Talagrand"
    },
    {
      "index": 30,
      "title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Networks",
      "authors": "Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken"
    },
    {
      "index": 31,
      "title": "A logical calculus of the ideas immanent in nervous activity",
      "abstract": "",
      "year": "1943",
      "venue": "The bulletin of mathematical biophysics",
      "authors": "Warren¬†S. McCulloch and Walter Pitts"
    },
    {
      "index": 32,
      "title": "Dimension-independent bounds on the degree of approximation by neural networks",
      "abstract": "",
      "year": "1994",
      "venue": "IBM Journal of Research and Development",
      "authors": "Hrushikesh¬†Narhar Mhaskar and Charles¬†A Micchelli"
    },
    {
      "index": 33,
      "title": "Degree of approximation by neural and translation networks with a single hidden layer",
      "abstract": "",
      "year": "1995",
      "venue": "Advances in Applied Mathematics",
      "authors": "Hrushikesh¬†Narhar Mhaskar and Charles¬†A Micchelli"
    },
    {
      "index": 34,
      "title": "Machine Learning",
      "abstract": "",
      "year": "1997",
      "venue": "McGraw-Hill series in computer science. WCB McGraw-Hill, Boston MA",
      "authors": "Tom¬†M. Mitchell"
    },
    {
      "index": 35,
      "title": "Topology",
      "abstract": "",
      "year": "2014",
      "venue": "Pearson, Harlow, Essex, UK",
      "authors": "James¬†R. Munkres"
    },
    {
      "index": 36,
      "title": "Approximation theory of the MLP model in neural networks",
      "abstract": "",
      "year": "1999",
      "venue": "Acta Numerica",
      "authors": "Allan Pinkus"
    },
    {
      "index": 37,
      "title": "Real and complex analysis",
      "abstract": "",
      "year": "1987",
      "venue": "McGraw-Hill series in higher mathematics. WCB/McGraw-Hill, Boston, Massachusetts",
      "authors": "Walter Rudin"
    },
    {
      "index": 38,
      "title": "Functional analysis",
      "abstract": "",
      "year": "1991",
      "venue": "International series in pure and applied mathematics. McGraw-Hill, Boston, Mass",
      "authors": "Walter Rudin"
    },
    {
      "index": 39,
      "title": "Approximation Rates for Neural Networks with General Activation Functions",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Networks",
      "authors": "Jonathan¬†W. Siegel and Jinchao Xu",
      "orig_title": "Approximation rates for neural networks with general activation functions",
      "paper_id": "1904.02311v7"
    },
    {
      "index": 40,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "David Silver, Aja Huang, Chris¬†J. Maddison, Arthur Guez, Laurent Sifre, George van¬†den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis"
    },
    {
      "index": 41,
      "title": "Neural network with unbounded activation functions is universal approximator",
      "abstract": "",
      "year": "2017",
      "venue": "Applied and Computational Harmonic Analysis",
      "authors": "Sho Sonoda and Noboru Murata"
    },
    {
      "index": 42,
      "title": "Computing machinery and intelligence",
      "abstract": "",
      "year": "1950",
      "venue": "Mind",
      "authors": "Alan¬†M. Turing"
    },
    {
      "index": 43,
      "title": "Noncompact uniform universal approximation",
      "abstract": "",
      "year": "2024",
      "venue": "Neural Networks",
      "authors": "Teun¬†D.H. van Nuland",
      "orig_title": "Noncompact uniform universal approximation",
      "paper_id": "2308.03812v2"
    },
    {
      "index": 44,
      "title": "Tauberian theorems",
      "abstract": "",
      "year": "1932",
      "venue": "Annals of Mathematics",
      "authors": "Norbert Wiener"
    }
  ]
}