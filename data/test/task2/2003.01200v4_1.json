{
  "paper_id": "2003.01200v4",
  "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
  "sections": {
    "ii-a2 deep learning architectures": "Numerous deep learning architectures have been developed in different research areas, e.g., in NLP applications employing recurrent neural networks (RNNs) [ref]30, convolutional neural networks (CNNs) , and more recently, recursive neural networks .\nWe focus our discussion on a review of the essential models, explained in relevant seminal publications. Multi Layer Perceptron:\nA multilayer perceptron (MLP) has at least three layers (input, hidden, and output layers).\nA layer is simply a collection of neurons operating to transform information from the previous layer to the next layer.\nIn the MLP architecture, the neurons in a layer do not communicate with each other.\nAn MLP employs nonlinear activation functions.\nEvery node in a layer connects to all nodes in the next layer, creating a fully connected network (Fig. 1).\nMLPs are the simplest type of Feed-Forward Neural Networks (FNNs).\nFNNs represent a general category of neural networks in which the connections between the nodes do not create any cycle, i.e., in a FNN there is no cycle of information flow. Convolutional Neural Networks: Convolutional neural networks (CNNs), whose architecture is inspired by the human visual cortex, are a subclass of feed-forward neural networks.\nCNNs are named after the underlying mathematical operation, convolution, which yields a measure of the interoperability of its input functions. Convolutional neural networks are usually employed in situations where data is or needs to be represented with a 2D or 3D data map. In the data map representation, the proximity of data points usually corresponds to their information correlation. In convolutional neural networks where the input is an image, the data map indicates that image pixels are highly correlated to their neighboring pixels. Consequently, the convolutional layers have 3 dimensions: width, height, and depth. That assumption possibly explains why the majority of research efforts dedicated to CNNs are conducted in the Computer Vision field . A CNN takes an image represented as an array of numeric values.\nAfter performing specific mathematical operations, it represents the image in a new output space.\nThis operation is also called feature extraction, and helps to capture and represent key image content.\nThe extracted features can be used for further analysis, for different tasks.\nOne example is image classification, which aims to categorize images according to some predefined classes.\nOther examples include determining which objects are present in an image and where they are located.\nSee Fig. 2. In the case of utilizing CNNs for NLP, the inputs are sentences or documents represented as matrices. Each row of the matrix is associated with a language element such as a word or a character. The majority of CNN architectures learn word or sentence representations in their training phase. A variety of CNN architectures were used in various classification tasks such as Sentiment Analysis and Topic Categorization   [ref]35 .\nCNNs were employed for Relation Extraction and Relation Classification as well  . Recurrent Neural Network: If we line up a sequence of FNNs and feed the output of each FNN as an input to the next one, a recurrent neural network (RNN) will be constructed.\nLike FNNs, layers in an RNN can be categorized into input, hidden, and output layers.\nIn discrete time frames, sequences of input vectors are fed as the input, one vector at a time, e.g., after inputting each batch of vectors, conducting some operations and updating the network weights, the next input batch will be fed to the network.\nThus, as shown in Fig. 3, at each time step we make predictions and use parameters of the current hidden layer as input to the next time step. Hidden layers in recurrent neural networks can carry information from the past, in other words, memory.\nThis characteristic makes them specifically useful for applications that deal with a sequence of inputs such as language modeling , i.e., representing language in a way that the machine understands.\nThis concept will be described later in detail. RNNs can carry rich information from the past.\nConsider the sentence: “Michael Jackson was a singer; some people consider him King of Pop.”\nIt’s easy for a human to identify him as referring to Michael Jackson.\nThe pronoun him happens seven words after Michael Jackson; capturing this dependency is one of the benefits of RNNs, where the hidden layers in an RNN act as memory units.\nLong Short Term Memory Network (LSTM)  is one of the most widely used classes of RNNs.\nLSTMs try to capture even long time dependencies between inputs from different time steps.\nModern Machine Translation and Speech Recognition often rely on LSTMs. Autoencoders: Autoencoders implement unsupervised methods in deep learning.\nThey are widely used in dimensionality reduction333Dimensionality reduction is an unsupervised learning approach which is the process of reducing the number of variables that were used to represent the data by identifying the most crucial information. or NLP applications which consist of sequence to sequence modeling (see Section III-B .\nFig. 4 illustrates the schematic of an Autoencoder.\nSince autoencoders are unsupervised, there is no label corresponding to each input.\nThey aim to learn a code representation for each input.\nThe encoder is like a feed-forward neural network in which the input gets encoded into a vector (code).\nThe decoder operates similarly to the encoder, but in reverse, i.e., constructing an output based on the encoded input.\nIn data compression applications, we want the created output to be as close as possible to the original input.\nAutoencoders are lossy, meaning the output is an approximate reconstruction of the input. Generative Adversarial Networks: Goodfellow [ref]41 introduced Generative Adversarial Networks (GANs).\nAs shown in Fig. 5, a GAN is a combination of two neural networks, a discriminator and a generator.\nThe whole network is trained in an iterative process.\nFirst, the generator network generates a fake sample.\nThen the discriminator network tries to determine whether this sample (ex.: an input image) is real or fake, i.e., whether it came from the real training data (data used for building the model) or not.\nThe goal of the generator is to fool the discriminator in a way that the discriminator believes the artificial (i.e., generated) samples synthesized by the generator are real. This iterative process continues until the generator produces samples that are indistinguishable by the discriminator.\nIn other words, the probability of classifying a sample as fake or real becomes like flipping a fair coin for the discriminator.\nThe goal of the generative model is to capture the distribution of real data while the discriminator tries to identify the fake data.\nOne of the interesting features of GANs (regarding being generative) is: once the training phase is finished, there is no need for the discrimination network, so we solely can work with the generation network.\nIn other words, having access to the trained generative model is sufficient. Different forms of GANs has been introduced, e.g., Sim GAN , Wasserstein GAN , info GAN , and DC GAN .\nIn one of the most elegant GAN implementations , entirely artificial, yet almost perfect, celebrity faces are generated; the pictures are not real, but fake photos produced by the network. GAN’s have since received significant attention in various applications and have generated astonishing result .\nIn the NLP domain, GANs often are used for text generation  ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Foundations of statistical natural language processing",
      "abstract": "",
      "year": "1999",
      "venue": "MIT Press",
      "authors": "C. D. Manning, C. D. Manning, and H. Schütze"
    },
    {
      "index": 1,
      "title": "Character-level convolutional networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Zhang, J. Zhao, and Y. LeCun"
    },
    {
      "index": 2,
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1406.1078",
      "authors": "K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio"
    },
    {
      "index": 3,
      "title": "Deep learning in clinical natural language processing: a methodical review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of the American Medical Informatics Association",
      "authors": "S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei, Y. Xiang, B. Zhao, and H. Xu"
    },
    {
      "index": 4,
      "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "abstract": "",
      "year": "2008",
      "venue": "25th international conference on Machine learning",
      "authors": "R. Collobert and J. Weston"
    },
    {
      "index": 5,
      "title": "Large-scale video classification with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei"
    },
    {
      "index": 6,
      "title": "Learning and transferring mid-level image representations using convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "M. Oquab, L. Bottou, I. Laptev, and J. Sivic"
    },
    {
      "index": 7,
      "title": "Learning from simulated and unsupervised images through adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb"
    },
    {
      "index": 8,
      "title": "Deep Learning for Computer Vision: A Brief Review",
      "abstract": "",
      "year": "2018",
      "venue": "Computational Intelligence and Neuroscience",
      "authors": "A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis"
    },
    {
      "index": 9,
      "title": "Deep learning vs. traditional computer vision",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Computer Vision",
      "authors": "N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh"
    },
    {
      "index": 10,
      "title": "Towards end-to-end speech recognition with recurrent neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Graves and N. Jaitly"
    },
    {
      "index": 11,
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al.",
      "orig_title": "Deep speech 2: End-to-end speech recognition in English and Mandarin",
      "paper_id": "1512.02595v1"
    },
    {
      "index": 12,
      "title": "Deep learning for NLP and speech recognition",
      "abstract": "",
      "year": "2019",
      "venue": "Springer",
      "authors": "U. Kamath, J. Liu, and J. Whitaker"
    },
    {
      "index": 13,
      "title": "Learning character-level representations for part-of-speech tagging",
      "abstract": "",
      "year": "2014",
      "venue": "31st International Conference on Machine Learning (ICML-14)",
      "authors": "C. D. Santos and B. Zadrozny"
    },
    {
      "index": 14,
      "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1604.05529",
      "authors": "B. Plank, A. Søgaard, and Y. Goldberg",
      "orig_title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "paper_id": "1604.05529v3"
    },
    {
      "index": 15,
      "title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics",
      "authors": "C. D. Manning"
    },
    {
      "index": 16,
      "title": "Deep learning techniques for part of speech tagging by natural language processing",
      "abstract": "",
      "year": "2020",
      "venue": "2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)",
      "authors": "R. D. Deshmukh and A. Kiwelekar"
    },
    {
      "index": 17,
      "title": "Neural Architectures for Named Entity Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1603.01360",
      "authors": "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer",
      "orig_title": "Neural architectures for named entity recognition",
      "paper_id": "1603.01360v3"
    },
    {
      "index": 18,
      "title": "Named entity recognition with bidirectional LSTM-CNNs",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.08308",
      "authors": "J. P. Chiu and E. Nichols"
    },
    {
      "index": 19,
      "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.11470",
      "authors": "V. Yadav and S. Bethard",
      "orig_title": "A survey on recent advances in named entity recognition from deep learning models",
      "paper_id": "1910.11470v1"
    },
    {
      "index": 20,
      "title": "A Survey on Deep Learning for Named Entity Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "J. Li, A. Sun, J. Han, and C. Li",
      "orig_title": "A survey on deep learning for named entity recognition",
      "paper_id": "1812.09449v3"
    },
    {
      "index": 21,
      "title": "End-to-end learning of semantic role labeling using recurrent neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "authors": "J. Zhou and W. Xu"
    },
    {
      "index": 22,
      "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.02593",
      "authors": "D. Marcheggiani, A. Frolov, and I. Titov",
      "orig_title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling",
      "paper_id": "1701.02593v2"
    },
    {
      "index": 23,
      "title": "Deep semantic role labeling: What works and what’s next",
      "abstract": "",
      "year": "2017",
      "venue": "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "L. He, K. Lee, M. Lewis, and L. Zettlemoyer"
    },
    {
      "index": 24,
      "title": "Syntax-aware Multilingual Semantic Role Labeling",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.00310",
      "authors": "S. He, Z. Li, and H. Zhao",
      "orig_title": "Syntax-aware multilingual semantic role labeling",
      "paper_id": "1909.00310v3"
    },
    {
      "index": 25,
      "title": "Recent Trends in Deep Learning Based Natural Language Processing",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine",
      "authors": "T. Young, D. Hazarika, S. Poria, and E. Cambria",
      "orig_title": "Recent trends in deep learning based natural language processing",
      "paper_id": "1708.02709v8"
    },
    {
      "index": 26,
      "title": "Natural language processing (NLP) in management research: A literature review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Management Analytics",
      "authors": "Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu"
    },
    {
      "index": 27,
      "title": "What exactly is artificial intelligence, anyway?.",
      "abstract": "",
      "year": "2018",
      "venue": "Wall Street Journal Online Article",
      "authors": "T. Greenwald"
    },
    {
      "index": 28,
      "title": "Critical analysis of big data challenges and analytical methods",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Business Research",
      "authors": "U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody"
    },
    {
      "index": 29,
      "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1506.00019",
      "authors": "Z. C. Lipton, J. Berkowitz, and C. Elkan",
      "orig_title": "A critical review of recurrent neural networks for sequence learning",
      "paper_id": "1506.00019v4"
    },
    {
      "index": 30,
      "title": "Convolutional Neural Networks for Sentence Classification",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1408.5882",
      "authors": "Y. Kim",
      "orig_title": "Convolutional neural networks for sentence classification",
      "paper_id": "1408.5882v2"
    },
    {
      "index": 31,
      "title": "Parsing natural scenes and natural language with recursive neural networks",
      "abstract": "",
      "year": "2011",
      "venue": "28th international conference on machine learning (ICML-11)",
      "authors": "R. Socher, C. C. Lin, C. Manning, and A. Y. Ng"
    },
    {
      "index": 32,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 33,
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "C. dos Santos and M. Gatti"
    },
    {
      "index": 34,
      "title": "Effective use of word order for text categorization with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.1058",
      "authors": "R. Johnson and T. Zhang"
    },
    {
      "index": 35,
      "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Johnson and T. Zhang",
      "orig_title": "Semi-supervised convolutional neural networks for text categorization via region embedding",
      "paper_id": "1504.01255v3"
    },
    {
      "index": 36,
      "title": "Relation classification via convolutional deep neural network",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao"
    },
    {
      "index": 37,
      "title": "Relation extraction: Perspective from convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "1st Workshop on Vector Space Modeling for Natural Language Processing",
      "authors": "T. H. Nguyen and R. Grishman"
    },
    {
      "index": 38,
      "title": "Recurrent neural network based language model",
      "abstract": "",
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association",
      "authors": "T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur"
    },
    {
      "index": 39,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "S. Hochreiter and J. Schmidhuber"
    },
    {
      "index": 40,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio"
    },
    {
      "index": 41,
      "title": "Wasserstein t-SNE",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.07875",
      "authors": "M. Arjovsky, S. Chintala, and L. Bottou",
      "orig_title": "Wasserstein gan",
      "paper_id": "2205.07531v2"
    },
    {
      "index": 42,
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel"
    },
    {
      "index": 43,
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06434",
      "authors": "A. Radford, L. Metz, and S. Chintala",
      "orig_title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "paper_id": "1511.06434v2"
    },
    {
      "index": 44,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.10196",
      "authors": "T. Karras, T. Aila, S. Laine, and J. Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 45,
      "title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.03135",
      "authors": "N. Tavaf, A. Torfi, K. Ugurbil, and P.-F. Van de Moortele",
      "orig_title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "paper_id": "2101.03135v2"
    },
    {
      "index": 46,
      "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "L. Yu, W. Zhang, J. Wang, and Y. Yu"
    },
    {
      "index": 47,
      "title": "Adversarial Learning for Neural Dialogue Generation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.06547",
      "authors": "J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky",
      "orig_title": "Adversarial learning for neural dialogue generation",
      "paper_id": "1701.06547v5"
    },
    {
      "index": 48,
      "title": "Thumbs up?: sentiment classification using machine learning techniques",
      "abstract": "",
      "year": "2002",
      "venue": "ACL-02 conference on Empirical methods in natural language processing-Volume 10",
      "authors": "B. Pang, L. Lee, and S. Vaithyanathan"
    },
    {
      "index": 49,
      "title": "Distributional structure",
      "abstract": "",
      "year": "1954",
      "venue": "Word",
      "authors": "Z. S. Harris"
    },
    {
      "index": 50,
      "title": "“A neural probabilistic language model",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "Distributed Representations of Sentences and Documents",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "“Distributed representations of sentences and documents",
      "paper_id": "1405.4053v2"
    },
    {
      "index": 52,
      "title": "“Distributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "Skip-Thought Vectors",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Skip-thought vectors",
      "paper_id": "1506.06726v1"
    },
    {
      "index": 54,
      "title": "“Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "LAP LAMBERT Academic Publishing, 2015",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "G. Lebanon et al., Riemannian geometry and statistical machine\nlearning."
    },
    {
      "index": 56,
      "title": "Cambridge University Press, 2014",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive datasets."
    },
    {
      "index": 57,
      "title": "“Neural network methods for natural language processing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "“A character-based convolutional neural network for language-agnostic Twitter sentiment analysis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Enriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 60,
      "title": "Compositional Morphology for Word Representations and Language Modelling",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "“Compositional morphology for word representations and language modelling",
      "paper_id": "1405.4273v1"
    },
    {
      "index": 61,
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Get to the point: Summarization with pointer-generator networks",
      "paper_id": "1704.04368v2"
    },
    {
      "index": 62,
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“A deep reinforced model for abstractive summarization",
      "paper_id": "1705.04304v3"
    },
    {
      "index": 63,
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Scheduled sampling for sequence prediction with recurrent neural networks",
      "paper_id": "1506.03099v3"
    },
    {
      "index": 64,
      "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“A continuous relaxation of beam search for end-to-end training of neural sequence models",
      "paper_id": "1708.00111v2"
    },
    {
      "index": 65,
      "title": "Stochastic Beams and Where to Find Them: The Gumbel-Top-𝑘 Trick for Sampling Sequences Without Replacement",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement",
      "paper_id": "1903.06059v2"
    },
    {
      "index": 66,
      "title": "“Rouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "“BLEU: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "“METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Deep reinforcement learning for sequence to sequence models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Sequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 71,
      "title": "“Reinforcement learning neural Turing machines-revised",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction."
    },
    {
      "index": 74,
      "title": "” Machine Learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "“Search-based structured prediction",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 77,
      "title": "“Recurrent models of visual attention",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
      "paper_id": "1903.09588v1"
    },
    {
      "index": 79,
      "title": "“Evaluation of NLP systems",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "“Ask me anything: Dynamic memory networks for natural language processing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Bidirectional LSTM-CRF models for sequence tagging",
      "paper_id": "1508.01991v1"
    },
    {
      "index": 82,
      "title": "Globally Normalized Transition-Based Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Globally normalized transition-based neural networks",
      "paper_id": "1603.06042v2"
    },
    {
      "index": 83,
      "title": "“Part-of-speech tagging of building codes empowered by deep learning and transformational rules",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "Empower Sequence Labeling with Task-Aware Neural Language Model",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Empower sequence labeling with task-aware neural language model",
      "paper_id": "1709.04109v4"
    },
    {
      "index": 85,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
      "paper_id": "1603.01354v5"
    },
    {
      "index": 87,
      "title": "“Robust multilingual part-of-speech tagging via adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "“Finding function in form: Compositional character models for open vocabulary word representation",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "“Contextual string embeddings for sequence labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Morphosyntactic tagging with a Meta-BiLSTM model over context sensitive token encodings",
      "paper_id": "1805.08237v1"
    },
    {
      "index": 91,
      "title": "“Joint RNN-based greedy parsing and word composition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "“Deep neural networks for syntactic parsing of morphologically rich languages",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 93,
      "title": "“What do recurrent neural network grammars learn about syntax?",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "“In-order transition-based constituent parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“Improving neural parsing by disentangling model combination and reranking effects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "Constituency Parsing with a Self-Attentive Encoder",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Constituency parsing with a self-attentive encoder",
      "paper_id": "1805.01052v1"
    },
    {
      "index": 97,
      "title": "“A fast and accurate dependency parser using neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 99,
      "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Simple and accurate dependency parsing using bidirectional LSTM feature representations",
      "paper_id": "1603.04351v3"
    },
    {
      "index": 100,
      "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Transition-based dependency parsing with stack long short-term memory",
      "paper_id": "1505.08075v1"
    },
    {
      "index": 101,
      "title": "“Deep learning for natural language parsing",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "“Parsing clinical text using the state-of-the-art deep learning based parsers: a systematic comparison",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient second-order treecrf for neural dependency parsing",
      "paper_id": "2005.00975v2"
    },
    {
      "index": 104,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 105,
      "title": "“Deep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Encoding sentences with graph convolutional networks for semantic role labeling",
      "paper_id": "1703.04826v4"
    },
    {
      "index": 107,
      "title": "Linguistically-Informed Self-Attention for Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Linguistically-informed self-attention for semantic role labeling",
      "paper_id": "1804.08199v3"
    },
    {
      "index": 108,
      "title": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Jointly predicting predicates and arguments in neural semantic role labeling",
      "paper_id": "1805.04787v2"
    },
    {
      "index": 109,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 110,
      "title": "“Deep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "“Dependency or span",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "“Towards robust linguistic analysis using OntoNotes",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "“A convolutional neural network for modelling sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "“Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "“Hierarchical attention networks for document classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "“Recurrent convolutional neural networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "A C-LSTM Neural Network for Text Classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“A C-LSTM neural network for text classification",
      "paper_id": "1511.08630v2"
    },
    {
      "index": 118,
      "title": "Deep Learning Based Text Classification: A Comprehensive Review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep learning based text classification: A comprehensive review",
      "paper_id": "2004.03705v3"
    },
    {
      "index": 119,
      "title": "“A comparative review on deep learning models for text classification",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "Very Deep Convolutional Networks for Text Classification",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Very deep convolutional networks for text classification",
      "paper_id": "1606.01781v2"
    },
    {
      "index": 121,
      "title": "“Deep pyramid convolutional neural networks for text categorization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Supervised and semi-supervised text categorization using LSTM for region embeddings",
      "paper_id": "1602.02373v2"
    },
    {
      "index": 123,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Universal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 124,
      "title": "“Natural language processing (almost) from scratch",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "“Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "NeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“NeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "paper_id": "1705.05487v1"
    },
    {
      "index": 127,
      "title": "Cloze-driven Pretraining of Self-attention Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Cloze-driven pretraining of self-attention networks",
      "paper_id": "1903.07785v1"
    },
    {
      "index": 128,
      "title": "“Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Semi-Supervised Sequence Modeling with Cross-View Training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Semi-supervised sequence modeling with cross-view training",
      "paper_id": "1809.08370v1"
    },
    {
      "index": 130,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 131,
      "title": "“Semantic compositionality through recursive matrix-vector spaces",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "“Semantic relation extraction using sequential and tree-structured lstm with attention",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "more context and more openness: A review and outlook for relation extraction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "“Deep reinforcement learning for mention-ranking coreference models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "“Higher-order coreference resolution with coarse-to-fine inference",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“End-to-end deep reinforcement learning based coreference resolution",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "CorefQA: Coreference Resolution as Query-based Span Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Corefqa: Coreference resolution as query-based span prediction",
      "paper_id": "1911.01746v4"
    },
    {
      "index": 138,
      "title": "“Event extraction via dynamic multi-pooling convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "“Graph convolutional networks with argument-aware pooling for event detection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "“Joint entity and event extraction with generative adversarial imitation learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“A novel joint biomedical event extraction framework via two-level modeling of documents",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Sentiment analysis: Capturing favorability using natural language processing",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Mining the peanut gallery: Opinion extraction and semantic classification of product reviews",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "“Application of deep learning approaches for sentiment analysis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "“Sentiment analysis using deep learning architectures: a review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "“Document modeling with gated recurrent neural network for sentiment classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "“Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "“Lstm with sentence representations for document-level sentiment classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“A cnn-bilstm model for document-level sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "“Predicting polarities of tweets by composing word embeddings with long short-term memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "“Recursive deep models for semantic compositionality over a sentiment treebank",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "“Classification of sentence level sentiment analysis using cloud machine learning techniques",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "” Information Processing & Management",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "“Attention-based LSTM for aspect-level sentiment classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "“Sentic lstm: a hybrid network for targeted aspect-based sentiment analysis",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "paper_id": "1904.02232v2"
    },
    {
      "index": 158,
      "title": "Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Double embeddings and CNN-based sequence labeling for aspect extraction",
      "paper_id": "1805.04601v1"
    },
    {
      "index": 159,
      "title": "“Deep learning for aspect-based sentiment analysis: a comparative review",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "“A multi-layer dual attention deep learning model with refined word embeddings for aspect-based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "“A novel aspect-guided deep transition model for aspect based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Prentice Hall, 2008",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "D. Jurafsky and J. H. Martin, Speech and Language Processing."
    },
    {
      "index": 163,
      "title": "“Recurrent continuous translation models",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "“Machine translation using deep learning: An overview",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "“A survey of deep learning techniques for neural machine translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "“The Georgetown-IBM experiment",
      "abstract": "",
      "year": "1955",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 168,
      "title": "B. Van Merriënboer",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "“Sequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 171,
      "title": "“Convolutional sequence to sequence learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 173,
      "title": "Weighted Transformer Network for Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Weighted transformer network for machine translation",
      "paper_id": "1711.02132v1"
    },
    {
      "index": 174,
      "title": "Self-Attention with Relative Position Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-attention with relative position representations",
      "paper_id": "1803.02155v2"
    },
    {
      "index": 175,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding back-translation at scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 176,
      "title": "Massively Multilingual Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Massively multilingual neural machine translation",
      "paper_id": "1903.00089v3"
    },
    {
      "index": 177,
      "title": "Incorporating BERT into Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Incorporating bert into neural machine translation",
      "paper_id": "2002.06823v1"
    },
    {
      "index": 178,
      "title": "M. Ghazvininejad",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Robust neural machine translation with doubly adversarial inputs",
      "paper_id": "1906.02443v1"
    },
    {
      "index": 180,
      "title": "Bridging the Gap between Training and Inference for Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Bridging the gap between training and inference for neural machine translation",
      "paper_id": "1906.02448v2"
    },
    {
      "index": 181,
      "title": "Towards Making the Most of BERT in Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards making the most of bert in neural machine translation",
      "paper_id": "1908.05672v5"
    },
    {
      "index": 182,
      "title": "“Question answering with subgraph embeddings",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "“Baseball: an automatic question-answerer",
      "abstract": "",
      "year": "1961",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "“IBM’s statistical question answering system",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "“Question answering passage retrieval using dependency relations",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "“Convolutional neural tensor network architecture for community-based question answering",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "“A machine learning approach to answering questions for reading comprehension tests",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "Dynamic Coattention Networks for Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Dynamic coattention networks for question answering",
      "paper_id": "1611.01604v4"
    },
    {
      "index": 189,
      "title": "C. Lawrence Zitnick",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Ask your neurons: A neural-based approach to answering questions about images",
      "paper_id": "1505.01121v3"
    },
    {
      "index": 191,
      "title": "attend and answer: Exploring question-guided spatial attention for visual question answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Human attention in visual question answering: Do humans and deep networks look at the same regions?",
      "paper_id": "1606.03556v2"
    },
    {
      "index": 193,
      "title": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection",
      "paper_id": "1902.00038v2"
    },
    {
      "index": 194,
      "title": "“Self-critical reasoning for robust visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 195,
      "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents",
      "paper_id": "1611.04230v1"
    },
    {
      "index": 196,
      "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Ranking sentences for extractive summarization with reinforcement learning",
      "paper_id": "1802.08636v2"
    },
    {
      "index": 197,
      "title": "“A neural attention model for abstractive sentence summarization",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "“Abstractive document summarization with a graph-based attentional neural model",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "“Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "“Teaching machines to read and comprehend",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Incorporating copying mechanism in sequence-to-sequence learning",
      "paper_id": "1603.06393v3"
    },
    {
      "index": 202,
      "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Fast abstractive summarization with reinforce-selected sentence rewriting",
      "paper_id": "1805.11080v1"
    },
    {
      "index": 203,
      "title": "Neural Document Summarization by Jointly Learning to Score and Select Sentences",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural document summarization by jointly learning to score and select sentences",
      "paper_id": "1807.02305v1"
    },
    {
      "index": 204,
      "title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural abstractive text summarization with sequence-to-sequence models",
      "paper_id": "1812.02303v4"
    },
    {
      "index": 205,
      "title": "“Multi-document summarization via deep learning techniques: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "“A hybrid deep learning architecture for opinion-oriented multi-document summarization based on multi-feature fusion",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 207,
      "title": "“Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "“Dialogue systems for intelligent human computer interactions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 209,
      "title": "“Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "Understanding Chatbot-mediated Task Management",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding chatbot-mediated task management",
      "paper_id": "1802.03109v1"
    },
    {
      "index": 211,
      "title": "Goal-Oriented Chatbot Dialog Management Bootstrapping with Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Goal-oriented chatbot dialog management bootstrapping with transfer learning",
      "paper_id": "1802.00500v2"
    },
    {
      "index": 212,
      "title": "“Deep reinforcement learning for dialogue generation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“A network-based end-to-end trainable task-oriented dialogue system",
      "paper_id": "1604.04562v3"
    },
    {
      "index": 214,
      "title": "“End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "“End-to-end memory networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "Learning End-to-End Goal-Oriented Dialog",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning end-to-end goal-oriented dialog",
      "paper_id": "1605.07683v4"
    },
    {
      "index": 217,
      "title": "“Data-driven response generation in social media",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "“An information retrieval approach to short text conversation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "“Convolutional neural network architectures for matching natural language sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "paper_id": "1506.08909v3"
    },
    {
      "index": 221,
      "title": "“Learning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "“Multi-turn response selection for chatbots with deep attention matching network",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "“Generating sentences from a continuous space",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "Adversarial Evaluation of Dialogue Models",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Adversarial evaluation of dialogue models",
      "paper_id": "1701.08198v1"
    },
    {
      "index": 225,
      "title": "A Neural Conversational Model",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“A neural conversational model",
      "paper_id": "1506.05869v3"
    }
  ]
}