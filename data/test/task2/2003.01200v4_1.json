{
  "paper_id": "2003.01200v4",
  "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
  "sections": {
    "ii-a2 deep learning architectures": "Numerous deep learning architectures have been developed in different research areas, e.g., in NLP applications employing recurrent neural networks (RNNs)¬†[ref]30, convolutional neural networks (CNNs)¬†, and more recently, recursive neural networks¬†.\nWe focus our discussion on a review of the essential models, explained in relevant seminal publications. Multi Layer Perceptron:\nA multilayer perceptron (MLP) has at least three layers¬†(input, hidden, and output layers).\nA layer is simply a collection of neurons operating to transform information from the previous layer to the next layer.\nIn the MLP architecture, the neurons in a layer do not communicate with each other.\nAn MLP employs nonlinear activation functions.\nEvery node in a layer connects to all nodes in the next layer,¬†creating a fully connected network¬†(Fig.¬†1).\nMLPs are the simplest type of Feed-Forward Neural Networks¬†(FNNs).\nFNNs represent a general category of neural networks in which the connections between the nodes do not create any cycle, i.e., in a FNN there is no cycle of information flow. Convolutional Neural Networks: Convolutional neural networks¬†(CNNs), whose architecture is inspired by the human visual cortex, are a subclass of feed-forward neural networks.\nCNNs are named after the underlying mathematical operation, convolution, which yields a measure of the interoperability of its input functions.¬†Convolutional neural networks are usually employed in situations where data is or needs to be represented with a 2D or 3D data map.¬†In the data map representation, the proximity of data points usually corresponds to their information correlation. In convolutional neural networks where the input is an image, the data map indicates that image pixels are highly correlated to their neighboring pixels.¬†Consequently, the convolutional layers have 3 dimensions: width, height, and depth.¬†That assumption possibly explains why the majority of research efforts dedicated to CNNs are conducted in the Computer Vision field¬†. A CNN takes an image represented as an array of numeric values.\nAfter performing specific mathematical operations, it represents the image in a new output space.\nThis operation is also called feature extraction, and helps to capture and represent key image content.\nThe extracted features can be used for further analysis, for different tasks.\nOne example is image classification, which aims to categorize images according to some predefined classes.\nOther examples include determining which objects are present in an image and where they are located.\nSee Fig.¬†2. In the case of utilizing CNNs for NLP,¬†the inputs are sentences or documents represented as matrices.¬†Each row of the matrix is associated with a language element such as a word or a character.¬†The majority of CNN architectures learn word or sentence representations in their training phase.¬†A variety of CNN architectures were used in various classification tasks such as Sentiment Analysis and Topic Categorization   [ref]35 .\nCNNs were employed for Relation Extraction and Relation Classification as well¬† . Recurrent Neural Network: If we line up a sequence of FNNs and feed the output of each FNN as an input to the next one, a recurrent neural network (RNN) will be constructed.\nLike FNNs, layers in an RNN can be categorized into input, hidden, and output layers.\nIn discrete time frames, sequences of input vectors are fed as the input, one vector at a time, e.g., after inputting each batch of vectors, conducting some operations and updating the network weights, the next input batch will be fed to the network.\nThus, as shown in Fig.¬†3, at each time step we make predictions and use parameters of the current hidden layer as input to the next time step. Hidden layers in recurrent neural networks can carry information from the past, in other words, memory.\nThis characteristic makes them specifically useful for applications that deal with a sequence of inputs such as language modeling¬†, i.e., representing language in a way that the machine understands.\nThis concept will be described later in detail. RNNs can carry rich information from the past.\nConsider the sentence: ‚ÄúMichael Jackson was a singer; some people consider him King of Pop.‚Äù\nIt‚Äôs easy for a human to identify him as referring to Michael Jackson.\nThe pronoun him happens seven words after Michael Jackson; capturing this dependency is one of the benefits of RNNs, where the hidden layers in an RNN act as memory units.\nLong Short Term Memory Network (LSTM)¬† is one of the most widely used classes of RNNs.\nLSTMs try to capture even long time dependencies between inputs from different time steps.\nModern Machine Translation and Speech Recognition often rely on LSTMs. Autoencoders: Autoencoders implement unsupervised methods in deep learning.\nThey are widely used in dimensionality reduction333Dimensionality reduction is an unsupervised learning approach which is the process of reducing the number of variables that were used to represent the data by identifying the most crucial information. or NLP applications which consist of sequence to sequence modeling¬†(see Section¬†III-B¬†.\nFig.¬†4 illustrates the schematic of an Autoencoder.\nSince autoencoders are unsupervised, there is no label corresponding to each input.\nThey aim to learn a code representation for each input.\nThe encoder is like a feed-forward neural network in which the input gets encoded into a vector (code).\nThe decoder operates similarly to the encoder, but in reverse, i.e., constructing an output based on the encoded input.\nIn data compression applications, we want the created output to be as close as possible to the original input.\nAutoencoders are lossy, meaning the output is an approximate reconstruction of the input. Generative Adversarial Networks: Goodfellow¬†[ref]41 introduced Generative Adversarial Networks¬†(GANs).\nAs shown in Fig.¬†5, a GAN is a combination of two neural networks, a discriminator and a generator.\nThe whole network is trained in an iterative process.\nFirst, the generator network generates a fake sample.\nThen the discriminator network tries to determine whether this sample (ex.: an input image) is real or fake, i.e., whether it came from the real training data (data used for building the model) or not.\nThe goal of the generator is to fool the discriminator in a way that the discriminator believes the artificial (i.e., generated) samples synthesized by the generator are real. This iterative process continues until the generator produces samples that are indistinguishable by the discriminator.\nIn other words, the probability of classifying a sample as fake or real becomes like flipping a fair coin for the discriminator.\nThe goal of the generative model is to capture the distribution of real data while the discriminator tries to identify the fake data.\nOne of the interesting features of GANs¬†(regarding being generative) is: once the training phase is finished, there is no need for the discrimination network, so we solely can work with the generation network.\nIn other words, having access to the trained generative model is sufficient. Different forms of GANs has been introduced, e.g., Sim GAN¬†, Wasserstein GAN¬†, info GAN¬†, and DC GAN¬†.\nIn one of the most elegant GAN implementations , entirely artificial, yet almost perfect, celebrity faces are generated; the pictures are not real, but fake photos produced by the network. GAN‚Äôs have since received significant attention in various applications and have generated astonishing result¬†.\nIn the NLP domain, GANs often are used for text generation¬† ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Foundations of statistical natural language processing",
      "abstract": "",
      "year": "1999",
      "venue": "MIT Press",
      "authors": "C. D. Manning, C. D. Manning, and H. Sch√ºtze"
    },
    {
      "index": 1,
      "title": "Character-level convolutional networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Zhang, J. Zhao, and Y. LeCun"
    },
    {
      "index": 2,
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1406.1078",
      "authors": "K. Cho, B. Van Merri√´nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio"
    },
    {
      "index": 3,
      "title": "Deep learning in clinical natural language processing: a methodical review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of the American Medical Informatics Association",
      "authors": "S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei, Y. Xiang, B. Zhao, and H. Xu"
    },
    {
      "index": 4,
      "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "abstract": "",
      "year": "2008",
      "venue": "25th international conference on Machine learning",
      "authors": "R. Collobert and J. Weston"
    },
    {
      "index": 5,
      "title": "Large-scale video classification with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei"
    },
    {
      "index": 6,
      "title": "Learning and transferring mid-level image representations using convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition",
      "authors": "M. Oquab, L. Bottou, I. Laptev, and J. Sivic"
    },
    {
      "index": 7,
      "title": "Learning from simulated and unsupervised images through adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb"
    },
    {
      "index": 8,
      "title": "Deep Learning for Computer Vision: A Brief Review",
      "abstract": "",
      "year": "2018",
      "venue": "Computational Intelligence and Neuroscience",
      "authors": "A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis"
    },
    {
      "index": 9,
      "title": "Deep learning vs. traditional computer vision",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Computer Vision",
      "authors": "N. O‚ÄôMahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh"
    },
    {
      "index": 10,
      "title": "Towards end-to-end speech recognition with recurrent neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Graves and N. Jaitly"
    },
    {
      "index": 11,
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al.",
      "orig_title": "Deep speech 2: End-to-end speech recognition in English and Mandarin",
      "paper_id": "1512.02595v1"
    },
    {
      "index": 12,
      "title": "Deep learning for NLP and speech recognition",
      "abstract": "",
      "year": "2019",
      "venue": "Springer",
      "authors": "U. Kamath, J. Liu, and J. Whitaker"
    },
    {
      "index": 13,
      "title": "Learning character-level representations for part-of-speech tagging",
      "abstract": "",
      "year": "2014",
      "venue": "31st International Conference on Machine Learning (ICML-14)",
      "authors": "C. D. Santos and B. Zadrozny"
    },
    {
      "index": 14,
      "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1604.05529",
      "authors": "B. Plank, A. S√∏gaard, and Y. Goldberg",
      "orig_title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "paper_id": "1604.05529v3"
    },
    {
      "index": 15,
      "title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics",
      "authors": "C. D. Manning"
    },
    {
      "index": 16,
      "title": "Deep learning techniques for part of speech tagging by natural language processing",
      "abstract": "",
      "year": "2020",
      "venue": "2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)",
      "authors": "R. D. Deshmukh and A. Kiwelekar"
    },
    {
      "index": 17,
      "title": "Neural Architectures for Named Entity Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1603.01360",
      "authors": "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer",
      "orig_title": "Neural architectures for named entity recognition",
      "paper_id": "1603.01360v3"
    },
    {
      "index": 18,
      "title": "Named entity recognition with bidirectional LSTM-CNNs",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.08308",
      "authors": "J. P. Chiu and E. Nichols"
    },
    {
      "index": 19,
      "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.11470",
      "authors": "V. Yadav and S. Bethard",
      "orig_title": "A survey on recent advances in named entity recognition from deep learning models",
      "paper_id": "1910.11470v1"
    },
    {
      "index": 20,
      "title": "A Survey on Deep Learning for Named Entity Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "J. Li, A. Sun, J. Han, and C. Li",
      "orig_title": "A survey on deep learning for named entity recognition",
      "paper_id": "1812.09449v3"
    },
    {
      "index": 21,
      "title": "End-to-end learning of semantic role labeling using recurrent neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "authors": "J. Zhou and W. Xu"
    },
    {
      "index": 22,
      "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.02593",
      "authors": "D. Marcheggiani, A. Frolov, and I. Titov",
      "orig_title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling",
      "paper_id": "1701.02593v2"
    },
    {
      "index": 23,
      "title": "Deep semantic role labeling: What works and what‚Äôs next",
      "abstract": "",
      "year": "2017",
      "venue": "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "L. He, K. Lee, M. Lewis, and L. Zettlemoyer"
    },
    {
      "index": 24,
      "title": "Syntax-aware Multilingual Semantic Role Labeling",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.00310",
      "authors": "S. He, Z. Li, and H. Zhao",
      "orig_title": "Syntax-aware multilingual semantic role labeling",
      "paper_id": "1909.00310v3"
    },
    {
      "index": 25,
      "title": "Recent Trends in Deep Learning Based Natural Language Processing",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine",
      "authors": "T. Young, D. Hazarika, S. Poria, and E. Cambria",
      "orig_title": "Recent trends in deep learning based natural language processing",
      "paper_id": "1708.02709v8"
    },
    {
      "index": 26,
      "title": "Natural language processing (NLP) in management research: A literature review",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Management Analytics",
      "authors": "Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu"
    },
    {
      "index": 27,
      "title": "What exactly is artificial intelligence, anyway?.",
      "abstract": "",
      "year": "2018",
      "venue": "Wall Street Journal Online Article",
      "authors": "T. Greenwald"
    },
    {
      "index": 28,
      "title": "Critical analysis of big data challenges and analytical methods",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Business Research",
      "authors": "U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody"
    },
    {
      "index": 29,
      "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1506.00019",
      "authors": "Z. C. Lipton, J. Berkowitz, and C. Elkan",
      "orig_title": "A critical review of recurrent neural networks for sequence learning",
      "paper_id": "1506.00019v4"
    },
    {
      "index": 30,
      "title": "Convolutional Neural Networks for Sentence Classification",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1408.5882",
      "authors": "Y. Kim",
      "orig_title": "Convolutional neural networks for sentence classification",
      "paper_id": "1408.5882v2"
    },
    {
      "index": 31,
      "title": "Parsing natural scenes and natural language with recursive neural networks",
      "abstract": "",
      "year": "2011",
      "venue": "28th international conference on machine learning (ICML-11)",
      "authors": "R. Socher, C. C. Lin, C. Manning, and A. Y. Ng"
    },
    {
      "index": 32,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 33,
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "C. dos Santos and M. Gatti"
    },
    {
      "index": 34,
      "title": "Effective use of word order for text categorization with convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.1058",
      "authors": "R. Johnson and T. Zhang"
    },
    {
      "index": 35,
      "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Johnson and T. Zhang",
      "orig_title": "Semi-supervised convolutional neural networks for text categorization via region embedding",
      "paper_id": "1504.01255v3"
    },
    {
      "index": 36,
      "title": "Relation classification via convolutional deep neural network",
      "abstract": "",
      "year": "2014",
      "venue": "COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
      "authors": "D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao"
    },
    {
      "index": 37,
      "title": "Relation extraction: Perspective from convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "1st Workshop on Vector Space Modeling for Natural Language Processing",
      "authors": "T. H. Nguyen and R. Grishman"
    },
    {
      "index": 38,
      "title": "Recurrent neural network based language model",
      "abstract": "",
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association",
      "authors": "T. Mikolov, M. Karafi√°t, L. Burget, J. ƒåernock·ª≥, and S. Khudanpur"
    },
    {
      "index": 39,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "S. Hochreiter and J. Schmidhuber"
    },
    {
      "index": 40,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio"
    },
    {
      "index": 41,
      "title": "Wasserstein t-SNE",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.07875",
      "authors": "M. Arjovsky, S. Chintala, and L. Bottou",
      "orig_title": "Wasserstein gan",
      "paper_id": "2205.07531v2"
    },
    {
      "index": 42,
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel"
    },
    {
      "index": 43,
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06434",
      "authors": "A. Radford, L. Metz, and S. Chintala",
      "orig_title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "paper_id": "1511.06434v2"
    },
    {
      "index": 44,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.10196",
      "authors": "T. Karras, T. Aila, S. Laine, and J. Lehtinen",
      "orig_title": "Progressive growing of GANs for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 45,
      "title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.03135",
      "authors": "N. Tavaf, A. Torfi, K. Ugurbil, and P.-F. Van de Moortele",
      "orig_title": "GRAPPA-GANs for Parallel MRI Reconstruction",
      "paper_id": "2101.03135v2"
    },
    {
      "index": 46,
      "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "L. Yu, W. Zhang, J. Wang, and Y. Yu"
    },
    {
      "index": 47,
      "title": "Adversarial Learning for Neural Dialogue Generation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1701.06547",
      "authors": "J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky",
      "orig_title": "Adversarial learning for neural dialogue generation",
      "paper_id": "1701.06547v5"
    },
    {
      "index": 48,
      "title": "Thumbs up?: sentiment classification using machine learning techniques",
      "abstract": "",
      "year": "2002",
      "venue": "ACL-02 conference on Empirical methods in natural language processing-Volume 10",
      "authors": "B. Pang, L. Lee, and S. Vaithyanathan"
    },
    {
      "index": 49,
      "title": "Distributional structure",
      "abstract": "",
      "year": "1954",
      "venue": "Word",
      "authors": "Z. S. Harris"
    },
    {
      "index": 50,
      "title": "‚ÄúA neural probabilistic language model",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "Distributed Representations of Sentences and Documents",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDistributed representations of sentences and documents",
      "paper_id": "1405.4053v2"
    },
    {
      "index": 52,
      "title": "‚ÄúDistributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "Skip-Thought Vectors",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSkip-thought vectors",
      "paper_id": "1506.06726v1"
    },
    {
      "index": 54,
      "title": "‚ÄúEfficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "LAP LAMBERT Academic Publishing, 2015",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "G.¬†Lebanon et¬†al., Riemannian geometry and statistical machine\nlearning."
    },
    {
      "index": 56,
      "title": "Cambridge University Press, 2014",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "J.¬†Leskovec, A.¬†Rajaraman, and J.¬†D. Ullman, Mining of massive datasets."
    },
    {
      "index": 57,
      "title": "‚ÄúNeural network methods for natural language processing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "‚ÄúA character-based convolutional neural network for language-agnostic Twitter sentiment analysis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEnriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 60,
      "title": "Compositional Morphology for Word Representations and Language Modelling",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCompositional morphology for word representations and language modelling",
      "paper_id": "1405.4273v1"
    },
    {
      "index": 61,
      "title": "Get To The Point: Summarization with Pointer-Generator Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGet to the point: Summarization with pointer-generator networks",
      "paper_id": "1704.04368v2"
    },
    {
      "index": 62,
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA deep reinforced model for abstractive summarization",
      "paper_id": "1705.04304v3"
    },
    {
      "index": 63,
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúScheduled sampling for sequence prediction with recurrent neural networks",
      "paper_id": "1506.03099v3"
    },
    {
      "index": 64,
      "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA continuous relaxation of beam search for end-to-end training of neural sequence models",
      "paper_id": "1708.00111v2"
    },
    {
      "index": 65,
      "title": "Stochastic Beams and Where to Find Them: The Gumbel-Top-ùëò Trick for Sampling Sequences Without Replacement",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúStochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement",
      "paper_id": "1903.06059v2"
    },
    {
      "index": 66,
      "title": "‚ÄúRouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "‚ÄúBLEU: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "‚ÄúMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "‚ÄúDeep reinforcement learning for sequence to sequence models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 71,
      "title": "‚ÄúReinforcement learning neural Turing machines-revised",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "‚ÄúSimple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "R.¬†S. Sutton and A.¬†G. Barto, Reinforcement learning: An introduction."
    },
    {
      "index": 74,
      "title": "‚Äù Machine Learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "‚ÄúSearch-based structured prediction",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEnd-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 77,
      "title": "‚ÄúRecurrent models of visual attention",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúUtilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
      "paper_id": "1903.09588v1"
    },
    {
      "index": 79,
      "title": "‚ÄúEvaluation of NLP systems",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "‚ÄúAsk me anything: Dynamic memory networks for natural language processing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBidirectional LSTM-CRF models for sequence tagging",
      "paper_id": "1508.01991v1"
    },
    {
      "index": 82,
      "title": "Globally Normalized Transition-Based Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGlobally normalized transition-based neural networks",
      "paper_id": "1603.06042v2"
    },
    {
      "index": 83,
      "title": "‚ÄúPart-of-speech tagging of building codes empowered by deep learning and transformational rules",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "Empower Sequence Labeling with Task-Aware Neural Language Model",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEmpower sequence labeling with task-aware neural language model",
      "paper_id": "1709.04109v4"
    },
    {
      "index": 85,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEnd-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
      "paper_id": "1603.01354v5"
    },
    {
      "index": 87,
      "title": "‚ÄúRobust multilingual part-of-speech tagging via adversarial training",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "‚ÄúFinding function in form: Compositional character models for open vocabulary word representation",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "‚ÄúContextual string embeddings for sequence labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMorphosyntactic tagging with a Meta-BiLSTM model over context sensitive token encodings",
      "paper_id": "1805.08237v1"
    },
    {
      "index": 91,
      "title": "‚ÄúJoint RNN-based greedy parsing and word composition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "‚ÄúDeep neural networks for syntactic parsing of morphologically rich languages",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 93,
      "title": "‚ÄúWhat do recurrent neural network grammars learn about syntax?",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "‚ÄúIn-order transition-based constituent parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "‚ÄúImproving neural parsing by disentangling model combination and reranking effects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "Constituency Parsing with a Self-Attentive Encoder",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúConstituency parsing with a self-attentive encoder",
      "paper_id": "1805.01052v1"
    },
    {
      "index": 97,
      "title": "‚ÄúA fast and accurate dependency parser using neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 99,
      "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSimple and accurate dependency parsing using bidirectional LSTM feature representations",
      "paper_id": "1603.04351v3"
    },
    {
      "index": 100,
      "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTransition-based dependency parsing with stack long short-term memory",
      "paper_id": "1505.08075v1"
    },
    {
      "index": 101,
      "title": "‚ÄúDeep learning for natural language parsing",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "‚ÄúParsing clinical text using the state-of-the-art deep learning based parsers: a systematic comparison",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEfficient second-order treecrf for neural dependency parsing",
      "paper_id": "2005.00975v2"
    },
    {
      "index": 104,
      "title": "Deep Biaffine Attention for Neural Dependency Parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeep biaffine attention for neural dependency parsing",
      "paper_id": "1611.01734v3"
    },
    {
      "index": 105,
      "title": "‚ÄúDeep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEncoding sentences with graph convolutional networks for semantic role labeling",
      "paper_id": "1703.04826v4"
    },
    {
      "index": 107,
      "title": "Linguistically-Informed Self-Attention for Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLinguistically-informed self-attention for semantic role labeling",
      "paper_id": "1804.08199v3"
    },
    {
      "index": 108,
      "title": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúJointly predicting predicates and arguments in neural semantic role labeling",
      "paper_id": "1805.04787v2"
    },
    {
      "index": 109,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 110,
      "title": "‚ÄúDeep semantic role labeling with self-attention",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "‚ÄúDependency or span",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "‚ÄúTowards robust linguistic analysis using OntoNotes",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "‚ÄúA convolutional neural network for modelling sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "‚ÄúDeep sentence embedding using long short-term memory networks: Analysis and application to information retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "‚ÄúHierarchical attention networks for document classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "‚ÄúRecurrent convolutional neural networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "A C-LSTM Neural Network for Text Classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA C-LSTM neural network for text classification",
      "paper_id": "1511.08630v2"
    },
    {
      "index": 118,
      "title": "Deep Learning Based Text Classification: A Comprehensive Review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeep learning based text classification: A comprehensive review",
      "paper_id": "2004.03705v3"
    },
    {
      "index": 119,
      "title": "‚ÄúA comparative review on deep learning models for text classification",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "Very Deep Convolutional Networks for Text Classification",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúVery deep convolutional networks for text classification",
      "paper_id": "1606.01781v2"
    },
    {
      "index": 121,
      "title": "‚ÄúDeep pyramid convolutional neural networks for text categorization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSupervised and semi-supervised text categorization using LSTM for region embeddings",
      "paper_id": "1602.02373v2"
    },
    {
      "index": 123,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúUniversal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 124,
      "title": "‚ÄúNatural language processing (almost) from scratch",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "‚ÄúInvestigation of recurrent-neural-network architectures and learning methods for spoken language understanding",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "NeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúNeuroNER: an easy-to-use program for named-entity recognition based on neural networks",
      "paper_id": "1705.05487v1"
    },
    {
      "index": 127,
      "title": "Cloze-driven Pretraining of Self-attention Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCloze-driven pretraining of self-attention networks",
      "paper_id": "1903.07785v1"
    },
    {
      "index": 128,
      "title": "‚ÄúIntroduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Semi-Supervised Sequence Modeling with Cross-View Training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSemi-supervised sequence modeling with cross-view training",
      "paper_id": "1809.08370v1"
    },
    {
      "index": 130,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 131,
      "title": "‚ÄúSemantic compositionality through recursive matrix-vector spaces",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "‚ÄúSemantic relation extraction using sequential and tree-structured lstm with attention",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "more context and more openness: A review and outlook for relation extraction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "‚ÄúDeep reinforcement learning for mention-ranking coreference models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "‚ÄúHigher-order coreference resolution with coarse-to-fine inference",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "‚ÄúEnd-to-end deep reinforcement learning based coreference resolution",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "CorefQA: Coreference Resolution as Query-based Span Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCorefqa: Coreference resolution as query-based span prediction",
      "paper_id": "1911.01746v4"
    },
    {
      "index": 138,
      "title": "‚ÄúEvent extraction via dynamic multi-pooling convolutional neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "‚ÄúGraph convolutional networks with argument-aware pooling for event detection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "‚ÄúJoint entity and event extraction with generative adversarial imitation learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "‚ÄúA novel joint biomedical event extraction framework via two-level modeling of documents",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "‚ÄúSentiment analysis: Capturing favorability using natural language processing",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "‚ÄúMining the peanut gallery: Opinion extraction and semantic classification of product reviews",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "‚ÄúApplication of deep learning approaches for sentiment analysis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "‚ÄúSentiment analysis using deep learning architectures: a review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "‚ÄúDocument modeling with gated recurrent neural network for sentiment classification",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "‚ÄúDomain adaptation for large-scale sentiment classification: A deep learning approach",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "‚ÄúLstm with sentence representations for document-level sentiment classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "‚ÄúA cnn-bilstm model for document-level sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "‚ÄúSemi-supervised recursive autoencoders for predicting sentiment distributions",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "‚ÄúPredicting polarities of tweets by composing word embeddings with long short-term memory",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "‚ÄúRecursive deep models for semantic compositionality over a sentiment treebank",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "‚ÄúClassification of sentence level sentiment analysis using cloud machine learning techniques",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "‚Äù Information Processing & Management",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "‚ÄúAttention-based LSTM for aspect-level sentiment classification",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "‚ÄúSentic lstm: a hybrid network for targeted aspect-based sentiment analysis",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "paper_id": "1904.02232v2"
    },
    {
      "index": 158,
      "title": "Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDouble embeddings and CNN-based sequence labeling for aspect extraction",
      "paper_id": "1805.04601v1"
    },
    {
      "index": 159,
      "title": "‚ÄúDeep learning for aspect-based sentiment analysis: a comparative review",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "‚ÄúA multi-layer dual attention deep learning model with refined word embeddings for aspect-based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "‚ÄúA novel aspect-guided deep transition model for aspect based sentiment analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Prentice Hall, 2008",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "D.¬†Jurafsky and J.¬†H. Martin, Speech and Language Processing."
    },
    {
      "index": 163,
      "title": "‚ÄúRecurrent continuous translation models",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "‚ÄúMachine translation using deep learning: An overview",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "‚ÄúA survey of deep learning techniques for neural machine translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "‚ÄúThe Georgetown-IBM experiment",
      "abstract": "",
      "year": "1955",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúNeural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 168,
      "title": "B. Van Merri√´nboer",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "‚ÄúSequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGoogle‚Äôs neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 171,
      "title": "‚ÄúConvolutional sequence to sequence learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúAttention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 173,
      "title": "Weighted Transformer Network for Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúWeighted transformer network for machine translation",
      "paper_id": "1711.02132v1"
    },
    {
      "index": 174,
      "title": "Self-Attention with Relative Position Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSelf-attention with relative position representations",
      "paper_id": "1803.02155v2"
    },
    {
      "index": 175,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúUnderstanding back-translation at scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 176,
      "title": "Massively Multilingual Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMassively multilingual neural machine translation",
      "paper_id": "1903.00089v3"
    },
    {
      "index": 177,
      "title": "Incorporating BERT into Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúIncorporating bert into neural machine translation",
      "paper_id": "2002.06823v1"
    },
    {
      "index": 178,
      "title": "M. Ghazvininejad",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRobust neural machine translation with doubly adversarial inputs",
      "paper_id": "1906.02443v1"
    },
    {
      "index": 180,
      "title": "Bridging the Gap between Training and Inference for Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBridging the gap between training and inference for neural machine translation",
      "paper_id": "1906.02448v2"
    },
    {
      "index": 181,
      "title": "Towards Making the Most of BERT in Neural Machine Translation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTowards making the most of bert in neural machine translation",
      "paper_id": "1908.05672v5"
    },
    {
      "index": 182,
      "title": "‚ÄúQuestion answering with subgraph embeddings",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "‚ÄúBaseball: an automatic question-answerer",
      "abstract": "",
      "year": "1961",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "‚ÄúIBM‚Äôs statistical question answering system",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "‚ÄúQuestion answering passage retrieval using dependency relations",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "‚ÄúConvolutional neural tensor network architecture for community-based question answering",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "‚ÄúA machine learning approach to answering questions for reading comprehension tests",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "Dynamic Coattention Networks for Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDynamic coattention networks for question answering",
      "paper_id": "1611.01604v4"
    },
    {
      "index": 189,
      "title": "C. Lawrence Zitnick",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúAsk your neurons: A neural-based approach to answering questions about images",
      "paper_id": "1505.01121v3"
    },
    {
      "index": 191,
      "title": "attend and answer: Exploring question-guided spatial attention for visual question answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúHuman attention in visual question answering: Do humans and deep networks look at the same regions?",
      "paper_id": "1606.03556v2"
    },
    {
      "index": 193,
      "title": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBlock: Bilinear superdiagonal fusion for visual question answering and visual relationship detection",
      "paper_id": "1902.00038v2"
    },
    {
      "index": 194,
      "title": "‚ÄúSelf-critical reasoning for robust visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 195,
      "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents",
      "paper_id": "1611.04230v1"
    },
    {
      "index": 196,
      "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRanking sentences for extractive summarization with reinforcement learning",
      "paper_id": "1802.08636v2"
    },
    {
      "index": 197,
      "title": "‚ÄúA neural attention model for abstractive sentence summarization",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "‚ÄúAbstractive document summarization with a graph-based attentional neural model",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "‚ÄúAbstractive text summarization using sequence-to-sequence RNNs and beyond",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "‚ÄúTeaching machines to read and comprehend",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúIncorporating copying mechanism in sequence-to-sequence learning",
      "paper_id": "1603.06393v3"
    },
    {
      "index": 202,
      "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúFast abstractive summarization with reinforce-selected sentence rewriting",
      "paper_id": "1805.11080v1"
    },
    {
      "index": 203,
      "title": "Neural Document Summarization by Jointly Learning to Score and Select Sentences",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúNeural document summarization by jointly learning to score and select sentences",
      "paper_id": "1807.02305v1"
    },
    {
      "index": 204,
      "title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúNeural abstractive text summarization with sequence-to-sequence models",
      "paper_id": "1812.02303v4"
    },
    {
      "index": 205,
      "title": "‚ÄúMulti-document summarization via deep learning techniques: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "‚ÄúA hybrid deep learning architecture for opinion-oriented multi-document summarization based on multi-feature fusion",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 207,
      "title": "‚ÄúHibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "‚ÄúDialogue systems for intelligent human computer interactions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 209,
      "title": "‚ÄúMulti-domain joint semantic frame parsing using bi-directional RNN-LSTM",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "Understanding Chatbot-mediated Task Management",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúUnderstanding chatbot-mediated task management",
      "paper_id": "1802.03109v1"
    },
    {
      "index": 211,
      "title": "Goal-Oriented Chatbot Dialog Management Bootstrapping with Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGoal-oriented chatbot dialog management bootstrapping with transfer learning",
      "paper_id": "1802.00500v2"
    },
    {
      "index": 212,
      "title": "‚ÄúDeep reinforcement learning for dialogue generation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA network-based end-to-end trainable task-oriented dialogue system",
      "paper_id": "1604.04562v3"
    },
    {
      "index": 214,
      "title": "‚ÄúEnd-to-end LSTM-based dialog control optimized with supervised and reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "‚ÄúEnd-to-end memory networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "Learning End-to-End Goal-Oriented Dialog",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLearning end-to-end goal-oriented dialog",
      "paper_id": "1605.07683v4"
    },
    {
      "index": 217,
      "title": "‚ÄúData-driven response generation in social media",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "‚ÄúAn information retrieval approach to short text conversation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "‚ÄúConvolutional neural network architectures for matching natural language sentences",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúThe Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "paper_id": "1506.08909v3"
    },
    {
      "index": 221,
      "title": "‚ÄúLearning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "‚ÄúMulti-turn response selection for chatbots with deep attention matching network",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "‚ÄúGenerating sentences from a continuous space",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "Adversarial Evaluation of Dialogue Models",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúAdversarial evaluation of dialogue models",
      "paper_id": "1701.08198v1"
    },
    {
      "index": 225,
      "title": "A Neural Conversational Model",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA neural conversational model",
      "paper_id": "1506.05869v3"
    }
  ]
}