{
  "paper_id": "2008.11573v1",
  "title": "Proof.",
  "sections": {
    "i-b prior art and comparisons": "Deep learning based methods have been shown to be successful in various classification tasks [ref]8. Transfer learning approaches have been popular in the sentiment analysis and shown to be successful especially in datasets with small number of instances [ref]9. Through transfer learning, an extensive amount of unlabeled data in the social media have been incorporated to increase the performance of the target sentiment analysis task, e.g., [ref]9 employs 1.7 billion tweets with emojis to pretrain the network. However,  demonstrates that the transfer learning approach does not improve the performance on the SemEval emotion classification competition datasets, which is our target due to the richness of its labels, which has significantly more number of instances compared to the number of instances used in [ref]9.  emulates a multi-label classifier through a binary classifier for each of the four opposite emotions that are on the opposite sides of the Plutchik’s wheel of emotions as shown in Fig. 1 such as joy and sadness. However, their approach does not include the correlations to the rest of the labels since they train each of the four classifiers with the objective of binary classification of the opposite sides. To remedy those issues, we introduce a multi-label deep learning model for the emoji prediction task that directly predicts the active set of labels simultaneously, i.e., in the multi-task setting. Moreover, the multi-label classification is a generalization of binary and multi-class classification tasks as we describe through remarks, thus, our method is also applicable to these tasks. The multi-label classification also requires a prediction method that converts scores into the predictions, for which we derive a class specific thresholding by macro-f1 maximization in linear time complexity. Multi-label classification has an inherent issue of data imbalance . Although significant research has been performed in the literature, the class imbalance problem remains a challenge for multi-label classification . Consider the multi-label classification task with 161616 distinct labels. There are 216superscript2162^{16} possible combinations in the superset of the labels. Accordingly, it is not feasible to obtain balanced data for each combination of the labels. Many studies in multi-output classification either try to balance the data by resampling or ignore the imbalance . Yet, the over-sampling and under-sampling methods are not designed for the multi-label classification, thus, their adaptation to the multi-label setting is not straightforward . One heuristic that is widely adapted is using inverse class frequency per class as a weighting factor . However, this heuristic results in suboptimal performance as shown by  and in Section IV-E.  replaces the inverse number of instances with the expected volume of instances and a controlling hyperparameter.  proposes to use class prior probabilities as weights for the cross-entropy loss. Commonly, these methods propose static weights for each class. To remedy the label imbalance in the multi-label setting, we introduce a novel dynamic weighting method, which equalizes the contribution of each class to the loss. We use focal loss  to incorporate the hardness of the instances and our dynamic weighting method can readily be adapted to other losses as we show through a remark. Recent language models such as BERT 6 have been dominating the areas in the NLP literature, however, they contain an excessive amount of parameters. Accordingly, training or fine-tuning these models require an excessive amount of resources 6. We employ RoBERTa-XLM , which is a robustly trained BERT on 100 languages, as feature extractor to benefit from BERT as well as reducing the amount of required resources. SemEval emotion classification competition  has paved the way for many multi-label sentiment analysis models. EMA, PARTNA are among the models that opt for the more traditional support vector machine approaches and still achieve the best results in the Arabic language 8. On the other hand, more recent long short-term memory (LSTM), convolutional neural network (CNNs) and attention models are also adopted to obtain the highest ranked results in English and Spanish 9 . It is important to note that most of these models are language specific, and use special embeddings such as AraVec  or special lexicons paired with language specific preprocessing steps. Tw-StAR attempts to create a generic model to apply multiple languages, yet, is ranked behind the language-specific models . We introduce a framework that uses bidirectional LSTM with attention and multi-label focal loss, which achieves the best score only using a single model on 7 of the 9 metrics on three different languages of the SemEval emotion classification competition ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Semeval-2018 task 1: Affect in tweets",
      "abstract": "",
      "year": "2018",
      "venue": "International Workshop on Semantic Evaluation (Semeval-2018)",
      "authors": "S. M. Mohammad, F. Bravo-Marquez, M. Salameh, and S. Kiritchenko"
    },
    {
      "index": 1,
      "title": "Sentivec: Learning sentiment-context vector via kernel optimization function for sentiment analysis",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "L. Zhu, W. Li, Y. Shi, and K. Guo"
    },
    {
      "index": 2,
      "title": "Sentiment analysis and opinion mining",
      "abstract": "",
      "year": "2012",
      "venue": "Synthesis Lectures on Human Language Technologies",
      "authors": "B. Liu"
    },
    {
      "index": 3,
      "title": "Coarse alignment of topic and sentiment: A unified model for cross-lingual sentiment classification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "D. Wang, B. Jing, C. Lu, J. Wu, G. Liu, C. Du, and F. Zhuang"
    },
    {
      "index": 4,
      "title": "A general psychoevolutionary theory of emotion",
      "abstract": "",
      "year": "1980",
      "venue": "Theories of Emotion",
      "authors": "R. Plutchik"
    },
    {
      "index": 5,
      "title": "A Survey on Multi-output Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "D. Xu, Y. Shi, I. W. Tsang, Y.-S. Ong, C. Gong, and X. Shen",
      "orig_title": "Survey on multi-output learning",
      "paper_id": "1901.00248v2"
    },
    {
      "index": 6,
      "title": "Does tail label help for large-scale multi-label learning?",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "T. Wei and Y.-F. Li"
    },
    {
      "index": 7,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Y. LeCun, Y. Bengio, and G. Hinton",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 8,
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann",
      "orig_title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "paper_id": "1708.00524v2"
    },
    {
      "index": 9,
      "title": "Practical Text Classification With Large Pre-Trained Language Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.01207",
      "authors": "N. Kant, R. Puri, N. Yakovenko, and B. Catanzaro",
      "orig_title": "Practical text classification with large pre-trained language models",
      "paper_id": "1812.01207v1"
    },
    {
      "index": 10,
      "title": "Distant supervision for emotion classification with discrete binary values",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics",
      "authors": "J. Suttles and N. Ide"
    },
    {
      "index": 11,
      "title": "Learning deep representation for imbalanced classification",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "C. Huang, Y. Li, C. Change Loy, and X. Tang"
    },
    {
      "index": 12,
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie",
      "orig_title": "Class-balanced loss based on effective number of samples",
      "paper_id": "1901.05555v1"
    },
    {
      "index": 13,
      "title": "Learning from imbalanced data sets with weighted cross-entropy function",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Processing Letters",
      "authors": "Y. S. Aurelio, G. M. de Almeida, C. L. de Castro, and A. P. Braga"
    },
    {
      "index": 14,
      "title": "Focal Loss for Dense Object Detection",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár",
      "orig_title": "Focal loss for dense object detection",
      "paper_id": "1708.02002v2"
    },
    {
      "index": 15,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 16,
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.02116",
      "authors": "A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "Unsupervised cross-lingual representation learning at scale",
      "paper_id": "1911.02116v2"
    },
    {
      "index": 17,
      "title": "Ema at semeval-2018 task 1: Emotion mining for arabic",
      "abstract": "",
      "year": "2018",
      "venue": "The 12th International Workshop on Semantic Evaluation",
      "authors": "G. Badaro, O. El Jundi, A. Khaddaj, A. Maarouf, R. Kain, H. Hajj, and W. El-Hajj"
    },
    {
      "index": 18,
      "title": "NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.06658",
      "authors": "C. Baziotis, N. Athanasiou, A. Chronopoulou, A. Kolovou, G. Paraskevopoulos, N. Ellinas, S. Narayanan, and A. Potamianos",
      "orig_title": "Ntua-slp at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning",
      "paper_id": "1804.06658v1"
    },
    {
      "index": 19,
      "title": "Psyml at semeval-2018 task 1: Transfer learning for sentiment and emotion analysis",
      "abstract": "",
      "year": "2018",
      "venue": "The 12th International Workshop on Semantic Evaluation",
      "authors": "G. Gee and E. Wang"
    },
    {
      "index": 20,
      "title": "Aravec: A set of arabic word embedding models for use in arabic nlp",
      "abstract": "",
      "year": "2017",
      "venue": "Procedia Computer Science",
      "authors": "A. B. Mohammad, K. Eissa, and S. El-Beltagy"
    },
    {
      "index": 21,
      "title": "Tw-star at semeval-2018 task 1: Preprocessing impact on multi-label emotion classification",
      "abstract": "",
      "year": "2018",
      "venue": "The 12th International Workshop on Semantic Evaluation",
      "authors": "H. Mulki, C. B. Ali, H. Haddad, and I. Babaoğlu"
    },
    {
      "index": 22,
      "title": "Natural language processing (almost) from scratch",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of Machine Learning Research",
      "authors": "R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa"
    },
    {
      "index": 23,
      "title": "Nistributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean"
    },
    {
      "index": 24,
      "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.06226",
      "authors": "T. Kudo and J. Richardson"
    },
    {
      "index": 25,
      "title": "Finding structure in time",
      "abstract": "",
      "year": "1990",
      "venue": "Cognitive Science",
      "authors": "J. L. Elman"
    },
    {
      "index": 26,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "S. Hochreiter and J. Schmidhuber"
    },
    {
      "index": 27,
      "title": "Hierarchical attention networks for document classification",
      "abstract": "",
      "year": "2016",
      "venue": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy"
    },
    {
      "index": 28,
      "title": "Smart Mining for Deep Metric Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "B. Harwood, V. Kumar BG, G. Carneiro, I. Reid, and T. Drummond",
      "orig_title": "Smart mining for deep metric learning",
      "paper_id": "1704.01285v3"
    },
    {
      "index": 29,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1607.04606",
      "authors": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov",
      "orig_title": "Enriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 30,
      "title": "A context integrated model for multi-label emotion detection",
      "abstract": "",
      "year": "2018",
      "venue": "Procedia Computer Science",
      "authors": "A. E. Samy, S. R. El-Beltagy, and E. Hassanien"
    },
    {
      "index": 31,
      "title": "Hybrid feature model for emotion recognition in arabic text",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "N. Alswaidan and M. E. B. Menai"
    },
    {
      "index": 32,
      "title": "Elirf-upv at irosva: Transformer encoders for spanish irony detection",
      "abstract": "",
      "year": "2019",
      "venue": "IberLEF@SEPLN",
      "authors": "J.-Á. González, L.-F. Hurtado, and F. Pla"
    },
    {
      "index": 33,
      "title": "Optuna: A Next-generation Hyperparameter Optimization Framework",
      "abstract": "",
      "year": "2019",
      "venue": "ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama",
      "orig_title": "Optuna: A next-generation hyperparameter optimization framework",
      "paper_id": "1907.10902v1"
    },
    {
      "index": 34,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 35,
      "title": "Generating semantic similarity atlas for natural languages",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)",
      "authors": "L. K. Senel, İ. Utlu, V. Yücesoy, A. Koc, and T. Cukur"
    },
    {
      "index": 36,
      "title": "Deep learning for extreme multi-label text classification",
      "abstract": "",
      "year": "2017",
      "venue": "International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": "J. Liu, W.-C. Chang, Y. Wu, and Y. Yang"
    }
  ]
}