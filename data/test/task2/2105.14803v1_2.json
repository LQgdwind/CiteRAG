{
  "paper_id": "2105.14803v1",
  "title": "Gradient-based Data Subversion Attack Against Binary Classifiers",
  "sections": {
    "related work": "Studies on the label contamination attacks have been more focused on Support Vector Machines (SVM) and are not generalized to other classifiers  [ref]6 [ref]39. The attack strategy by Biggio et al in  flips the labels of the data points that are non-support vectors and decreases the probability of contaminating the labels of support vectors. Also, they demonstrated that a random label flip of 40% of data samples in a synthetic dataset can deteriorate the performance of SVMs by reducing their accuracy (≈\\approx25%). They further crafted inputs using a gradient ascent method and inserted them into the training sets and degraded the performance of the classifier [ref]6. Xiao et al  proposed the attacks on feature selection algorithms and investigated their robustness. Xiao et al [ref]39 proposed the approach ALFA for finding the near-optimal label flips and compared it with uniform random flip, nearest-first flip (minimum distance to the decision boundary) and furthest-first flip (maximum distance to the decision boundary) strategies. Moreover, they assumed that the attacker has full knowledge of the victim’s learning model. This assumption considered as the starting point for understanding the attack strategies [ref]27. Paudice et al  used a heuristic approach which aims to find the required number of instances that maximizes the error function of the model on a validation set. Further, the authors proposed a defense strategy that re-labels the points that are suspicious to be malicious based on the nearest neighbor approach. Mei et al [ref]27 formalized the training set attack as a bilevel optimization framework. Their objective was to modify the training data such that the distance between the learner’s learned model and the original model is minimum, in addition to satisfying the attacker’s objectives using the least number of manipulations. They demonstrated their strategy on the SVM, Logistic and Linear Regressions. Zhao et al  studied the label contamination attack as a mixed integer bilevel optimal problem and proposed Projected Gradient Ascent (PGA) algorithm to solve it. Their approach was more realistic compared to prior-arts as they considered attackers with arbitrary objectives in a black-box setting. Also, they studied the transferability of poisoning attacks. Transferability of data poisoning attacks has been studied in prior works   . Muñoz-González et al developed a data poisoning attack for deep networks  and demonstrated the transferability of their attack across different learning algorithms. Demontis et al   studied the transferability of evasion and poisoning attacks in their recent work. These studies shed some insights on the factors that can influence the attack transfer. In our work, we show the transferability of our attack in various learning algorithms. The existing label contamination attack strategies analyze all the training instances for finding a set of near-optimal data instances to flip. They solve a bilevel optimization problem [ref]39  [ref]27  which is complex in terms of solving two optimization problems within each iteration. The attack strategy PGA  trains a linear classifier in each iteration which is more efficient than the former. However, they consider the dual form of the classifiers that increases the number of computations when the training set is large. Koh et al  efficiently approximated influence functions which formalize the impact of a training point on a machine learning model. They computed the influence by forming a quadratic approximation to the empirical risk around model parameters and taking a single newton step. Since we compute the label flip attack on a given dataset, we filter the points based on the gradients of the loss function with respect to the predicted label. Recent studies have developed data poisoning attacks and defenses for online learning   . In this setting, attacker has to consider the online nature of the problem where data is sequential in nature and the classifier is a more complex function of the data stream. Moreover, the order of the data influences the attack. Further, the knowledge about the future data point is limited for the attacker. All of these together makes the online learning attack more challenging. We consider the offline setting in this work and propose an algorithm to find the candidate set by filtering the points from the training instances. We evaluate our approach in multiple datasets. Further, the transferability of the proposed algorithm on various machine learning models is studied."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Explicit defense actions against test-set attacks",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Alfeld, S., Zhu, X., Barford, P."
    },
    {
      "index": 1,
      "title": "Stackelberg security games (ssg) basics and application overview",
      "abstract": "",
      "year": "2016",
      "venue": "Improving Homeland Security Decisions. Cambridge Univ. Press",
      "authors": "An, B., Tambe, M., Sinha, A."
    },
    {
      "index": 2,
      "title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.04637",
      "authors": "Anderson, H.S., Roth, P.",
      "orig_title": "Ember: An open dataset for training static pe malware machine learning models",
      "paper_id": "1804.04637v2"
    },
    {
      "index": 3,
      "title": "Bagging classifiers for fighting poisoning attacks in adversarial classification tasks",
      "abstract": "",
      "year": "2011",
      "venue": "International workshop on multiple classifier systems",
      "authors": "Biggio, B., Corona, I., Fumera, G., Giacinto, G., Roli, F."
    },
    {
      "index": 4,
      "title": "Support vector machines under adversarial label noise",
      "abstract": "",
      "year": "2011",
      "venue": "Asian Conference on Machine Learning",
      "authors": "Biggio, B., Nelson, B., Laskov, P."
    },
    {
      "index": 5,
      "title": "Poisoning attacks against support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "29th International Coference on International Conference on Machine Learning. Omnipress",
      "authors": "Biggio, B., Nelson, B., Laskov, P."
    },
    {
      "index": 6,
      "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "Biggio, B., Roli, F.",
      "orig_title": "Wild patterns: Ten years after the rise of adversarial machine learning",
      "paper_id": "1712.03141v2"
    },
    {
      "index": 7,
      "title": "XGBoost: A Scalable Tree Boosting System",
      "abstract": "",
      "year": "2016",
      "venue": "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM",
      "authors": "Chen, T., Guestrin, C.",
      "orig_title": "Xgboost: A scalable tree boosting system",
      "paper_id": "1603.02754v3"
    },
    {
      "index": 8,
      "title": "Casting out demons: Sanitizing training data for anomaly sensors",
      "abstract": "",
      "year": "2008",
      "venue": "2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE",
      "authors": "Cretu, G.F., Stavrou, A., Locasto, M.E., Stolfo, S.J., Keromytis, A.D."
    },
    {
      "index": 9,
      "title": "Adversarial classification",
      "abstract": "",
      "year": "2004",
      "venue": "tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM",
      "authors": "Dalvi, N., Domingos, P., Sanghai, S., Verma, D., et al."
    },
    {
      "index": 10,
      "title": "Securing Machine Learning against Adversarial Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Ph.D. thesis, University of Cagliari",
      "authors": "Demontis, A."
    },
    {
      "index": 11,
      "title": "Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "28th USENIX Security Symposium (USENIX Security 19). USENIX Association, Santa Clara, CA",
      "authors": "Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru, C., Roli, F.",
      "orig_title": "Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks",
      "paper_id": "1809.02861v4"
    },
    {
      "index": 12,
      "title": "UCI machine learning repository",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Dua, D., Graff, C."
    },
    {
      "index": 13,
      "title": "Greedy function approximation: a gradient boosting machine",
      "abstract": "",
      "year": "2001",
      "venue": "Annals of statistics",
      "authors": "Friedman, J.H."
    },
    {
      "index": 14,
      "title": "Making machine learning robust against adversarial inputs",
      "abstract": "",
      "year": "2018",
      "venue": "Communications of the ACM",
      "authors": "Goodfellow, I., McDaniel, P., Papernot, N."
    },
    {
      "index": 15,
      "title": "Insight into Insiders and IT: A Survey of Insider Threat Taxonomies, Analysis, Modeling, and Countermeasures",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "Homoliak, I., Toffalini, F., Guarnizo, J., Elovici, Y., Ochoa, M.",
      "orig_title": "Insight into insiders and it: A survey of insider threat taxonomies, analysis, modeling, and countermeasures",
      "paper_id": "1805.01612v2"
    },
    {
      "index": 16,
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.Y."
    },
    {
      "index": 17,
      "title": "Understanding Black-box Predictions via Influence Functions",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70. JMLR. org",
      "authors": "Koh, P.W., Liang, P.",
      "orig_title": "Understanding black-box predictions via influence functions",
      "paper_id": "1703.04730v3"
    },
    {
      "index": 18,
      "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.00741",
      "authors": "Koh, P.W., Steinhardt, J., Liang, P.",
      "orig_title": "Stronger data poisoning attacks break data sanitization defenses",
      "paper_id": "1811.00741v2"
    },
    {
      "index": 19,
      "title": "Poster: Attacking malware classifiers by crafting gradient-attacks that preserve functionality",
      "abstract": "",
      "year": "2019",
      "venue": "2019 ACM SIGSAC Conference on Computer and Communications Security. ACM",
      "authors": "Labaca-Castro, R., Biggio, B., Dreo Rodosek, G."
    },
    {
      "index": 20,
      "title": "Shilling recommender systems for fun and profit",
      "abstract": "",
      "year": "2004",
      "venue": "13th international conference on World Wide Web. ACM",
      "authors": "Lam, S.K., Riedl, J."
    },
    {
      "index": 21,
      "title": "Powers of Tensors and Fast Matrix Multiplication",
      "abstract": "",
      "year": "2014",
      "venue": "39th international symposium on symbolic and algebraic computation. ACM",
      "authors": "Le Gall, F.",
      "orig_title": "Powers of tensors and fast matrix multiplication",
      "paper_id": "1401.7714v1"
    },
    {
      "index": 22,
      "title": "Trust region newton methods for large-scale logistic regression",
      "abstract": "",
      "year": "2007",
      "venue": "24th international conference on Machine learning. ACM",
      "authors": "Lin, C.J., Weng, R.C., Keerthi, S.S."
    },
    {
      "index": 23,
      "title": "Good word attacks on statistical spam filters",
      "abstract": "",
      "year": "2005",
      "venue": "Second Conference on Email and Anti-Spam (CEAS)",
      "authors": "Lowd, D."
    },
    {
      "index": 24,
      "title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection",
      "abstract": "",
      "year": "2013",
      "venue": "8th ACM SIGSAC symposium on Information, computer and communications security. ACM",
      "authors": "Maiorca, D., Corona, I., Giacinto, G."
    },
    {
      "index": 25,
      "title": "On the complexity of linear programming",
      "abstract": "",
      "year": "1986",
      "venue": "IBM Thomas J. Watson Research Division",
      "authors": "Megiddo, N., et al."
    },
    {
      "index": 26,
      "title": "Using machine teaching to identify optimal training-set attacks on machine learners",
      "abstract": "",
      "year": "2015",
      "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "authors": "Mei, S., Zhu, X."
    },
    {
      "index": 27,
      "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "10th ACM Workshop on Artificial Intelligence and Security. ACM",
      "authors": "Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E.C., Roli, F.",
      "orig_title": "Towards poisoning of deep learning algorithms with back-gradient optimization",
      "paper_id": "1708.08689v1"
    },
    {
      "index": 28,
      "title": "Exploiting machine learning to subvert your spam filter",
      "abstract": "",
      "year": "2008",
      "venue": "LEET",
      "authors": "Nelson, B., Barreno, M., Chi, F.J., Joseph, A.D., Rubinstein, B.I., Saini, U., Sutton, C.A., Tygar, J.D., Xia, K."
    },
    {
      "index": 29,
      "title": "Distillation as a defense to adversarial perturbations against deep neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE Symposium on Security and Privacy (SP). IEEE",
      "authors": "Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A."
    },
    {
      "index": 30,
      "title": "Label Sanitization against Label Flipping Poisoning Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer",
      "authors": "Paudice, A., Muñoz-González, L., Lupu, E.C.",
      "orig_title": "Label sanitization against label flipping poisoning attacks",
      "paper_id": "1803.00992v2"
    },
    {
      "index": 31,
      "title": "Microsoft is deleting its ai chatbot’s incredibly racist tweets",
      "abstract": "",
      "year": "2016",
      "venue": "Business Insider",
      "authors": "Price, R."
    },
    {
      "index": 32,
      "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers",
      "abstract": "",
      "year": "2018",
      "venue": "International Symposium on Research in Attacks, Intrusions, and Defenses. Springer",
      "authors": "Rosenberg, I., Shabtai, A., Rokach, L., Elovici, Y.",
      "orig_title": "Generic black-box end-to-end attack against state of the art api call based malware classifiers",
      "paper_id": "1707.05970v5"
    },
    {
      "index": 33,
      "title": "Sitatapatra: Blocking the Transfer of Adversarial Samples",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.08121",
      "authors": "Shumailov, I., Gao, X., Zhao, Y., Mullins, R., Anderson, R., Xu, C.Z.",
      "orig_title": "Sitatapatra: Blocking the transfer of adversarial samples",
      "paper_id": "1901.08121v2"
    },
    {
      "index": 34,
      "title": "Certified defenses for data poisoning attacks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Steinhardt, J., Koh, P.W.W., Liang, P.S."
    },
    {
      "index": 35,
      "title": "When does machine learning {{\\{FAIL\\}}}? generalized transferability for evasion and poisoning attacks",
      "abstract": "",
      "year": "2018",
      "venue": "27th USENIX Security Symposium (USENIX Security 18)",
      "authors": "Suciu, O., Marginean, R., Kaya, Y., Daume III, H., Dumitras, T."
    },
    {
      "index": 36,
      "title": "Data Poisoning Attacks against Online Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.08994",
      "authors": "Wang, Y., Chaudhuri, K.",
      "orig_title": "Data poisoning attacks against online learning",
      "paper_id": "1808.08994v1"
    },
    {
      "index": 37,
      "title": "An Investigation of Data Poisoning Defenses for Online Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.12121",
      "authors": "Wang, Y., Chaudhuri, K.",
      "orig_title": "An investigation of data poisoning defenses for online learning",
      "paper_id": "1905.12121v3"
    },
    {
      "index": 38,
      "title": "Adversarial label flips attack on support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "ECAI",
      "authors": "Xiao, H., Xiao, H., Eckert, C."
    },
    {
      "index": 39,
      "title": "Is Feature Selection Secure against Training Data Poisoning?",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F.",
      "orig_title": "Is feature selection secure against training data poisoning?",
      "paper_id": "1804.07933v1"
    },
    {
      "index": 40,
      "title": "Dual coordinate descent methods for logistic regression and maximum entropy models",
      "abstract": "",
      "year": "2011",
      "venue": "Machine Learning",
      "authors": "Yu, H.F., Huang, F.L., Lin, C.J."
    },
    {
      "index": 41,
      "title": "Online Data Poisoning Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1903.01666",
      "authors": "Zhang, X., Zhu, X.",
      "orig_title": "Online data poisoning attack",
      "paper_id": "1903.01666v2"
    },
    {
      "index": 42,
      "title": "Efficient label contamination attacks against black-box learning models",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Zhao, M., An, B., Gao, W., Zhang, T."
    },
    {
      "index": 43,
      "title": "Generalized resilience and robust statistics",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Zhu, B., Jiao, J., Steinhardt, J."
    },
    {
      "index": 44,
      "title": "Decentralizing privacy: Using blockchain to protect personal data",
      "abstract": "",
      "year": "2015",
      "venue": "2015 IEEE Security and Privacy Workshops. IEEE",
      "authors": "Zyskind, G., Nathan, O., et al."
    }
  ]
}