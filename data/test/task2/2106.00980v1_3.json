{
  "paper_id": "2106.00980v1",
  "title": "End-to-End Hierarchical Relation Extraction for Generic Form Understanding",
  "sections": {
    "iv-d results": "We visualize the final output by images in table II. The first and second row are input and the char-grid of images after pre-processing respectively. The result of model can be visualized in the O‚Äãu‚Äãt‚Äãp‚Äãu‚ÄãtùëÇùë¢ùë°ùëùùë¢ùë°Output row. As we can see, the H‚Äãe‚Äãa‚Äãt‚Äãm‚Äãa‚Äãpùêªùëíùëéùë°ùëöùëéùëùHeatmap row shows the entity locations through bottom-left points, while the output row shows the full results of our model. The header, question, and answer are drawn by yellow, green, and blue segmentation masks respectively. The linking between entities (if have) are visualized through orange lines.\nThese lines start from the bottom-left points of entities labeled as the answer to the bottom-left points of entities called the question. Our model has this ability because from keypoint detection of each entity, PAFs are likely to link these text-regions by using a directed vector. In order to benchmark the performance of our model, we evaluate it with other methods on FUNSD dataset [ref]5. All results are measured by F1-score. Entity Labeling Task\nTable¬†III shows the comparison between MSAU-PAF and the baselines.\nMSAU-PAF model achieves the highest F1-score 0.83, which significantly improves the accuracy compared to MLP [ref]5 the baseline model - MSAU [ref]9 by 0.26 and 0.1 respectively. On top of that, our model outperforms the state of the art LayoutLM [ref]17 by a large margin.\nThe improvement in accuracy hints that the supervision from Entity Linking can be one of the factors improving the accuracy of the model in comparison with segmentation-only supervision of MSAU [ref]9.\nIt is noted that the MLP baseline [ref]5 utilized a much more sophisticated pre-trained model than MSAU and MSAU-PAF (BERT  is proven to be state-of-the-arts in many tasks from the aforementioned paper in comparison with one-hot character encoding).\nThe improvement in MSAU and MSAU-PAF accuracy is potential because of the model making better use of spatial information and char-gird embeddings, as mentioned in [ref]9. Meanwhile, the LayoutLM mainly relies on the image embeddings via utilizing bounding box regions in the prior step, which appears to fail to capture well the spatial and relational information among entities. Entity Linking Task\nWith this task, MSAU-PAF continues outperforming other methods (see in Table¬†IV).\nIt is observable that the F1-score of GraphCNN and MLP baselines are low, this can be due to the effect of highly dense and imbalanced class distribution in conjunction with Deep-learning based matching. The low F1 scores come mainly from low precision (model predicts most of non-link for most of the pairs).\nThe same hypothesis has been made in the original paper [ref]5.\nIn contrast, the PIF-PAF heads could particularly handle well in densely and occluded documents via effectively encoding fine-grained information and confidently predicting confidence association between two entities with composite field structure.\nThis leads to 0.71 and 0.62 improvement in F1-score from MLP and GraphCNN with Link prediction respectively. It is noted that other baselines are assumed to already know the ground-truth of the class of entities, while our model only uses the text-line regions and optical character. Compared to the Heuristic method with Ground-truth entities class, the MSAU-PAF seems to have a little lower F1-score. This can be because of the use of the ground-truth label for each entity, while in the MSAU-PAF experiment, we combine both two tasks and the result is the joint probability of Entity Labeling and Entity Linking. The hypothesis is verified when we applied MSAU-PAF middle output as input to the heuristic rules."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "PDF-TREX: An approach for recognizing and extracting tables from PDF documents",
      "abstract": "",
      "year": "2009",
      "venue": "ICDR",
      "authors": "Ermelinda Oro and Massimo Ruffolo"
    },
    {
      "index": 1,
      "title": "Information Extraction tasks : a survey",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Luisa Simoes, Goncalo; Galhardas, Helena; Coheur"
    },
    {
      "index": 2,
      "title": "Attend, Copy, Parse End-to-end information extraction from documents",
      "abstract": "",
      "year": "2019",
      "venue": "CBDAR",
      "authors": "Rasmus¬†Berg Palm, Florian Laws, and Ole Winther",
      "orig_title": "Attend, Copy, Parse - End-to-end information extraction from documents",
      "paper_id": "1812.07248v3"
    },
    {
      "index": 3,
      "title": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao",
      "orig_title": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
      "paper_id": "1903.11279v1"
    },
    {
      "index": 4,
      "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents",
      "abstract": "",
      "year": "2019",
      "venue": "ICDR",
      "authors": "Guillaume Jaume, Hazim¬†Kemal Ekenel, and Jean-Philippe Thiran"
    },
    {
      "index": 5,
      "title": "Image Generation from Scene Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Justin Johnson, Agrim Gupta, and Li¬†Fei-Fei",
      "orig_title": "Image Generation from Scene Graphs",
      "paper_id": "1804.01622v1"
    },
    {
      "index": 6,
      "title": "Pixels to Graphs by Associative Embedding",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Alejandro Newell and Jia Deng",
      "orig_title": "Pixels to Graphs by Associative Embedding",
      "paper_id": "1706.07365v2"
    },
    {
      "index": 7,
      "title": "Graph R-CNN for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh",
      "orig_title": "Graph R-CNN for Scene Graph Generation",
      "paper_id": "1808.00191v1"
    },
    {
      "index": 8,
      "title": "End-to-End Information Extraction by Character-Level Embedding and Multi-Stage",
      "abstract": "",
      "year": "2019",
      "venue": "BMVC",
      "authors": "Nguyen¬†Dang Tuan Anh and Nguyen¬†Thanh Dat"
    },
    {
      "index": 9,
      "title": "Link Prediction Based on Graph Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Muhan Zhang and Yixin Chen",
      "orig_title": "Link Prediction Based on Graph Neural Networks",
      "paper_id": "1802.09691v3"
    },
    {
      "index": 10,
      "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh"
    },
    {
      "index": 11,
      "title": "Chargrid: Towards Understanding 2D Documents",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Anoop¬†Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes H√∂hne, and Jean¬†Baptiste Faddoul",
      "orig_title": "Chargrid: Towards Understanding 2D Documents",
      "paper_id": "1809.08799v1"
    },
    {
      "index": 12,
      "title": "PifPaf: Composite Fields for Human Pose Estimation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi",
      "orig_title": "PifPaf: Composite Fields for Human Pose Estimation",
      "paper_id": "1903.06593v2"
    },
    {
      "index": 13,
      "title": "CornerNet: Detecting Objects as Paired Keypoints",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Hei Law and Jia Deng",
      "orig_title": "CornerNet: Detecting Objects as Paired Keypoints",
      "paper_id": "1808.01244v2"
    },
    {
      "index": 14,
      "title": "An intriguing failing of convolutional neural networks and the CoordConv solution",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Rosanne Liu, Joel Lehman, Piero Molino, Felipe¬†Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski",
      "orig_title": "An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution",
      "paper_id": "1807.03247v2"
    },
    {
      "index": 15,
      "title": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Rasmus¬†Berg Palm, Ole Winther, and Florian Laws"
    },
    {
      "index": 16,
      "title": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou"
    },
    {
      "index": 17,
      "title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Subhojeet Pramanik, Shashank Mujumdar, and Hima Patel",
      "orig_title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning",
      "paper_id": "2009.14457v2"
    },
    {
      "index": 18,
      "title": "Robust Spatial Filtering with Graph Convolutional Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE JSTSP",
      "authors": "Felipe¬†Petroski Such, Shagan Sah, Miguel¬†Alexander Dominguez, Suhas Pillai, Chao Zhang, Andrew Michael, Nathan¬†D. Cahill, and Raymond Ptucha",
      "orig_title": "Robust Spatial Filtering with Graph Convolutional Neural Networks",
      "paper_id": "1703.00792v3"
    },
    {
      "index": 19,
      "title": "High Performance Layout Analysis of Arabic and Urdu Document Images",
      "abstract": "",
      "year": "2011",
      "venue": "ICDR",
      "authors": "Syed¬†Saqib Bukhari, Faisal Shafait, and Thomas¬†M Breuel"
    },
    {
      "index": 20,
      "title": "Textline detection in degraded historical document images",
      "abstract": "",
      "year": "2017",
      "venue": "Eurasip Journal on Image and Video Processing",
      "authors": "Byeongyong Ahn, Jewoong Ryu, Hyung¬†Il Koo, and Nam¬†Ik Cho"
    },
    {
      "index": 21,
      "title": "A graph-theoretic approach to webpage segmentation",
      "abstract": "",
      "year": "2008",
      "venue": "WWW",
      "authors": "Deepayan Chakrabarti, Ravi Kumar, and Kunal Punera"
    },
    {
      "index": 22,
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Jonathan Long, Evan Shelhamer, and Trevor Darrell",
      "orig_title": "Fully convolutional networks for semantic segmentation",
      "paper_id": "1605.06211v1"
    },
    {
      "index": 23,
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "MICCAI",
      "authors": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox",
      "orig_title": "U-net: Convolutional networks for biomedical image segmentation",
      "paper_id": "1505.04597v1"
    },
    {
      "index": 24,
      "title": "dhSegment: A generic deep-learning approach for document segmentation",
      "abstract": "",
      "year": "2018",
      "venue": "ICFHR",
      "authors": "Sofia¬†Ares Oliveira, Benoit Seguin, and Frederic Kaplan",
      "orig_title": "dhSegment: A generic deep-learning approach for document segmentation",
      "paper_id": "1804.10371v2"
    },
    {
      "index": 25,
      "title": "An Invoice Reading System Using a Graph Convolutional Network",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "D.¬†Lohani, A.¬†Bela√Ød, and Y.¬†Bela√Ød"
    },
    {
      "index": 26,
      "title": "Towards Accurate Multi-person Pose Estimation in the Wild",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy",
      "orig_title": "Towards Accurate Multi-person Pose Estimation in the Wild",
      "paper_id": "1701.01779v2"
    },
    {
      "index": 27,
      "title": "Simple Baselines for Human Pose Estimation and Tracking",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Bin Xiao, Haiping Wu, and Yichen Wei",
      "orig_title": "Simple Baselines for Human Pose Estimation and Tracking",
      "paper_id": "1804.06208v2"
    },
    {
      "index": 28,
      "title": "Non-local Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He",
      "orig_title": "Non-local Neural Networks",
      "paper_id": "1711.07971v3"
    },
    {
      "index": 29,
      "title": "CU-Net: Coupled U-Nets",
      "abstract": "",
      "year": "2019",
      "venue": "BMVC",
      "authors": "Zhiqiang Tang, Xi¬†Peng, Shijie Geng, Yizhe Zhu, and Dimitris¬†N. Metaxas",
      "orig_title": "Cu-Net: Coupled U-nets",
      "paper_id": "1808.06521v1"
    },
    {
      "index": 30,
      "title": "Non-local Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He",
      "orig_title": "Non-local Neural Networks",
      "paper_id": "1711.07971v3"
    },
    {
      "index": 31,
      "title": "Deep Neural Networks using Box Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Egor Burkov"
    },
    {
      "index": 32,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "paper_id": "1810.04805v2"
    }
  ]
}