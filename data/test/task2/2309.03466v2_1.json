{
  "paper_id": "2309.03466v2",
  "title": "Mira: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks",
  "sections": {
    "background": "∙∙\\bullet Model Watermarks.\nDigital watermarking is originally designed for protecting the copyright of digital media  and recently applied to protect the intellectual property of DNN models.\nA white-box watermarking scheme typically embeds a sequence of secret messages into the parameters or neural activations of the target model, and thus requires white-box access of the suspect model to extract the watermark 8   .\nOn the contrary, a black-box watermark is embedded in the model’s prediction behavior on a set of specially-crafted secret samples. Therefore, the verification process of black-box watermarking only requires the access to the prediction API [ref]3   .\nIn this work, we mainly focus on the robustness of black-box watermarks due to their increasing popularity in industry. Existing black-box watermark schemes have diverse designs in the choices of the watermark data and the target label. In terms of watermark data, some work leverages specially-crafted patterns   or random noise  as watermark patterns.\nOther black-box watermarks may use out-of-distribution samples as watermark data [ref]3 .\nBesides, in-distribution clean samples , adversarial samples near the decision boundary  or images generated by an encoder with exclusive logo embedded indistinguishably [ref]36 can also be exploited as watermark data.\nIn terms of the target label setting, existing black-box DNN watermarks can be categorized into fixed-class watermarks, where all the watermark data are paired with the identical target class (such as   ),\nand non-fixed-class watermarks, where each watermark data is paired with its own target class (such as [ref]3   [ref]36 ). To build a trustworthy ownership verification process, an ideal watermarking scheme should satisfy the following three key requirements  .\n(1) Fidelity. The performance degradation on the target model should be as low as possible after the watermark is embedded.\n(2) Integrity. Models which are trained independently without access to the target model should not be verified as containing the watermark.\n(3) Robustness. The embedded watermark in the target model should be resistant to potential removal attacks. In the extreme settings when the watermark is removed, the utility of the model should decrease dramatically. ∙∙\\bullet Watermark Removal Attacks.\nBlack-box model watermarks are to some extent similar to backdoor attacks, as they both establish connections between some specified data and the target labels.\nTherefore, classical methods in backdoor defense, such as pruning, finetuning or trigger reverse-engineering, are usually considered as potential threats for black-box watermarks [ref]3  .\nRecently, some attacks specially targeting black-box watermarks are also proposed. According to Wang et al. , existing watermark removal attacks are mainly categorized into three types as follows. Pruning-based Attacks: They prune the redundant weights 8 or neurons  in DNNs.\nHowever, to invalidate the underlying watermarks, these methods need to prune a large proportion of weights or neurons, which causes an unacceptable utility loss. Finetuning-based Attacks: They continue to train the target model for a few epochs with some carefully designed finetuning techniques, such as learning rate schedule  , dataset augmentation , weight regularization , or continue learning with attention distraction .\nUnfortunately, these attacks are usually effective only against a subset of watermarks, and might hamper the model’s utility due to the heavy training restrictions. Unlearning-based Attacks: They are mainly designed for black-box watermarks whose watermark data have the fixed pattern and the identical target class  .\nSpecifically, they use conventional backdoor trigger reverse-engineering techniques in e.g., NeuralCleanse , which poses rather strong assumptions on the forms of underlying watermarks and are therefore not watermark-agnostic.\nAlso, current unlearning-based attacks all need access to source training data, which further limits their applicable scenarios. Table 2 summarizes the pros-and-cons of attacks from each category. Note that there also exists another type of attacks aiming at training a new surrogate model using the knowledge transferred from the given watermarked model, i.e., model extraction attacks   .\nHowever, extraction attacks typically requires large query dataset and entails heavy computation costs . Therefore, our current work is not compared with extraction attacks. Our work focuses on the vulnerabilities revealed by model inversion.\nNote that this has also been studied by Zhang et al. .\nHowever, they used a rather naive inversion method from  and only presented the visually-meaningless inverted samples.\nNeither have they delved into the neuron coverage between the inverted samples and the real watermark samples, nor attempted to unlearn the inverted ones for watermark removal."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Hugging Face Datasets",
      "abstract": "",
      "year": "2023",
      "venue": "https://huggingface.co/docs/datasets/index",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Protecting intellectual property of deep neural networks with watermarking",
      "abstract": "",
      "year": "2023",
      "venue": "https://research.ibm.com/publications/protecting-intellectual-property-of-deep-neural-networks-with-watermarking",
      "authors": ""
    },
    {
      "index": 2,
      "title": "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring",
      "abstract": "",
      "year": "2018",
      "venue": "USENIX Security Symposium",
      "authors": "Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet",
      "orig_title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
      "paper_id": "1802.04633v3"
    },
    {
      "index": 3,
      "title": "Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "Computers & Security",
      "authors": "William Aiken, Hyoungshick Kim, and Simon S. Woo",
      "orig_title": "Neural network laundering: Removing black-box backdoor watermarks from deep neural networks",
      "paper_id": "2004.11368v1"
    },
    {
      "index": 4,
      "title": "Machine learning in tracking associations with stereo vision and lidar observations for an autonomous vehicle",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Intelligent Vehicles Symposium (IV)",
      "authors": "Marco Allodi, Alberto Broggi, Domenico Giaquinto, Marco Patander, and Antonio Prioletti"
    },
    {
      "index": 5,
      "title": "Autonomous driving architectures: insights of machine learning and deep learning algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning with Applications",
      "authors": "Mrinal R Bachute and Javed M Subhedar"
    },
    {
      "index": 6,
      "title": "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Ben Edwards, Taesung Lee, Ian Molloy, and B. Srivastava",
      "orig_title": "Detecting backdoor attacks on deep neural networks by activation clustering",
      "paper_id": "1811.03728v1"
    },
    {
      "index": 7,
      "title": "REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": "Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, and Dawn Song",
      "orig_title": "Refit: a unified watermark removal framework for deep learning systems with limited data",
      "paper_id": "1911.07205v3"
    },
    {
      "index": 8,
      "title": "Leveraging unlabeled data for watermark removal of deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "ICML Workshop on Security and Privacy of Machine Learning",
      "authors": "Xinyun Chen, Wenxiao Wang, Yiming Ding, Chris Bender, Ruoxi Jia, Bo Li, and Dawn Song"
    },
    {
      "index": 9,
      "title": "You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Xuxi Chen, Tianlong Chen, Zhenyu Zhang, and Zhangyang Wang",
      "orig_title": "You are caught stealing my winning lottery ticket! making a lottery ticket claim its ownership",
      "paper_id": "2111.00162v1"
    },
    {
      "index": 10,
      "title": "Linkbreaker: Breaking the backdoor-trigger link in dnns via neurons consistency check",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": "Zhenzhu Chen, Shang Wang, Anmin Fu, Yansong Gao, Shui Yu, and Robert H. Deng"
    },
    {
      "index": 11,
      "title": "Emnist: Extending mnist to handwritten letters",
      "abstract": "",
      "year": "2017",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "authors": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik"
    },
    {
      "index": 12,
      "title": "Certified Adversarial Robustness via Randomized Smoothing",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Jeremy Cohen, Elan Rosenfeld, and Zico Kolter",
      "orig_title": "Certified adversarial robustness via randomized smoothing",
      "paper_id": "1902.02918v2"
    },
    {
      "index": 13,
      "title": "Deepsigns: An end-to-end watermarking framework for ownership protection of deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "authors": "Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar"
    },
    {
      "index": 14,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 15,
      "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
      "abstract": "",
      "year": "2015",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "Matt Fredrikson, Somesh Jha, and Thomas Ristenpart"
    },
    {
      "index": 16,
      "title": "Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "Computers & Security",
      "authors": "Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, and Ian Molloy",
      "orig_title": "Backdoor smoothing: Demystifying backdoor attacks on deep neural networks",
      "paper_id": "2006.06721v4"
    },
    {
      "index": 17,
      "title": "Watermarking deep neural networks for embedded systems",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD)",
      "authors": "Jia Guo and Miodrag Potkonjak"
    },
    {
      "index": 18,
      "title": "Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal Attack for DNN Models",
      "abstract": "",
      "year": "2020",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang Liu",
      "orig_title": "Fine-tuning is not enough: A simple yet effective watermark removal attack for dnn models",
      "paper_id": "2009.08697v2"
    },
    {
      "index": 19,
      "title": "Convolutional recurrent deep learning model for sentence classification",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "Abdalraouf Hassan and Ausif Mahmood"
    },
    {
      "index": 20,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 21,
      "title": "Densely connected convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger"
    },
    {
      "index": 22,
      "title": "Adversarial examples are not bugs, they are features",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry"
    },
    {
      "index": 23,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Sergey Ioffe and Christian Szegedy"
    },
    {
      "index": 24,
      "title": "Entangled watermarks as a defense against model extraction",
      "abstract": "",
      "year": "2021",
      "venue": "USENIX Security Symposium",
      "authors": "Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, and Nicolas Papernot"
    },
    {
      "index": 25,
      "title": "Watermarking techniques for intellectual property protection",
      "abstract": "",
      "year": "1998",
      "venue": "Design Automation Conference",
      "authors": "Andrew B Kahng, John Lach, William H Mangione-Smith, Stefanus Mantik, Igor L Markov, Miodrag Potkonjak, Paul Tucker, Huijuan Wang, and Gregory Wolfe"
    },
    {
      "index": 26,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "National Academy of Sciences",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 27,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Alex Krizhevsky, Geoffrey Hinton, et al."
    },
    {
      "index": 28,
      "title": "Adversarial frontier stitching for remote neural network watermarking",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "authors": "Erwan Le Merrer, Patrick Perez, and Gilles Trédan"
    },
    {
      "index": 29,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "Proceedings of the IEEE",
      "authors": "Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner"
    },
    {
      "index": 30,
      "title": "The mnist database of handwritten digits",
      "abstract": "",
      "year": "1998",
      "venue": "http://yann.lecun.com/exdb/mnist/",
      "authors": "Yann LeCun, Corinna Cortes, and Christopher J.C. Burges"
    },
    {
      "index": 31,
      "title": "Identifying appropriate intellectual property protection mechanisms for machine learning models: A systematization of watermarking, fingerprinting, model access, and attacks",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.11285",
      "authors": "Isabell Lederer, Rudolf Mayer, and Andreas Rauber"
    },
    {
      "index": 32,
      "title": "Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": "Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, and Sooel Son",
      "orig_title": "Evaluating the robustness of trigger set-based watermarks embedded in deep neural networks",
      "paper_id": "2106.10147v2"
    },
    {
      "index": 33,
      "title": "Piracy Resistant Watermarks for Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01226",
      "authors": "Huiying Li, Emily Wenger, Shawn Shan, Ben Y Zhao, and Haitao Zheng",
      "orig_title": "Piracy resistant watermarks for deep neural networks",
      "paper_id": "1910.01226v3"
    },
    {
      "index": 34,
      "title": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma",
      "orig_title": "Anti-backdoor learning: Training clean models on poisoned data",
      "paper_id": "2110.11571v3"
    },
    {
      "index": 35,
      "title": "How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN",
      "abstract": "",
      "year": "2019",
      "venue": "Annual Computer Security Applications Conference",
      "authors": "Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing Guo",
      "orig_title": "How to prove your model belongs to you: A blind-watermark based framework to protect intellectual property of dnn",
      "paper_id": "1903.01743v4"
    },
    {
      "index": 36,
      "title": "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Symposium on Research in Attacks, Intrusions, and Defenses (RAID)",
      "authors": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg",
      "orig_title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
      "paper_id": "1805.12185v1"
    },
    {
      "index": 37,
      "title": "Removing Backdoor-Based Watermarks in Neural Networks with Limited Data",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Pattern Recognition (ICPR)",
      "authors": "Xuankai Liu, Fengting Li, Bihan Wen, and Qi Li",
      "orig_title": "Removing backdoor-based watermarks in neural networks with limited data",
      "paper_id": "2008.00407v2"
    },
    {
      "index": 38,
      "title": "Sok: How robust is image classification deep neural network watermarking?",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "Nils Lukas, Edward Jiang, Xinda Li, and Florian Kerschbaum"
    },
    {
      "index": 39,
      "title": "Inceptionism: Going deeper into neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Alexander Mordvintsev, Christopher Olah, and Mike Tyka"
    },
    {
      "index": 40,
      "title": "Robust Watermarking of Neural Network with Exponential Weighting",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": "Ryota Namba and Jun Sakuma",
      "orig_title": "Robust watermarking of neural network with exponential weighting",
      "paper_id": "1901.06151v1"
    },
    {
      "index": 41,
      "title": "Deep face recognition",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman"
    },
    {
      "index": 42,
      "title": "On the robustness of backdoor-based watermarking in deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Workshop on Information Hiding and Multimedia Security",
      "authors": "Masoumeh Shafieinejad, Jiaqi Wang, Nils Lukas, and Florian Kerschbaum"
    },
    {
      "index": 43,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 44,
      "title": "Single Image Backdoor Inversion via Robust Smoothed Classifiers",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Mingjie Sun and Zico Kolter",
      "orig_title": "Single image backdoor inversion via robust smoothed classifiers",
      "paper_id": "2303.00215v2"
    },
    {
      "index": 45,
      "title": "Detect and remove watermark in deep neural networks via generative adversarial networks",
      "abstract": "",
      "year": "2021",
      "venue": "Information Security - International Conference ISC",
      "authors": "Shichang Sun, Haoqi Wang, Mingfu Xue, Yushu Zhang, Jian Wang, and Weiqiang Liu",
      "orig_title": "Detect and remove watermark in deep neural networks via generative adversarial networks",
      "paper_id": "2106.08104v1"
    },
    {
      "index": 46,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna"
    },
    {
      "index": 47,
      "title": "Embedding watermarks into deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "ACM International Conference on Multimedia Retrieval",
      "authors": "Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin’ichi Satoh"
    },
    {
      "index": 48,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research",
      "authors": "Laurens Van der Maaten and Geoffrey Hinton",
      "orig_title": "Visualizing data using t-sne",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 49,
      "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao"
    },
    {
      "index": 50,
      "title": "RIGA: Covert and Robust White-Box Watermarking of Deep Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "Web Conference",
      "authors": "Tianhao Wang and Florian Kerschbaum",
      "orig_title": "Riga: Covert and robust white-box watermarking of deep neural networks",
      "paper_id": "1910.14268v4"
    },
    {
      "index": 51,
      "title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation",
      "abstract": "",
      "year": "2023",
      "venue": "USENIX Security Symposium",
      "authors": "Yifan Yan, Xudong Pan, Mi Zhang, and Min Yang",
      "orig_title": "Rethinking white-box watermarks on deep learning models under neural structural obfuscation",
      "paper_id": "2303.09732v1"
    },
    {
      "index": 52,
      "title": "Effectiveness of Distillation Attack and Countermeasure on Neural Network Watermarking",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Ziqi Yang, Hung Dang, and Ee-Chien Chang",
      "orig_title": "Effectiveness of distillation attack and countermeasure on neural network watermarking",
      "paper_id": "1906.06046v1"
    },
    {
      "index": 53,
      "title": "Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz",
      "orig_title": "Dreaming to distill: Data-free knowledge transfer via deepinversion",
      "paper_id": "1912.08795v2"
    },
    {
      "index": 54,
      "title": "Few-shot Unlearning by Model Inversion",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.15567",
      "authors": "Youngsik Yoon, Jinhwan Nam, Hyojeong Yun, Dongwoo Kim, and Jungseul Ok",
      "orig_title": "Few-shot unlearning by model inversion",
      "paper_id": "2205.15567v2"
    },
    {
      "index": 55,
      "title": "Protecting intellectual property of deep neural networks with watermarking",
      "abstract": "",
      "year": "2018",
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": "Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian Molloy"
    },
    {
      "index": 56,
      "title": "Medical image classification using synergic deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "Medical Image Analysis",
      "authors": "Jianpeng Zhang, Yutong Xie, Qi Wu, and Yong Xia"
    },
    {
      "index": 57,
      "title": "Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)",
      "authors": "Qi Zhong, Leo Yu Zhang, Shengshan Hu, Longxiang Gao, Jun Zhang, and Yang Xiang",
      "orig_title": "Attention distraction: Watermark removal through continual learning with selective forgetting",
      "paper_id": "2204.01934v1"
    }
  ]
}