{
  "paper_id": "2204.04812v2",
  "title": "OutfitTransformer: Learning Outfit Representations for Fashion Recommendation",
  "sections": {
    "experiments": "We compare our proposed approach with the state-of-the-art baselines such as Bi-LSTM , GCN [ref]6, SiameseNet [ref]27, Type-aware [ref]25 , SCE-Net  and CSA-Net  on the Polyvore Outfits dataset [ref]25. For evaluation, we compare our method with these state-of-the-art baselines on three different tasks: Outfit Compatibility Prediction (CP) task that predicts the compatibility of items in an outfit. Fill in the Blank (FITB) task that selects the most compatible item for an incomplete outfit given a set of candidate choices (e.g., 4 candidates). Outfit Complementary Item Retrieval (CIR) task that retrieves complementary items from the database for a target category given an incomplete outfit. The Polyvore Outfits dataset [ref]25 has two sets, the disjoint and non-disjoint sets. In the disjoint set, the training split items (and outfits) do not overlap with the validation and test splits. In the non-disjoint set, the training split items can overlap with those of validation and test splits, but outfits do not overlap. The non-disjoint set contains 53306 training and 10000 test outfits, while the disjoint set comprises of 16995 training and 15154 test outfits. For the standard compatibility prediction and FITB tasks, we evaluate our model on the Polyvore Outfits dataset. Since the Polyvore Outfits dataset does not provide the annotations for the complementary item retrieval task, we adopt a modified version of the Polyvore Outfits dataset proposed by CSA-Net ."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "github.com/nmslib/hnswlib",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "github.com/facebookresearch/faiss",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 2,
      "title": "End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko"
    },
    {
      "index": 3,
      "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Chun-Fu Chen, Quanfu Fan, and Rameswar Panda",
      "orig_title": "Crossvit: Cross-attention multi-scale vision transformer for image classification",
      "paper_id": "2103.14899v2"
    },
    {
      "index": 4,
      "title": "Pog: Personalized outfit generation for fashion recommendation at alibaba ifashion",
      "abstract": "",
      "year": "2019",
      "venue": "SIGKDD",
      "authors": "Wen Feng Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler, Huan Zhao, and Binqiang Zhao"
    },
    {
      "index": 5,
      "title": "Context-Aware Visual Compatibility Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Guillem Cucurull, Perouz Taslakian, and David Vazquez",
      "orig_title": "Context-aware visual compatibility prediction",
      "paper_id": "1902.03646v2"
    },
    {
      "index": 6,
      "title": "Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "The World Wide Web Conference",
      "authors": "Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang",
      "orig_title": "Dressing as a whole: Outfit compatibility learning based on node-wise graph neural networks",
      "paper_id": "1902.08009v1"
    },
    {
      "index": 7,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 8,
      "title": "Learning fashion compatibility with bidirectional lstms",
      "abstract": "",
      "year": "2017",
      "venue": "ACM MM",
      "authors": "Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis"
    },
    {
      "index": 9,
      "title": "Learning attribute-driven disentangled representations for interactive fashion retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "The International Conference on Computer Vision (ICCV)",
      "authors": "Yuxin Hou, Eleonora Vig, Michael Donoser, and Loris Bazzani"
    },
    {
      "index": 10,
      "title": "Learning the Latent “Look”: Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Wei-Lin Hsiao and Kristen Grauman",
      "orig_title": "Learning the latent “look”: Unsupervised discovery of a style-coherent embedding from fashion images",
      "paper_id": "1707.03376v2"
    },
    {
      "index": 11,
      "title": "Creating Capsule Wardrobes from Fashion Images",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Wei-Lin Hsiao and Kristen Grauman",
      "orig_title": "Creating capsule wardrobes from fashion images",
      "paper_id": "1712.02662v2"
    },
    {
      "index": 12,
      "title": "Event Transformer",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh",
      "orig_title": "Set transformer",
      "paper_id": "2204.05172v2"
    },
    {
      "index": 13,
      "title": "Coherent and controllable outfit generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Kedan Li, Chen Liu, and David Forsyth"
    },
    {
      "index": 14,
      "title": "Focal Loss for Dense Object Detection",
      "abstract": "",
      "year": "2020",
      "venue": "TPAMI",
      "authors": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár",
      "orig_title": "Focal loss for dense object detection",
      "paper_id": "1708.02002v2"
    },
    {
      "index": 15,
      "title": "Fashion Outfit Complementary Item Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Yen-Liang Lin, S. Tran, and Larry Davis",
      "orig_title": "Fashion outfit complementary item retrieval",
      "paper_id": "1912.08967v2"
    },
    {
      "index": 16,
      "title": "Scalable and explainable outfit generation",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR Workshop",
      "authors": "Alexander Lorbert, David Neiman, Arik Poznanski, Eduard Oks, and Larry Davis"
    },
    {
      "index": 17,
      "title": "Stand-Alone Self-Attention in Vision Models",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens",
      "orig_title": "Stand-alone self-attention in vision models",
      "paper_id": "1906.05909v1"
    },
    {
      "index": 18,
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Nils Reimers and Iryna Gurevych",
      "orig_title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "paper_id": "1908.10084v1"
    },
    {
      "index": 19,
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Florian Schroff, Dmitry Kalenichenko, and James Philbin",
      "orig_title": "Facenet: A unified embedding for face recognition and clustering",
      "paper_id": "1503.03832v3"
    },
    {
      "index": 20,
      "title": "Deep Metric Learning via Lifted Structured Feature Embedding",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese",
      "orig_title": "Deep metric learning via lifted structured feature embedding",
      "paper_id": "1511.06452v1"
    },
    {
      "index": 21,
      "title": "Learning Similarity Conditions Without Explicit Supervision",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Reuben Tan, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer",
      "orig_title": "Learning similarity conditions without explicit supervision",
      "paper_id": "1908.08589v1"
    },
    {
      "index": 22,
      "title": "Personalized compatibility metric learning",
      "abstract": "",
      "year": "2021",
      "venue": "KDD Workshop",
      "authors": "Meet Taraviya, Anurag Beniwal, Yen-Liang Lin, , and Larry Davis"
    },
    {
      "index": 23,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou",
      "orig_title": "Training data-efficient image transformers; distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 24,
      "title": "Learning type-aware embeddings for fashion compatibility",
      "abstract": "",
      "year": "2018",
      "venue": "ICCV",
      "authors": "Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and David Forsyth"
    },
    {
      "index": 25,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 26,
      "title": "Conditional Similarity Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Andreas Veit, Serge Belongie, and Theofanis Karaletsos",
      "orig_title": "Conditional similarity networks",
      "paper_id": "1603.07810v3"
    },
    {
      "index": 27,
      "title": "Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie",
      "orig_title": "Learning visual clothing style with heterogeneous dyadic co-occurrences",
      "paper_id": "1509.07473v1"
    },
    {
      "index": 28,
      "title": "Sampling Matters in Deep Embedding Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krahenbuhl",
      "orig_title": "Sampling matters in deep embedding learning",
      "paper_id": "1706.07567v2"
    },
    {
      "index": 29,
      "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo",
      "orig_title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
      "paper_id": "2105.15203v3"
    },
    {
      "index": 30,
      "title": "Hard negative examples are hard, but useful",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless",
      "orig_title": "Hard negative examples are hard, but useful",
      "paper_id": "2007.12749v2"
    },
    {
      "index": 31,
      "title": "Learning Texture Transformer Network for Image Super-Resolution",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo",
      "orig_title": "Learning texture transformer network for image super-resolution",
      "paper_id": "2006.04139v2"
    },
    {
      "index": 32,
      "title": "Focal self-attention for local-global interactions in vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao"
    },
    {
      "index": 33,
      "title": "TransNFCM: Translation-Based Neural Fashion Compatibility Modeling",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "X. Yang, Yunshan Ma, Lizi Liao, M. Wang, and Tat-Seng Chua",
      "orig_title": "Transnfcm: Translation-based neural fashion compatibility modeling",
      "paper_id": "1812.10021v1"
    }
  ]
}