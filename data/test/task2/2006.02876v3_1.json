{
  "paper_id": "2006.02876v3",
  "title": "Enhanced back-translation for low resource neural machine translation using self-trainingSupported by NITDEF PhD Scholarship Scheme 2018.",
  "sections": {
    "introduction": "The neural machine translation (NMT)    is currently the simplest and yet the state-of-the-art approach for training improved translation systems [ref]4 . They out-perform other statistical machine translation approaches if there exists a large amount of parallel data between the languages  . Given the “right” amount of qualitative parallel data only, the models can learn the probability of mapping sentences in the source language to their equivalents in another language – the target language . This “right” amount of qualitative parallel data is usually very large and, therefore, expensive to compile because it requires manual translation. The absence of large amounts of high-quality parallel data in many languages has led to various proposals for leveraging the abundant monolingual data that exists in either or both of the languages. These approaches include the self-training , forward translation 0, back-translation [ref]4 1 2 3, dual learning 5 and transfer learning  6 7 8. The back-translation has been used in current state-of-the-art neural machine translation systems [ref]4 9 0, outperforming other approaches in high resource languages and improving performance in low resource conditions [ref]4 1 2. The approach involves training a target-to-source (backward) model on the available parallel data and using that model to generate synthetic translations of a large number of monolingual sentences in the target language. The available authentic parallel data is then mixed with the generated synthetic parallel data without differentiating between the two 1 to train a final source-to-target (forward) model. The quality of the forward translation model depends on the NMT architecture used in building the models 1, the quality of the backward model 1 3 4, the suitability of the synthetic data generation method used [ref]4 5 and the ratio of the authentic data to the synthetic data 6 7. In low resource NMT, the authentic parallel data available is not sufficient to train a backward model that will generate qualitative synthetic data. Thus, various methods have been proposed to improve the quality of the backward model despite the lack of sufficient parallel data. Hoang et al. 1 and Zhang et al. 3 used an iterative approach to enable the forward model to generate synthetic data that will be used to improve the backward model. Imamura et al. 2 suggest generating multiple synthetic sources through sampling given a target sentence. Niu et al 2 trained a bilingual model for both the backward and forward translations and they reported improvement in low resource translations. Graca et al. 5 proposed that selecting the most suitable synthetic data generation method will help reduce the inadequacies of the backward model. Dabre et al. 7 and Kocmi and Bojar 8 proposed the use of a high-resource parent language pair through transfer learning to improve the backward model. This work proposes the use of self-training – also referred within the document as self-learning and forward translation – 8 0  approach to improve the backward model. The output of the backward model – which is ideally used with the authentic data to train the forward model in back-translation – is used to improve the backward model itself. The self-training approach used is similar to that in 8 0  where a synthetic target-side data is used to improve the performance of the translation model instead of the synthetic source-side data in back-translation. But instead of using the approach to enhance the final model, we aim to enhance the backward model which then generates improved synthetic data for enhancing the final model. We also simplify the approach by removing the need for synthetic data quality estimation  or freezing of training parameters 0. The work is similar to the iterative back-translation of Hoang et al. 1 and Zhang et al. 3. The iterative back-translation requires the use of the monolingual source and target data to improve the backward and forward models respectively. The backward model generates synthetic sources to improve the forward model while the forward does the same for the backward model. This process is repeated iteratively until the required quality of translations are obtained. Instead, this work relies only on the monolingual target data to improve both models. Whereas the approaches above perform iterative back-translation to improve both models, our work uses forward translation (self-learning) to improve the backward model and back-translation to improve the forward model. It was shown by Specia and Shah  and Zhang and Zong 0 that using the monolingual source – or the synthetic target – data will potentially reduce the performance of the decoder. To mitigate this, Ueffing 8 and Specia and Shah  used quality estimation 9 to determine the best-translated sentences to be used to retraining, while Zhang and Zong 0 proposed freezing the parameters of the decoder when training the model on the synthetic data. In this work, we showed that the self-learning approach is capable of improving a translation model even without synthetic data cleaning or freezing any learned parameters. We hypothesize that the amount of parallel data used in retraining the model is sufficient to improve the quality of the model if the model can differentiate between and learn effectively from the synthetic and natural data."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Bahdanau D, Cho K, Bengio Y.",
      "orig_title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 1,
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "Luong MT, Pham H, Manning CD.",
      "orig_title": "Effective Approaches to Attention-based Neural Machine Translation",
      "paper_id": "1508.04025v5"
    },
    {
      "index": 2,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "31st Conference on Neural Information Processing Systems",
      "authors": "Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.",
      "orig_title": "Attention Is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 3,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Edunov S, Ott M, Auli M, Grangier D.",
      "orig_title": "Understanding Back-Translation at Scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 4,
      "title": "Scaling Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Ott M, Edunov S, Grangier D, Auli M.",
      "orig_title": "Scaling Neural Machine Translation",
      "paper_id": "1806.00187v3"
    },
    {
      "index": 5,
      "title": "Statistical Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Koehn P."
    },
    {
      "index": 6,
      "title": "Transfer Learning for Low-Resource Neural Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Zoph B, Yuret D, May J, Knight K.",
      "orig_title": "Transfer Learning for Low-Resource Neural Machine Translation",
      "paper_id": "1604.02201v1"
    },
    {
      "index": 7,
      "title": "Effectively training neural machine translation models with monolingual data",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "Yang Z, Chen W, Wang F, Xu B."
    },
    {
      "index": 8,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2018",
      "venue": "Machine Translation Quality Estimation: Applications and Future Perspectives",
      "authors": "Specia L, Shah K.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 9,
      "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Zhang J, Zong C."
    },
    {
      "index": 10,
      "title": "Improving Neural Machine Translation Models with Monolingual Data",
      "abstract": "",
      "year": "2016",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Sennrich R, Haddow B, Birch A"
    },
    {
      "index": 11,
      "title": "Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "2nd Workshop on Neural Machine Translation and Generation",
      "authors": "Imamura K, Fujita A, Sumita E."
    },
    {
      "index": 12,
      "title": "Tagged Back-Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Caswell I, Chelba C, Grangier D."
    },
    {
      "index": 13,
      "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
      "abstract": "",
      "year": "2002",
      "venue": "40th Annual Meeting on Association for Computational Linguistics",
      "authors": "Papineni K, Roukos S, Ward T, Zhu WJ."
    },
    {
      "index": 14,
      "title": "Dual Learning for Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "30th International Conference on Neural Information Processing Systems",
      "authors": "He D, Xia Y, Qin T, Wang L, Yu N, Liu TY, et al.",
      "orig_title": "Dual Learning for Machine Translation",
      "paper_id": "1611.00179v1"
    },
    {
      "index": 15,
      "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Eighth International Joint Conference on Natural Language Processing",
      "authors": "Nguyen TQ, Chiang D."
    },
    {
      "index": 16,
      "title": "NICT’s Supervised Neural Machine Translation Systems for the WMT19 News Translation Task",
      "abstract": "",
      "year": "2019",
      "venue": "Fourth Conference on Machine Translation (WMT)",
      "authors": "Dabre R, Chen K, Marie B, Wang R, Fujita A, Utiyama M, et al."
    },
    {
      "index": 17,
      "title": "CUNI Submission for Low-Resource Languages in WMT News 2019",
      "abstract": "",
      "year": "2019",
      "venue": "Fourth Conference on Machine Translation (WMT)",
      "authors": "Kocmi T, Bojar O."
    },
    {
      "index": 18,
      "title": "Cross-lingual Language Model Pretraining",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Lample G, Conneau A.",
      "orig_title": "Cross-lingual Language Model Pretraining",
      "paper_id": "1901.07291v1"
    },
    {
      "index": 19,
      "title": "Time-aware Large Kernel Convolutions",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Lioutas V, Guo Y.",
      "orig_title": "Time-aware Large Kernel Convolutions",
      "paper_id": "2002.03184v2"
    },
    {
      "index": 20,
      "title": "Iterative Back-Translation for Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "2nd Workshop on Neural Machine Translation and Generation",
      "authors": "Hoang VCD, Koehn P, Haffari G, Cohn T."
    },
    {
      "index": 21,
      "title": "Bi-Directional Neural Machine Translation with Synthetic Parallel Data",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Niu X, Denkowski M, Carpuat M.",
      "orig_title": "Bi-Directional Neural Machine Translation with Synthetic Parallel Data",
      "paper_id": "1805.11213v2"
    },
    {
      "index": 22,
      "title": "Joint Training for Neural Machine Translation Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Zhang Z, Liu S, Li M, Zhou M, Chen E."
    },
    {
      "index": 23,
      "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Burlot F, Yvon F.",
      "orig_title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
      "paper_id": "1903.11437v1"
    },
    {
      "index": 24,
      "title": "Generalizing Back-Translation in Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Graca M, Kim Y, Schamper J, Khadivi S, Ney H."
    },
    {
      "index": 25,
      "title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Fadaee M, Monz C.",
      "orig_title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
      "paper_id": "1808.09006v2"
    },
    {
      "index": 26,
      "title": "Investigating Backtranslation in Neural Machine Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Poncelas A, Shterionov D, Way A, Maillette de Buy GW, Passban P.",
      "orig_title": "Investigating Backtranslation in Neural Machine Translation",
      "paper_id": "1804.06189v1"
    },
    {
      "index": 27,
      "title": "Using Monolingual Source-Language Data to Improve MT Performance",
      "abstract": "",
      "year": "2006",
      "venue": "International Workshop on Spoken Language Translation",
      "authors": "Ueffing N."
    },
    {
      "index": 28,
      "title": "QuEst - A translation quality estimation framework",
      "abstract": "",
      "year": "2013",
      "venue": "51st ACL: System Demonstrations",
      "authors": "Specia L, Shah K, de Souza JGC, Cohn T."
    },
    {
      "index": 29,
      "title": "Convolutional Sequence to Sequence Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Gehring J, Michael A, Grangier D, Yarats D, Dauphin YN."
    },
    {
      "index": 30,
      "title": "Pay less attention with Lightweight and Dynamic Convolutions",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Wu F, Fan A, Baevski A, Dauphin YN, Auli M.",
      "orig_title": "Pay Less Attention with Lightweight and Dynamic Convolutions",
      "paper_id": "1901.10430v2"
    },
    {
      "index": 31,
      "title": "Universal Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser Ł.",
      "orig_title": "Universal Transformers",
      "paper_id": "1807.03819v3"
    },
    {
      "index": 32,
      "title": "Sequence to Sequence Learning with Neural Networks",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS",
      "authors": "Sutskever I, Vinyals O, Le QV."
    },
    {
      "index": 33,
      "title": "Long Short-Term Memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "Hochreiter S, Schmidhuber J."
    },
    {
      "index": 34,
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "Cho K, van Merrienboer B, Gülçehre Ç, Bougares F, Schwenk H, Bengio Y."
    },
    {
      "index": 35,
      "title": "On integrating a language model into neural machine translation",
      "abstract": "",
      "year": "2017",
      "venue": "Computer Speech & Language",
      "authors": "Gulcehre C, Firat O, Xu K, Cho K, Bengio Y."
    },
    {
      "index": 36,
      "title": "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Second Conference on Machine Translation",
      "authors": "Currey A, Miceli Barone AV, Heafield K."
    },
    {
      "index": 37,
      "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Sennrich R, Haddow B, Birch A."
    },
    {
      "index": 38,
      "title": "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
      "abstract": "",
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Johnson M, Schuster M, Le QV, Krikun M, Wu Y, Chen Z, et al.",
      "orig_title": "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
      "paper_id": "1611.04558v2"
    },
    {
      "index": 39,
      "title": "Tag-less Back-Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Abdulmumin I, Galadanci BS, Garba A."
    },
    {
      "index": 40,
      "title": "Report on the 11th IWSLT Evaluation Campaign, IWSLT 2014",
      "abstract": "",
      "year": "2014",
      "venue": "11th Workshop on Spoken Language Translation",
      "authors": "Cettolo M, Niehues J, Stüker S, Bentivogli L, Federico M."
    },
    {
      "index": 41,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Ranzato M, Chopra S, Auli M, Zaremba W.",
      "orig_title": "Sequence Level Training with Recurrent Neural Networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 42,
      "title": "WIT3: Web Inventory of Transcribed and Translated Talks",
      "abstract": "",
      "year": "2012",
      "venue": "Conference of European Association for Machine Translation",
      "authors": "Cettolo M, Christian G, Federico M."
    },
    {
      "index": 43,
      "title": "Findings of the 2017 Conference on Machine Translation (WMT17)",
      "abstract": "",
      "year": "2017",
      "venue": "Second Conference on Machine Translation, Volume 2: Shared Task Papers",
      "authors": "Bojar O, Chatterjee R, Federmann C, Graham Y, Haddow B, Huang S, et al."
    },
    {
      "index": 44,
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Sennrich R, Haddow B, Birch A."
    },
    {
      "index": 45,
      "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv e-prints",
      "authors": "Klein G, Kim Y, Deng Y, Senellart J, Rush AM.",
      "orig_title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
      "paper_id": "1709.03815v1"
    },
    {
      "index": 46,
      "title": "TensorFlow: A system for large-scale machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "12th USENIX Conference on Operating Systems Design and Implementation",
      "authors": "Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al.",
      "orig_title": "TensorFlow: A System for Large-scale Machine Learning",
      "paper_id": "1605.08695v2"
    },
    {
      "index": 47,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Kingma DP, Ba J."
    }
  ]
}