{
  "paper_id": "2412.04747v1",
  "title": "Code Generation and Runtime Techniques for Enabling Data-Efficient Deep Learning Training on GPUs",
  "sections": {
    "introduction": "GPU memory capacity has become a bottleneck for the continued growth of LLMs.\nAs FigureÂ 5.1 shows, the increase of GPU memory capacity is around 60% slower than the LLM size scaling speed and the GPU FP16 throughput improvement. About 80% of the GPU memory used to train recent LLMs consists of activationsÂ [ref]143 , the intermediate tensors produced by forward propagation and reused in backward propagation.\nFurthermore, the memory needed for activations is growing more rapidly than any other memory use, making GPU memory a more severe constraint for future LLM training (see SectionÂ 5.2.1 for details). Common mitigations are to reduce batch size or through gradient accumulation. With gradient accumulation, a batch is divided into micro-batches that are processed separately between gradient updates. Although gradient accumulation has been adopted by many LLMsÂ   , the GPU computation stack is not designed for small inputs, and both mitigations lead to device under-utilizationÂ [ref]147 [ref]148 and suboptimal math library performanceÂ .\nIntuitively, a smaller batch size might reduce total training computation through faster convergence. However, LLM trainers have identified a critical batch size for each model, below which convergence speed increases negligibly or even decreasesÂ  . Notably, critical batch size grows during training as training loss is reduced. Another common approach to reducing GPU memory use is activation checkpointing. With this strategy, only some activations are kept in GPU memory, while others are flushed and then recomputed during backward propagation. For a model with Lğ¿LÂ layers, activation checkpointing can reduce memory requirements from Oâ€‹(L)ğ‘‚ğ¿O(L) to Oâ€‹(L)ğ‘‚ğ¿O(\\sqrt{L})Â . However, as we show in SectionÂ 5.2.1, even this reduction is insufficient to eliminate the bottleneck posed by the GPU memory limits for future LLMs. This chapter proposes SSDTrain, a software framework that offloads activations to NVMe SSDs and reloads activations just before they are needed in backward propagation. SSDTrain can fully overlap activation transfers with computation, reducing activation memory usage without incurring significant performance overhead. SSDs are a more attractive target than mainÂ (CPU) memory for several reasons. First, as illustrated in FigureÂ 5.2, clusters and cloud instancesÂ    typically have limited host memory capacity (100â€“250 GB per GPU), while SSDs offer much greater capacity. Host memory capacity is further consumed by input data, checkpointing buffers, and other training management buffers, leaving even less capacity for activation offloading. In contrast, as modeled in SectionÂ 5.3.6, the activation size per GPU per training step in large LLM models can reach hundreds of GBs or even TBs, exceeding the capacity of host memory. Additionally, as SectionÂ 5.2.1 will detail, SSD capacity is increasing faster than the main memory, making SSD the more viable choice in the future. Second, host memory bandwidth is shared across training management tasks and offloaded computationÂ    running on the host CPUÂ (Please see further elaboration on Swapping and offloading in SectionÂ 5.5). This shared usage can make host memory bandwidth both limited and unpredictableÂ  for saving and restoring activations. In contrast, the SSD bandwidth can be dedicated to activation offloading during training.\nThird, SSDs are more elastic, both by adding more SSDs and even PCIe switches if necessaryâ€”as well as through the use of optional remote high-throughput storageÂ  . Such elasticity allows the data centers to keep up with the fast-growing size of activations. In contrast, the memory capacity of GPU cloud instances and cluster nodes is much more challenging to extend. SSDTrain makes the following main contributions: To address the GPU memory capacity issue and the resulting GPU under-utilization during LLM model training, we design and implement the SSDTrain framework to offload activations in LLM training to NVMe SSDs. We demonstrate the viability of SSDTrain on large-scale systems by modeling the performance, estimated SSD lifespan, and the required per-GPU PCIe bandwidth. With all code in Python except for a tiny CUDA memory allocation API hooking library, SSDTrain works with the latest PyTorch and distributed frameworks, including MegatronÂ  and DeepSpeedÂ . We developed and tested SSDTrain with Megatron-DeepSpeedÂ  on a two-GPU node with seven Intel Optane SSDs. Because SSDTrain overlaps the data transfer entirely with computation, it incurs almost no performance overhead. To achieve this, we introduce several optimization techniques, including tensor deduplication, tensor forwarding, and adaptive offloading algorithm. Evaluation shows SSDTrain achieves almost the same training time per step as the original system without SSDTrain while reducing the activations peak memory use by up to 47%. We introduce the recompute-offload-keep (ROK) curve to compare the SSDTrain offloading with two other tensor placement strategies, keeping activations in memory and layerwise full recomputation. SSDTrain has the same performance as keeping activations in memory and a lower memory peak than activation checkpointing. We further analyze how the reduced activation memory use may be leveraged to increase throughput by increasing micro-batch size and reducing pipeline parallelism bubbles."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGraph convolutional neural networks for web-scale recommender systems",
      "paper_id": "1806.01973v1"
    },
    {
      "index": 1,
      "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
      "abstract": "",
      "year": "2018",
      "venue": "ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec",
      "orig_title": "Graph convolutional neural networks for web-scale recommender systems",
      "paper_id": "1806.01973v1"
    },
    {
      "index": 2,
      "title": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich, I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira, X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy",
      "orig_title": "Deep learning recommendation model for personalization and recommendation systems",
      "paper_id": "1906.00091v1"
    },
    {
      "index": 3,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 4,
      "title": "â€œTrading off compute in training and inference",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 5,
      "title": "â€œDeep learning hardware: Past",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 6,
      "title": "â€œTPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 7,
      "title": "â€œLLM-Analysis: Latency and memory analysis of transformer models for training and inference",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 8,
      "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLLM inference unveiled: Survey and roofline model insights",
      "paper_id": "2402.16363v6"
    },
    {
      "index": 9,
      "title": "compute and data trends in machine learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 10,
      "title": "â€œGPU specs database",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 11,
      "title": "â€œLots of questions on Googleâ€™s â€˜Trilliumâ€™ TPU v6",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 12,
      "title": "â€œNVIDIA Blackwell architecture and B200/B100 accelerators announced: Going bigger with smaller data",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 13,
      "title": "â€œTensor processing unit",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 14,
      "title": "R. Narayanaswami",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 15,
      "title": "â€œLarge graph convolutional network training with GPU-oriented data communication architecture",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 16,
      "title": "EMOGI: Efficient Memory-access for Out-of-memory Graph-traversal in GPUs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEMOGI: Efficient memory-access for out-of-memory graph-traversal in GPUs",
      "paper_id": "2006.06890v2"
    },
    {
      "index": 17,
      "title": "â€œFine-grained memory access over I/O interconnect for efficient remote sparse data access",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 18,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 19,
      "title": "â€œMemory capacity growth: A major contributor to the success of computers",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 20,
      "title": "â€œNo silver bullet essence and accidents of software engineering",
      "abstract": "",
      "year": "1987",
      "venue": "",
      "authors": ""
    },
    {
      "index": 21,
      "title": "â€œcuBLAS library user guide v12.0",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 22,
      "title": "â€ NVIDIA Corporation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 23,
      "title": "â€œHector: An efficient programming and compilation framework for implementing relational graph neural networks in GPU architectures",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 24,
      "title": "â€œPyTorch-Direct: Enabling GPU centric data access for very large graph neural network training with irregular accesses",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 25,
      "title": "â€œGraph neural network training with data tiering",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 26,
      "title": "TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œTBA: Faster large language model training using SSD-based activation offloading",
      "paper_id": "2408.10013v2"
    },
    {
      "index": 27,
      "title": "â€œBackpropagation applied to handwritten zip code recognition",
      "abstract": "",
      "year": "1989",
      "venue": "",
      "authors": ""
    },
    {
      "index": 28,
      "title": "â€œSpectral networks and locally connected networks on graphs",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 29,
      "title": "Inductive Representation Learning on Large Graphs",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œInductive representation learning on large graphs",
      "paper_id": "1706.02216v4"
    },
    {
      "index": 30,
      "title": "â€œConvolutional neural networks on graphs with fast localized spectral filtering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 31,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSemi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 32,
      "title": "â€œVariational graph auto-encoders",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 33,
      "title": "Learning Convolutional Neural Networks for Graphs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLearning convolutional neural networks for graphs",
      "paper_id": "1605.05273v4"
    },
    {
      "index": 34,
      "title": "Representation Learning on Graphs: Methods and Applications",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œRepresentation learning on graphs: Methods and applications",
      "paper_id": "1709.05584v3"
    },
    {
      "index": 35,
      "title": "DeepWalk: Online Learning of Social Representations",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeepWalk: Online learning of social representations",
      "paper_id": "1403.6652v2"
    },
    {
      "index": 36,
      "title": "node2vec: Scalable Feature Learning for Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "â€œNode2vec: Scalable feature learning for networks",
      "paper_id": "1607.00653v1"
    },
    {
      "index": 37,
      "title": "â€œBing Chat â€” Microsoft Edge",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "â€ 2022. [Online]. Available: https://github.com/langchain-ai/langchain",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 39,
      "title": "Emergent Abilities of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEmergent abilities of large language models",
      "paper_id": "2206.07682v2"
    },
    {
      "index": 40,
      "title": "â€œLanguage models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAttention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 42,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "â€œBERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 43,
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLlama 2: Open foundation and fine-tuned chat models",
      "paper_id": "2307.09288v2"
    },
    {
      "index": 44,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "1910",
      "venue": "",
      "authors": "",
      "orig_title": "â€œExploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 45,
      "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGSPMD: General and scalable parallelization for ML computation graphs",
      "paper_id": "2105.04663v2"
    },
    {
      "index": 46,
      "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "abstract": "",
      "year": "1909",
      "venue": "",
      "authors": "",
      "orig_title": "â€œMegatron-LM: Training multi-billion parameter language models using model parallelism",
      "paper_id": "1909.08053v4"
    },
    {
      "index": 47,
      "title": "â€œDeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 48,
      "title": "â€œPyTorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 49,
      "title": "â€œZeRO: Memory optimizations toward training trillion parameter models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 50,
      "title": "Programming Massively Parallel Processors",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "â€œNvidia Tesla V100 GPU architecture whitepaper",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 52,
      "title": "â€œDissecting the NVIDIA Volta GPU architecture via microbenchmarking",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "â€œInstructions for managing a parallel cache hierarchy",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "â€œEE 7722 GPU microarchitecture lecture notes",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "â€œ2. kernel profiling guide â€” NsightCompute 12.6 documentation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "â€œSorting of vector of tuple in C++ (ascending order)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 58,
      "title": "â€œCan pytorch by-pass Python GIL?â€ 2019",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 60,
      "title": "â€œPEP 703 â€“ making the global interpreter lock optional in CPython â€” peps.python.org",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "â€œAutomated GPU kernel fusion with XLA",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "â€œPyTorch 2 tutorial and paper presentation @ ASPLOSâ€™2024",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "â€œCompiling high-level scripting languages to performant code",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 65,
      "title": "â€œGPU acceleration of automatic differentiation in C++ with Clad",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "â€œReverse-mode automatic differentiation and optimization of GPU kernels via Enzyme",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "â€œpybind11 â€” seamless operability between c++11 and python",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "â€œDeep graph library: A graph-centric",
      "abstract": "",
      "year": "1909",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "Fast Graph Representation Learning with PyTorch Geometric",
      "abstract": "",
      "year": "1903",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFast graph representation learning with PyTorch Geometric",
      "paper_id": "1903.02428v3"
    },
    {
      "index": 70,
      "title": "FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFeatGraph: A flexible and efficient backend for graph neural network systems",
      "paper_id": "2008.11359v2"
    },
    {
      "index": 71,
      "title": "Efficient Sparse Matrix Kernels based on Adaptive Workload-Balancing and Parallel-Reduction",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEfficient sparse matrix kernels based on adaptive workload-balancing and parallel-reduction",
      "paper_id": "2106.16064v2"
    },
    {
      "index": 72,
      "title": "SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSparseTIR: Composable abstractions for sparse compilation in deep learning",
      "paper_id": "2207.04606v4"
    },
    {
      "index": 73,
      "title": "â€œModeling relational data with graph convolutional networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "Heterogeneous Graph Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œHeterogeneous graph transformer",
      "paper_id": "2003.01332v1"
    },
    {
      "index": 75,
      "title": "â€œEmpirical analysis of performance bottlenecks in graph neural network training and inference with GPUs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "â€œThe nature of graph neural network workloads",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "â€œcublas<<t>>gemmBatched() â€” cuBLAS library user guide v12.2",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "â€œAccelerating matrix multiplication with block sparse format and NVIDIA tensor cores â€” NVIDIA technical blog",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "â€œ[feature] gather mm by isratnisa â‹…â‹…\\cdot pull request #3641 â‹…â‹…\\cdot dmlc/dgl",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "â€œSeastar: Vertex-centric programming for graph neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "â€œGraphiler: Optimizing graph neural networks with message passing data flow graph",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "â€œHGL: Accelerating heterogeneous GNN training with holistic representation and optimization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "Relational Graph Attention Networks",
      "abstract": "",
      "year": "1904",
      "venue": "",
      "authors": "",
      "orig_title": "â€œRelational graph attention networks",
      "paper_id": "1904.05811v1"
    },
    {
      "index": 84,
      "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "",
      "orig_title": "â€œOpen graph benchmark: Datasets for machine learning on graphs",
      "paper_id": "2005.00687v7"
    },
    {
      "index": 85,
      "title": "â€œKernel methods for mining instance data in ontologies",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "â€ Journal of Medicinal Chemistry",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "â€œA fast approximation of the weisfeiler-lehman graph kernel for RDF data",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "J. van Ossenbruggen",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "â€œObserved versus latent features for knowledge base and text inference",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "â€œFast algorithms for convolutional neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "â€œOptimizing irregular dense operators of heterogeneous gnn models on gpu",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "â€œTorchScript â€” PyTorch 2.2 documentation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 93,
      "title": "â€œNumba: A LLVM-based Python JIT compiler",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 95,
      "title": "â€œTVM: An automated end-to-end optimizing compiler for deep learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGE-SpMM: General-purpose sparse matrix-matrix multiplication on GPUs for graph neural networks",
      "paper_id": "2007.03179v1"
    },
    {
      "index": 97,
      "title": "At-Scale Sparse Deep Neural Network Inference With Efficient GPU Implementation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAt-scale sparse deep neural network inference with efficient gpu implementation",
      "paper_id": "2007.14152v2"
    },
    {
      "index": 98,
      "title": "â€œTLPGNN: A lightweight two-level parallelism paradigm for graph neural network computation on GPU",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "â€œThe tensor algebra compiler",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "â€œMLIR: Scaling compiler infrastructure for domain specific computation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFusedMM: A unified sddmm-spmm kernel for graph embedding and graph neural networks",
      "paper_id": "2011.06391v2"
    },
    {
      "index": 102,
      "title": "â€œDense dynamic blocks: Optimizing SpMM for processors with vector and matrix units using machine learning techniques",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "H. Asghari-Moghaddam",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "â€œAutomatic performance tuning of sparse matrix kernels",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "â€œDesign principles for sparse matrix multiplication on the GPU",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "CSR5: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œCSR5: An efficient storage format for cross-platform sparse matrix-vector multiplication",
      "paper_id": "1503.05032v2"
    },
    {
      "index": 107,
      "title": "â€œyaSpMV: Yet another SpMV framework on GPUs",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "â€œChapter 31 - abstraction for AoS and SoA layout in C++",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "SoAx: A generic C++ Structure of Arrays for handling particles in HPC codes",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSoAx: A generic C++ structure of arrays for handling particles in HPC codes",
      "paper_id": "1710.03462v1"
    },
    {
      "index": 110,
      "title": "DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDistDGL: Distributed graph neural network training for billion-scale graphs",
      "paper_id": "2010.05337v3"
    },
    {
      "index": 111,
      "title": "â€œTensor algebra compilation with workspaces",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "â€œCluster-GCN: An efficient algorithm for training deep and large graph convolutional networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "â€œGraphSAINT: Graph sampling based inductive learning method",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "A Comprehensive Survey on Graph Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA comprehensive survey on graph neural networks",
      "paper_id": "1901.00596v4"
    },
    {
      "index": 115,
      "title": "â€œStellargraph machine learning library",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Graph Neural Networks in TensorFlow and Keras with Spektral",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGraph neural networks in tensorflow and keras with spektral",
      "paper_id": "2006.12138v1"
    },
    {
      "index": 117,
      "title": "SIGN: Scalable Inception Graph Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSIGN: Scalable inception graph neural networks",
      "paper_id": "2004.11198v3"
    },
    {
      "index": 118,
      "title": "efficient and scalable graph embedding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "â€œImproving the accuracy",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "â€œLeaderboards for node property prediction â€” open graph benchmark",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "â€œTowards efficient large-scale graph neural network computing",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "â€œTraversing large graphs on GPUs with unified memory",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "â€œSubway: Minimizing data transfer during out-of-GPU-memory graph processing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "â€œNvidia Tesla P100 whitepaper",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "â€œNvidia A100 TensorCore GPU architecture whitepaper",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 127,
      "title": "â€œEvaluating characteristics of CUDA communication primitives on high-bandwidth interconnects",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "â€œData transfer matters for GPU computing",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "â€œDeveloping a linux kernel module using GPUDirect RDMA",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "â€œUnified memory for CUDA beginners",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 131,
      "title": "â€œPerformance evaluation of advanced features in CUDA unified memory",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "â€œImageNet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 134,
      "title": "Graph Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œGraph attention networks",
      "paper_id": "1710.10903v3"
    },
    {
      "index": 135,
      "title": "â€œTorchvision the machine-vision package of torch",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "â€œBeyond GPU memory limits with unified memory on pascal",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "â€œAutograd - pytorch/pytorch",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 138,
      "title": "â€œAten/aten/src/readme.md at master Â· zdevito/aten Â· GitHub",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "â€œThe WebGraph framework I: Compression techniques",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "â€œWhat is Twitter",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "â€œKonect: The koblenz network collection",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œWinner-take-all column row sampling for memory efficient adaptation of language model",
      "paper_id": "2305.15265v4"
    },
    {
      "index": 143,
      "title": "Reducing Activation Recomputation in Large Transformer Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "â€œReducing activation recomputation in large transformer models",
      "paper_id": "2205.05198v1"
    },
    {
      "index": 144,
      "title": "â€œMegaScale: Scaling large language model training to more than 10",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "A. McMillan-Major",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "â€œDissecting batching effects in GPT inference",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "The Case for Co-Designing Model Architectures with Hardware",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œThe case for co-designing model architectures with hardware",
      "paper_id": "2401.14489v2"
    },
    {
      "index": 148,
      "title": "DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeepSpeed Inference: Enabling efficient inference of transformer models at unprecedented scale",
      "paper_id": "2207.00032v1"
    },
    {
      "index": 149,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "",
      "orig_title": "â€œScaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 150,
      "title": "An Empirical Model of Large-Batch Training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAn empirical model of large-batch training",
      "paper_id": "1812.06162v1"
    },
    {
      "index": 151,
      "title": "â€œTraining deep nets with sublinear memory cost",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "â€œAnnouncing Epoch AIâ€™s updated parameter",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "â€œND A100 V4-series - Azure virtual machines",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "â€œGPU machine types | compute engine documentation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "â€œDelta project profile",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFiddler: CPU-GPU orchestration for fast inference of mixture-of-experts models",
      "paper_id": "2402.07033v3"
    },
    {
      "index": 157,
      "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œZeRO-Offload: Democratizing billion-scale model training",
      "paper_id": "2101.06840v1"
    },
    {
      "index": 158,
      "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œPowerInfer: Fast large language model serving with a consumer-grade GPU",
      "paper_id": "2312.12456v2"
    },
    {
      "index": 159,
      "title": "â€œFlashNeuron: SSD-enabled large-batch training of very deep neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "â€œAbout Google cloud hyperdisk â€” compute engine documentation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "â€œArchitecture and performance of Perlmutterâ€™s 35 PB ClusterStor E1000 allâ€flash file system",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "â€œMegatron-DeepSpeed: Ongoing research training transformer language models at scale",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "Training Compute-Optimal Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "â€œTraining compute-optimal large language models",
      "paper_id": "2203.15556v1"
    },
    {
      "index": 164,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLanguage models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 165,
      "title": "â€œAll SPEC/OSG results.â€ [Online]. Available: http://spec.org/cgi-bin/osgresults?conf=cpu2017;op=dump;format=csvdump",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "â€œUltra-low latency with Samsung Z-NAND SSD",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "JESD218B: Solid-State Drive (SSD) Requirements and Endurance Test Method",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 168,
      "title": "Available: https://thinksystem",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "â€œQNAP NAS solution: QTS SSD extra over-provisioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "â€œWhy SMARTâ€™s over-provisioning?â€ 2024",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 171,
      "title": "â€œSolidigmâ„¢ SSD endurance estimator",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "â€œOver-provisioning NAND-based IntelÂ® SSDs for better endurance",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 173,
      "title": "â€œOver-provisioning benefits for Samsung data center SSDs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "â€œOperational characteristics of SSDs in enterprise storage systems: A large-scale field study",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 175,
      "title": "â€œD7-P5620 mid-endurance PCIe 4.0 NVMe SSD for data centers | Solidigm D7 SSD",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 177,
      "title": "â€œSolidigmâ„¢ solid state drive D7-P5620 series (12.8TB",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 178,
      "title": "â€œFL6 series (2.5-inch) | KIOXIA - United States (English)",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "â€œKioxia fl6xhul1t60 1.6TB PCIe4 NVMe SSD brand new",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 180,
      "title": "â€œSOLIDIGM ssdpf2sq800gz01 D7-P5810 solid state drive â€“ Dihuni â€“ GPU server for AI",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 181,
      "title": "â€œQuantifying performance gains of GPUDirect Storage",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 182,
      "title": "â€œSTRONGHOLD: Fast and affordable billion-scale deep learning model training",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "â€œZeRO-Infinity: Breaking the GPU memory wall for extreme scale deep learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFlexGen: High-throughput generative inference of large language models with a single GPU",
      "paper_id": "2303.06865v2"
    },
    {
      "index": 185,
      "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLLM in a Flash: Efficient large language model inference with limited memory",
      "paper_id": "2312.11514v3"
    },
    {
      "index": 186,
      "title": "â€œKvikIO - high performance file IO",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 188,
      "title": "â€œFlash Correct-and-Refresh: Retention-aware error management for increased Flash memory lifetime",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 189,
      "title": "â€œError patterns in MLC NAND Flash memory: Measurement",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "â€œOptimizing NAND Flash-based SSDs via retention relaxation",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 191,
      "title": "â€œBehemoth: A Flash-centric training accelerator for extreme-scale DNNs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œA monolingual approach to contextualized word embeddings for mid-resource languages",
      "paper_id": "2006.06202v2"
    },
    {
      "index": 193,
      "title": "â€œAsynchronous pipelines for processing huge corpora on medium to low resource infrastructures",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 194,
      "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œFlashAttention-2: Faster attention with better parallelism and work partitioning",
      "paper_id": "2307.08691v1"
    },
    {
      "index": 195,
      "title": "â€œScaling large language model training with Pax on GPUs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "â€œFlashAttention: Fast and memory-efficient exact attention with IO-awareness",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 197,
      "title": "â€œPaxml (aka Pax)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "analytics and research â€“ Dihuni â€“ GPU server for AI",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "â€œIntel Optane DC P5800X series 1.6TB",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "â€œSamsung V-NAND SSD 980 pro 2021 data sheet revision 2.1",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "â€œSamsung 980 pro 1TB internal gaming SSD PCIe gen 4 x4 nvme mz-v8p1t0b/am",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 202,
      "title": "â€œDonâ€™t be a blockhead: Zoned namespaces make work on conventional SSDs obsolete",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "â€œZNS+: Advanced zoned namespace interface for supporting in-storage zone compaction",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "The Datacenter as a Computer: Designing Warehouse-Scale Machines",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 205,
      "title": "â€œTotal cost of ownership (TCO) analysis",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "â€œNvidia Blackwell perf TCO analysis â€“ B100 vs B200 vs GB200NVL72 â€“ SemiAnalysis",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 207,
      "title": "â€œSNIA enterprise TCO calculator",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "â€œIntroduction to NVIDIA DGX H100/H200 systems â€” NVIDIA DGX H100/H200 user guide",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 209,
      "title": "â€œNVM Express moves into the future",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "â€œSoftware:Lustre (file system)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "â€œGPU Direct I/O with HDF5",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEfficient memory management for large language model serving with PagedAttention",
      "paper_id": "2309.06180v1"
    },
    {
      "index": 213,
      "title": "â€œCapuchin: Tensor-based GPU memory management for deep learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 214,
      "title": "SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSuperNeurons: Dynamic GPU memory management for training deep neural networks",
      "paper_id": "1801.04380v1"
    },
    {
      "index": 215,
      "title": "â€œvDNN: Virtualized deep neural networks for scalable",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "â€œSwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "â€œNVIDIA H100 tensor core GPU architecture",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "â€œBig Bird: Transformers for longer sequences",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDeja Vu: Contextual sparsity for efficient LLMs at inference time",
      "paper_id": "2310.17157v1"
    },
    {
      "index": 220,
      "title": "SqueezeLLM: Dense-and-Sparse Quantization",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSqueezeLLM: Dense-and-sparse quantization",
      "paper_id": "2306.07629v4"
    },
    {
      "index": 221,
      "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSpQR: A sparse-quantized representation for near-lossless LLM weight compression",
      "paper_id": "2306.03078v1"
    },
    {
      "index": 222,
      "title": "â€œSparseGPT: Massive language models can be accurately pruned in one-shot",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "â€œOutrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "paper_id": "1701.06538v1"
    },
    {
      "index": 224,
      "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œLearning n:m fine-grained structured sparse neural networks from scratch",
      "paper_id": "2102.04010v2"
    },
    {
      "index": 225,
      "title": "â€œChannel permutations for N:M sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 226,
      "title": "â€œMegaBlocks: Efficient sparse training with mixture-of-experts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 227,
      "title": "â€œSparTA: Deep-learning model sparsity via tensor-with-sparsity-attribute",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 228,
      "title": "Sparse GPU Kernels for Deep Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "â€œSparse GPU kernels for deep learning",
      "paper_id": "2006.10901v2"
    },
    {
      "index": 229,
      "title": "Efficient Quantized Sparse Matrix Operations on Tensor Cores",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œEfficient quantized sparse matrix operations on tensor cores",
      "paper_id": "2209.06979v4"
    },
    {
      "index": 230,
      "title": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "â€œDynamic N:M fine-grained structured sparse attention mechanism",
      "paper_id": "2203.00091v1"
    },
    {
      "index": 231,
      "title": "Accelerating Sparse Deep Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "â€œAccelerating sparse deep neural networks",
      "paper_id": "2104.08378v1"
    },
    {
      "index": 232,
      "title": "â€œTensorRT-LLM: A TensorRT toolbox for optimized large language model inference",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 233,
      "title": "â€œTransformer engine",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 234,
      "title": "â€œAdd new keys for Graphcore IPU (DispatchKey / Backend / DeviceType) by AnthonyBarbier Â· pull request #74763 Â· pytorch/pytorch",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 235,
      "title": "â€œRelease v0.8.0 Â· dmlc/dgl",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 236,
      "title": "â€œ[doc] add an official documentation of UnifiedTensor by davidmin7 Â· pull request #3194 Â· dmlc/dgl",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 237,
      "title": "â€œ[feature] add multi-GPU UnifiedTensor unit test by davidmin7 Â· pull request #3184 Â· dmlc/dgl",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 238,
      "title": "â€ 2021. [Online]. Available: https://github.com/dmlc/dgl/pull/3086",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 239,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 240,
      "title": "â€œBridging the gap between deep learning and sparse matrix format selection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 241,
      "title": "â€œParallelizing maximal clique enumeration on GPUs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 242,
      "title": "â€œHyLAC: Hybrid linear assignment solver in CUDA",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 243,
      "title": "â€œControlling data movement to boost performance on the NVIDIA ampere architecture",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 244,
      "title": "â€œMixture-of-experts with expert choice routing",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 245,
      "title": "â€œ2017 Kaggle machine learning & data science survey",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 246,
      "title": "â€œ2017 data scientist report",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 247,
      "title": "â€œDoing a reality check on GPU-accelerated databases",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 248,
      "title": "â€œRAPIDS: GPU-accelerated data analytics & machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 249,
      "title": "â€œGPU database systems characterization and optimization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 250,
      "title": "â€œGPU databasesâ€”the new modality of data analytics",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 251,
      "title": "â€œDatabase optimization techniques #1: Indexing",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 252,
      "title": "â€œMySQL :: MySQL 8.4 reference manual :: 10.3 optimization and indexes",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 253,
      "title": "ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 254,
      "title": "â€œSQreamâ€™s unique architecture: Comparing and contrasting to leading data architectures",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 255,
      "title": "â€œEvaluating end-to-end optimization for data analytics applications in Weld",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 256,
      "title": "â€œA tensor compiler for uniï¬ed machine learning prediction serving",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    }
  ]
}