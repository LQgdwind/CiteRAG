{
  "paper_id": "2010.10814v1",
  "title": "Improving Generalization in Reinforcement Learning with Mixture Regularization",
  "sections": {
    "related works": "Generalization in RL draws increasing attention in recent years.\nSome works [ref]3 [ref]29 [ref]2 show the generalization gap comes from the limited diversity of training environments.\nThis provides a direction to minimize the generalization gap via increasing the training diversity.\nCobbe et al. [ref]3 find augmenting the observations with cutout  helps improve generalization.\nLee et al.  propose to use a randomized convolutional layer for randomly perturbing the input observations.\nApart from cutout and random convolution, Laskin et al.  further evaluate a wide spectrum of data augmentations on improving generalization. Concurrently, Kostrikov et al.  propose DrQ to apply data augmentations in model-free RL algorithms. Rather than finding the best augmentation for all tasks, Raileanu et al.  propose to automatically select an appropriate augmentation for each given task. In addition to increasing training diversity, regularization techniques such as ℓ2subscriptℓ2\\ell_{2} regularization, dropout  and batch normalization  also help improve RL agents’ generalizability  [ref]3 .\nOur work follows the direction of increasing training diversity but proposes a more effective approach than prior data augmentation techniques. It can be combined with other methods such as ℓ2subscriptℓ2\\ell_{2} regularization to yield further improvement. Our work is also related to mixup  in supervised learning.\nMixup establishes a linear relationship between the interpolation of the features and that of the class labels, increasing the generalization and robustness of the trained classification model.\nThe authors of  argued the interpolation principle seems like a reasonable inductive bias that can be possibly extended to RL. Inspired by this, our work investigates on whether enforcing a linear relationship between interpolations of observations and supervision signals helps improve generalization in RL and how it affects the learned policy. To the best of our knowledge, only one existing work  applies mixup in RL but their aim is\ndistinct from ours. They uses mixup regularization for learning a reward function while our target is improving generalization."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Agent57: Outperforming the Atari Human Benchmark",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.13350",
      "authors": "A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell",
      "orig_title": "Agent57: outperforming the atari human benchmark",
      "paper_id": "2003.13350v1"
    },
    {
      "index": 1,
      "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01588",
      "authors": "K. Cobbe, C. Hesse, J. Hilton, and J. Schulman",
      "orig_title": "Leveraging procedural generation to benchmark reinforcement learning",
      "paper_id": "1912.01588v2"
    },
    {
      "index": 2,
      "title": "Quantifying Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.02341",
      "authors": "K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman",
      "orig_title": "Quantifying generalization in reinforcement learning",
      "paper_id": "1812.02341v3"
    },
    {
      "index": 3,
      "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04552",
      "authors": "T. DeVries and G. W. Taylor",
      "orig_title": "Improved regularization of convolutional neural networks with cutout",
      "paper_id": "1708.04552v2"
    },
    {
      "index": 4,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.01561",
      "authors": "L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al.",
      "orig_title": "Impala: scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 5,
      "title": "Generalization and Regularization in DQN",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00123",
      "authors": "J. Farebrother, M. C. Machado, and M. Bowling",
      "orig_title": "Generalization and regularization in dqn",
      "paper_id": "1810.00123v3"
    },
    {
      "index": 6,
      "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07377",
      "authors": "S. Gamrian and Y. Goldberg",
      "orig_title": "Transfer learning for related reinforcement learning tasks via image-to-image translation",
      "paper_id": "1806.07377v6"
    },
    {
      "index": 7,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver",
      "orig_title": "Rainbow: combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 8,
      "title": "Generalization in reinforcement learning with selective noise injection and information bottleneck",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann"
    },
    {
      "index": 9,
      "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1502.03167",
      "authors": "S. Ioffe and C. Szegedy"
    },
    {
      "index": 10,
      "title": "Image augmentation is all you need: regularizing deep reinforcement learning from pixels",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.13649",
      "authors": "I. Kostrikov, D. Yarats, and R. Fergus"
    },
    {
      "index": 11,
      "title": "Reinforcement Learning with Augmented Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.14990",
      "authors": "M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas",
      "orig_title": "Reinforcement learning with augmented data",
      "paper_id": "2004.14990v5"
    },
    {
      "index": 12,
      "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "K. Lee, K. Lee, J. Shin, and H. Lee",
      "orig_title": "Network randomization: a simple technique for generalization in deep reinforcement learning",
      "paper_id": "1910.05396v3"
    },
    {
      "index": 13,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1509.02971",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra"
    },
    {
      "index": 14,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature 518 (7540)",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al."
    },
    {
      "index": 15,
      "title": "Assessing Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.12282",
      "authors": "C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song",
      "orig_title": "Assessing generalization in deep reinforcement learning",
      "paper_id": "1810.12282v2"
    },
    {
      "index": 16,
      "title": "Automatic data augmentation for generalization in deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12862",
      "authors": "R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus"
    },
    {
      "index": 17,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 18,
      "title": "Mastering the game of go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "nature 529 (7587)",
      "authors": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al."
    },
    {
      "index": 19,
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.01815",
      "authors": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al.",
      "orig_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm",
      "paper_id": "1712.01815v1"
    },
    {
      "index": 20,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature 550 (7676)",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al."
    },
    {
      "index": 21,
      "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.07854",
      "authors": "A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine",
      "orig_title": "End-to-end robotic reinforcement learning without reward engineering",
      "paper_id": "1904.07854v2"
    },
    {
      "index": 22,
      "title": "Observational Overfitting in Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.02975",
      "authors": "X. Song, Y. Jiang, Y. Du, and B. Neyshabur",
      "orig_title": "Observational overfitting in reinforcement learning",
      "paper_id": "1912.02975v2"
    },
    {
      "index": 23,
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "abstract": "",
      "year": "2014",
      "venue": "The journal of machine learning research 15 (1)",
      "authors": "N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov"
    },
    {
      "index": 24,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour"
    },
    {
      "index": 25,
      "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)",
      "authors": "J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel",
      "orig_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
      "paper_id": "1703.06907v1"
    },
    {
      "index": 26,
      "title": "StarCraft II: A New Challenge for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.04782",
      "authors": "O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, et al.",
      "orig_title": "Starcraft ii: a new challenge for reinforcement learning",
      "paper_id": "1708.04782v1"
    },
    {
      "index": 27,
      "title": "Learning to reinforcement learn",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.05763",
      "authors": "J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick"
    },
    {
      "index": 28,
      "title": "A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07937",
      "authors": "A. Zhang, N. Ballas, and J. Pineau",
      "orig_title": "A dissection of overfitting and generalization in continuous reinforcement learning",
      "paper_id": "1806.07937v2"
    },
    {
      "index": 29,
      "title": "Natural Environment Benchmarks for Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.06032",
      "authors": "A. Zhang, Y. Wu, and J. Pineau",
      "orig_title": "Natural environment benchmarks for reinforcement learning",
      "paper_id": "1811.06032v1"
    },
    {
      "index": 30,
      "title": "A study on overfitting in deep reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.06893",
      "authors": "C. Zhang, O. Vinyals, R. Munos, and S. Bengio"
    },
    {
      "index": 31,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.09412",
      "authors": "H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz",
      "orig_title": "Mixup: beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    }
  ]
}