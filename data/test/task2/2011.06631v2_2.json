{
  "paper_id": "2011.06631v2",
  "title": "Steady State Analysis of Episodic Reinforcement Learning",
  "sections": {
    "introduction": "Given a decision task, reinforcement learning (RL) agents iteratively optimize decision policy based on empirical data collected from continuously interacting with a learning environment of the task. To have successful learning, the empirical data used for policy update should represent some desired distribution. Every RL algorithm then faces two basic questions: what data distribution shall be considered desired for the sake of learning, and how should the agent interact with the environment to actually obtain the data as desired? Depending on the type of the given task, though, existing RL literature have treated this data collection problem in two different ways. In continual reinforcement learning, the agent immerses itself in a single everlasting sequential-decision episode that is conceptually of infinite length (or of life-long length). In this case, a stationary (or steady-state) distribution is a fixed-point distribution over the agent‚Äôs input space under the transition dynamic induced by a decision policy. The concept of steady state has been pivotal in continual RL literature. It is typically presumed that a unique stationary distribution exists when rolling out any policy in the continual RL model being studied, and that the empirical data distribution of the rollout indeed converges over time to this stationary distribution due to (again assumed) ergodicity of the system dynamic¬†[ref]29 [ref]4  . Continual RL algorithms can be derived and analyzed by examining system properties under the steady state¬† [ref]29  , and many of the resulted algorithms require the training data to follow the stationary distribution of some behavior policy¬†[ref]4  (or a mixture of such distributions¬†), which can then be efficiently collected from a few (or even a single) time steps once the rollout of the behavior policy converges to its steady state. The situation has been quite different for episodic reinforcement learning, in which the agent makes a finite number of decisions before an episode of the task terminates. Episodic RL tasks account for the vast majority of experimental RL benchmarks and of empirical RL applications at the moment¬† . Due to the finiteness of decision horizon, for episodic tasks steady state was widely considered non-existent¬† or having a degenerate form¬†, in either case irrelevant. The lack of meaningful steady state in such tasks has in turn led to more modeling disparities in the episodic RL formalism.\nIndeed, as Sutton and Barto  (chap. 9, p. 200) wrote: \"[t]he two cases, continuing and episodic, behave similarly, but with [function] approximation they must be treated separately in formal analyses, as we will see repeatedly in this part of the book\". In particular, the desired data distributions for episodic RL algorithms are usually characterized by alternative concepts, such as the expected episode-wise visiting frequencies¬†, or a Œ≥ùõæ\\gamma-discounted variant of it¬† . Intriguingly, despite the rather different theoretical framework behind, many episodic RL algorithms¬†[ref]14  update policies using data from a small time slice (such as from one time step¬† or from several consecutive steps¬†[ref]14 ) of the episodic learning process, in almost the same way as how continual RL algorithms¬†2 [ref]14 collect data from ergodic models. This ‚Äúonline-style‚Äù data collection¬†, while being popular in modern RL algorithms[ref]14     thanks to its simplicity and sample efficiency, is often not well justified in episodic tasks by the prescribed data distribution which requires collecting a whole episode trajectory as one sample point of the distribution¬†. In this paper, we propose a new perspective to treat the data collection problem for episodic RL tasks. We argue that in spite of the finite decision horizon, the RL agent has the chance to repeat the same decision process in the learning environment of an episodic RL task. The resulted learning process is an infinite loop of homogeneous and finite episodes. By formalizing such an episodic learning environment into a family of infinite-horizon MDPs with special structures (Section 3), we mathematically proved that a unique stationary distribution exists in every such episodic learning process, regardless of the agent policy, and that the marginal distribution of the rollout data indeed converges to this stationary distribution in ‚Äúproper models‚Äù of the episodic RL task (Section 4). Conceptually, our theoretical observation supports a reversed mindset against conventional wisdom: while unique steady state is (only) presumed to exist in continual tasks, it is now guaranteed to exist in all episodic tasks. Moreover, through analyzing the Bellman equation under the steady state, we obtained interesting and rigorous connections between the separately defined semantics of important concepts (such as performance measure and on-policy distribution) across episodic and continual RL. These results help with unifying the two currently disparate RL formalisms (Section 5). Algorithmically, the unified framework enables us to collect data efficiently in episodic tasks in a similar way as in the continual setting: Write the quantity to be computed as an expectation over the stationary distribution of some policy Œ≤ùõΩ\\beta, rollout Œ≤ùõΩ\\beta and wait for it to converge, after which we obtain an unbiased sample point in every single time step. As a demonstration of this general approach, we derived a new steady-state policy gradient theorem, which writes policy gradient into such an expected value. The new policy gradient theorem not only better justifies the popular few-step-sampling practice used in modern policy gradient algorithms¬†[ref]14 , but also makes explicit some less explored bias in those algorithms (Section 6.1). Finally, to facilitate rapid steady-state convergence, we proposed a perturbation trick, and experimentally showed that it is both necessary and effective for data collection in episodic tasks of standard RL benchmark (Section 6.2)."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Pybullet gymperium.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Dynamic programming and optimal control, volume 1.",
      "abstract": "",
      "year": "1995",
      "venue": "Athena scientific Belmont, MA",
      "authors": "D. P. Bertsekas."
    },
    {
      "index": 2,
      "title": "Openai gym.",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba."
    },
    {
      "index": 3,
      "title": "Off-policy actor-critic.",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1205.4839",
      "authors": "T. Degris, M. White, and R. S. Sutton."
    },
    {
      "index": 4,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel.",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control.",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 5,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.09477",
      "authors": "S. Fujimoto, H. Van Hoof, and D. Meger.",
      "orig_title": "Addressing function approximation error in actor-critic methods.",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 6,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Gu, T. Lillicrap, I. Sutskever, and S. Levine.",
      "orig_title": "Continuous deep q-learning with model-based acceleration.",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 7,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine.",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 8,
      "title": "A natural policy gradient.",
      "abstract": "",
      "year": "2002",
      "venue": "Advances in neural information processing systems",
      "authors": "S. M. Kakade."
    },
    {
      "index": 9,
      "title": "Continuous control with deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1509.02971",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra."
    },
    {
      "index": 10,
      "title": "Simulation-based optimization of markov reward processes.",
      "abstract": "",
      "year": "2001",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "P. Marbach and J. N. Tsitsiklis."
    },
    {
      "index": 11,
      "title": "Markov chains and stochastic stability.",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "S. P. Meyn and R. L. Tweedie."
    },
    {
      "index": 12,
      "title": "Human-level control through deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al."
    },
    {
      "index": 13,
      "title": "Asynchronous methods for deep reinforcement learning.",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu."
    },
    {
      "index": 14,
      "title": "Time Limits in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.00378",
      "authors": "F. Pardo, A. Tavakoli, V. Levdik, and P. Kormushev.",
      "orig_title": "Time limits in reinforcement learning.",
      "paper_id": "1712.00378v4"
    },
    {
      "index": 15,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz.",
      "orig_title": "Trust region policy optimization.",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 16,
      "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
      "abstract": "",
      "year": "2017a",
      "venue": "arXiv preprint arXiv:1704.06440",
      "authors": "J. Schulman, X. Chen, and P. Abbeel.",
      "orig_title": "Equivalence between policy gradients and soft q-learning.",
      "paper_id": "1704.06440v4"
    },
    {
      "index": 17,
      "title": "Proximal policy optimization algorithms.",
      "abstract": "",
      "year": "2017b",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov."
    },
    {
      "index": 18,
      "title": "Basics of applied stochastic processes.",
      "abstract": "",
      "year": "2009",
      "venue": "Springer Science & Business Media",
      "authors": "R. Serfozo."
    },
    {
      "index": 19,
      "title": "Deterministic policy gradient algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller."
    },
    {
      "index": 20,
      "title": "The predictron: End-to-end learning and planning.",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "D. Silver, H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley, G. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, et al."
    },
    {
      "index": 21,
      "title": "A new q (lambda) with interim forward view and monte carlo equivalence.",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "R. Sutton, A. R. Mahmood, D. Precup, and H. Hasselt."
    },
    {
      "index": 22,
      "title": "Td models: modeling the world at a mixture of time scales.",
      "abstract": "",
      "year": "1995",
      "venue": "Twelfth International Conference on Machine Learning",
      "authors": "R. S. Sutton."
    },
    {
      "index": 23,
      "title": "Reinforcement learning: An introduction.",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto."
    },
    {
      "index": 24,
      "title": "Policy gradient methods for reinforcement learning with function approximation.",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour."
    },
    {
      "index": 25,
      "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.",
      "abstract": "",
      "year": "2011",
      "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup."
    },
    {
      "index": 26,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "R. S. Sutton, A. R. Mahmood, and M. White.",
      "orig_title": "An emphatic approach to the problem of off-policy temporal-difference learning.",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 27,
      "title": "Bias in natural actor-critic algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "P. Thomas."
    },
    {
      "index": 28,
      "title": "On average versus discounted reward temporal-difference learning.",
      "abstract": "",
      "year": "2002",
      "venue": "Machine Learning",
      "authors": "J. N. Tsitsiklis and B. Van Roy."
    },
    {
      "index": 29,
      "title": "Deep Reinforcement Learning with Double Q-learning",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI",
      "authors": "H. Van Hasselt, A. Guez, and D. Silver.",
      "orig_title": "Deep reinforcement learning with double q-learning.",
      "paper_id": "1509.06461v3"
    },
    {
      "index": 30,
      "title": "Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.",
      "abstract": "",
      "year": "2011",
      "venue": "Utrecht University",
      "authors": "H. P. van Hasselt."
    },
    {
      "index": 31,
      "title": "Learning from delayed rewards.",
      "abstract": "",
      "year": "1989",
      "venue": "King‚Äôs College, University of Cambridge",
      "authors": "C. Watkins."
    },
    {
      "index": 32,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "M. White.",
      "orig_title": "Unifying task specification in reinforcement learning.",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 33,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "R. J. Williams."
    },
    {
      "index": 34,
      "title": "On convergence of emphatic temporal-difference learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "H. Yu."
    },
    {
      "index": 35,
      "title": "Generalized Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Zhang, W. Boehmer, and S. Whiteson.",
      "orig_title": "Generalized off-policy actor-critic.",
      "paper_id": "1903.11329v8"
    }
  ]
}