{
  "paper_id": "2203.13381v2",
  "title": "Probing Representation Forgetting in Supervised and Unsupervised Continual Learning",
  "sections": {
    "related work": "The design of CL methods is often focused on mitigating the catastrophic forgetting phenomenon, with aspects such as maximization of forward and backward transfer between tasks taken as secondary¬†7. One class of methods focuses on bypassing this problem by growing architectures over time as new tasks arrive¬† 5  . Under the fixed architecture setting, one can identify two main categories. The first category of methods rely on storing and re-using samples from the previous history while learning new ones; this includes approaches such as GEM¬†7 and ER¬†. The second category of methods encode the knowledge of the previous tasks in a prior that is used to regularize the training of the new task ¬†6 2   . A classic method in this vein is Learning without Forgetting (LwF)¬†6, which mitigates forgetting by a regularization term that distills knowledge¬†8 from the earlier tasks. The network representations from earlier stages are recorded, and are used during training for a new task to regularize the objective by distilling knowledge from the earlier state of the network.\nSimilarly, Elastic Weight Consolidation (EWC)¬†0 preserves the knowledge of the past tasks through a quadratic penalty on the network parameters important to the earlier tasks. The importance of the parameters is approximated via the diagonal of the Fisher information matrix¬†. The scale of the importance matrix, ŒªùúÜ\\lambda, determines the network‚Äôs preference towards preserving old representations or acquiring new ones for the current task.\nIn¬†Sec.¬†4.1 we examine the effectiveness of these approaches in mitigating representation forgetting. Recent works on elucidating the nature of catastrophic forgetting have examined the influence of task sequence¬†, network architecture¬†, and change in representation similarity¬†8. Our work is related in spirit to¬†8 as we pursue measuring how much forgetting has occurred on the learned representation and we additionally study this for intermediate representations. One significant difference is that in¬†8, the authors use linear CKA (centred Kernel Alignment)¬†3 to measure the similarity between intermediate representations influenced by forgetting, while in our work we measure how much forgetting has occurred on the representations using LP. Several recent works have also studied the behavior of networks with increasing model capacity. In 9 the authors examine several common architectures under the task incremental setting and demonstrate that pre-training is essential to combat forgetting and to achieve high performance on all tasks. They conclude that training only with larger models yields no benefit for continual learning. Our analysis revisit this setting and take a closer look at how representation forgetting is affected with increase in model width and depth. Several works¬†[ref]40 have focused on modifying the last layer of a classification network to make more effective use of the representation for prior tasks. This indirectly highlights the fact that the last layer can be modified to yield better performance on prior tasks. Particularly¬†[ref]40 0 use a buffer of old examples at training time to improve learning and then use them at evaluation time to construct a class mean prototype. This allows for more effective use of the representations of the network. These works consider settings where the CL methods are used to control forgetting, while we also emphasize that naive continuation of training under task shift can yield strong representations. Our work can also be seen as both a way to explain and to motivate the need for such approaches. Self-supervised learning (SSL) is becoming increasingly popular in visual representation learning. Some of the best performing methods rely on contrastive learning¬† 5. These methods have been recently evaluated in a limited continual learning setting¬†9 where a sequence was trained on non-iid unsupervised streaming data and then applied in transfer learning settings on multiple datasets. However, forgetting was not evaluated. In contrast our work, which also uses a SSL loss, focuses on the LP evaluation and the study of representation forgetting with respect to previously seen distributions. Contrastive methods are also often used in the supervised setting, for example, the recently popular SupCon loss¬†1.\nIn¬† and¬†0 the use of SupCon is proposed in the online class-incremental setting in combination with experience replay. Our work too considers SupCon as one of the supervised representation learning approaches. However distinct from the other works we consider it in the offline task-incremental setting. We do not look at its use in combination with replay or other approaches, but study the effect of standard finetuning with SupCon loss, distinct from¬†, where it is used to facilitate separation of contrasts between old and new classes, specific to the class incremental setting."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Memory Aware Synapses: Learning what (not) to forget",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV 2018",
      "authors": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars",
      "orig_title": "Memory aware synapses: Learning what (not) to forget",
      "paper_id": "1711.09601v4"
    },
    {
      "index": 1,
      "title": "Expert Gate: Lifelong Learning with a Network of Experts",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars",
      "orig_title": "Expert gate: Lifelong learning with a network of experts",
      "paper_id": "1611.06194v2"
    },
    {
      "index": 2,
      "title": "Does an LSTM forget more than a CNN? an empirical study of catastrophic forgetting in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "The 17th Annual Workshop of the Australasian Language Technology Association",
      "authors": "Gaurav Arora, Afshin Rahimi, and Timothy Baldwin"
    },
    {
      "index": 3,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Yoshua Bengio, Aaron Courville, and Pascal Vincent"
    },
    {
      "index": 4,
      "title": "Reducing representation drift in online continual learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.05025",
      "authors": "Lucas Caccia, Rahaf Aljundi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky"
    },
    {
      "index": 5,
      "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
      "abstract": "",
      "year": "2001",
      "venue": "Advances in neural information processing systems",
      "authors": "Rich Caruana, Steve Lawrence, and Lee Giles"
    },
    {
      "index": 6,
      "title": "Continual learning with tiny episodic memories",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.10486",
      "authors": "Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet¬†K Dokania, Philip¬†HS Torr, and Marc‚ÄôAurelio Ranzato"
    },
    {
      "index": 7,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.05709",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 8,
      "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.08819",
      "authors": "Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter",
      "orig_title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "paper_id": "1707.08819v3"
    },
    {
      "index": 9,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "MIT Press",
      "authors": "Ian Goodfellow, Yoshua Bengio, and Aaron Courville",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 10,
      "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6211",
      "authors": "Ian¬†J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio"
    },
    {
      "index": 11,
      "title": "Dissecting supervised constrastive learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt"
    },
    {
      "index": 12,
      "title": "A neural representation of sketch drawings",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.03477",
      "authors": "David Ha and Douglas Eck"
    },
    {
      "index": 13,
      "title": "Embracing change: Continual learning in deep neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Trends in cognitive sciences",
      "authors": "Raia Hadsell, Dushyant Rao, Andrei¬†A Rusu, and Razvan Pascanu"
    },
    {
      "index": 14,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.05722",
      "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 15,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 16,
      "title": "Task Agnostic Continual Learning via Meta Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.05201",
      "authors": "Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei¬†A Rusu, Yee¬†Whye Teh, and Razvan Pascanu",
      "orig_title": "Task agnostic continual learning via meta learning",
      "paper_id": "1906.05201v1"
    },
    {
      "index": 17,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 18,
      "title": "How well self-supervised pre-training performs with streaming data?",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.12081",
      "authors": "Dapeng Hu, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Alfred Shen, and Jiashi Feng"
    },
    {
      "index": 19,
      "title": "On Quadratic Penalties in Elastic Weight Consolidation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.03847",
      "authors": "Ferenc Husz√°r",
      "orig_title": "On quadratic penalties in elastic weight consolidation",
      "paper_id": "1712.03847v1"
    },
    {
      "index": 20,
      "title": "Supervised Contrastive Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan",
      "orig_title": "Supervised contrastive learning",
      "paper_id": "2004.11362v5"
    },
    {
      "index": 21,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1612.00796",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei¬†A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et¬†al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 22,
      "title": "Similarity of Neural Network Representations Revisited",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton",
      "orig_title": "Similarity of neural network representations revisited",
      "paper_id": "1905.00414v4"
    },
    {
      "index": 23,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "University of Toronto",
      "authors": "Alex Krizhevsky, Geoffrey Hinton, et¬†al."
    },
    {
      "index": 24,
      "title": "Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong",
      "orig_title": "Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting",
      "paper_id": "1904.00310v3"
    },
    {
      "index": 25,
      "title": "Learning without Forgetting",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Zhizhong Li and Derek Hoiem",
      "orig_title": "Learning without forgetting",
      "paper_id": "1606.09282v3"
    },
    {
      "index": 26,
      "title": "Gradient Episodic Memory for Continual Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Lopez-Paz et¬†al.",
      "orig_title": "Gradient episodic memory for continual learning",
      "paper_id": "1706.08840v6"
    },
    {
      "index": 27,
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1608.03983",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Sgdr: Stochastic gradient descent with warm restarts",
      "paper_id": "1608.03983v5"
    },
    {
      "index": 28,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.05101",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 29,
      "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner",
      "orig_title": "Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning",
      "paper_id": "2103.13885v3"
    },
    {
      "index": 30,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of learning and motivation",
      "authors": "Michael McCloskey and Neal¬†J Cohen"
    },
    {
      "index": 31,
      "title": "Tutorial on maximum likelihood estimation",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of mathematical Psychology",
      "authors": "In¬†Jae Myung"
    },
    {
      "index": 32,
      "title": "Toward Understanding Catastrophic Forgetting in Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.01091",
      "authors": "Cuong¬†V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto",
      "orig_title": "Toward understanding catastrophic forgetting in continual learning",
      "paper_id": "1908.01091v1"
    },
    {
      "index": 33,
      "title": "Variational Continual Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.10628",
      "authors": "Cuong¬†V Nguyen, Yingzhen Li, Thang¬†D Bui, and Richard¬†E Turner",
      "orig_title": "Variational continual learning",
      "paper_id": "1710.10628v3"
    },
    {
      "index": 34,
      "title": "Automated flower classification over a large number of classes",
      "abstract": "",
      "year": "2008",
      "venue": "Indian Conference on Computer Vision, Graphics and Image Processing",
      "authors": "M-E. Nilsback and A. Zisserman"
    },
    {
      "index": 35,
      "title": "Building a Regular Decision Boundary with Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Edouard Oyallon",
      "orig_title": "Building a regular decision boundary with deep networks",
      "paper_id": "1703.01775v1"
    },
    {
      "index": 36,
      "title": "Recognizing indoor scenes",
      "abstract": "",
      "year": "2009",
      "venue": "Computer Vision and Pattern Recognition, CVPR 2009. IEEE Conference on",
      "authors": "Ariadna Quattoni and Antonio Torralba"
    },
    {
      "index": 37,
      "title": "Anatomy of catastrophic forgetting: Hidden representations and task semantics",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.07400",
      "authors": "Vinay¬†V Ramasesh, Ethan Dyer, and Maithra Raghu"
    },
    {
      "index": 38,
      "title": "Effect of scale on catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Vinay¬†Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer"
    },
    {
      "index": 39,
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph¬†H Lampert",
      "orig_title": "icarl: Incremental classifier and representation learning",
      "paper_id": "1611.07725v2"
    },
    {
      "index": 40,
      "title": "Incremental Learning Through Deep Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Amir Rosenfeld and John¬†K Tsotsos",
      "orig_title": "Incremental learning through deep adaptation",
      "paper_id": "1705.04228v2"
    },
    {
      "index": 41,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander¬†C. Berg, and Li Fei-Fei",
      "orig_title": "ImageNet Large Scale Visual Recognition Challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 42,
      "title": "Progressive Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.04671",
      "authors": "Andrei¬†A Rusu, Neil¬†C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell",
      "orig_title": "Progressive neural networks",
      "paper_id": "1606.04671v4"
    },
    {
      "index": 43,
      "title": "The sketchy database: learning to retrieve badly drawn bunnies",
      "abstract": "",
      "year": "2016",
      "venue": "ACM Transactions on Graphics (TOG)",
      "authors": "Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays"
    },
    {
      "index": 44,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1556",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 45,
      "title": "The Caltech-UCSD Birds-200-2011 Dataset",
      "abstract": "",
      "year": "2011",
      "venue": "California Institute of Technology",
      "authors": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie"
    },
    {
      "index": 46,
      "title": "Caltech-UCSD Birds 200",
      "abstract": "",
      "year": "2010",
      "venue": "California Institute of Technology",
      "authors": "P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona"
    },
    {
      "index": 47,
      "title": "Visualizing and understanding convolutional networks",
      "abstract": "",
      "year": "2014",
      "venue": "European conference on computer vision",
      "authors": "Matthew¬†D Zeiler and Rob Fergus"
    },
    {
      "index": 48,
      "title": "Improved multitask learning through synaptic intelligence",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Friedemann Zenke, Ben Poole, and Surya Ganguli"
    }
  ]
}