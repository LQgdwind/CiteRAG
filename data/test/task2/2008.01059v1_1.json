{
  "paper_id": "2008.01059v1",
  "title": "Improving One-stage Visual Grounding by Recursive Sub-query Construction",
  "sections": {
    "introduction": "Visual grounding aims to ground a natural language query onto a region of the image. There are mainly two threads of works in visual grounding: the two-stage approach¬†[ref]41      and one-stage approach¬†[ref]47  7. Two-stage approaches first extract region proposals and then rank the proposals based on their similarities with the query. The recently proposed one-stage approach takes a different paradigm but soon becomes prevailing. The one-stage approach fuses visual-text features at image-level and directly predicts bounding boxes to ground the referred object. By densely sampling the possible object locations and reducing the redundant computation over region proposals, the one-stage methods¬†[ref]47  7 are simple, fast, and accurate. In this paper, we improve the state-of-the-art one-stage methods by addressing their weaknesses in modeling long and complex queries. The overall advantage of our method is shown in Figure¬†2. Compared to the current state-of-the-art one-stage method¬†[ref]47, whose performance drops dramatically on longer queries, our approach achieves remarkably superior performance. We analyze the limitations of current one-stage methods as follows. Existing one-stage methods¬†[ref]47  7 encode the entire query as a single embedding vector, such as directly adopting the first token‚Äôs embedding ([CLS]delimited-[]CLS\\left[\\textrm{CLS}\\right]) from BERT¬† [ref]47 or aggregating hidden states from LSTM¬† [ref]47  7. The single vector is then concatenated at all spatial locations with visual features to obtain the fused features for grounding box prediction.\nModeling the entire language query as a single embedding vector tends to increase representation ambiguity, such as focusing on some words, yet overlooking other important ones. Such a problem potentially causes the loss of referring information, especially on those long and complex queries.\nFor example in Figure¬†2 (a), the model seems to overlook detailed descriptions such as ‚Äúsitting on the couch‚Äù or ‚Äúlooking tv,‚Äù and grounds the wrong region with the head noun ‚Äúman.‚Äù As for Figure¬†2 (b), the model appears to look into the wrong word ‚Äúmountain‚Äù and grounds the target without full consideration of the modifier of ‚Äúwater.‚Äù Neglecting the query modeling problem, thus, causes the performance drop on long queries for the one-stage approach. Several two-stage visual grounding works¬†  [ref]46    have studied a similar query modeling problem. The main idea of these works is to link object regions with the parsed sub-queries to have a comprehensive understanding of the referring. Among them, MattNet¬† parses the query into the subject, location, and relationship phrases, and links each phrase with the related object regions for matching score computing. NMTREE¬† parses the query with a dependency tree parser¬† and links each tree node with a visual region. DGA¬†[ref]46 parses the query with text self-attention and links the text with regions via dynamic graph attention. Though elegant enough, these methods are designed intuitively for two-stage methods, requiring candidate region features to be extracted at the first stage. Since the main benefit of doing one-stage visual grounding is to avoid explicitly extracting candidate region features for the sake of computational cost, the query modeling in two-stage methods cannot be directly applied to the one-stage framework¬†[ref]47  7.\nTherefore, in this paper, to address the query modeling problem in a unified one-stage framework, we propose the recursive sub-query construction framework. When presented with a referring problem such as Figure¬†2 (a), humans tend to solve it by reasoning back-and-forth between the image and query for multiple rounds and recursively reduce the referring ambiguity, i.e., the possible region that contains the referred object.\nInspired by this, we proposed to represent the intermediate understanding of the referring in each round as the text-conditional visual feature, which starts as the image feature and updated after multiple rounds, ending up as the fused visual-text feature ready for box prediction. In each round, the model constructs a new sub-query as a group of words attended with scores to refine the text-conditional visual feature. Gradually, with multiple rounds, our model reduces the referring ambiguity. Such a multi-round solution is in contrast to previous one-stage approaches that try to remember the entire query and ground the region in a single round. Our framework recursively constructs sub-queries to refine the grounding prediction. Each round faces with two core problems that facilitate recursive reasoning, namely 1) how to construct the sub-query; and 2) how to refine the text-conditional visual feature with the sub-query. We propose a sub-query learner and a sub-query modulation network to solve the above two problems, respectively.\nThey work alternately and recursively to reduce the referring ambiguity. Using the text-conditional visual features in the last round, a final output stage predicts bounding boxes to grounding the referred object. We benchmark our framework on the ReferItGame¬†, RefCOCO¬†, RefCOCO+¬†, RefCOCOg¬†9 datasets, with 5.0%,4.5%,7.5%,12.8%percent5.0percent4.5percent7.5percent12.85.0\\%,4.5\\%,7.5\\%,12.8\\% absolute improvements over the state-of-the-art one-stage method¬†[ref]47.\nMeanwhile, our method runs fast at 38 FPS (26‚Äãm‚Äãs26ùëöùë†26ms).\nMoreover, the relative gain curve according to the query length changes in Figure¬†2 shows the effectiveness of our approach in solving the aforementioned query modeling problem. Our main contributions are: We improve one-stage visual grounding by addressing previous one-stage methods‚Äô limitations on grounding long and complex queries. We propose a recursive sub-query construction framework that recursively reduces the referring ambiguity with different constructed sub-queries. Our proposed method shows significantly improved results on multiple datasets and meanwhile maintains the real-time inference speed. Extensive experiments and ablations validate the effectiveness of our method."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "G3raphground: Graph-based language grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Bajaj, M., Wang, L., Sigal, L."
    },
    {
      "index": 1,
      "title": "A fast and accurate dependency parser using neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Chen, D., Manning, C.D."
    },
    {
      "index": 2,
      "title": "Msrc: Multimodal spatial regression with semantic context for phrase grounding",
      "abstract": "",
      "year": "2017",
      "venue": "2017 ACM on International Conference on Multimedia Retrieval",
      "authors": "Chen, K., Kovvuri, R., Gao, J., Nevatia, R."
    },
    {
      "index": 3,
      "title": "Query-guided Regression Network with Context Policy for Phrase Grounding",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Chen, K., Kovvuri, R., Nevatia, R.",
      "orig_title": "Query-guided regression network with context policy for phrase grounding",
      "paper_id": "1708.01676v1"
    },
    {
      "index": 4,
      "title": "Real-time referring expression comprehension by single-stage grounding network",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.03426",
      "authors": "Chen, X., Ma, L., Chen, J., Jie, Z., Liu, W., Luo, J."
    },
    {
      "index": 5,
      "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S."
    },
    {
      "index": 6,
      "title": "Modulating early visual processing by language",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "De¬†Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., Courville, A.C.",
      "orig_title": "Modulating early visual processing by language",
      "paper_id": "1707.00683v3"
    },
    {
      "index": 7,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 8,
      "title": "Neural sequential phrase grounding (seqground)",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Dogan, P., Sigal, L., Gross, M."
    },
    {
      "index": 9,
      "title": "A Learned Representation for Artistic Style",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Dumoulin, V., Shlens, J., Kudlur, M.",
      "orig_title": "A learned representation for artistic style",
      "paper_id": "1610.07629v5"
    },
    {
      "index": 10,
      "title": "The segmented and annotated iapr tc-12 benchmark",
      "abstract": "",
      "year": "2010",
      "venue": "CVIU",
      "authors": "Escalante, H.J., Hern√°ndez, C.A., Gonzalez, J.A., L√≥pez-L√≥pez, A., Montes, M., Morales, E.F., Sucar, L.E., Villase√±or, L., Grubinger, M."
    },
    {
      "index": 11,
      "title": "LSTM: A Search Space Odyssey",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "Greff, K., Srivastava, R.K., Koutn√≠k, J., Steunebrink, B.R., Schmidhuber, J.",
      "orig_title": "Lstm: A search space odyssey",
      "paper_id": "1503.04069v2"
    },
    {
      "index": 12,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "He, K., Gkioxari, G., Doll√°r, P., Girshick, R.",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 13,
      "title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Hu, R., Rohrbach, M., Andreas, J., Darrell, T., Saenko, K.",
      "orig_title": "Modeling relationships in referential expressions with compositional modular networks",
      "paper_id": "1611.09978v1"
    },
    {
      "index": 14,
      "title": "Natural language object retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Hu, R., Xu, H., Rohrbach, M., Feng, J., Saenko, K., Darrell, T."
    },
    {
      "index": 15,
      "title": "Compositional Attention Networks for Machine Reasoning",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Hudson, D.A., Manning, C.D.",
      "orig_title": "Compositional attention networks for machine reasoning",
      "paper_id": "1803.03067v2"
    },
    {
      "index": 16,
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T."
    },
    {
      "index": 17,
      "title": "Deep attribute-preserving metric learning for natural language object retrieval",
      "abstract": "",
      "year": "2017",
      "venue": "25th ACM international conference on Multimedia",
      "authors": "Li, J., Wei, Y., Liang, X., Zhao, F., Li, J., Xu, T., Feng, J."
    },
    {
      "index": 18,
      "title": "Person Search with Natural Language Description",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Li, S., Xiao, T., Li, H., Zhou, B., Yue, D., Wang, X.",
      "orig_title": "Person search with natural language description",
      "paper_id": "1702.05729v2"
    },
    {
      "index": 19,
      "title": "A real-time cross-modality correlation filtering method for referring expression comprehension",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Liao, Y., Liu, S., Li, G., Wang, F., Chen, Y., Qian, C., Li, B."
    },
    {
      "index": 20,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll√°r, P., Zitnick, C.L.",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 21,
      "title": "A Structured Self-attentive Sentence Embedding",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Lin, Z., Feng, M., Santos, C.N.d., Yu, M., Xiang, B., Zhou, B., Bengio, Y.",
      "orig_title": "A structured self-attentive sentence embedding",
      "paper_id": "1703.03130v1"
    },
    {
      "index": 22,
      "title": "Recurrent Multimodal Interaction for Referring Image Segmentation",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.",
      "orig_title": "Recurrent multimodal interaction for referring image segmentation",
      "paper_id": "1703.07939v2"
    },
    {
      "index": 23,
      "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Liu, D., Zhang, H., Wu, F., Zha, Z.J.",
      "orig_title": "Learning to assemble neural module tree networks for visual grounding",
      "paper_id": "1812.03299v3"
    },
    {
      "index": 24,
      "title": "Referring expression generation and comprehension via attributes",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Liu, J., Wang, L., Yang, M.H."
    },
    {
      "index": 25,
      "title": "SSD: Single Shot MultiBox Detector",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.",
      "orig_title": "Ssd: Single shot multibox detector",
      "paper_id": "1512.02325v5"
    },
    {
      "index": 26,
      "title": "Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Liu, X., Wang, Z., Shao, J., Wang, X., Li, H.",
      "orig_title": "Improving referring expression grounding with cross-modal attention-guided erasing",
      "paper_id": "1903.00839v2"
    },
    {
      "index": 27,
      "title": "Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Luo, G., Zhou, Y., Sun, X., Cao, L., Wu, C., Deng, C., Ji, R.",
      "orig_title": "Multi-task collaborative network for joint referring expression comprehension and segmentation",
      "paper_id": "2003.08813v1"
    },
    {
      "index": 28,
      "title": "Generation and comprehension of unambiguous object descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K."
    },
    {
      "index": 29,
      "title": "Modeling Context Between Objects for Referring Expression Understanding",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Nagaraja, V.K., Morariu, V.I., Davis, L.S.",
      "orig_title": "Modeling context between objects for referring expression understanding",
      "paper_id": "1608.00525v1"
    },
    {
      "index": 30,
      "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Perez, E., Strub, F., De¬†Vries, H., Dumoulin, V., Courville, A.",
      "orig_title": "Film: Visual reasoning with a general conditioning layer",
      "paper_id": "1709.07871v2"
    },
    {
      "index": 31,
      "title": "Conditional Image-Text Embedding Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Plummer, B.A., Kordas, P., Kiapour, M.H., Zheng, S., Piramuthu, R., Lazebnik, S.",
      "orig_title": "Conditional image-text embedding networks",
      "paper_id": "1711.08389v4"
    },
    {
      "index": 32,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2017",
      "venue": "International journal of computer vision",
      "authors": "Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 33,
      "title": "Yolov3: An incremental improvement",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.02767",
      "authors": "Redmon, J., Farhadi, A."
    },
    {
      "index": 34,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Ren, S., He, K., Girshick, R., Sun, J.",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 35,
      "title": "Grounding of textual phrases in images by reconstruction",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B."
    },
    {
      "index": 36,
      "title": "Zero-Shot Grounding of Objects from Natural Language Queries",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Sadhu, A., Chen, K., Nevatia, R.",
      "orig_title": "Zero-shot grounding of objects from natural language queries",
      "paper_id": "1908.07129v1"
    },
    {
      "index": 37,
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "abstract": "",
      "year": "2012",
      "venue": "COURSERA: Neural networks for machine learning",
      "authors": "Tieleman, T., Hinton, G."
    },
    {
      "index": 38,
      "title": "Selective search for object recognition",
      "abstract": "",
      "year": "2013",
      "venue": "International journal of computer vision",
      "authors": "Uijlings, J.R., Van De¬†Sande, K.E., Gevers, T., Smeulders, A.W."
    },
    {
      "index": 39,
      "title": "Learning two-branch neural networks for image-text matching tasks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Wang, L., Li, Y., Huang, J., Lazebnik, S."
    },
    {
      "index": 40,
      "title": "Learning deep structure-preserving image-text embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Wang, L., Li, Y., Lazebnik, S."
    },
    {
      "index": 41,
      "title": "Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Wang, P., Wu, Q., Cao, J., Shen, C., Gao, L., Hengel, A.v.d.",
      "orig_title": "Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks",
      "paper_id": "1812.04794v1"
    },
    {
      "index": 42,
      "title": "Huggingface‚Äôs transformers: State-of-the-art natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.03771",
      "authors": "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Brew, J."
    },
    {
      "index": 43,
      "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.",
      "orig_title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "paper_id": "1506.04214v2"
    },
    {
      "index": 44,
      "title": "Cross-modal relationship inference for grounding referring expressions",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Yang, S., Li, G., Yu, Y."
    },
    {
      "index": 45,
      "title": "Dynamic Graph Attention for Referring Expression Comprehension",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Yang, S., Li, G., Yu, Y.",
      "orig_title": "Dynamic graph attention for referring expression comprehension",
      "paper_id": "1909.08164v1"
    },
    {
      "index": 46,
      "title": "A Fast and Accurate One-Stage Approach to Visual Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.",
      "orig_title": "A fast and accurate one-stage approach to visual grounding",
      "paper_id": "1908.06354v1"
    },
    {
      "index": 47,
      "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.",
      "orig_title": "Mattnet: Modular attention network for referring expression comprehension",
      "paper_id": "1801.08186v3"
    },
    {
      "index": 48,
      "title": "Modeling context in referring expressions",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L."
    },
    {
      "index": 49,
      "title": "A joint speaker-listener-reinforcer model for referring expressions",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Yu, L., Tan, H., Bansal, M., Berg, T.L."
    },
    {
      "index": 50,
      "title": "Grounding Referring Expressions in Images by Variational Context",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhang, H., Niu, Y., Chang, S.F.",
      "orig_title": "Grounding referring expressions in images by variational context",
      "paper_id": "1712.01892v2"
    },
    {
      "index": 51,
      "title": "Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhuang, B., Wu, Q., Shen, C., Reid, I., van¬†den Hengel, A.",
      "orig_title": "Parallel attention: A unified framework for visual object discovery through dialogs and queries",
      "paper_id": "1711.06370v1"
    },
    {
      "index": 52,
      "title": "Edge boxes: Locating object proposals from edges",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Zitnick, C.L., Doll√°r, P."
    }
  ]
}