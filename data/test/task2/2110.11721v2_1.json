{
  "paper_id": "2110.11721v2",
  "title": "Projection-Free Stochastic Bi-level Optimization",
  "sections": {
    "i-b related work": "We review some of the related work in the context of\nbi-level optimization, compositional optimization, and projection-free algorithms. Bi-level optimization has had a long history, with the earliest applications in economic game theory . Bi-level optimization has recently received great attention from the machine learning community due to the number of applications in the area . A series of works that proposed to solve the problem of the form (1) has appeared recently [ref]7  [ref]19 [ref]9   . Of these, the seminal works in [ref]7  proposed a class of double-loop approximation algorithms to iteratively approximate the stochastic gradient of the outer objective and incurred a sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}) in order to achieve the œµitalic-œµ\\epsilon-stationary point. The double loop structure of these approaches made them impractical for large-scale problems; [ref]7 required solving an inner optimization problem to a predefined accuracy, while  required a large batch size of ùí™‚Äã(œµ‚àí1)ùí™superscriptitalic-œµ1\\mathcal{O}(\\epsilon^{-1}) at each iteration. To address this issue, various single-loop methods, involving simultaneous update of inner and outer optimization variables, have been developed [ref]19 [ref]9  . A single-loop two-time scale stochastic algorithm proposed in [ref]19 incurred a sub-optimal sample complexity of ùí™‚Äã(œµ‚àí2.5)ùí™superscriptitalic-œµ2.5\\mathcal{O}(\\epsilon^{-2.5}). This is further improved recently in [ref]9  , in which the authors have utilized the momentum-based variance reduction technique from  to obtain optimal convergence rates. While all of the above-mentioned works seek to solve (1), they are projection-based and require a projection on to ùí≥ùí≥{{\\mathcal{X}}} at every iteration. In this work, we are interested in developing projection-free stochastic optimization algorithms for bi-level problems, which is still an open problem and the subject of the work in this paper. Compositional optimization problems have been recently studied and various algorithms have been proposed in       . The seminal work in  proposed a quasi-gradient method called stochastic compositional gradient descent (CGD) to solve the problem via a two time-scale approach. In , the authors proposed an accelerated SCGD method that achieved an improved sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}) for convex objectives and ùí™‚Äã(œµ‚àí2.25)ùí™superscriptitalic-œµ2.25\\mathcal{O}(\\epsilon^{-2.25}) for non-convex objectives. Further, different variance-reduced SCGD variants have been proposed, such as SCVR , VRSC-PG , SARAH-Compositional  , and STORM-Compositional . In the literature, we can also find some single time-scale algorithms to solve compositional problems  . Work in  presented a nested averaged stochastic approximation (NASA) and proved a sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}). Recently, another single time-scale algorithm called the stochastically corrected stochastic compositional gradient method (SCSC) is proposed in  that converges at the same rate as the SGD methods for non-compositional stochastic optimization. It further adopted the Adam-type adaptive gradient approach and achieved the optimal sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}). Again, all the above-mentioned algorithms either solve an unconstrained problem or use projection operation at each iteration to deal with the constraints. In this work, we developed a projection-free algorithm for compositional problems as well. Note that even-though compositional problems are a special case of bi-level problems, we have studied them separately in this work and proposed a novel projection-free algorithm specifically for compositional problems to achieve the optimal sample complexity. Projection-free algorithms have been extensively studied to solve the single-level optimization problems of the form (1) in the literature   . A number of first-order projection-free algorithms have been developed for stochastic optimization problems as well     . The stochastic FW method proposed in  achieves a sample complexity of ùí™‚Äã(œµ‚àí3)ùí™superscriptitalic-œµ3\\mathcal{O}(\\epsilon^{-3}) but requires an the batch size b=ùí™‚Äã(t)ùëèùí™ùë°b=\\mathcal{O}(t), where tùë°t is the iteration index. The need for increasing batch sizes was dropped in , which worked with a standard mini-batch but still achieved the same sample complexity. Finally, an improved stochastic recursive gradient estimator-based algorithm called ORGFW was proposed in  and achieved a sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}). For non-convex problems,  proposed an approach where the batch-size depends on the total number of iterations, resulting in a sample complexity of ùí™‚Äã(œµ‚àí4)ùí™superscriptitalic-œµ4\\mathcal{O}(\\epsilon^{-4}). Later, work in  came up with a two-sample strategy and achieved ùí™‚Äã(œµ‚àí3)ùí™superscriptitalic-œµ3\\mathcal{O}(\\epsilon^{-3}) sample complexity. We remark that the idea of projection-free algorithms is limited only to single-level optimization problems in the existing literature. Therefore, there are no corresponding oracle complexity bounds for projection-free algorithms in bilevel settings, and our work fills this crucial gap. We present our main contributions as follows."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Meta-learning with implicit gradients",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.04630",
      "authors": "A. Rajeswaran, C. Finn, S. Kakade, and S. Levine"
    },
    {
      "index": 1,
      "title": "Coresets via Bilevel Optimization for Continual Learning and Streaming",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.03875",
      "authors": "Z. Borsos, M. Mutn·ª≥, and A. Krause",
      "orig_title": "Coresets via bilevel optimization for continual learning and streaming",
      "paper_id": "2006.03875v2"
    },
    {
      "index": 2,
      "title": "Bi-level actor-critic for multi-agent coordination",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "H. Zhang, W. Chen, Z. Huang, M. Li, Y. Yang, W. Zhang, and J. Wang"
    },
    {
      "index": 3,
      "title": "On the Iteration Complexity of Hypergradient Computation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo",
      "orig_title": "On the iteration complexity of hypergradient computation",
      "paper_id": "2006.16218v2"
    },
    {
      "index": 4,
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil",
      "orig_title": "Bilevel programming for hyperparameter optimization and meta-learning",
      "paper_id": "1806.04910v2"
    },
    {
      "index": 5,
      "title": "Bilevel Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "S. Dempe and A. Zemkoho"
    },
    {
      "index": 6,
      "title": "Approximation methods for bilevel programming",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.02246",
      "authors": "S. Ghadimi and M. Wang"
    },
    {
      "index": 7,
      "title": "Provably Faster Algorithms for Bilevel Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.04692",
      "authors": "J. Yang, K. Ji, and Y. Liang",
      "orig_title": "Provably faster algorithms for bilevel optimization",
      "paper_id": "2106.04692v2"
    },
    {
      "index": 8,
      "title": "A single-timescale stochastic bilevel optimization method",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.04671",
      "authors": "T. Chen, Y. Sun, and W. Yin"
    },
    {
      "index": 9,
      "title": "Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Mokhtari, H. Hassani, and A. Karbasi",
      "orig_title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
      "paper_id": "1804.09554v2"
    },
    {
      "index": 10,
      "title": "Efficient projection-free online methods with stochastic recursive gradient.",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "J. Xie, Z. Shen, C. Zhang, B. Wang, and H. Qian"
    },
    {
      "index": 11,
      "title": "One Sample Stochastic Frank-Wolfe",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "M. Zhang, Z. Shen, A. Mokhtari, H. Hassani, and A. Karbasi",
      "orig_title": "One sample stochastic frank-wolfe",
      "paper_id": "1910.04322v1"
    },
    {
      "index": 12,
      "title": "Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions",
      "abstract": "",
      "year": "2017",
      "venue": "Mathematical Programming",
      "authors": "M. Wang, E. X. Fang, and H. Liu"
    },
    {
      "index": 13,
      "title": "Accelerating stochastic composition optimization",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Machine Learning Research",
      "authors": "M. Wang, J. Liu, and E. X. Fang"
    },
    {
      "index": 14,
      "title": "A single timescale stochastic approximation method for nested stochastic optimization",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Optimization",
      "authors": "S. Ghadimi, A. Ruszczynski, and M. Wang"
    },
    {
      "index": 15,
      "title": "Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.10847",
      "authors": "T. Chen, Y. Sun, and W. Yin"
    },
    {
      "index": 16,
      "title": "Bilevel optimization: Nonasymptotic analysis and faster algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.07962",
      "authors": "K. Ji, J. Yang, and Y. Liang"
    },
    {
      "index": 17,
      "title": "A momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv e-prints",
      "authors": "P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang"
    },
    {
      "index": 18,
      "title": "A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.05170",
      "authors": "M. Hong, H.-T. Wai, Z. Wang, and Z. Yang"
    },
    {
      "index": 19,
      "title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.07367",
      "authors": "P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang",
      "orig_title": "A near-optimal algorithm for stochastic bilevel optimization via double-momentum",
      "paper_id": "2102.07367v3"
    },
    {
      "index": 20,
      "title": "Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.13781",
      "authors": "T. Chen, Y. Sun, and W. Yin",
      "orig_title": "Tighter analysis of alternating stochastic gradient method for stochastic nested problems",
      "paper_id": "2106.13781v1"
    },
    {
      "index": 21,
      "title": "Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Fallah, A. Mokhtari, and A. Ozdaglar"
    },
    {
      "index": 22,
      "title": "Revisiting frank-wolfe: Projection-free sparse convex optimization",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "M. Jaggi"
    },
    {
      "index": 23,
      "title": "An algorithm for quadratic programming",
      "abstract": "",
      "year": "1956",
      "venue": "Naval research logistics quarterly",
      "authors": "M. Frank, P. Wolfe et al."
    },
    {
      "index": 24,
      "title": "Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Z. Akhtar and K. Rajawat",
      "orig_title": "Zeroth and first order stochastic frank-wolfe algorithms for constrained optimization",
      "paper_id": "2107.06534v2"
    },
    {
      "index": 25,
      "title": "Enhanced Bilevel Optimization via Bregman Distance",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.12301",
      "authors": "F. Huang and H. Huang",
      "orig_title": "Enhanced bilevel optimization via bregman distance",
      "paper_id": "2107.12301v3"
    },
    {
      "index": 26,
      "title": "Sparse model-agnostic meta-learning algorithm for few-shot learning",
      "abstract": "",
      "year": "2019",
      "venue": "2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)",
      "authors": "S. Gai and D. Wang"
    },
    {
      "index": 27,
      "title": "Bilevel methods for image reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.09610",
      "authors": "C. Crockett and J. A. Fessler"
    },
    {
      "index": 28,
      "title": "Low-rank matrix completion using alternating minimization",
      "abstract": "",
      "year": "2013",
      "venue": "ACM symposium on Theory of computing",
      "authors": "P. Jain, P. Netrapalli, and S. Sanghavi"
    },
    {
      "index": 29,
      "title": "Low-rank quaternion approximation for color image processing",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "authors": "Y. Chen, X. Xiao, and Y. Zhou"
    },
    {
      "index": 30,
      "title": "Multi-task feature learning",
      "abstract": "",
      "year": "2007",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Evgeniou and M. Pontil"
    },
    {
      "index": 31,
      "title": "Transfer learning for collaborative filtering via a rating-matrix generative model",
      "abstract": "",
      "year": "2009",
      "venue": "26th annual international conference on machine learning",
      "authors": "B. Li, Q. Yang, and X. Xue"
    },
    {
      "index": 32,
      "title": "Simultaneous visual data completion and denoising based on tensor rank and total variation minimization and its primal-dual splitting algorithm",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "T. Yokota and H. Hontani"
    },
    {
      "index": 33,
      "title": "Low-rank matrix completion and denoising under Poisson noise",
      "abstract": "",
      "year": "2021",
      "venue": "Information and Inference: A Journal of the IMA",
      "authors": "A. D. McRae and M. A. Davenport",
      "orig_title": "Low-rank matrix completion and denoising under poisson noise",
      "paper_id": "1907.05325v2"
    },
    {
      "index": 34,
      "title": "Robust video denoising using low rank matrix completion",
      "abstract": "",
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "authors": "H. Ji, C. Liu, Z. Shen, and Y. Xu"
    },
    {
      "index": 35,
      "title": "Matrix completion from noisy entries",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in neural information processing systems",
      "authors": "R. Keshavan, A. Montanari, and S. Oh"
    },
    {
      "index": 36,
      "title": "Supervised learning of sparsity-promoting regularizers for denoising",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.05521",
      "authors": "M. T. McCann and S. Ravishankar"
    },
    {
      "index": 37,
      "title": "Motivating bilevel approaches to filter learning: A case study",
      "abstract": "",
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing (ICIP)",
      "authors": "C. Crockett and J. A. Fessler"
    },
    {
      "index": 38,
      "title": "The theory of the market economy",
      "abstract": "",
      "year": "1952",
      "venue": "Oxford University Press",
      "authors": "H. Von Stackelberg and S. H. Von"
    },
    {
      "index": 39,
      "title": "Bilevel optimization and machine learning",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE World Congress on Computational Intelligence",
      "authors": "K. P. Bennett, G. Kunapuli, J. Hu, and J.-S. Pang"
    },
    {
      "index": 40,
      "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.11396",
      "authors": "F. Huang and H. Huang",
      "orig_title": "Biadam: Fast adaptive bilevel optimization methods",
      "paper_id": "2106.11396v4"
    },
    {
      "index": 41,
      "title": "Momentum-Based Variance Reduction in Non-Convex SGD",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.10018",
      "authors": "A. Cutkosky and F. Orabona",
      "orig_title": "Momentum-based variance reduction in non-convex sgd",
      "paper_id": "1905.10018v3"
    },
    {
      "index": 42,
      "title": "Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.13515",
      "authors": "H. Yuan, X. Lian, and J. Liu"
    },
    {
      "index": 43,
      "title": "Stochastic recursive momentum method for non-convex compositional optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.01688",
      "authors": "J. Yang and W. Hu"
    },
    {
      "index": 44,
      "title": "Stochastic Compositional Gradient Descent under Compositional Csonstraints",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.09400",
      "authors": "S. T. Thomdapu, K. Rajawat et al.",
      "orig_title": "Stochastic compositional gradient descent under compositional constraints",
      "paper_id": "2012.09400v4"
    },
    {
      "index": 45,
      "title": "Variance reduced methods for non-convex composition optimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.04416",
      "authors": "L. Liu, J. Liu, and D. Tao"
    },
    {
      "index": 46,
      "title": "Accelerated Method for Stochastic Composition Optimization with Nonsmooth Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Z. Huo, B. Gu, J. Liu, and H. Huang",
      "orig_title": "Accelerated method for stochastic composition optimization with nonsmooth regularization",
      "paper_id": "1711.03937v2"
    },
    {
      "index": 47,
      "title": "Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "H. Yuan, X. Lian, C. J. Li, and J. Liu"
    },
    {
      "index": 48,
      "title": "Robust stochastic approximation approach to stochastic programming",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on optimization",
      "authors": "A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro"
    },
    {
      "index": 49,
      "title": "Stochastic proximal gradient descent with acceleration techniques",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Nitanda"
    },
    {
      "index": 50,
      "title": "An optimal method for stochastic composite optimization",
      "abstract": "",
      "year": "2012",
      "venue": "Mathematical Programming",
      "authors": "G. Lan"
    },
    {
      "index": 51,
      "title": "Variance-Reduced and Projection-Free Stochastic Optimization",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "E. Hazan and H. Luo",
      "orig_title": "Variance-reduced and projection-free stochastic optimization",
      "paper_id": "1602.02101v2"
    },
    {
      "index": 52,
      "title": "Stochastic frank-wolfe methods for nonconvex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",
      "authors": "S. J. Reddi, S. Sra, B. P√≥czos, and A. Smola"
    },
    {
      "index": 53,
      "title": "Conservative Stochastic Optimization with Expectation Constraints",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Z. Akhtar, A. S. Bedi, and K. Rajawat",
      "orig_title": "Conservative stochastic optimization with expectation constraints",
      "paper_id": "2008.05758v2"
    },
    {
      "index": 54,
      "title": "Stochastic conditional gradient++:(non) convex minimization and continuous submodular maximization",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Optimization",
      "authors": "H. Hassani, A. Karbasi, A. Mokhtari, and Z. Shen"
    },
    {
      "index": 55,
      "title": "A stochastic compositional gradient method using markov samples",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Winter Simulation Conference (WSC)",
      "authors": "M. Wang and J. Liu"
    },
    {
      "index": 56,
      "title": "First-order methods in optimization",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM",
      "authors": "A. Beck"
    },
    {
      "index": 57,
      "title": "Convergence Rate of Frank-Wolfe for Non-Convex Objectives",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1607.00345",
      "authors": "S. Lacoste-Julien",
      "orig_title": "Convergence rate of frank-wolfe for non-convex objectives",
      "paper_id": "1607.00345v1"
    },
    {
      "index": 58,
      "title": "A stochastic composite gradient method with incremental variance reduction",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.10186",
      "authors": "J. Zhang and L. Xiao"
    },
    {
      "index": 59,
      "title": "Investigating Practical Linear Temporal Difference Learning ‚Äã‚Äã‚Äã",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1602.08771",
      "authors": "A. White and M. White",
      "orig_title": "Investigating practical linear temporal difference learning",
      "paper_id": "1602.08771v2"
    }
  ]
}