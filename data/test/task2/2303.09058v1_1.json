{
  "paper_id": "2303.09058v1",
  "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning",
  "sections": {
    "ii related work": "Recent work in MARL has shown a diversified development with the progress of deep learning, and has gradually moved from the tabular methods  to various deep neural networks .\nOur method is based on the cooperative value-decomposition algorithm, which is related to the recent advances in CTDE .\nSpecifically, our method takes advantage of the efficiency and scalability of CTDE to improve the performance of cooperative MARL in large-scale environments. In MARL, one challenge is to learn effective policies in a decentralized environment.\nPrior works have explored different approaches to this problem.\nOn the one hand, some methods directly learn decentralized value functions or policies, which are simple and intuitive.\nIndependent Q-learning (IQL)  [ref]30 integrates q-learning into multi-agent systems, learning a separate value function for each agent and enabling fully decentralized learning.\nHowever, these methods can suffer from environmental non-stationary problems during the training process, induced by the policy update and exploration of each agent simultaneously.\nStabilising experience replay  addresses learning stabilization under the decentralized paradigm, but still cannot completely solve this problem.\nOn the other hand, centralized methods can handle non-stationary problems at the cost of scalability.\nCoordinated reinforcement learning  exploits the conditional independence between agents to decompose the global reward into the sum of each agent’s local rewards.\nSparse cooperative Q-learning [ref]33 uses state sparsity to tabulate the state, allowing the use of tabular Q-learning.\nThese methods have shown promising results in various applications, but may have limitations in scalability or complexity. A common method is to balance the benefits of centralized and decentralized learning\nOne compromise is to use global information for centralized training and decentralized execution.\nCOMA  is a policy-based MARL algorithm in Counterfactual Multi-Agent Decision Making (CMDM) to estimate the counterfactual baseline through a centralized critic.\nSimilarly, MARS  is easier to scale to more agents but reduces the centralization advantage.\nHowever, these methods use on-policy policy gradient learning, which can be data-inefficient and prone to suboptimal results.\nExisting distributed reinforcement learning (RL) frameworks such as IMPALA  and SEED  only support SARL algorithms, while Ray , TLeague , and MAVA  not only have high requirements for computing resources but are not fully adapted to cooperative MARL. Recently, value-based methods that are intermediate between IQL and COMA have achieved great success in complex multi-agent tasks .\nThese methods decompose the joint action-value function into functions of independent action-value functions by satisfying the constraints of Individual-Global-Max (IGM) consistency [ref]22.\nVDN , QMIX , and WQMIX  enforce additivity, monotonicity, and weighted monotonicity constraints, respectively.\nQatten  uses both linearity and monotonicity constraints.\nQTRAN [ref]22 and Qplex  completely realize the expression ability of value-decomposition by converting IGM to optimization constraints and by duplex dueling architecture, respectively, but they are found to have high computational complexity in experiments.\nHowever, none of the above methods can help agents explore high-dimensional multi-agent systems efficiently, which limits their performance. While some works has been made in multi-agent reinforcement learning (MARL) exploration, it is still in its early stages due to the difficulty of exploring the joint state-action space.\nSeveral methods have been proposed to address this issue.\nEUIR  introduces a centralized agent with intrinsic rewards to interact with the environment, and stores experience in a shared replay buffer to update policies.\nCEIR  defines several types of intrinsic rewards by combining the decentralized curiosity of each agent.\nLIIR  learns an additional proxy critic for each agent to learn the individual intrinsic reward and uses it to update the policy.\nHowever, these types of intrinsic rewards are domain-specific and cannot be extended to other scenarios.\nMAVEN  leverages a hierarchical policy to control shared latent variables as a signal for coordinated exploration patterns, but it still has limitations in some scenarios such as reward sparse environments. In this paper, we study how to solve the problems of low data-efficiency and insufficient exploration in the value-decomposition method."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Multitask learning and reinforcement learning for personalized dialog generation: An empirical study",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "M. Yang, W. Huang, W. Tu, Q. Qu, Y. Shen, and K. Lei"
    },
    {
      "index": 1,
      "title": "Learning automata-based multiagent reinforcement learning for optimization of cooperative tasks",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems",
      "authors": "Z. Zhang, D. Wang, and J. Gao"
    },
    {
      "index": 2,
      "title": "MASER: Multi-Agent Reinforcement Learning with Subgoals Generated from Experience Replay Buffer",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Jeon, W. Kim, W. Jung, and Y. Sung",
      "orig_title": "Maser: Multi-agent reinforcement learning with subgoals generated from experience replay buffer",
      "paper_id": "2206.10607v1"
    },
    {
      "index": 3,
      "title": "Intrinsic motivated multi-agent communication",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "C. Sun, B. Wu, R. Wang, X. Hu, X. Yang, and C. Cong"
    },
    {
      "index": 4,
      "title": "Minimax-optimal multi-agent rl in zero-sum markov games with a generative model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.10458",
      "authors": "G. Li, Y. Chi, Y. Wei, and Y. Chen"
    },
    {
      "index": 5,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 6,
      "title": "Learning Individually Inferred Communication for Multi-Agent Cooperation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Z. Ding, T. Huang, and Z. Lu",
      "orig_title": "Learning individually inferred communication for multi-agent cooperation",
      "paper_id": "2006.06455v2"
    },
    {
      "index": 7,
      "title": "Graph Convolutional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.09202",
      "authors": "J. Jiang, C. Dun, T. Huang, and Z. Lu",
      "orig_title": "Graph convolutional reinforcement learning",
      "paper_id": "1810.09202v5"
    },
    {
      "index": 8,
      "title": "Distributed multiagent reinforcement learning with action networks for dynamic economic dispatch",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "C. Hu, G. Wen, S. Wang, J. Fu, and W. Yu"
    },
    {
      "index": 9,
      "title": "Exploration with task information for meta reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "P. Jiang, S. Song, and G. Huang"
    },
    {
      "index": 10,
      "title": "Feudal latent space exploration for coordinated multi-agent reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "X. Liu and Y. Tan"
    },
    {
      "index": 11,
      "title": "Understanding via exploration: Discovery of interpretable features with deep reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. Wei, Z. Qiu, F. Wang, W. Lin, N. Gui, and W. Gui"
    },
    {
      "index": 12,
      "title": "Exploration in deep reinforcement learning: a comprehensive survey",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.06668",
      "authors": "T. Yang, H. Tang, C. Bai, J. Liu, J. Hao, Z. Meng, P. Liu, and Z. Wang"
    },
    {
      "index": 13,
      "title": "First return, then explore",
      "abstract": "",
      "year": "2021",
      "venue": "Nature",
      "authors": "A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune"
    },
    {
      "index": 14,
      "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multi-Agent Domain",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and Z. Wang",
      "orig_title": "Exploration in deep reinforcement learning: From single-agent to multiagent domain",
      "paper_id": "2109.06668v6"
    },
    {
      "index": 15,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al.",
      "orig_title": "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 16,
      "title": "SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.06591",
      "authors": "L. Espeholt, R. Marinier, P. Stanczyk, K. Wang, and M. Michalski",
      "orig_title": "Seed rl: Scalable and efficient deep-rl with accelerated central inference",
      "paper_id": "1910.06591v2"
    },
    {
      "index": 17,
      "title": "Ray: A distributed framework for emerging {{\\{AI}}} applications",
      "abstract": "",
      "year": "2018",
      "venue": "13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)",
      "authors": "P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, et al."
    },
    {
      "index": 18,
      "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.05296",
      "authors": "P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.",
      "orig_title": "Value-decomposition networks for cooperative multi-agent learning",
      "paper_id": "1706.05296v1"
    },
    {
      "index": 19,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 20,
      "title": "Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "T. Rashid, G. Farquhar, B. Peng, and S. Whiteson",
      "orig_title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "2006.10800v2"
    },
    {
      "index": 21,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 22,
      "title": "MAVEN: Multi-Agent Variational Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson",
      "orig_title": "Maven: Multi-agent variational exploration",
      "paper_id": "1910.07483v2"
    },
    {
      "index": 23,
      "title": "Parallel data processing with mapreduce: a survey",
      "abstract": "",
      "year": "2012",
      "venue": "AcM sIGMoD record",
      "authors": "K.-H. Lee, Y.-J. Lee, H. Choi, Y. D. Chung, and B. Moon"
    },
    {
      "index": 24,
      "title": "Liir: Learning individual intrinsic reward in multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Du, L. Han, M. Fang, J. Liu, T. Dai, and D. Tao"
    },
    {
      "index": 25,
      "title": "Exploration by Random Network Distillation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.12894",
      "authors": "Y. Burda, H. Edwards, A. Storkey, and O. Klimov",
      "orig_title": "Exploration by random network distillation",
      "paper_id": "1810.12894v1"
    },
    {
      "index": 26,
      "title": "Dynamic analysis of multiagent q-learning with ε𝜀\\varepsilon-greedy exploration",
      "abstract": "",
      "year": "2009",
      "venue": "26th annual international conference on machine learning",
      "authors": "E. Rodrigues Gomes and R. Kowalczyk"
    },
    {
      "index": 27,
      "title": "A survey and critique of multiagent deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Autonomous Agents and Multi-Agent Systems",
      "authors": "P. Hernandez-Leal, B. Kartal, and M. E. Taylor"
    },
    {
      "index": 28,
      "title": "Multi-agent reinforcement learning: Independent vs. cooperative agents",
      "abstract": "",
      "year": "1993",
      "venue": "tenth international conference on machine learning",
      "authors": "M. Tan"
    },
    {
      "index": 29,
      "title": "Multiagent cooperation and competition with deep reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "PloS one",
      "authors": "A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente"
    },
    {
      "index": 30,
      "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson",
      "orig_title": "Stabilising experience replay for deep multi-agent reinforcement learning",
      "paper_id": "1702.08887v3"
    },
    {
      "index": 31,
      "title": "Coordinated reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "ICML",
      "authors": "C. Guestrin, M. Lagoudakis, and R. Parr"
    },
    {
      "index": 32,
      "title": "Collaborative multiagent reinforcement learning by payoff propagation",
      "abstract": "",
      "year": "2006",
      "venue": "Journal of Machine Learning Research",
      "authors": "J. R. Kok and N. Vlassis"
    },
    {
      "index": 33,
      "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1702.03037",
      "authors": "J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel",
      "orig_title": "Multi-agent reinforcement learning in sequential social dilemmas",
      "paper_id": "1702.03037v1"
    },
    {
      "index": 34,
      "title": "TLeague: A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.12895",
      "authors": "P. Sun, J. Xiong, L. Han, X. Sun, S. Li, J. Xu, M. Fang, and Z. Zhang",
      "orig_title": "Tleague: A framework for competitive self-play based distributed multi-agent reinforcement learning",
      "paper_id": "2011.12895v2"
    },
    {
      "index": 35,
      "title": "Mava: A research framework for distributed multi-agent reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.01460",
      "authors": "A. Pretorius, K. ab Tessera, A. P. Smit, K. Eloff, C. Formanek, S. J. Grimbly, S. Danisa, L. Francis, J. Shock, H. Kamper, W. Brink, H. Engelbrecht, A. Laterre, and K. Beguir"
    },
    {
      "index": 36,
      "title": "Learning automata-based multiagent reinforcement learning for optimization of cooperative tasks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Z. Zhang, D. Wang, and J. Gao"
    },
    {
      "index": 37,
      "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.03939",
      "authors": "Y. Yang, J. Hao, B. Liao, K. Shao, G. Chen, W. Liu, and H. Tang",
      "orig_title": "Qatten: A general framework for cooperative multiagent reinforcement learning",
      "paper_id": "2002.03939v2"
    },
    {
      "index": 38,
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.01062",
      "authors": "J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang",
      "orig_title": "Qplex: Duplex dueling multi-agent q-learning",
      "paper_id": "2008.01062v3"
    },
    {
      "index": 39,
      "title": "Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.02138",
      "authors": "W. Böhmer, T. Rashid, and S. Whiteson",
      "orig_title": "Exploration with unreliable intrinsic reward in multi-agent reinforcement learning",
      "paper_id": "1906.02138v1"
    },
    {
      "index": 40,
      "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.12127",
      "authors": "S. Iqbal and F. Sha",
      "orig_title": "Coordinated exploration via intrinsic rewards for multi-agent reinforcement learning",
      "paper_id": "1905.12127v3"
    },
    {
      "index": 41,
      "title": "A concise introduction to decentralized POMDPs",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "F. A. Oliehoek and C. Amato"
    },
    {
      "index": 42,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "nature",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al."
    },
    {
      "index": 43,
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell",
      "orig_title": "Curiosity-driven exploration by self-supervised prediction",
      "paper_id": "1705.05363v1"
    },
    {
      "index": 44,
      "title": "Count-Based Exploration with Neural Density Models",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos",
      "orig_title": "Count-based exploration with neural density models",
      "paper_id": "1703.01310v2"
    },
    {
      "index": 45,
      "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
      "abstract": "",
      "year": "2015",
      "venue": "2015 aaai fall symposium series",
      "authors": "M. Hausknecht and P. Stone",
      "orig_title": "Deep recurrent q-learning for partially observable mdps",
      "paper_id": "1507.06527v4"
    },
    {
      "index": 46,
      "title": "Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic Environments",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Control Systems Letters",
      "authors": "H. Chen and C. Liu",
      "orig_title": "Safe and sample-efficient reinforcement learning for clustered dynamic environments",
      "paper_id": "2303.14265v1"
    },
    {
      "index": 47,
      "title": "Self-adaptive priority correction for prioritized experience replay",
      "abstract": "",
      "year": "2020",
      "venue": "Applied Sciences",
      "authors": "H. Zhang, C. Qu, J. Zhang, and J. Li"
    },
    {
      "index": 48,
      "title": "The StarCraft Multi-Agent Challenge",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.04043",
      "authors": "M. Samvelyan, T. Rashid, C. S. De Witt, G. Farquhar, N. Nardelli, T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, and S. Whiteson",
      "orig_title": "The starcraft multi-agent challenge",
      "paper_id": "1902.04043v5"
    }
  ]
}