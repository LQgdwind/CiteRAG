{
  "paper_id": "2210.01628v2",
  "title": "Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization",
  "sections": {
    "synthetic functions": "We use Hartmann (d=6𝑑6d=6) and Levy (d=10𝑑10d=10) as the synthetic benchmark functions, and extend them to high dimensions by adding unrelated variables as   . For example, Hartmann666_300300300 has the dimension D=300𝐷300D=300, and is generated by appending 294294294 unrelated dimensions to Hartmann. The variables affecting the value of f𝑓f are called valid variables. Effectiveness of Variable Selection. Dropout  is the previous variable selection method which randomly selects d𝑑d variables in each iteration, while our proposed MCTS-VS applies MCTS to automatically select important variables. We compare them against vanilla BO [ref]32 without variable selection. The first two subfigures in Figure 1 show that Dropout-BO and MCTS-VS-BO are better than vanilla BO, implying the effectiveness of variable selection. We can also see that MCTS-VS-BO performs the best, implying the superiority of MCTS-based variable selection over random selection. We also equip MCTS-VS and Dropout with the advanced BO algorithm TuRBO , resulting in MCTS-VS-TuRBO and Dropout-TuRBO. The last two subfigures in Figure 1 show the similar results except that MCTS-VS-TuRBO needs more evaluations to be better than Dropout-TuRBO. This is because TuRBO costs more evaluations than BO on the same selected variables, and thus needs more evaluations to generate sufficient samples for an accurate estimation of the variable score in Eq. (2). Comparison with State-of-The-Art Methods. We compare MCTS-VS with the state-of-the-art methods, including TuRBO , LA-MCTS-TuRBO , SAASBO [ref]9, HeSBO , ALEBO  and CMA-ES [ref]14. TuRBO fits a collection of local models to optimize in the trust regions for overcoming the homogeneity of the global model and over-exploration. LA-MCTS-TuRBO applies MCTS to partition the search space and uses TuRBO to optimize in a small sub-region. SAASBO uses sparsity-inducing prior to select variables implicitly. HeSBO and ALEBO are state-of-the-art embedding methods. CMA-ES is a popular evolutionary algorithm. We also implement VAE-BO by combining VAE  with vanilla BO directly, as a baseline of learning-based embedding. For MCTS-VS, we implement the two versions of MCTS-VS-BO and MCTS-VS-TuRBO, i.e., MCTS-VS equipped with vanilla BO and TuRBO. As shown in Figure 2, MCTS-VS can achieve the best performance except on Levy101010_100100100, where it is a little worse than TuRBO. For low-dimensional functions (e.g., D=100𝐷100D=100 for Levy101010_100100100), TuRBO can adjust the trust region quickly while MCTS-VS needs samples to estimate the variable score. But as the dimension increases, the search space increases exponentially and it becomes difficult for TuRBO to adjust the trust region; while the number of variables only increases linearly, making MCTS-VS more scalable. SAASBO has similar performance to MCTS-VS due to the advantage of sparsity-inducing prior. HeSBO is not stable, which has a moderate performance on Hartmann but a relatively good performance on Levy. Note that we only run SAASBO and ALEBO for 200200200 evaluations on Hartmann functions because it has already taken more than hours to finish one iteration when the number of samples is large. More details about runtime are shown in Table 1. VAE-BO has the worst performance, suggesting that the learning algorithm in high-dimensional BO needs to be designed carefully. We also conduct experiments on extremely low and high dimensional variants of Hartmann (i.e., Hartmann666_100100100 and Hartmann666_100010001000), showing that MCTS-VS still performs well, and perform the significance test by running each method more times. Please see Appendix E. Next, we compare the practical running overheads of these methods. We run each method for 100100100 evaluations independently using 303030 different random seeds, and calculate the average wall clock time. The results are shown in Table 1. As expected, when using variable selection (i.e., Dropout and MCTS-VS), the time is less than that of Vanilla BO or TuRBO, because we only optimize a subset of variables. MCTS-VS is a little slower than Dropout, which is because MCTS-VS needs to build the search tree and calculate the variable score, while Dropout only randomly selects variables. MCTS-VS is much faster than LA-MCTS-TuRBO, showing the advantage of partitioning the variables to partitioning the search space. SAASBO optimizes all variables instead of only a subset of variables and uses No-U-Turn sampler (NUTS) to inference, consuming ×500​ –×1000absent500 –1000\\times 500\\text{ --}\\times 1000 time. HeSBO and ALEBO consume ×10​ –×500absent10 –500\\times 10\\text{ --}\\times 500 time compared with the variable selection methods. CMA-ES is very fast because it does not need to fit a GP model or optimize an acquisition function. The reasons for the small running overhead of MCTS-VS can be summarized as follows: 1) it only optimizes a selected subset of variables; 2) the depth of the search tree is shallow, i.e., O​(log⁡D)𝑂𝐷O(\\log D) in expectation and less than D𝐷D in the worse case; 3) the variable score vector in Eq. (2) is easy to calculate for bifurcating a tree node. Why MCTS-VS Can Perform Well. The theoretical results have suggested that a good variable selection method should select as important variables as possible. Thus, we compare the quality of the variables selected by MCTS-VS and Dropout (i.e., random selection), measured by the recall dt∗/dsubscriptsuperscript𝑑𝑡𝑑d^{*}_{t}/d, where d𝑑d is the number of valid variables, and dt∗subscriptsuperscript𝑑𝑡d^{*}_{t} is the number of valid variables selected at iteration t𝑡t. Dropout randomly selects d𝑑d variables at each iteration, and thus, the recall is d/D𝑑𝐷d/D in expectation. For MCTS-VS, we run MCTS-VS-BO for 600600600 evaluations on five different random seeds, and calculate the average recall. As shown in Table 2, the average recall of MCTS-VS is much larger than that of Dropout, implying that MCTS-VS can select better variables than random selection, and thus achieve a good performance as shown before. Meanwhile, the recall between 0.350.350.35 and 0.4330.4330.433 of MCTS-VS also implies that the variable selection method could be further improved."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Finite-time analysis of the multiarmed bandit problem",
      "abstract": "",
      "year": "2002",
      "venue": "Machine learning",
      "authors": "P. Auer, N. Cesa-Bianchi, and P. Fischer"
    },
    {
      "index": 1,
      "title": "A survey on high-dimensional Gaussian process modeling with application to Bayesian optimization",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Transactions on Evolutionary Learning and Optimization",
      "authors": "M. Binois and N. Wycoff"
    },
    {
      "index": 2,
      "title": "A warped kernel improving robustness in Bayesian optimization via random embeddings",
      "abstract": "",
      "year": "2015",
      "venue": "9th International Conference on Learning and Intelligent Optimization (LION’15)",
      "authors": "M. Binois, D. Ginsbourger, and O. Roustant"
    },
    {
      "index": 3,
      "title": "On the choice of the low-dimensional domain for global optimization via random embeddings",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Global Optimization",
      "authors": "M. Binois, D. Ginsbourger, and O. Roustant"
    },
    {
      "index": 4,
      "title": "A survey of Monte Carlo tree search methods",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
      "authors": "C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton"
    },
    {
      "index": 5,
      "title": "Bayesian optimization for learning gaits under uncertainty",
      "abstract": "",
      "year": "2015",
      "venue": "Annals of Mathematics and Artificial Intelligence",
      "authors": "R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth"
    },
    {
      "index": 6,
      "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "8th International Conference on Learning Representations (ICLR’20)",
      "authors": "X. Dong and Y. Yang",
      "orig_title": "NAS-Bench-201: Extending the scope of reproducible neural architecture search",
      "paper_id": "2001.00326v2"
    },
    {
      "index": 7,
      "title": "TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Y. Duan, X. Chen, H. Xu, Z. Chen, X. Liang, T. Zhang, and Z. Li",
      "orig_title": "TransNAS-Bench-101: Improving transferability and generalizability of cross-task neural architecture search",
      "paper_id": "2105.11871v1"
    },
    {
      "index": 8,
      "title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces",
      "abstract": "",
      "year": "2021",
      "venue": "37th Conference on Uncertainty in Artificial Intelligence (UAI’21)",
      "authors": "D. Eriksson and M. Jankowiak",
      "orig_title": "High-dimensional Bayesian optimization with sparse axis-aligned subspaces",
      "paper_id": "2103.00349v2"
    },
    {
      "index": 9,
      "title": "Scalable Global Optimization via Local Bayesian Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32 (NeurIPS’19)",
      "authors": "D. Eriksson, M. Pearce, J. R. Gardner, R. D. Turner, and M. Poloczek",
      "orig_title": "Scalable global optimization via local Bayesian optimization",
      "paper_id": "1910.01739v4"
    },
    {
      "index": 10,
      "title": "A tutorial on Bayesian optimization",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "P. I. Frazier"
    },
    {
      "index": 11,
      "title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
      "abstract": "",
      "year": "2018",
      "venue": "ACS Central Science",
      "authors": "R. Gómez-Bombarelli, D. K. Duvenaud, J. M. Hernández-Lobato, J. Aguilera-Iparraguirre, T.D. Hirzel, R. P. Adams, and A. Aspuru-Guzik",
      "orig_title": "Automatic chemical design using a data-driven continuous representation of molecules",
      "paper_id": "1610.02415v3"
    },
    {
      "index": 12,
      "title": "High-Dimensional Bayesian Optimization via Tree-Structured Additive Models",
      "abstract": "",
      "year": "2021",
      "venue": "35th Association for the Advancement of Artificial Intelligence (AAAI’21)",
      "authors": "E. Han, I. Arora, and J. Scarlett",
      "orig_title": "High-dimensional Bayesian optimization via tree-structured additive models",
      "paper_id": "2012.13088v1"
    },
    {
      "index": 13,
      "title": "The CMA evolution strategy: A tutorial",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "N. Hansen"
    },
    {
      "index": 14,
      "title": "Decentralized High-Dimensional Bayesian Optimization with Factor Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "32nd Association for the Advancement of Artificial Intelligence (AAAI’18)",
      "authors": "T. N. Hoang, Q. M. Hoang, R. Ouyang, and K. H. Low",
      "orig_title": "Decentralized high-dimensional Bayesian optimization with factor graphs",
      "paper_id": "1711.07033v3"
    },
    {
      "index": 15,
      "title": "Efficient global optimization of expensive black-box functions",
      "abstract": "",
      "year": "1998",
      "venue": "Journal of Global Optimization",
      "authors": "D. R. Jones, M. Schonlau, and W. J. Welch"
    },
    {
      "index": 16,
      "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models",
      "abstract": "",
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning (ICML’15)",
      "authors": "K. Kandasamy, J. G. Schneider, and B. Póczos",
      "orig_title": "High dimensional Bayesian optimisation and bandits via additive models",
      "paper_id": "1503.01673v3"
    },
    {
      "index": 17,
      "title": "Auto-encoding variational Bayes",
      "abstract": "",
      "year": "2014",
      "venue": "CoRR",
      "authors": "D. P. Kingma and M. Welling"
    },
    {
      "index": 18,
      "title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise",
      "abstract": "",
      "year": "1964",
      "venue": "Journal of Basic Engineering",
      "authors": "H. J. Kushner"
    },
    {
      "index": 19,
      "title": "Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33 (NeurIPS’20)",
      "authors": "B. Letham, R. Calandra, A. Rai, and E. Bakshy",
      "orig_title": "Re-examining linear embeddings for high-dimensional Bayesian optimization",
      "paper_id": "2001.11659v2"
    },
    {
      "index": 20,
      "title": "High dimensional Bayesian optimization using dropout",
      "abstract": "",
      "year": "2017",
      "venue": "26th International Joint Conference on Artificial Intelligence (IJCAI’17)",
      "authors": "C. Li, S. Gupta, S. Rana, V. Nguyen, S. Venkatesh, and A. Shilton"
    },
    {
      "index": 21,
      "title": "Structured variationally auto-encoded optimization",
      "abstract": "",
      "year": "2018",
      "venue": "35th International Conference on Machine Learning (ICML’18)",
      "authors": "X. Lu, J. I. González, Z. Dai, and N. D. Lawrence"
    },
    {
      "index": 22,
      "title": "Bayesian optimization in high-dimensional spaces: A brief survey",
      "abstract": "",
      "year": "2021",
      "venue": "12th International Conference on Information, Intelligence, Systems & Applications (IISA’21)",
      "authors": "M. Malu, G. Dasarathy, and A. Spanias"
    },
    {
      "index": 23,
      "title": "A comparison of three methods for selecting values of input variables in the analysis of output from a computer code",
      "abstract": "",
      "year": "1979",
      "venue": "Technometrics",
      "authors": "M. D. McKay, R. J. Beckman, and W. J. Conover"
    },
    {
      "index": 24,
      "title": "NAS-Bench-ASR: Reproducible neural architecture search for speech recognition",
      "abstract": "",
      "year": "2021",
      "venue": "9th International Conference on Learning Representations (ICLR’21)",
      "authors": "A. Mehrotra, A. G. C. P. Ramos, S. Bhattacharya, Ł. Dudziak, R. Vipperla, T. Chau, M. S. Abdelfattah, S. Ishtiaq, and N. D. Lane"
    },
    {
      "index": 25,
      "title": "Efficient high dimensional Bayesian optimization with additivity and quadrature Fourier features",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31 (NeurIPS’18)",
      "authors": "M. Mutný and A. Krause"
    },
    {
      "index": 26,
      "title": "A framework for Bayesian optimization in embedded subspaces",
      "abstract": "",
      "year": "2019",
      "venue": "36th International Conference on Machine LearninG (ICML’19)",
      "authors": "A. Nayebi, A. Munteanu, and M. Poloczek"
    },
    {
      "index": 27,
      "title": "Numerical Optimization",
      "abstract": "",
      "year": "2006",
      "venue": "Springer",
      "authors": "J. Nocedal and S. J. Wright"
    },
    {
      "index": 28,
      "title": "Bayesian Optimization using Pseudo-Points",
      "abstract": "",
      "year": "2020",
      "venue": "29th International Joint Conference on Artificial Intelligence (IJCAI’20)",
      "authors": "C. Qian, H. Xiong, and K. Xue",
      "orig_title": "Bayesian optimization using pseudo-points",
      "paper_id": "1910.05484v2"
    },
    {
      "index": 29,
      "title": "Gaussian Processes for Machine Learning",
      "abstract": "",
      "year": "2006",
      "venue": "The MIT Press",
      "authors": "C. E. Rasmussen and C. K. I. Williams"
    },
    {
      "index": 30,
      "title": "High-dimensional Bayesian optimization via additive models with overlapping groups",
      "abstract": "",
      "year": "2018",
      "venue": "21st International Conference on Artificial Intelligence and Statistics (AISTATS’18)",
      "authors": "P. Rolland, J. Scarlett, I. Bogunovic, and V. Cevher"
    },
    {
      "index": 31,
      "title": "Taking the human out of the loop: A review of Bayesian optimization",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE",
      "authors": "B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas"
    },
    {
      "index": 32,
      "title": "Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Y. Shen and C. Kingsford",
      "orig_title": "Computationally efficient high-dimensional Bayesian optimization via variable selection",
      "paper_id": "2109.09264v2"
    },
    {
      "index": 33,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis"
    },
    {
      "index": 34,
      "title": "Mastering the game of Go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis"
    },
    {
      "index": 35,
      "title": "Scalable Bayesian optimization using deep neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning (ICML’15)",
      "authors": "J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary, Prabhat, and R. P. Adams"
    },
    {
      "index": 36,
      "title": "Bayesian optimization in effective dimensions via kernel-based sensitivity indices",
      "abstract": "",
      "year": "2019",
      "venue": "13th International Conference on Applications of Statistics and Probability in Civil Engineering (ICASP’13)",
      "authors": "A. Spagnol, R. L. Riche, and S. D. Veiga"
    },
    {
      "index": 37,
      "title": "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "N. Srinivas, A. Krause, S. M. Kakade, and M. W. Seeger"
    },
    {
      "index": 38,
      "title": "MuJoCo: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "E. Todorov, E. Erez, and Y. Tassa"
    },
    {
      "index": 39,
      "title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33 (NeurIPS’20)",
      "authors": "L. Wang, R. Fonseca, and Y. Tian",
      "orig_title": "Learning search space partition for black-box optimization using Monte Carlo tree search",
      "paper_id": "2007.00708v2"
    },
    {
      "index": 40,
      "title": "Sample-efficient neural architecture search by learning actions for Monte Carlo tree search",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "L. Wang, S. Xie, T. Li, R. Fonseca, and Y. Tian"
    },
    {
      "index": 41,
      "title": "Bayesian optimization in a billion dimensions via random embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Z. Wang, F. Hutter, M. Zoghi, D. Matheson, and N. de Feitas"
    },
    {
      "index": 42,
      "title": "Batched large-scale Bayesian optimization in high-dimensional spaces",
      "abstract": "",
      "year": "2018",
      "venue": "21st International Conference on Artificial Intelligence and Statistics (AISTATS’18)",
      "authors": "Z. Wang, C. Gehring, P. Kohli, and S. Jegelka"
    },
    {
      "index": 43,
      "title": "The reparameterization trick for acquisition functions",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "J. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth",
      "orig_title": "The reparameterization trick for acquisition functions",
      "paper_id": "1712.00424v1"
    },
    {
      "index": 44,
      "title": "NAS-bench-101: Towards reproducible neural architecture search",
      "abstract": "",
      "year": "2019",
      "venue": "36th International Conference on Machine Learning (ICML’19)",
      "authors": "C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter"
    },
    {
      "index": 45,
      "title": "NAS-Bench-1Shot1: Benchmarking and dissecting one-shot neural architecture search",
      "abstract": "",
      "year": "2020",
      "venue": "8th International Conference on Learning Representations (ICLR’20)",
      "authors": "A. Zela, J. Siems, and F. Hutter"
    }
  ]
}