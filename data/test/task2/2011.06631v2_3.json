{
  "paper_id": "2011.06631v2",
  "title": "Steady State Analysis of Episodic Reinforcement Learning",
  "sections": {
    "appendix a notes on the mdp formulation": "This section provides more discussions about the MDP formulation as introduced in Section 2. A faithful implementation of this formulation was used to generate all the results reported in Section 6 and Section F. Our MDP formulation assumes both SSSS\\SS and ğ’œğ’œ\\mathcal{A} are countable sets. This is mainly for aligning to the standard markov chain theory, and also for enabling convenient notations like transition matrix Mğ‘€M and summation âˆ‘s,asubscriptğ‘ ğ‘\\sum_{s,a}. Results in this paper are readily generalizable to uncountable action spaces (which still induce countable transitions after marginalizing over the actions), and may also be generalized to uncountable state spaces based on the theory of general-state-space markov chainsÂ . Our MDP formulation also assumes state-based deterministic reward function Rğ‘…R. While this formulation was used in many previous worksÂ [ref]16, some literatureÂ  explicitly assign stochastic rewards to transitions, in the form of Râ€‹(r|s,a,sâ€²)=â„™[rt+1=r|st=s,at=a,st+1=sâ€²]ğ‘…conditionalğ‘Ÿğ‘ ğ‘superscriptğ‘ â€²â„™subscriptğ‘Ÿğ‘¡1conditionalğ‘Ÿsubscriptğ‘ ğ‘¡ğ‘ subscriptğ‘ğ‘¡ğ‘subscriptğ‘ ğ‘¡1superscriptğ‘ â€²R(r|s,a,s^{\\prime})=\\operatorname*{\\mathbb{P}}[r_{t+1}=r|s_{t}=s,a_{t}=a,s_{t+1}=s^{\\prime}]. Our reward-function formulation has no loss of generality here, as one can think of a â€œstateâ€ in our formulation as a (s,r)ğ‘ ğ‘Ÿ(s,r) pair in stochastic-reward models, with the transition-dependent stochasticity of the reward captured by the transition function Pğ‘ƒP. Our MDP formulation has replaced the (often included) discounting constant Î³csubscriptğ›¾ğ‘\\gamma_{c} with the (often excluded) initial state distribution Ï0subscriptğœŒ0\\rho_{0}. Similar perspective to â€œdowngradeâ€ the discounting constant was discussed in  (Chapter 10.4). As will become evident later in the paper, the discounting constant is neither necessary for the purpose of defining the problem, nor is it necessary for treating the problem. On the other hand, an explicit specification of Ï0subscriptğœŒ0\\rho_{0} is necessary to define the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} as used in all episodic tasks, and will be also needed to define the terminal states in the formulation of episodic learning process proposed in this paper. A finite-horizon task can have degenerated steady states if it is formulated as an infinite-horizon MDP with an imaginary absorbing state sâ–¡subscriptğ‘ â–¡s_{\\scriptscriptstyle\\Box}. In the absorbing MDP formulation, any finite episode will end up with moving from its terminal state to the absorbing state sâ–¡subscriptğ‘ â–¡s_{\\scriptscriptstyle\\Box}, from there the rollout is trapped in the absorbing state forever without generating effective rewardÂ  . As a result, the stationary (and limiting) distribution of the absorbing MDP of any finite-horizon task concentrates fully and only to the absorbing state, making it of limited use for designing and analyzing RL algorithms. The definition of the steady-state performance measure Jaâ€‹vâ€‹gâ€‹(Ï€)â‰limTâ†’âˆ1Tâ€‹âˆ‘t=1TRâ€‹(st)=ğ”¼sâˆ¼ÏÏ€[Râ€‹(s)]approaches-limitsubscriptğ½ğ‘ğ‘£ğ‘”ğœ‹subscriptâ†’ğ‘‡1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘…subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğœ‹ğ‘…ğ‘ J_{avg}(\\pi)\\doteq\\lim\\limits_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}R(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\pi}\\end{subarray}}[R(s)], as introduced in Section 2, is based on the following ergodic theorem of markov chain: In ergodic markov chain Mğ‘€M with stationary distribution ÏMsubscriptğœŒğ‘€\\rho_{\\scriptscriptstyle M}, let fâ€‹(s)ğ‘“ğ‘ f(s) be any function such that ğ”¼sâˆ¼ÏM[|fâ€‹(s)|]<âˆsubscriptğ”¼similar-toğ‘ subscriptğœŒğ‘€ğ‘“ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[~{}|f(s)|~{}]<\\infty, then the time-average of fğ‘“f converges almost surely to the state-average of fğ‘“f, i.e., limTâ†’âˆâ„™[1Tâ€‹âˆ‘t=1Tfâ€‹(st)=ğ”¼sâˆ¼ÏM[fâ€‹(s)]]=1subscriptâ†’ğ‘‡â„™1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘“subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğ‘€ğ‘“ğ‘ 1\\lim\\limits_{T\\rightarrow\\infty}\\operatorname*{\\mathbb{P}}[~{}\\frac{1}{T}\\sum_{t=1}^{T}f(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[f(s)]~{}]=1.\n(, Theorem 74) The value function Qğ‘„Q is often more specifically called the action-value function. We called the Qğ‘„Q-function just value function because of its symmetric role with policy function Ï€ğœ‹\\pi. Moreover, in classic literature, the term â€œaction-value functionâ€ was often referred specifically to\nQÏ€Î³câ€‹(s,a)â‰ğ”¼s1âˆ¼Pâ€‹(s,a),{sâ‰¥2}âˆ¼MÏ€[âˆ‘t=1âˆRâ€‹(st)â‹…Î³ctâˆ’1]approaches-limitsubscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœ‹ğ‘ ğ‘subscriptğ”¼formulae-sequencesimilar-tosubscriptğ‘ 1ğ‘ƒğ‘ ğ‘similar-tosubscriptğ‘ absent2subscriptğ‘€ğœ‹superscriptsubscriptğ‘¡1â‹…ğ‘…subscriptğ‘ ğ‘¡superscriptsubscriptğ›¾ğ‘ğ‘¡1Q^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}(s,a)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s_{1}\\sim P(s,a),\\{s_{\\geq 2}\\}\\sim M_{\\pi}\\end{subarray}}[\\sum_{t=1}^{\\infty}R(s_{t})\\cdot\\gamma_{c}^{t-1}]\n, in which the constant 0â‰¤Î³c<10subscriptğ›¾ğ‘10\\leq\\gamma_{c}<1 is called the discounting factor. The specialized definition of QÏ€Î³csuperscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}} entails the specialized version of Bellman equation\nQÏ€Î³câ€‹(s,a)=ğ”¼sâ€²âˆ¼Pâ€‹(s,a),aâ€²âˆ¼Ï€â€‹(sâ€²)[Râ€‹(sâ€²)+Î³câ‹…QÏ€Î³câ€‹(sâ€²,aâ€²)],superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘ğ‘ ğ‘subscriptğ”¼formulae-sequencesimilar-tosuperscriptğ‘ â€²ğ‘ƒğ‘ ğ‘similar-tosuperscriptğ‘â€²ğœ‹superscriptğ‘ â€²ğ‘…superscriptğ‘ â€²â‹…subscriptğ›¾ğ‘superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘superscriptğ‘ â€²superscriptğ‘â€²Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s^{\\prime}\\sim P(s,a),a^{\\prime}\\sim\\pi(s^{\\prime})\\end{subarray}}[R(s^{\\prime})+\\gamma_{c}\\cdot Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s^{\\prime},a^{\\prime})],\nand induces the specialized version of state-value function\nVÏ€Î³câ€‹(s)â‰ğ”¼aâˆ¼Ï€â€‹(s)[QÏ€Î³câ€‹(s,a)].approaches-limitsuperscriptsubscriptğ‘‰ğœ‹subscriptğ›¾ğ‘ğ‘ subscriptğ”¼similar-toğ‘ğœ‹ğ‘ superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘ğ‘ ğ‘V_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}a\\sim\\pi(s)\\end{subarray}}[Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)]. The product of Î³ğ›¾\\gamma in the general-form value functions (1) was originally proposed as a virtual probabilistic termination in the agentâ€™s mindÂ ,\nbut was latter found useful to account for the real episode-boundaries when the MDP is used to model multi-episode rollouts  .\nOur paper uses product-form value functions for the same purpose as in  (Section 2.1.1) and . However, both  and  used transition-based Î³ğ›¾\\gamma-discounting, while the value function in our treatment uses state-based discounting and is probably closer to the method of Figure 1(c) in , which was considered an sub-optimal design in that paper. In fact,  attributes much of its main technical contribution to the adoption and contrastive analysis of transition-based discounting (against state-based discounting) which concludes that â€œtransition-based discounting is necessary to enable the unified specification of episodic and continuing tasksâ€ ( Section 2.1, see also Section 6 and B). Despite this major technical disparity, we nevertheless think the approach as described in Section 3 of our paper actually aligns with  (Section 2.1.1) and  in terms of the bigger idea that connecting finite episodes into a single infinite-horizon MDP greatly helps with unifying the episodic and continual formalisms."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Pybullet gymperium.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Dynamic programming and optimal control, volume 1.",
      "abstract": "",
      "year": "1995",
      "venue": "Athena scientific Belmont, MA",
      "authors": "D. P. Bertsekas."
    },
    {
      "index": 2,
      "title": "Openai gym.",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba."
    },
    {
      "index": 3,
      "title": "Off-policy actor-critic.",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1205.4839",
      "authors": "T. Degris, M. White, and R. S. Sutton."
    },
    {
      "index": 4,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel.",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control.",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 5,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.09477",
      "authors": "S. Fujimoto, H. Van Hoof, and D. Meger.",
      "orig_title": "Addressing function approximation error in actor-critic methods.",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 6,
      "title": "Continuous Deep Q-Learning with Model-based Acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "S. Gu, T. Lillicrap, I. Sutskever, and S. Levine.",
      "orig_title": "Continuous deep q-learning with model-based acceleration.",
      "paper_id": "1603.00748v1"
    },
    {
      "index": 7,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine.",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 8,
      "title": "A natural policy gradient.",
      "abstract": "",
      "year": "2002",
      "venue": "Advances in neural information processing systems",
      "authors": "S. M. Kakade."
    },
    {
      "index": 9,
      "title": "Continuous control with deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1509.02971",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra."
    },
    {
      "index": 10,
      "title": "Simulation-based optimization of markov reward processes.",
      "abstract": "",
      "year": "2001",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "P. Marbach and J. N. Tsitsiklis."
    },
    {
      "index": 11,
      "title": "Markov chains and stochastic stability.",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "S. P. Meyn and R. L. Tweedie."
    },
    {
      "index": 12,
      "title": "Human-level control through deep reinforcement learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al."
    },
    {
      "index": 13,
      "title": "Asynchronous methods for deep reinforcement learning.",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu."
    },
    {
      "index": 14,
      "title": "Time Limits in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.00378",
      "authors": "F. Pardo, A. Tavakoli, V. Levdik, and P. Kormushev.",
      "orig_title": "Time limits in reinforcement learning.",
      "paper_id": "1712.00378v4"
    },
    {
      "index": 15,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz.",
      "orig_title": "Trust region policy optimization.",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 16,
      "title": "Equivalence Between Policy Gradients and Soft Q-Learning",
      "abstract": "",
      "year": "2017a",
      "venue": "arXiv preprint arXiv:1704.06440",
      "authors": "J. Schulman, X. Chen, and P. Abbeel.",
      "orig_title": "Equivalence between policy gradients and soft q-learning.",
      "paper_id": "1704.06440v4"
    },
    {
      "index": 17,
      "title": "Proximal policy optimization algorithms.",
      "abstract": "",
      "year": "2017b",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov."
    },
    {
      "index": 18,
      "title": "Basics of applied stochastic processes.",
      "abstract": "",
      "year": "2009",
      "venue": "Springer Science & Business Media",
      "authors": "R. Serfozo."
    },
    {
      "index": 19,
      "title": "Deterministic policy gradient algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller."
    },
    {
      "index": 20,
      "title": "The predictron: End-to-end learning and planning.",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "D. Silver, H. van Hasselt, M. Hessel, T. Schaul, A. Guez, T. Harley, G. Dulac-Arnold, D. Reichert, N. Rabinowitz, A. Barreto, et al."
    },
    {
      "index": 21,
      "title": "A new q (lambda) with interim forward view and monte carlo equivalence.",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "R. Sutton, A. R. Mahmood, D. Precup, and H. Hasselt."
    },
    {
      "index": 22,
      "title": "Td models: modeling the world at a mixture of time scales.",
      "abstract": "",
      "year": "1995",
      "venue": "Twelfth International Conference on Machine Learning",
      "authors": "R. S. Sutton."
    },
    {
      "index": 23,
      "title": "Reinforcement learning: An introduction.",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto."
    },
    {
      "index": 24,
      "title": "Policy gradient methods for reinforcement learning with function approximation.",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour."
    },
    {
      "index": 25,
      "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.",
      "abstract": "",
      "year": "2011",
      "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup."
    },
    {
      "index": 26,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "R. S. Sutton, A. R. Mahmood, and M. White.",
      "orig_title": "An emphatic approach to the problem of off-policy temporal-difference learning.",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 27,
      "title": "Bias in natural actor-critic algorithms.",
      "abstract": "",
      "year": "2014",
      "venue": "International conference on machine learning",
      "authors": "P. Thomas."
    },
    {
      "index": 28,
      "title": "On average versus discounted reward temporal-difference learning.",
      "abstract": "",
      "year": "2002",
      "venue": "Machine Learning",
      "authors": "J. N. Tsitsiklis and B. Van Roy."
    },
    {
      "index": 29,
      "title": "Deep Reinforcement Learning with Double Q-learning",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI",
      "authors": "H. Van Hasselt, A. Guez, and D. Silver.",
      "orig_title": "Deep reinforcement learning with double q-learning.",
      "paper_id": "1509.06461v3"
    },
    {
      "index": 30,
      "title": "Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.",
      "abstract": "",
      "year": "2011",
      "venue": "Utrecht University",
      "authors": "H. P. van Hasselt."
    },
    {
      "index": 31,
      "title": "Learning from delayed rewards.",
      "abstract": "",
      "year": "1989",
      "venue": "Kingâ€™s College, University of Cambridge",
      "authors": "C. Watkins."
    },
    {
      "index": 32,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "M. White.",
      "orig_title": "Unifying task specification in reinforcement learning.",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 33,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "R. J. Williams."
    },
    {
      "index": 34,
      "title": "On convergence of emphatic temporal-difference learning.",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "H. Yu."
    },
    {
      "index": 35,
      "title": "Generalized Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "S. Zhang, W. Boehmer, and S. Whiteson.",
      "orig_title": "Generalized off-policy actor-critic.",
      "paper_id": "1903.11329v8"
    }
  ]
}