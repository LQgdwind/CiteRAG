{
  "paper_id": "2110.14248v2",
  "title": "Learning Domain Invariant Representations in Goal-conditioned Block MDPs",
  "sections": {
    "domain generalization theory for gbmdp": "In a seminal work, Ben-David et al. [ref]21 shows it is possible to bound the error of a classifier trained on a source domain on a target domain with a different data distribution. Follow-up work extends the theory to the domain generalization setting [ref]22 . In GBMDP, we can also derive similar theory to characterize the generalization from training environments â„°trainsubscriptâ„°train{\\mathcal{E}}_{\\text{train}} to target test environment tğ‘¡t. The theory relies on the Total Variation Distance DTVsubscriptğ·TVD_{\\text{TV}}  of two policies Ï€1,Ï€2subscriptğœ‹1subscriptğœ‹2\\pi_{1},\\pi_{2} with input (xe,g)superscriptğ‘¥ğ‘’ğ‘”(x^{e},g), which is defined as follows. In the following statements, we denote Ïâ€‹(x,g)ğœŒğ‘¥ğ‘”\\rho(x,g) as some joint distributions of goals and observations that gâˆ¼ğ’¢similar-toğ‘”ğ’¢g\\sim{\\mathcal{G}} and xğ‘¥x is determined by Ïâ€‹(x|g)ğœŒconditionalğ‘¥ğ‘”\\rho(x|g). Additionally, we use ÏÏ€eâ€‹(xe|g)superscriptsubscriptğœŒğœ‹ğ‘’conditionalsuperscriptğ‘¥ğ‘’ğ‘”\\rho_{\\pi}^{e}(x^{e}|g) to denote the discounted occupancy measure of xesuperscriptğ‘¥ğ‘’x^{e} in environment eğ‘’e under policy Ï€(â‹…|xe,g)\\pi(\\cdot|x^{e},g) and refer ÏÏ€eâ€‹(xe)superscriptsubscriptğœŒğœ‹ğ‘’superscriptğ‘¥ğ‘’\\rho_{\\pi}^{e}(x^{e}) as the marginal distribution. Furthermore, we denote ÏµÏâ€‹(x,g)â€‹(Ï€1âˆ¥Ï€2)superscriptitalic-ÏµğœŒğ‘¥ğ‘”conditionalsubscriptğœ‹1subscriptğœ‹2\\epsilon^{\\rho(x,g)}(\\pi_{1}\\parallel\\pi_{2}) as the average DTVsubscriptğ·TVD_{\\text{TV}} between Ï€1subscriptğœ‹1\\pi_{1} and Ï€2subscriptğœ‹2\\pi_{2}, i.e., ÏµÏâ€‹(x,g)(Ï€1âˆ¥Ï€2)=ğ”¼Ïâ€‹(x,g)[DTV(Ï€1(â‹…|x,g)âˆ¥Ï€2(â‹…|x,g))]\\epsilon^{\\rho(x,g)}(\\pi_{1}\\parallel\\pi_{2})=\\mathbb{E}_{\\rho(x,g)}[D_{\\text{TV}}(\\pi_{1}(\\cdot|x,g)\\parallel\\pi_{2}(\\cdot|x,g))]. This quantity is crucial in our theory as it can characterize the performance gap between two policies (see AppendixÂ C). Then, similar to the famous â„‹â€‹Î”â€‹â„‹â„‹Î”â„‹{\\mathcal{H}}\\Delta{\\mathcal{H}}-divergence [ref]21 [ref]22 in domain adaptation theory, we define Î â€‹Î”â€‹Î Î Î”Î \\Pi\\Delta\\Pi-divergence of two joint distributions Ïâ€‹(x,g)ğœŒğ‘¥ğ‘”\\rho(x,g) and Ïâ€‹(x,g)â€²ğœŒsuperscriptğ‘¥ğ‘”â€²\\rho(x,g)^{\\prime} in terms of the policy class Î Î \\Pi: On one hand, dÎ â€‹Î”â€‹Î subscriptğ‘‘Î Î”Î d_{\\Pi\\Delta\\Pi} is a distance metric which reflects the distance between two distributions w.r.t function class Î Î \\Pi. On the other hand, if we fix these two distributions, it also reveals the quality of the function class Î Î \\Pi, i.e., smaller dÎ â€‹Î”â€‹Î subscriptğ‘‘Î Î”Î d_{\\Pi\\Delta\\Pi} means more invariance to the distribution change. Finally, we state the following Proposition in which Ï€Gsubscriptğœ‹ğº\\pi_{G} is some optimal and invariant policy. For any Ï€âˆˆÎ ğœ‹Î \\pi\\in\\Pi, we consider the occupancy measure {ÏÏ€eiâ€‹(xei,g)}i=1NsuperscriptsubscriptsuperscriptsubscriptğœŒğœ‹subscriptğ‘’ğ‘–superscriptğ‘¥subscriptğ‘’ğ‘–ğ‘”ğ‘–1ğ‘\\{\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g)\\}_{i=1}^{N} for training environments and ÏÏ€Gtâ€‹(xt,g)superscriptsubscriptğœŒsubscriptğœ‹ğºğ‘¡superscriptğ‘¥ğ‘¡ğ‘”\\rho_{\\pi_{G}}^{t}(x^{t},g) for the target environment. For simplicity, we use Ïµeisuperscriptitalic-Ïµsubscriptğ‘’ğ‘–\\epsilon^{e_{i}} as the abbreviation of ÏµÏÏ€eiâ€‹(xei,g)superscriptitalic-ÏµsuperscriptsubscriptğœŒğœ‹subscriptğ‘’ğ‘–superscriptğ‘¥subscriptğ‘’ğ‘–ğ‘”\\epsilon^{\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g)}, Ïµtsuperscriptitalic-Ïµğ‘¡\\epsilon^{t} as ÏµÏÏ€Gtâ€‹(xt,g)superscriptitalic-ÏµsuperscriptsubscriptğœŒsubscriptğœ‹ğºğ‘¡superscriptğ‘¥ğ‘¡ğ‘”\\epsilon^{\\rho_{\\pi_{G}}^{t}(x^{t},g)} and Î´=maxei,eiâ€²âˆˆâ„°trainâ¡dÎ â€‹Î”â€‹Î â€‹(ÏÏ€eiâ€‹(xei,g),ÏÏ€eiâ€²â€‹(xeiâ€²,g))ğ›¿subscriptsubscriptğ‘’ğ‘–superscriptsubscriptğ‘’ğ‘–â€²subscriptâ„°trainsubscriptğ‘‘Î Î”Î superscriptsubscriptğœŒğœ‹subscriptğ‘’ğ‘–superscriptğ‘¥subscriptğ‘’ğ‘–ğ‘”superscriptsubscriptğœŒğœ‹superscriptsubscriptğ‘’ğ‘–â€²superscriptğ‘¥superscriptsubscriptğ‘’ğ‘–â€²ğ‘”\\delta=\\max_{e_{i},e_{i}^{\\prime}\\in{\\mathcal{E}}_{\\text{train}}}d_{\\Pi\\Delta\\Pi}(\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g),\\rho_{\\pi}^{e_{i}^{\\prime}}(x^{e_{i}^{\\prime}},g)). Let Then, we have where BğµB is a characteristic set of joint distributions determined by â„°trainsubscriptâ„°train{\\mathcal{E}}_{\\text{train}} and policy class Î Î \\Pi. The formal statement and the proof are shown in AppendixÂ C.2. Generally speaking, the first term of the right hand side in Eq.Â (2) quantifies the performance of Ï€ğœ‹\\pi in the Nğ‘N training environments. Î»ğœ†\\lambda quantifies the optimality of the policy class Î Î \\Pi over all environments. Î´ğ›¿\\delta reflects how the policy class Î Î \\Pi can reflect the difference among {ÏÏ€eiâ€‹(xei,g),eiâˆˆâ„°train}subscriptsuperscriptğœŒsubscriptğ‘’ğ‘–ğœ‹superscriptğ‘¥subscriptğ‘’ğ‘–ğ‘”subscriptğ‘’ğ‘–subscriptâ„°train\\{\\rho^{e_{i}}_{\\pi}(x^{e_{i}},g),e_{i}\\in{\\mathcal{E}}_{\\text{train}}\\}, which should be small if the policy class is invariant. The last term characterizes the distance between training environment and target environment and will be small if the training environments are diversely distributed. Many works on domain generalization of supervised learning [ref]21  [ref]22  [ref]26 spend much effort in discussing the trade-offs among different terms similar to the ones in Eq.Â (2), e.g., minimizing Î´ğ›¿\\delta may increase Î»ğœ†\\lambda [ref]26, and in developing sophisticated techniques to optimize the bound, e.g. distribution matching    or adversarial learning . Different from their perspectives, in GBMDPs, we propose a simple but effective criteria to minimize the bound. From now on, we only consider the policy class Î =Î Î¦={wâ€‹(Î¦â€‹(x),g),âˆ€w}Î subscriptÎ Î¦ğ‘¤Î¦ğ‘¥ğ‘”for-allğ‘¤\\Pi=\\Pi_{\\Phi}=\\{w(\\Phi(x),g),\\forall w\\}. Usually, Î¦Î¦\\Phi will be referred as an encoder which maps xâˆˆğ’³â„°ğ‘¥superscriptğ’³â„°x\\in{\\mathcal{X}}^{{\\mathcal{E}}} to some latent representation z=Î¦â€‹(x)ğ‘§Î¦ğ‘¥z=\\Phi(x). We will also use the notation zâ€‹(s)=Î¦â€‹(xâ€‹(s))ğ‘§ğ‘ Î¦ğ‘¥ğ‘ z(s)=\\Phi(x(s)) if we do not emphasize on the specific environment. An encoder is called a perfect alignment encoder Î¦Î¦\\Phi w.r.t environment set Eğ¸E if âˆ€e,eâ€²âˆˆEfor-allğ‘’superscriptğ‘’â€²ğ¸\\forall e,e^{\\prime}\\in E and âˆ€s,sâ€²âˆˆğ’®for-allğ‘ superscriptğ‘ â€²ğ’®\\forall s,s^{\\prime}\\in{\\mathcal{S}}, Î¦â€‹(xeâ€‹(s))=Î¦â€‹(xeâ€²â€‹(sâ€²))Î¦superscriptğ‘¥ğ‘’ğ‘ Î¦superscriptğ‘¥superscriptğ‘’â€²superscriptğ‘ â€²\\Phi(x^{e}(s))=\\Phi(x^{e^{\\prime}}(s^{\\prime})) if and only if s=sâ€²ğ‘ superscriptğ‘ â€²s=s^{\\prime}. As illustrated in FigureÂ 5, an encoder is in perfect alignment if it maps two observations of the same underlying state sğ‘ s to the same latent encoding zâ€‹(s)ğ‘§ğ‘ z(s) while also preventing meaningless embedding, i.e., mapping observations of different states to the same zğ‘§z. We believe perfect alignment plays an important role in domain generalization for goal-conditioned RL agents. Specifically, it can minimize the bound of Eq.Â (2) as follows. If the encoder Î¦Î¦\\Phi is a perfect alignment over â„°trainsubscriptâ„°train{\\mathcal{E}}_{\\text{train}}, then where Ï~â€‹(x,g)~ğœŒğ‘¥ğ‘”\\tilde{\\rho}(x,g) and Ï€âˆ—superscriptğœ‹\\pi^{*} are defined in PropositionÂ 1 (also AppendixÂ C). In AppendixÂ C.3, we formally prove PropositionÂ 2 when Î¦Î¦\\Phi is a (Î·,Ïˆ)ğœ‚ğœ“(\\eta,\\psi)-perfect alignment, i.e., Î¦Î¦\\Phi is only near perfect alignment. The proof shows that the generalization error bound is minimized on the R.H.S of Eq.Â (3) when Î¦Î¦\\Phi asymptotically becomes an exact perfect alignment encoder. Therefore, in our following method, we aim to learn a perfect alignment encoder via aligned sampling (SectionÂ 3.2). For the remaining terms in the R.H.S of Eq.Â (3), we find it hard to quantify them task agnostically, as similar difficulties also exist in the domain generalization theory of supervised learning [ref]22. Fortunately, we can derive upper bounds for the remaining terms under certain assumptions and we observe that these upper bounds are significantly reduced via our method in the experiments (SectionÂ 4). The (E)ğ¸(E) term represents how well the learnt policy Ï€ğœ‹\\pi approximates the optimal invariant policy on the training environments and is reduced to almost zero via RL (TableÂ 1). For the (t)ğ‘¡(t) term, we show that an upper bound of (t)ğ‘¡(t) is proportion to the invariant quality of Î¦Î¦\\Phi on the target environment. Moreover, we find that learning a perfect alignment encoder over â„°trainsubscriptâ„°train{\\mathcal{E}}_{\\text{train}} empirically improves the invariant quality over other unseen environments (tğ‘¡t) (FigureÂ 4). Thus, this (t)ğ‘¡(t) term upperbound is reduced by learning perfect alignment. Please refer to AppendixÂ C.4 for more details. Based on the theory we derived in this subsection, we adopt perfect alignment as the heuristic to address GBMDPs in our work. In the following subsections, we propose a practical method to acquire a perfect alignment encoder over the training environments."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "David Silver, Julian Schrittwieser, Karen Simonyan, Aj Antonoglou, Ioannis abd Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis"
    },
    {
      "index": 1,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg strovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan umaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 2,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 3,
      "title": "Assessing Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Charles Packer, Katelyn Gao, Jernej Kos, Philipp KrÃ¤henbÃ¼hl, Vladlen Koltun, and Dawn Song",
      "orig_title": "Assessing generalization in deep reinforcement learning",
      "paper_id": "1810.12282v2"
    },
    {
      "index": 4,
      "title": "Efficient adaptation for end-to-end vision-based robotic manipulation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.10190",
      "authors": "Ryan Julian, Benjamin Swanson, Gaurav S Sukhatme, Sergey Levine, Chelsea Finn, and Karol Hausman"
    },
    {
      "index": 5,
      "title": "Invariant Causal Prediction for Block MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup",
      "orig_title": "Invariant causal prediction for block MDPs",
      "paper_id": "2003.06016v2"
    },
    {
      "index": 6,
      "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.10742",
      "authors": "Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine",
      "orig_title": "Learning invariant representations for reinforcement learning without reconstruction",
      "paper_id": "2006.10742v2"
    },
    {
      "index": 7,
      "title": "Hindsight experience replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Andrychowicz Marcin, Wolski Filip, Ray Alex, Schneider Jonas, Fong Rachel, Welinder Peter, McGrew Bob, Tobin Josh, Abbeel Pieter, and Zaremba Wojciech"
    },
    {
      "index": 8,
      "title": "C-learning: Learning to achieve goals via recursive classification",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.08909",
      "authors": "Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine"
    },
    {
      "index": 9,
      "title": "Planning from Pixels using Inverse Dynamics Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.02419",
      "authors": "Keiran Paster, Sheila A McIlraith, and Jimmy Ba",
      "orig_title": "Planning from pixels using inverse dynamics models",
      "paper_id": "2012.02419v1"
    },
    {
      "index": 10,
      "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Alexandre PÃ©rÃ©, SÃ©bastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer",
      "orig_title": "Unsupervised learning of goal spaces for intrinsically motivated goal exploration",
      "paper_id": "1803.00781v3"
    },
    {
      "index": 11,
      "title": "Bisimulation metrics for continuous markov decision processes",
      "abstract": "",
      "year": "2011",
      "venue": "SIAM Journal on Computing",
      "authors": "Norm Ferns, Prakash Panangaden, and Doina Precup"
    },
    {
      "index": 12,
      "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine",
      "orig_title": "Skew-fit: State-covering self-supervised reinforcement learning",
      "paper_id": "1903.03698v4"
    },
    {
      "index": 13,
      "title": "Provably efficient RL with Rich Observations via Latent State Decoding",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford",
      "orig_title": "Provably efficient RL with rich observations via latent state decoding",
      "paper_id": "1901.09018v3"
    },
    {
      "index": 14,
      "title": "Learning to achieve goals",
      "abstract": "",
      "year": "1993",
      "venue": "IJCAI",
      "authors": "Leslie Pack Kaelbling"
    },
    {
      "index": 15,
      "title": "Universal value function approximators",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Brockman Greg, Cheung Vicki, Pettersson Ludwig, Schneider Jonas, Schulman John, Tang Jie, and Zaremba Wojciech"
    },
    {
      "index": 17,
      "title": "multiworld",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Vitchyr Pong, Murtaza Dalal, Steven Lin, and Ashvin Nair"
    },
    {
      "index": 18,
      "title": "Wilds: A Benchmark of in-the-Wild Distribution Shifts",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.07421",
      "authors": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, et al.",
      "orig_title": "Wilds: A benchmark of in-the-wild distribution shifts",
      "paper_id": "2012.07421v3"
    },
    {
      "index": 19,
      "title": "Invariant risk minimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.02893",
      "authors": "Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz"
    },
    {
      "index": 20,
      "title": "A theory of learning from different domains",
      "abstract": "",
      "year": "2010",
      "venue": "Machine learning",
      "authors": "Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan"
    },
    {
      "index": 21,
      "title": "Domain Adversarial Neural Networks for Domain Generalization: When It Works and How to Improve",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03924",
      "authors": "Anthony Sicilia, Xingchen Zhao, and Seong Jae Hwang",
      "orig_title": "Domain adversarial neural networks for domain generalization: When it works and how to improve",
      "paper_id": "2102.03924v2"
    },
    {
      "index": 22,
      "title": "Generalizing to unseen domains via distribution matching",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.00804",
      "authors": "Isabela Albuquerque, JoÃ£o Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas",
      "orig_title": "Generalizing to unseen domains via distribution matching",
      "paper_id": "1911.00804v6"
    },
    {
      "index": 23,
      "title": "Total variation distance of probability measures â€” Wikipedia, the free encyclopedia",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wikipedia"
    },
    {
      "index": 24,
      "title": "Transferable adversarial training: A general approach to adapting deep classifiers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan"
    },
    {
      "index": 25,
      "title": "Adversarial invariant feature learning with accuracy constraint for domain generalization",
      "abstract": "",
      "year": "2019",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo"
    },
    {
      "index": 26,
      "title": "The Variational Fair Autoencoder",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S Zemel",
      "orig_title": "The variational fair autoencoder",
      "paper_id": "1511.00830v6"
    },
    {
      "index": 27,
      "title": "Deep domain generalization via conditional invariant adversarial networks",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao"
    },
    {
      "index": 28,
      "title": "Feature Alignment and Restoration for Domain Generalization and Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12009",
      "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen",
      "orig_title": "Feature alignment and restoration for domain generalization and adaptation",
      "paper_id": "2006.12009v1"
    },
    {
      "index": 29,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Scott Fujimoto, Herke Hoof, and David Meger",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 30,
      "title": "A kernel method for the two-sample problem",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research",
      "authors": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and Alexander Smola"
    },
    {
      "index": 31,
      "title": "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test",
      "abstract": "",
      "year": "2015",
      "venue": "Neural computation",
      "authors": "Ji Zhao and Deyu Meng"
    },
    {
      "index": 32,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 33,
      "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Michael Laskin, Aravind Srinivas, and Pieter Abbeel",
      "orig_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
      "paper_id": "2004.04136v4"
    },
    {
      "index": 34,
      "title": "Visual Reinforcement Learning with Imagined Goals",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine",
      "orig_title": "Visual reinforcement learning with imagined goals",
      "paper_id": "1807.04742v2"
    },
    {
      "index": 35,
      "title": "The distracting control suiteâ€“a challenging benchmark for reinforcement learning from pixels",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.02722",
      "authors": "Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski"
    },
    {
      "index": 36,
      "title": "Reinforcement Learning with Augmented Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.14990",
      "authors": "Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas",
      "orig_title": "Reinforcement learning with augmented data",
      "paper_id": "2004.14990v5"
    },
    {
      "index": 37,
      "title": "Roll: Visual self-supervised reinforcement learning with object reasoning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.06777",
      "authors": "Yufei Wang, Gautham Narayan Narasimhan, Xingyu Lin, Brian Okorn, and David Held"
    },
    {
      "index": 38,
      "title": "Weakly-Supervised Reinforcement Learning for Controllable Behavior",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.02860",
      "authors": "Lisa Lee, Benjamin Eysenbach, Ruslan Salakhutdinov, Chelsea Finn, et al.",
      "orig_title": "Weakly-supervised reinforcement learning for controllable behavior",
      "paper_id": "2004.02860v2"
    },
    {
      "index": 39,
      "title": "Exploration via hindsight goal generation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng"
    },
    {
      "index": 40,
      "title": "Learning to Reach Goals via Iterated Supervised Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv e-prints",
      "authors": "Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine",
      "orig_title": "Learning to reach goals via iterated supervised learning",
      "paper_id": "1912.06088v4"
    },
    {
      "index": 41,
      "title": "Automatic Goal Generation for Reinforcement Learning Agents",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel",
      "orig_title": "Automatic goal generation for reinforcement learning agents",
      "paper_id": "1705.06366v5"
    },
    {
      "index": 42,
      "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Suraj Nair and Chelsea Finn",
      "orig_title": "Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation",
      "paper_id": "1909.05829v1"
    },
    {
      "index": 43,
      "title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba"
    },
    {
      "index": 44,
      "title": "Goal-aware prediction: Learning to model what matters",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Suraj Nair, Silvio Savarese, and Chelsea Finn"
    },
    {
      "index": 45,
      "title": "Learning subgoal representation with slow dynamics",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Siyuan Li, Lulu Zheng, Jianhao Wang, and Chongjie Zhang"
    },
    {
      "index": 46,
      "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine",
      "orig_title": "Near-optimal representation learning for hierarchical reinforcement learning",
      "paper_id": "1810.01257v2"
    },
    {
      "index": 47,
      "title": "World Model as a Graph: Learning Latent Landmarks for Planning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.12491",
      "authors": "Lunjun Zhang, Ge Yang, and Bradly C Stadie",
      "orig_title": "World model as a graph: Learning latent landmarks for planning",
      "paper_id": "2011.12491v3"
    },
    {
      "index": 48,
      "title": "Data-efficient hierarchical reinforcement learning for robotic assembly control applications",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Industrial Electronics",
      "authors": "Zhimin Hou, Jiajun Fei, Yuelin Deng, and Jing Xu"
    },
    {
      "index": 49,
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare",
      "orig_title": "Deepmdp: Learning continuous latent space models for representation learning",
      "paper_id": "1906.02736v1"
    },
    {
      "index": 50,
      "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.05265",
      "authors": "Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare",
      "orig_title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning",
      "paper_id": "2101.05265v2"
    },
    {
      "index": 51,
      "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.13649",
      "authors": "Ilya Kostrikov, Denis Yarats, and Rob Fergus"
    },
    {
      "index": 52,
      "title": "Self-Supervised Policy Adaptation during Deployment",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.04309",
      "authors": "Nicklas Hansen, Yu Sun, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang",
      "orig_title": "Self-supervised policy adaptation during deployment",
      "paper_id": "2007.04309v3"
    },
    {
      "index": 53,
      "title": "A Geometric Perspective on Self-Supervised Policy Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.07318",
      "authors": "Cristian Bodnar, Karol Hausman, Gabriel Dulac-Arnold, and Rico Jonschkowski",
      "orig_title": "A geometric perspective on self-supervised policy adaptation",
      "paper_id": "2011.07318v1"
    },
    {
      "index": 54,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 55,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Christopher JCH Watkins and Peter Dayan"
    }
  ]
}