{
  "paper_id": "2110.14248v2",
  "title": "Learning Domain Invariant Representations in Goal-conditioned Block MDPs",
  "sections": {
    "domain generalization theory for gbmdp": "In a seminal work, Ben-David et al. [ref]21 shows it is possible to bound the error of a classifier trained on a source domain on a target domain with a different data distribution. Follow-up work extends the theory to the domain generalization setting [ref]22 . In GBMDP, we can also derive similar theory to characterize the generalization from training environments ℰtrainsubscriptℰtrain{\\mathcal{E}}_{\\text{train}} to target test environment t𝑡t. The theory relies on the Total Variation Distance DTVsubscript𝐷TVD_{\\text{TV}}  of two policies π1,π2subscript𝜋1subscript𝜋2\\pi_{1},\\pi_{2} with input (xe,g)superscript𝑥𝑒𝑔(x^{e},g), which is defined as follows. In the following statements, we denote ρ​(x,g)𝜌𝑥𝑔\\rho(x,g) as some joint distributions of goals and observations that g∼𝒢similar-to𝑔𝒢g\\sim{\\mathcal{G}} and x𝑥x is determined by ρ​(x|g)𝜌conditional𝑥𝑔\\rho(x|g). Additionally, we use ρπe​(xe|g)superscriptsubscript𝜌𝜋𝑒conditionalsuperscript𝑥𝑒𝑔\\rho_{\\pi}^{e}(x^{e}|g) to denote the discounted occupancy measure of xesuperscript𝑥𝑒x^{e} in environment e𝑒e under policy π(⋅|xe,g)\\pi(\\cdot|x^{e},g) and refer ρπe​(xe)superscriptsubscript𝜌𝜋𝑒superscript𝑥𝑒\\rho_{\\pi}^{e}(x^{e}) as the marginal distribution. Furthermore, we denote ϵρ​(x,g)​(π1∥π2)superscriptitalic-ϵ𝜌𝑥𝑔conditionalsubscript𝜋1subscript𝜋2\\epsilon^{\\rho(x,g)}(\\pi_{1}\\parallel\\pi_{2}) as the average DTVsubscript𝐷TVD_{\\text{TV}} between π1subscript𝜋1\\pi_{1} and π2subscript𝜋2\\pi_{2}, i.e., ϵρ​(x,g)(π1∥π2)=𝔼ρ​(x,g)[DTV(π1(⋅|x,g)∥π2(⋅|x,g))]\\epsilon^{\\rho(x,g)}(\\pi_{1}\\parallel\\pi_{2})=\\mathbb{E}_{\\rho(x,g)}[D_{\\text{TV}}(\\pi_{1}(\\cdot|x,g)\\parallel\\pi_{2}(\\cdot|x,g))]. This quantity is crucial in our theory as it can characterize the performance gap between two policies (see Appendix C). Then, similar to the famous ℋ​Δ​ℋℋΔℋ{\\mathcal{H}}\\Delta{\\mathcal{H}}-divergence [ref]21 [ref]22 in domain adaptation theory, we define Π​Δ​ΠΠΔΠ\\Pi\\Delta\\Pi-divergence of two joint distributions ρ​(x,g)𝜌𝑥𝑔\\rho(x,g) and ρ​(x,g)′𝜌superscript𝑥𝑔′\\rho(x,g)^{\\prime} in terms of the policy class ΠΠ\\Pi: On one hand, dΠ​Δ​Πsubscript𝑑ΠΔΠd_{\\Pi\\Delta\\Pi} is a distance metric which reflects the distance between two distributions w.r.t function class ΠΠ\\Pi. On the other hand, if we fix these two distributions, it also reveals the quality of the function class ΠΠ\\Pi, i.e., smaller dΠ​Δ​Πsubscript𝑑ΠΔΠd_{\\Pi\\Delta\\Pi} means more invariance to the distribution change. Finally, we state the following Proposition in which πGsubscript𝜋𝐺\\pi_{G} is some optimal and invariant policy. For any π∈Π𝜋Π\\pi\\in\\Pi, we consider the occupancy measure {ρπei​(xei,g)}i=1Nsuperscriptsubscriptsuperscriptsubscript𝜌𝜋subscript𝑒𝑖superscript𝑥subscript𝑒𝑖𝑔𝑖1𝑁\\{\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g)\\}_{i=1}^{N} for training environments and ρπGt​(xt,g)superscriptsubscript𝜌subscript𝜋𝐺𝑡superscript𝑥𝑡𝑔\\rho_{\\pi_{G}}^{t}(x^{t},g) for the target environment. For simplicity, we use ϵeisuperscriptitalic-ϵsubscript𝑒𝑖\\epsilon^{e_{i}} as the abbreviation of ϵρπei​(xei,g)superscriptitalic-ϵsuperscriptsubscript𝜌𝜋subscript𝑒𝑖superscript𝑥subscript𝑒𝑖𝑔\\epsilon^{\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g)}, ϵtsuperscriptitalic-ϵ𝑡\\epsilon^{t} as ϵρπGt​(xt,g)superscriptitalic-ϵsuperscriptsubscript𝜌subscript𝜋𝐺𝑡superscript𝑥𝑡𝑔\\epsilon^{\\rho_{\\pi_{G}}^{t}(x^{t},g)} and δ=maxei,ei′∈ℰtrain⁡dΠ​Δ​Π​(ρπei​(xei,g),ρπei′​(xei′,g))𝛿subscriptsubscript𝑒𝑖superscriptsubscript𝑒𝑖′subscriptℰtrainsubscript𝑑ΠΔΠsuperscriptsubscript𝜌𝜋subscript𝑒𝑖superscript𝑥subscript𝑒𝑖𝑔superscriptsubscript𝜌𝜋superscriptsubscript𝑒𝑖′superscript𝑥superscriptsubscript𝑒𝑖′𝑔\\delta=\\max_{e_{i},e_{i}^{\\prime}\\in{\\mathcal{E}}_{\\text{train}}}d_{\\Pi\\Delta\\Pi}(\\rho_{\\pi}^{e_{i}}(x^{e_{i}},g),\\rho_{\\pi}^{e_{i}^{\\prime}}(x^{e_{i}^{\\prime}},g)). Let Then, we have where B𝐵B is a characteristic set of joint distributions determined by ℰtrainsubscriptℰtrain{\\mathcal{E}}_{\\text{train}} and policy class ΠΠ\\Pi. The formal statement and the proof are shown in Appendix C.2. Generally speaking, the first term of the right hand side in Eq. (2) quantifies the performance of π𝜋\\pi in the N𝑁N training environments. λ𝜆\\lambda quantifies the optimality of the policy class ΠΠ\\Pi over all environments. δ𝛿\\delta reflects how the policy class ΠΠ\\Pi can reflect the difference among {ρπei​(xei,g),ei∈ℰtrain}subscriptsuperscript𝜌subscript𝑒𝑖𝜋superscript𝑥subscript𝑒𝑖𝑔subscript𝑒𝑖subscriptℰtrain\\{\\rho^{e_{i}}_{\\pi}(x^{e_{i}},g),e_{i}\\in{\\mathcal{E}}_{\\text{train}}\\}, which should be small if the policy class is invariant. The last term characterizes the distance between training environment and target environment and will be small if the training environments are diversely distributed. Many works on domain generalization of supervised learning [ref]21  [ref]22  [ref]26 spend much effort in discussing the trade-offs among different terms similar to the ones in Eq. (2), e.g., minimizing δ𝛿\\delta may increase λ𝜆\\lambda [ref]26, and in developing sophisticated techniques to optimize the bound, e.g. distribution matching    or adversarial learning . Different from their perspectives, in GBMDPs, we propose a simple but effective criteria to minimize the bound. From now on, we only consider the policy class Π=ΠΦ={w​(Φ​(x),g),∀w}ΠsubscriptΠΦ𝑤Φ𝑥𝑔for-all𝑤\\Pi=\\Pi_{\\Phi}=\\{w(\\Phi(x),g),\\forall w\\}. Usually, ΦΦ\\Phi will be referred as an encoder which maps x∈𝒳ℰ𝑥superscript𝒳ℰx\\in{\\mathcal{X}}^{{\\mathcal{E}}} to some latent representation z=Φ​(x)𝑧Φ𝑥z=\\Phi(x). We will also use the notation z​(s)=Φ​(x​(s))𝑧𝑠Φ𝑥𝑠z(s)=\\Phi(x(s)) if we do not emphasize on the specific environment. An encoder is called a perfect alignment encoder ΦΦ\\Phi w.r.t environment set E𝐸E if ∀e,e′∈Efor-all𝑒superscript𝑒′𝐸\\forall e,e^{\\prime}\\in E and ∀s,s′∈𝒮for-all𝑠superscript𝑠′𝒮\\forall s,s^{\\prime}\\in{\\mathcal{S}}, Φ​(xe​(s))=Φ​(xe′​(s′))Φsuperscript𝑥𝑒𝑠Φsuperscript𝑥superscript𝑒′superscript𝑠′\\Phi(x^{e}(s))=\\Phi(x^{e^{\\prime}}(s^{\\prime})) if and only if s=s′𝑠superscript𝑠′s=s^{\\prime}. As illustrated in Figure 5, an encoder is in perfect alignment if it maps two observations of the same underlying state s𝑠s to the same latent encoding z​(s)𝑧𝑠z(s) while also preventing meaningless embedding, i.e., mapping observations of different states to the same z𝑧z. We believe perfect alignment plays an important role in domain generalization for goal-conditioned RL agents. Specifically, it can minimize the bound of Eq. (2) as follows. If the encoder ΦΦ\\Phi is a perfect alignment over ℰtrainsubscriptℰtrain{\\mathcal{E}}_{\\text{train}}, then where ρ~​(x,g)~𝜌𝑥𝑔\\tilde{\\rho}(x,g) and π∗superscript𝜋\\pi^{*} are defined in Proposition 1 (also Appendix C). In Appendix C.3, we formally prove Proposition 2 when ΦΦ\\Phi is a (η,ψ)𝜂𝜓(\\eta,\\psi)-perfect alignment, i.e., ΦΦ\\Phi is only near perfect alignment. The proof shows that the generalization error bound is minimized on the R.H.S of Eq. (3) when ΦΦ\\Phi asymptotically becomes an exact perfect alignment encoder. Therefore, in our following method, we aim to learn a perfect alignment encoder via aligned sampling (Section 3.2). For the remaining terms in the R.H.S of Eq. (3), we find it hard to quantify them task agnostically, as similar difficulties also exist in the domain generalization theory of supervised learning [ref]22. Fortunately, we can derive upper bounds for the remaining terms under certain assumptions and we observe that these upper bounds are significantly reduced via our method in the experiments (Section 4). The (E)𝐸(E) term represents how well the learnt policy π𝜋\\pi approximates the optimal invariant policy on the training environments and is reduced to almost zero via RL (Table 1). For the (t)𝑡(t) term, we show that an upper bound of (t)𝑡(t) is proportion to the invariant quality of ΦΦ\\Phi on the target environment. Moreover, we find that learning a perfect alignment encoder over ℰtrainsubscriptℰtrain{\\mathcal{E}}_{\\text{train}} empirically improves the invariant quality over other unseen environments (t𝑡t) (Figure 4). Thus, this (t)𝑡(t) term upperbound is reduced by learning perfect alignment. Please refer to Appendix C.4 for more details. Based on the theory we derived in this subsection, we adopt perfect alignment as the heuristic to address GBMDPs in our work. In the following subsections, we propose a practical method to acquire a perfect alignment encoder over the training environments."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "David Silver, Julian Schrittwieser, Karen Simonyan, Aj Antonoglou, Ioannis abd Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis"
    },
    {
      "index": 1,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg strovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan umaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 2,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 3,
      "title": "Assessing Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song",
      "orig_title": "Assessing generalization in deep reinforcement learning",
      "paper_id": "1810.12282v2"
    },
    {
      "index": 4,
      "title": "Efficient adaptation for end-to-end vision-based robotic manipulation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.10190",
      "authors": "Ryan Julian, Benjamin Swanson, Gaurav S Sukhatme, Sergey Levine, Chelsea Finn, and Karol Hausman"
    },
    {
      "index": 5,
      "title": "Invariant Causal Prediction for Block MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup",
      "orig_title": "Invariant causal prediction for block MDPs",
      "paper_id": "2003.06016v2"
    },
    {
      "index": 6,
      "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.10742",
      "authors": "Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine",
      "orig_title": "Learning invariant representations for reinforcement learning without reconstruction",
      "paper_id": "2006.10742v2"
    },
    {
      "index": 7,
      "title": "Hindsight experience replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Andrychowicz Marcin, Wolski Filip, Ray Alex, Schneider Jonas, Fong Rachel, Welinder Peter, McGrew Bob, Tobin Josh, Abbeel Pieter, and Zaremba Wojciech"
    },
    {
      "index": 8,
      "title": "C-learning: Learning to achieve goals via recursive classification",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.08909",
      "authors": "Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine"
    },
    {
      "index": 9,
      "title": "Planning from Pixels using Inverse Dynamics Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.02419",
      "authors": "Keiran Paster, Sheila A McIlraith, and Jimmy Ba",
      "orig_title": "Planning from pixels using inverse dynamics models",
      "paper_id": "2012.02419v1"
    },
    {
      "index": 10,
      "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Alexandre Péré, Sébastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer",
      "orig_title": "Unsupervised learning of goal spaces for intrinsically motivated goal exploration",
      "paper_id": "1803.00781v3"
    },
    {
      "index": 11,
      "title": "Bisimulation metrics for continuous markov decision processes",
      "abstract": "",
      "year": "2011",
      "venue": "SIAM Journal on Computing",
      "authors": "Norm Ferns, Prakash Panangaden, and Doina Precup"
    },
    {
      "index": 12,
      "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine",
      "orig_title": "Skew-fit: State-covering self-supervised reinforcement learning",
      "paper_id": "1903.03698v4"
    },
    {
      "index": 13,
      "title": "Provably efficient RL with Rich Observations via Latent State Decoding",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford",
      "orig_title": "Provably efficient RL with rich observations via latent state decoding",
      "paper_id": "1901.09018v3"
    },
    {
      "index": 14,
      "title": "Learning to achieve goals",
      "abstract": "",
      "year": "1993",
      "venue": "IJCAI",
      "authors": "Leslie Pack Kaelbling"
    },
    {
      "index": 15,
      "title": "Universal value function approximators",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Brockman Greg, Cheung Vicki, Pettersson Ludwig, Schneider Jonas, Schulman John, Tang Jie, and Zaremba Wojciech"
    },
    {
      "index": 17,
      "title": "multiworld",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Vitchyr Pong, Murtaza Dalal, Steven Lin, and Ashvin Nair"
    },
    {
      "index": 18,
      "title": "Wilds: A Benchmark of in-the-Wild Distribution Shifts",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.07421",
      "authors": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, et al.",
      "orig_title": "Wilds: A benchmark of in-the-wild distribution shifts",
      "paper_id": "2012.07421v3"
    },
    {
      "index": 19,
      "title": "Invariant risk minimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.02893",
      "authors": "Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz"
    },
    {
      "index": 20,
      "title": "A theory of learning from different domains",
      "abstract": "",
      "year": "2010",
      "venue": "Machine learning",
      "authors": "Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan"
    },
    {
      "index": 21,
      "title": "Domain Adversarial Neural Networks for Domain Generalization: When It Works and How to Improve",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03924",
      "authors": "Anthony Sicilia, Xingchen Zhao, and Seong Jae Hwang",
      "orig_title": "Domain adversarial neural networks for domain generalization: When it works and how to improve",
      "paper_id": "2102.03924v2"
    },
    {
      "index": 22,
      "title": "Generalizing to unseen domains via distribution matching",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.00804",
      "authors": "Isabela Albuquerque, João Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas",
      "orig_title": "Generalizing to unseen domains via distribution matching",
      "paper_id": "1911.00804v6"
    },
    {
      "index": 23,
      "title": "Total variation distance of probability measures — Wikipedia, the free encyclopedia",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wikipedia"
    },
    {
      "index": 24,
      "title": "Transferable adversarial training: A general approach to adapting deep classifiers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan"
    },
    {
      "index": 25,
      "title": "Adversarial invariant feature learning with accuracy constraint for domain generalization",
      "abstract": "",
      "year": "2019",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo"
    },
    {
      "index": 26,
      "title": "The Variational Fair Autoencoder",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S Zemel",
      "orig_title": "The variational fair autoencoder",
      "paper_id": "1511.00830v6"
    },
    {
      "index": 27,
      "title": "Deep domain generalization via conditional invariant adversarial networks",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao"
    },
    {
      "index": 28,
      "title": "Feature Alignment and Restoration for Domain Generalization and Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12009",
      "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen",
      "orig_title": "Feature alignment and restoration for domain generalization and adaptation",
      "paper_id": "2006.12009v1"
    },
    {
      "index": 29,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Scott Fujimoto, Herke Hoof, and David Meger",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 30,
      "title": "A kernel method for the two-sample problem",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research",
      "authors": "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola"
    },
    {
      "index": 31,
      "title": "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test",
      "abstract": "",
      "year": "2015",
      "venue": "Neural computation",
      "authors": "Ji Zhao and Deyu Meng"
    },
    {
      "index": 32,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 33,
      "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Michael Laskin, Aravind Srinivas, and Pieter Abbeel",
      "orig_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
      "paper_id": "2004.04136v4"
    },
    {
      "index": 34,
      "title": "Visual Reinforcement Learning with Imagined Goals",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine",
      "orig_title": "Visual reinforcement learning with imagined goals",
      "paper_id": "1807.04742v2"
    },
    {
      "index": 35,
      "title": "The distracting control suite–a challenging benchmark for reinforcement learning from pixels",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.02722",
      "authors": "Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski"
    },
    {
      "index": 36,
      "title": "Reinforcement Learning with Augmented Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.14990",
      "authors": "Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas",
      "orig_title": "Reinforcement learning with augmented data",
      "paper_id": "2004.14990v5"
    },
    {
      "index": 37,
      "title": "Roll: Visual self-supervised reinforcement learning with object reasoning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.06777",
      "authors": "Yufei Wang, Gautham Narayan Narasimhan, Xingyu Lin, Brian Okorn, and David Held"
    },
    {
      "index": 38,
      "title": "Weakly-Supervised Reinforcement Learning for Controllable Behavior",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.02860",
      "authors": "Lisa Lee, Benjamin Eysenbach, Ruslan Salakhutdinov, Chelsea Finn, et al.",
      "orig_title": "Weakly-supervised reinforcement learning for controllable behavior",
      "paper_id": "2004.02860v2"
    },
    {
      "index": 39,
      "title": "Exploration via hindsight goal generation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng"
    },
    {
      "index": 40,
      "title": "Learning to Reach Goals via Iterated Supervised Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv e-prints",
      "authors": "Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine",
      "orig_title": "Learning to reach goals via iterated supervised learning",
      "paper_id": "1912.06088v4"
    },
    {
      "index": 41,
      "title": "Automatic Goal Generation for Reinforcement Learning Agents",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel",
      "orig_title": "Automatic goal generation for reinforcement learning agents",
      "paper_id": "1705.06366v5"
    },
    {
      "index": 42,
      "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Suraj Nair and Chelsea Finn",
      "orig_title": "Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation",
      "paper_id": "1909.05829v1"
    },
    {
      "index": 43,
      "title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba"
    },
    {
      "index": 44,
      "title": "Goal-aware prediction: Learning to model what matters",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Suraj Nair, Silvio Savarese, and Chelsea Finn"
    },
    {
      "index": 45,
      "title": "Learning subgoal representation with slow dynamics",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Siyuan Li, Lulu Zheng, Jianhao Wang, and Chongjie Zhang"
    },
    {
      "index": 46,
      "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine",
      "orig_title": "Near-optimal representation learning for hierarchical reinforcement learning",
      "paper_id": "1810.01257v2"
    },
    {
      "index": 47,
      "title": "World Model as a Graph: Learning Latent Landmarks for Planning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.12491",
      "authors": "Lunjun Zhang, Ge Yang, and Bradly C Stadie",
      "orig_title": "World model as a graph: Learning latent landmarks for planning",
      "paper_id": "2011.12491v3"
    },
    {
      "index": 48,
      "title": "Data-efficient hierarchical reinforcement learning for robotic assembly control applications",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Industrial Electronics",
      "authors": "Zhimin Hou, Jiajun Fei, Yuelin Deng, and Jing Xu"
    },
    {
      "index": 49,
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare",
      "orig_title": "Deepmdp: Learning continuous latent space models for representation learning",
      "paper_id": "1906.02736v1"
    },
    {
      "index": 50,
      "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.05265",
      "authors": "Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare",
      "orig_title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning",
      "paper_id": "2101.05265v2"
    },
    {
      "index": 51,
      "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.13649",
      "authors": "Ilya Kostrikov, Denis Yarats, and Rob Fergus"
    },
    {
      "index": 52,
      "title": "Self-Supervised Policy Adaptation during Deployment",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.04309",
      "authors": "Nicklas Hansen, Yu Sun, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang",
      "orig_title": "Self-supervised policy adaptation during deployment",
      "paper_id": "2007.04309v3"
    },
    {
      "index": 53,
      "title": "A Geometric Perspective on Self-Supervised Policy Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.07318",
      "authors": "Cristian Bodnar, Karol Hausman, Gabriel Dulac-Arnold, and Rico Jonschkowski",
      "orig_title": "A geometric perspective on self-supervised policy adaptation",
      "paper_id": "2011.07318v1"
    },
    {
      "index": 54,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 55,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Christopher JCH Watkins and Peter Dayan"
    }
  ]
}