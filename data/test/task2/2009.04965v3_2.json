{
  "paper_id": "2009.04965v3",
  "title": "Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations",
  "sections": {
    "-a results on visual genome": "We provide additional experimental results on Visual Genome (VG) dataset . We follow [ref]19 [ref]21  to adopt the most widely-used dataset split which consists of 108K images and includes the most frequent 150 object classes and 50 predicates.\nWhen evaluating visual relationship detection/scene graph generation on VG, there are three common evaluation modes including (1) Predicate Classification (PredCls): ground truth bounding boxes and object labels are given, (2) Scene Graph Classification (SGCls): only ground truth boxes given, and (3) Scene Graph Detection (SGDet): nothing other than input images is given. We experiment with PredCls, which is a similar setting to what we perform on VRD dataset  and SpatialSense  dataset. \n\n\n\nModel\nRecall@50\nRecall@100\n\n\n\nJoint CNN \n27.9\n35.0\n\nGraph R-CNN \n54.2\n59.1\n\nMessage Passing [ref]19\n59.3\n61.3\n\nFreq [ref]21\n59.9\n64.1\n\nFreq+Overlap [ref]21\n60.6\n62.2\n\nSMN [ref]21\n65.2\n67.1\n\nKERN [ref]25\n65.8\n67.7\n\nVCTree \n66.4\n68.1\n\nRVL-BERT\n62.9\n66.6\n\n Comparison results on VG are presented in Table V, where our proposed RVL-BERT achieves competitive results.\nNote that as mentioned in section IV-A, visual relationship detection can be biased as predicates could be ”guessed” accurately given explicit correlations between object labels and predicates.\nBoth SMN [ref]21 and KERN [ref]25 exploit this property and use the frequency bias and object co-occurrence, respectively.\nHowever, the usage of bias could reversely undermine the capability of generalization which has been demonstrated by comparing mean recall in recent works (e.g., )."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Recognition using visual phrases",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. A. Sadeghi and A. Farhadi"
    },
    {
      "index": 1,
      "title": "Understanding kin relationships in a photo",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "S. Xia, M. Shao, J. Luo, and Y. Fu"
    },
    {
      "index": 2,
      "title": "Visual Relationship Detection with Language Priors",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Computer Vision",
      "authors": "C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei",
      "orig_title": "Visual relationship detection with language priors",
      "paper_id": "1608.00187v1"
    },
    {
      "index": 3,
      "title": "Vip-cnn: Visual phrase guided convolutional neural network",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Y. Li, W. Ouyang, X. Wang, and X. Tang"
    },
    {
      "index": 4,
      "title": "Towards context-aware interaction recognition for visual relationship detection",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "B. Zhuang, L. Liu, C. Shen, and I. Reid"
    },
    {
      "index": 5,
      "title": "Detecting Visual Relationships with Deep Relational Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "B. Dai, Y. Zhang, and D. Lin",
      "orig_title": "Detecting visual relationships with deep relational networks",
      "paper_id": "1704.03114v2"
    },
    {
      "index": 6,
      "title": "Visual Translation Embedding Network for Visual Relation Detection",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua",
      "orig_title": "Visual translation embedding network for visual relation detection",
      "paper_id": "1702.08319v1"
    },
    {
      "index": 7,
      "title": "Weakly-supervised learning of visual relations",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "J. Peyre, J. Sivic, I. Laptev, and C. Schmid"
    },
    {
      "index": 8,
      "title": "Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "R. Yu, A. Li, V. I. Morariu, and L. S. Davis",
      "orig_title": "Visual relationship detection with internal and external linguistic knowledge distillation",
      "paper_id": "1707.09423v2"
    },
    {
      "index": 9,
      "title": "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang",
      "orig_title": "Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn",
      "paper_id": "1708.01956v1"
    },
    {
      "index": 10,
      "title": "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, J. Shao, and C. Change Loy",
      "orig_title": "Zoom-net: Mining deep feature interactions for visual relationship recognition",
      "paper_id": "1807.04979v1"
    },
    {
      "index": 11,
      "title": "Tensorize, factorize and regularize: Robust visual relationship learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Jae Hwang, S. N. Ravi, Z. Tao, H. J. Kim, M. D. Collins, and V. Singh"
    },
    {
      "index": 12,
      "title": "Visual relationship detection with language prior and softmax",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE International Conference on Image Processing, Applications and Systems (IPAS)",
      "authors": "J. Jung and J. Park"
    },
    {
      "index": 13,
      "title": "Union visual translation embedding for visual relationship detection and scene graph generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.11624",
      "authors": "Z.-S. Hung, A. Mallya, and S. Lazebnik"
    },
    {
      "index": 14,
      "title": "Scene graph generation with external knowledge and image reconstruction",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Gu, H. Zhao, Z. Lin, S. Li, J. Cai, and M. Ling"
    },
    {
      "index": 15,
      "title": "On Exploring Undetermined Relationships for Visual Relationship Detection",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Y. Zhan, J. Yu, T. Yu, and D. Tao",
      "orig_title": "On exploring undetermined relationships for visual relationship detection",
      "paper_id": "1905.01595v1"
    },
    {
      "index": 16,
      "title": "Exploring depth information for spatial relation recognition",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)",
      "authors": "X. Ding, Y. Li, Y. Pan, D. Zeng, and T. Yao"
    },
    {
      "index": 17,
      "title": "Hierarchical graph attention network for visual relationship detection",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "L. Mi and Z. Chen"
    },
    {
      "index": 18,
      "title": "Scene Graph Generation by Iterative Message Passing",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei",
      "orig_title": "Scene graph generation by iterative message passing",
      "paper_id": "1701.02426v2"
    },
    {
      "index": 19,
      "title": "Scene Graph Generation from Objects, Phrases and Region Captions",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Y. Li, W. Ouyang, B. Zhou, K. Wang, and X. Wang",
      "orig_title": "Scene graph generation from objects, phrases and region captions",
      "paper_id": "1707.09700v2"
    },
    {
      "index": 20,
      "title": "Neural Motifs: Scene Graph Parsing with Global Context",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "R. Zellers, M. Yatskar, S. Thomson, and Y. Choi",
      "orig_title": "Neural motifs: Scene graph parsing with global context",
      "paper_id": "1711.06640v2"
    },
    {
      "index": 21,
      "title": "Graph R-CNN for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh",
      "orig_title": "Graph r-cnn for scene graph generation",
      "paper_id": "1808.00191v1"
    },
    {
      "index": 22,
      "title": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision",
      "authors": "Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang",
      "orig_title": "Factorizable net: an efficient subgraph-based framework for scene graph generation",
      "paper_id": "1806.11538v2"
    },
    {
      "index": 23,
      "title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "R. Herzig, M. Raboh, G. Chechik, J. Berant, and A. Globerson",
      "orig_title": "Mapping images to scene graphs with permutation-invariant structured prediction",
      "paper_id": "1802.05451v4"
    },
    {
      "index": 24,
      "title": "Knowledge-Embedded Routing Network for Scene Graph Generation",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "T. Chen, W. Yu, R. Chen, and L. Lin",
      "orig_title": "Knowledge-embedded routing network for scene graph generation",
      "paper_id": "1903.03326v1"
    },
    {
      "index": 25,
      "title": "Graphical Contrastive Losses for Scene Graph Parsing",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Zhang, K. J. Shih, A. Elgammal, A. Tao, and B. Catanzaro",
      "orig_title": "Graphical contrastive losses for scene graph parsing",
      "paper_id": "1903.02728v5"
    },
    {
      "index": 26,
      "title": "Attentive Relational Networks for Mapping Images to Scene Graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "M. Qi, W. Li, Z. Yang, Y. Wang, and J. Luo",
      "orig_title": "Attentive relational networks for mapping images to scene graphs",
      "paper_id": "1811.10696v2"
    },
    {
      "index": 27,
      "title": "Bridging knowledge graphs to generate scene graphs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.02314",
      "authors": "A. Zareian, S. Karaman, and S.-F. Chang"
    },
    {
      "index": 28,
      "title": "Unbiased Scene Graph Generation from Biased Training",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "K. Tang, Y. Niu, J. Huang, J. Shi, and H. Zhang",
      "orig_title": "Unbiased scene graph generation from biased training",
      "paper_id": "2002.11949v3"
    },
    {
      "index": 29,
      "title": "Know more say less: Image captioning based on scene graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "X. Li and S. Jiang"
    },
    {
      "index": 30,
      "title": "Visual relationship embedding network for image paragraph generation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "W. Che, X. Fan, R. Xiong, and D. Zhao"
    },
    {
      "index": 31,
      "title": "Graph-Structured Representations for Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "D. Teney, L. Liu, and A. van Den Hengel",
      "orig_title": "Graph-structured representations for visual question answering",
      "paper_id": "1609.05600v2"
    },
    {
      "index": 32,
      "title": "Explainable and Explicit Visual Reasoning over Scene Graphs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Shi, H. Zhang, and J. Li",
      "orig_title": "Explainable and explicit visual reasoning over scene graphs",
      "paper_id": "1812.01855v2"
    },
    {
      "index": 33,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 34,
      "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "J. Lu, D. Batra, D. Parikh, and S. Lee"
    },
    {
      "index": 35,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "P. Sharma, N. Ding, S. Goodman, and R. Soricut"
    },
    {
      "index": 36,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai"
    },
    {
      "index": 37,
      "title": "Interact as You Intend: Intention-Driven Human-Object Interaction Detection",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "B. Xu, J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli",
      "orig_title": "Interact as you intend: Intention-driven human-object interaction detection",
      "paper_id": "1808.09796v2"
    },
    {
      "index": 38,
      "title": "Context-associative hierarchical memory model for human activity recognition and prediction",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "L. Wang, X. Zhao, Y. Si, L. Cao, and Y. Liu"
    },
    {
      "index": 39,
      "title": "Image retrieval using scene graphs",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein, and L. Fei-Fei"
    },
    {
      "index": 40,
      "title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval",
      "abstract": "",
      "year": "2015",
      "venue": "fourth Workshop on Vision and Language",
      "authors": "S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning"
    },
    {
      "index": 41,
      "title": "Ask me anything: Dynamic memory networks for natural language processing",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani, V. Zhong, R. Paulus, and R. Socher"
    },
    {
      "index": 42,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 43,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
      "authors": "M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer",
      "orig_title": "Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 44,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.14165",
      "authors": "T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 45,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler"
    },
    {
      "index": 46,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 47,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.08415",
      "authors": "D. Hendrycks and K. Gimpel",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 48,
      "title": "SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "K. Yang, O. Russakovsky, and J. Deng",
      "orig_title": "Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition",
      "paper_id": "1908.02660v2"
    },
    {
      "index": 49,
      "title": "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1609.08144",
      "authors": "Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al.",
      "orig_title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 50,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "K. He, G. Gkioxari, P. Dollár, and R. Girshick",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 51,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International Journal of Computer Vision",
      "authors": "R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al."
    },
    {
      "index": 52,
      "title": "Fast R-CNN",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "R. Girshick",
      "orig_title": "Fast r-cnn",
      "paper_id": "1504.08083v2"
    },
    {
      "index": 53,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "S. Ren, K. He, R. Girshick, and J. Sun",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 54,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 55,
      "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "K. Tang, H. Zhang, B. Wu, W. Luo, and W. Liu",
      "orig_title": "Learning to compose dynamic tree structures for visual contexts",
      "paper_id": "1812.01880v1"
    }
  ]
}