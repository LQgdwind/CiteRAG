{
  "paper_id": "2102.04252v3",
  "title": "HINT: Hierarchical Interaction Network for Clinical Trial Outcome Prediction",
  "sections": {
    "experimental setting": "Implementation Details\nWe use our TOP benchmark for model evaluation.\nThe implementation details are described below.\nThe curated dataset and code repository is available at 888https://github.com/futianfan/clinical-trial-outcome-prediction. ∙∙\\bullet Molecule Embedding (Section 3.3). Regarding the molecule embedding function fm​(⋅)subscript𝑓𝑚⋅f_{m}(\\cdot) in Eq. (5), HINT supports Morgan fingerprint , SMILES encoder [ref]10, graph message passing neural network (MPNN)  [ref]19  , we choose MPNN because it usually works better in our experiments.\nThe depth of MPNN is 3, with hidden layer dimension 100. The input feature of MPNN is molecular graph with atom and bond features. Following  [ref]19 , atom features is a 38 dimensional vector, including its atom type (23 dim, 22 frequent atoms and 1 unknown indicator), degree (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}), its formal charge (5-dim one-hot vector, {−1,−2,1,2,0}12120\\{-1,-2,1,2,0\\}) and its chiral configuration (4-dim one-hot vector, {0,1,2,3}0123\\{0,1,2,3\\}). Bond feature is a 11 dim vector, which is concatenation of its bond type (4-dim one-hot vector, {single, double, triple, aromatic}single, double, triple, aromatic\\{\\text{single, double, triple, aromatic}\\}), whether the bond is in a ring (1-dim), and its cis-trans configuration (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}).\n\n∙∙\\bullet Disease Embedding (Section 3.3). Disease embedding is obtained by GRAM  , as defined in Eq. (6). The embedding size of both 𝐡dsubscript𝐡𝑑\\mathbf{h}_{d} (Eq. 6) and 𝐞𝐞\\mathbf{e} (Eq. 7) are both 100. Following , the attention model defined in Eq. (8) is feed-forward network with a single hidden layer (with dimension 100).\n\n∙∙\\bullet Protocol Embedding (Section 3.3). 𝐬iIsubscriptsuperscript𝐬𝐼𝑖\\mathbf{s}^{I}_{i} and 𝐬iEsubscriptsuperscript𝐬𝐸𝑖\\mathbf{s}^{E}_{i} in Eq. (9) are both 768 dimensional vectors. Following , the kernel size k1,k2,k3,k4subscript𝑘1subscript𝑘2subscript𝑘3subscript𝑘4k_{1},k_{2},k_{3},k_{4} in Eq. (10) is set to 1,3,5,7, respectively.\n∙∙\\bullet Interaction Graph (Section 3.5). All the node embedding in Interaction Graph have same dimension to support graph neural network reasoning. The dimension is set to 100. The neural architecture of the connections between different nodes are one-layer fully-connected neural network followed by a two-layer highway network. The input dimension of fully-connected neural network is determined by the number of input node number while the output dimension is 100, equal to the hidden dimension of highway network and node dimension in interaction graph. For example, to obtain pharmaco-kinetics node’s embedding in Eq. (20), the input dimension of one-layer fully-connected neural network is 500=5∗1005005100500=5*100 (𝐡A,𝐡D,𝐡M,𝐡E,𝐡Tsubscript𝐡𝐴subscript𝐡𝐷subscript𝐡𝑀subscript𝐡𝐸subscript𝐡𝑇\\mathbf{h}_{A},\\mathbf{h}_{D},\\mathbf{h}_{M},\\mathbf{h}_{E},\\mathbf{h}_{T}), the output dimension is 100. To obtain Interaction node’s embedding in Eq. (21), the input dimension of one-layer fully-connected NN is 300=3∗1003003100300=3*100 (𝐡d,𝐡m,𝐡psubscript𝐡𝑑subscript𝐡𝑚subscript𝐡𝑝\\mathbf{h}_{d},\\mathbf{h}_{m},\\mathbf{h}_{p}).\n\n∙∙\\bullet Graph Neural Architecture (Section 3.5).\nThe hidden size of GCN in Eq. (25) is set to 200. The dropout rate is set to 0.6. The feature dimension of GCN is 100, equal to node’s embedding size. The graph attention model defined in Eq. (26) is a two-layer fully-connected neural network with output dimension 1 with sigmoid activation. The input size is 200=2∗1002002100200=2*100 (concatenation of two nodes’ embedding), the hidden size is 50, the output dimension is 1. As shown in Eq. (26), the output is a scalar, 𝐕i,j∈ℝ+subscript𝐕𝑖𝑗subscriptℝ\\mathbf{V}_{i,j}\\in\\mathbb{R}_{+}. The depth of GNN L𝐿L is set to 3.\n\n∙∙\\bullet Learning. During both pre-training and training procedure, we use the Adam as optimizer [ref]28. The learning rate is selected from {1​e−4,5​e−4,1​e−3}1superscript𝑒45superscript𝑒41superscript𝑒3\\{1e^{-4},5e^{-4},1e^{-3}\\} and tuned on validation set. When pretraining ADMET models, the learning rate is set to 5​e−45superscript𝑒45e^{-4}. When pretraining disease risk model, learning rate is set to 1​e−31superscript𝑒31e^{-3}. When training HINT, the learning rate is set to 5​e−45superscript𝑒45e^{-4}. The maximal epochs are set to 10. We observe all the models converged within maximal epochs.\nWe save the model every epoch and choose the model that performs best on validation set. All hyperparameters are tuned on validation set. Evaluation Settings We consider two realistic evaluation setups. The first is phase-level evaluation where we predict the outcome of a single phase study. Since each phase has different goals (e.g. Phase I is for safety whereas Phase II and III is for efficacy), we conduct evaluation for Phase I, II and III individually. We create the test datasets using the FDA guideline999https://www.fda.gov/patients/drug-development-process/step-3-clinical-research on the success-failure ratio for each phase, specifically 70% success rate for Phase I, 33% for Phase II and 30% for Phase III. Second, we also consider indication-level evaluation where we test if the drug can pass all three phases for the final market approval. To imitate it, we assemble all phase studies related to the drug and disease of the study and then use the latest phase protocol as the input to our model. Drugs that have Phase III success are labelled positive and other drugs that fail in any of the three phases are labelled negative. Data statistics are shown in Table 2. Evaluation Metrics\nWe use the following metrics to measure the performance of all methods.\n\n∙∙\\bullet PR-AUC (Precision-Recall Area Under Curve).\n\n∙∙\\bullet F1. The F1 score is the harmonic mean of the precision and recall.\n\n∙∙\\bullet ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n\n∙∙\\bullet p-value. We report the results of hypothesis testing in terms of p-value to showcase the statistical significance of our method over the best baseline results. If p-value is smaller than 0.05, we claim our method significantly outperforms the best baseline method. For PR-AUC, F1, and ROC-AUC, higher value represent better performance.\nWe split the dataset based on registration date. The earlier trials are used for learning while the later trials are used for inference. For example, for Phase I dataset, we learn the model using the trials before Aug 13th, 2014 and make inference using the trials after that date, as shown in Table 2.\nWe use Bootstrap  to estimate the mean and standard deviation of accuracy on test set. Baselines. We compare HINT with several baselines, including both conventional machine learning models and deep learning methods. LR (Logistic Regression). It was used in  on trial prediction with disease features only. For fair comparison, we adapt it such that the input features include (i) 1024 dimensional Morgan fingerprint feature , (ii) GRAM embedding (Eq. 7, GRAM is pretrained using disease risk module Eq. 18) and (iii) BERT embedding of eligibility criteria for protocol. Then these three features are concatenated as the input of LR model. RF (Random Forest). Similarly to LR, it was used in  on trial prediction and we adapt it to use the same feature set. XGBoost. An implementation of gradient boosted decision trees designed for speed and performance. It was used in context of individual patient trial outcome prediction in . We adapt it to use the same feature set for general trial outcome prediction. AdaBoost (Adaptive Boosting). It was used in  for individual Alzheimer’s patient’s trial result prediction. We adapt it to use the same feature set. kNN+RF.  leverages statistical imputation techniques to handle missing data, and finds using (1) kNN (k-Nearest Neighbor) as imputation technique and (2) Random Forest as classifier would achieve best performance. We adapt this method to use the same feature set. FFNN (Feed-Forward Neural Network) . It uses the same feature with LR. The feature vectors are fed into a three-layer feedforward neural network. DeepEnroll . DeepEnroll was originally designed for patient trial matching, it uses (1) pre-trained BERT model  to encode eligibility criteria into sentence embedding; (2) a hierarchical embedding model to disease information and (3) alignment model to capture the protocol-disease interaction information. To adapt it to our scenario, molecule embedding (𝐡msubscript𝐡𝑚\\mathbf{h}_{m}) is concatenated to the output of alignment model to make prediction. COMPOSE (cross-modal pseudo-siamese network) . COMPOSE was also originally designed for patient trial matching, it uses convolutional highway network and memory network to encode eligibility criteria and diseases respectively and an alignment model to model the interaction. COMPOSE incorporate the molecule information in the same way with DeepEnroll, as mentioned above."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Blood-brain barrier permeation models: discriminating between potential cns and non-cns drugs including p-glycoprotein substrates",
      "abstract": "",
      "year": "2004",
      "venue": "Journal of chemical information and computer sciences",
      "authors": "Marc Adenot and Roger Lahana"
    },
    {
      "index": 1,
      "title": "Publicly Available Clinical BERT Embeddings",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.03323",
      "authors": "Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott",
      "orig_title": "Publicly available clinical bert embeddings",
      "paper_id": "1904.03323v3"
    },
    {
      "index": 2,
      "title": "Welcome to the icd-10 code for sarcopenia",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of cachexia, sarcopenia and muscle",
      "authors": "Stefan D Anker et al."
    },
    {
      "index": 3,
      "title": "Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes",
      "abstract": "",
      "year": "2016",
      "venue": "BioRxiv",
      "authors": "Artem V Artemov et al."
    },
    {
      "index": 4,
      "title": "Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "S. Biswal et al.",
      "orig_title": "Doctor2vec: Dynamic doctor representation learning for clinical trial recruitment",
      "paper_id": "1911.10395v1"
    },
    {
      "index": 5,
      "title": "Molecular fingerprint similarity search in virtual screening",
      "abstract": "",
      "year": "2015",
      "venue": "Methods",
      "authors": "Adrià Cereto-Massagué et al."
    },
    {
      "index": 6,
      "title": "Bootstrap methods",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Michael R Chernick et al."
    },
    {
      "index": 7,
      "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "KDD",
      "authors": "Edward Choi et al.",
      "orig_title": "Gram: graph-based attention model for healthcare representation learning",
      "paper_id": "1611.07012v3"
    },
    {
      "index": 8,
      "title": "Convolutional embedding of attributed molecular graphs for physical property prediction",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of chemical information and modeling",
      "authors": "Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen"
    },
    {
      "index": 9,
      "title": "Discriminative embeddings of latent variable models for structured data",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "Hanjun Dai et al."
    },
    {
      "index": 10,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Jacob Devlin et al.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 11,
      "title": "Admetlab: a platform for systematic admet evaluation based on a comprehensively collected admet database",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of cheminformatics",
      "authors": "Jie Dong et al."
    },
    {
      "index": 12,
      "title": "Interpretable Molecular Graph Generation via Monotonic Constraints",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.00412",
      "authors": "Yuanqi Du, Xiaojie Guo, Amarda Shehu, and Liang Zhao",
      "orig_title": "Interpretable molecular graph generation via monotonic constraints",
      "paper_id": "2203.00412v1"
    },
    {
      "index": 13,
      "title": "Application of kpca and adaboost algorithm in classification of functional magnetic resonance imaging of alzheimer’s disease",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "authors": "Zhao Fan, Fanyu Xu, Cai Li, and Lili Yao"
    },
    {
      "index": 14,
      "title": "Fundamentals of clinical trials",
      "abstract": "",
      "year": "2015",
      "venue": "Springer",
      "authors": "Lawrence M Friedman, Curt D Furberg, David L DeMets, David M Reboussin, and Christopher B Granger"
    },
    {
      "index": 15,
      "title": "Differentiable Scaffolding Tree for Molecular Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun",
      "orig_title": "Differentiable scaffolding tree for molecular optimization",
      "paper_id": "2109.10469v2"
    },
    {
      "index": 16,
      "title": "MIMOSA: Multi-constraint molecule sampling for molecule optimization",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Tianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, and Jimeng Sun"
    },
    {
      "index": 17,
      "title": "Probabilistic and dynamic molecule-disease interaction modeling for drug discovery",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
      "authors": "Tianfan Fu, Cao Xiao, Cheng Qian, Lucas M Glass, and Jimeng Sun"
    },
    {
      "index": 18,
      "title": "CORE: Automatic Molecule Optimization Using Copy & Refine Strategy",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Tianfan Fu, Cao Xiao, and Jimeng Sun",
      "orig_title": "Core: Automatic molecule optimization using copy & refine strategy",
      "paper_id": "1912.05910v1"
    },
    {
      "index": 19,
      "title": "COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching",
      "abstract": "",
      "year": "2020",
      "venue": "KDD",
      "authors": "Junyi Gao, Cao Xiao, Lucas M Glass, and Jimeng Sun",
      "orig_title": "Compose: Cross-modal pseudo-siamese network for patient trial matching",
      "paper_id": "2006.08765v1"
    },
    {
      "index": 20,
      "title": "A data-driven approach to predicting successes and failures of clinical trials",
      "abstract": "",
      "year": "2016",
      "venue": "Cell chemical biology",
      "authors": "Kaitlyn M Gayvert, Neel S Madhukar, and Olivier Elemento"
    },
    {
      "index": 21,
      "title": "Modeling admet",
      "abstract": "",
      "year": "2016",
      "venue": "In Silico Methods for Predicting Drug Toxicity",
      "authors": "Jayeeta Ghosh et al."
    },
    {
      "index": 22,
      "title": "Clinical development success rates for investigational drugs",
      "abstract": "",
      "year": "2014",
      "venue": "Nat. Biotechnol.",
      "authors": "Michael Hay et al."
    },
    {
      "index": 23,
      "title": "Predicting successes and failures of clinical trials with an ensemble ls-svr",
      "abstract": "",
      "year": "2020",
      "venue": "medRxiv",
      "authors": "Zhen Yu Hong, Jooyong Shim, Woo Chan Son, and Changha Hwang"
    },
    {
      "index": 24,
      "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1904.05342",
      "authors": "Kexin Huang et al.",
      "orig_title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
      "paper_id": "1904.05342v3"
    },
    {
      "index": 25,
      "title": "DeepPurpose: A Deep Learning Library for Drug-Target Interaction Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Bioinformatics",
      "authors": "Kexin Huang et al.",
      "orig_title": "Deeppurpose: a deep learning library for drug–target interaction prediction",
      "paper_id": "2004.08919v3"
    },
    {
      "index": 26,
      "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Wengong Jin, Regina Barzilay, and Tommi Jaakkola",
      "orig_title": "Junction tree variational autoencoder for molecular graph generation",
      "paper_id": "1802.04364v4"
    },
    {
      "index": 27,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 28,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Thomas N Kipf and Max Welling",
      "orig_title": "Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 29,
      "title": "4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. nature looks at ways they might be saved",
      "abstract": "",
      "year": "2011",
      "venue": "Nature",
      "authors": "Heidi Ledford"
    },
    {
      "index": 30,
      "title": "Machine learning with statistical imputation for predicting drug approvals",
      "abstract": "",
      "year": "2019",
      "venue": "Harvard Data Science Review",
      "authors": "Andrew W. Lo, Kien Wei Siah, and Chi Heem Wong"
    },
    {
      "index": 31,
      "title": "Prediction models of human plasma protein binding rate and oral bioavailability derived by using ga–cg–svm method",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of pharmaceutical and biomedical analysis",
      "authors": "Chang-Ying Ma et al."
    },
    {
      "index": 32,
      "title": "How much do clinical trials cost?",
      "abstract": "",
      "year": "2017",
      "venue": "Nat. Rev. Drug Discov.",
      "authors": "Linda Martin et al."
    },
    {
      "index": 33,
      "title": "Clinical trial methodology",
      "abstract": "",
      "year": "1978",
      "venue": "Nature",
      "authors": "Richard Peto"
    },
    {
      "index": 34,
      "title": "e-drug3d: 3d structure collections dedicated to drug repurposing and fragment-based drug design",
      "abstract": "",
      "year": "2012",
      "venue": "Bioinformatics",
      "authors": "Emilie Pihan et al."
    },
    {
      "index": 35,
      "title": "Predicting phase 3 clinical trial results by modeling phase 2 clinical trial subject level data using deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of Machine Learning Research",
      "authors": "Youran Qi and Qi Tang"
    },
    {
      "index": 36,
      "title": "Evaluation of a Machine Learning Model Based on Pretreatment Symptoms and Electroencephalographic Features to Predict Outcomes of Antidepressant Treatment in Adults With Depression: A Prespecified Secondary Analysis of a Randomized Clinical Trial",
      "abstract": "",
      "year": "2020",
      "venue": "JAMA Network Open",
      "authors": "Pranav Rajpurkar et al."
    },
    {
      "index": 37,
      "title": "Toxcast chemical landscape: paving the road to 21st century toxicology",
      "abstract": "",
      "year": "2016",
      "venue": "Chemical research in toxicology",
      "authors": "Ann M Richard et al."
    },
    {
      "index": 38,
      "title": "Training very deep networks",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "Rupesh Kumar Srivastava et al."
    },
    {
      "index": 39,
      "title": "Clinical development success rates 2006–2015",
      "abstract": "",
      "year": "2016",
      "venue": "BIO Industry Analysis",
      "authors": "David W Thomas, Justin Burns, John Audette, Adam Carroll, Corey Dow-Hygelund, and Michael Hay"
    },
    {
      "index": 40,
      "title": "A deep neural network approach to predicting clinical outcomes of neuroblastoma patients",
      "abstract": "",
      "year": "2019",
      "venue": "bioRxiv",
      "authors": "Léon-Charles Tranchevent, Francisco Azuaje, and Jagath C. Rajapakse"
    },
    {
      "index": 41,
      "title": "Comprehensive characterization of cytochrome p450 isozyme selectivity across chemical libraries",
      "abstract": "",
      "year": "2009",
      "venue": "Nature biotechnology",
      "authors": "Henrike Veith, Noel Southall, Ruili Huang, Tim James, Darren Fayne, Natalia Artemenko, Min Shen, James Inglese, Christopher P Austin, David G Lloyd, et al."
    },
    {
      "index": 42,
      "title": "SMILES-BERT: large scale unsupervised pre-training for molecular property prediction",
      "abstract": "",
      "year": "2019",
      "venue": "ACM BCB",
      "authors": "Sheng Wang et al."
    },
    {
      "index": 43,
      "title": "Bitcoin Transaction Forecasting with Deep Network Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": "Wenqi Wei, Qi Zhang, and Ling Liu",
      "orig_title": "Bitcoin transaction forecasting with deep network representation learning",
      "paper_id": "2007.07993v2"
    },
    {
      "index": 44,
      "title": "Drugbank 5.0: a major update to the drugbank database for 2018",
      "abstract": "",
      "year": "2018",
      "venue": "Nucleic acids research",
      "authors": "David S Wishart et al."
    },
    {
      "index": 45,
      "title": "Identifying the status of genetic lesions in cancer clinical trial documents using machine learning",
      "abstract": "",
      "year": "2012",
      "venue": "BMC genomics",
      "authors": "Yonghui Wu et al."
    },
    {
      "index": 46,
      "title": "Stochastic gradient boosted distributed decision trees",
      "abstract": "",
      "year": "2009",
      "venue": "CIKM",
      "authors": "Jerry Ye, Jyh-Herng Chow, Jiang Chen, and Zhaohui Zheng"
    },
    {
      "index": 47,
      "title": "End-to-end convolutional semantic embeddings",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Quanzeng You, Zhengyou Zhang, and Jiebo Luo"
    },
    {
      "index": 48,
      "title": "Patient-trial matching with deep embedding and entailment prediction",
      "abstract": "",
      "year": "2020",
      "venue": "WWW",
      "authors": "Xingyao Zhang et al."
    },
    {
      "index": 49,
      "title": "PyHealth: A Python Library for Health Predictive Models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.04209",
      "authors": "Yue Zhao, Zhi Qiao, Cao Xiao, Lucas Glass, and Jimeng Sun",
      "orig_title": "Pyhealth: A python library for health predictive models",
      "paper_id": "2101.04209v1"
    },
    {
      "index": 50,
      "title": "Network representation learning: From preprocessing, feature extraction to node embedding",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "Jingya Zhou, Ling Liu, Wenqi Wei, and Jianxi Fan"
    }
  ]
}