{
  "paper_id": "2102.04252v3",
  "title": "HINT: Hierarchical Interaction Network for Clinical Trial Outcome Prediction",
  "sections": {
    "experimental setting": "Implementation Details\nWe use our TOP benchmark for model evaluation.\nThe implementation details are described below.\nThe curated dataset and code repository is available at 888https://github.com/futianfan/clinical-trial-outcome-prediction. âˆ™âˆ™\\bullet Molecule Embedding (SectionÂ 3.3). Regarding the molecule embedding function fmâ€‹(â‹…)subscriptğ‘“ğ‘šâ‹…f_{m}(\\cdot) in Eq.Â (5), HINT supports Morgan fingerprintÂ , SMILES encoderÂ [ref]10, graph message passing neural network (MPNN)Â  [ref]19  , we choose MPNN because it usually works better in our experiments.\nThe depth of MPNN is 3, with hidden layer dimension 100. The input feature of MPNN is molecular graph with atom and bond features. FollowingÂ  [ref]19 , atom features is a 38 dimensional vector, including its atom type (23 dim, 22 frequent atoms and 1 unknown indicator), degree (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}), its formal charge (5-dim one-hot vector, {âˆ’1,âˆ’2,1,2,0}12120\\{-1,-2,1,2,0\\}) and its chiral configuration (4-dim one-hot vector, {0,1,2,3}0123\\{0,1,2,3\\}). Bond feature is a 11 dim vector, which is concatenation of its bond type (4-dim one-hot vector, {single, double, triple, aromatic}single, double, triple, aromatic\\{\\text{single, double, triple, aromatic}\\}), whether the bond is in a ring (1-dim), and its cis-trans configuration (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}).\n\nâˆ™âˆ™\\bullet Disease Embedding (SectionÂ 3.3). Disease embedding is obtained by GRAMÂ  , as defined in Eq.Â (6). The embedding size of both ğ¡dsubscriptğ¡ğ‘‘\\mathbf{h}_{d} (Eq.Â 6) and ğğ\\mathbf{e} (Eq.Â 7) are both 100. Following , the attention model defined in Eq.Â (8) is feed-forward network with a single hidden layer (with dimension 100).\n\nâˆ™âˆ™\\bullet Protocol Embedding (SectionÂ 3.3). ğ¬iIsubscriptsuperscriptğ¬ğ¼ğ‘–\\mathbf{s}^{I}_{i} and ğ¬iEsubscriptsuperscriptğ¬ğ¸ğ‘–\\mathbf{s}^{E}_{i} in Eq.Â (9) are both 768 dimensional vectors. FollowingÂ , the kernel size k1,k2,k3,k4subscriptğ‘˜1subscriptğ‘˜2subscriptğ‘˜3subscriptğ‘˜4k_{1},k_{2},k_{3},k_{4} in Eq.Â (10) is set to 1,3,5,7, respectively.\nâˆ™âˆ™\\bullet Interaction Graph (SectionÂ 3.5). All the node embedding in Interaction Graph have same dimension to support graph neural network reasoning. The dimension is set to 100. The neural architecture of the connections between different nodes are one-layer fully-connected neural network followed by a two-layer highway network. The input dimension of fully-connected neural network is determined by the number of input node number while the output dimension is 100, equal to the hidden dimension of highway network and node dimension in interaction graph. For example, to obtain pharmaco-kinetics nodeâ€™s embedding in Eq.Â (20), the input dimension of one-layer fully-connected neural network is 500=5âˆ—1005005100500=5*100 (ğ¡A,ğ¡D,ğ¡M,ğ¡E,ğ¡Tsubscriptğ¡ğ´subscriptğ¡ğ·subscriptğ¡ğ‘€subscriptğ¡ğ¸subscriptğ¡ğ‘‡\\mathbf{h}_{A},\\mathbf{h}_{D},\\mathbf{h}_{M},\\mathbf{h}_{E},\\mathbf{h}_{T}), the output dimension is 100. To obtain Interaction nodeâ€™s embedding in Eq.Â (21), the input dimension of one-layer fully-connected NN is 300=3âˆ—1003003100300=3*100 (ğ¡d,ğ¡m,ğ¡psubscriptğ¡ğ‘‘subscriptğ¡ğ‘šsubscriptğ¡ğ‘\\mathbf{h}_{d},\\mathbf{h}_{m},\\mathbf{h}_{p}).\n\nâˆ™âˆ™\\bullet Graph Neural Architecture (SectionÂ 3.5).\nThe hidden size of GCN in Eq.Â (25) is set to 200. The dropout rate is set to 0.6. The feature dimension of GCN is 100, equal to nodeâ€™s embedding size. The graph attention model defined in Eq.Â (26) is a two-layer fully-connected neural network with output dimension 1 with sigmoid activation. The input size is 200=2âˆ—1002002100200=2*100 (concatenation of two nodesâ€™ embedding), the hidden size is 50, the output dimension is 1. As shown in Eq.Â (26), the output is a scalar, ğ•i,jâˆˆâ„+subscriptğ•ğ‘–ğ‘—subscriptâ„\\mathbf{V}_{i,j}\\in\\mathbb{R}_{+}. The depth of GNN Lğ¿L is set to 3.\n\nâˆ™âˆ™\\bullet Learning. During both pre-training and training procedure, we use the Adam as optimizerÂ [ref]28. The learning rate is selected from {1â€‹eâˆ’4,5â€‹eâˆ’4,1â€‹eâˆ’3}1superscriptğ‘’45superscriptğ‘’41superscriptğ‘’3\\{1e^{-4},5e^{-4},1e^{-3}\\} and tuned on validation set. When pretraining ADMET models, the learning rate is set to 5â€‹eâˆ’45superscriptğ‘’45e^{-4}. When pretraining disease risk model, learning rate is set to 1â€‹eâˆ’31superscriptğ‘’31e^{-3}. When training HINT, the learning rate is set to 5â€‹eâˆ’45superscriptğ‘’45e^{-4}. The maximal epochs are set to 10. We observe all the models converged within maximal epochs.\nWe save the model every epoch and choose the model that performs best on validation set. All hyperparameters are tuned on validation set. Evaluation Settings We consider two realistic evaluation setups. The first is phase-level evaluation where we predict the outcome of a single phase study. Since each phase has different goals (e.g. Phase I is for safety whereas Phase II and III is for efficacy), we conduct evaluation for Phase I, II and III individually. We create the test datasets using the FDA guideline999https://www.fda.gov/patients/drug-development-process/step-3-clinical-research on the success-failure ratio for each phase, specifically 70% success rate for Phase I, 33% for Phase II and 30% for Phase III. Second, we also consider indication-level evaluation where we test if the drug can pass all three phases for the final market approval. To imitate it, we assemble all phase studies related to the drug and disease of the study and then use the latest phase protocol as the input to our model. Drugs that have Phase III success are labelled positive and other drugs that fail in any of the three phases are labelled negative. Data statistics are shown in TableÂ 2. Evaluation Metrics\nWe use the following metrics to measure the performance of all methods.\n\nâˆ™âˆ™\\bullet PR-AUC (Precision-Recall Area Under Curve).\n\nâˆ™âˆ™\\bullet F1. The F1 score is the harmonic mean of the precision and recall.\n\nâˆ™âˆ™\\bullet ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n\nâˆ™âˆ™\\bullet p-value. We report the results of hypothesis testing in terms of p-value to showcase the statistical significance of our method over the best baseline results. If p-value is smaller than 0.05, we claim our method significantly outperforms the best baseline method. For PR-AUC, F1, and ROC-AUC, higher value represent better performance.\nWe split the dataset based on registration date. The earlier trials are used for learning while the later trials are used for inference. For example, for Phase I dataset, we learn the model using the trials before Aug 13th, 2014 and make inference using the trials after that date, as shown in TableÂ 2.\nWe use BootstrapÂ  to estimate the mean and standard deviation of accuracy on test set. Baselines. We compare HINT with several baselines, including both conventional machine learning models and deep learning methods. LR (Logistic Regression). It was used in  on trial prediction with disease features only. For fair comparison, we adapt it such that the input features include (i) 1024 dimensional Morgan fingerprint featureÂ , (ii) GRAM embedding (Eq.Â 7, GRAM is pretrained using disease risk moduleÂ Eq.Â 18) and (iii) BERT embedding of eligibility criteria for protocol. Then these three features are concatenated as the input of LR model. RF (Random Forest). Similarly to LR, it was used in  on trial prediction and we adapt it to use the same feature set. XGBoost. An implementation of gradient boosted decision trees designed for speed and performance. It was used in context of individual patient trial outcome prediction inÂ . We adapt it to use the same feature set for general trial outcome prediction. AdaBoost (Adaptive Boosting). It was used inÂ  for individual Alzheimerâ€™s patientâ€™s trial result prediction. We adapt it to use the same feature set. kNN+RF.  leverages statistical imputation techniques to handle missing data, and finds using (1) kNN (k-Nearest Neighbor) as imputation technique and (2) Random Forest as classifier would achieve best performance. We adapt this method to use the same feature set. FFNN (Feed-Forward Neural Network)Â . It uses the same feature with LR. The feature vectors are fed into a three-layer feedforward neural network. DeepEnrollÂ . DeepEnroll was originally designed for patient trial matching, it uses (1) pre-trained BERT modelÂ  to encode eligibility criteria into sentence embedding; (2) a hierarchical embedding model to disease information and (3) alignment model to capture the protocol-disease interaction information. To adapt it to our scenario, molecule embedding (ğ¡msubscriptğ¡ğ‘š\\mathbf{h}_{m}) is concatenated to the output of alignment model to make prediction. COMPOSE (cross-modal pseudo-siamese network)Â . COMPOSE was also originally designed for patient trial matching, it uses convolutional highway network and memory network to encode eligibility criteria and diseases respectively and an alignment model to model the interaction. COMPOSE incorporate the molecule information in the same way with DeepEnroll, as mentioned above."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Blood-brain barrier permeation models: discriminating between potential cns and non-cns drugs including p-glycoprotein substrates",
      "abstract": "",
      "year": "2004",
      "venue": "Journal of chemical information and computer sciences",
      "authors": "Marc Adenot and Roger Lahana"
    },
    {
      "index": 1,
      "title": "Publicly Available Clinical BERT Embeddings",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.03323",
      "authors": "Emily Alsentzer, JohnÂ R Murphy, Willie Boag, Wei-Hung Weng, DiÂ Jin, Tristan Naumann, and Matthew McDermott",
      "orig_title": "Publicly available clinical bert embeddings",
      "paper_id": "1904.03323v3"
    },
    {
      "index": 2,
      "title": "Welcome to the icd-10 code for sarcopenia",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of cachexia, sarcopenia and muscle",
      "authors": "StefanÂ D Anker etÂ al."
    },
    {
      "index": 3,
      "title": "Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes",
      "abstract": "",
      "year": "2016",
      "venue": "BioRxiv",
      "authors": "ArtemÂ V Artemov etÂ al."
    },
    {
      "index": 4,
      "title": "Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "S.Â Biswal etÂ al.",
      "orig_title": "Doctor2vec: Dynamic doctor representation learning for clinical trial recruitment",
      "paper_id": "1911.10395v1"
    },
    {
      "index": 5,
      "title": "Molecular fingerprint similarity search in virtual screening",
      "abstract": "",
      "year": "2015",
      "venue": "Methods",
      "authors": "AdriÃ  Cereto-MassaguÃ© etÂ al."
    },
    {
      "index": 6,
      "title": "Bootstrap methods",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "MichaelÂ R Chernick etÂ al."
    },
    {
      "index": 7,
      "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "KDD",
      "authors": "Edward Choi etÂ al.",
      "orig_title": "Gram: graph-based attention model for healthcare representation learning",
      "paper_id": "1611.07012v3"
    },
    {
      "index": 8,
      "title": "Convolutional embedding of attributed molecular graphs for physical property prediction",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of chemical information and modeling",
      "authors": "ConnorÂ W Coley, Regina Barzilay, WilliamÂ H Green, TommiÂ S Jaakkola, and KlavsÂ F Jensen"
    },
    {
      "index": 9,
      "title": "Discriminative embeddings of latent variable models for structured data",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "Hanjun Dai etÂ al."
    },
    {
      "index": 10,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Jacob Devlin etÂ al.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 11,
      "title": "Admetlab: a platform for systematic admet evaluation based on a comprehensively collected admet database",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of cheminformatics",
      "authors": "Jie Dong etÂ al."
    },
    {
      "index": 12,
      "title": "Interpretable Molecular Graph Generation via Monotonic Constraints",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.00412",
      "authors": "Yuanqi Du, Xiaojie Guo, Amarda Shehu, and Liang Zhao",
      "orig_title": "Interpretable molecular graph generation via monotonic constraints",
      "paper_id": "2203.00412v1"
    },
    {
      "index": 13,
      "title": "Application of kpca and adaboost algorithm in classification of functional magnetic resonance imaging of alzheimerâ€™s disease",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "authors": "Zhao Fan, Fanyu Xu, Cai Li, and Lili Yao"
    },
    {
      "index": 14,
      "title": "Fundamentals of clinical trials",
      "abstract": "",
      "year": "2015",
      "venue": "Springer",
      "authors": "LawrenceÂ M Friedman, CurtÂ D Furberg, DavidÂ L DeMets, DavidÂ M Reboussin, and ChristopherÂ B Granger"
    },
    {
      "index": 15,
      "title": "Differentiable Scaffolding Tree for Molecular Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, ConnorÂ W Coley, and Jimeng Sun",
      "orig_title": "Differentiable scaffolding tree for molecular optimization",
      "paper_id": "2109.10469v2"
    },
    {
      "index": 16,
      "title": "MIMOSA: Multi-constraint molecule sampling for molecule optimization",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Tianfan Fu, Cao Xiao, Xinhao Li, LucasÂ M Glass, and Jimeng Sun"
    },
    {
      "index": 17,
      "title": "Probabilistic and dynamic molecule-disease interaction modeling for drug discovery",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
      "authors": "Tianfan Fu, Cao Xiao, Cheng Qian, LucasÂ M Glass, and Jimeng Sun"
    },
    {
      "index": 18,
      "title": "CORE: Automatic Molecule Optimization Using Copy & Refine Strategy",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Tianfan Fu, Cao Xiao, and Jimeng Sun",
      "orig_title": "Core: Automatic molecule optimization using copy & refine strategy",
      "paper_id": "1912.05910v1"
    },
    {
      "index": 19,
      "title": "COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching",
      "abstract": "",
      "year": "2020",
      "venue": "KDD",
      "authors": "Junyi Gao, Cao Xiao, LucasÂ M Glass, and Jimeng Sun",
      "orig_title": "Compose: Cross-modal pseudo-siamese network for patient trial matching",
      "paper_id": "2006.08765v1"
    },
    {
      "index": 20,
      "title": "A data-driven approach to predicting successes and failures of clinical trials",
      "abstract": "",
      "year": "2016",
      "venue": "Cell chemical biology",
      "authors": "KaitlynÂ M Gayvert, NeelÂ S Madhukar, and Olivier Elemento"
    },
    {
      "index": 21,
      "title": "Modeling admet",
      "abstract": "",
      "year": "2016",
      "venue": "In Silico Methods for Predicting Drug Toxicity",
      "authors": "Jayeeta Ghosh etÂ al."
    },
    {
      "index": 22,
      "title": "Clinical development success rates for investigational drugs",
      "abstract": "",
      "year": "2014",
      "venue": "Nat. Biotechnol.",
      "authors": "Michael Hay etÂ al."
    },
    {
      "index": 23,
      "title": "Predicting successes and failures of clinical trials with an ensemble ls-svr",
      "abstract": "",
      "year": "2020",
      "venue": "medRxiv",
      "authors": "ZhenÂ Yu Hong, Jooyong Shim, WooÂ Chan Son, and Changha Hwang"
    },
    {
      "index": 24,
      "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1904.05342",
      "authors": "Kexin Huang etÂ al.",
      "orig_title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
      "paper_id": "1904.05342v3"
    },
    {
      "index": 25,
      "title": "DeepPurpose: A Deep Learning Library for Drug-Target Interaction Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "Bioinformatics",
      "authors": "Kexin Huang etÂ al.",
      "orig_title": "Deeppurpose: a deep learning library for drugâ€“target interaction prediction",
      "paper_id": "2004.08919v3"
    },
    {
      "index": 26,
      "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Wengong Jin, Regina Barzilay, and Tommi Jaakkola",
      "orig_title": "Junction tree variational autoencoder for molecular graph generation",
      "paper_id": "1802.04364v4"
    },
    {
      "index": 27,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "DiederikÂ P Kingma and Jimmy Ba"
    },
    {
      "index": 28,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "ThomasÂ N Kipf and Max Welling",
      "orig_title": "Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 29,
      "title": "4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. nature looks at ways they might be saved",
      "abstract": "",
      "year": "2011",
      "venue": "Nature",
      "authors": "Heidi Ledford"
    },
    {
      "index": 30,
      "title": "Machine learning with statistical imputation for predicting drug approvals",
      "abstract": "",
      "year": "2019",
      "venue": "Harvard Data Science Review",
      "authors": "AndrewÂ W. Lo, KienÂ Wei Siah, and ChiÂ Heem Wong"
    },
    {
      "index": 31,
      "title": "Prediction models of human plasma protein binding rate and oral bioavailability derived by using gaâ€“cgâ€“svm method",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of pharmaceutical and biomedical analysis",
      "authors": "Chang-Ying Ma etÂ al."
    },
    {
      "index": 32,
      "title": "How much do clinical trials cost?",
      "abstract": "",
      "year": "2017",
      "venue": "Nat. Rev. Drug Discov.",
      "authors": "Linda Martin etÂ al."
    },
    {
      "index": 33,
      "title": "Clinical trial methodology",
      "abstract": "",
      "year": "1978",
      "venue": "Nature",
      "authors": "Richard Peto"
    },
    {
      "index": 34,
      "title": "e-drug3d: 3d structure collections dedicated to drug repurposing and fragment-based drug design",
      "abstract": "",
      "year": "2012",
      "venue": "Bioinformatics",
      "authors": "Emilie Pihan etÂ al."
    },
    {
      "index": 35,
      "title": "Predicting phase 3 clinical trial results by modeling phase 2 clinical trial subject level data using deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of Machine Learning Research",
      "authors": "Youran Qi and QiÂ Tang"
    },
    {
      "index": 36,
      "title": "Evaluation of a Machine Learning Model Based on Pretreatment Symptoms and Electroencephalographic Features to Predict Outcomes of Antidepressant Treatment in Adults With Depression: A Prespecified Secondary Analysis of a Randomized Clinical Trial",
      "abstract": "",
      "year": "2020",
      "venue": "JAMA Network Open",
      "authors": "Pranav Rajpurkar etÂ al."
    },
    {
      "index": 37,
      "title": "Toxcast chemical landscape: paving the road to 21st century toxicology",
      "abstract": "",
      "year": "2016",
      "venue": "Chemical research in toxicology",
      "authors": "AnnÂ M Richard etÂ al."
    },
    {
      "index": 38,
      "title": "Training very deep networks",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "RupeshÂ Kumar Srivastava etÂ al."
    },
    {
      "index": 39,
      "title": "Clinical development success rates 2006â€“2015",
      "abstract": "",
      "year": "2016",
      "venue": "BIO Industry Analysis",
      "authors": "DavidÂ W Thomas, Justin Burns, John Audette, Adam Carroll, Corey Dow-Hygelund, and Michael Hay"
    },
    {
      "index": 40,
      "title": "A deep neural network approach to predicting clinical outcomes of neuroblastoma patients",
      "abstract": "",
      "year": "2019",
      "venue": "bioRxiv",
      "authors": "LÃ©on-Charles Tranchevent, Francisco Azuaje, and JagathÂ C. Rajapakse"
    },
    {
      "index": 41,
      "title": "Comprehensive characterization of cytochrome p450 isozyme selectivity across chemical libraries",
      "abstract": "",
      "year": "2009",
      "venue": "Nature biotechnology",
      "authors": "Henrike Veith, Noel Southall, Ruili Huang, Tim James, Darren Fayne, Natalia Artemenko, Min Shen, James Inglese, ChristopherÂ P Austin, DavidÂ G Lloyd, etÂ al."
    },
    {
      "index": 42,
      "title": "SMILES-BERT: large scale unsupervised pre-training for molecular property prediction",
      "abstract": "",
      "year": "2019",
      "venue": "ACM BCB",
      "authors": "Sheng Wang etÂ al."
    },
    {
      "index": 43,
      "title": "Bitcoin Transaction Forecasting with Deep Network Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": "Wenqi Wei, QiÂ Zhang, and Ling Liu",
      "orig_title": "Bitcoin transaction forecasting with deep network representation learning",
      "paper_id": "2007.07993v2"
    },
    {
      "index": 44,
      "title": "Drugbank 5.0: a major update to the drugbank database for 2018",
      "abstract": "",
      "year": "2018",
      "venue": "Nucleic acids research",
      "authors": "DavidÂ S Wishart etÂ al."
    },
    {
      "index": 45,
      "title": "Identifying the status of genetic lesions in cancer clinical trial documents using machine learning",
      "abstract": "",
      "year": "2012",
      "venue": "BMC genomics",
      "authors": "Yonghui Wu etÂ al."
    },
    {
      "index": 46,
      "title": "Stochastic gradient boosted distributed decision trees",
      "abstract": "",
      "year": "2009",
      "venue": "CIKM",
      "authors": "Jerry Ye, Jyh-Herng Chow, Jiang Chen, and Zhaohui Zheng"
    },
    {
      "index": 47,
      "title": "End-to-end convolutional semantic embeddings",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Quanzeng You, Zhengyou Zhang, and Jiebo Luo"
    },
    {
      "index": 48,
      "title": "Patient-trial matching with deep embedding and entailment prediction",
      "abstract": "",
      "year": "2020",
      "venue": "WWW",
      "authors": "Xingyao Zhang etÂ al."
    },
    {
      "index": 49,
      "title": "PyHealth: A Python Library for Health Predictive Models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.04209",
      "authors": "Yue Zhao, Zhi Qiao, Cao Xiao, Lucas Glass, and Jimeng Sun",
      "orig_title": "Pyhealth: A python library for health predictive models",
      "paper_id": "2101.04209v1"
    },
    {
      "index": 50,
      "title": "Network representation learning: From preprocessing, feature extraction to node embedding",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "Jingya Zhou, Ling Liu, Wenqi Wei, and Jianxi Fan"
    }
  ]
}