{
  "paper_id": "2207.08739v2",
  "title": "Rethinking Data Augmentation for Robust Visual Question Answering",
  "sections": {
    "comparisons with state-of-the-arts": "Settings. We incorporated the KDDAug into model UpDn , LMH 9 and CSS+ , and compared them with the SOTA VQA models both on VQA-CP v2 and VQA v2. According to the model framework design, we group them into: 1) Non-DA Methods: UpDn , AReg [ref]46, MuRel , GRL [ref]25, CF-VQA , GGE-DQ , D-VQA 4, IntroD , CSS+CL , and LMH 9. 2) DA Methods: CVL , Unshuffling , CSS 6, CSS+ , RandImg 0, SSL 8, MUTANT , SimpleAug , and ECD . All results are reported in Table 2. Results. Compared with all existing DA methods, KDDAug achieves the best OOD and trade-off performance on two datasets. For UpDn backbone, KDDAug improves the OOD performance of UpDn with a 20% absolute performance gain (60.24% vs. 39.74%) and improves accuracies on all different question categories. For LMH backbone, KDDAug boosts the performance on both ID and OOD benchmarks. Compared with other non-DA methods, KDDAug still outperforms most of them. It is worth noting that our KDDAug can also be incorporated into these advanced non-DA models to further boost their performance."
  },
  "reference_labels": [
    {
      "index": 0,
      "title": "Counterfactual vision and language learning",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Abbasnejad, E., Teney, D., Parvaneh, A., Shi, J., Hengel, A.v.d."
    },
    {
      "index": 1,
      "title": "Towards causal vqa: Reveling and reducing spurious correlations by invariant and covariant semantic editing",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Agarwal, V., Shetty, R., Fritz, M."
    },
    {
      "index": 2,
      "title": "Analyzing the behavior of visual question answering models",
      "abstract": "",
      "year": "2016",
      "venue": "EMNLP",
      "authors": "Agrawal, A., Batra, D., Parikh, D."
    },
    {
      "index": 3,
      "title": "Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Agrawal, A., Batra, D., Parikh, D., Kembhavi, A.",
      "orig_title": "Don’t just assume; look and answer: Overcoming priors for visual question answering",
      "paper_id": "1712.00377v2"
    },
    {
      "index": 4,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 5,
      "title": "Vqa: Visual question answering",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D."
    },
    {
      "index": 6,
      "title": "Inductive biases for low data vqa: A data augmentation approach",
      "abstract": "",
      "year": "2022",
      "venue": "WACV",
      "authors": "Askarian, N., Abbasnejad, E., Zukerman, I., Buntine, W., Haffari, G."
    },
    {
      "index": 7,
      "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA",
      "abstract": "",
      "year": "2021",
      "venue": "NAACL",
      "authors": "Bitton, Y., Stanovsky, G., Schwartz, R., Elhadad, M.",
      "orig_title": "Automatic generation of contrast sets from scene graphs: Probing the compositional consistency of GQA",
      "paper_id": "2103.09591v1"
    },
    {
      "index": 8,
      "title": "COIN: Counterfactual Image Generation for VQA Interpretation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Boukhers, Z., Hartmann, T., Jürjens, J.",
      "orig_title": "Coin: Counterfactual image generation for vqa interpretation",
      "paper_id": "2201.03342v1"
    },
    {
      "index": 9,
      "title": "Murel: Multimodal relational reasoning for visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Cadene, R., Ben-Younes, H., Cord, M., Thome, N."
    },
    {
      "index": 10,
      "title": "Rubi: Reducing unimodal biases in visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Cadene, R., Dancette, C., Ben-younes, H., Cord, M., Parikh, D."
    },
    {
      "index": 11,
      "title": "Learning efficient object detection models with knowledge distillation",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Chen, G., Choi, W., Yu, X., Han, T., Chandraker, M."
    },
    {
      "index": 12,
      "title": "Human-like Controllable Image Captioning with Verb-specific Semantic Roles",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Chen, L., Jiang, Z., Xiao, J., Liu, W.",
      "orig_title": "Human-like controllable image captioning with verb-specific semantic roles",
      "paper_id": "2103.12204v1"
    },
    {
      "index": 13,
      "title": "Rethinking the bottom-up framework for query-based video localization",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Chen, L., Lu, C., Tang, S., Xiao, J., Zhang, D., Tan, C., Li, X."
    },
    {
      "index": 14,
      "title": "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Chen, L., Ma, W., Xiao, J., Zhang, H., Chang, S.F.",
      "orig_title": "Ref-nms: Breaking proposal bottlenecks in two-stage referring expression grounding",
      "paper_id": "2009.01449v3"
    },
    {
      "index": 15,
      "title": "Counterfactual Samples Synthesizing for Robust Visual Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Chen, L., Yan, X., Xiao, J., Zhang, H., Pu, S., Zhuang, Y.",
      "orig_title": "Counterfactual samples synthesizing for robust visual question answering",
      "paper_id": "2003.06576v1"
    },
    {
      "index": 16,
      "title": "Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S."
    },
    {
      "index": 17,
      "title": "Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Chen, L., Zheng, Y., Niu, Y., Zhang, H., Xiao, J.",
      "orig_title": "Counterfactual samples synthesizing and training for robust visual question answering",
      "paper_id": "2110.01013v2"
    },
    {
      "index": 18,
      "title": "Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Clark, C., Yatskar, M., Zettlemoyer, L."
    },
    {
      "index": 19,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Devlin, J., Chang, M., Lee, K., Toutanova, K.",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 20,
      "title": "Visual turing test for computer vision systems",
      "abstract": "",
      "year": "2015",
      "venue": "PNAS",
      "authors": "Geman, D., Geman, S., Hallonquist, N., Younes, L."
    },
    {
      "index": 21,
      "title": "MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP",
      "authors": "Gokhale, T., Banerjee, P., Baral, C., Yang, Y.",
      "orig_title": "Mutant: A training paradigm for out-of-distribution generalization in visual question answering",
      "paper_id": "2009.08566v2"
    },
    {
      "index": 22,
      "title": "VQA-LOL: Visual Question Answering under the Lens of Logic",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Gokhale, T., Banerjee, P., Baral, C., Yang, Y.",
      "orig_title": "Vqa-lol: Visual question answering under the lens of logic",
      "paper_id": "2002.08325v2"
    },
    {
      "index": 23,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 24,
      "title": "Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects",
      "abstract": "",
      "year": "2019",
      "venue": "ACLW",
      "authors": "Grand, G., Belinkov, Y.",
      "orig_title": "Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects",
      "paper_id": "1906.08430v1"
    },
    {
      "index": 25,
      "title": "Greedy Gradient Ensemble for Robust Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Han, X., Wang, S., Su, C., Huang, Q., Tian, Q.",
      "orig_title": "Greedy gradient ensemble for robust visual question answering",
      "paper_id": "2107.12651v4"
    },
    {
      "index": 26,
      "title": "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Honnibal, M., Montani, I."
    },
    {
      "index": 27,
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.",
      "orig_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "paper_id": "1612.06890v1"
    },
    {
      "index": 28,
      "title": "Data augmentation for visual question answering",
      "abstract": "",
      "year": "2017",
      "venue": "INLG",
      "authors": "Kafle, K., Yousefhussien, M., Kanan, C."
    },
    {
      "index": 29,
      "title": "Contrast and classify: Training robust vqa models",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Kant, Y., Moudgil, A., Batra, D., Parikh, D., Agrawal, H."
    },
    {
      "index": 30,
      "title": "Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "EMNLP",
      "authors": "Kil, J., Zhang, C., Xuan, D., Chao, W.L.",
      "orig_title": "Discovering the unknown knowns: Turning implicit knowledge in the dataset into explicit training examples for visual question answering",
      "paper_id": "2109.06122v2"
    },
    {
      "index": 31,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Kingma, D.P., Ba, J."
    },
    {
      "index": 32,
      "title": "Efficient counterfactual debiasing for visual question answering",
      "abstract": "",
      "year": "2022",
      "venue": "WACV",
      "authors": "Kolling, C., More, M., Gavenski, N., Pooch, E., Parraga, O., Barros, R.C."
    },
    {
      "index": 33,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "IJCV",
      "authors": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al."
    },
    {
      "index": 34,
      "title": "Integrating Object-aware and Interaction-aware Knowledge for Weakly Supervised Scene Graph Generation",
      "abstract": "",
      "year": "2022",
      "venue": "ACM MM",
      "authors": "Li, X., Chen, L., Ma, W., Yang, Y., Xiao, J.",
      "orig_title": "Integrating object-aware and interaction-aware knowledge for weakly supervised scene graph generation",
      "paper_id": "2208.01834v1"
    },
    {
      "index": 35,
      "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "ACM SIGIR",
      "authors": "Liang, Z., Hu, H., Zhu, J.",
      "orig_title": "Lpf: A language-prior feedback objective function for de-biased visual question answering",
      "paper_id": "2105.14300v2"
    },
    {
      "index": 36,
      "title": "Learning to contrast the counterfactual samples for robust visual question answering",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP",
      "authors": "Liang, Z., Jiang, W., Hu, H., Zhu, J."
    },
    {
      "index": 37,
      "title": "Debug: A dense bottom-up grounding approach for natural language video localization",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Lu, C., Chen, L., Tan, C., Li, X., Xiao, J."
    },
    {
      "index": 38,
      "title": "End-to-End Bias Mitigation by Modelling Biases in Corpora",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Mahabadi, R.K., Belinkov, Y., Henderson, J.",
      "orig_title": "End-to-end bias mitigation by modelling biases in corpora",
      "paper_id": "1909.06321v3"
    },
    {
      "index": 39,
      "title": "Rethinking the reference-based distinctive image captioning",
      "abstract": "",
      "year": "2022",
      "venue": "ACM MM",
      "authors": "Mao, Y., Chen, L., Jiang, Z., Zhang, D., Zhang, Z., Shao, J., Xiao, J."
    },
    {
      "index": 40,
      "title": "Counterfactual vqa: A cause-effect look at language bias",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Niu, Y., Tang, K., Zhang, H., Lu, Z., Hua, X.S., Wen, J.R."
    },
    {
      "index": 41,
      "title": "Introspective distillation for robust question answering",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Niu, Y., Zhang, H."
    },
    {
      "index": 42,
      "title": "Spatio-temporal graph for video captioning with knowledge distillation",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Pan, B., Cai, H., Huang, D.A., Lee, K.H., Gaidon, A., Adeli, E., Niebles, J.C."
    },
    {
      "index": 43,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 44,
      "title": "Data Distillation: Towards Omni-Supervised Learning",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Radosavovic, I., Dollár, P., Girshick, R., Gkioxari, G., He, K.",
      "orig_title": "Data distillation: Towards omni-supervised learning",
      "paper_id": "1712.04440v1"
    },
    {
      "index": 45,
      "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "Ramakrishnan, S., Agrawal, A., Lee, S.",
      "orig_title": "Overcoming language priors in visual question answering with adversarial regularization",
      "paper_id": "1810.03649v2"
    },
    {
      "index": 46,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Ren, S., He, K., Girshick, R., Sun, J.",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 47,
      "title": "Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Tang, R., Ma, C., Zhang, W.E., Wu, Q., Yang, X.",
      "orig_title": "Semantic equivalent adversarial data augmentation for visual question answering",
      "paper_id": "2007.09592v1"
    },
    {
      "index": 48,
      "title": "Unshuffling Data for Improved Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Teney, D., Abbasnejad, E., Hengel, A.v.d.",
      "orig_title": "Unshuffling data for improved generalization",
      "paper_id": "2002.11894v3"
    },
    {
      "index": 49,
      "title": "On the value of out-of-distribution testing: An example of goodhart’s law",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Teney, D., Kafle, K., Shrestha, R., Abbasnejad, E., Kanan, C., Hengel, A.v.d."
    },
    {
      "index": 50,
      "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE TPAMI",
      "authors": "Wang, L., Yoon, K.J.",
      "orig_title": "Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks",
      "paper_id": "2004.05937v7"
    },
    {
      "index": 51,
      "title": "Distilling Object Detectors with Fine-grained Feature Imitation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Wang, T., Yuan, L., Zhang, X., Feng, J.",
      "orig_title": "Distilling object detectors with fine-grained feature imitation",
      "paper_id": "1906.03609v1"
    },
    {
      "index": 52,
      "title": "Cross-Modal Generative Augmentation for Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "BMVC",
      "authors": "Wang, Z., Miao, Y., Specia, L.",
      "orig_title": "Cross-modal generative augmentation for visual question answering",
      "paper_id": "2105.04780v2"
    },
    {
      "index": 53,
      "title": "Debiased visual question answering from feature and sample perspectives",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Wen, Z., Xu, G., Tan, M., Wu, Q., Wu, Q."
    },
    {
      "index": 54,
      "title": "Boundary Proposal Network for Two-Stage Natural Language Video Localization",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Xiao, S., Chen, L., Zhang, S., Ji, W., Shao, J., Ye, L., Xiao, J.",
      "orig_title": "Boundary proposal network for two-stage natural language video localization",
      "paper_id": "2103.08109v2"
    },
    {
      "index": 55,
      "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Zhang, P., Goyal, Y., Summers-Stay, D., Batra, D., Parikh, D.",
      "orig_title": "Yin and yang: Balancing and answering binary visual questions",
      "paper_id": "1511.05099v5"
    },
    {
      "index": 56,
      "title": "Object Relational Graph with Teacher-Recommended Learning for Video Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Zhang, Z., Shi, Y., Yuan, C., Li, B., Wang, P., Hu, W., Zha, Z.J.",
      "orig_title": "Object relational graph with teacher-recommended learning for video captioning",
      "paper_id": "2002.11566v1"
    },
    {
      "index": 57,
      "title": "Overcoming Language Priors with Self-supervised Learning for Visual Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "IJCAI",
      "authors": "Zhu, X., Mao, Z., Liu, C., Zhang, P., Wang, B., Zhang, Y.",
      "orig_title": "Overcoming language priors with self-supervised learning for visual question answering",
      "paper_id": "2012.11528v1"
    },
    {
      "index": 58,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler, S."
    }
  ]
}