{
  "paper_id": "2006.14091v2",
  "title": "Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences",
  "abstract": "Abstract\nReward functions are a common way to specify the objective of a robot. As designing reward functions can be extremely challenging, a more promising approach is to directly learn reward functions from human teachers. Importantly, data from human teachers can be collected either passively or actively in a variety of forms: passive data sources include demonstrations, (e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings) are actively elicited. Prior research has independently applied reward learning to these different data sources. However, there exist many domains where multiple sources are complementary and expressive. Motivated by this general problem, we present a framework to integrate multiple sources of information, which are either passively or actively collected from human users. In particular, we present an algorithm that first utilizes user demonstrations to initialize a belief about the reward function, and then actively probes the user with preference queries to zero-in on their true reward. This algorithm not only enables us combine multiple data sources, but it also informs the robot when it should leverage each type of information. Further, our approach accounts for the human’s ability to provide data: yielding user-friendly preference queries which are also theoretically optimal. Our extensive simulated experiments and user studies on a Fetch mobile manipulator demonstrate the superiority and the usability of our integrated framework.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Apprenticeship learning via inverse reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "twenty-first international conference on Machine learning",
      "authors": "Abbeel P and Ng AY"
    },
    {
      "index": 1,
      "title": "Exploration and apprenticeship learning in reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "22nd international conference on Machine learning",
      "authors": "Abbeel P and Ng AY"
    },
    {
      "index": 2,
      "title": "An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity",
      "abstract": "",
      "year": "2012",
      "venue": "Journal of Machine Learning Research",
      "authors": "Ailon N"
    },
    {
      "index": 3,
      "title": "Keyframe-based learning from demonstration",
      "abstract": "",
      "year": "2012",
      "venue": "International Journal of Social Robotics",
      "authors": "Akgun B, Cakmak M, Jiang K and Thomaz AL"
    },
    {
      "index": 4,
      "title": "April: Active preference learning-based reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Akrour R, Schoenauer M and Sebag M"
    },
    {
      "index": 5,
      "title": "Learning from physical human corrections, one feature at a time",
      "abstract": "",
      "year": "2018",
      "venue": "2018 ACM/IEEE International Conference on Human-Robot Interaction",
      "authors": "Bajcsy A, Losey DP, O’Malley MK and Dragan AD"
    },
    {
      "index": 6,
      "title": "Learning robot objectives from physical human interaction",
      "abstract": "",
      "year": "2017",
      "venue": "Machine Learning Research",
      "authors": "Bajcsy A, Losey DP, O’Malley MK and Dragan AD"
    },
    {
      "index": 7,
      "title": "Active learning of reward dynamics from hierarchical queries",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "Basu C, Biyik E, He Z, Singhal M and Sadigh D"
    },
    {
      "index": 8,
      "title": "Do You Want Your Autonomous Car To Drive Like You?",
      "abstract": "",
      "year": "2017",
      "venue": "12th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": "Basu C, Yang Q, Hungerman D, Sinahal M and Draqan AD",
      "orig_title": "Do you want your autonomous car to drive like you?",
      "paper_id": "1802.01636v1"
    },
    {
      "index": 9,
      "title": "Discrete choice analysis: theory and application to travel demand, volume 9",
      "abstract": "",
      "year": "1985",
      "venue": "MIT press",
      "authors": "Ben-Akiva ME, Lerman SR and Lerman SR"
    },
    {
      "index": 10,
      "title": "Active Preference-Based Gaussian Process Regression for Reward Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Biyik E, Huynh N, Kochenderfer MJ and Sadigh D",
      "orig_title": "Active preference-based gaussian process regression for reward learning",
      "paper_id": "2005.02575v2"
    },
    {
      "index": 11,
      "title": "The Green Choice: Learning and Influencing Human Decisions on Shared Roads",
      "abstract": "",
      "year": "2019",
      "venue": "58th IEEE Conference on Decision and Control (CDC)",
      "authors": "Biyik E, Lazar DA, Sadigh D and Pedarsani R",
      "orig_title": "The green choice: Learning and influencing human decisions on shared roads",
      "paper_id": "1904.02209v2"
    },
    {
      "index": 12,
      "title": "Asking Easy Questions: A User-Friendly Approach to Active Reward Learning",
      "abstract": "",
      "year": "2019",
      "venue": "3rd Conference on Robot Learning (CoRL)",
      "authors": "Biyik E, Palan M, Landolfi NC, Losey DP and Sadigh D",
      "orig_title": "Asking easy questions: A user-friendly approach to active reward learning",
      "paper_id": "1910.04365v1"
    },
    {
      "index": 13,
      "title": "Batch Active Preference-Based Learning of Reward Functions",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Robot Learning (CoRL)",
      "authors": "Biyik E and Sadigh D",
      "orig_title": "Batch active preference-based learning of reward functions",
      "paper_id": "1810.04303v1"
    },
    {
      "index": 14,
      "title": "Batch Active Learning Using Determinantal Point Processes",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.07975",
      "authors": "Bıyık E, Wang K, Anari N and Sadigh D",
      "orig_title": "Batch active learning using determinantal point processes",
      "paper_id": "1906.07975v1"
    },
    {
      "index": 15,
      "title": "Learning under Misspecified Objective Spaces",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Robot Learning",
      "authors": "Bobu A, Bajcsy A, Fisac JF and Dragan AD",
      "orig_title": "Learning under misspecified objective spaces",
      "paper_id": "1810.05157v4"
    },
    {
      "index": 16,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J and Zaremba W"
    },
    {
      "index": 17,
      "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Brown D, Goo W, Nagarajan P and Niekum S",
      "orig_title": "Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations",
      "paper_id": "1904.06387v5"
    },
    {
      "index": 18,
      "title": "Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on robot learning",
      "authors": "Brown DS, Goo W and Niekum S",
      "orig_title": "Better-than-demonstrator imitation learning via automatically-ranked demonstrations",
      "paper_id": "1907.03976v3"
    },
    {
      "index": 19,
      "title": "Deep Bayesian Reward Learning from Preferences",
      "abstract": "",
      "year": "2019",
      "venue": "Workshop on Safety and Robustness in Decision Making at the 33rd Conference on Neural Information Processing Systems (NeurIPS)",
      "authors": "Brown DS and Niekum S",
      "orig_title": "Deep bayesian reward learning from preferences",
      "paper_id": "1912.04472v1"
    },
    {
      "index": 20,
      "title": "Human preferences for robot-human hand-over configurations",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Cakmak M, Srinivasa SS, Lee MK, Forlizzi J and Kiesler S"
    },
    {
      "index": 21,
      "title": "Learning from Suboptimal Demonstration via Self-Supervised Reward Regression",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on robot learning",
      "authors": "Chen L, Paleja R and Gombolay M",
      "orig_title": "Learning from suboptimal demonstration via self-supervised reward regression",
      "paper_id": "2010.11723v3"
    },
    {
      "index": 22,
      "title": "On the Utility of Model Learning in HRI",
      "abstract": "",
      "year": "2019",
      "venue": "14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": "Choudhury R, Swamy G, Hadfield-Menell D and Dragan AD",
      "orig_title": "On the utility of model learning in hri",
      "paper_id": "1901.01291v2"
    },
    {
      "index": 23,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Christiano PF, Leike J, Brown T, Martic M, Legg S and Amodei D",
      "orig_title": "Deep reinforcement learning from human preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 24,
      "title": "Gaussian processes for ordinal regression",
      "abstract": "",
      "year": "2005",
      "venue": "Journal of machine learning research",
      "authors": "Chu W and Ghahramani Z"
    },
    {
      "index": 25,
      "title": "Elements of information theory",
      "abstract": "",
      "year": "2012",
      "venue": "John Wiley & Sons",
      "authors": "Cover TM and Thomas JA"
    },
    {
      "index": 26,
      "title": "Cortical substrates for exploratory decisions in humans",
      "abstract": "",
      "year": "2006",
      "venue": "Nature",
      "authors": "Daw ND, O’doherty JP, Dayan P, Seymour B and Dolan RJ"
    },
    {
      "index": 27,
      "title": "Formalizing assistive teleoperation",
      "abstract": "",
      "year": "2012",
      "venue": "MIT Press",
      "authors": "Dragan AD and Srinivasa SS"
    },
    {
      "index": 28,
      "title": "Real-time multiattribute bayesian preference elicitation with pairwise comparison queries",
      "abstract": "",
      "year": "2010",
      "venue": "Thirteenth International Conference on Artificial Intelligence and Statistics",
      "authors": "Guo S and Sanner S"
    },
    {
      "index": 29,
      "title": "Here’s What I’ve Learned: Asking Questions that Reveal Reward Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2107.01995",
      "authors": "Habibian S, Jonnavittula A and Losey DP",
      "orig_title": "Here’s what I’ve learned: Asking questions that reveal reward learning",
      "paper_id": "2107.01995v1"
    },
    {
      "index": 30,
      "title": "Active comparison based learning incorporating user uncertainty and noise",
      "abstract": "",
      "year": "2016",
      "venue": "RSS Workshop on Model Learning for Human-Robot Communication",
      "authors": "Holladay R, Javdani S, Dragan A and Srinivasa S"
    },
    {
      "index": 31,
      "title": "Reward learning from human preferences and demonstrations in Atari",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Ibarz B, Leike J, Pohlen T, Irving G, Legg S and Amodei D",
      "orig_title": "Reward learning from human preferences and demonstrations in atari",
      "paper_id": "1811.06521v1"
    },
    {
      "index": 32,
      "title": "Shared autonomy via hindsight optimization",
      "abstract": "",
      "year": "2015",
      "venue": "Robotics science and systems: online proceedings",
      "authors": "Javdani S, Srinivasa SS and Bagnell JA"
    },
    {
      "index": 33,
      "title": "Preference-based learning of reward function features",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.02727",
      "authors": "Katz S, Maleki A, Biyik E and Kochenderfer MJ"
    },
    {
      "index": 34,
      "title": "Learning an urban air mobility encounter model from expert preferences",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)",
      "authors": "Katz SM, Bihan ACL and Kochenderfer MJ"
    },
    {
      "index": 35,
      "title": "Data-driven motion mappings improve transparency in teleoperation",
      "abstract": "",
      "year": "2015",
      "venue": "Presence: Teleoperators and Virtual Environments",
      "authors": "Khurshid RP and Kuchenbecker KJ"
    },
    {
      "index": 36,
      "title": "Incorporating thresholds of indifference in probabilistic choice models",
      "abstract": "",
      "year": "1977",
      "venue": "Management science",
      "authors": "Krishnan K"
    },
    {
      "index": 37,
      "title": "Structured labeling for facilitating concept evolution in machine learning",
      "abstract": "",
      "year": "2014",
      "venue": "SIGCHI Conference on Human Factors in Computing Systems",
      "authors": "Kulesza T, Amershi S, Caruana R, Fisher D and Charles D"
    },
    {
      "index": 38,
      "title": "When Humans Aren’t Optimal: Robots that Collaborate with Risk-Aware Humans",
      "abstract": "",
      "year": "2020",
      "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": "Kwon M, Biyik E, Talati A, Bhasin K, Losey DP and Sadigh D",
      "orig_title": "When humans aren’t optimal: Robots that collaborate with risk-aware humans",
      "paper_id": "2001.04377v1"
    },
    {
      "index": 39,
      "title": "Bayesian preference elicitation for multiobjective engineering design optimization",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Aerospace Information Systems",
      "authors": "Lepird JR, Owen MP and Kochenderfer MJ"
    },
    {
      "index": 40,
      "title": "ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "Li K, Tucker M, Biyik E, Novoseller E, Burdick JW, Sui Y, Sadigh D, Yue Y and Ames AD",
      "orig_title": "Roial: Region of interest active learning for characterizing exoskeleton gait preference landscapes",
      "paper_id": "2011.04812v2"
    },
    {
      "index": 41,
      "title": "Learning Human Objectives from Sequences of Physical Corrections",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Robotics and Automation (ICRA)",
      "authors": "Li M, Canberk A, Losey DP and Sadigh D",
      "orig_title": "Learning human objectives from sequences of physical corrections",
      "paper_id": "2104.00078v1"
    },
    {
      "index": 42,
      "title": "A rational model of preference learning and choice prediction by children",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in neural information processing systems",
      "authors": "Lucas CG, Griffiths TL, Xu F and Fawcett C"
    },
    {
      "index": 43,
      "title": "Individual choice behavior: A theoretical analysis",
      "abstract": "",
      "year": "2012",
      "venue": "Courier Corporation",
      "authors": "Luce RD"
    },
    {
      "index": 44,
      "title": "Bayesian nonparametric inverse reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "Joint European conference on machine learning and knowledge discovery in databases",
      "authors": "Michini B and How JP"
    },
    {
      "index": 45,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "Icml",
      "authors": "Ng AY, Russell SJ et al."
    },
    {
      "index": 46,
      "title": "Efficient model learning from joint-action demonstrations for human-robot collaborative tasks",
      "abstract": "",
      "year": "2015",
      "venue": "10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": "Nikolaidis S, Ramakrishnan R, Gu K and Shah J"
    },
    {
      "index": 47,
      "title": "Learning Reward Functions by Integrating Human Demonstrations and Preferences",
      "abstract": "",
      "year": "2019",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Palan M, Shevchuk G, Landolfi NC and Sadigh D",
      "orig_title": "Learning reward functions by integrating human demonstrations and preferences",
      "paper_id": "1906.08928v1"
    },
    {
      "index": 48,
      "title": "Inferring task goals and constraints using bayesian nonparametric inverse reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Robot Learning, Proceedings of Machine Learning Research",
      "authors": "Park D, Noseworthy M, Paul R, Roy S and Roy N"
    },
    {
      "index": 49,
      "title": "Bayesian inverse reinforcement learning",
      "abstract": "",
      "year": "2007",
      "venue": "IJCAI",
      "authors": "Ramachandran D and Amir E"
    },
    {
      "index": 50,
      "title": "Active preference-based learning of reward functions",
      "abstract": "",
      "year": "2017",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Sadigh D, Dragan AD, Sastry SS and Seshia SA"
    },
    {
      "index": 51,
      "title": "Planning for autonomous cars that leverage effects on human actions",
      "abstract": "",
      "year": "2016",
      "venue": "Robotics: Science and Systems",
      "authors": "Sadigh D, Sastry S, Seshia SA and Dragan AD"
    },
    {
      "index": 52,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "Schulman J, Wolski F, Dhariwal P, Radford A and Klimov O"
    },
    {
      "index": 53,
      "title": "Interactive Robot Training for Non-Markov Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.02232",
      "authors": "Shah A and Shah J",
      "orig_title": "Interactive robot training for non-markov tasks",
      "paper_id": "2003.02232v2"
    },
    {
      "index": 54,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Todorov E, Erez T and Tassa Y"
    },
    {
      "index": 55,
      "title": "Preference-Based Learning for Exoskeleton Gait Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "Tucker M, Novoseller E, Kann C, Sui Y, Yue Y, Burdick J and Ames AD",
      "orig_title": "Preference-based learning for exoskeleton gait optimization",
      "paper_id": "1909.12316v3"
    },
    {
      "index": 56,
      "title": "Optimal bayesian recommendation sets and myopically optimal choice query sets",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in neural information processing systems",
      "authors": "Viappiani P and Boutilier C"
    },
    {
      "index": 57,
      "title": "Bayesian Active Learning for Collaborative Task Specification Using Equivalence Regions",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters",
      "authors": "Wilde N, Kulić D and Smith SL",
      "orig_title": "Bayesian active learning for collaborative task specification using equivalence regions",
      "paper_id": "1901.09470v1"
    },
    {
      "index": 58,
      "title": "Fetch and freight: Standard platforms for service robot applications",
      "abstract": "",
      "year": "2016",
      "venue": "Workshop on Autonomous Mobile Service Robots",
      "authors": "Wise M, Ferguson M, King D, Diehr E and Dymesich D"
    },
    {
      "index": 59,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Aaai",
      "authors": "Ziebart BD, Maas AL, Bagnell JA and Dey AK"
    }
  ]
}