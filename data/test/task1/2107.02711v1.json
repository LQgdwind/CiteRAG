{
  "paper_id": "2107.02711v1",
  "title": "A Unified Off-Policy Evaluation Approach for General Value Function",
  "abstract": "Abstract\nGeneral Value Function (GVF) is a powerful tool to represent both the predictive and retrospective knowledge in reinforcement learning (RL). In practice, often multiple interrelated GVFs need to be evaluated jointly with pre-collected off-policy samples. In the literature, the gradient temporal difference (GTD) learning method has been adopted to evaluate GVFs in the off-policy setting, but such an approach may suffer from a large estimation error even if the function approximation class is sufficiently expressive. Moreover, none of the previous work have formally established the convergence guarantee to the ground truth GVFs under the function approximation settings. In this paper, we address both issues through the lens of a class of GVFs with causal filtering, which cover a wide range of RL applications such as reward variance, value gradient, cost in anomaly detection, stationary distribution gradient, etc. We propose a new algorithm called GenTD for off-policy GVFs evaluation and show that GenTD learns multiple interrelated multi-dimensional GVFs as efficiently as a single canonical scalar value function. We further show that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to the ground truth GVFs as long as the function approximation power is sufficiently large. To our best knowledge, GenTD is the first off-policy GVF evaluation algorithm that has global optimality guarantee.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Residual algorithms: reinforcement learning with function approximation",
      "abstract": "",
      "year": "1995",
      "venue": "Machine Learning Proceedings",
      "authors": "L. Baird"
    },
    {
      "index": 1,
      "title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Learning Theory (COLT)",
      "authors": "J. Bhandari, D. Russo, and R. Singal",
      "orig_title": "A finite time analysis of temporal difference learning with linear function approximation",
      "paper_id": "1806.02450v2"
    },
    {
      "index": 2,
      "title": "Neural temporal-difference and q-learning provably converge to global optima",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "Q. Cai, Z. Yang, J. D. Lee, and Z. Wang"
    },
    {
      "index": 3,
      "title": "Universal Off-Policy Evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Y. Chandak, S. Niekum, B. C. da Silva, E. Learned-Miller, E. Brunskill, and P. S. Thomas",
      "orig_title": "Universal off-policy evaluation",
      "paper_id": "2104.12820v2"
    },
    {
      "index": 4,
      "title": "Knowledge representation for reinforcement learning using general value functions",
      "abstract": "",
      "year": "2018",
      "venue": "OpenReview",
      "authors": "G. Comanici, D. Precup, A. Barreto, D. K. Toyama, E. Aygün, P. Hamel, S. Vezhnevets, S. Hou, and S. Mourad"
    },
    {
      "index": 5,
      "title": "A Tale of Two-Timescale Reinforcement Learning with the Tightest Finite-Time Bound",
      "abstract": "",
      "year": "",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)",
      "authors": "G. Dalal, B. Szorenyi, and G. Thoppe",
      "orig_title": "A tale of two-timescale reinforcement learning with the tightest finite-time bound",
      "paper_id": "1911.09157v2"
    },
    {
      "index": 6,
      "title": "Finite Sample Analyses for TD(0) with Function Approximation",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)",
      "authors": "G. Dalal, B. Szörényi, G. Thoppe, and S. Mannor",
      "orig_title": "Finite sample analyses for TD (0) with function approximation",
      "paper_id": "1704.01161v4"
    },
    {
      "index": 7,
      "title": "How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "P. D’Oro and W. Jaśkowski",
      "orig_title": "How to learn a useful critic? model-based action-gradient-estimator policy optimization",
      "paper_id": "2004.14309v2"
    },
    {
      "index": 8,
      "title": "Predictive state recurrent neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "C. Downey, A. Hefny, B. Li, B. Boots, and G. Gordon"
    },
    {
      "index": 9,
      "title": "Consistent On-Line Off-Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "A. Hallak and S. Mannor",
      "orig_title": "Consistent on-line off-policy evaluation",
      "paper_id": "1702.07121v1"
    },
    {
      "index": 10,
      "title": "Provably Efficient Maximum Entropy Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "E. Hazan, S. Kakade, K. Singh, and A. Van Soest",
      "orig_title": "Provably efficient maximum entropy exploration",
      "paper_id": "1812.02690v2"
    },
    {
      "index": 11,
      "title": "Learning Continuous Control Policies by Stochastic Value Gradients",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint",
      "authors": "N. Heess, G. Wayne, D. Silver, T. Lillicrap, Y. Tassa, and T. Erez",
      "orig_title": "Learning continuous control policies by stochastic value gradients",
      "paper_id": "1510.09142v1"
    },
    {
      "index": 12,
      "title": "Matrix analysis",
      "abstract": "",
      "year": "2012",
      "venue": "Cambridge University Press",
      "authors": "R. A. Horn and C. R. Johnson"
    },
    {
      "index": 13,
      "title": "From Importance Sampling to Doubly Robust Policy Gradient",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "J. Huang and N. Jiang",
      "orig_title": "From importance sampling to doubly robust policy gradient",
      "paper_id": "1910.09066v3"
    },
    {
      "index": 14,
      "title": "Variance Penalized On-Policy and Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "A. Jain, G. Patil, A. Jain, K. Khetarpal, and D. Precup",
      "orig_title": "Variance penalized on-policy and off-policy actor-critic",
      "paper_id": "2102.01985v1"
    },
    {
      "index": 15,
      "title": "Doubly robust off-policy value evaluation for reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "N. Jiang and L. Li"
    },
    {
      "index": 16,
      "title": "Finite time analysis of linear two-timescale stochastic approximation with markovian noise",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory (COLT)",
      "authors": "M. Kaledin, E. Moulines, A. Naumov, V. Tadic, and H.-T. Wai"
    },
    {
      "index": 17,
      "title": "Statistically Efficient Off-Policy Policy Gradients",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "N. Kallus and M. Uehara",
      "orig_title": "Statistically efficient off-policy policy gradients",
      "paper_id": "2002.04014v2"
    },
    {
      "index": 18,
      "title": "The fixed points of off-policy TD",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems (NIPS)",
      "authors": "J. Kolter"
    },
    {
      "index": 19,
      "title": "First-order and Stochastic Optimization Methods for Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Springer",
      "authors": "G. Lan"
    },
    {
      "index": 20,
      "title": "Finite-Sample Analysis of Proximal Gradient TD Algorithms",
      "abstract": "",
      "year": "2015",
      "venue": "UAI",
      "authors": "B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik",
      "orig_title": "Finite-sample analysis of proximal gradient TD algorithms",
      "paper_id": "2006.14364v2"
    },
    {
      "index": 21,
      "title": "Gradient temporal-difference learning algorithms",
      "abstract": "",
      "year": "2011",
      "venue": "University of Alberta",
      "authors": "H. R. Maei"
    },
    {
      "index": 22,
      "title": "Representation search through generate and test",
      "abstract": "",
      "year": "2013",
      "venue": "AAI Workshop: Learning Rich Representations from Low-Level Sensors",
      "authors": "A. R. Mahmood and R. S. Sutton"
    },
    {
      "index": 23,
      "title": "Algorithmic aspects of mean–variance optimization in markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "European Journal of Operational Research",
      "authors": "S. Mannor and J. N. Tsitsiklis"
    },
    {
      "index": 24,
      "title": "Derivatives of logarithmic stationary distributions for policy gradient reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Neural computation",
      "authors": "T. Morimura, E. Uchibe, J. Yoshimoto, J. Peters, and K. Doya"
    },
    {
      "index": 25,
      "title": "Error bounds for approximate policy iteration",
      "abstract": "",
      "year": "2003",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "R. Munos"
    },
    {
      "index": 26,
      "title": "Risk-averse dynamic programming for markov decision processes",
      "abstract": "",
      "year": "2010",
      "venue": "Mathematical programming",
      "authors": "A. Ruszczyński"
    },
    {
      "index": 27,
      "title": "TD algorithm for the variance of return and mean-variance reinforcement learning",
      "abstract": "",
      "year": "2001",
      "venue": "Transactions of the Japanese Society for Artificial Intelligence",
      "authors": "M. Sato, H. Kimura, and S. Kobayashi"
    },
    {
      "index": 28,
      "title": "Better generalization with forecasts",
      "abstract": "",
      "year": "2013",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
      "authors": "T. Schaul and M. Ring"
    },
    {
      "index": 29,
      "title": "Mutual fund performance",
      "abstract": "",
      "year": "1966",
      "venue": "The Journal of business",
      "authors": "W. F. Sharpe"
    },
    {
      "index": 30,
      "title": "Representation and general value functions",
      "abstract": "",
      "year": "2020",
      "venue": "University of Alberta",
      "authors": "C. Sherstan"
    },
    {
      "index": 31,
      "title": "Informing sequential clinical decision-making through reinforcement learning: an empirical study",
      "abstract": "",
      "year": "2011",
      "venue": "Machine Learning",
      "authors": "S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy"
    },
    {
      "index": 32,
      "title": "Gradient temporal difference networks",
      "abstract": "",
      "year": "2013",
      "venue": "European Workshop on Reinforcement Learning",
      "authors": "D. Silver"
    },
    {
      "index": 33,
      "title": "Deterministic policy gradient algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller"
    },
    {
      "index": 34,
      "title": "The variance of discounted Markov decision processes",
      "abstract": "",
      "year": "1982",
      "venue": "Journal of Applied Probability",
      "authors": "M. J. Sobel"
    },
    {
      "index": 35,
      "title": "Finite-time error bounds for linear stochastic approximation andtd learning",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory (COLT)",
      "authors": "R. Srikant and L. Ying"
    },
    {
      "index": 36,
      "title": "Learning to filter with predictive state inference machines",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "W. Sun, A. Venkatraman, B. Boots, and J. A. Bagnell"
    },
    {
      "index": 37,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "Machine Learning",
      "authors": "R. S. Sutton"
    },
    {
      "index": 38,
      "title": "The grand challenge of predictive empirical abstract knowledge",
      "abstract": "",
      "year": "2009",
      "venue": "IJCAI Workshop on Grand Challenges for Reasoning from Experiences",
      "authors": "R. S. Sutton"
    },
    {
      "index": 39,
      "title": "Reinforcement Learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 40,
      "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesvári, and E. Wiewiora"
    },
    {
      "index": 41,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "R. S. Sutton, A. R. Mahmood, and M. White",
      "orig_title": "An emphatic approach to the problem of off-policy temporal-difference learning",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 42,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour"
    },
    {
      "index": 43,
      "title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup"
    },
    {
      "index": 44,
      "title": "Temporal-Difference Networks",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "R. S. Sutton and B. Tanner",
      "orig_title": "Temporal-difference networks",
      "paper_id": "1504.05539v1"
    },
    {
      "index": 45,
      "title": "Policy gradients with variance related risk criteria",
      "abstract": "",
      "year": "2012",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "A. Tamar, D. Di Castro, and S. Mannor"
    },
    {
      "index": 46,
      "title": "Learning the variance of the reward-to-go",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "A. Tamar, D. Di Castro, and S. Mannor"
    },
    {
      "index": 47,
      "title": "Variance adjusted actor critic algorithms",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint",
      "authors": "A. Tamar and S. Mannor"
    },
    {
      "index": 48,
      "title": "Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Z. Tang, Y. Feng, L. Li, D. Zhou, and Q. Liu",
      "orig_title": "Doubly robust bias reduction in infinite horizon off-policy estimation",
      "paper_id": "1910.07186v1"
    },
    {
      "index": 49,
      "title": "Analysis of temporal-diffference learning with function approximation",
      "abstract": "",
      "year": "1997",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "J. N. Tsitsiklis and B. Van Roy"
    },
    {
      "index": 50,
      "title": "Average cost temporal-difference learning",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Conference on Decision and Control",
      "authors": "J. N. Tsitsiklis and B. Van Roy"
    },
    {
      "index": 51,
      "title": "Developing a predictive approach to knowledge",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "A. White et al."
    },
    {
      "index": 52,
      "title": "Sample complexity bounds for two timescale value-based reinforcement learning algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS)",
      "authors": "T. Xu and Y. Liang"
    },
    {
      "index": 53,
      "title": "Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "T. Xu, Z. Wang, and Y. Liang",
      "orig_title": "Improving sample complexity bounds for (natural) actor-critic algorithms",
      "paper_id": "2004.12956v4"
    },
    {
      "index": 54,
      "title": "Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "T. Xu, Z. Wang, and Y. Liang"
    },
    {
      "index": 55,
      "title": "Doubly robust off-policy actor-critic: convergence and optimality",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "T. Xu, Z. Yang, Z. Wang, and Y. Liang"
    },
    {
      "index": 56,
      "title": "Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "T. Xu, S. Zou, and Y. Liang",
      "orig_title": "Two time-scale off-policy TD learning: Non-asymptotic analysis over markovian samples",
      "paper_id": "1909.11907v1"
    },
    {
      "index": 57,
      "title": "Reinforcement ranking",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint",
      "authors": "H. Yao and D. Schuurmans"
    },
    {
      "index": 58,
      "title": "Variational Policy Gradient Method for Reinforcement Learning with General Utilities",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "J. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and M. Wang",
      "orig_title": "Variational policy gradient method for reinforcement learning with general utilities",
      "paper_id": "2007.02151v1"
    },
    {
      "index": 59,
      "title": "GenDICE: Generalized Offline Estimation of Stationary Values",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "R. Zhang, B. Dai, L. Li, and D. Schuurmans",
      "orig_title": "Gendice: generalized offline estimation of stationary values",
      "paper_id": "2002.09072v1"
    },
    {
      "index": 60,
      "title": "Generalized Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "S. Zhang, W. Boehmer, and S. Whiteson",
      "orig_title": "Generalized off-policy actor-critic",
      "paper_id": "1903.11329v8"
    },
    {
      "index": 61,
      "title": "GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "S. Zhang, B. Liu, and S. Whiteson",
      "orig_title": "GradientDICE: rethinking generalized offline estimation of stationary values",
      "paper_id": "2001.11113v7"
    },
    {
      "index": 62,
      "title": "Provably convergent off-policy actor-critic with function approximation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "S. Zhang, B. Liu, H. Yao, and S. Whiteson"
    },
    {
      "index": 63,
      "title": "Provably convergent two-timescale off-policy actor-critic with function approximation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "S. Zhang, B. Liu, H. Yao, and S. Whiteson"
    },
    {
      "index": 64,
      "title": "Learning Retrospective Knowledge with Reverse Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "S. Zhang, V. Veeriah, and S. Whiteson",
      "orig_title": "Learning retrospective knowledge with reverse reinforcement learning",
      "paper_id": "2007.06703v3"
    }
  ]
}