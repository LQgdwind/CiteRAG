{
  "paper_id": "2111.11485v2",
  "title": "A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning",
  "abstract": "Abstract\nRepresentation learning lies at the heart of the empirical success of deep learning for dealing with the curse of dimensionality. However, the power of representation learning has not been fully exploited yet in reinforcement learning (RL), due to i), the trade-off between expressiveness and tractability; and ii), the coupling between exploration and representation learning. In this paper, we first reveal the fact that under some noise assumption in the stochastic control model, we can obtain the linear spectral feature of its corresponding Markov transition operator in closed-form for free. Based on this observation, we propose¬†Spectral Dynamics Embedding¬†(SPEDE), which breaks the trade-off and completes optimistic exploration for representation learning by exploiting the structure of the noise. We provide rigorous theoretical analysis of¬†SPEDE, and demonstrate the practical superior performance over the existing state-of-the-art empirical algorithms on several benchmarks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Regret bounds for the adaptive control of linear quadratic systems",
      "abstract": "",
      "year": "2011",
      "venue": "COLT. PMLR",
      "authors": "Yasin Abbasi-Yadkori and Csaba Szepesv√°ri"
    },
    {
      "index": 1,
      "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.10814",
      "authors": "Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun",
      "orig_title": "Flambe: Structural complexity and representation learning of low rank mdps",
      "paper_id": "2006.10814v2"
    },
    {
      "index": 2,
      "title": "Optimality and approximation with policy gradient methods in markov decision processes",
      "abstract": "",
      "year": "2020",
      "venue": "COLT. PMLR",
      "authors": "Alekh Agarwal, Sham¬†M Kakade, Jason¬†D Lee, and Gaurav Mahajan"
    },
    {
      "index": 3,
      "title": "Kernels for vector-valued functions: A review",
      "abstract": "",
      "year": "2012",
      "venue": "Foundations and Trends¬Æ in Machine Learning",
      "authors": "Mauricio¬†A √Ålvarez, Lorenzo Rosasco, and Neil¬†D Lawrence"
    },
    {
      "index": 4,
      "title": "Fitted q-iteration in continuous action-space mdps",
      "abstract": "",
      "year": "2008",
      "venue": "NeurIPS",
      "authors": "Andr√°s Antos, Csaba Szepesv√°ri, and R√©mi Munos"
    },
    {
      "index": 5,
      "title": "Theory of reproducing kernels",
      "abstract": "",
      "year": "1950",
      "venue": "Transactions of the AMS",
      "authors": "Nachman Aronszajn"
    },
    {
      "index": 6,
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ICML. PMLR",
      "authors": "Mohammad¬†Gheshlaghi Azar, Ian Osband, and R√©mi Munos",
      "orig_title": "Minimax regret bounds for reinforcement learning",
      "paper_id": "1703.05449v2"
    },
    {
      "index": 7,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE TPAMI",
      "authors": "Yoshua Bengio, Aaron Courville, and Pascal Vincent"
    },
    {
      "index": 8,
      "title": "Variational Inference: A Review for Statisticians",
      "abstract": "",
      "year": "2017",
      "venue": "JASA",
      "authors": "David¬†M Blei, Alp Kucukelbir, and Jon¬†D McAuliffe",
      "orig_title": "Variational inference: A review for statisticians",
      "paper_id": "1601.00670v9"
    },
    {
      "index": 9,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 10,
      "title": "Convergence of langevin mcmc in kl-divergence",
      "abstract": "",
      "year": "2018",
      "venue": "ALT. PMLR",
      "authors": "Xiang Cheng and Peter Bartlett"
    },
    {
      "index": 11,
      "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1805.12114",
      "authors": "Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine",
      "orig_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
      "paper_id": "1805.12114v2"
    },
    {
      "index": 12,
      "title": "Model-Based Reinforcement Learning via Meta-Policy Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "CoRL. PMLR",
      "authors": "Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel",
      "orig_title": "Model-based reinforcement learning via meta-policy optimization",
      "paper_id": "1809.05214v1"
    },
    {
      "index": 13,
      "title": "Learning linear-quadratic regulators efficiently with only tùë°\\sqrt{t} regret",
      "abstract": "",
      "year": "2019",
      "venue": "ICML. PMLR",
      "authors": "Alon Cohen, Tomer Koren, and Yishay Mansour"
    },
    {
      "index": 14,
      "title": "Scalable Kernel Methods via Doubly Stochastic Gradients",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Bo¬†Dai, Bo¬†Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina¬†F Balcan, and Le¬†Song",
      "orig_title": "Scalable kernel methods via doubly stochastic gradients",
      "paper_id": "1407.5599v4"
    },
    {
      "index": 15,
      "title": "Sbeed: Convergent reinforcement learning with nonlinear function approximation",
      "abstract": "",
      "year": "2018",
      "venue": "ICML. PMLR",
      "authors": "Bo¬†Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le¬†Song"
    },
    {
      "index": 16,
      "title": "Stochastic linear optimization under bandit feedback",
      "abstract": "",
      "year": "2008",
      "venue": "COLT",
      "authors": "Varsha Dani, Thomas¬†P Hayes, and Sham¬†M Kakade"
    },
    {
      "index": 17,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation",
      "authors": "Peter Dayan"
    },
    {
      "index": 18,
      "title": "Provably efficient RL with Rich Observations via Latent State Decoding",
      "abstract": "",
      "year": "2019",
      "venue": "ICML. PMLR",
      "authors": "Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford",
      "orig_title": "Provably efficient rl with rich observations via latent state decoding",
      "paper_id": "1901.09018v3"
    },
    {
      "index": 19,
      "title": "State Aggregation Learning from Markov Transition Data",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.02619",
      "authors": "Yaqi Duan, Zheng¬†Tracy Ke, and Mengdi Wang",
      "orig_title": "State aggregation learning from markov transition data",
      "paper_id": "1811.02619v3"
    },
    {
      "index": 20,
      "title": "Metrics for finite markov decision processes",
      "abstract": "",
      "year": "2004",
      "venue": "UAI",
      "authors": "Norm Ferns, Prakash Panangaden, and Doina Precup"
    },
    {
      "index": 21,
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ICML. PMLR",
      "authors": "Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc¬†G Bellemare",
      "orig_title": "Deepmdp: Learning continuous latent space models for representation learning",
      "paper_id": "1906.02736v1"
    },
    {
      "index": 22,
      "title": "Probability and random processes",
      "abstract": "",
      "year": "2020",
      "venue": "Oxford university press",
      "authors": "Geoffrey Grimmett and David Stirzaker"
    },
    {
      "index": 23,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "ICML. PMLR",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 24,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "ICML. PMLR",
      "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 25,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "JMLR",
      "authors": "Thomas Jaksch, Ronald Ortner, and Peter Auer"
    },
    {
      "index": 26,
      "title": "Is q-learning provably efficient?",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael¬†I Jordan"
    },
    {
      "index": 27,
      "title": "A short note on concentration inequalities for random vectors with subgaussian norm",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.03736",
      "authors": "Chi Jin, Praneeth Netrapalli, Rong Ge, Sham¬†M Kakade, and Michael¬†I Jordan"
    },
    {
      "index": 28,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "COLT. PMLR",
      "authors": "Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael¬†I Jordan",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 29,
      "title": "Information Theoretic Regret Bounds for Online Nonlinear Control",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.12466",
      "authors": "Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun",
      "orig_title": "Information theoretic regret bounds for online nonlinear control",
      "paper_id": "2006.12466v1"
    },
    {
      "index": 30,
      "title": "Deep Successor Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.02396",
      "authors": "Tejas¬†D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel¬†J Gershman",
      "orig_title": "Deep successor reinforcement learning",
      "paper_id": "1606.02396v1"
    },
    {
      "index": 31,
      "title": "Model-Ensemble Trust-Region Policy Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.10592",
      "authors": "Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel",
      "orig_title": "Model-ensemble trust-region policy optimization",
      "paper_id": "1802.10592v2"
    },
    {
      "index": 32,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "JMLR",
      "authors": "Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel",
      "orig_title": "End-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 33,
      "title": "Proto-value functions: A laplacian framework for learning representation and control in markov decision processes",
      "abstract": "",
      "year": "2007",
      "venue": "JMLR",
      "authors": "Sridhar Mahadevan and Mauro Maggioni"
    },
    {
      "index": 34,
      "title": "Certainty Equivalence is Efficient for Linear Quadratic Control",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Horia Mania, Stephen Tu, and Benjamin Recht",
      "orig_title": "Certainty equivalence is efficient for linear quadratic control",
      "paper_id": "1902.07826v2"
    },
    {
      "index": 35,
      "title": "Active Learning for Nonlinear System Identification with Guarantees",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.10277",
      "authors": "Horia Mania, Michael¬†I Jordan, and Benjamin Recht",
      "orig_title": "Active learning for nonlinear system identification with guarantees",
      "paper_id": "2006.10277v1"
    },
    {
      "index": 36,
      "title": "Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML. PMLR",
      "authors": "Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford",
      "orig_title": "Kinematic state abstraction and provably efficient rich-observation reinforcement learning",
      "paper_id": "1911.05815v1"
    },
    {
      "index": 37,
      "title": "Playing atari with deep reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.5602",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller"
    },
    {
      "index": 38,
      "title": "Model-free Representation Learning and Exploration in Low-rank MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.07035",
      "authors": "Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal",
      "orig_title": "Model-free representation learning and exploration in low-rank mdps",
      "paper_id": "2102.07035v2"
    },
    {
      "index": 39,
      "title": "Provable Representation Learning for Imitation with Contrastive Fourier Features",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.12272",
      "authors": "Ofir Nachum and Mengjiao Yang",
      "orig_title": "Provable representation learning for imitation with contrastive fourier features",
      "paper_id": "2105.12272v2"
    },
    {
      "index": 40,
      "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1702.08892",
      "authors": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans",
      "orig_title": "Bridging the gap between value and policy based reinforcement learning",
      "paper_id": "1702.08892v3"
    },
    {
      "index": 41,
      "title": "Mcmc using hamiltonian dynamics",
      "abstract": "",
      "year": "2011",
      "venue": "Handbook of markov chain monte carlo",
      "authors": "Radford¬†M Neal et¬†al."
    },
    {
      "index": 42,
      "title": "Model-based Reinforcement Learning and the Eluder Dimension",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Ian Osband and Benjamin Van¬†Roy",
      "orig_title": "Model-based reinforcement learning and the eluder dimension",
      "paper_id": "1406.1853v2"
    },
    {
      "index": 43,
      "title": "Markov decision processes: discrete stochastic dynamic programming",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Martin¬†L Puterman"
    },
    {
      "index": 44,
      "title": "Random features for large-scale kernel machines",
      "abstract": "",
      "year": "2007",
      "venue": "NeurIPS",
      "authors": "Ali Rahimi and Benjamin Recht"
    },
    {
      "index": 45,
      "title": "Fourier analysis on groups",
      "abstract": "",
      "year": "2017",
      "venue": "Courier Dover Publications",
      "authors": "Walter Rudin"
    },
    {
      "index": 46,
      "title": "Eluder dimension and the sample complexity of optimistic exploration",
      "abstract": "",
      "year": "2013",
      "venue": "NeurIPS",
      "authors": "Daniel Russo and Benjamin Van¬†Roy"
    },
    {
      "index": 47,
      "title": "Learning to optimize via posterior sampling",
      "abstract": "",
      "year": "2014",
      "venue": "Mathematics of Operations Research",
      "authors": "Daniel Russo and Benjamin Van¬†Roy"
    },
    {
      "index": 48,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "nature",
      "authors": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et¬†al."
    },
    {
      "index": 49,
      "title": "Naive Exploration is Optimal for Online LQR",
      "abstract": "",
      "year": "2020",
      "venue": "ICML. PMLR",
      "authors": "Max Simchowitz and Dylan Foster",
      "orig_title": "Naive exploration is optimal for online lqr",
      "paper_id": "2001.09576v4"
    },
    {
      "index": 50,
      "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
      "abstract": "",
      "year": "1990",
      "venue": "Machine learning proceedings 1990. Elsevier",
      "authors": "Richard¬†S Sutton"
    },
    {
      "index": 51,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Richard¬†S Sutton and Andrew¬†G Barto"
    },
    {
      "index": 52,
      "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.04652",
      "authors": "Masatoshi Uehara, Xuezhou Zhang, and Wen Sun",
      "orig_title": "Representation learning for online and offline rl in low-rank mdps",
      "paper_id": "2110.04652v3"
    },
    {
      "index": 53,
      "title": "High-dimensional statistics: A non-asymptotic viewpoint",
      "abstract": "",
      "year": "2019",
      "venue": "Cambridge University Press",
      "authors": "Martin¬†J Wainwright"
    },
    {
      "index": 54,
      "title": "Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Ruosong Wang, Russ¬†R Salakhutdinov, and Lin Yang",
      "orig_title": "Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension",
      "paper_id": "2005.10804v3"
    },
    {
      "index": 55,
      "title": "Benchmarking model-based reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.02057",
      "authors": "Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba"
    },
    {
      "index": 56,
      "title": "Bayesian learning via stochastic gradient langevin dynamics",
      "abstract": "",
      "year": "2011",
      "venue": "ICML",
      "authors": "Max Welling and Yee¬†W Teh"
    },
    {
      "index": 57,
      "title": "The Laplacian in RL: Learning Representations with Efficient Approximations",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04586",
      "authors": "Yifan Wu, George Tucker, and Ofir Nachum",
      "orig_title": "The laplacian in rl: Learning representations with efficient approximations",
      "paper_id": "1810.04586v1"
    },
    {
      "index": 58,
      "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Lin Yang and Mengdi Wang",
      "orig_title": "Sample-optimal parametric q-learning using linearly additive features",
      "paper_id": "1902.04779v2"
    },
    {
      "index": 59,
      "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound",
      "abstract": "",
      "year": "2020",
      "venue": "ICML. PMLR",
      "authors": "Lin Yang and Mengdi Wang",
      "orig_title": "Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound",
      "paper_id": "1905.10389v2"
    },
    {
      "index": 60,
      "title": "Spectral state compression of markov processes",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE TIT",
      "authors": "Anru Zhang and Mengdi Wang"
    },
    {
      "index": 61,
      "title": "Making Linear MDPs Practical via Contrastive Representation Learning",
      "abstract": "",
      "year": "2022",
      "venue": "ICML. PMLR",
      "authors": "Tianjun¬†Zhan Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo¬†Dai",
      "orig_title": "Making linear mdps practical via contrastive representation learnin",
      "paper_id": "2207.07150v2"
    }
  ]
}