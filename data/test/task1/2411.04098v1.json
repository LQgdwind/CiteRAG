{
  "paper_id": "2411.04098v1",
  "title": "Interpretable and efficient data-driven Discovery and Control of distributed systems",
  "abstract": "Abstract\nEffectively controlling systems governed by Partial Differential Equations (PDEs) is crucial in several fields of Applied Sciences and Engineering. These systems usually yield significant challenges to conventional control schemes due\nto their nonlinear dynamics, partial observability, high-dimensionality once discretized, distributed nature, and the requirement for low-latency feedback control. Reinforcement Learning (RL),\nparticularly Deep RL (DRL), has recently emerged as a promising control paradigm for such systems,\ndemonstrating exceptional capabilities in managing high-dimensional, nonlinear dynamics.\nHowever, DRL faces challenges including sample inefficiency, robustness issues, and an overall lack of interpretability.\nTo address these issues, we propose a data-efficient, interpretable, and scalable Dyna-style\nModel-Based RL framework for PDE control, combining the Sparse Identification of Nonlinear\nDynamics with Control (SINDy-C) algorithm and an autoencoder (AE) framework for the sake of dimensionality reduction of PDE states and actions. This novel approach enables fast rollouts, reducing the need for extensive environment interactions, and provides an\ninterpretable latent space representation of the PDE forward dynamics.\nWe validate our method on two PDE problems describing fluid flows – namely, the 1D Burgers equation and 2D Navier-Stokes equations – comparing it against a model-free baseline, and carrying out an extensive analysis of the learned dynamics.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A review of wind turbine-oriented active flow control strategies",
      "abstract": "",
      "year": "2017",
      "venue": "Experiments in Fluids",
      "authors": "Sandrine Aubrun, Annie Leroy and Philippe Devinant"
    },
    {
      "index": 1,
      "title": "Online identification and control of PDEs via Reinforcement Learning methods",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Alessandro Alla, Agnese Pacifico, Michele Palladino and Andrea Pesare",
      "orig_title": "Online identification and control of PDEs via Reinforcement Learning methods",
      "paper_id": "2308.04068v1"
    },
    {
      "index": 2,
      "title": "Model Predictive Control for Partial Differential Equations",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Nils Altmüller"
    },
    {
      "index": 3,
      "title": "Model-Based Reinforcement Learning with SINDy",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Rushiv Arora, Bruno Castro Silva and Eliot Moss",
      "orig_title": "Model-Based Reinforcement Learning with SINDy",
      "paper_id": "2208.14501v1"
    },
    {
      "index": 4,
      "title": "Discovering Governing Equations from Partial Measurements with Deep Delay Autoencoders",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "Joseph Bakarji, Kathleen P. Champion, J. Kutz and Steven L. Brunton",
      "orig_title": "Discovering Governing Equations from Partial Measurements with Deep Delay Autoencoders",
      "paper_id": "2201.05136v1"
    },
    {
      "index": 5,
      "title": "Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives",
      "abstract": "",
      "year": "2012",
      "venue": "CoRR",
      "authors": "Yoshua Bengio, Aaron C. Courville and Pascal Vincent"
    },
    {
      "index": 6,
      "title": "Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "Nicolò Botteghi and Urban Fasel",
      "orig_title": "Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies",
      "paper_id": "2403.15267v2"
    },
    {
      "index": 7,
      "title": "PDE Control Gym: A Benchmark for Data-Driven Boundary Control of Partial Differential Equations",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "Luke Bhan, Yuexin Bian, Miroslav Krstic and Yuanyuan Shi",
      "orig_title": "PDE Control Gym: A Benchmark for Data-Driven Boundary Control of Partial Differential Equations",
      "paper_id": "2405.11401v2"
    },
    {
      "index": 8,
      "title": "Machine Learning for Fluid Mechanics",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Review of Fluid Mechanics",
      "authors": "Steven L. Brunton, Bernd R. Noack and Petros Koumoutsakos"
    },
    {
      "index": 9,
      "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Nicolò Botteghi, Mannes Poel and Christoph Brune",
      "orig_title": "Unsupervised representation learning in deep reinforcement learning: A review",
      "paper_id": "2208.14226v3"
    },
    {
      "index": 10,
      "title": "Sparse Identification of Nonlinear Dynamics with Control (SINDYc)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Steven L. Brunton, Joshua L. Proctor and J. Kutz"
    },
    {
      "index": 11,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Greg Brockman et al."
    },
    {
      "index": 12,
      "title": "Data-driven discovery of coordinates and governing equations",
      "abstract": "",
      "year": "2019",
      "venue": "National Academy of Sciences",
      "authors": "Kathleen Champion, Bethany Lusch, J. Kutz and Steven L. Brunton"
    },
    {
      "index": 13,
      "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kurtland Chua, Roberto Calandra, Rowan McAllister and Sergey Levine",
      "orig_title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "paper_id": "1805.12114v2"
    },
    {
      "index": 14,
      "title": "Model-Based Reinforcement Learning via Meta-Policy Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Ignasi Clavera et al.",
      "orig_title": "Model-Based Reinforcement Learning via Meta-Policy Optimization",
      "paper_id": "1809.05214v1"
    },
    {
      "index": 15,
      "title": "Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions",
      "abstract": "",
      "year": "2023",
      "venue": "Computer Methods in Applied Mechanics and Engineering",
      "authors": "Paolo Conti et al.",
      "orig_title": "Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions",
      "paper_id": "2211.06786v2"
    },
    {
      "index": 16,
      "title": "VENI, VINDy, VICI – a variational reduced-order modeling framework with uncertainty quantification",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "Paolo Conti et al.",
      "orig_title": "VENI, VINDy, VICI: a variational reduced-order modeling framework with uncertainty quantification",
      "paper_id": "2405.20905v1"
    },
    {
      "index": 17,
      "title": "PILCO: a model-based and data-efficient approach to policy search",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Machine Learning",
      "authors": "Marc Peter Deisenroth and Carl Edward Rasmussen"
    },
    {
      "index": 18,
      "title": "SINDy with Control: A Tutorial",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Conference on Decision and Control (CDC)",
      "authors": "Urban Fasel et al."
    },
    {
      "index": 19,
      "title": "Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control",
      "abstract": "",
      "year": "2022",
      "venue": "Royal Society A: Mathematical, Physical and Engineering Sciences",
      "authors": "Urban Fasel, J. Kutz, Bing W. Brunton and Steve L. Brunton",
      "orig_title": "Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control",
      "paper_id": "2111.10992v1"
    },
    {
      "index": 20,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "MIT Press",
      "authors": "Ian Goodfellow, Yoshua Bengio and Aaron Courville",
      "orig_title": "Deep Learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 21,
      "title": "Bayesian autoencoders for data-driven discovery of coordinates, governing equations and fundamental constants",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "L. Gao and J. Kutz"
    },
    {
      "index": 22,
      "title": "Nonlinear Model Predictive Control : Theory and Algorithms. 2nd Edition",
      "abstract": "",
      "year": "2017",
      "venue": "Springer",
      "authors": "Lars Grüne and Jürgen Pannek"
    },
    {
      "index": 23,
      "title": "Mastering Atari with Discrete World Models",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi and Jimmy Ba",
      "orig_title": "Mastering Atari with Discrete World Models",
      "paper_id": "2010.02193v4"
    },
    {
      "index": 24,
      "title": "Sparsifying Priors for Bayesian Uncertainty Quantification in Model Discovery",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Seth M. Hirsh, David A. Barajas-Solano and J. Kutz"
    },
    {
      "index": 25,
      "title": "Optimization with PDE Constraints",
      "abstract": "",
      "year": "2008",
      "venue": "Springer Netherlands",
      "authors": "Michael Hinze, Rene Pinnau, Michael Ulbrich and Stefan Ulbrich"
    },
    {
      "index": 26,
      "title": "Reducing the Dimensionality of Data with Neural Networks",
      "abstract": "",
      "year": "2006",
      "venue": "Science",
      "authors": "Geoffrey Hinton and Ruslan Salakhutdinov"
    },
    {
      "index": 27,
      "title": "When to Trust Your Model: Model-Based Policy Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Michael Janner, Justin Fu, Marvin Zhang and Sergey Levine"
    },
    {
      "index": 28,
      "title": "A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Zhengyao Jiang, Dixing Xu and Jinjun Liang",
      "orig_title": "A deep reinforcement learning framework for the financial portfolio management problem",
      "paper_id": "1706.10059v2"
    },
    {
      "index": 29,
      "title": "PySINDy: A comprehensive Python package for robust sparse system identification",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Alan A Kaptanoglu et al."
    },
    {
      "index": 30,
      "title": "Adam: A Method for Stochastic Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Diederik Kingma and Jimmy Ba"
    },
    {
      "index": 31,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "National Academy of Sciences",
      "authors": "James Kirkpatrick et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 32,
      "title": "Deep reinforcement learning for feedback control in a collective flashing ratchet",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Dong-Kyum Kim and Hawoong Jeong",
      "orig_title": "Deep Reinforcement Learning for Feedback Control in a Collective Flashing Ratchet",
      "paper_id": "2011.10357v3"
    },
    {
      "index": 33,
      "title": "Sparse identification of nonlinear dynamics for model predictive control in the low-data limit",
      "abstract": "",
      "year": "2018",
      "venue": "Royal Society A: Mathematical, Physical and Engineering Sciences",
      "authors": "Eurika Kaiser, J. Kutz and Steven L. Brunton"
    },
    {
      "index": 34,
      "title": "State Representation Learning for Control: An Overview",
      "abstract": "",
      "year": "2018",
      "venue": "Neural Networks",
      "authors": "Timothée Lesort, Natalia Díaz-Rodríguez, Jean-Franois Goudou and David Filliat",
      "orig_title": "State representation learning for control: An overview",
      "paper_id": "1802.04181v2"
    },
    {
      "index": 35,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "Sergey Levine, Chelsea Finn, Trevor Darrell and Pieter Abbeel",
      "orig_title": "End-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 36,
      "title": "Deep reinforcement learning for dialogue generation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint",
      "authors": "Jiwei Li et al."
    },
    {
      "index": 37,
      "title": "Ray RLLib: A Composable and Scalable Reinforcement Learning Library",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Eric Liang et al."
    },
    {
      "index": 38,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of Learning and Motivation",
      "authors": "Michael McCloskey and Neal J Cohen"
    },
    {
      "index": 39,
      "title": "Playing Atari with deep reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint",
      "authors": "Volodymyr Mnih et al."
    },
    {
      "index": 40,
      "title": "Optimal Control of Partial Differential Equations: Analysis, Approximation, and Applications",
      "abstract": "",
      "year": "2022",
      "venue": "Springer International Publishing",
      "authors": "Andrea Manzoni, Alfio Quarteroni and Sandro Salsa"
    },
    {
      "index": 41,
      "title": "Learning Dexterous In-Hand Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "OpenAI et al.",
      "orig_title": "Learning Dexterous In-Hand Manipulation",
      "paper_id": "1808.00177v5"
    },
    {
      "index": 42,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "OpenAI et al.",
      "orig_title": "GPT-4 Technical Report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 43,
      "title": "Automatic differentiation in PyTorch",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS-W",
      "authors": "Adam Paszke et al."
    },
    {
      "index": 44,
      "title": "Distributed Control of Partial Differential Equations Using Convolutional Reinforcement Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Sebastian Peitz et al.",
      "orig_title": "Distributed Control of Partial Differential Equations Using Convolutional Reinforcement Learning",
      "paper_id": "2301.10737v2"
    },
    {
      "index": 45,
      "title": "Feedback Control of Nonlinear PDEs Using Data-Efficient Reduced Order Models Based on the Koopman Operator",
      "abstract": "",
      "year": "2020",
      "venue": "Koopman Operator in Systems and Control",
      "authors": "Sebastian Peitz and Stefan Klus"
    },
    {
      "index": 46,
      "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Antonin Raffin et al."
    },
    {
      "index": 47,
      "title": "Data-driven discovery of partial differential equations",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor and J. Kutz"
    },
    {
      "index": 48,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 49,
      "title": "Proximal Policy Optimization Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "John Schulman et al."
    },
    {
      "index": 50,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "David Silver et al."
    },
    {
      "index": 51,
      "title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming",
      "abstract": "",
      "year": "1990",
      "venue": "Machine Learning Proceedings 1990",
      "authors": "Richard S. Sutton"
    },
    {
      "index": 52,
      "title": "Dyna, an integrated architecture for learning, planning, and reacting",
      "abstract": "",
      "year": "1991",
      "venue": "SIGART Bull.",
      "authors": "Richard S. Sutton"
    },
    {
      "index": 53,
      "title": "Perspectives on predicting and controlling turbulent flows through deep learning",
      "abstract": "",
      "year": "2024",
      "venue": "Physics of Fluids",
      "authors": "Ricardo Vinuesa"
    },
    {
      "index": 54,
      "title": "Benchmarking Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Tingwu Wang et al."
    },
    {
      "index": 55,
      "title": "Information Theoretic Model Predictive Control: Theory and Applications to Autonomous Driving",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Robotics",
      "authors": "Gregory P Williams et al.",
      "orig_title": "Information-theoretic model predictive control: Theory and applications to autonomous driving",
      "paper_id": "1707.02342v1"
    },
    {
      "index": 56,
      "title": "Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv",
      "authors": "Stefan Werner and Sebastian Peitz",
      "orig_title": "Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs",
      "paper_id": "2302.07160v2"
    },
    {
      "index": 57,
      "title": "Optimizing flow control with deep reinforcement learning: Plasma actuator placement around a square cylinder",
      "abstract": "",
      "year": "2023",
      "venue": "Physics of Fluids",
      "authors": "Mustafa Z. Yousif et al."
    },
    {
      "index": 58,
      "title": "Data-Driven Equation Discovery of Ocean Mesoscale Closures",
      "abstract": "",
      "year": "2020",
      "venue": "Geophysical Research Letters",
      "authors": "Laure Zanna and Thomas Bolton"
    },
    {
      "index": 59,
      "title": "Understanding Overfitting in Deep Learning via Analogies to Adaptive Control",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Chiyuan Zhang et al."
    },
    {
      "index": 60,
      "title": "Controlgym: Large-Scale Safety-Critical Control Environments for Benchmarking Reinforcement Learning Algorithms",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Xiangyuan Zhang et al.",
      "orig_title": "Controlgym: Large-Scale Safety-Critical Control Environments for Benchmarking Reinforcement Learning Algorithms",
      "paper_id": "2311.18736v2"
    },
    {
      "index": 61,
      "title": "Jet mixing optimization using machine learning control",
      "abstract": "",
      "year": "2018",
      "venue": "Experiments in Fluids",
      "authors": "Wu Zhi et al."
    },
    {
      "index": 62,
      "title": "Neural Architecture Search with Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Barret Zoph and Quoc V. Le",
      "orig_title": "Neural Architecture Search with Reinforcement Learning",
      "paper_id": "1611.01578v2"
    },
    {
      "index": 63,
      "title": "SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "Nicholas Zolman, Urban Fasel, J. Kutz and Steven L. Brunton",
      "orig_title": "SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning",
      "paper_id": "2403.09110v1"
    }
  ]
}