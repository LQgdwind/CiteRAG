{
  "paper_id": "2309.09888v2",
  "title": "1 Introduction",
  "abstract": "Abstract\nTwo lines of work are taking the central stage in AI research.\nOn the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments.\nUnfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline.\nOn the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to eclectic contextual circumstances that users enforce by means of prompting.\nIn this paper, we argue that context is environment, and posit that in-context learning holds the key to better domain generalization.\nVia extensive theory and experiments, we show that paying attention to context—unlabeled examples as they arrive—allows\nour proposed In-Context Risk Minimization (ICRM) algorithm\nto zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements.\nFrom all of this, two messages are worth taking home.\nResearchers in domain generalization should consider environment as context, and harness the adaptive power of in-context learning.\nResearchers in LLMs should consider context as environment, to better structure data towards generalization.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A Closer Look at In-Context Learning under Distribution Shifts",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Kartik Ahuja and David Lopez-Paz",
      "orig_title": "A closer look at in-context learning under distribution shifts",
      "paper_id": "2305.16704v1"
    },
    {
      "index": 1,
      "title": "Invariant risk minimization games",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar"
    },
    {
      "index": 2,
      "title": "Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish",
      "orig_title": "Invariance principle meets information bottleneck for out-of-distribution generalization",
      "paper_id": "2106.06607v2"
    },
    {
      "index": 3,
      "title": "Predictive processing and relevance realization: exploring convergent solutions to the frame problem",
      "abstract": "",
      "year": "2022",
      "venue": "Phenomenology and the Cognitive Sciences",
      "authors": "Brett P Andersen, Mark Miller, and John Vervaeke"
    },
    {
      "index": 4,
      "title": "Machine bias",
      "abstract": "",
      "year": "2016",
      "venue": "ProPublica",
      "authors": "Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner"
    },
    {
      "index": 5,
      "title": "Invariant risk minimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz"
    },
    {
      "index": 6,
      "title": "Probability and measure theory",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": "Robert B Ash and Catherine A Doléans-Dade"
    },
    {
      "index": 7,
      "title": "Contextual Vision Transformers for Robust Representation Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv e-prints",
      "authors": "Yujia Bao and Theofanis Karaletsos"
    },
    {
      "index": 8,
      "title": "A neural probabilistic language model",
      "abstract": "",
      "year": "2000",
      "venue": "NeurIPS",
      "authors": "Yoshua Bengio, Réjean Ducharme, and Pascal Vincent"
    },
    {
      "index": 9,
      "title": "Domain generalization by marginal transfer learning",
      "abstract": "",
      "year": "2011",
      "venue": "JMLR",
      "authors": "Gilles Blanchard, Aniket Anand Deshmukh, Ürun Dogan, Gyemin Lee, and Clayton Scott"
    },
    {
      "index": 10,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 11,
      "title": "Invariant Rationalization",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola",
      "orig_title": "Invariant rationalization",
      "paper_id": "2003.09772v1"
    },
    {
      "index": 12,
      "title": "Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Yining Chen, Elan Rosenfeld, Mark Sellke, Tengyu Ma, and Andrej Risteski",
      "orig_title": "Iterative feature matching: Toward provable domain generalization with logarithmic environments",
      "paper_id": "2106.09913v2"
    },
    {
      "index": 13,
      "title": "Emnist: Extending mnist to handwritten letters",
      "abstract": "",
      "year": "2017",
      "venue": "2017 international joint conference on neural networks (IJCNN)",
      "authors": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik"
    },
    {
      "index": 14,
      "title": "Probable domain generalization via quantile risk minimization",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Cian Eastwood, Alexander Robey, Shashank Singh, Julius Von Kügelgen, Hamed Hassani, George J Pappas, and Bernhard Schölkopf"
    },
    {
      "index": 15,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 16,
      "title": "WOODS: Benchmarks for out-of-distribution generalization in time series",
      "abstract": "",
      "year": "2023",
      "venue": "TMLR",
      "authors": "Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad Javad Darvishi Bayazi, Pooneh Mousavi, Guillaume Dumas, and Irina Rish"
    },
    {
      "index": 17,
      "title": "Domain-Adversarial Training of Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "JMLR",
      "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky",
      "orig_title": "Domain-adversarial training of neural networks",
      "paper_id": "1505.07818v4"
    },
    {
      "index": 18,
      "title": "In Search of Lost Domain Generalization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Ishaan Gulrajani and David Lopez-Paz",
      "orig_title": "In search of lost domain generalization",
      "paper_id": "2007.01434v1"
    },
    {
      "index": 19,
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Dan Hendrycks and Thomas Dietterich",
      "orig_title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "paper_id": "1903.12261v1"
    },
    {
      "index": 20,
      "title": "On Feature Learning in the Presence of Spurious Correlations",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew G Wilson",
      "orig_title": "On feature learning in the presence of spurious correlations",
      "paper_id": "2210.11369v1"
    },
    {
      "index": 21,
      "title": "Enforcing Predictive Invariance across Structured Biomedical Domains",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wengong Jin, Regina Barzilay, and Tommi Jaakkola",
      "orig_title": "Enforcing predictive invariance across structured biomedical domains",
      "paper_id": "2006.03908v3"
    },
    {
      "index": 22,
      "title": "Variational autoencoders and nonlinear ica: A unifying framework",
      "abstract": "",
      "year": "2020",
      "venue": "AISTATS",
      "authors": "Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen"
    },
    {
      "index": 23,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 24,
      "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson",
      "orig_title": "Last layer re-training is sufficient for robustness to spurious correlations",
      "paper_id": "2204.02937v2"
    },
    {
      "index": 25,
      "title": "Wilds: A Benchmark of in-the-Wild Distribution Shifts",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al.",
      "orig_title": "Wilds: A benchmark of in-the-wild distribution shifts",
      "paper_id": "2012.07421v3"
    },
    {
      "index": 26,
      "title": "Out-of-distribution generalization with maximal invariant predictor",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Masanori Koyama and Shoichiro Yamaguchi"
    },
    {
      "index": 27,
      "title": "Out-of-distribution generalization via risk extrapolation (rex)",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville"
    },
    {
      "index": 28,
      "title": "Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear ICA",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Causal Learning and Reasoning",
      "authors": "Sébastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien",
      "orig_title": "Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ica",
      "paper_id": "2107.10098v3"
    },
    {
      "index": 29,
      "title": "Are All Vision Models Created Equal? A Study of the Open-Loop to Closed-Loop Causality Gap",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Mathias Lechner, Ramin Hasani, Alexander Amini, Tsun-Hsuan Wang, Thomas A Henzinger, and Daniela Rus",
      "orig_title": "Are all vision models created equal? a study of the open-loop to closed-loop causality gap",
      "paper_id": "2210.04303v1"
    },
    {
      "index": 30,
      "title": "Domain Generalization via Conditional Invariant Representations",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI conference on artificial intelligence",
      "authors": "Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao",
      "orig_title": "Domain generalization via conditional invariant representations",
      "paper_id": "1807.08479v1"
    },
    {
      "index": 31,
      "title": "Revisiting Batch Normalization For Practical Domain Adaptation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou",
      "orig_title": "Revisiting batch normalization for practical domain adaptation",
      "paper_id": "1603.04779v4"
    },
    {
      "index": 32,
      "title": "Domain Generalization using Causal Matching",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Divyat Mahajan, Shruti Tople, and Amit Sharma",
      "orig_title": "Domain generalization using causal matching",
      "paper_id": "2006.07500v3"
    },
    {
      "index": 33,
      "title": "Causally motivated shortcut removal using auxiliary labels",
      "abstract": "",
      "year": "2022",
      "venue": "AISTATS",
      "authors": "Maggie Makar, Ben Packer, Dan Moldovan, Davis Blalock, Yoni Halpern, and Alexander D’Amour"
    },
    {
      "index": 34,
      "title": "Counterfactual Theories of Causation",
      "abstract": "",
      "year": "2020",
      "venue": "The Stanford Encyclopedia of Philosophy",
      "authors": "Peter Menzies and Helen Beebee"
    },
    {
      "index": 35,
      "title": "Domain generalization via invariant feature representation",
      "abstract": "",
      "year": "2013",
      "venue": "ICML",
      "authors": "Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf"
    },
    {
      "index": 36,
      "title": "Learning Robust Models Using the Principle of Independent Causal Mechanisms",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Jens Müller, Robert Schmier, Lynton Ardizzone, Carsten Rother, and Ullrich Köthe",
      "orig_title": "Learning robust models using the principle of independent causal mechanisms",
      "paper_id": "2010.07167v2"
    },
    {
      "index": 37,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv e-prints",
      "authors": "OpenAI",
      "orig_title": "GPT-4 Technical Report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 38,
      "title": "Learning explanations that are hard to vary",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard Schölkopf"
    },
    {
      "index": 39,
      "title": "Causal inference using invariant prediction: identification and confidence intervals",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
      "authors": "Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen",
      "orig_title": "Causal inference by using invariant prediction: identification and confidence intervals",
      "paper_id": "1501.01332v3"
    },
    {
      "index": 40,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al."
    },
    {
      "index": 41,
      "title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Alexandre Rame, Corentin Dancette, and Matthieu Cord",
      "orig_title": "Fishr: Invariant gradient variances for out-of-distribution generalization",
      "paper_id": "2109.02934v3"
    },
    {
      "index": 42,
      "title": "Model-Based Domain Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Alexander Robey, George J Pappas, and Hamed Hassani",
      "orig_title": "Model-based domain generalization",
      "paper_id": "2102.11436v5"
    },
    {
      "index": 43,
      "title": "Invariant models for causal transfer learning",
      "abstract": "",
      "year": "2018",
      "venue": "JMLR",
      "authors": "Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters"
    },
    {
      "index": 44,
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.08731",
      "authors": "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang",
      "orig_title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "paper_id": "1911.08731v2"
    },
    {
      "index": 45,
      "title": "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-… hook",
      "abstract": "",
      "year": "1987",
      "venue": "Technische Universität München",
      "authors": "Jürgen Schmidhuber"
    },
    {
      "index": 46,
      "title": "Towards out-of-distribution generalization: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui"
    },
    {
      "index": 47,
      "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
      "abstract": "",
      "year": "2016",
      "venue": "Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Part III 14",
      "authors": "Baochen Sun and Kate Saenko",
      "orig_title": "Deep coral: Correlation alignment for deep domain adaptation",
      "paper_id": "1607.01719v1"
    },
    {
      "index": 48,
      "title": "Unshuffling Data for Improved Generalization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel",
      "orig_title": "Unshuffling data for improved generalization",
      "paper_id": "2002.11894v3"
    },
    {
      "index": 49,
      "title": "Statistical learning theory",
      "abstract": "",
      "year": "1998",
      "venue": "Wile",
      "authors": "Vladimir Vapnik"
    },
    {
      "index": 50,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 51,
      "title": "Counterfactual invariance to spurious correlations in text classification",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein"
    },
    {
      "index": 52,
      "title": "On Calibration and Out-of-domain Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in neural information processing systems",
      "authors": "Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit",
      "orig_title": "On calibration and out-of-domain generalization",
      "paper_id": "2102.10395v4"
    },
    {
      "index": 53,
      "title": "Workshop on spurious correlations, invariance and stability",
      "abstract": "",
      "year": "2023",
      "venue": "ICML",
      "authors": "Yoav Wald, Claudia Shi, Aahlad Puli, Amir Feder, Limor Gultchin, Mark Goldstein, Maggie Makar, Victor Veitch, and Uri Shalit"
    },
    {
      "index": 54,
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell",
      "orig_title": "Tent: Fully test-time adaptation by entropy minimization",
      "paper_id": "2006.10726v3"
    },
    {
      "index": 55,
      "title": "Provable domain generalization via invariant-feature subspace recovery",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Haoxiang Wang, Haozhe Si, Bo Li, and Han Zhao"
    },
    {
      "index": 56,
      "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.10429",
      "authors": "Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu",
      "orig_title": "Doremi: Optimizing data mixtures speeds up language model pretraining",
      "paper_id": "2305.10429v4"
    },
    {
      "index": 57,
      "title": "On the identifiability of finite mixtures",
      "abstract": "",
      "year": "1968",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "Sidney J Yakowitz and John D Spragins"
    },
    {
      "index": 58,
      "title": "Wild-time: A benchmark of in-the-wild distribution shift over time",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn"
    },
    {
      "index": 59,
      "title": "Adaptive Risk Minimization: Learning to Adapt to Domain Shift",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn",
      "orig_title": "Adaptive risk minimization: Learning to adapt to domain shift",
      "paper_id": "2007.02931v4"
    },
    {
      "index": 60,
      "title": "What is missing in irm training and evaluation? challenges and solutions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Yihua Zhang, Pranay Sharma, Parikshit Ram, Mingyi Hong, Kush Varshney, and Sijia Liu"
    }
  ]
}