{
  "paper_id": "2102.05776v2",
  "title": "Defense Against Reward Poisoning Attacks in Reinforcement Learning",
  "abstract": "Abstract\nWe study defense strategies against reward poisoning attacks in reinforcement learning. As a threat model, we consider attacks that minimally alter rewards to make the attacker’s target policy uniquely optimal under the poisoned rewards, with the optimality gap specified by an attack parameter. Our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true, unpoisoned, rewards while computing their policies under the poisoned rewards. We propose an optimization framework for deriving optimal defense policies, both when the attack parameter is known and unknown. Moreover, we show that defense policies that are solutions to the proposed optimization problems have provable performance guarantees. In particular, we provide the following bounds with respect to the true, unpoisoned, rewards: a) lower bounds on the expected return of the defense policies, and b) upper bounds on how suboptimal these defense policies are compared to the attacker’s target policy. Using simulation-based experiments, we demonstrate the effectiveness of our defense approach.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A Rewriting System for Convex Optimization Problems",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Control and Decision",
      "authors": "A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd",
      "orig_title": "A rewriting system for convex optimization problems",
      "paper_id": "1709.04494v2"
    },
    {
      "index": 1,
      "title": "Prediction with Corrupted Expert Advice",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "I. Amir, I. Attias, T. Koren, R. Livni, and Y. Mansour",
      "orig_title": "Prediction with corrupted expert advice",
      "paper_id": "2002.10286v2"
    },
    {
      "index": 2,
      "title": "Solving uncertain markov decision processes",
      "abstract": "",
      "year": "2001",
      "venue": "Carnegie Mellon University",
      "authors": "J. A. Bagnell, A. Y. Ng, and J. G. Schneider"
    },
    {
      "index": 3,
      "title": "Whatever does not kill deep reinforcement learning, makes it stronger",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "V. Behzadan and A. Munir"
    },
    {
      "index": 4,
      "title": "Poisoning attacks against support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "ICML",
      "authors": "B. Biggio, B. Nelson, and P. Laskov"
    },
    {
      "index": 5,
      "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "B. Biggio and F. Roli",
      "orig_title": "Wild patterns: Ten years after the rise of adversarial machine learning",
      "paper_id": "1712.03141v2"
    },
    {
      "index": 6,
      "title": "Stochastic linear bandits robust to adversarial attacks",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "I. Bogunovic, A. Losalka, A. Krause, and J. Scarlett"
    },
    {
      "index": 7,
      "title": "Learning from untrusted data",
      "abstract": "",
      "year": "2017",
      "venue": "STOC",
      "authors": "M. Charikar, J. Steinhardt, and G. Valiant"
    },
    {
      "index": 8,
      "title": "Casting out demons: Sanitizing training data for anomaly sensors",
      "abstract": "",
      "year": "2008",
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": "G. F. Cretu, A. Stavrou, M. E. Locasto, S. J. Stolfo, and A. D. Keromytis"
    },
    {
      "index": 9,
      "title": "Sever: A Robust Meta-Algorithm for Stochastic Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart",
      "orig_title": "Sever: A robust meta-algorithm for stochastic optimization",
      "paper_id": "1803.02815v2"
    },
    {
      "index": 10,
      "title": "Cvxpy: A python-embedded modeling language for convex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "S. Diamond and S. Boyd"
    },
    {
      "index": 11,
      "title": "Multi-view decision processes: The helper-ai problem",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "C. Dimitrakakis, D. C. Parkes, G. Radanovic, and P. Tylkin"
    },
    {
      "index": 12,
      "title": "Ethics Guidelines for Trustworthy Artificial Intelligence",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "European Commission"
    },
    {
      "index": 13,
      "title": "Towards deployment of robust cooperative ai agents: An algorithmic framework for learning adaptive policies",
      "abstract": "",
      "year": "2020",
      "venue": "AAMAS",
      "authors": "A. Ghosh, S. Tschiatschek, H. Mahdavi, and A. Singla"
    },
    {
      "index": 14,
      "title": "Better Algorithms for Stochastic Bandits with Adversarial Corruptions",
      "abstract": "",
      "year": "2019",
      "venue": "COLT",
      "authors": "A. Gupta, T. Koren, and K. Talwar",
      "orig_title": "Better algorithms for stochastic bandits with adversarial corruptions",
      "paper_id": "1902.08647v2"
    },
    {
      "index": 15,
      "title": "Robustness and explainability of artificial intelligence",
      "abstract": "",
      "year": "2020",
      "venue": "Publications Office of the European Union",
      "authors": "R. Hamon, H. Junklewitz, and I. Sanchez"
    },
    {
      "index": 16,
      "title": "Adversarial machine learning",
      "abstract": "",
      "year": "2011",
      "venue": "ACM workshop on Security and artificial intelligence",
      "authors": "L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar"
    },
    {
      "index": 17,
      "title": "Adversarial attacks on neural network policies",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel"
    },
    {
      "index": 18,
      "title": "Deceptive reinforcement learning under adversarial manipulations on cost signals",
      "abstract": "",
      "year": "2019",
      "venue": "GameSec",
      "authors": "Y. Huang and Q. Zhu"
    },
    {
      "index": 19,
      "title": "Robust dynamic programming",
      "abstract": "",
      "year": "2005",
      "venue": "Mathematics of Operations Research",
      "authors": "G. N. Iyengar"
    },
    {
      "index": 20,
      "title": "Adversarial Attacks on Stochastic Bandits",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "K. Jun, L. Li, Y. Ma, and X. Zhu",
      "orig_title": "Adversarial attacks on stochastic bandits",
      "paper_id": "1810.12188v1"
    },
    {
      "index": 21,
      "title": "Understanding Black-box Predictions via Influence Functions",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "P. W. Koh and P. Liang",
      "orig_title": "Understanding black-box predictions via influence functions",
      "paper_id": "1703.04730v3"
    },
    {
      "index": 22,
      "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "P. W. Koh, J. Steinhardt, and P. Liang",
      "orig_title": "Stronger data poisoning attacks break data sanitization defenses",
      "paper_id": "1811.00741v2"
    },
    {
      "index": 23,
      "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "B. Li, Y. Wang, A. Singh, and Y. Vorobeychik",
      "orig_title": "Data poisoning attacks on factorization-based collaborative filtering",
      "paper_id": "1608.08182v2"
    },
    {
      "index": 24,
      "title": "Reinforcement learning in robust markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "NeurIPS",
      "authors": "S. H. Lim, H. Xu, and S. Mannor"
    },
    {
      "index": 25,
      "title": "Tactics of adversarial attack on deep reinforcement learning agents",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun"
    },
    {
      "index": 26,
      "title": "Data poisoning attacks on stochastic bandits",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "F. Liu and N. B. Shroff"
    },
    {
      "index": 27,
      "title": "Stochastic bandits robust to adversarial corruptions",
      "abstract": "",
      "year": "2018",
      "venue": "STOC",
      "authors": "T. Lykouris, V. Mirrokni, and R. Paes Leme"
    },
    {
      "index": 28,
      "title": "Corruption robust exploration in episodic reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "T. Lykouris, M. Simchowitz, A. Slivkins, and W. Sun"
    },
    {
      "index": 29,
      "title": "Data Poisoning Attacks in Contextual Bandits",
      "abstract": "",
      "year": "2018",
      "venue": "GameSec",
      "authors": "Y. Ma, K. Jun, L. Li, and X. Zhu",
      "orig_title": "Data poisoning attacks in contextual bandits",
      "paper_id": "1808.05760v2"
    },
    {
      "index": 30,
      "title": "Policy Poisoning in Batch Reinforcement Learning and Control",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Y. Ma, X. Zhang, W. Sun, and J. Zhu",
      "orig_title": "Policy poisoning in batch reinforcement learning and control",
      "paper_id": "1910.05821v2"
    },
    {
      "index": 31,
      "title": "Planning in the presence of cost functions controlled by an adversary",
      "abstract": "",
      "year": "2003",
      "venue": "ICML",
      "authors": "H. B. McMahan, G. J. Gordon, and A. Blum"
    },
    {
      "index": 32,
      "title": "Using machine teaching to identify optimal training-set attacks on machine learners",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI",
      "authors": "S. Mei and X. Zhu"
    },
    {
      "index": 33,
      "title": "DeepFool: a simple and accurate method to fool deep neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard",
      "orig_title": "Deepfool: a simple and accurate method to fool deep neural networks",
      "paper_id": "1511.04599v3"
    },
    {
      "index": 34,
      "title": "Exploiting machine learning to subvert your spam filter",
      "abstract": "",
      "year": "2008",
      "venue": "LEET",
      "authors": "B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini, C. A. Sutton, J. D. Tygar, and K. Xia"
    },
    {
      "index": 35,
      "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "A. Nguyen, J. Yosinski, and J. Clune",
      "orig_title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "paper_id": "1412.1897v4"
    },
    {
      "index": 36,
      "title": "Robust control of markov decision processes with uncertain transition matrices",
      "abstract": "",
      "year": "2005",
      "venue": "Operations Research",
      "authors": "A. Nilim and L. El Ghaoui"
    },
    {
      "index": 37,
      "title": "Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "A. Paudice, L. Muñoz-González, A. Gyorgy, and E. C. Lupu",
      "orig_title": "Detection of adversarial training examples in poisoning attacks through anomaly detection",
      "paper_id": "1802.03041v1"
    },
    {
      "index": 38,
      "title": "Robust Adversarial Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta",
      "orig_title": "Robust adversarial reinforcement learning",
      "paper_id": "1703.02702v1"
    },
    {
      "index": 39,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "John Wiley & Sons, Inc.",
      "authors": "M. L. Puterman"
    },
    {
      "index": 40,
      "title": "Learning to Collaborate in Markov Decision Processes",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "G. Radanovic, R. Devidze, D. Parkes, and A. Singla",
      "orig_title": "Learning to collaborate in markov decision processes",
      "paper_id": "1901.08029v2"
    },
    {
      "index": 41,
      "title": "Policy teaching in reinforcement learning via environment poisoning attacks",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla"
    },
    {
      "index": 42,
      "title": "Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla",
      "orig_title": "Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning",
      "paper_id": "2003.12909v2"
    },
    {
      "index": 43,
      "title": "Robust policy computation in reward-uncertain mdps using nondominated policies",
      "abstract": "",
      "year": "2010",
      "venue": "AAAI",
      "authors": "K. Regan and C. Boutilier"
    },
    {
      "index": 44,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 45,
      "title": "Certified defenses for data poisoning attacks",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "J. Steinhardt, P. W. Koh, and P. Liang"
    },
    {
      "index": 46,
      "title": "Vulnerability-aware poisoning mechanism for online rl with unknown dynamics",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Y. Sun and F. Huang"
    },
    {
      "index": 47,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 48,
      "title": "Apprenticeship learning using linear programming",
      "abstract": "",
      "year": "2008",
      "venue": "ICML",
      "authors": "U. Syed, M. Bowling, and R. E. Schapire"
    },
    {
      "index": 49,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "ICLR",
      "authors": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus"
    },
    {
      "index": 50,
      "title": "The asymptotic convergence-rate of q-learning",
      "abstract": "",
      "year": "1997",
      "venue": "NeurIPS",
      "authors": "C. Szepesvári"
    },
    {
      "index": 51,
      "title": "Scaling up robust mdps using function approximation",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "A. Tamar, S. Mannor, and H. Xu"
    },
    {
      "index": 52,
      "title": "Sequential Attacks on Agents for Long-Term Adversarial Goals",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "E. Tretschk, S. J. Oh, and M. Fritz",
      "orig_title": "Sequential attacks on agents for long-term adversarial goals",
      "paper_id": "1805.12487v2"
    },
    {
      "index": 53,
      "title": "Is Feature Selection Secure against Training Data Poisoning?",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli",
      "orig_title": "Is feature selection secure against training data poisoning?",
      "paper_id": "1804.07933v1"
    },
    {
      "index": 54,
      "title": "Adversarial label flips attack on support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "ECAI",
      "authors": "H. Xiao, H. Xiao, and C. Eckert"
    },
    {
      "index": 55,
      "title": "Robust deep reinforcement learning against adversarial perturbations on observations",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-J. Hsieh"
    },
    {
      "index": 56,
      "title": "Value-based policy teaching with active indirect elicitation",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI",
      "authors": "H. Zhang and D. C. Parkes"
    },
    {
      "index": 57,
      "title": "Robust policy gradient against strong data corruption",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "X. Zhang, Y. Chen, X. Zhu, and W. Sun"
    },
    {
      "index": 58,
      "title": "Adaptive Reward-Poisoning Attacks against Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "X. Zhang, Y. Ma, A. Singla, and X. Zhu",
      "orig_title": "Adaptive reward-poisoning attacks against reinforcement learning",
      "paper_id": "2003.12613v2"
    },
    {
      "index": 59,
      "title": "Training Set Debugging Using Trusted Items",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "X. Zhang, X. Zhu, and S. Wright",
      "orig_title": "Training set debugging using trusted items",
      "paper_id": "1801.08019v1"
    }
  ]
}