{
  "paper_id": "2111.00876v2",
  "title": "On the Expressivity of Markov Reward",
  "abstract": "Abstract\nReward is the driving force for reinforcement-learning agents.\nThis paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform.\nWe frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories.\nOur main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture.\nWe then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists.\nWe conclude with an empirical study that corroborates and illustrates our theoretical findings.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Apprenticeship learning via inverse reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "International Conference on Machine Learning",
      "authors": "Pieter Abbeel and Andrew Y. Ng"
    },
    {
      "index": 1,
      "title": "Interactions between learning and evolution",
      "abstract": "",
      "year": "1992",
      "venue": "Artificial Life II",
      "authors": "David Ackley and Michael L. Littman"
    },
    {
      "index": 2,
      "title": "The steady-state control problem for Markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Quantitative Evaluation of Systems",
      "authors": "Sundararaman Akshay, Nathalie Bertrand, Serge Haddad, and Loic Helouet"
    },
    {
      "index": 3,
      "title": "Repeated Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kareem Amin, Nan Jiang, and Satinder Singh",
      "orig_title": "Repeated inverse reinforcement learning",
      "paper_id": "1705.05427v3"
    },
    {
      "index": 4,
      "title": "A distributional perspective on reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Marc G. Bellemare, Will Dabney, and Rémi Munos"
    },
    {
      "index": 5,
      "title": "The Alignment Problem: Machine Learning and Human Values, pages 130–131",
      "abstract": "",
      "year": "2021",
      "venue": "Atlantic Books",
      "authors": "Brian Christian"
    },
    {
      "index": 6,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei",
      "orig_title": "Deep reinforcement learning from human preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 7,
      "title": "Representation of a preference ordering by a numerical function",
      "abstract": "",
      "year": "1954",
      "venue": "Decision Processes",
      "authors": "Gerard Debreu"
    },
    {
      "index": 8,
      "title": "Reinforcement learning and the reward engineering principle",
      "abstract": "",
      "year": "2014",
      "venue": "AAAI Spring Symposium Series",
      "authors": "Daniel Dewey"
    },
    {
      "index": 9,
      "title": "Reinforcement Learning with a Corrupted Reward Channel",
      "abstract": "",
      "year": "2017",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg",
      "orig_title": "Reinforcement learning with a corrupted reward channel",
      "paper_id": "1705.08417v2"
    },
    {
      "index": 10,
      "title": "Hyperbolic Discounting and Learning over Multiple Horizons",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.06865",
      "authors": "William Fedus, Carles Gelada, Yoshua Bengio, Marc G. Bellemare, and Hugo Larochelle",
      "orig_title": "Hyperbolic discounting and learning over multiple horizons",
      "paper_id": "1902.06865v3"
    },
    {
      "index": 11,
      "title": "The free-energy principle: a unified brain theory?",
      "abstract": "",
      "year": "2010",
      "venue": "Nature reviews neuroscience",
      "authors": "Karl J. Friston"
    },
    {
      "index": 12,
      "title": "Reinforcement learning or active inference?",
      "abstract": "",
      "year": "2009",
      "venue": "PloS One",
      "authors": "Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel"
    },
    {
      "index": 13,
      "title": "Cooperative inverse reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell"
    },
    {
      "index": 14,
      "title": "The off-switch game",
      "abstract": "",
      "year": "2017",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell"
    },
    {
      "index": 15,
      "title": "Inverse Reward Design",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan",
      "orig_title": "Inverse reward design",
      "paper_id": "1711.02827v2"
    },
    {
      "index": 16,
      "title": "Action and Perception as Divergence Minimization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.01791",
      "authors": "Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl J. Friston, and Nicolas Heess",
      "orig_title": "Action and perception as divergence minimization",
      "paper_id": "2009.01791v3"
    },
    {
      "index": 17,
      "title": "Multi-Agent Reinforcement Learning with Temporal Logic Specifications",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Lewis Hammond, Alessandro Abate, Julian Gutierrez, and Michael Wooldridge",
      "orig_title": "Multi-agent reinforcement learning with temporal logic specifications",
      "paper_id": "2102.00582v2"
    },
    {
      "index": 18,
      "title": "Using reward machines for high-level task specification and decomposition in reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith"
    },
    {
      "index": 19,
      "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hong Jun Jeon, Smitha Milli, and Anca Dragan",
      "orig_title": "Reward-rational (implicit) choice: A unifying formalism for reward learning",
      "paper_id": "2002.04833v4"
    },
    {
      "index": 20,
      "title": "A Composable Specification Language for Reinforcement Learning Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kishor Jothimurugan, Rajeev Alur, and Osbert Bastani",
      "orig_title": "A composable specification language for reinforcement learning tasks",
      "paper_id": "2008.09293v2"
    },
    {
      "index": 21,
      "title": "A new polynomial-time algorithm for linear programming",
      "abstract": "",
      "year": "1984",
      "venue": "Annual ACM Symposium on Theory of Computing",
      "authors": "Narendra Karmarkar"
    },
    {
      "index": 22,
      "title": "Interactively shaping agents via human reinforcement: The TAMER framework",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Knowledge Capture",
      "authors": "W. Bradley Knox and Peter Stone"
    },
    {
      "index": 23,
      "title": "Stationary ordinal utility and impatience",
      "abstract": "",
      "year": "1960",
      "venue": "Econometrica: Journal of the Econometric Society",
      "authors": "Tjalling C. Koopmans"
    },
    {
      "index": 24,
      "title": "Notes on the Theory of Choice",
      "abstract": "",
      "year": "1988",
      "venue": "Westview Press",
      "authors": "David Kreps"
    },
    {
      "index": 25,
      "title": "Maximum Reward Formulation In Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS Workshop on Deep Reinforcement Learning",
      "authors": "Sai Krishna Gottipati, Yashaswi Pathak, Rohan Nuttall, Raviteja Chunduru, Ahmed Touati, Sriram Ganapathi Subramanian, Matthew E. Taylor, and Sarath Chandar",
      "orig_title": "Maximum reward formulation in reinforcement learning",
      "paper_id": "2010.03744v2"
    },
    {
      "index": 26,
      "title": "REALab: An embedded perspective on tampering",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.08820",
      "authors": "Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, and Shane Legg"
    },
    {
      "index": 27,
      "title": "Reinforcement Learning With Temporal Logic Rewards",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Intelligent Robots and Systems",
      "authors": "Xiao Li, Cristian-Ioan Vasile, and Calin Belta",
      "orig_title": "Reinforcement learning with temporal logic rewards",
      "paper_id": "1612.03471v2"
    },
    {
      "index": 28,
      "title": "The reward hypothesis, 2017",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Michael L. Littman"
    },
    {
      "index": 29,
      "title": "Environment-Independent Task Specifications via GLTL",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.04341",
      "authors": "Michael L. Littman, Ufuk Topcu, Jie Fu, Charles Isbell, Min Wen, and James MacGlashan",
      "orig_title": "Environment-independent task specifications via GLTL",
      "paper_id": "1704.04341v1"
    },
    {
      "index": 30,
      "title": "Grounding English commands to reward functions",
      "abstract": "",
      "year": "2015",
      "venue": "Robotics: Science and Systems",
      "authors": "James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael L. Littman, Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang"
    },
    {
      "index": 31,
      "title": "Convergent actor critic by humans",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Intelligent Robots and Systems",
      "authors": "James MacGlashan, Michael L. Littman, David L. Roberts, Robert Loftin, Bei Peng, and Matthew E. Taylor"
    },
    {
      "index": 32,
      "title": "Interactive Learning from Policy-Dependent Human Feedback",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "James MacGlashan, Mark K. Ho, Robert Loftin, Bei Peng, Guan Wang, David L. Roberts, Matthew E. Taylor, and Michael L. Littman",
      "orig_title": "Interactive learning from policy-dependent human feedback",
      "paper_id": "1701.06049v2"
    },
    {
      "index": 33,
      "title": "Reward functions for accelerated learning",
      "abstract": "",
      "year": "1994",
      "venue": "International Conference on Machine Learning",
      "authors": "Maja J. Mataric"
    },
    {
      "index": 34,
      "title": "Preference order dynamic programming",
      "abstract": "",
      "year": "1974",
      "venue": "Management Science",
      "authors": "L. G. Mitten"
    },
    {
      "index": 35,
      "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
      "abstract": "",
      "year": "1999",
      "venue": "International Conference on Machine Learning",
      "authors": "Andrew Y. Ng, Daishi Harada, and Stuart Russell"
    },
    {
      "index": 36,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "International Conference on Machine Learning",
      "authors": "Andrew Y. Ng, Stuart J. Russell, et al."
    },
    {
      "index": 37,
      "title": "Dueling posterior sampling for preference-based reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": "Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick"
    },
    {
      "index": 38,
      "title": "Building safe artificial intelligence: specification, robustness, and assurance, 2018",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Pedro A. Ortega, Vishal Maini, and the DeepMind Safety Team"
    },
    {
      "index": 39,
      "title": "Rethinking the discount factor in reinforcement learning: A decision theoretic approach",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Silviu Pitis"
    },
    {
      "index": 40,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Martin L. Puterman"
    },
    {
      "index": 41,
      "title": "Artificial Intelligence: A Modern Approach",
      "abstract": "",
      "year": "1994",
      "venue": "Prentice-Hall, Englewood Cliffs, NJ",
      "authors": "Stuart J. Russell and Peter Norvig"
    },
    {
      "index": 42,
      "title": "Benefits of assistance over reward learning, 2021",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, Dmitrii Krasheninnikov, Lawrence Chan, Michael D. Dennis, Pieter Abbeel, Anca Dragan, and Stuart Russell"
    },
    {
      "index": 43,
      "title": "Reward is enough",
      "abstract": "",
      "year": "2021",
      "venue": "Artificial Intelligence",
      "authors": "David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton"
    },
    {
      "index": 44,
      "title": "Intrinsically motivated reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "University of Massachusetts at Amherst Department of Computer Science",
      "authors": "Satinder Singh, Andrew G. Barto, and Nuttapong Chentanez"
    },
    {
      "index": 45,
      "title": "Where do rewards come from?",
      "abstract": "",
      "year": "2009",
      "venue": "Annual Conference of the Cognitive Science Society",
      "authors": "Satinder Singh, Richard L Lewis, and Andrew G Barto"
    },
    {
      "index": 46,
      "title": "On separating agent designer goals from agent goals: Breaking the preferences–parameters confound, 2010",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Satinder Singh, Richard L. Lewis, Jonathan Sorg, Andrew G. Barto, and Akram Helou"
    },
    {
      "index": 47,
      "title": "Ordinal dynamic programming",
      "abstract": "",
      "year": "1975",
      "venue": "Management science",
      "authors": "Matthew J. Sobel"
    },
    {
      "index": 48,
      "title": "Discounting axioms imply risk neutrality",
      "abstract": "",
      "year": "2013",
      "venue": "Annals of Operations Research",
      "authors": "Matthew J. Sobel"
    },
    {
      "index": 49,
      "title": "The Optimal Reward Problem: Designing Effective Reward for Bounded Agents",
      "abstract": "",
      "year": "2011",
      "venue": "University of Michigan",
      "authors": "Jonathan Sorg"
    },
    {
      "index": 50,
      "title": "Reward design via online gradient ascent",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jonathan Sorg, Richard L. Lewis, and Satinder Singh"
    },
    {
      "index": 51,
      "title": "Axioms for rational reinforcement learning",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Algorithmic Learning Theory",
      "authors": "Peter Sunehag and Marcus Hutter"
    },
    {
      "index": 52,
      "title": "The reward hypothesis, 2004",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "Richard S. Sutton"
    },
    {
      "index": 53,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 54,
      "title": "Apprenticeship learning using linear programming",
      "abstract": "",
      "year": "2008",
      "venue": "International Conference on Machine Learning",
      "authors": "Umar Syed, Michael Bowling, and Robert E. Schapire"
    },
    {
      "index": 55,
      "title": "Constrained MDPs and the reward hypothesis, 2020",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Csaba Szepesvári"
    },
    {
      "index": 56,
      "title": "A Boolean Task Algebra For Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Geraud Nangue Tasse, Steven James, and Benjamin Rosman",
      "orig_title": "A Boolean task algebra for reinforcement learning",
      "paper_id": "2001.01394v2"
    },
    {
      "index": 57,
      "title": "Teaching multiple tasks to an RL agent using LTL",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Rodrigo Toro Icarte, Toryn Q. Klassen, Richard Valenzano, and Sheila A. McIlraith"
    },
    {
      "index": 58,
      "title": "Theory of Games and Economic Behavior",
      "abstract": "",
      "year": "1953",
      "venue": "Princeton University Press",
      "authors": "John von Neumann and Oskar Morgenstern"
    },
    {
      "index": 59,
      "title": "Markov decision processes with ordinal rewards: Reference point-based preferences",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Automated Planning and Scheduling",
      "authors": "Paul Weng"
    },
    {
      "index": 60,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Martha White",
      "orig_title": "Unifying task specification in reinforcement learning",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 61,
      "title": "Learning to parse natural language to grounded reward functions with weak supervision",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Robotics and Automation",
      "authors": "Edward C. Williams, Nakul Gopalan, Mine Rhee, and Stefanie Tellex"
    },
    {
      "index": 62,
      "title": "A Bayesian approach for policy learning from trajectory preference queries",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Aaron Wilson, Alan Fern, and Prasad Tadepalli"
    },
    {
      "index": 63,
      "title": "A survey of preference-based reinforcement learning methods",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Machine Learning Research",
      "authors": "Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz"
    },
    {
      "index": 64,
      "title": "Preference-based reinforcement learning with finite-time guarantees",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski"
    },
    {
      "index": 65,
      "title": "What Can Learned Intrinsic Rewards Capture?",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver, and Satinder Singh",
      "orig_title": "What can learned intrinsic rewards capture?",
      "paper_id": "1912.05500v3"
    }
  ]
}