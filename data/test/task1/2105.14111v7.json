{
  "paper_id": "2105.14111v7",
  "title": "Goal Misgeneralization in Deep Reinforcement Learning",
  "abstract": "Abstract\nWe study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL).\nGoal misgeneralization occurs when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal.\nFor instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place.\nIn contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time.\nWe formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Towards Resolving Unidentifiability in Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Amin, K. and Singh, S. P.",
      "orig_title": "Towards resolving unidentifiability in inverse reinforcement learning",
      "paper_id": "1601.06569v1"
    },
    {
      "index": 1,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Mané, D.",
      "orig_title": "Concrete problems in AI safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 2,
      "title": "Out of Distribution Generalization in Machine Learning",
      "abstract": "",
      "year": "2021",
      "venue": "New York University",
      "authors": "Arjovsky, M."
    },
    {
      "index": 3,
      "title": "Invariant risk minimization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D."
    },
    {
      "index": 4,
      "title": "Good and safe uses of AI oracles",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Armstrong, S."
    },
    {
      "index": 5,
      "title": "A simple environment for showing mesa misalignment",
      "abstract": "",
      "year": "2019",
      "venue": "AI Alignment Forum",
      "authors": "Barnett, M."
    },
    {
      "index": 6,
      "title": "Recognition in Terra Incognita",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Beery, S., Van Horn, G., and Perona, P.",
      "orig_title": "Recognition in terra incognita",
      "paper_id": "1807.04975v2"
    },
    {
      "index": 7,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M."
    },
    {
      "index": 8,
      "title": "Superintelligence: Paths, Dangers, Strategies",
      "abstract": "",
      "year": "2014",
      "venue": "Oxford University Press, Inc.",
      "authors": "Bostrom, N."
    },
    {
      "index": 9,
      "title": "What does the universal prior actually look like?",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Christiano, P. F."
    },
    {
      "index": 10,
      "title": "Faulty reward functions in the wild",
      "abstract": "",
      "year": "2016",
      "venue": "OpenAI Blog",
      "authors": "Clark, J. and Amodei, D."
    },
    {
      "index": 11,
      "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.01588",
      "authors": "Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.",
      "orig_title": "Leveraging procedural generation to benchmark reinforcement learning",
      "paper_id": "1912.01588v2"
    },
    {
      "index": 12,
      "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., Hormozdiari, F., Houlsby, N., Hou, S., Jerfel, G., Karthikesalingam, A., Lucic, M., Ma, Y., McLean, C., Mincu, D., Mitani, A., Montanari, A., Nado, Z., Natarajan, V., Nielson, C., Osborne, T. F., Raman, R., Ramasamy, K., Sayres, R., Schrouff, J., Seneviratne, M., Sequeira, S., Suresh, H., Veitch, V., Vladymyrov, M., Wang, X., Webster, K., Yadlowsky, S., Yun, T., Zhai, X., and Sculley, D.",
      "orig_title": "Underspecification presents challenges for credibility in modern machine learning",
      "paper_id": "2011.03395v2"
    },
    {
      "index": 13,
      "title": "Embedded agency",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.09469",
      "authors": "Demski, A. and Garrabrant, S."
    },
    {
      "index": 14,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.01561",
      "authors": "Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.",
      "orig_title": "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 15,
      "title": "Shortcut Learning in Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Machine Intelligence",
      "authors": "Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A.",
      "orig_title": "Shortcut learning in deep neural networks",
      "paper_id": "2004.07780v5"
    },
    {
      "index": 16,
      "title": "Explaining and Harnessing Adversarial Examples",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Goodfellow, I. J., Shlens, J., and Szegedy, C.",
      "orig_title": "Explaining and harnessing adversarial examples",
      "paper_id": "1412.6572v3"
    },
    {
      "index": 17,
      "title": "The off-switch game",
      "abstract": "",
      "year": "2017",
      "venue": "Workshops at the Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Hadfield-Menell, D., Dragan, A., Abbeel, P., and Russell, S."
    },
    {
      "index": 18,
      "title": "Inverse Reward Design",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S., and Dragan, A.",
      "orig_title": "Inverse reward design",
      "paper_id": "1711.02827v2"
    },
    {
      "index": 19,
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Hendrycks, D. and Dietterich, T.",
      "orig_title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "paper_id": "1903.12261v1"
    },
    {
      "index": 20,
      "title": "Unsolved Problems in ML Safety",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Hendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J.",
      "orig_title": "Unsolved problems in ml safety",
      "paper_id": "2109.13916v5"
    },
    {
      "index": 21,
      "title": "Understanding rl vision",
      "abstract": "",
      "year": "2020",
      "venue": "Distill",
      "authors": "Hilton, J., Cammarata, N., Carter, S., Goh, G., and Olah, C."
    },
    {
      "index": 22,
      "title": "Clarifying inner alignment terminology",
      "abstract": "",
      "year": "2020",
      "venue": "AI Alignment Forum",
      "authors": "Hubinger, E."
    },
    {
      "index": 23,
      "title": "Towards an empirical investigation of inner alignment",
      "abstract": "",
      "year": "2020",
      "venue": "AI Alignment Forum",
      "authors": "Hubinger, E."
    },
    {
      "index": 24,
      "title": "Risks from Learned Optimization in Advanced Machine Learning Systems",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1906.01820",
      "authors": "Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S.",
      "orig_title": "Risks from learned optimization in advanced machine learning systems",
      "paper_id": "1906.01820v3"
    },
    {
      "index": 25,
      "title": "Reward learning from human preferences and demonstrations in Atari",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.",
      "orig_title": "Reward learning from human preferences and demonstrations in atari",
      "paper_id": "1811.06521v1"
    },
    {
      "index": 26,
      "title": "Adversarial examples are not bugs, they are features",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A."
    },
    {
      "index": 27,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015",
      "authors": "Kingma, D. P. and Ba, J."
    },
    {
      "index": 28,
      "title": "A survey of generalisation in deep reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.09794",
      "authors": "Kirk, R., Zhang, A., Grefenstette, E., and Rocktäschel, T."
    },
    {
      "index": 29,
      "title": "Hidden Incentives for Auto-induced Distributional Shift",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Krueger, D., Maharaj, T., and Leike, J.",
      "orig_title": "Hidden incentives for auto-induced distributional shift",
      "paper_id": "2009.09153v1"
    },
    {
      "index": 30,
      "title": "Out-of-distribution generalization via risk extrapolation (rex)",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., Priol, R. L., and Courville, A."
    },
    {
      "index": 31,
      "title": "On the shortest spanning subtree of a graph and the traveling salesman problem",
      "abstract": "",
      "year": "1956",
      "venue": "American Mathematical Society",
      "authors": "Kruskal, J. B."
    },
    {
      "index": 32,
      "title": "Training procgen environment with pytorch",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Lee, H."
    },
    {
      "index": 33,
      "title": "Scalable agent alignment via reward modeling: a research direction",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.",
      "orig_title": "Scalable agent alignment via reward modeling: a research direction",
      "paper_id": "1811.07871v1"
    },
    {
      "index": 34,
      "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Levine, S., Kumar, A., Tucker, G., and Fu, J.",
      "orig_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "paper_id": "2005.01643v3"
    },
    {
      "index": 35,
      "title": "2d robustness",
      "abstract": "",
      "year": "2019",
      "venue": "AI Alignment Forum",
      "authors": "Mikulik, V."
    },
    {
      "index": 36,
      "title": "Robust reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "Neural computation",
      "authors": "Morimoto, J. and Doya, K."
    },
    {
      "index": 37,
      "title": "The basic ai drives",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Omohundro, S."
    },
    {
      "index": 38,
      "title": "Agents and Devices: A Relative Definition of Agency",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Orseau, L., McGill, S. M., and Legg, S.",
      "orig_title": "Agents and devices: A relative definition of agency",
      "paper_id": "1805.12387v1"
    },
    {
      "index": 39,
      "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Pan, A., Bhatia, K., and Steinhardt, J.",
      "orig_title": "The effects of reward misspecification: Mapping and mitigating misaligned models",
      "paper_id": "2201.03544v2"
    },
    {
      "index": 40,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 41,
      "title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P.",
      "orig_title": "Sim-to-real transfer of robotic control with dynamics randomization",
      "paper_id": "1710.06537v3"
    },
    {
      "index": 42,
      "title": "Dataset shift in machine learning",
      "abstract": "",
      "year": "2009",
      "venue": "Mit Press",
      "authors": "Quiñonero-Candela, J., Sugiyama, M., Lawrence, N. D., and Schwaighofer, A."
    },
    {
      "index": 43,
      "title": "Do ImageNet Classifiers Generalize to ImageNet?",
      "abstract": "",
      "year": "2019",
      "venue": "36th International Conference on Machine Learning",
      "authors": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.",
      "orig_title": "Do ImageNet classifiers generalize to ImageNet?",
      "paper_id": "1902.10811v2"
    },
    {
      "index": 44,
      "title": "Mesa-optimization",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Rice, I. and many authors."
    },
    {
      "index": 45,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O."
    },
    {
      "index": 46,
      "title": "Generalization ¿ utility",
      "abstract": "",
      "year": "2021",
      "venue": "Stanford Existential Risk Initiative",
      "authors": "Shah, R."
    },
    {
      "index": 47,
      "title": "Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals",
      "abstract": "",
      "year": "2022",
      "venue": "Forthcoming",
      "authors": "Shah, R., Varma, V., Kumar, R., Phuong, M., Krakovna, V., Uesato, J., and Kenton, Z.",
      "orig_title": "Goal misgeneralization: Why correct specifications aren’t enough for correct goals",
      "paper_id": "2210.01790v2"
    },
    {
      "index": 48,
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1312.6034",
      "authors": "Simonyan, K., Vedaldi, A., and Zisserman, A."
    },
    {
      "index": 49,
      "title": "Intrinsically motivated reinforcement learning: An evolutionary perspective",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "authors": "Singh, S., Lewis, R. L., Barto, A. G., and Sorg, J."
    },
    {
      "index": 50,
      "title": "Corrigibility",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI Workshop: AI and Ethics",
      "authors": "Soares, N., Fallenstein, B., Armstrong, S., and Yudkowsky, E."
    },
    {
      "index": 51,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "1998",
      "venue": "A Bradford book. MIT Press",
      "authors": "Sutton, R., Barto, R., Barto, A., Barto, C., Bach, F., and Press, M."
    },
    {
      "index": 52,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Sutton, R. S. and Barto, A. G."
    },
    {
      "index": 53,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R."
    },
    {
      "index": 54,
      "title": "On motivations for miri’s highly reliable agent design research",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Taylor, J."
    },
    {
      "index": 55,
      "title": "Unbiased look at dataset bias",
      "abstract": "",
      "year": "2011",
      "venue": "CVPR 2011",
      "authors": "Torralba, A. and Efros, A. A."
    },
    {
      "index": 56,
      "title": "Optimal Policies Tend To Seek Power",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Turner, A., Smith, L., Shah, R., Critch, A., and Tadepalli, P.",
      "orig_title": "Optimal policies tend to seek power",
      "paper_id": "1912.01683v10"
    },
    {
      "index": 57,
      "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1805.08522",
      "authors": "Valle-Pérez, G., Camargo, C. Q., and Louis, A. A.",
      "orig_title": "Deep learning generalizes because the parameter-function map is biased towards simple functions",
      "paper_id": "1805.08522v5"
    },
    {
      "index": 58,
      "title": "Optimization daemons",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Yudkowsky, E."
    },
    {
      "index": 59,
      "title": "Consequences of Misaligned AI",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03896",
      "authors": "Zhuang, S. and Hadfield-Menell, D.",
      "orig_title": "Consequences of misaligned ai",
      "paper_id": "2102.03896v1"
    },
    {
      "index": 60,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Aaai",
      "authors": "Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al."
    }
  ]
}