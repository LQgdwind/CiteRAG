{
  "paper_id": "2412.02104v1",
  "title": "Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey",
  "abstract": "Abstract\nThe rapid development of Artificial Intelligence (AI) has revolutionized numerous fields, with large language models (LLMs) and computer vision (CV) systems driving advancements in natural language understanding and visual processing, respectively.\nThe convergence of these technologies has catalyzed the rise of multimodal AI, enabling richer, cross-modal understanding that spans text, vision, audio, and video modalities.\nMultimodal large language models (MLLMs), in particular, have emerged as a powerful framework, demonstrating impressive capabilities in tasks like image-text generation, visual question answering, and cross-modal retrieval.\nDespite these advancements, the complexity and scale of MLLMs introduce significant challenges in interpretability and explainability, essential for establishing transparency, trustworthiness, and reliability in high-stakes applications.\nThis paper provides a comprehensive survey on the interpretability and explainability of MLLMs, proposing a novel framework that categorizes existing research across three perspectives: (I) Data, (II) Model, (III) Training & Inference.\nWe systematically analyze interpretability from token-level to embedding-level representations, assess approaches related to both architecture analysis and design, and explore training and inference strategies that enhance transparency. By comparing various methodologies, we identify their strengths and limitations and propose future research directions to address unresolved challenges in multimodal explainability. This survey offers a foundational resource for advancing interpretability and transparency in MLLMs, guiding researchers and practitioners toward developing more accountable and robust multimodal AI systems.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.18223",
      "authors": "W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al.",
      "orig_title": "A survey of large language models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 1,
      "title": "Deep learning for computer vision: A brief review",
      "abstract": "",
      "year": "2018",
      "venue": "Computational intelligence and neuroscience",
      "authors": "A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis"
    },
    {
      "index": 2,
      "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu, Z. Yang, K.-D. Liao et al.",
      "orig_title": "A survey on multimodal large language models for autonomous driving",
      "paper_id": "2311.12320v1"
    },
    {
      "index": 3,
      "title": "Efficient Multimodal Large Language Models: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2405.10739",
      "authors": "Y. Jin, J. Li, Y. Liu, T. Gu, K. Wu, Z. Jiang, M. He, B. Zhao, X. Tan, Z. Gan et al.",
      "orig_title": "Efficient multimodal large language models: A survey",
      "paper_id": "2405.10739v2"
    },
    {
      "index": 4,
      "title": "The (R)Evolution of Multimodal Large Language Models: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2402.12451",
      "authors": "D. Caffagni, F. Cocchi, L. Barsellotti, N. Moratelli, S. Sarto, L. Baraldi, M. Cornia, and R. Cucchiara",
      "orig_title": "The (r) evolution of multimodal large language models: A survey",
      "paper_id": "2402.12451v2"
    },
    {
      "index": 5,
      "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2401.13601",
      "authors": "D. Zhang, Y. Yu, J. Dong, C. Li, D. Su, C. Chu, and D. Yu",
      "orig_title": "Mm-llms: Recent advances in multimodal large language models",
      "paper_id": "2401.13601v5"
    },
    {
      "index": 6,
      "title": "Visual Prompting in Multimodal Large Language Models: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2409.15310",
      "authors": "J. Wu, Z. Zhang, Y. Xia, X. Li, Z. Xia, A. Chang, T. Yu, S. Kim, R. A. Rossi, R. Zhang et al.",
      "orig_title": "Visual prompting in multimodal large language models: A survey",
      "paper_id": "2409.15310v1"
    },
    {
      "index": 7,
      "title": "Large Multimodal Agents: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2402.15116",
      "authors": "J. Xie, Z. Chen, R. Zhang, X. Wan, and G. Li",
      "orig_title": "Large multimodal agents: A survey",
      "paper_id": "2402.15116v1"
    },
    {
      "index": 8,
      "title": "Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2410.04509",
      "authors": "Y. Yan, S. Wang, J. Huo, H. Li, B. Li, J. Su, X. Gao, Y.-F. Zhang, T. Xu, Z. Chu et al."
    },
    {
      "index": 9,
      "title": "Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web",
      "abstract": "",
      "year": "2024",
      "venue": "ACM on Web Conference 2024",
      "authors": "Y. Yan, H. Wen, S. Zhong, W. Chen, H. Chen, Q. Wen, R. Zimmermann, and Y. Liang"
    },
    {
      "index": 10,
      "title": "Reefknot: A comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2408.09429",
      "authors": "K. Zheng, J. Chen, Y. Yan, X. Zou, and X. Hu"
    },
    {
      "index": 11,
      "title": "A Survey on Multimodal Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2306.13549",
      "authors": "S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen",
      "orig_title": "A survey on multimodal large language models",
      "paper_id": "2306.13549v4"
    },
    {
      "index": 12,
      "title": "Multimodal Large Language Models: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "2023 IEEE International Conference on Big Data (BigData)",
      "authors": "J. Wu, W. Gan, Z. Chen, S. Wan, and S. Y. Philip",
      "orig_title": "Multimodal large language models: A survey",
      "paper_id": "2311.13165v1"
    },
    {
      "index": 13,
      "title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",
      "abstract": "",
      "year": "2025",
      "venue": "Information Fusion",
      "authors": "X. Zou, Y. Yan, X. Hao, Y. Hu, H. Wen, E. Liu, J. Zhang, Y. Li, T. Li, Y. Zheng et al.",
      "orig_title": "Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook",
      "paper_id": "2402.19348v2"
    },
    {
      "index": 14,
      "title": "How to Bridge the Gap between Modalities: A Comprehensive Survey on Multi-modal Large Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2311.07594",
      "authors": "S. Song, X. Li, S. Li, S. Zhao, J. Yu, J. Ma, X. Mao, and W. Zhang",
      "orig_title": "How to bridge the gap between modalities: A comprehensive survey on multimodal large language model",
      "paper_id": "2311.07594v3"
    },
    {
      "index": 15,
      "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2410.03577",
      "authors": "X. Zou, Y. Wang, Y. Yan, S. Huang, K. Zheng, J. Chen, C. Tang, and X. Hu",
      "orig_title": "Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models",
      "paper_id": "2410.03577v2"
    },
    {
      "index": 16,
      "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2410.04780",
      "authors": "G. Zhou, Y. Yan, X. Zou, K. Wang, A. Liu, and X. Hu",
      "orig_title": "Mitigating modality prior-induced hallucinations in multimodal large language models via deciphering attention causality",
      "paper_id": "2410.04780v2"
    },
    {
      "index": 17,
      "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
      "abstract": "",
      "year": "2021",
      "venue": "29th ACM International Conference on Multimedia",
      "authors": "Y. Huang, H. Xue, B. Liu, and Y. Lu",
      "orig_title": "Unifying multimodal transformer for bi-directional image and text generation",
      "paper_id": "2110.09753v1"
    },
    {
      "index": 18,
      "title": "Generating Images with Multimodal Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Y. Koh, D. Fried, and R. R. Salakhutdinov",
      "orig_title": "Generating images with multimodal language models",
      "paper_id": "2305.17216v3"
    },
    {
      "index": 19,
      "title": "Instruct-Imagen: Image Generation with Multi-modal Instruction",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "H. Hu, K. C. Chan, Y.-C. Su, W. Chen, Y. Li, K. Sohn, Y. Zhao, X. Ben, B. Gong, W. Cohen et al.",
      "orig_title": "Instruct-imagen: Image generation with multi-modal instruction",
      "paper_id": "2401.01952v1"
    },
    {
      "index": 20,
      "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2306.13394",
      "authors": "C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun et al.",
      "orig_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models",
      "paper_id": "2306.13394v4"
    },
    {
      "index": 21,
      "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
      "abstract": "",
      "year": "2025",
      "venue": "European Conference on Computer Vision",
      "authors": "Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu et al.",
      "orig_title": "Mmbench: Is your multi-modal model an all-around player?",
      "paper_id": "2307.06281v5"
    },
    {
      "index": 22,
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al.",
      "orig_title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
      "paper_id": "2311.16502v4"
    },
    {
      "index": 23,
      "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2310.02255",
      "authors": "P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao",
      "orig_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
      "paper_id": "2310.02255v3"
    },
    {
      "index": 24,
      "title": "Unveiling the Tapestry of Consistency in Large Vision-Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2405.14156",
      "authors": "Y. Zhang, F. Xiao, T. Huang, C.-K. Fan, H. Dong, J. Li, J. Wang, K. Cheng, S. Zhang, and H. Guo",
      "orig_title": "Unveiling the tapestry of consistency in large vision-language models",
      "paper_id": "2405.14156v4"
    },
    {
      "index": 25,
      "title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2404.16790",
      "authors": "B. Li, Y. Ge, Y. Chen, Y. Ge, R. Zhang, and Y. Shan",
      "orig_title": "Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension",
      "paper_id": "2404.16790v1"
    },
    {
      "index": 26,
      "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2311.17092",
      "authors": "B. Li, Y. Ge, Y. Ge, G. Wang, R. Wang, R. Zhang, and Y. Shan",
      "orig_title": "Seed-bench-2: Benchmarking multimodal large language models",
      "paper_id": "2311.17092v1"
    },
    {
      "index": 27,
      "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.16125",
      "authors": "B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan",
      "orig_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
      "paper_id": "2307.16125v2"
    },
    {
      "index": 28,
      "title": "A Comprehensive Survey on Cross-modal Retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1607.06215",
      "authors": "K. Wang, Q. Yin, W. Wang, S. Wu, and L. Wang",
      "orig_title": "A comprehensive survey on cross-modal retrieval",
      "paper_id": "1607.06215v1"
    },
    {
      "index": 29,
      "title": "Deep supervised cross-modal retrieval",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "L. Zhen, P. Hu, X. Wang, and D. Peng"
    },
    {
      "index": 30,
      "title": "Probabilistic Embeddings for Cross-Modal Retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Chun, S. J. Oh, R. S. De Rezende, Y. Kalantidis, and D. Larlus",
      "orig_title": "Probabilistic embeddings for cross-modal retrieval",
      "paper_id": "2101.05068v2"
    },
    {
      "index": 31,
      "title": "Multimodal video sentiment analysis using deep learning approaches, a survey",
      "abstract": "",
      "year": "2021",
      "venue": "Information Fusion",
      "authors": "S. A. Abdu, A. H. Yousef, and A. Salem"
    },
    {
      "index": 32,
      "title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2108.05080",
      "authors": "H. Khalid, S. Tariq, M. Kim, and S. S. Woo",
      "orig_title": "Fakeavceleb: A novel audio-video multimodal deepfake dataset",
      "paper_id": "2108.05080v4"
    },
    {
      "index": 33,
      "title": "End-to-end Generative Pretraining for Multimodal Video Captioning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "P. H. Seo, A. Nagrani, A. Arnab, and C. Schmid",
      "orig_title": "End-to-end generative pretraining for multimodal video captioning",
      "paper_id": "2201.08264v2"
    },
    {
      "index": 34,
      "title": "Videoagent: A memory-augmented multimodal agent for video understanding",
      "abstract": "",
      "year": "2025",
      "venue": "European Conference on Computer Vision",
      "authors": "Y. Fan, X. Ma, R. Wu, Y. Du, J. Li, Z. Gao, and Q. Li"
    },
    {
      "index": 35,
      "title": "Mdmmt: Multidomain multimodal transformer for video retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "M. Dzabraev, M. Kalashnikov, S. Komkov, and A. Petiushko"
    },
    {
      "index": 36,
      "title": "End-to-End Referring Video Object Segmentation with Multimodal Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "A. Botach, E. Zheltonozhskii, and C. Baskin",
      "orig_title": "End-to-end referring video object segmentation with multimodal transformers",
      "paper_id": "2111.14821v2"
    },
    {
      "index": 37,
      "title": "UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.06353",
      "authors": "H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti, and M. Zhou",
      "orig_title": "Univl: A unified video and language pre-training model for multimodal understanding and generation",
      "paper_id": "2002.06353v3"
    },
    {
      "index": 38,
      "title": "Evolution and prospects of foundation models: From large language models to large multimodal models.",
      "abstract": "",
      "year": "2024",
      "venue": "Computers, Materials & Continua",
      "authors": "Z. Chen, L. Xu, H. Zheng, L. Chen, A. Tolba, L. Zhao, K. Yu, and H. Feng"
    },
    {
      "index": 39,
      "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2408.01319",
      "authors": "J. Wang, H. Jiang, Y. Liu, C. Ma, X. Zhang, Y. Pan, M. Liu, P. Gu, S. Xia, W. Li et al.",
      "orig_title": "A comprehensive review of multimodal large language models: Performance and challenges across different tasks",
      "paper_id": "2408.01319v1"
    },
    {
      "index": 40,
      "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding",
      "abstract": "",
      "year": "2024",
      "venue": "33rd ACM International Conference on Information and Knowledge Management",
      "authors": "Y. Yan and J. Lee",
      "orig_title": "Georeasoner: Reasoning on geospatially grounded context for natural language understanding",
      "paper_id": "2408.11366v1"
    },
    {
      "index": 41,
      "title": "“Chatgpt for shaping the future of dentistry: the potential of multi-modal large language model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "“mplug-docowl: Modularized multimodal large language model for document understanding",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 43,
      "title": "“A survey on deep multimodal learning for computer vision: advances",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 44,
      "title": "“A survey of multimodal hybrid deep learning for computer vision: Architectures",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 45,
      "title": "Multimodal Learning with Transformers: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal learning with transformers: A survey",
      "paper_id": "2206.06488v2"
    },
    {
      "index": 46,
      "title": "Video Understanding with Large Language Models: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Video understanding with large language models: A survey",
      "paper_id": "2312.17432v5"
    },
    {
      "index": 47,
      "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving",
      "abstract": "",
      "year": "2025",
      "venue": "",
      "authors": "",
      "orig_title": "“Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving",
      "paper_id": "2312.03661v3"
    },
    {
      "index": 48,
      "title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Cog-ga: A large language models-based generative agent for vision-language navigation in continuous environments",
      "paper_id": "2409.02522v2"
    },
    {
      "index": 49,
      "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable bilingual multimodal large language model for diverse biomedical tasks",
      "paper_id": "2410.18387v4"
    },
    {
      "index": 50,
      "title": "“Mcan: multimodal causal adversarial networks for dynamic effective connectivity learning from fmri and eeg data",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A comprehensive survey of large language models and multimodal large language models in medicine",
      "paper_id": "2405.08603v3"
    },
    {
      "index": 52,
      "title": "“Mmro: Are multimodal llms eligible as the brain for in-home robotics?” arXiv preprint arXiv:2406.19693",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "“Toward proactive human–robot collaborative assembly: A multimodal transfer-learning-enabled action prediction approach",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "“Progress and prospects of multimodal fusion methods in physical human–robot interaction: A review",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "“Multimodal sensors and ml-based data fusion for advanced robots",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration",
      "paper_id": "2311.12015v4"
    },
    {
      "index": 57,
      "title": "K. Gopalakrishnan",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "“Enhanced decision-making through multimodal training",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "“Deep multimodal data fusion",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "N. Díaz-Rodríguez",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "“Opportunities and challenges in explainable artificial intelligence (xai): A survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "“Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "“Towards a rigorous science of interpretable machine learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "Techniques for Interpretable Machine Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Techniques for interpretable machine learning",
      "paper_id": "1808.00033v3"
    },
    {
      "index": 65,
      "title": "“Visualizing and understanding convolutional networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "Interpretable Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable convolutional neural networks",
      "paper_id": "1710.00935v4"
    },
    {
      "index": 67,
      "title": "Transformer Interpretability Beyond Attention Visualization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Transformer interpretability beyond attention visualization",
      "paper_id": "2012.09838v2"
    },
    {
      "index": 68,
      "title": "“Explainability for large language models: A survey",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai",
      "paper_id": "2201.08164v3"
    },
    {
      "index": 70,
      "title": "“The mythos of model interpretability: In machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 71,
      "title": "“Interpreting interpretability: understanding data scientists’ use of interpretability tools for machine learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "“Designing fuzzy inference systems from data: An interpretability-oriented review",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "Shapley explainability on the data manifold",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Shapley explainability on the data manifold",
      "paper_id": "2006.01272v4"
    },
    {
      "index": 75,
      "title": "P.-J. Kindermans",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "H. Corrada Bravo",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Interpretability of deep learning models: A survey of results",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "A Survey on Neural Network Interpretability",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey on neural network interpretability",
      "paper_id": "2012.14261v3"
    },
    {
      "index": 79,
      "title": "“On interpretability of artificial neural networks: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "V. Papastefanopoulos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "Interpretability and Explainability: A Machine Learning Zoo Mini-tour",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretability and explainability: A machine learning zoo mini-tour",
      "paper_id": "2012.01805v2"
    },
    {
      "index": 82,
      "title": "“Explainable artificial intelligence: A survey",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Explaining explanations: An overview of interpretability of machine learning",
      "paper_id": "1806.00069v3"
    },
    {
      "index": 84,
      "title": "Post-hoc Interpretability for Neural NLP: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Post-hoc interpretability for neural nlp: A survey",
      "paper_id": "2108.04840v5"
    },
    {
      "index": 85,
      "title": "“Interpretability of machine learning models and representations: an introduction",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "“Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "A.-K. Dombrowski et al",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards tracing trustworthiness dynamics: Revisiting pre-training period of large language models",
      "paper_id": "2402.19465v2"
    },
    {
      "index": 89,
      "title": "Visual Interpretability for Deep Learning: a Survey",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual interpretability for deep learning: a survey",
      "paper_id": "1802.00614v2"
    },
    {
      "index": 90,
      "title": "Neuron-level Interpretation of Deep NLP Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Neuron-level interpretation of deep nlp models: A survey",
      "paper_id": "2108.13138v2"
    },
    {
      "index": 91,
      "title": "Improving Neuron-level Interpretability with White-box Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Improving neuron-level interpretability with white-box language models",
      "paper_id": "2410.16443v4"
    },
    {
      "index": 92,
      "title": "DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Dean: Deactivating the coupled neurons to mitigate fairness-privacy conflicts in large language models",
      "paper_id": "2410.16672v2"
    },
    {
      "index": 93,
      "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Identifying semantic induction heads to understand in-context learning",
      "paper_id": "2402.13055v2"
    },
    {
      "index": 94,
      "title": "“Model-agnostic interpretability of machine learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“Hybridization of model-specific and model-agnostic methods for interpretability of neural network predictions: Application to a power plant",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "“Model agnostic interpretability",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "Model Agnostic Interpretability for Multiple Instance Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Model agnostic interpretability for multiple instance learning",
      "paper_id": "2201.11701v3"
    },
    {
      "index": 98,
      "title": "A Survey on Benchmarks of Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey on benchmarks of multimodal large language models",
      "paper_id": "2408.08632v2"
    },
    {
      "index": 99,
      "title": "A Survey on Visual Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey on vision transformer",
      "paper_id": "2012.12556v6"
    },
    {
      "index": 100,
      "title": "Explainability of Vision Transformers: A Comprehensive Review and New Perspectives",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Explainability of vision transformers: A comprehensive review and new perspectives",
      "paper_id": "2311.06786v1"
    },
    {
      "index": 101,
      "title": "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal explanations: Justifying decisions and pointing to the evidence",
      "paper_id": "1802.08129v1"
    },
    {
      "index": 102,
      "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Mitigating object hallucinations in large vision-language models through visual contrastive decoding",
      "paper_id": "2311.16922v1"
    },
    {
      "index": 103,
      "title": "Causal disentanglement of multimodal data",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Causal disentanglement of multimodal data",
      "paper_id": "2310.18471v2"
    },
    {
      "index": 104,
      "title": "J. Teichert-Kluge",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“High-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning",
      "paper_id": "2203.01311v4"
    },
    {
      "index": 106,
      "title": "A Survey on Evaluation of Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey on evaluation of multimodal large language models",
      "paper_id": "2408.15769v1"
    },
    {
      "index": 107,
      "title": "“Vista: A visual and textual attention dataset for interpreting multimodal models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Benchlmm: Benchmarking cross-style visual capability of large multimodal models",
      "paper_id": "2312.02896v2"
    },
    {
      "index": 109,
      "title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring response uncertainty in mllms: An empirical evaluation under misleading scenarios",
      "paper_id": "2411.02708v1"
    },
    {
      "index": 110,
      "title": "Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
      "paper_id": "2406.07057v2"
    },
    {
      "index": 111,
      "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering",
      "paper_id": "2303.11897v3"
    },
    {
      "index": 112,
      "title": "J. Watson-Daniels",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“What are we measuring when we evaluate large vision-language models? an analysis of latent factors and biases",
      "paper_id": "2404.02415v1"
    },
    {
      "index": 114,
      "title": "A Study on Multimodal and Interactive Explanations for Visual Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A study on multimodal and interactive explanations for visual question answering",
      "paper_id": "2003.00431v1"
    },
    {
      "index": 115,
      "title": "“A deep multi-modal explanation model for zero-shot learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "paper_id": "2209.09513v2"
    },
    {
      "index": 117,
      "title": "A Multimodal Automated Interpretability Agent",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A multimodal automated interpretability agent",
      "paper_id": "2404.14394v2"
    },
    {
      "index": 118,
      "title": "“Digital forms for all: A holistic multimodal large language model agent for health data entry",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "paper_id": "2209.02604v1"
    },
    {
      "index": 120,
      "title": "Multimodal Explanations by Predicting Counterfactuality in Videos",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal explanations by predicting counterfactuality in videos",
      "paper_id": "1812.01263v2"
    },
    {
      "index": 121,
      "title": "“Discovering the real association: Multimodal causal reasoning in video question answering",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Large language models are temporal and causal reasoners for video question answering",
      "paper_id": "2310.15747v2"
    },
    {
      "index": 123,
      "title": "Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Holmes-vad: Towards unbiased and explainable video anomaly detection via multi-modal llm",
      "paper_id": "2406.12235v2"
    },
    {
      "index": 124,
      "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Tv-trees: Multimodal entailment trees for neuro-symbolic video reasoning",
      "paper_id": "2402.19467v4"
    },
    {
      "index": 125,
      "title": "Multi-modal Probabilistic Prediction of Interactive Behavior via an Interpretable Model",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-modal probabilistic prediction of interactive behavior via an interpretable model",
      "paper_id": "1903.09381v2"
    },
    {
      "index": 126,
      "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Drivegpt4: Interpretable end-to-end autonomous driving via large language model",
      "paper_id": "2310.01412v5"
    },
    {
      "index": 127,
      "title": "Group Gated Fusion on Attention-based Bidirectional Alignment for Multimodal Emotion Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "paper_id": "2201.06309v1"
    },
    {
      "index": 128,
      "title": "“Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "MusicLIME: Explainable Multimodal Music Understanding",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Musiclime: Explainable multimodal music understanding",
      "paper_id": "2409.10496v5"
    },
    {
      "index": 130,
      "title": "Enhancing Explainability in Multimodal Large Language Models Using Ontological Context",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Enhancing explainability in multimodal large language models using ontological context",
      "paper_id": "2409.18753v1"
    },
    {
      "index": 131,
      "title": "Interpreting CLIP’s Image Representation via Text-Based Decomposition",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpreting clip’s image representation via text-based decomposition",
      "paper_id": "2310.05916v4"
    },
    {
      "index": 132,
      "title": "Towards Interpreting Visual Information Processing in Vision-Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards interpreting visual information processing in vision-language models",
      "paper_id": "2410.07149v2"
    },
    {
      "index": 133,
      "title": "From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“From redundancy to relevance: Enhancing explainability in multimodal large language models",
      "paper_id": "2406.06579v3"
    },
    {
      "index": 134,
      "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models",
      "paper_id": "2403.06764v3"
    },
    {
      "index": 135,
      "title": "Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual explanations of image-text representations via multi-modal information bottleneck attribution",
      "paper_id": "2312.17174v2"
    },
    {
      "index": 136,
      "title": "DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Deco: Decoupling token compression from semantic abstraction in multimodal large language models",
      "paper_id": "2405.20985v1"
    },
    {
      "index": 137,
      "title": "“Vl-match: Enhancing vision-language pretraining with token-level and instance-level matching",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 138,
      "title": "“The first to know: How token distributions reveal hidden knowledge in large vision-language models?” arXiv preprint arXiv:2403.09037",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Unified lexical representation for interpretable visual-language alignment",
      "paper_id": "2407.17827v2"
    },
    {
      "index": 140,
      "title": "“Plausible may not be faithful: Probing object hallucination in vision-language pre-training",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "S. Radhakrishnan",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Probing image-language transformers for verb understanding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "Text-To-Concept (and Back) via Cross-Model Alignment",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Text-to-concept (and back) via cross-model alignment",
      "paper_id": "2305.06386v1"
    },
    {
      "index": 145,
      "title": "Using sparse semantic embeddings learned from multimodal text and image data to model human conceptual knowledge",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Using sparse semantic embeddings learned from multimodal text and image data to model human conceptual knowledge",
      "paper_id": "1809.02534v3"
    },
    {
      "index": 146,
      "title": "STAIR: Learning Sparse Text and Image Representation in Grounded Tokens",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Stair: learning sparse text and image representation in grounded tokens",
      "paper_id": "2301.13081v2"
    },
    {
      "index": 147,
      "title": "“Sharcs: Shared concept space for explainable multimodal learning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "“Interpreting clip with sparse linear concept embeddings (splice)",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Freebind: Free lunch in unified multimodal space via knowledge fusion",
      "paper_id": "2405.04883v2"
    },
    {
      "index": 151,
      "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A concept-based explainability framework for large multimodal models",
      "paper_id": "2406.08074v3"
    },
    {
      "index": 152,
      "title": "“From text to pixels: Enhancing user understanding through text-to-image model explanations",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "“Are vision-language transformers learning multimodal representations? a probing perspective",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "“Investigation of explainability techniques for multimodal transformers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "“Multimodal neurons in artificial neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "Interpreting the Second-Order Effects of Neurons in CLIP",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpreting the second-order effects of neurons in clip",
      "paper_id": "2406.04341v3"
    },
    {
      "index": 157,
      "title": "Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Finding and editing multi-modal neurons in pre-trained transformer",
      "paper_id": "2311.07470v2"
    },
    {
      "index": 158,
      "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Mmneuron: Discovering neuron-level domain-specific interpretation in multimodal large language model",
      "paper_id": "2406.11193v2"
    },
    {
      "index": 159,
      "title": "MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Miner: Mining the underlying pattern of modality-specific neurons in multimodal large language models",
      "paper_id": "2410.04819v1"
    },
    {
      "index": 160,
      "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Behind the scene: Revealing the secrets of pre-trained vision-and-language models",
      "paper_id": "2005.07310v2"
    },
    {
      "index": 161,
      "title": "“How and where does clip process negation?” arXiv preprint arXiv:2407.10488",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Bridging vision and language encoders: Parameter-efficient tuning for referring image segmentation",
      "paper_id": "2307.11545v1"
    },
    {
      "index": 163,
      "title": "Probing Multimodal Large Language Models for Global and Local Semantic Representation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Probing multimodal large language models for global and local semantic representation",
      "paper_id": "2402.17304v3"
    },
    {
      "index": 164,
      "title": "Multi-task Learning of Hierarchical Vision-Language Representation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-task learning of hierarchical vision-language representation",
      "paper_id": "1812.00500v1"
    },
    {
      "index": 165,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "“Towards transparent ai systems: Interpreting visual question answering models",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "Faithful Multimodal Explanation for Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Faithful multimodal explanation for visual question answering",
      "paper_id": "1809.02805v2"
    },
    {
      "index": 168,
      "title": "VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Vale: A multimodal visual and language explanation framework for image classifiers using explainable ai and language models",
      "paper_id": "2408.12808v1"
    },
    {
      "index": 169,
      "title": "A. Bhiwandiwalla",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Diffusion explainer: Visual explanation for text-to-image stable diffusion",
      "paper_id": "2305.03509v3"
    },
    {
      "index": 171,
      "title": "Law of Vision Representation in MLLMs",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Law of vision representation in mllms",
      "paper_id": "2408.16357v2"
    },
    {
      "index": 172,
      "title": "NBDT: Neural-Backed Decision Tree",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Nbdt: Neural-backed decision trees",
      "paper_id": "2004.00221v3"
    },
    {
      "index": 173,
      "title": "C. Callison-Burch",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Trace: Temporal grounding video llm via causal event modeling",
      "paper_id": "2410.05643v3"
    },
    {
      "index": 175,
      "title": "“Multimodal graph causal embedding for multimedia-based recommendation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "interpretable modular networks",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 177,
      "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Scaling vision-language models with sparse mixture of experts",
      "paper_id": "2303.07226v1"
    },
    {
      "index": 178,
      "title": "IMKGA-SM: Interpretable Multimodal Knowledge Graph Answer Prediction via Sequence Modeling",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Imkga-sm: Interpretable multimodal knowledge graph answer prediction via sequence modeling",
      "paper_id": "2301.02445v4"
    },
    {
      "index": 179,
      "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Pre-trained vision-language models learn discoverable visual concepts",
      "paper_id": "2404.12652v2"
    },
    {
      "index": 180,
      "title": "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Disttrain: Addressing model and data heterogeneity with disaggregated training for multimodal large language models",
      "paper_id": "2408.04275v2"
    },
    {
      "index": 181,
      "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Aligning large multimodal models with factually augmented rlhf",
      "paper_id": "2309.14525v1"
    },
    {
      "index": 182,
      "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Vigor: Improving visual grounding of large vision language models with fine-grained reward modeling",
      "paper_id": "2402.06118v3"
    },
    {
      "index": 183,
      "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback",
      "paper_id": "2312.00849v2"
    },
    {
      "index": 184,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "J. Benois-Pineau",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization",
      "paper_id": "2311.16839v2"
    },
    {
      "index": 187,
      "title": "T.-S. Chua et al",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "“Aligning modalities in vision large language models via preference fine-tuning",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 189,
      "title": "A Survey on Hallucination in Large Vision-Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey on hallucination in large vision-language models",
      "paper_id": "2402.00253v2"
    },
    {
      "index": 190,
      "title": "Hallucination of Multimodal Large Language Models: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Hallucination of multimodal large language models: A survey",
      "paper_id": "2404.18930v2"
    },
    {
      "index": 191,
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal chain-of-thought reasoning in language models",
      "paper_id": "2302.00923v5"
    },
    {
      "index": 192,
      "title": "“Chain of thought prompt tuning in vision language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 193,
      "title": "Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals",
      "paper_id": "2308.06207v1"
    },
    {
      "index": 194,
      "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Visualizing deep neural network decisions: Prediction difference analysis",
      "paper_id": "1702.04595v1"
    },
    {
      "index": 195,
      "title": "“Intriguing properties of neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Rise: Randomized input sampling for explanation of black-box models",
      "paper_id": "1806.07421v3"
    },
    {
      "index": 197,
      "title": "Understanding Deep Networks via Extremal Perturbations and Smooth Masks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding deep networks via extremal perturbations and smooth masks",
      "paper_id": "1910.08485v1"
    },
    {
      "index": 198,
      "title": "Learning to Explain with Complemental Examples",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to explain with complemental examples",
      "paper_id": "1812.01280v2"
    },
    {
      "index": 199,
      "title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A holistic approach to unifying automatic concept extraction and concept importance estimation",
      "paper_id": "2306.07304v2"
    },
    {
      "index": 200,
      "title": "“Connectivity-contrastive learning: Combining causal discovery and representation learning for multimodal data",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Diffusion-ts: Interpretable diffusion for general time series generation",
      "paper_id": "2403.01742v3"
    },
    {
      "index": 202,
      "title": "“Understanding diffusion models: A unified perspective",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "“Diffusion models: A comprehensive survey of methods and applications",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "Diffusion Models for Counterfactual Explanations",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Diffusion models for counterfactual explanations",
      "paper_id": "2203.15636v1"
    },
    {
      "index": 205,
      "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“What the daam: Interpreting stable diffusion using cross attention",
      "paper_id": "2210.04885v5"
    },
    {
      "index": 206,
      "title": "Interpretable Diffusion via Information Decomposition",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable diffusion via information decomposition",
      "paper_id": "2310.07972v3"
    },
    {
      "index": 207,
      "title": "COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Coco-o: A benchmark for object detectors under natural distribution shifts",
      "paper_id": "2307.12730v2"
    },
    {
      "index": 208,
      "title": "Defoiling Foiled Image Captions",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Defoiling foiled image captions",
      "paper_id": "1805.06549v1"
    },
    {
      "index": 209,
      "title": "Designing and Interpreting Probes with Control Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Designing and interpreting probes with control tasks",
      "paper_id": "1909.03368v1"
    },
    {
      "index": 210,
      "title": "FIND: A Function Description Benchmark for Evaluating Interpretability Methods",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Find: A function description benchmark for evaluating interpretability methods",
      "paper_id": "2309.03886v3"
    },
    {
      "index": 211,
      "title": "“Explainable multi-task learning for multi-modality biological data analysis",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "“An integrated multimodal model of alcohol use disorder generated by data-driven causal discovery analysis",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "paper_id": "2311.07919v2"
    },
    {
      "index": 214,
      "title": "Multimodal Attention Merging For Improved Speech Recognition and Audio Event Classification",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal attention merging for improved speech recognition and audio event classification",
      "paper_id": "2312.14378v2"
    },
    {
      "index": 215,
      "title": "“Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "Toward Interpretable Music Tagging with Self-Attention",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Toward interpretable music tagging with self-attention",
      "paper_id": "1906.04972v1"
    },
    {
      "index": 217,
      "title": "perceptual musical features for interpretable audio tagging",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Perceptual musical features for interpretable audio tagging",
      "paper_id": "2312.11234v3"
    },
    {
      "index": 218,
      "title": "Leveraging pre-trained autoencoders for interpretable prototype learning of music audio",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Leveraging pre-trained autoencoders for interpretable prototype learning of music audio",
      "paper_id": "2402.09318v1"
    },
    {
      "index": 219,
      "title": "“Concept-based techniques for” musicologist-friendly” explanations in a deep music classifier",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "“Dopra: Decoding over-accumulation penalization and re-allocation in specific weighting layer",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 221,
      "title": "“Explaining generative diffusion models via visual analysis for interpretable decision-making process",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "The Tree of Diffusion Life: Evolutionary Embeddings to Understand the Generation Process of Diffusion Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“The tree of diffusion life: Evolutionary embeddings to understand the generation process of diffusion models",
      "paper_id": "2406.17462v2"
    },
    {
      "index": 223,
      "title": "Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Contrastive visual semantic pretraining magnifies the semantics of natural language representations",
      "paper_id": "2203.07511v1"
    },
    {
      "index": 224,
      "title": "Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Probing multimodal embeddings for linguistic properties: the visual-semantic case",
      "paper_id": "2102.11115v1"
    },
    {
      "index": 225,
      "title": "Robust multimodal models have outlier features and encode more concepts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Robust multimodal models have outlier features and encode more concepts",
      "paper_id": "2310.13040v2"
    },
    {
      "index": 226,
      "title": "“Diffusion models already have a semantic latent space",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 227,
      "title": "“Interpretable basis decomposition for visual explanation",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 228,
      "title": "“Visualizing and understanding patch interactions in vision transformer",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 229,
      "title": "“Vit-net: Interpretable vision transformers with neural tree decoder",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 230,
      "title": "Interpretability-Aware Vision Transformer",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretability-aware vision transformer",
      "paper_id": "2309.08035v2"
    },
    {
      "index": 231,
      "title": "“Dynamicvit: Efficient vision transformers with dynamic token sparsification",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 232,
      "title": "Understanding intermediate layers using linear classifier probes",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding intermediate layers using linear classifier probes",
      "paper_id": "1610.01644v4"
    },
    {
      "index": 233,
      "title": "F. Viegas et al",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 234,
      "title": "The Better Angels of Machine Personality: How Personality Relates to LLM Safety",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“The better angels of machine personality: How personality relates to llm safety",
      "paper_id": "2407.12344v1"
    },
    {
      "index": 235,
      "title": "REEF: Representation Encoding Fingerprints for Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Reef: Representation encoding fingerprints for large language models",
      "paper_id": "2410.14273v1"
    },
    {
      "index": 236,
      "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity",
      "paper_id": "2401.01967v1"
    },
    {
      "index": 237,
      "title": "“Axiomatic attribution for deep networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 238,
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Network dissection: Quantifying interpretability of deep visual representations",
      "paper_id": "1704.05796v1"
    },
    {
      "index": 239,
      "title": "Interpreting Deep Visual Representations via Network Dissection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpreting deep visual representations via network dissection",
      "paper_id": "1711.05611v2"
    },
    {
      "index": 240,
      "title": "Where and What? Examining Interpretable Disentangled Representations",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Where and what? examining interpretable disentangled representations",
      "paper_id": "2104.05622v1"
    },
    {
      "index": 241,
      "title": "“Intrinsic probing through dimension selection",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 242,
      "title": "Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards interpretable sparse graph representation learning with laplacian pooling",
      "paper_id": "1905.11577v4"
    },
    {
      "index": 243,
      "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Gan dissection: Visualizing and understanding generative adversarial networks",
      "paper_id": "1811.10597v2"
    },
    {
      "index": 244,
      "title": "Understanding the Role of Individual Units in a Deep Neural Network",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding the role of individual units in a deep neural network",
      "paper_id": "2009.05041v2"
    },
    {
      "index": 245,
      "title": "Natural Language Descriptions of Deep Visual Features",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Natural language descriptions of deep visual features",
      "paper_id": "2201.11114v2"
    },
    {
      "index": 246,
      "title": "Knowledge Neurons in Pretrained Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Knowledge neurons in pretrained transformers",
      "paper_id": "2104.08696v2"
    },
    {
      "index": 247,
      "title": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Clip-dissect: Automatic description of neuron representations in deep vision networks",
      "paper_id": "2204.10965v5"
    },
    {
      "index": 248,
      "title": "Locating and Editing Factual Associations in GPT",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Locating and editing factual associations in gpt",
      "paper_id": "2202.05262v5"
    },
    {
      "index": 249,
      "title": "HINT: Hierarchical Neuron Concept Explainer",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Hint: Hierarchical neuron concept explainer",
      "paper_id": "2203.14196v1"
    },
    {
      "index": 250,
      "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
      "abstract": "",
      "year": "1943",
      "venue": "",
      "authors": "",
      "orig_title": "“Rosetta neurons: Mining the common units in a model zoo",
      "paper_id": "2306.09346v2"
    },
    {
      "index": 251,
      "title": "“Language models can explain neurons in language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 252,
      "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Cones: Concept neurons in diffusion models for customized generation",
      "paper_id": "2303.05125v1"
    },
    {
      "index": 253,
      "title": "Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons",
      "paper_id": "2308.13198v2"
    },
    {
      "index": 254,
      "title": "Scaling and evaluating sparse autoencoders",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Scaling and evaluating sparse autoencoders",
      "paper_id": "2406.04093v1"
    },
    {
      "index": 255,
      "title": "“Finding nemo: Localizing neurons responsible for memorization in diffusion models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 256,
      "title": "“High-low frequency detectors",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 257,
      "title": "“Curve detectors",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 258,
      "title": "“Zoom in: An introduction to circuits",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 259,
      "title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Causal analysis of syntactic agreement neurons in multilingual language models",
      "paper_id": "2210.14328v1"
    },
    {
      "index": 260,
      "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal neurons in pretrained text-only transformers",
      "paper_id": "2308.01544v2"
    },
    {
      "index": 261,
      "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Language-specific neurons: The key to multilingual capabilities in large language models",
      "paper_id": "2402.16438v2"
    },
    {
      "index": 262,
      "title": "On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“On the multilingual ability of decoder-based pre-trained language models: Finding and controlling language-specific neurons",
      "paper_id": "2404.02431v1"
    },
    {
      "index": 263,
      "title": "Universal Neurons in GPT2 Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Universal neurons in gpt2 language models",
      "paper_id": "2401.12181v1"
    },
    {
      "index": 264,
      "title": "“Measuring and interpreting neuronal correlations",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 265,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 266,
      "title": "Understanding Deep Image Representations by Inverting Them",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Understanding deep image representations by inverting them",
      "paper_id": "1412.0035v1"
    },
    {
      "index": 267,
      "title": "“Inverting convolutional networks with convolutional networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 268,
      "title": "On the Relationship between Self-Attention and Convolutional Layers",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“On the relationship between self-attention and convolutional layers",
      "paper_id": "1911.03584v2"
    },
    {
      "index": 269,
      "title": "“Augmenting self-attention with persistent memory",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 270,
      "title": "“Transformer feed-forward layers are key-value memories",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 271,
      "title": "“Are sixteen heads really better than one?” Advances in neural information processing systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 272,
      "title": "“Analyzing multi-head self-attention: Specialized heads do the heavy lifting",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 273,
      "title": "What Does BERT Look At? An Analysis of BERT’s Attention",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“What does bert look at? an analysis of bert’s attention",
      "paper_id": "1906.04341v1"
    },
    {
      "index": 274,
      "title": "Revealing the Dark Secrets of BERT",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Revealing the dark secrets of bert",
      "paper_id": "1908.08593v2"
    },
    {
      "index": 275,
      "title": "“Do attention heads in bert track syntactic dependencies?” arXiv preprint arXiv:1911.12246",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 276,
      "title": "FreeU: Free Lunch in Diffusion U-Net",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Freeu: Free lunch in diffusion u-net",
      "paper_id": "2309.11497v2"
    },
    {
      "index": 277,
      "title": "“Visual concept connectome (vcc): Open world concept discovery and their interlayer connections in deep models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 278,
      "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“How does bert answer questions? a layer-wise analysis of transformer representations",
      "paper_id": "1909.04925v1"
    },
    {
      "index": 279,
      "title": "BERT Rediscovers the Classical NLP Pipeline",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Bert rediscovers the classical nlp pipeline",
      "paper_id": "1905.05950v2"
    },
    {
      "index": 280,
      "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards vision-language mechanistic interpretability: A causal tracing tool for blip",
      "paper_id": "2308.14179v1"
    },
    {
      "index": 281,
      "title": "Unraveling the Temporal Dynamics of the Unet in Diffusion Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Unraveling the temporal dynamics of the unet in diffusion models",
      "paper_id": "2312.14965v1"
    },
    {
      "index": 282,
      "title": "“Why Should I Trust You?” Explaining the Predictions of Any Classifier",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“” why should i trust you?” explaining the predictions of any classifier",
      "paper_id": "1602.04938v3"
    },
    {
      "index": 283,
      "title": "“Learning important features through propagating activation differences",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 284,
      "title": "A Unified Approach to Interpreting Model Predictions",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“A unified approach to interpreting model predictions",
      "paper_id": "1705.07874v2"
    },
    {
      "index": 285,
      "title": "Learning Deep Features for Discriminative Localization",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning deep features for discriminative localization",
      "paper_id": "1512.04150v1"
    },
    {
      "index": 286,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 287,
      "title": "“U-cam: Visual explanation using uncertainty based class activation maps",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 288,
      "title": "Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Score-cam: Score-weighted visual explanations for convolutional neural networks",
      "paper_id": "1910.01279v2"
    },
    {
      "index": 289,
      "title": "“gscorecam: What objects is clip looking at?” in Proceedings of the Asian Conference on Computer Vision",
      "abstract": "",
      "year": "1975",
      "venue": "",
      "authors": ""
    },
    {
      "index": 290,
      "title": "“Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 291,
      "title": "Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Layer-wise relevance propagation for neural networks with local renormalization layers",
      "paper_id": "1604.00825v1"
    },
    {
      "index": 292,
      "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Explaining nonlinear classification decisions with deep taylor decomposition",
      "paper_id": "1512.02479v1"
    },
    {
      "index": 293,
      "title": "“Visualizing deep networks by optimizing with integrated gradients.” in CVPR workshops",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 294,
      "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers",
      "paper_id": "2103.15679v1"
    },
    {
      "index": 295,
      "title": "“The building blocks of interpretability",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 296,
      "title": "“On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 297,
      "title": "The Shapley Taylor Interaction Index",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“The shapley taylor interaction index",
      "paper_id": "1902.05622v2"
    },
    {
      "index": 298,
      "title": "Explaining Explanations: Axiomatic Feature Interactions for Deep Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Explaining explanations: Axiomatic feature interactions for deep networks",
      "paper_id": "2002.04138v3"
    },
    {
      "index": 299,
      "title": "Faith-Shap: The Faithful Shapley Interaction Index",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Faith-shap: The faithful shapley interaction index",
      "paper_id": "2203.00870v3"
    },
    {
      "index": 300,
      "title": "“Defining and quantifying the emergence of sparse concepts in dnns",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 301,
      "title": "“Does a neural network really encode symbolic concepts?” in International conference on machine learning. PMLR",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 302,
      "title": "“Where we have arrived in proving the emergence of sparse interaction primitives in dnns",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 303,
      "title": "“Can we faithfully represent masked states to compute shapley values on a dnn?” arXiv preprint arXiv:2105.10719",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 304,
      "title": "Defining and Extracting generalizable interaction primitives from DNNs",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Defining and extracting generalizable interaction primitives from dnns",
      "paper_id": "2401.16318v2"
    },
    {
      "index": 305,
      "title": "Layerwise Change of Knowledge in Neural Networks",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Layerwise change of knowledge in neural networks",
      "paper_id": "2409.08712v1"
    },
    {
      "index": 306,
      "title": "GANSpace: Discovering Interpretable GAN Controls",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Ganspace: Discovering interpretable gan controls",
      "paper_id": "2004.02546v3"
    },
    {
      "index": 307,
      "title": "Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Where we have arrived in proving the emergence of sparse symbolic concepts in ai models",
      "paper_id": "2305.01939v2"
    },
    {
      "index": 308,
      "title": "A Unified Approach to Interpreting and Boosting Adversarial Transferability",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A unified approach to interpreting and boosting adversarial transferability",
      "paper_id": "2010.04055v2"
    },
    {
      "index": 309,
      "title": "“Towards a unified game-theoretic view of adversarial perturbations and robustness",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 310,
      "title": "“Explaining generalization power of a dnn using interactive concepts",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 311,
      "title": "“Towards the difficulty for a deep neural network to learn concepts of different complexities",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 312,
      "title": "“Discovering and explaining the representation bottleneck of dnns",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 313,
      "title": "“Bayesian neural networks avoid encoding complex and perturbation-sensitive concepts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 314,
      "title": "Towards the Dynamics of a DNN Learning Symbolic Interactions",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards the dynamics of a dnn learning symbolic interactions",
      "paper_id": "2407.19198v2"
    },
    {
      "index": 315,
      "title": "Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Two-phase dynamics of interactions explains the starting point of a dnn learning over-fitted features",
      "paper_id": "2405.10262v1"
    },
    {
      "index": 316,
      "title": "“Unifying fourteen post-hoc attribution methods with taylor interactions",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 317,
      "title": "The Importance of Being Recurrent for Modeling Hierarchical Structure",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“The importance of being recurrent for modeling hierarchical structure",
      "paper_id": "1803.03585v2"
    },
    {
      "index": 318,
      "title": "Interpretable Visual Understanding with Cognitive Attention Network",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable visual understanding with cognitive attention network",
      "paper_id": "2108.02924v3"
    },
    {
      "index": 319,
      "title": "Improving the Interpretability of Deep Neural Networks with Knowledge Distillation",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Improving the interpretability of deep neural networks with knowledge distillation",
      "paper_id": "1812.10924v1"
    },
    {
      "index": 320,
      "title": "Leveraging Sparse Linear Layers for Debuggable Deep Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Leveraging sparse linear layers for debuggable deep networks",
      "paper_id": "2105.04857v1"
    },
    {
      "index": 321,
      "title": "Concept Bottleneck Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Concept bottleneck models",
      "paper_id": "2007.04612v3"
    },
    {
      "index": 322,
      "title": "Post-hoc Concept Bottleneck Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Post-hoc concept bottleneck models",
      "paper_id": "2205.15480v2"
    },
    {
      "index": 323,
      "title": "“Llcp: Learning latent causal processes for reasoning-based video question answer",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 324,
      "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Steering llms towards unbiased responses: A causality-guided debiasing framework",
      "paper_id": "2403.08743v2"
    },
    {
      "index": 325,
      "title": "Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Revealing multimodal contrastive representation learning through latent partial causal models",
      "paper_id": "2402.06223v2"
    },
    {
      "index": 326,
      "title": "“Do vision-language pretrained models learn composable primitive concepts?” arXiv preprint arXiv:2203.17271",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 327,
      "title": "LIMA: Less Is More for Alignment",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Lima: Less is more for alignment",
      "paper_id": "2305.11206v1"
    },
    {
      "index": 328,
      "title": "Silkie: Preference Distillation for Large Visual Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Silkie: Preference distillation for large visual language models",
      "paper_id": "2312.10665v1"
    },
    {
      "index": 329,
      "title": "SmoothGrad: removing noise by adding noise",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Smoothgrad: removing noise by adding noise",
      "paper_id": "1706.03825v1"
    },
    {
      "index": 330,
      "title": "“A survey on hallucination in large language models: Principles",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 331,
      "title": "A Survey of Hallucination in “Large” Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey of hallucination in large foundation models",
      "paper_id": "2309.05922v1"
    },
    {
      "index": 332,
      "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models",
      "paper_id": "2310.16436v2"
    },
    {
      "index": 333,
      "title": "Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models",
      "paper_id": "2403.16999v3"
    },
    {
      "index": 334,
      "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning",
      "paper_id": "2401.12863v1"
    },
    {
      "index": 335,
      "title": "“M3cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 336,
      "title": "Visual Chain-of-Thought: Bridging Logical Gaps with Multimodal Infillings",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual chain of thought: bridging logical gaps with multimodal infillings",
      "paper_id": "2305.02317v3"
    },
    {
      "index": 337,
      "title": "“3vl: using trees to teach vision & language models compositional concepts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 338,
      "title": "“A survey on in-context learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 339,
      "title": "K. Gopalakrishnan",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 340,
      "title": "Multimodal Contrastive In-Context Learning",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal contrastive in-context learning",
      "paper_id": "2408.12959v1"
    }
  ]
}