{
  "paper_id": "2404.09356v1",
  "title": "LLeMpower: Understanding Disparities in the Control and Access of Large Language Models",
  "abstract": "Abstract\nLarge Language Models (LLMs) are a powerful technology that augment human skill to create new opportunities, akin to the development of steam engines and the internet.\nHowever, LLMs come with a high cost.\nThey require significant computing resources and energy to train and serve. Inequity in their control and access has led to concentration of ownership and power to a small collection of corporations.\nIn our study, we collect training and inference requirements for various LLMs.\nWe then analyze the economic strengths of nations and organizations in the context of developing and serving these models.\nAdditionally, we also look at whether individuals around the world can access and use this emerging technology.\nWe compare and contrast these groups to show that these technologies are monopolized by a surprisingly few entities.\nWe conclude with a qualitative study on the ethical implications of our findings and discuss future directions towards equity in LLM access.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Zijian Ding et al.",
      "orig_title": "Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation",
      "paper_id": "2310.10706v2"
    },
    {
      "index": 1,
      "title": "Large language models in medicine",
      "abstract": "",
      "year": "2023",
      "venue": "Nature medicine",
      "authors": "Arun James Thirunavukarasu et al."
    },
    {
      "index": 2,
      "title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
      "abstract": "",
      "year": "2023",
      "venue": "bioRxiv",
      "authors": "Shreyas Bhat Brahmavar et al."
    },
    {
      "index": 3,
      "title": "14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon",
      "abstract": "",
      "year": "2023",
      "venue": "Digital Discovery",
      "authors": "Kevin Maik Jablonka et al.",
      "orig_title": "14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon",
      "paper_id": "2306.06283v4"
    },
    {
      "index": 4,
      "title": "ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Jiaxi Cui et al.",
      "orig_title": "Chatlaw: Open-source legal large language model with integrated external knowledge bases",
      "paper_id": "2306.16092v2"
    },
    {
      "index": 5,
      "title": "Empowering education with llms-the next-gen interface and content generation",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Artificial Intelligence in Education",
      "authors": "Steven Moore et al."
    },
    {
      "index": 6,
      "title": "Evaluating reading comprehension exercises generated by LLMs: A showcase of ChatGPT in education applications",
      "abstract": "",
      "year": "2023",
      "venue": "18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
      "authors": "Changrong Xiao et al."
    },
    {
      "index": 7,
      "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint",
      "authors": "Runchu Tian et al."
    },
    {
      "index": 8,
      "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Xinyi Hou et al.",
      "orig_title": "Large language models for software engineering: A systematic literature review",
      "paper_id": "2308.10620v6"
    },
    {
      "index": 9,
      "title": "Codeplan: Repository-level coding using llms and planning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Ramakrishna Bairi et al."
    },
    {
      "index": 10,
      "title": "LLMs for Coding and Robotics Education",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint",
      "authors": "Peng Shu et al.",
      "orig_title": "LLMs for Coding and Robotics Education",
      "paper_id": "2402.06116v1"
    },
    {
      "index": 11,
      "title": "Large Language Models for Robotics: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Fanlong Zeng et al.",
      "orig_title": "Large language models for robotics: A survey",
      "paper_id": "2311.07226v1"
    },
    {
      "index": 12,
      "title": "The economic potential of Generative AI: The Next Productivity Frontier",
      "abstract": "",
      "year": "2023",
      "venue": "McKinsey & Company",
      "authors": "Michael Chui et al."
    },
    {
      "index": 13,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ashish Vaswani et al.",
      "orig_title": "Attention is All you Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 14,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Jacob Devlin et al.",
      "orig_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 15,
      "title": "Large Language Models: A Survey",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint",
      "authors": "Shervin Minaee et al.",
      "orig_title": "Large Language Models: A Survey",
      "paper_id": "2402.06196v3"
    },
    {
      "index": 16,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Jared Kaplan et al.",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 17,
      "title": "Training Compute-Optimal Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Jordan Hoffmann et al.",
      "orig_title": "Training compute-optimal large language models",
      "paper_id": "2203.15556v1"
    },
    {
      "index": 18,
      "title": "Anglekindling: Supporting journalistic angle ideation with large language models",
      "abstract": "",
      "year": "2023",
      "venue": "CHI Conference on Human Factors in Computing Systems",
      "authors": "Savvas Petridis et al."
    },
    {
      "index": 19,
      "title": "Artificial intelligence practices in everyday news production: The case of South Africaâ€™s mainstream newsrooms",
      "abstract": "",
      "year": "2023",
      "venue": "Journalism Practice",
      "authors": "Allen Munoriyarwa, Sarah Chiumbu and Gilbert Motsaathebe"
    },
    {
      "index": 20,
      "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Zeming Chen et al."
    },
    {
      "index": 21,
      "title": "Brand guidlines",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 22,
      "title": "Google PaLM Logo",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wikipedia, the free encyclopedia"
    },
    {
      "index": 23,
      "title": "Meta brand resources and guidelines",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Meta"
    },
    {
      "index": 24,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Hugo Touvron et al.",
      "orig_title": "LLaMA: Open and Efficient Foundation Language Models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 25,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Susan Zhang et al.",
      "orig_title": "OPT: Open Pre-trained Transformer Language Models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 26,
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Hugo Touvron et al.",
      "orig_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "paper_id": "2307.09288v2"
    },
    {
      "index": 27,
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "BigScience Workshop et al.",
      "orig_title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "paper_id": "2211.05100v4"
    },
    {
      "index": 28,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Wayne Xin Zhao et al.",
      "orig_title": "A Survey of Large Language Models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 29,
      "title": "A Comprehensive Overview of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Humza Naveed et al.",
      "orig_title": "A Comprehensive Overview of Large Language Models",
      "paper_id": "2307.06435v10"
    },
    {
      "index": 30,
      "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv",
      "authors": "Ahmad Faiz et al."
    },
    {
      "index": 31,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Emma Strubell, Ananya Ganesh and Andrew McCallum",
      "orig_title": "Energy and Policy Considerations for Deep Learning in NLP",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 32,
      "title": "Green AI",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Roy Schwartz et al."
    },
    {
      "index": 33,
      "title": "What are quantized llms?",
      "abstract": "",
      "year": "2023",
      "venue": "TensorOps",
      "authors": "Miguel Carreira Neves"
    },
    {
      "index": 34,
      "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Yilong Zhao et al."
    },
    {
      "index": 35,
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Tim Dettmers et al.",
      "orig_title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "paper_id": "2305.14314v1"
    },
    {
      "index": 36,
      "title": "On the Opportunities and Risks of Foundation Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Rishi Bommasani et al."
    },
    {
      "index": 37,
      "title": "How language gaps constrain generative AI development",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Regina Ta and Nicol Turner Lee"
    },
    {
      "index": 38,
      "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Krithika Ramesh, Sunayana Sitaram and Monojit Choudhury",
      "orig_title": "Fairness in Language Models Beyond English: Gaps and Challenges",
      "paper_id": "2302.12578v2"
    },
    {
      "index": 39,
      "title": "â€˜Itâ€™s destroyed me completelyâ€™: Kenyan moderators decry toll of training of AI models",
      "abstract": "",
      "year": "2023",
      "venue": "The Guardian",
      "authors": "Niamh Rowe"
    },
    {
      "index": 40,
      "title": "The Turing Trap: The Promise and Peril of Human-Like Artificial Intelligence",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Erik Brynjolfsson"
    },
    {
      "index": 41,
      "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Conference on Fairness, Accountability, and Transparency, FAccT",
      "authors": "Emily M. Bender et al."
    },
    {
      "index": 42,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Colin Raffel et al.",
      "orig_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 43,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Tom B. Brown et al.",
      "orig_title": "Language Models are Few-Shot Learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 44,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Aakanksha Chowdhery et al.",
      "orig_title": "PaLM: Scaling Language Modeling with Pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 45,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "OpenAI et al.",
      "orig_title": "GPT-4 Technical Report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 46,
      "title": "The Cost of Training NLP Models A Concise Overview",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Or Sharir, Barak Peleg and Yoav Shoham",
      "orig_title": "The Cost of Training NLP Models: A Concise Overview",
      "paper_id": "2004.08900v1"
    },
    {
      "index": 47,
      "title": "A100 GPU Cloud Availability and Pricing",
      "abstract": "",
      "year": "2024",
      "venue": "GPU Utils",
      "authors": ""
    },
    {
      "index": 48,
      "title": "On-demand GPU cloud pricing",
      "abstract": "",
      "year": "2024",
      "venue": "Lambda Labs",
      "authors": ""
    },
    {
      "index": 49,
      "title": "LLaMA2: Model Details",
      "abstract": "",
      "year": "2024",
      "venue": "Meta Research",
      "authors": ""
    },
    {
      "index": 50,
      "title": "Optimizing LLMs for Speed and Memory",
      "abstract": "",
      "year": "2024",
      "venue": "HuggingFace",
      "authors": ""
    },
    {
      "index": 51,
      "title": "Measuring Massive Multitask Language Understanding",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Dan Hendrycks et al.",
      "orig_title": "Measuring Massive Multitask Language Understanding",
      "paper_id": "2009.03300v3"
    },
    {
      "index": 52,
      "title": "Scaling Instruction-Finetuned Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Hyung Won Chung et al.",
      "orig_title": "Scaling Instruction-Finetuned Language Models",
      "paper_id": "2210.11416v5"
    },
    {
      "index": 53,
      "title": "BloombergGPT: A Large Language Model for Finance",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Shijie Wu et al.",
      "orig_title": "BloombergGPT: A Large Language Model for Finance",
      "paper_id": "2303.17564v3"
    },
    {
      "index": 54,
      "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Wenhao Zhu et al.",
      "orig_title": "Multilingual machine translation with large language models: Empirical results and analysis",
      "paper_id": "2304.04675v4"
    },
    {
      "index": 55,
      "title": "Document-Level Machine Translation with Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Longyue Wang et al.",
      "orig_title": "Document-level machine translation with large language models",
      "paper_id": "2304.02210v2"
    },
    {
      "index": 56,
      "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Shima Imani, Liang Du and Harsh Shrivastava"
    },
    {
      "index": 57,
      "title": "Venture capital investments (market statistics)",
      "abstract": "",
      "year": "2022",
      "venue": "OECD",
      "authors": ""
    },
    {
      "index": 58,
      "title": "Locations funding heatmap)",
      "abstract": "",
      "year": "2023",
      "venue": "dealroom.co",
      "authors": ""
    },
    {
      "index": 59,
      "title": "Research and development expenditure (% of GDP)",
      "abstract": "",
      "year": "2024",
      "venue": "World Bank",
      "authors": ""
    },
    {
      "index": 60,
      "title": "Median Income by Country 2024",
      "abstract": "",
      "year": "",
      "venue": "World Population Review",
      "authors": ""
    },
    {
      "index": 61,
      "title": "Introducing ChatGPT Plus",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 62,
      "title": "Subscription Service Statistics and Costs",
      "abstract": "",
      "year": "",
      "venue": "C+R Research",
      "authors": ""
    },
    {
      "index": 63,
      "title": "Bias and Fairness in Large Language Models: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Isabel O. Gallegos et al."
    },
    {
      "index": 64,
      "title": "Quantifying and alleviating political bias in language models",
      "abstract": "",
      "year": "2022",
      "venue": "Artificial Intelligence",
      "authors": "Ruibo Liu et al."
    },
    {
      "index": 65,
      "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Shangbin Feng et al.",
      "orig_title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
      "paper_id": "2305.08283v3"
    },
    {
      "index": 66,
      "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "Esin Durmus et al.",
      "orig_title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
      "paper_id": "2306.16388v2"
    },
    {
      "index": 67,
      "title": "ChatGPT and large language models in academia: opportunities and challenges",
      "abstract": "",
      "year": "2023",
      "venue": "BioData Mining",
      "authors": "Jesse G Meyer et al."
    },
    {
      "index": 68,
      "title": "Knowledge distillation: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "International Journal of Computer Vision",
      "authors": "Jianping Gou et al."
    },
    {
      "index": 69,
      "title": "Llm-pruner: On the structural pruning of large language models",
      "abstract": "",
      "year": "2024",
      "venue": "Advances in neural information processing systems",
      "authors": "Xinyin Ma, Gongfan Fang and Xinchao Wang"
    },
    {
      "index": 70,
      "title": "Green ai",
      "abstract": "",
      "year": "2020",
      "venue": "Communications of the ACM",
      "authors": "Roy Schwartz et al."
    }
  ]
}