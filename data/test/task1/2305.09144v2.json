{
  "paper_id": "2305.09144v2",
  "title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models",
  "abstract": "Abstract\nMemory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities.\nIn recent years, large-scale pre-trained language models have shown remarkable memorizing ability.\nOn the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem.\nTo investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation.\nThese conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Human memory: A proposed system and its control processes",
      "abstract": "",
      "year": "1968",
      "venue": "Psychology of learning and motivation",
      "authors": "Richard C Atkinson and Richard M Shiffrin"
    },
    {
      "index": 1,
      "title": "The control of short-term memory",
      "abstract": "",
      "year": "1971",
      "venue": "Scientific american",
      "authors": "Richard C Atkinson and Richard M Shiffrin"
    },
    {
      "index": 2,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 3,
      "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View",
      "abstract": "",
      "year": "2022",
      "venue": "Association for Computational Linguistics",
      "authors": "Boxi Cao, Hongyu Lin, Xianpei Han, Fangchao Liu, and Le Sun",
      "orig_title": "Can prompt probe pretrained language models? understanding the invisible risks from a causal view",
      "paper_id": "2203.12258v1"
    },
    {
      "index": 4,
      "title": "Knowledgeable or educated guess? revisiting language models as knowledge bases",
      "abstract": "",
      "year": "2021",
      "venue": "Association for Computational Linguistics",
      "authors": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu"
    },
    {
      "index": 5,
      "title": "Quantifying Memorization Across Neural Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang",
      "orig_title": "Quantifying memorization across neural language models",
      "paper_id": "2202.07646v3"
    },
    {
      "index": 6,
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "USENIX Security Symposium",
      "authors": "Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.",
      "orig_title": "Extracting training data from large language models",
      "paper_id": "2012.07805v2"
    },
    {
      "index": 7,
      "title": "The interference theory of forgetting",
      "abstract": "",
      "year": "1967",
      "venue": "Scientific American",
      "authors": "John Ceraso"
    },
    {
      "index": 8,
      "title": "Lifelong machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
      "authors": "Zhiyuan Chen and Bing Liu"
    },
    {
      "index": 9,
      "title": "What are the differences between long-term, short-term, and working memory?",
      "abstract": "",
      "year": "2008",
      "venue": "Progress in brain research",
      "authors": "Nelson Cowan"
    },
    {
      "index": 10,
      "title": "Long-term retention of basic science knowledge: a review study",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Health Sciences Education",
      "authors": "Eugène JFM Custers"
    },
    {
      "index": 11,
      "title": "A continual learning survey: Defying forgetting in classification tasks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars",
      "orig_title": "A continual learning survey: Defying forgetting in classification tasks",
      "paper_id": "1909.08383v3"
    },
    {
      "index": 12,
      "title": "Moderate levels of activation lead to forgetting in the think/no-think paradigm",
      "abstract": "",
      "year": "2013",
      "venue": "Neuropsychologia",
      "authors": "Greg J Detre, Annamalai Natarajan, Samuel J Gershman, and Kenneth A Norman"
    },
    {
      "index": 13,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Association for Computational Linguistics",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 14,
      "title": "Cognitive architectures: Where do we go from here?",
      "abstract": "",
      "year": "2008",
      "venue": "Agi",
      "authors": "Wlodzislaw Duch, Richard Jayadi Oentaryo, and Michel Pasquier"
    },
    {
      "index": 15,
      "title": "Memory: A contribution to experimental psychology",
      "abstract": "",
      "year": "2013",
      "venue": "Annals of neurosciences",
      "authors": "Hermann Ebbinghaus"
    },
    {
      "index": 16,
      "title": "Measuring and Improving Consistency in Pretrained Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg",
      "orig_title": "Measuring and improving consistency in pretrained language models",
      "paper_id": "2102.01017v2"
    },
    {
      "index": 17,
      "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "abstract": "",
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Allyson Ettinger",
      "orig_title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "paper_id": "1907.13528v2"
    },
    {
      "index": 18,
      "title": "Attention and arousal: Cognition and performance",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "Michael Eysenck"
    },
    {
      "index": 19,
      "title": "Does Learning Require Memorization? A Short Tale about a Long Tail",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGACT Symposium on Theory of Computing",
      "authors": "Vitaly Feldman",
      "orig_title": "Does learning require memorization? a short tale about a long tail",
      "paper_id": "1906.05271v4"
    },
    {
      "index": 20,
      "title": "Do Neural Language Representations Learn Physical Commonsense?",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv preprint",
      "authors": "Maxwell Forbes, Ari Holtzman, and Yejin Choi",
      "orig_title": "Do Neural Language Representations Learn Physical Commonsense?",
      "paper_id": "1908.02899v1"
    },
    {
      "index": 21,
      "title": "Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks",
      "abstract": "",
      "year": "1991",
      "venue": "Cognitive science society conference",
      "authors": "Robert M French"
    },
    {
      "index": 22,
      "title": "Catastrophic forgetting in connectionist networks",
      "abstract": "",
      "year": "1999",
      "venue": "Trends in cognitive sciences",
      "authors": "Robert M French"
    },
    {
      "index": 23,
      "title": "The development of memory",
      "abstract": "",
      "year": "1998",
      "venue": "The Journal of Child Psychology and Psychiatry and Allied Disciplines",
      "authors": "Susan E Gathercole"
    },
    {
      "index": 24,
      "title": "A bio-inspired incremental learning architecture for applied perceptual problems",
      "abstract": "",
      "year": "2016",
      "venue": "Cognitive Computation",
      "authors": "Alexander Gepperth and Cem Karaoguz"
    },
    {
      "index": 25,
      "title": "The long and the short of long–term memory—a molecular framework",
      "abstract": "",
      "year": "1986",
      "venue": "Nature",
      "authors": "Philip Goelet, Vincent F Castellucci, Samuel Schacher, and Eric R Kandel"
    },
    {
      "index": 26,
      "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint",
      "authors": "Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio"
    },
    {
      "index": 27,
      "title": "X-FACTR: Multilingual factual knowledge retrieval from pretrained language models",
      "abstract": "",
      "year": "2020",
      "venue": "Association for Computational Linguistics",
      "authors": "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham Neubig"
    },
    {
      "index": 28,
      "title": "How Can We Know What Language Models Know?",
      "abstract": "",
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig",
      "orig_title": "How can we know what language models know?",
      "paper_id": "1911.12543v2"
    },
    {
      "index": 29,
      "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "Association for Computational Linguistics",
      "authors": "Nora Kassner, Philipp Dufter, and Hinrich Schütze",
      "orig_title": "Multilingual LAMA: Investigating knowledge in multilingual pretrained language models",
      "paper_id": "2102.00894v1"
    },
    {
      "index": 30,
      "title": "Competition between items in working memory leads to forgetting",
      "abstract": "",
      "year": "2014",
      "venue": "Nature Communications",
      "authors": "Jarrod A Lewis-Peacock and Kenneth A Norman"
    },
    {
      "index": 31,
      "title": "Learning without Forgetting",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Zhizhong Li and Derek Hoiem",
      "orig_title": "Learning without forgetting",
      "paper_id": "1606.09282v3"
    },
    {
      "index": 32,
      "title": "Open Sesame: Getting Inside BERT’s Linguistic Knowledge",
      "abstract": "",
      "year": "2019",
      "venue": "Association for Computational Linguistics",
      "authors": "Yongjie Lin, Yi Chern Tan, and Robert Frank",
      "orig_title": "Open sesame: Getting inside BERT’s linguistic knowledge",
      "paper_id": "1906.01698v1"
    },
    {
      "index": 33,
      "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
      "abstract": "",
      "year": "1995",
      "venue": "Psychological review",
      "authors": "James L McClelland, Bruce L McNaughton, and Randall C O’Reilly"
    },
    {
      "index": 34,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of learning and motivation",
      "authors": "Michael McCloskey and Neal J Cohen"
    },
    {
      "index": 35,
      "title": "How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "R Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz"
    },
    {
      "index": 36,
      "title": "An empirical investigation of the role of pre-training in lifelong learning",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell"
    },
    {
      "index": 37,
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "abstract": "",
      "year": "1956",
      "venue": "Psychological review",
      "authors": "George A Miller"
    },
    {
      "index": 38,
      "title": "Replication and analysis of ebbinghaus’ forgetting curve",
      "abstract": "",
      "year": "2015",
      "venue": "PloS one",
      "authors": "Jaap MJ Murre and Joeri Dros"
    },
    {
      "index": 39,
      "title": "Moderate excitation leads to weakening of perceptual representations",
      "abstract": "",
      "year": "2010",
      "venue": "Cerebral Cortex",
      "authors": "Ehren L Newman and Kenneth A Norman"
    },
    {
      "index": 40,
      "title": "Continual Lifelong Learning with Neural Networks: A Review",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter",
      "orig_title": "Continual lifelong learning with neural networks: A review",
      "paper_id": "1802.07569v4"
    },
    {
      "index": 41,
      "title": "Language models as knowledge bases?",
      "abstract": "",
      "year": "2019",
      "venue": "Association for Computational Linguistics",
      "authors": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller"
    },
    {
      "index": 42,
      "title": "A comprehensive, application-oriented study of catastrophic forgetting in dnns",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Benedikt Pfülb and Alexander Gepperth"
    },
    {
      "index": 43,
      "title": "The present status of interference theory",
      "abstract": "",
      "year": "1961",
      "venue": "Conference on Verbal Learning and Verbal Behavior",
      "authors": "Leo Postman"
    },
    {
      "index": 44,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever"
    },
    {
      "index": 45,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "J. Mach. Learn. Res.",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 46,
      "title": "Effect of scale on catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer"
    },
    {
      "index": 47,
      "title": "Critical periods and catastrophic interference effects in the development of self-organizing feature maps",
      "abstract": "",
      "year": "2008",
      "venue": "Developmental science",
      "authors": "Fiona M Richardson and Michael SC Thomas"
    },
    {
      "index": 48,
      "title": "Catastrophic forgetting, rehearsal and pseudorehearsal",
      "abstract": "",
      "year": "1995",
      "venue": "Connection Science",
      "authors": "Anthony Robins"
    },
    {
      "index": 49,
      "title": "An overview of multi-task learning in deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Sebastian Ruder"
    },
    {
      "index": 50,
      "title": "Analysis of rehearsal processes in free recall",
      "abstract": "",
      "year": "1971",
      "venue": "Journal of experimental psychology",
      "authors": "Dewey Rundus"
    },
    {
      "index": 51,
      "title": "Mechanisms of memory",
      "abstract": "",
      "year": "1986",
      "venue": "Science",
      "authors": "Larry R Squire"
    },
    {
      "index": 52,
      "title": "Memorisation versus Generalisation in Pre-trained Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Association for Computational Linguistics",
      "authors": "Michael Tänzer, Sebastian Ruder, and Marek Rei",
      "orig_title": "Memorisation versus generalisation in pre-trained language models",
      "paper_id": "2105.00828v2"
    },
    {
      "index": 53,
      "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick",
      "orig_title": "What do you learn from context? probing for sentence structure in contextualized word representations",
      "paper_id": "1905.06316v1"
    },
    {
      "index": 54,
      "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Kushal Tirumala, Aram H Markosyan, Luke Zettlemoyer, and Armen Aghajanyan",
      "orig_title": "Memorization without overfitting: Analyzing the training dynamics of large language models",
      "paper_id": "2205.10770v2"
    },
    {
      "index": 55,
      "title": "Wikidata: a free collaborative knowledgebase",
      "abstract": "",
      "year": "2014",
      "venue": "Communications of the ACM",
      "authors": "Denny Vrandečić and Markus Krötzsch"
    },
    {
      "index": 56,
      "title": "On the form of forgetting",
      "abstract": "",
      "year": "1991",
      "venue": "Psychological science",
      "authors": "John T Wixted and Ebbe B Ebbesen"
    },
    {
      "index": 57,
      "title": "Continual learning through synaptic intelligence",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Friedemann Zenke, Ben Poole, and Surya Ganguli"
    },
    {
      "index": 58,
      "title": "A Survey on Multi-Task Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": "Yu Zhang and Qiang Yang",
      "orig_title": "A survey on multi-task learning",
      "paper_id": "1707.08114v3"
    },
    {
      "index": 59,
      "title": "Evaluating Commonsense in Pre-trained Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang",
      "orig_title": "Evaluating commonsense in pre-trained language models",
      "paper_id": "1911.11931v2"
    }
  ]
}