{
  "paper_id": "2208.10498v1",
  "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
  "abstract": "Abstract\nDeep learning technologies have demonstrated remarkable effectiveness in a wide range of tasks, and deep learning holds the potential to advance a multitude of applications, including in edge computing, where deep models are deployed on edge devices to enable instant data processing and response. A key challenge is that while the application of deep models often incurs substantial memory and computational costs, edge devices typically offer only very limited storage and computational capabilities that may vary substantially across devices. These characteristics make it difficult to build deep learning solutions that unleash the potential of edge devices while complying with their constraints.\nA promising approach to addressing this challenge is to automate the design of effective deep learning models that are lightweight, require only a little storage, and incur only low computational overheads. This survey offers comprehensive coverage of studies of design automation techniques for deep learning models targeting edge computing. It offers an overview and comparison of key metrics that are used commonly to quantify the proficiency of models in terms of effectiveness, lightness, and computational costs. The survey then proceeds to cover three categories of the state-of-the-art of deep model design automation techniques: automated neural architecture search, automated model compression, and joint automated design and compression. Finally, the survey covers open issues and directions for future research.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Deep Learning based Recommender System: A Survey and New Perspectives",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Computing Surveys",
      "authors": "S. Zhang, L. Yao, A. Sun, and Y. Tay",
      "orig_title": "Deep learning based recommender system: A survey and new perspectives",
      "paper_id": "1707.07435v7"
    },
    {
      "index": 1,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 2,
      "title": "Learning attentional temporal cues of brainwaves with spatial embedding for motion intent detection",
      "abstract": "",
      "year": "2019",
      "venue": "ICDM",
      "authors": "D. Zhang, K. Chen, D. Jian, L. Yao, S. Wang, and P. Li"
    },
    {
      "index": 3,
      "title": "Data security and privacy protection issues in cloud computing",
      "abstract": "",
      "year": "2012",
      "venue": "ICCSEE",
      "authors": "D. Chen and H. Zhao"
    },
    {
      "index": 4,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "E. Strubell, A. Ganesh, and A. McCallum",
      "orig_title": "Energy and policy considerations for deep learning in NLP",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 5,
      "title": "Green ai",
      "abstract": "",
      "year": "2020",
      "venue": "Communications of the ACM",
      "authors": "R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni"
    },
    {
      "index": 6,
      "title": "Predicting parameters in deep learning",
      "abstract": "",
      "year": "2013",
      "venue": "NIPS",
      "authors": "M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas"
    },
    {
      "index": 7,
      "title": "Optimal brain damage",
      "abstract": "",
      "year": "1989",
      "venue": "NIPS",
      "authors": "Y. LeCun, J. S. Denker, and S. A. Solla"
    },
    {
      "index": 8,
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.04861",
      "authors": "A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam"
    },
    {
      "index": 9,
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen",
      "orig_title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "paper_id": "1801.04381v4"
    },
    {
      "index": 10,
      "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "X. Zhang, X. Zhou, M. Lin, and J. Sun",
      "orig_title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "paper_id": "1707.01083v2"
    },
    {
      "index": 11,
      "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "N. Ma, X. Zhang, H.-T. Zheng, and J. Sun",
      "orig_title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
      "paper_id": "1807.11164v1"
    },
    {
      "index": 12,
      "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Y. He, X. Zhang, and J. Sun",
      "orig_title": "Channel pruning for accelerating very deep neural networks",
      "paper_id": "1707.06168v2"
    },
    {
      "index": 13,
      "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "J. Luo, J. Wu, and W. Lin",
      "orig_title": "Thinet: A filter level pruning method for deep neural network compression",
      "paper_id": "1707.06342v1"
    },
    {
      "index": 14,
      "title": "Training deep neural networks with low precision multiplications",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR Workshop",
      "authors": "M. Courbariaux, Y. Bengio, and J.-P. David",
      "orig_title": "Training deep neural networks with low precision multiplications",
      "paper_id": "1412.7024v5"
    },
    {
      "index": 15,
      "title": "A logical calculus of the ideas immanent in nervous activity",
      "abstract": "",
      "year": "1943",
      "venue": "The Bulletin of Mathematical Biophysics",
      "authors": "W. S. McCulloch and W. Pitts"
    },
    {
      "index": 16,
      "title": "Hippocampal spine head sizes are highly precise",
      "abstract": "",
      "year": "2015",
      "venue": "bioRxiv",
      "authors": "T. M. Bartol, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M. Harris, and T. J. Sejnowski"
    },
    {
      "index": 17,
      "title": "Think Tank: Forty Neuroscientists Explore the Biological Roots of Human Experience",
      "abstract": "",
      "year": "2018",
      "venue": "Yale University Press",
      "authors": "D. J. Linden"
    },
    {
      "index": 18,
      "title": "Fixed Point Quantization of Deep Convolutional Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "D. Lin, S. Talathi, and S. Annapureddy",
      "orig_title": "Fixed point quantization of deep convolutional networks",
      "paper_id": "1511.06393v3"
    },
    {
      "index": 19,
      "title": "Deep Learning with Limited Numerical Precision",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan",
      "orig_title": "Deep learning with limited numerical precision",
      "paper_id": "1502.02551v1"
    },
    {
      "index": 20,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2014",
      "venue": "NIPS Workshop",
      "authors": "G. Hinton, O. Vinyals, and J. Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 21,
      "title": "Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu",
      "orig_title": "Learning compact recurrent neural networks with block-term tensor decomposition",
      "paper_id": "1712.05134v2"
    },
    {
      "index": 22,
      "title": "Proxylessnas: Direct neural architecture search on target task and hardware",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "H. Cai, L. Zhu, and S. Han"
    },
    {
      "index": 23,
      "title": "Autoprune: Automatic network pruning by regularizing auxiliary parameters",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "X. Xiao, Z. Wang, and S. Rajasekaran"
    },
    {
      "index": 24,
      "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han",
      "orig_title": "HAQ: hardware-aware automated quantization with mixed precision",
      "paper_id": "1811.08886v3"
    },
    {
      "index": 25,
      "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Y. He, J. Lin, Z. Liu, H. Wang, L. Li, and S. Han",
      "orig_title": "AMC: automl for model compression and acceleration on mobile devices",
      "paper_id": "1802.03494v4"
    },
    {
      "index": 26,
      "title": "Beyond floating-point ops: CNN performance prediction with critical datapath length",
      "abstract": "",
      "year": "2020",
      "venue": "HPEC",
      "authors": "D. Langerman, A. Johnson, K. Buettner, and A. D. George"
    },
    {
      "index": 27,
      "title": "FastDeepIoT: Towards Understanding and Optimizing Neural Network Execution Time on Mobile and Embedded Devices",
      "abstract": "",
      "year": "2018",
      "venue": "SenSys",
      "authors": "S. Yao, Y. Zhao, H. Shao, S. Liu, D. Liu, L. Su, and T. Abdelzaher",
      "orig_title": "Fastdeepiot: Towards understanding and optimizing neural network execution time on mobile and embedded devices",
      "paper_id": "1809.06970v1"
    },
    {
      "index": 28,
      "title": "CondenseNet V2: Sparse Feature Reactivation for Deep Networks",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, and Q. Tian",
      "orig_title": "Condensenet v2: Sparse feature reactivation for deep networks",
      "paper_id": "2104.04382v1"
    },
    {
      "index": 29,
      "title": "Survey of precision-scalable multiply-accumulate units for neural-network processing",
      "abstract": "",
      "year": "2019",
      "venue": "AICAS",
      "authors": "V. Camus, C. Enz, and M. Verhelst"
    },
    {
      "index": 30,
      "title": "Discovering multi-hardware mobile models via architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR Workshops",
      "authors": "G. Chu, O. Arikan, G. Bender, W. Wang, A. Brighton, P.-J. Kindermans, H. Liu, B. Akin, S. Gupta, and A. Howard"
    },
    {
      "index": 31,
      "title": "Benchmark Analysis of Representative Deep Neural Network Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "S. Bianco, R. Cadene, L. Celona, and P. Napoletano",
      "orig_title": "Benchmark analysis of representative deep neural network architectures",
      "paper_id": "1810.00736v2"
    },
    {
      "index": 32,
      "title": "Research and design of activation function hardware implementation methods",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Physics: Conference Series",
      "authors": "C. Zhengbo, T. Lei, and C. Zuoning"
    },
    {
      "index": 33,
      "title": "Improving Strong-Scaling of CNN Training by Exploiting Finer-Grained Parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "IPDPS",
      "authors": "N. Dryden, N. Maruyama, T. Benson, T. Moon, M. Snir, and B. Van Essen",
      "orig_title": "Improving strong-scaling of cnn training by exploiting finer-grained parallelism",
      "paper_id": "1903.06681v1"
    },
    {
      "index": 34,
      "title": "RT3D: Achieving Real-Time Execution of 3D Convolutional Neural Networks on Mobile Devices",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "W. Niu, M. Sun, Z. Li, J. Chen, J. Guan, X. Shen, Y. Wang, S. Liu, X. Lin, and B. Ren",
      "orig_title": "Rt3d: Achieving real-time execution of 3d convolutional neural networks on mobile devices",
      "paper_id": "2007.09835v2"
    },
    {
      "index": 35,
      "title": "cuDNN: Efficient Primitives for Deep Learning",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1410.0759",
      "authors": "S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer",
      "orig_title": "cudnn: Efficient primitives for deep learning",
      "paper_id": "1410.0759v3"
    },
    {
      "index": 36,
      "title": "Neural networks on microcontrollers: saving memory at inference via operator reordering",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.05110",
      "authors": "E. Liberis and N. D. Lane",
      "orig_title": "Neural networks on microcontrollers: saving memory at inference via operator reordering",
      "paper_id": "1910.05110v2"
    },
    {
      "index": 37,
      "title": "Machine learning on arm cortex-m microcontrollers",
      "abstract": "",
      "year": "2019",
      "venue": "Arm Ltd.: Cambridge, UK",
      "authors": "N. Suda and D. Loh"
    },
    {
      "index": 38,
      "title": "Memory requirements for convolutional neural network hardware accelerators",
      "abstract": "",
      "year": "2018",
      "venue": "IISWC",
      "authors": "K. Siu, D. M. Stuart, M. Mahmoud, and A. Moshovos"
    },
    {
      "index": 39,
      "title": "Saving energy on the edge: In-memory caching for multi-tier heterogeneous networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Communications Magazine",
      "authors": "J. Xu, K. Ota, and M. Dong"
    },
    {
      "index": 40,
      "title": "Efficient Neural Network Deployment for Microcontroller",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.01348",
      "authors": "H. Unlu",
      "orig_title": "Efficient neural network deployment for microcontroller",
      "paper_id": "2007.01348v1"
    },
    {
      "index": 41,
      "title": "Deep roots: Improving cnn efficiency with hierarchical filter groups",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Y. Ioannou, D. Robertson, R. Cipolla, and A. Criminisi"
    },
    {
      "index": 42,
      "title": "A note on latency variability of deep neural networks for mobile inference",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.00138",
      "authors": "L. Yang, B. Lu, and S. Ren"
    },
    {
      "index": 43,
      "title": "Generalized Latency Performance Estimation for Once-For-All Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.00732",
      "authors": "M. Syed and A. A. Srinivasan",
      "orig_title": "Generalized latency performance estimation for once-for-all neural architecture search",
      "paper_id": "2101.00732v1"
    },
    {
      "index": 44,
      "title": "GPGPU Performance Estimation with Core and Memory Frequency Scaling",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Parallel and Distributed Systems",
      "authors": "Q. Wang and X. Chu",
      "orig_title": "Gpgpu performance estimation with core and memory frequency scaling",
      "paper_id": "1701.05308v2"
    },
    {
      "index": 45,
      "title": "MCUNet: Tiny Deep Learning on IoT Devices",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "J. Lin, W. Chen, Y. Lin, J. Cohn, C. Gan, and S. Han",
      "orig_title": "Mcunet: Tiny deep learning on iot devices",
      "paper_id": "2007.10319v2"
    },
    {
      "index": 46,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 47,
      "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le",
      "orig_title": "Mnasnet: Platform-aware neural architecture search for mobile",
      "paper_id": "1807.11626v3"
    },
    {
      "index": 48,
      "title": "Learning Transferable Architectures for Scalable Image Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le",
      "orig_title": "Learning transferable architectures for scalable image recognition",
      "paper_id": "1707.07012v4"
    },
    {
      "index": 49,
      "title": "AutoML: A Survey of the State-of-the-Art",
      "abstract": "",
      "year": "2021",
      "venue": "Knowledge-Based Systems",
      "authors": "X. He, K. Zhao, and X. Chu",
      "orig_title": "Automl: A survey of the state-of-the-art",
      "paper_id": "1908.00709v6"
    },
    {
      "index": 50,
      "title": "A survey on evolutionary neural architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan"
    },
    {
      "index": 51,
      "title": "Neural Architecture Search with Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural architecture search with reinforcement learning",
      "paper_id": "1611.01578v2"
    },
    {
      "index": 52,
      "title": "Searching for A Robust Neural Architecture in Four GPU Hours",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Searching for a robust neural architecture in four GPU hours",
      "paper_id": "1910.04465v2"
    },
    {
      "index": 53,
      "title": "“Darts: Differentiable architecture search",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation",
      "paper_id": "2004.01803v2"
    },
    {
      "index": 55,
      "title": "“Miniseg: An extremely minimum network for efficient covid-19 segmentation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "Neural Architecture Search: A Survey",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural architecture search: A survey",
      "paper_id": "1808.05377v3"
    },
    {
      "index": 57,
      "title": "Hierarchical Representations for Efficient Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Hierarchical representations for efficient architecture search",
      "paper_id": "1711.00436v2"
    },
    {
      "index": 58,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 59,
      "title": "L. van der Maaten",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "NSGA-Net: Neural Architecture Search using Multi-Objective Genetic Algorithm",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Nsga-net: neural architecture search using multi-objective genetic algorithm",
      "paper_id": "1810.03522v2"
    },
    {
      "index": 61,
      "title": "“Constrained deep neural network architecture search for iot devices accounting for hardware calibration",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "On Neural Architecture Search for Resource-Constrained Hardware Platforms",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“On neural architecture search for resource-constrained hardware platforms",
      "paper_id": "1911.00105v1"
    },
    {
      "index": 63,
      "title": "Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient multi-objective neural architecture search via lamarckian evolution",
      "paper_id": "1804.09081v4"
    },
    {
      "index": 64,
      "title": "Resource-Efficient Neural Architect",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Resource-efficient neural architect",
      "paper_id": "1806.07912v1"
    },
    {
      "index": 65,
      "title": "D. Lymberopoulos",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "Densely Connected Search Space for More Flexible Neural Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Densely connected search space for more flexible neural architecture search",
      "paper_id": "1906.09607v3"
    },
    {
      "index": 67,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficientnet: Rethinking model scaling for convolutional neural networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 68,
      "title": "MobileDets: Searching for Object Detection Architectures for Mobile Accelerators",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Mobiledets: Searching for object detection architectures for mobile accelerators",
      "paper_id": "2004.14525v3"
    },
    {
      "index": 69,
      "title": "Searching for Fast Model Families on Datacenter Accelerators",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Searching for fast model families on datacenter accelerators",
      "paper_id": "2102.05610v1"
    },
    {
      "index": 70,
      "title": "Squeeze-and-Excitation Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Squeeze-and-excitation networks",
      "paper_id": "1709.01507v4"
    },
    {
      "index": 71,
      "title": "“Searching for mobilenetv3",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Semantic segmentation of satellite images using a modified CNN with hard-swish activation function",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Binaryconnect: Training deep neural networks with binary weights during propagations",
      "paper_id": "1511.00363v3"
    },
    {
      "index": 74,
      "title": "“Bignas: Scaling up neural architecture search with big single-stage models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "“Moga: Searching beyond mobilenetv3",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "“Once-for-all: Train one network and specialize it for efficient deployment",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Fbnetv3: Joint architecture-recipe search using predictor pretraining",
      "paper_id": "2006.02049v3"
    },
    {
      "index": 78,
      "title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions",
      "paper_id": "2004.05565v1"
    },
    {
      "index": 79,
      "title": "“Mnasfpn: Learning latency-aware pyramid architecture for object detection on mobile devices",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "“Lighttrack: Finding lightweight neural networks for object tracking via one-shot architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 82,
      "title": "“Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Vipnas: Efficient video pose estimation via neural architecture search",
      "paper_id": "2105.10154v1"
    },
    {
      "index": 84,
      "title": "DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Dpp-net: Device-aware progressive search for pareto-optimal neural architectures",
      "paper_id": "1806.08198v2"
    },
    {
      "index": 85,
      "title": "“Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "“Resource constrained neural network architecture search: Will a submodularity assumption help?” in ICCV",
      "abstract": "",
      "year": "1910",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "MONAS: Multi-Objective Neural Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Monas: Multi-objective neural architecture search",
      "paper_id": "1806.10332v2"
    },
    {
      "index": 88,
      "title": "L. van der Maaten",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "“Fasterseg: Searching for faster real-time semantic segmentation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Searching efficient 3d architectures with sparse point-voxel convolution",
      "paper_id": "2007.16100v2"
    },
    {
      "index": 91,
      "title": "HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“HR-NAS: searching efficient high-resolution neural architectures with lightweight transformers",
      "paper_id": "2106.06560v1"
    },
    {
      "index": 92,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 93,
      "title": "The Evolved Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“The evolved transformer",
      "paper_id": "1901.11117v4"
    },
    {
      "index": 94,
      "title": "“End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 96,
      "title": "ModuleNet: Knowledge-inherited Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Modulenet: Knowledge-inherited neural architecture search",
      "paper_id": "2004.05020v2"
    },
    {
      "index": 97,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 98,
      "title": "“Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "“Progressive neural architecture search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "Deep Pyramidal Residual Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep pyramidal residual networks",
      "paper_id": "1610.02915v4"
    },
    {
      "index": 101,
      "title": "Path-Level Network Transformation for Efficient Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Path-level network transformation for efficient architecture search",
      "paper_id": "1806.02639v1"
    },
    {
      "index": 102,
      "title": "Exploring Randomly Wired Neural Networks for Image Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring randomly wired neural networks for image recognition",
      "paper_id": "1904.01569v2"
    },
    {
      "index": 103,
      "title": "“Random search for hyper-parameter optimization",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Single path one-shot neural architecture search with uniform sampling",
      "paper_id": "1904.00420v4"
    },
    {
      "index": 105,
      "title": "Can weight sharing outperform random architecture search? An investigation with TuNAS",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Can weight sharing outperform random architecture search? an investigation with tunas",
      "paper_id": "2008.06120v1"
    },
    {
      "index": 106,
      "title": "LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Lens: Layer distribution enabled neural architecture search in edge-cloud hierarchies",
      "paper_id": "2107.09309v1"
    },
    {
      "index": 107,
      "title": "“Efficient resource-aware convolutional neural architecture search for edge computing with pareto-bayesian optimization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "“Bayesian multi-objective hyperparameter optimization for accurate",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "G. Venkatesh et al",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "“High-dimensional bayesian multi-objective optimization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-objective bayesian optimization over high-dimensional search spaces",
      "paper_id": "2109.10964v4"
    },
    {
      "index": 112,
      "title": "Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Distilling optimal neural networks: Rapid search in diverse spaces",
      "paper_id": "2012.08859v3"
    },
    {
      "index": 113,
      "title": "“Hsconas: Hardware-software co-design of efficient dnns via neural architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Chamnet: Towards efficient network design through platform-aware model adaptation",
      "paper_id": "1812.08934v1"
    },
    {
      "index": 115,
      "title": "“Adaptive probabilities of crossover and mutation in genetic algorithms",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Regularized Evolution for Image Classifier Architecture Search",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Regularized evolution for image classifier architecture search",
      "paper_id": "1802.01548v7"
    },
    {
      "index": 117,
      "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
      "paper_id": "1907.01845v5"
    },
    {
      "index": 118,
      "title": "“A fast and elitist multiobjective genetic algorithm: NSGA-II",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "“Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "“Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "Efficient Neural Architecture Search via Parameter Sharing",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient neural architecture search via parameters sharing",
      "paper_id": "1802.03268v2"
    },
    {
      "index": 122,
      "title": "“Categorical reparameterization with gumbel-softmax",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "Efficient Neural Architecture Search via Parameter Sharing",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient neural architecture search via parameter sharing",
      "paper_id": "1802.03268v2"
    },
    {
      "index": 124,
      "title": "“Understanding and simplifying one-shot architecture search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "“Universally slimmable networks and improved training techniques",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "“Model compression and hardware acceleration for neural networks: A comprehensive survey",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "“A comprehensive survey on model compression and acceleration",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "“Some mathematical notes on three-mode factor analysis",
      "abstract": "",
      "year": "1966",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Compression of deep convolutional neural networks for fast and low power mobile applications",
      "paper_id": "1511.06530v2"
    },
    {
      "index": 130,
      "title": "Tensor Contraction Layers for Parsimonious Deep Nets",
      "abstract": "",
      "year": "1946",
      "venue": "",
      "authors": "",
      "orig_title": "“Tensor contraction layers for parsimonious deep nets",
      "paper_id": "1706.00439v1"
    },
    {
      "index": 131,
      "title": "“Analysis of individual differences in multidimensional scaling via an n-way generalization of “eckart-young” decomposition",
      "abstract": "",
      "year": "1970",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Cp-decomposition with tensor power method for convolutional neural networks compression",
      "paper_id": "1701.07148v1"
    },
    {
      "index": 133,
      "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Speeding-up convolutional neural networks using fine-tuned cp-decomposition",
      "paper_id": "1412.6553v3"
    },
    {
      "index": 134,
      "title": "“Global analytic solution of fully-observed variational bayesian matrix factorization",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "M. Kholyavchenko",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“Towards compact neural networks via end-to-end training: A bayesian tensor approach with automatic rank determination",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "Bayesian Tensorized Neural Networks with Automatic Rank Selection",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Bayesian tensorized neural networks with automatic rank selection",
      "paper_id": "1905.10478v1"
    },
    {
      "index": 138,
      "title": "“Mars: Masked automatic ranks selection in tensor decompositions",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "“Autorank: Automated rank selection for effective neural network customization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "“Autorank: Automated rank selection for effective neural network customization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“A bayesian optimization framework for neural network compression",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Model compression",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Do deep nets really need to be deep?” in NIPS",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "Search to Distill: Pearls are Everywhere but not the Eyes",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Search to distill: Pearls are everywhere but not the eyes",
      "paper_id": "1911.09074v2"
    },
    {
      "index": 145,
      "title": "RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Rosearch: Search for robust student architectures when distilling pre-trained language models",
      "paper_id": "2106.03613v1"
    },
    {
      "index": 146,
      "title": "“Kdas-reid: Architecture search for person re-identification via distilled knowledge with dynamic temperature",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "AutoKD: Automatic Knowledge Distillation into a Student Architecture Family",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Autokd: Automatic knowledge distillation into a student architecture family",
      "paper_id": "2111.03555v1"
    },
    {
      "index": 148,
      "title": "“Autodistill: an end-to-end framework to explore and distill hardware-efficient language models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“Scene-adaptive knowledge distillation for sequential recommendation via differentiable architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“Automatic student network search for knowledge distillation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "GAN Compression: Efficient Architectures for Interactive Conditional GANs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Gan compression: Efficient architectures for interactive conditional gans",
      "paper_id": "2003.08936v4"
    },
    {
      "index": 152,
      "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Mfagan: A compression framework for memory-efficient on-device super-resolution gan",
      "paper_id": "2107.12679v1"
    },
    {
      "index": 153,
      "title": "PPCD-GAN: Progressive Pruning and Class-Aware Distillation for Large-Scale Conditional GANs Compression",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“PPCD-GAN: progressive pruning and class-aware distillation for large-scale conditional gans compression",
      "paper_id": "2203.08456v1"
    },
    {
      "index": 154,
      "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards oracle knowledge distillation with neural architecture search",
      "paper_id": "1911.13019v1"
    },
    {
      "index": 155,
      "title": "“Channel planting for deep neural networks using knowledge distillation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search",
      "abstract": "",
      "year": "1943",
      "venue": "",
      "authors": "",
      "orig_title": "“NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search",
      "paper_id": "2105.14444v1"
    },
    {
      "index": 157,
      "title": "Blockwisely Supervised Neural Architecture Search with Knowledge Distillation",
      "abstract": "",
      "year": "1995",
      "venue": "",
      "authors": "",
      "orig_title": "“Block-wisely supervised neural architecture search with knowledge distillation",
      "paper_id": "1911.13053v2"
    },
    {
      "index": 158,
      "title": "Lookahead: A Far-sighted Alternative of Magnitude-based Pruning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Lookahead: A far-sighted alternative of magnitude-based pruning",
      "paper_id": "2002.04809v1"
    },
    {
      "index": 159,
      "title": "“A systematic weight pruning framework of dnns using alternating direction method of multipliers",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "Network Automatic Pruning: Start NAP and Take a Nap",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Network automatic pruning: Start nap and take a nap",
      "paper_id": "2101.06608v1"
    },
    {
      "index": 161,
      "title": "“Ss-auto: A single-shot",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "“An information theory-inspired strategy for automatic network pruning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Autocompress: An automatic DNN structured pruning framework for ultra-high compression rates",
      "paper_id": "1907.03141v2"
    },
    {
      "index": 164,
      "title": "“Auto-prune: automated dnn pruning and mapping for reram-based accelerator",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "“Automatic channel pruning with hyper-parameter search and dynamic masking",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Fine-pruning: Joint fine-tuning and compression of a convolutional network with bayesian optimization",
      "paper_id": "1707.09102v1"
    },
    {
      "index": 167,
      "title": "“Constraint-aware deep neural network compression",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 168,
      "title": "“High-dimensional bayesian optimization for cnn auto pruning with clustering and rollback",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "ABCP: Automatic Block-wise and Channel-wise Network Pruning via Joint Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Abcp: Automatic block-wise and channel-wise network pruning via joint search",
      "paper_id": "2110.03858v1"
    },
    {
      "index": 170,
      "title": "Automatic Pruning for Quantized Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Automatic pruning for quantized neural networks",
      "paper_id": "2002.00523v1"
    },
    {
      "index": 171,
      "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Netadapt: Platform-aware neural network adaptation for mobile applications",
      "paper_id": "1804.03230v2"
    },
    {
      "index": 172,
      "title": "NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Netadaptv2: Efficient neural architecture search with fast super-network training and architecture optimization",
      "paper_id": "2104.00031v1"
    },
    {
      "index": 173,
      "title": "DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Dais: Automatic channel pruning via differentiable annealing indicator search",
      "paper_id": "2011.02166v2"
    },
    {
      "index": 174,
      "title": "Learning Efficient Convolutional Networks through Network Slimming",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning efficient convolutional networks through network slimming",
      "paper_id": "1708.06519v1"
    },
    {
      "index": 175,
      "title": "DHP: Differentiable Meta Pruning via HyperNetworks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“DHP: differentiable meta pruning via hypernetworks",
      "paper_id": "2003.13683v3"
    },
    {
      "index": 176,
      "title": "“ASBP: automatic structured bit-pruning for rram-based NN accelerator",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 177,
      "title": "Network Pruning via Transformable Architecture Search",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Network pruning via transformable architecture search",
      "paper_id": "1905.09717v5"
    },
    {
      "index": 178,
      "title": "“Network compression via cooperative architecture search and distillation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "“Joint-detnas: Upgrade your detector with nas",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 180,
      "title": "Search for Better Students to Learn Distilled Knowledge",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Search for better students to learn distilled knowledge",
      "paper_id": "2001.11612v1"
    },
    {
      "index": 181,
      "title": "“1.1 computing’s energy problem (and what we can do about it)",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 182,
      "title": "BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Batchquant: Quantized-for-all architecture search with robust quantizer",
      "paper_id": "2105.08952v1"
    },
    {
      "index": 183,
      "title": "“Apple describes 7nm a12 bionic chips",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "“Nvidia tensor cores",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "“Hardware-centric automl for mixed-precision quantization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "Stochastic Layer-Wise Precision in Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Stochastic layer-wise precision in deep neural networks",
      "paper_id": "1807.00942v1"
    },
    {
      "index": 187,
      "title": "Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Search what you want: Barrier panelty NAS for mixed precision quantization",
      "paper_id": "2007.10026v1"
    },
    {
      "index": 188,
      "title": "Mixed Precision Quantization of Transformer Language Models for Speech Recognition",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Mixed precision quantization of transformer language models for speech recognition",
      "paper_id": "2112.11540v1"
    },
    {
      "index": 189,
      "title": "“Effective and fast: A novel sequential single path search for mixed-precision quantization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "“Mixed-precision quantization for cnn-based remote sensing scene classification",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 191,
      "title": "Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Mixed precision quantization of convnets via differentiable neural architecture search",
      "paper_id": "1812.00090v1"
    },
    {
      "index": 192,
      "title": "A White Paper on Neural Network Quantization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“A white paper on neural network quantization",
      "paper_id": "2106.08295v1"
    },
    {
      "index": 193,
      "title": "Differentiable Feature Aggregation Search for Knowledge Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Differentiable feature aggregation search for knowledge distillation",
      "paper_id": "2008.00506v1"
    },
    {
      "index": 194,
      "title": "Rethinking the Value of Network Pruning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Rethinking the value of network pruning",
      "paper_id": "1810.05270v2"
    },
    {
      "index": 195,
      "title": "“Metapruning: Meta learning for automatic neural network channel pruning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "“Latency-aware automatic cnn channel pruning with gpu runtime analysis",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 197,
      "title": "Channel Pruning via Automatic Structure Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Channel pruning via automatic structure search",
      "paper_id": "2001.08565v3"
    },
    {
      "index": 198,
      "title": "ACP: Automatic Channel Pruning via Clustering and Swarm Intelligence Optimization for CNN",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Acp: Automatic channel pruning via clustering and swarm intelligence optimization for cnn",
      "paper_id": "2101.06407v1"
    },
    {
      "index": 199,
      "title": "“Superpruner: Automatic neural network pruning via super network",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "AACP: Model Compression by Accurate and Automatic Channel Pruning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Aacp: Model compression by accurate and automatic channel pruning",
      "paper_id": "2102.00390v1"
    },
    {
      "index": 201,
      "title": "“Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 202,
      "title": "“On-device image classification with proxyless neural architecture search and quantization-aware fine-tuning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "on-device and explainable neural architecture search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Resource-efficient dnns for keyword spotting using neural architecture search and quantization",
      "paper_id": "2012.10138v1"
    },
    {
      "index": 205,
      "title": "“BATS: binary architecture search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "Binarizing MobileNet via Evolution-based Searching",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Binarizing mobilenet via evolution-based searching",
      "paper_id": "2005.06305v2"
    },
    {
      "index": 207,
      "title": "“Learning architectures for binary networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "“Searching for accurate binary neural architectures",
      "abstract": "",
      "year": "2044",
      "venue": "",
      "authors": ""
    },
    {
      "index": 209,
      "title": "FrostNet: Towards Quantization-Aware Network Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Frostnet: Towards quantization-awareof network architecture search",
      "paper_id": "2006.09679v4"
    },
    {
      "index": 210,
      "title": "“Layer-wise searching for 1-bit detectors",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "Joint Neural Architecture Search and Quantization",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Joint neural architecture search and quantization",
      "paper_id": "1811.09426v1"
    },
    {
      "index": 212,
      "title": "“Mixed precision neural architecture search for energy efficient deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "“APQ: joint search for network architecture",
      "abstract": "",
      "year": "2084",
      "venue": "",
      "authors": ""
    },
    {
      "index": 214,
      "title": "Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Once quantization-aware training: High performance extremely low-bit architecture search",
      "paper_id": "2010.04354v3"
    },
    {
      "index": 215,
      "title": "“Zero-cost proxies for lightweight NAS",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "Zen-NAS: A Zero-Shot NAS for High-Performance Image Recognition",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Zen-nas: A zero-shot NAS for high-performance image recognition",
      "paper_id": "2102.01063v4"
    },
    {
      "index": 217,
      "title": "“Squeezenext: Hardware-aware neural network design",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "“HSIM-DNN: hardware simulator for computation-",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "HW-NAS-Bench: HardWare-aware Neural Architecture Search Benchmark",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Hw-nas-bench: Hardware-aware neural architecture search benchmark",
      "paper_id": "2103.10584v2"
    }
  ]
}