{
  "paper_id": "2410.03972v2",
  "title": "Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks",
  "abstract": "Abstract\nTask-trained recurrent neural networks (RNNs) are versatile models of dynamical processes widely used in machine learning and neuroscience.\nWhile RNNs are easily trained to perform a wide range of tasks, the nature and extent of the degeneracy in the resultant solutions (i.e., the variability across trained RNNs) remain poorly understood.\nHere, we provide a unified framework for analyzing degeneracy across three levels: behavior, neural dynamics, and weight space.\nWe analyzed RNNs trained on diverse tasks across machine learning and neuroscience domains, including N-bit flip-flop, sine wave generation, delayed discrimination, and path integration.\nOur key finding is that the variability across RNN solutions, quantified on the basis of neural dynamics and trained weights, depends primarily on network capacity and task characteristics such as complexity.\nWe introduce information-theoretic measures to quantify task complexity and demonstrate that increasing task complexity consistently reduces degeneracy in neural dynamics and generalization behavior while increasing degeneracy in weight space.\nThese relationships hold across diverse tasks and can be used to control the degeneracy of the solution space of task-trained RNNs.\nFurthermore, we provide several strategies to control solution degeneracy, enabling task-trained RNNs to learn more consistent or diverse solutions as needed.\nWe envision that these insights will lead to more reliable machine learning models and could inspire strategies to better understand and control degeneracy observed in neuroscience experiments.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Git Re-Basin: Merging Models Modulo Permutation Symmetries",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.04836",
      "authors": "Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa",
      "orig_title": "Git re-basin: Merging models modulo permutation symmetries",
      "paper_id": "2209.04836v6"
    },
    {
      "index": 1,
      "title": "Statistical mechanics of deep learning",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Review of Condensed Matter Physics",
      "authors": "Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Ganguli"
    },
    {
      "index": 2,
      "title": "Recurrent neural networks as versatile tools of neuroscience research",
      "abstract": "",
      "year": "2017",
      "venue": "Current Opinion in Neurobiology",
      "authors": "Omri Barak"
    },
    {
      "index": 3,
      "title": "Predictability, complexity and learning",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "William Bialek, Ilya Nemenman, and Naftali Tishby"
    },
    {
      "index": 4,
      "title": "Accounting for variance in machine learning benchmarks",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning and Systems",
      "authors": "Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al."
    },
    {
      "index": 5,
      "title": "Statistics of critical points of gaussian fields on large-dimensional spaces",
      "abstract": "",
      "year": "2007",
      "venue": "Physical review letters",
      "authors": "Alan J Bray and David S Dean"
    },
    {
      "index": 6,
      "title": "Explanatory models in neuroscience, part 2: Functional intelligibility and the contravariance principle",
      "abstract": "",
      "year": "2024",
      "venue": "Cognitive Systems Research",
      "authors": "Rosa Cao and Daniel Yamins"
    },
    {
      "index": 7,
      "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina"
    },
    {
      "index": 8,
      "title": "Capacity and Trainability in Recurrent Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo",
      "orig_title": "Capacity and trainability in recurrent neural networks",
      "paper_id": "1611.09913v3"
    },
    {
      "index": 9,
      "title": "Inferring statistical complexity",
      "abstract": "",
      "year": "1989",
      "venue": "Phys. Rev. Lett.",
      "authors": "James P. Crutchfield and Karl Young"
    },
    {
      "index": 10,
      "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Alexander Dâ€™Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley",
      "orig_title": "Underspecification presents challenges for credibility in modern machine learning",
      "paper_id": "2011.03395v2"
    },
    {
      "index": 11,
      "title": "Sloppiness, robustness, and evolvability in systems biology",
      "abstract": "",
      "year": "2008",
      "venue": "Current opinion in biotechnology",
      "authors": "Bryan C Daniels, Yan-Jiun Chen, James P Sethna, Ryan N Gutenkunst, and Christopher R Myers"
    },
    {
      "index": 12,
      "title": "Systematic errors in connectivity inferred from activity in strongly recurrent networks",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Neuroscience",
      "authors": "Abhranil Das and Ila R Fiete"
    },
    {
      "index": 13,
      "title": "Flexible multitask computation in recurrent networks utilizes shared dynamical motifs",
      "abstract": "",
      "year": "2024",
      "venue": "Nature Neuroscience",
      "authors": "Laura N. Driscoll, Krishna Shenoy, and David Sussillo"
    },
    {
      "index": 14,
      "title": "Reconstructing computational system dynamics from neural data with recurrent neural networks",
      "abstract": "",
      "year": "2023",
      "venue": "Nature Reviews Neuroscience",
      "authors": "Daniel Durstewitz, Georgia Koppe, and Max Ingo Thurm"
    },
    {
      "index": 15,
      "title": "Degeneracy and complexity in biological systems",
      "abstract": "",
      "year": "2001",
      "venue": "National Academy of Sciences",
      "authors": "Gerald M. Edelman and Joseph A. Gally"
    },
    {
      "index": 16,
      "title": "Neural representational geometries reflect behavioral differences in monkeys and recurrent neural networks",
      "abstract": "",
      "year": "2024",
      "venue": "Nature Communications",
      "authors": "Valeria Fascianelli, Aldo Battista, Fabio Stefanini, Satoshi Tsujimoto, Aldo Genovesio, and Stefano Fusi"
    },
    {
      "index": 17,
      "title": "Deep Ensembles: A Loss Landscape Perspective",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.02757",
      "authors": "Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan",
      "orig_title": "Deep ensembles: A loss landscape perspective",
      "paper_id": "1912.02757v2"
    },
    {
      "index": 18,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Jonathan Frankle and Michael Carbin",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 19,
      "title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity",
      "abstract": "",
      "year": "2007",
      "venue": "Journal of Statistical Physics",
      "authors": "Yan V Fyodorov and Ian Williams"
    },
    {
      "index": 20,
      "title": "Residual dynamics resolves recurrent contributions to neural computation",
      "abstract": "",
      "year": "2023",
      "venue": "Nature Neuroscience",
      "authors": "Aniruddh R Galgali, Maneesh Sahani, and Valerio Mante"
    },
    {
      "index": 21,
      "title": "A theory of multineuronal dimensionality, dynamics and measurement",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya Ganguli"
    },
    {
      "index": 22,
      "title": "A theory of multineuronal dimensionality, dynamics and measurement",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya Ganguli"
    },
    {
      "index": 23,
      "title": "Individual differences in skilled reaching for food related to increased number of gestures: Evidence for goal and habit learning of skilled reaching",
      "abstract": "",
      "year": "2009",
      "venue": "Behavioral Neuroscience",
      "authors": "Gita Gholamrezaei and Ian Q. Whishaw"
    },
    {
      "index": 24,
      "title": "Generative learning for nonlinear dynamics",
      "abstract": "",
      "year": "2024",
      "venue": "Nature Reviews Physics",
      "authors": "William Gilpin",
      "orig_title": "Generative learning for nonlinear dynamics",
      "paper_id": "2311.04128v1"
    },
    {
      "index": 25,
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "abstract": "",
      "year": "2010",
      "venue": "thirteenth international conference on artificial intelligence and statistics",
      "authors": "Xavier Glorot and Yoshua Bengio"
    },
    {
      "index": 26,
      "title": "Learning by neural reassociation",
      "abstract": "",
      "year": "2018",
      "venue": "Nature Neuroscience",
      "authors": "Matthew D. Golub, Patrick T. Sadtler, Emily R. Oby, Kristin M. Quick, Stephen I. Ryu, Elizabeth C. Tyler-Kabara, Aaron P. Batista, Steven M. Chase, and Byron M. Yu"
    },
    {
      "index": 27,
      "title": "Qualitatively Characterizing Neural Network Optimization Problems",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe",
      "orig_title": "Qualitatively characterizing neural network optimization problems",
      "paper_id": "1412.6544v6"
    },
    {
      "index": 28,
      "title": "Extracting falsifiable predictions from sloppy models",
      "abstract": "",
      "year": "2007",
      "venue": "Annals of the New York Academy of Sciences",
      "authors": "Ryan N Gutenkunst, Fergal P Casey, Joshua J Waterfall, Christopher R Myers, and James P Sethna"
    },
    {
      "index": 29,
      "title": "Universally sloppy parameter sensitivities in systems biology models",
      "abstract": "",
      "year": "2007",
      "venue": "PLoS computational biology",
      "authors": "Ryan N Gutenkunst, Joshua J Waterfall, Fergal P Casey, Kevin S Brown, Christopher R Myers, and James P Sethna"
    },
    {
      "index": 30,
      "title": "The impact of sparsity in low-rank recurrent neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "PLOS Computational Biology",
      "authors": "Elizabeth Herbert and Srdjan Ostojic"
    },
    {
      "index": 31,
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "abstract": "",
      "year": "1982",
      "venue": "National Academy of Sciences",
      "authors": "J J Hopfield"
    },
    {
      "index": 32,
      "title": "Control of variability",
      "abstract": "",
      "year": "2002",
      "venue": "ILAR journal",
      "authors": "BR Howard"
    },
    {
      "index": 33,
      "title": "The Platonic Representation Hypothesis",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv preprint arXiv:2405.07987",
      "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola",
      "orig_title": "The platonic representation hypothesis",
      "paper_id": "2405.07987v5"
    },
    {
      "index": 34,
      "title": "Three Factors Influencing Minima in SGD",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "StanisÅ‚aw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey",
      "orig_title": "Three factors influencing minima in sgd",
      "paper_id": "1711.04623v3"
    },
    {
      "index": 35,
      "title": "Curriculum learning as a tool to uncover learning principles in the brain",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "D Kepple, Rainer Engelken, and Kanaka Rajan"
    },
    {
      "index": 36,
      "title": "Similarity of Neural Network Representations Revisited",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton",
      "orig_title": "Similarity of neural network representations revisited",
      "paper_id": "1905.00414v4"
    },
    {
      "index": 37,
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein",
      "orig_title": "Visualizing the loss landscape of neural nets",
      "paper_id": "1712.09913v3"
    },
    {
      "index": 38,
      "title": "Striatal dopamine reflects individual long-term learning trajectories",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Samuel Liebana Garcia, Aeron Laffere, Chiara Toschi, Louisa Schilling, Jacek Podlaski, Matthias Fritsche, Peter Zatka-Haas, Yulong Li, Rafal Bogacz, Andrew Saxe, and Armin Lak"
    },
    {
      "index": 39,
      "title": "Universality and individuality in neural dynamics across large populations of recurrent networks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo",
      "orig_title": "Universality and individuality in neural dynamics across large populations of recurrent networks",
      "paper_id": "1907.08549v2"
    },
    {
      "index": 40,
      "title": "Linking connectivity, dynamics, and computations in low-rank recurrent neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "Neuron",
      "authors": "Francesca Mastrogiuseppe and Srdjan Ostojic"
    },
    {
      "index": 41,
      "title": "Individual differences among deep neural network models",
      "abstract": "",
      "year": "2020",
      "venue": "Nature Communications",
      "authors": "Johannes Mehrer, Courtney J. Spoerer, Nikolaus Kriegeskorte, and Tim C. Kietzmann"
    },
    {
      "index": 42,
      "title": "Learning, fast and slow",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Markus Meister"
    },
    {
      "index": 43,
      "title": "Sloppy modeling",
      "abstract": "",
      "year": "2005",
      "venue": "Knowledge representation and organization in machine learning",
      "authors": "Katharina Morik"
    },
    {
      "index": 44,
      "title": "Exploring Sparsity in Recurrent Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1704.05119",
      "authors": "Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho Sengupta",
      "orig_title": "Exploring sparsity in recurrent neural networks",
      "paper_id": "1704.05119v2"
    },
    {
      "index": 45,
      "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2306.10168",
      "authors": "Mitchell Ostrow, Adam Eisen, Leo Kozachkov, and Ila Fiete",
      "orig_title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
      "paper_id": "2306.10168v3"
    },
    {
      "index": 46,
      "title": "Pre-existing visual responses in a projection-defined dopamine population explain individual learning trajectories",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "A Pan-Vazquez, Y Sanchez Araujo, B McMannon, M Louka, A Bandi, L Haetzel, International Brain Laboratory, JW Pillow, ND Daw, and IB Witten"
    },
    {
      "index": 47,
      "title": "Similar network activity from disparate circuit parameters",
      "abstract": "",
      "year": "2004",
      "venue": "Nature Neuroscience",
      "authors": "Astrid A Prinz, Dirk Bucher, and Eve Marder"
    },
    {
      "index": 48,
      "title": "Recurrent network models of sequence generation and memory",
      "abstract": "",
      "year": "2016",
      "venue": "Neuron",
      "authors": "Kanaka Rajan, Christopher D Harvey, and David W Tank"
    },
    {
      "index": 49,
      "title": "Dynamic mode decomposition and its variants",
      "abstract": "",
      "year": "2022",
      "venue": "Annual Review of Fluid Mechanics",
      "authors": "Peter J Schmid"
    },
    {
      "index": 50,
      "title": "A mathematical theory of communication",
      "abstract": "",
      "year": "1948",
      "venue": "The Bell System Technical Journal",
      "authors": "Claude Elwood Shannon"
    },
    {
      "index": 51,
      "title": "Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry and Engineering",
      "abstract": "",
      "year": "2000",
      "venue": "Westview Press",
      "authors": "Steven H. Strogatz"
    },
    {
      "index": 52,
      "title": "Neural circuits as computational dynamical systems",
      "abstract": "",
      "year": "2014",
      "venue": "Current opinion in neurobiology",
      "authors": "David Sussillo"
    },
    {
      "index": 53,
      "title": "Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Neural computation",
      "authors": "David Sussillo and Omri Barak"
    },
    {
      "index": 54,
      "title": "The information bottleneck method",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": "Naftali Tishby, Fernando C. Pereira, and William Bialek"
    },
    {
      "index": 55,
      "title": "The simplicity bias in multi-task rnns: shared attractors, reuse of dynamics, and geometric representation",
      "abstract": "",
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Elia Turner and Omri Barak"
    },
    {
      "index": 56,
      "title": "Charting and navigating the space of solutions for recurrent neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Elia Turner, Kabir V Dabholkar, and Omri Barak"
    },
    {
      "index": 57,
      "title": "Computation through neural population dynamics",
      "abstract": "",
      "year": "2020",
      "venue": "Annual Review of Neuroscience",
      "authors": "Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy"
    },
    {
      "index": 58,
      "title": "Backpropagation through time: what it does and how to do it",
      "abstract": "",
      "year": "1990",
      "venue": "IEEE",
      "authors": "Paul J Werbos"
    },
    {
      "index": 59,
      "title": "Does the Data Induce Capacity Control in Deep Learning?",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Rubing Yang, Jialin Mao, and Pratik Chaudhari",
      "orig_title": "Does the data induce capacity control in deep learning?",
      "paper_id": "2110.14163v3"
    }
  ]
}