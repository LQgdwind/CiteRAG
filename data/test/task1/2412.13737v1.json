{
  "paper_id": "2412.13737v1",
  "title": "On the Compression of Language Models for Code: An Empirical Study on CodeBERT",
  "abstract": "Abstract\nLanguage models have proven successful across a wide range of software engineering tasks, but their significant computational costs often hinder their practical adoption. To address this challenge, researchers have begun applying various compression strategies to improve the efficiency of language models for code.\nThese strategies aim to optimize inference latency and memory usage, though often at the cost of reduced model effectiveness.\nHowever, there is still a significant gap in understanding how these strategies influence the efficiency and effectiveness of language models for code.\nHere, we empirically investigate the impact of three well-known compression strategies – knowledge distillation, quantization, and pruning – across three different classes of software engineering tasks: vulnerability detection, code summarization, and code search.\nOur findings reveal that the impact of these strategies varies greatly depending on the task and the specific compression method employed. Practitioners and researchers can use these insights to make informed decisions when selecting the most appropriate compression strategy, balancing both efficiency and effectiveness based on their specific needs.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Vulnerability Detection with Code Language Models: How Far Are We?",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "Y. Ding, Y. Fu, O. Ibrahim et al.",
      "orig_title": "Vulnerability detection with code language models: How far are we?",
      "paper_id": "2403.18624v2"
    },
    {
      "index": 1,
      "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE/ACM 46th International Conference on Software Engineering",
      "authors": "Z. Sun, X. Du, F. Song et al.",
      "orig_title": "When neural code completion models size up the situation: Attaining cheaper and faster completion through dynamic model inference",
      "paper_id": "2401.09964v1"
    },
    {
      "index": 2,
      "title": "Code search is all you need? improving code suggestions with code search",
      "abstract": "",
      "year": "2024",
      "venue": "IEEE/ACM 46th International Conference on Software Engineering",
      "authors": "J. Chen, X. Hu, Z. Li et al."
    },
    {
      "index": 3,
      "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "X. Hou, Y. Zhao, Y. Liu et al.",
      "orig_title": "Large language models for software engineering: A systematic literature review",
      "paper_id": "2308.10620v6"
    },
    {
      "index": 4,
      "title": "Green ai",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "R. Schwartz, J. Dodge, N. A. Smith et al."
    },
    {
      "index": 5,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS Deep Learning and Representation Learning Workshop",
      "authors": "G. Hinton, O. Vinyals, and J. Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 6,
      "title": "Q8bert: Quantized 8bit bert",
      "abstract": "",
      "year": "2019",
      "venue": "Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)",
      "authors": "O. Zafrir, G. Boudoukh, P. Izsak et al."
    },
    {
      "index": 7,
      "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
      "abstract": "",
      "year": "2020",
      "venue": "34th International Conference on Neural Information Processing Systems",
      "authors": "V. Sanh, T. Wolf, and A. M. Rush",
      "orig_title": "Movement pruning: adaptive sparsity by fine-tuning",
      "paper_id": "2005.07683v2"
    },
    {
      "index": 8,
      "title": "Compressing Pre-trained Models of Code into 3 MB",
      "abstract": "",
      "year": "2023",
      "venue": "37th IEEE/ACM International Conference on Automated Software Engineering",
      "authors": "J. Shi, Z. Yang, B. Xu et al.",
      "orig_title": "Compressing pre-trained models of code into 3 mb",
      "paper_id": "2208.07120v2"
    },
    {
      "index": 9,
      "title": "Greening Large Language Models of Code",
      "abstract": "",
      "year": "2024",
      "venue": "46th International Conference on Software Engineering: Software Engineering in Society",
      "authors": "J. Shi, Z. Yang, H. J. Kang et al.",
      "orig_title": "Greening large language models of code",
      "paper_id": "2309.04076v3"
    },
    {
      "index": 10,
      "title": "Towards greener yet powerful code generation via quantization: An empirical study",
      "abstract": "",
      "year": "2023",
      "venue": "31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "authors": "X. Wei, S. K. Gonugondla, S. Wang et al."
    },
    {
      "index": 11,
      "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "authors": "Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou",
      "orig_title": "CodeBERT: A pre-trained model for programming and natural languages",
      "paper_id": "2002.08155v4"
    },
    {
      "index": 12,
      "title": "On the Compression of Language Models for Code: An Empirical Study on CodeBERT",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "G. d’Aloisio, L. Traini, F. Sarro, and A. Di Marco",
      "orig_title": "On the compression of language models for code: An empirical study on codebert",
      "paper_id": "2412.13737v1"
    },
    {
      "index": 13,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 14,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "A. Radford, J. Wu, R. Child et al."
    },
    {
      "index": 15,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "C. Raffel, N. Shazeer, A. Roberts et al.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 16,
      "title": "Openai codex",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 17,
      "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
      "abstract": "",
      "year": "2023",
      "venue": "2023 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Y. Wang, H. Le, A. Gotmare, N. Bui, J. Li, and S. Hoi",
      "orig_title": "CodeT5+: Open code large language models for code understanding and generation",
      "paper_id": "2305.07922v2"
    },
    {
      "index": 18,
      "title": "Evaluating Large Language Models Trained on Code",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "M. Chen, J. Tworek, H. Jun et al.",
      "orig_title": "Evaluating large language models trained on code",
      "paper_id": "2107.03374v2"
    },
    {
      "index": 19,
      "title": "Assemble foundation models for automatic code summarization",
      "abstract": "",
      "year": "2022",
      "venue": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
      "authors": "J. Gu, P. Salza, and H. C. Gall"
    },
    {
      "index": 20,
      "title": "Assessing generalizability of codebert",
      "abstract": "",
      "year": "2021",
      "venue": "2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
      "authors": "X. Zhou, D. Han, and D. Lo"
    },
    {
      "index": 21,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "V. Sanh, L. Debut, J. Chaumond et al.",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 22,
      "title": "Optimal brain damage",
      "abstract": "",
      "year": "1989",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. LeCun, J. Denker, and S. Solla"
    },
    {
      "index": 23,
      "title": "Pruning Filters for Efficient ConvNets",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "H. Li, A. Kadav, I. Durdanovic et al.",
      "orig_title": "Pruning filters for efficient convnets",
      "paper_id": "1608.08710v3"
    },
    {
      "index": 24,
      "title": "Dynamic Network Surgery for Efficient DNNs",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Y. Guo, A. Yao, and Y. Chen",
      "orig_title": "Dynamic network surgery for efficient dnns",
      "paper_id": "1608.04493v2"
    },
    {
      "index": 25,
      "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "P. Molchanov, S. Tyree, T. Karras et al.",
      "orig_title": "Pruning convolutional neural networks for resource efficient inference",
      "paper_id": "1611.06440v2"
    },
    {
      "index": 26,
      "title": "The state of sparsity in deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "T. Gale, E. Elsen, and S. Hooker"
    },
    {
      "index": 27,
      "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "D. Guo, S. Ren, S. Lu et al.",
      "orig_title": "Graphcodebert: Pre-training code representations with data flow",
      "paper_id": "2009.08366v4"
    },
    {
      "index": 28,
      "title": "Software defect prediction via transformer",
      "abstract": "",
      "year": "2020",
      "venue": "2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",
      "authors": "Q. Zhang and B. Wu"
    },
    {
      "index": 29,
      "title": "Transˆ 3: A transformer-based framework for unifying code summarization and code search",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.03238",
      "authors": "W. Wang, Y. Zhang, Z. Zeng et al."
    },
    {
      "index": 30,
      "title": "A coefficient of agreement as a measure of thematic classification accuracy.",
      "abstract": "",
      "year": "1986",
      "venue": "Photogrammetric Engineering and Remote Sensing",
      "authors": "G. Rosenfield and K. Fitzpatrick-Lins"
    },
    {
      "index": 31,
      "title": "Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool",
      "abstract": "",
      "year": "2015",
      "venue": "BMC Medical Imaging",
      "authors": "A. A. Taha and A. Hanbury"
    },
    {
      "index": 32,
      "title": "The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation",
      "abstract": "",
      "year": "2020",
      "venue": "BMC genomics",
      "authors": "D. Chicco and G. Jurman"
    },
    {
      "index": 33,
      "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
      "abstract": "",
      "year": "2002",
      "venue": "40th Annual Meeting of the Association for Computational Linguistics",
      "authors": "K. Papineni, S. Roukos, T. Ward et al."
    },
    {
      "index": 34,
      "title": "BERTScore: Evaluating Text Generation with BERT",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.09675",
      "authors": "T. Zhang, V. Kishore, F. Wu et al.",
      "orig_title": "Bertscore: Evaluating text generation with bert",
      "paper_id": "1904.09675v3"
    },
    {
      "index": 35,
      "title": "Evaluating Code Summarization Techniques: A New Metric and an Empirical Characterization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "A. Mastropaolo, M. Ciniselli, M. Di Penta et al.",
      "orig_title": "Evaluating Code Summarization Techniques: A New Metric and an Empirical Characterization",
      "paper_id": "2312.15475v1"
    },
    {
      "index": 36,
      "title": "Expected reciprocal rank for graded relevance",
      "abstract": "",
      "year": "2009",
      "venue": "18th ACM Conference on Information and Knowledge Management",
      "authors": "O. Chapelle, D. Metlzer, Y. Zhang et al."
    },
    {
      "index": 37,
      "title": "Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Zhou, S. Liu, J. Siow et al.",
      "orig_title": "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
      "paper_id": "1909.03496v1"
    },
    {
      "index": 38,
      "title": "CodeSearchNet Challenge Evaluating the State of Semantic Code Search",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.09436",
      "authors": "H. Husain, H.-H. Wu, T. Gazit et al.",
      "orig_title": "Codesearchnet challenge: Evaluating the state of semantic code search",
      "paper_id": "1909.09436v3"
    },
    {
      "index": 39,
      "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu",
      "orig_title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "paper_id": "2102.04664v2"
    },
    {
      "index": 40,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "V. Sanh, L. Debut, J. Chaumond et al.",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 41,
      "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "M. A. Gordon, K. Duh, and N. Andrews",
      "orig_title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning",
      "paper_id": "2002.08307v2"
    },
    {
      "index": 42,
      "title": "Pruning filters with l1-norm and capped l1-norm for cnn compression",
      "abstract": "",
      "year": "2021",
      "venue": "Applied Intelligence",
      "authors": "A. Kumar, A. M. Shaikh, Y. Li et al."
    },
    {
      "index": 43,
      "title": "AI-driven Java Performance Testing: Balancing Result Quality with Testing Time",
      "abstract": "",
      "year": "2024",
      "venue": "39th IEEE/ACM International Conference on Automated Software Engineering",
      "authors": "L. Traini, F. Di Menna, and V. Cortellessa",
      "orig_title": "Ai-driven java performance testing: Balancing result quality with testing time",
      "paper_id": "2408.05100v2"
    },
    {
      "index": 44,
      "title": "Automated generation and evaluation of jmh microbenchmark suites from unit tests",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Software Engineering",
      "authors": "M. Jangali, Y. Tang, N. Alexandersson et al."
    },
    {
      "index": 45,
      "title": "Faster or Slower? Performance Mystery of Python Idioms Unveiled with Empirical Evidence",
      "abstract": "",
      "year": "2023",
      "venue": "45th International Conference on Software Engineering",
      "authors": "Z. Zhang, Z. Xing, X. Xia et al.",
      "orig_title": "Faster or slower? performance mystery of python idioms unveiled with empirical evidence",
      "paper_id": "2301.12633v1"
    },
    {
      "index": 46,
      "title": "Dynamically reconfiguring software microbenchmarks: Reducing execution time without sacrificing result quality",
      "abstract": "",
      "year": "2020",
      "venue": "28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "authors": "C. Laaber, S. Würsten, H. C. Gall et al."
    },
    {
      "index": 47,
      "title": "Rigorous benchmarking in reasonable time",
      "abstract": "",
      "year": "2013",
      "venue": "2013 International Symposium on Memory Management",
      "authors": "T. Kalibera and R. Jones"
    },
    {
      "index": 48,
      "title": "Quantifying Performance Changes with Effect Size Confidence Intervals",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "——",
      "orig_title": "Quantifying performance changes with effect size confidence intervals",
      "paper_id": "2007.10899v1"
    },
    {
      "index": 49,
      "title": "Wilcoxon signed-rank test",
      "abstract": "",
      "year": "2005",
      "venue": "Encyclopedia of Biostatistics",
      "authors": "R. F. Woolson"
    },
    {
      "index": 50,
      "title": "How software refactoring impacts execution time",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Trans. Softw. Eng. Methodol.",
      "authors": "L. Traini, D. Di Pompeo, M. Tucci et al."
    },
    {
      "index": 51,
      "title": "Searching for a needle in a haystack: Predicting security vulnerabilities for windows vista",
      "abstract": "",
      "year": "2010",
      "venue": "3rd International Conference on Software Testing, Verification and Validation (ICST)",
      "authors": "T. Zimmermann, N. Nagappan, and L. Williams"
    },
    {
      "index": 52,
      "title": "The importance of accounting for real-world labelling when predicting software vulnerabilities",
      "abstract": "",
      "year": "2019",
      "venue": "2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "authors": "M. Jimenez, R. Rwemalika, M. Papadakis et al."
    },
    {
      "index": 53,
      "title": "On the use of evaluation measures for defect prediction studies",
      "abstract": "",
      "year": "2022",
      "venue": "31st ACM SIGSOFT International Symposium on Software Testing and Analysis",
      "authors": "R. Moussa and F. Sarro"
    },
    {
      "index": 54,
      "title": "Source Code Summarization in the Era of Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "W. Sun, Y. Miao, Y. Li et al.",
      "orig_title": "Source Code Summarization in the Era of Large Language Models",
      "paper_id": "2407.07959v1"
    },
    {
      "index": 55,
      "title": "Survey of Code Search Based on Deep Learning",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Trans. Softw. Eng. Methodol.",
      "authors": "Y. Xie, J. Lin, H. Dong et al.",
      "orig_title": "Survey of code search based on deep learning",
      "paper_id": "2305.05959v2"
    },
    {
      "index": 56,
      "title": "A survey of quantization methods for efficient neural network inference",
      "abstract": "",
      "year": "2022",
      "venue": "Low-Power Computer Vision",
      "authors": "A. Gholami, S. Kim, Z. Dong et al."
    },
    {
      "index": 57,
      "title": "Up or Down? Adaptive Rounding for Post-Training Quantization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "M. Nagel, R. A. Amjad, M. van Baalen et al.",
      "orig_title": "Up or down? adaptive rounding for post-training quantization",
      "paper_id": "2004.10568v2"
    },
    {
      "index": 58,
      "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT",
      "abstract": "",
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "P. Ganesh, Y. Chen, X. Lou et al.",
      "orig_title": "Compressing large-scale transformer-based models: A case study on bert",
      "paper_id": "2002.11985v2"
    },
    {
      "index": 59,
      "title": "Rethinking the Value of Network Pruning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Z. Liu, M. Sun, T. Zhou et al.",
      "orig_title": "Rethinking the value of network pruning",
      "paper_id": "1810.05270v2"
    },
    {
      "index": 60,
      "title": "Taming performance variability",
      "abstract": "",
      "year": "2018",
      "venue": "13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)",
      "authors": "A. Maricq, D. Duplyakin, I. Jimenez et al."
    }
  ]
}