{
  "paper_id": "2304.03589v1",
  "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review",
  "abstract": "Abstract\nThe field of deep learning has witnessed significant progress in recent times, particularly in areas such as computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. However, it extremely suffers from the unstable training process and stringent requirements of computational resources. With the increasing demands on the adaption of computational capacity, though numerous studies have explored the efficient training field to a certain extent, a comprehensive summarization/guideline on those general acceleration techniques of training large-scale deep learning models is still much anticipated. In this survey, we present a detailed review of the general techniques for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives:\n(1) “data-centric”: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples;\n(2) “model-centric”, including acceleration of basic modules, compression training, model initialization and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters and providing better initialization;\n(3) “optimization-centric”, including the selection of learning rate, the employment of large batchsize, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models;\n(4) “budgeted training”, including some distinctive acceleration methods on source-constrained situations, e.g. for limitation on the total iterations;\n(5) “system-centric”, including some efficient distributed frameworks and open-source libraries which provide adequate hardware support for the implementation of above mentioned acceleration algorithms.\nBy presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction. Meanwhile, we further provide a detailed analysis and discussion of future works on the development of general acceleration techniques, which could inspire us to re-think and design novel efficient paradigms.\nOverall, we hope that this survey will serve as a valuable guideline for general efficient training.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Iterative procedures for nonlinear integral equations",
      "abstract": "",
      "year": "1965",
      "venue": "Journal of the ACM (JACM)",
      "authors": "Donald G Anderson"
    },
    {
      "index": 1,
      "title": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach"
    },
    {
      "index": 2,
      "title": "Towards Understanding Sharpness-Aware Minimization",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Maksym Andriushchenko and Nicolas Flammarion",
      "orig_title": "Towards understanding sharpness-aware minimization",
      "paper_id": "2206.06232v1"
    },
    {
      "index": 3,
      "title": "How Important is Importance Sampling for Deep Budgeted Training?",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.14283",
      "authors": "Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness",
      "orig_title": "How important is importance sampling for deep budgeted training?",
      "paper_id": "2110.14283v1"
    },
    {
      "index": 4,
      "title": "Adaptive Input Representations for Neural Language Modeling",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1809.10853",
      "authors": "Alexei Baevski and Michael Auli",
      "orig_title": "Adaptive input representations for neural language modeling",
      "paper_id": "1809.10853v3"
    },
    {
      "index": 5,
      "title": "Efficient data loader for fast sampling-based gnn training on large graphs",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Parallel and Distributed Systems",
      "authors": "Youhui Bai, Cheng Li, Zhiqi Lin, Yufei Wu, Youshan Miao, Yunxin Liu, and Yinlong Xu"
    },
    {
      "index": 6,
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.08254",
      "authors": "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei",
      "orig_title": "Beit: Bert pre-training of image transformers",
      "paper_id": "2106.08254v2"
    },
    {
      "index": 7,
      "title": "Greedy layer-wise training of deep networks",
      "abstract": "",
      "year": "2006",
      "venue": "Advances in neural information processing systems",
      "authors": "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle"
    },
    {
      "index": 8,
      "title": "Curriculum learning",
      "abstract": "",
      "year": "2009",
      "venue": "26th annual international conference on machine learning",
      "authors": "Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston"
    },
    {
      "index": 9,
      "title": "Quasi-newton methods for machine learning: forget the past, just sample",
      "abstract": "",
      "year": "2022",
      "venue": "Optimization Methods and Software",
      "authors": "Albert S Berahas, Majid Jahani, Peter Richtárik, and Martin Takáč"
    },
    {
      "index": 10,
      "title": "Colossal-ai: A unified deep learning system for large-scale parallel training",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.14883",
      "authors": "Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang You"
    },
    {
      "index": 11,
      "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.14450",
      "authors": "Zhengda Bian, Qifan Xu, Boxiang Wang, and Yang You",
      "orig_title": "Maximizing parallelism in distributed training for huge neural networks",
      "paper_id": "2105.14450v1"
    },
    {
      "index": 12,
      "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "ACL Workshop on Challenges & Perspectives in Creating Large Language Models",
      "authors": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach",
      "orig_title": "GPT-NeoX-20B: An open-source autoregressive language model",
      "paper_id": "2204.06745v1"
    },
    {
      "index": 13,
      "title": "Token Merging: Your ViT But Faster",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.09461",
      "authors": "Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman",
      "orig_title": "Token merging: Your vit but faster",
      "paper_id": "2210.09461v3"
    },
    {
      "index": 14,
      "title": "On the opportunities and risks of foundation models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2108.07258",
      "authors": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al."
    },
    {
      "index": 15,
      "title": "JAX: composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang"
    },
    {
      "index": 16,
      "title": "FreezeOut: Accelerate Training by Progressively Freezing Layers",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.04983",
      "authors": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston",
      "orig_title": "Freezeout: Accelerate training by progressively freezing layers",
      "paper_id": "1706.04983v2"
    },
    {
      "index": 17,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 18,
      "title": "Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.14756",
      "authors": "Han Cai, Chuang Gan, and Song Han"
    },
    {
      "index": 19,
      "title": "Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.09967",
      "authors": "Brad Carlile, Guy Delamarter, Paul Kinney, Akiko Marti, and Brian Whitney",
      "orig_title": "Improving deep learning by inverse square root linear units (isrlus)",
      "paper_id": "1710.09967v2"
    },
    {
      "index": 20,
      "title": "Avalanche: A PyTorch Library for Deep Continual Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.01766",
      "authors": "Antonio Carta, Lorenzo Pellegrini, Andrea Cossu, Hamed Hemati, and Vincenzo Lomonaco",
      "orig_title": "Avalanche: A pytorch library for deep continual learning",
      "paper_id": "2302.01766v1"
    },
    {
      "index": 21,
      "title": "Scatterbrain: Unifying sparse and low-rank attention",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher\nRé."
    },
    {
      "index": 22,
      "title": "Transmix: Attend to mix for vision transformers",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan Yuille, and Song Bai."
    },
    {
      "index": 23,
      "title": "REX: Revisiting Budgeted Training with an Improved Schedule",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of Machine Learning and Systems, 4:64–76",
      "authors": "John Chen, Cameron Wolfe, and Tasos Kyrillidis.",
      "orig_title": "Rex: Revisiting budgeted training with an improved schedule",
      "paper_id": "2107.04197v1"
    },
    {
      "index": 24,
      "title": "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang.",
      "orig_title": "Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective",
      "paper_id": "2103.00397v3"
    },
    {
      "index": 25,
      "title": "Training deep nets with sublinear memory cost",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin."
    },
    {
      "index": 26,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 27,
      "title": "Symbolic Discovery of Optimization Algorithms",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu\nPham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al.",
      "orig_title": "Symbolic discovery of optimization algorithms",
      "paper_id": "2302.06675v4"
    },
    {
      "index": 28,
      "title": "Mask-guided vision transformer (mg-vit) for few-shot learning",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuzhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong\nLiu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, et al."
    },
    {
      "index": 29,
      "title": "Masked-attention mask transformer for universal image segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit\nGirdhar."
    },
    {
      "index": 30,
      "title": "Fast 2d convolution algorithms for convolutional neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers,\n67(5):",
      "authors": "Chao Cheng and Keshab K. Parhi."
    },
    {
      "index": 31,
      "title": "Rethinking attention with performers",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al."
    },
    {
      "index": 32,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 33,
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter.",
      "orig_title": "Fast and accurate deep network learning by exponential linear units (elus)",
      "paper_id": "1511.07289v5"
    },
    {
      "index": 34,
      "title": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
      "abstract": "",
      "year": "2020",
      "venue": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
      "authors": "Rosset Corby."
    },
    {
      "index": 35,
      "title": "Autoaugment: Learning augmentation policies from data",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le."
    },
    {
      "index": 36,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops",
      "authors": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le."
    },
    {
      "index": 37,
      "title": "Supermix: Supervising the mixing data augmentation",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition",
      "authors": "Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, and Nasser M Nasrabadi."
    },
    {
      "index": 38,
      "title": "Chataug: Leveraging chatgpt for text data augmentation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao,\nWei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al."
    },
    {
      "index": 39,
      "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré."
    },
    {
      "index": 40,
      "title": "A scalable pipeline for gigapixel whole slide imaging analysis on leadership class hpc systems",
      "abstract": "",
      "year": "2022",
      "venue": "2022 IEEE International Parallel and Distributed Processing\nSymposium Workshops (IPDPSW)",
      "authors": "Sajal Dash, Benjamín Hernández, Aristeidis Tsaris, Folami T Alamudun,\nHong-Jun Yoon, and Feivi Wang."
    },
    {
      "index": 41,
      "title": "Metainit: Initializing learning by learning to initialize",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems, 32",
      "authors": "Yann N Dauphin and Samuel Schoenholz."
    },
    {
      "index": 42,
      "title": "Scenic: A JAX Library for Computer Vision Research and Beyond",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR)",
      "authors": "Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and\nYi Tay.",
      "orig_title": "Scenic: A jax library for computer vision research and beyond",
      "paper_id": "2110.11403v1"
    },
    {
      "index": 43,
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan\nHeek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim\nAlabdulmohsin, et al.",
      "orig_title": "Scaling vision transformers to 22 billion parameters",
      "paper_id": "2302.05442v1"
    },
    {
      "index": 44,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern\nrecognition",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei."
    },
    {
      "index": 45,
      "title": "Quasi-newton methods, motivation and theory",
      "abstract": "",
      "year": "1977",
      "venue": "SIAM review, 19(1):46–89",
      "authors": "John E Dennis, Jr and Jorge J Moré."
    },
    {
      "index": 46,
      "title": "8-bit Optimizers via Block-wise Quantization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.",
      "orig_title": "8-bit optimizers via block-wise quantization",
      "paper_id": "2110.02861v2"
    },
    {
      "index": 47,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 48,
      "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Terrance DeVries and Graham W Taylor.",
      "orig_title": "Improved regularization of convolutional neural networks with cutout",
      "paper_id": "1708.04552v2"
    },
    {
      "index": 49,
      "title": "Sequential normalization: Embracing smaller sample sizes for normalization",
      "abstract": "",
      "year": "2022",
      "venue": "Information, 13(7):337",
      "authors": "Neofytos Dimitriou and Ognjen Arandjelović."
    },
    {
      "index": 50,
      "title": "Progressive multi-granularity training for non-autoregressive translation",
      "abstract": "",
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics",
      "authors": "Liang Ding, Longyue Wang, Xuebo Liu, Derek F Wong, Dacheng Tao, and Zhaopeng\nTu."
    },
    {
      "index": 51,
      "title": "Re-parameterizing Your Optimizers rather than Architectures",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi Huang, Jungong Han, and\nGuiguang Ding.",
      "orig_title": "Re-parameterizing your optimizers rather than architectures",
      "paper_id": "2205.15242v4"
    },
    {
      "index": 52,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 53,
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of machine learning research, 12(7)",
      "authors": "John Duchi, Elad Hazan, and Yoram Singer."
    },
    {
      "index": 54,
      "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training",
      "abstract": "",
      "year": "2022",
      "venue": "Uncertainty in Artificial Intelligence, pages 610–620.\nPMLR",
      "authors": "Chen Dun, Cameron R Wolfe, Christopher M Jermaine, and Anastasios Kyrillidis.",
      "orig_title": "Resist: Layer-wise decomposition of resnets for distributed training",
      "paper_id": "2107.00961v2"
    },
    {
      "index": 55,
      "title": "Understanding Back-Translation at Scale",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.",
      "orig_title": "Understanding back-translation at scale",
      "paper_id": "1808.09381v2"
    },
    {
      "index": 56,
      "title": "Learning and development in neural networks: The importance of starting small",
      "abstract": "",
      "year": "1993",
      "venue": "Cognition, 48(1):71–99",
      "authors": "Jeffrey L Elman."
    },
    {
      "index": 57,
      "title": "Fairscale: A general purpose modular pytorch library for high performance and large scale training",
      "abstract": "",
      "year": "2021",
      "venue": "https://github.com/facebookresearch/fairscale",
      "authors": "FairScale authors."
    },
    {
      "index": 58,
      "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design",
      "abstract": "",
      "year": "2022",
      "venue": "2022 55th IEEE/ACM International Symposium on\nMicroarchitecture (MICRO)",
      "authors": "Hongxiang Fan, Thomas Chau, Stylianos I Venieris, Royson Lee, Alexandros\nKouris, Wayne Luk, Nicholas D Lane, and Mohamed S Abdelfattah.",
      "orig_title": "Adaptable butterfly accelerator for attention-based nns via hardware and algorithm co-design",
      "paper_id": "2209.09570v1"
    },
    {
      "index": 59,
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "J. Mach. Learn. Res, 23:1–40",
      "authors": "William Fedus, Barret Zoph, and Noam Shazeer.",
      "orig_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "paper_id": "2101.03961v3"
    },
    {
      "index": 60,
      "title": "(beta) channels last memory format in pytorch, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "URL\nhttps://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#beta-channels-last-memory-format-in-pytorch",
      "authors": "Vitaly Fedyunin."
    },
    {
      "index": 61,
      "title": "A Survey of Data Augmentation Approaches for NLP",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,\nTeruko Mitamura, and Eduard Hovy.",
      "orig_title": "A survey of data augmentation approaches for nlp",
      "paper_id": "2105.03075v5"
    },
    {
      "index": 62,
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.",
      "orig_title": "Sharpness-aware minimization for efficiently improving generalization",
      "paper_id": "2010.01412v3"
    },
    {
      "index": 63,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonathan Frankle and Michael Carbin.",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 64,
      "title": "M-FAC: Efficient Matrix-Free Approximations of Second-Order Information",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Elias Frantar, Eldar Kurtic, and Dan Alistarh.",
      "orig_title": "M-fac: Efficient matrix-free approximations of second-order information",
      "paper_id": "2107.03356v5"
    },
    {
      "index": 65,
      "title": "Bagua: Scaling up distributed learning with system relaxations, 2021",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Shaoduo Gan, Xiangru Lian, Rui Wang, Jianbin Chang, Chengjun Liu, Hongmei Shi,\nShengzhuo Zhang, Xianghong Li, Tengxu Sun, Jiawei Jiang, Binhang Yuan, Sen\nYang, Ji Liu, and Ce Zhang."
    },
    {
      "index": 66,
      "title": "The pile: An 800gb dataset of diverse text for language modeling",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al."
    },
    {
      "index": 67,
      "title": "Cramming: Training a Language Model on a Single GPU in One Day",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonas Geiping and Tom Goldstein.",
      "orig_title": "Cramming: Training a language model on a single gpu in one day",
      "paper_id": "2212.14034v1"
    },
    {
      "index": 68,
      "title": "A survey on efficient convolutional neural networks and hardware acceleration",
      "abstract": "",
      "year": "2022",
      "venue": "Electronics, 11(6):945",
      "authors": "Deepak Ghimire, Dayoung Kil, and Seong-heum Kim."
    },
    {
      "index": 69,
      "title": "Resprop: Reuse sparsified backpropagation",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Negar Goli and Tor M Aamodt."
    },
    {
      "index": 70,
      "title": "On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami, Kai\nRothauge, Michael W Mahoney, and Joseph Gonzalez.",
      "orig_title": "On the computational inefficiency of large batch sizes for stochastic gradient descent",
      "paper_id": "1811.12941v1"
    },
    {
      "index": 71,
      "title": "Efficient training of bert by progressively stacking",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu."
    },
    {
      "index": 72,
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz\nWesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.",
      "orig_title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "paper_id": "1706.02677v2"
    },
    {
      "index": 73,
      "title": "Self-supervised Pretraining of Visual Features in the Wild",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek\nPai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al.",
      "orig_title": "Self-supervised pretraining of visual features in the wild",
      "paper_id": "2103.01988v2"
    },
    {
      "index": 74,
      "title": "On the Transformer Growth for Progressive BERT Training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han.",
      "orig_title": "On the transformer growth for progressive bert training",
      "paper_id": "2010.12562v3"
    },
    {
      "index": 75,
      "title": "Sequence-Level Mixed Sample Data Augmentation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Demi Guo, Yoon Kim, and Alexander M Rush.",
      "orig_title": "Sequence-level mixed sample data augmentation",
      "paper_id": "2011.09039v1"
    },
    {
      "index": 76,
      "title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Vineet Gupta, Tomer Koren, and Yoram Singer.",
      "orig_title": "Shampoo: Preconditioned stochastic tensor optimization",
      "paper_id": "1802.09568v2"
    },
    {
      "index": 77,
      "title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit",
      "abstract": "",
      "year": "2000",
      "venue": "nature, 405(",
      "authors": "Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and\nH Sebastian Seung."
    },
    {
      "index": 78,
      "title": "Turbo Training with Token Dropout",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tengda Han, Weidi Xie, and Andrew Zisserman.",
      "orig_title": "Turbo training with token dropout",
      "paper_id": "2210.04889v1"
    },
    {
      "index": 79,
      "title": "Pre-Trained Models: Past, Present and Future",
      "abstract": "",
      "year": "2021",
      "venue": "AI Open, 2:225–250",
      "authors": "Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu,\nYuan Yao, Ao Zhang, Liang Zhang, et al.",
      "orig_title": "Pre-trained models: Past, present and future",
      "paper_id": "2106.07139v3"
    },
    {
      "index": 80,
      "title": "Accelerating parallel stochastic gradient descent via non-blocking mini-batches",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Haoze He and Parijat Dube."
    },
    {
      "index": 81,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 82,
      "title": "Masked autoencoders are scalable vision learners",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross\nGirshick."
    },
    {
      "index": 83,
      "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li.",
      "orig_title": "Bag of tricks for image classification with convolutional neural networks",
      "paper_id": "1812.01187v2"
    },
    {
      "index": 84,
      "title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaoxin He, Fuzhao Xue, Xiaozhe Ren, and Yang You.",
      "orig_title": "Large-scale deep learning optimizations: A comprehensive survey",
      "paper_id": "2111.00856v2"
    },
    {
      "index": 85,
      "title": "Flax: A neural network library and ecosystem for JAX, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "URL http://github.com/google/flax",
      "authors": "Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand\nRondepierre, Andreas Steiner, and Marc van Zee."
    },
    {
      "index": 86,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dan Hendrycks and Kevin Gimpel.",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 87,
      "title": "Using pre-training can improve model robustness and uncertainty",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Dan Hendrycks, Kimin Lee, and Mantas Mazeika."
    },
    {
      "index": 88,
      "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji\nLakshminarayanan.",
      "orig_title": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "paper_id": "1912.02781v2"
    },
    {
      "index": 89,
      "title": "Haiku: Sonnet for JAX, 2020",
      "abstract": "",
      "year": "2020",
      "venue": "URL http://github.com/deepmind/dm-haiku",
      "authors": "Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin."
    },
    {
      "index": 90,
      "title": "Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel.",
      "orig_title": "Population based augmentation: Efficient learning of augmentation policy schedules",
      "paper_id": "1905.05393v1"
    },
    {
      "index": 91,
      "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems, 30",
      "authors": "Elad Hoffer, Itay Hubara, and Daniel Soudry.",
      "orig_title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
      "paper_id": "1705.08741v2"
    },
    {
      "index": 92,
      "title": "Training Compute-Optimal Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al.",
      "orig_title": "Training compute-optimal large language models",
      "paper_id": "2203.15556v1"
    },
    {
      "index": 93,
      "title": "Stylemix: Separating content and style for enhanced data augmentation",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition",
      "authors": "Minui Hong, Jinwoo Choi, and Gunhee Kim."
    },
    {
      "index": 94,
      "title": "Fastai - progressive resizing",
      "abstract": "",
      "year": "2018",
      "venue": "https://www.fast.ai/",
      "authors": "Jeremy Howard."
    },
    {
      "index": 95,
      "title": "Fastai: A layered api for deep learning",
      "abstract": "",
      "year": "2020",
      "venue": "Information, 11(2):108",
      "authors": "Jeremy Howard and Sylvain Gugger."
    },
    {
      "index": 96,
      "title": "Transformer Quality in Linear Time",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le.",
      "orig_title": "Transformer quality in linear time",
      "paper_id": "2202.10447v2"
    },
    {
      "index": 97,
      "title": "Hardware-aware quantization/mapping strategies for compute-in-memory accelerators",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Transactions on Design Automation of Electronic Systems",
      "authors": "Shanshi Huang, Hongwu Jiang, and Shimeng Yu."
    },
    {
      "index": 98,
      "title": "Improving transformer optimization through better initialization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs."
    },
    {
      "index": 99,
      "title": "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems, 32",
      "authors": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,\nHyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al."
    },
    {
      "index": 100,
      "title": "Decoupled Parallel Backpropagation with Convergence Guarantee",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhouyuan Huo, Bin Gu, Heng Huang, et al.",
      "orig_title": "Decoupled parallel backpropagation with convergence guarantee",
      "paper_id": "1804.10574v3"
    },
    {
      "index": 101,
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Sergey Ioffe and Christian Szegedy."
    },
    {
      "index": 102,
      "title": "Averaging Weights Leads to Wider Optima and Better Generalization",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and\nAndrew Gordon Wilson.",
      "orig_title": "Averaging weights leads to wider optima and better generalization",
      "paper_id": "1803.05407v3"
    },
    {
      "index": 103,
      "title": "How to train bert with an academic budget",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing",
      "authors": "Peter Izsak, Moshe Berchansky, and Omer Levy."
    },
    {
      "index": 104,
      "title": "Sparse is Enough in Scaling Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser,\nWojciech Gajewski, Henryk Michalewski, and Jonni Kanerva.",
      "orig_title": "Sparse is enough in scaling transformers",
      "paper_id": "2111.12763v1"
    },
    {
      "index": 105,
      "title": "A data-loader tunable knob to shorten gpu idleness for distributed deep learning",
      "abstract": "",
      "year": "2022",
      "venue": "2022 IEEE 15th International Conference on Cloud Computing\n(CLOUD)",
      "authors": "Danlin Jia, Geng Yuan, Xue Lin, and Ningfang Mi."
    },
    {
      "index": 106,
      "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou,\nLiqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al.",
      "orig_title": "Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes",
      "paper_id": "1807.11205v1"
    },
    {
      "index": 107,
      "title": "Accelerating deep learning by focusing on the biggest losers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean,\nGregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C\nLipton, et al."
    },
    {
      "index": 108,
      "title": "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging",
      "abstract": "",
      "year": "2022",
      "venue": "Has it Trained Yet? NeurIPS",
      "authors": "Jean Kaddour.",
      "orig_title": "Stop wasting my time! saving days of imagenet and BERT training with latest weight averaging",
      "paper_id": "2209.14981v2"
    },
    {
      "index": 109,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 110,
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.",
      "orig_title": "Progressive growing of gans for improved quality, stability, and variation",
      "paper_id": "1710.10196v3"
    },
    {
      "index": 111,
      "title": "Biased Importance Sampling for Deep Neural Network Training",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Angelos Katharopoulos and François Fleuret.",
      "orig_title": "Biased importance sampling for deep neural network training",
      "paper_id": "1706.00043v2"
    },
    {
      "index": 112,
      "title": "Not all samples are created equal: Deep learning with importance sampling",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Angelos Katharopoulos and François Fleuret."
    },
    {
      "index": 113,
      "title": "Ordered sgd: A new stochastic optimization framework for empirical risk minimization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and\nStatistics",
      "authors": "Kenji Kawaguchi and Haihao Lu."
    },
    {
      "index": 114,
      "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,\nand Ping Tak Peter Tang."
    },
    {
      "index": 115,
      "title": "Supervised Contrastive Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems,\n33:",
      "authors": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip\nIsola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.",
      "orig_title": "Supervised contrastive learning",
      "paper_id": "2004.11362v5"
    },
    {
      "index": 116,
      "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song.",
      "orig_title": "Co-mixup: Saliency guided joint mixup with supermodular diversity",
      "paper_id": "2102.03065v1"
    },
    {
      "index": 117,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:",
      "authors": "Diederik P Kingma and Jimmy Ba."
    },
    {
      "index": 118,
      "title": "Reformer: The Efficient Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.",
      "orig_title": "Reformer: The efficient transformer",
      "paper_id": "2001.04451v2"
    },
    {
      "index": 119,
      "title": "Self-normalizing neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems, 30",
      "authors": "Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter."
    },
    {
      "index": 120,
      "title": "Accelerating Self-Supervised Learning via Efficient Training Strategies",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision",
      "authors": "Mustafa Taha Koçyiğit, Timothy M Hospedales, and Hakan Bilen.",
      "orig_title": "Accelerating self-supervised learning via efficient training strategies",
      "paper_id": "2212.05611v1"
    },
    {
      "index": 121,
      "title": "Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhenglun Kong, Haoyu Ma, Geng Yuan, Mengshu Sun, Yanyue Xie, Peiyan Dong, Xin\nMeng, Xuan Shen, Hao Tang, Minghai Qin, et al.",
      "orig_title": "Peeling the onion: Hierarchical reduction of data redundancy for efficient vision transformer training",
      "paper_id": "2211.10801v1"
    },
    {
      "index": 122,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Communications of the ACM, 60(6):84–90",
      "authors": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton."
    },
    {
      "index": 123,
      "title": "On weight initialization in deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Siddharth Krishna Kumar.",
      "orig_title": "On weight initialization in deep neural networks",
      "paper_id": "1704.08863v2"
    },
    {
      "index": 124,
      "title": "Christopher Akiki",
      "abstract": "",
      "year": "2022",
      "venue": "Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track",
      "authors": "Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki,\nAlbert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, et al."
    },
    {
      "index": 125,
      "title": "FFCV: Accelerating Training by Removing Data Bottlenecks",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman,\nand Aleksander Madry.",
      "orig_title": "Ffcv: Accelerating training by removing data bottlenecks",
      "paper_id": "2306.12517v1"
    },
    {
      "index": 126,
      "title": "Deduplicating Training Data Makes Language Models Better",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,\nChris Callison-Burch, and Nicholas Carlini.",
      "orig_title": "Deduplicating training data makes language models better",
      "paper_id": "2107.06499v2"
    },
    {
      "index": 127,
      "title": "xformers: A modular and hackable transformer modelling library",
      "abstract": "",
      "year": "2022",
      "venue": "https://github.com/facebookresearch/xformers",
      "authors": "Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio\nCaggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick\nLabatut, and Daniel Haziza."
    },
    {
      "index": 128,
      "title": "Shallow-to-Deep Training for Neural Machine Translation",
      "abstract": "",
      "year": "2010",
      "venue": "arXiv preprint arXiv:",
      "authors": "Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang,\nand Jingbo Zhu.",
      "orig_title": "Shallow-to-deep training for neural machine translation",
      "paper_id": "2010.03737v1"
    },
    {
      "index": 129,
      "title": "Automated Progressive Learning for Efficient Training of Vision Transformers",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and\nYi Yang.",
      "orig_title": "Automated progressive learning for efficient training of vision transformers",
      "paper_id": "2203.14509v1"
    },
    {
      "index": 130,
      "title": "Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Conglong Li, Minjia Zhang, and Yuxiong He."
    },
    {
      "index": 131,
      "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, and Yuxiong He.",
      "orig_title": "Deepspeed data efficiency: Improving deep learning model quality and training efficiency via efficient data sampling and routing",
      "paper_id": "2212.03597v3"
    },
    {
      "index": 132,
      "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Conglong Li, Minjia Zhang, and Yuxiong He.",
      "orig_title": "The stability-efficiency dilemma: Investigating sequence length warmup for training gpt models",
      "paper_id": "2108.06084v4"
    },
    {
      "index": 133,
      "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Mengtian Li, Ersin Yumer, and Deva Ramanan.",
      "orig_title": "Budgeted training: Rethinking deep neural network training under resource constraints",
      "paper_id": "1905.04753v4"
    },
    {
      "index": 134,
      "title": "Sequence parallelism: Making 4d parallelism possible",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You."
    },
    {
      "index": 135,
      "title": "Efficient Quantized Sparse Matrix Operations on Tensor Cores",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shigang Li, Kazuki Osawa, and Torsten Hoefler.",
      "orig_title": "Efficient quantized sparse matrix operations on tensor cores",
      "paper_id": "2209.06979v4"
    },
    {
      "index": 136,
      "title": "Trainable weight averaging for fast convergence and better generalization",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tao Li, Zhehao Huang, Qinghua Tao, Yingwen Wu, and Xiaolin Huang."
    },
    {
      "index": 137,
      "title": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool.",
      "orig_title": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation",
      "paper_id": "2111.12707v4"
    },
    {
      "index": 138,
      "title": "LocalViT: Bringing Locality to Vision Transformers",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool.",
      "orig_title": "Localvit: Bringing locality to vision transformers",
      "paper_id": "2104.05707v2"
    },
    {
      "index": 139,
      "title": "Butterfly factorization",
      "abstract": "",
      "year": "2015",
      "venue": "Multiscale Modeling & Simulation, 13(2):714–732",
      "authors": "Yingzhou Li, Haizhao Yang, Eileen R Martin, Kenneth L Ho, and Lexing Ying."
    },
    {
      "index": 140,
      "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 49th Annual International Symposium on\nComputer Architecture",
      "authors": "Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, and Mingu\nKang.",
      "orig_title": "Accelerating attention through gradient-based learned runtime pruning",
      "paper_id": "2204.03227v3"
    },
    {
      "index": 141,
      "title": "Train big, then compress: Rethinking model size for efficient training and inference of transformers",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and\nJoey Gonzalez."
    },
    {
      "index": 142,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2021",
      "venue": "https://github.com/Oneflow-Inc/libai",
      "authors": "Xingyu Liao, Peng Cheng, Tianhe Ren, Depeng Liang, Kai Dang, Yi Wang, and\nXiaoyu Xu.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 143,
      "title": "Fast AutoAugment",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems, 32",
      "authors": "Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim.",
      "orig_title": "Fast autoaugment",
      "paper_id": "1905.00397v2"
    },
    {
      "index": 144,
      "title": "A Survey of Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "AI Open",
      "authors": "Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.",
      "orig_title": "A survey of transformers",
      "paper_id": "2106.04554v2"
    },
    {
      "index": 145,
      "title": "TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Computer Vision–ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23–27, 2022, Proceedings, Part XXVI, pages 455–471.\nSpringer",
      "authors": "Jihao Liu, Boxiao Liu, Hang Zhou, Hongsheng Li, and Yu Liu.",
      "orig_title": "Tokenmix: Rethinking image mixing for data augmentation in vision transformers",
      "paper_id": "2207.08409v3"
    },
    {
      "index": 146,
      "title": "EcoFormer: Energy-Saving Attention with Linear Complexity",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bohan Zhuang.",
      "orig_title": "Ecoformer: Energy-saving attention with linear complexity",
      "paper_id": "2209.09004v3"
    },
    {
      "index": 147,
      "title": "Communication-efficient distributed learning for large batch optimization",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Rui Liu and Barzan Mozafari."
    },
    {
      "index": 148,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 149,
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on\ncomputer vision",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo."
    },
    {
      "index": 150,
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zirui Liu, Shengyuan Chen, Kaixiong Zhou, Daochen Zha, Xiao Huang, and Xia Hu.",
      "orig_title": "Rsc: Accelerating graph neural networks training via randomized sparse computations",
      "paper_id": "2210.10737v2"
    },
    {
      "index": 151,
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Jonathan Long, Evan Shelhamer, and Trevor Darrell.",
      "orig_title": "Fully convolutional networks for semantic segmentation",
      "paper_id": "1605.06211v1"
    },
    {
      "index": 152,
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ilya Loshchilov and Frank Hutter.",
      "orig_title": "Sgdr: Stochastic gradient descent with warm restarts",
      "paper_id": "1608.03983v5"
    },
    {
      "index": 153,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ilya Loshchilov and Frank Hutter.",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 154,
      "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, and Yuxiong He.",
      "orig_title": "Maximizing communication efficiency for large-scale training via 0/1 adam",
      "paper_id": "2202.06009v3"
    },
    {
      "index": 155,
      "title": "PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis",
      "authors": "Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and\nMattan Erez.",
      "orig_title": "Prunetrain: fast neural network training by dynamic sparse model reconfiguration",
      "paper_id": "1901.09290v5"
    },
    {
      "index": 156,
      "title": "HRBP: Hardware-friendly regrouping towards block-wise pruning for sparse training, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "URL https://openreview.net/forum?id=OSS-yWzE9Yu",
      "authors": "Haoyu Ma, Chengming Zhang, lizhi xiang, Xiaolong Ma, Geng Yuan, Wenkai Zhang,\nShiwei Liu, Tianlong Chen, Dingwen Tao, Yanzhi Wang, Zhangyang Wang, and\nXiaohui Xie."
    },
    {
      "index": 157,
      "title": "TorchScale: Transformers at Scale",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR, abs/",
      "authors": "Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon\nBenhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei.",
      "orig_title": "TorchScale: Transformers at scale",
      "paper_id": "2211.13184v1"
    },
    {
      "index": 158,
      "title": "Luna: Linear unified nested attention",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and\nLuke Zettlemoyer."
    },
    {
      "index": 159,
      "title": "Paddlepaddle: An open-source deep learning platform from industrial practice",
      "abstract": "",
      "year": "2019",
      "venue": "Frontiers of Data and Domputing, 1(1):105–115",
      "authors": "Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang."
    },
    {
      "index": 160,
      "title": "Efficient Winograd or Cook-Toom Convolution Kernel Implementation on Widely Used Mobile CPUs",
      "abstract": "",
      "year": "2019",
      "venue": "2019 2nd Workshop on Energy Efficient Machine Learning and\nCognitive Computing for Embedded Applications (EMC2)",
      "authors": "Partha Maji, Andrew Mundy, Ganesh Dasika, Jesse Beu, Matthew Mattina, and\nRobert Mullins.",
      "orig_title": "Efficient winograd or cook-toom convolution kernel implementation on widely used mobile cpus",
      "paper_id": "1903.01521v1"
    },
    {
      "index": 161,
      "title": "Project cgx: Scalable deep learning on commodity gpus",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ilia Markov, Hamidreza Ramezani, and Dan Alistarh."
    },
    {
      "index": 162,
      "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "James Martens and Roger Grosse.",
      "orig_title": "Optimizing neural networks with kronecker-factored approximate curvature",
      "paper_id": "1503.05671v7"
    },
    {
      "index": 163,
      "title": "An Empirical Model of Large-Batch Training",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team.",
      "orig_title": "An empirical model of large-batch training",
      "paper_id": "1812.06162v1"
    },
    {
      "index": 164,
      "title": "ButterflyFlow: Building Invertible Layers with Butterfly Matrices",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Chenlin Meng, Linqi Zhou, Kristy Choi, Tri Dao, and Stefano Ermon.",
      "orig_title": "Butterflyflow: Building invertible layers with butterfly matrices",
      "paper_id": "2209.13774v1"
    },
    {
      "index": 165,
      "title": "Mixed Precision Training",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,\nDavid Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, et al.",
      "orig_title": "Mixed precision training",
      "paper_id": "1710.03740v3"
    },
    {
      "index": 166,
      "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Sören Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas\nKirsch, Winnie Xu, Benedikt Höltgen, Aidan N Gomez, Adrien Morisot,\nSebastian Farquhar, et al.",
      "orig_title": "Prioritized training on points that are learnable, worth learning, and not yet learnt",
      "paper_id": "2206.07137v3"
    },
    {
      "index": 167,
      "title": "Mindspore, 2021",
      "abstract": "",
      "year": "2021",
      "venue": "URL https://github.com/mindspore-ai/mindspore",
      "authors": "MindSpore-AI."
    },
    {
      "index": 168,
      "title": "All you need is a good init",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dmytro Mishkin and Jiri Matas.",
      "orig_title": "All you need is a good init",
      "paper_id": "1511.06422v7"
    },
    {
      "index": 169,
      "title": "Accelerating Sparse Deep Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,\nGanesh Venkatesh, Chong Yu, and Paulius Micikevicius.",
      "orig_title": "Accelerating sparse deep neural networks",
      "paper_id": "2104.08378v1"
    },
    {
      "index": 170,
      "title": "When Does Label Smoothing Help?",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems, 32",
      "authors": "Rafael Müller, Simon Kornblith, and Geoffrey E Hinton.",
      "orig_title": "When does label smoothing help?",
      "paper_id": "1906.02629v3"
    },
    {
      "index": 171,
      "title": "Pre-training a bert with curriculum learning by increasing block-size of input text",
      "abstract": "",
      "year": "2021",
      "venue": "Recent Advances in Natural Language Processing",
      "authors": "Koichi Nagatsuka, Clifford Broni-Bediako, and Masayasu Atsumi."
    },
    {
      "index": 172,
      "title": "Pipedream: Generalized pipeline parallelism for dnn training",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples",
      "authors": "Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R\nDevanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia."
    },
    {
      "index": 173,
      "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis",
      "authors": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa\nPatwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie\nBernauer, Bryan Catanzaro, et al.",
      "orig_title": "Efficient large-scale language model training on gpu clusters using megatron-lm",
      "paper_id": "2104.04473v5"
    },
    {
      "index": 174,
      "title": "Improving Transformers with Probabilistic Attention Keys",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Tam Minh Nguyen, Tan Minh Nguyen, Dung DD Le, Duy Khuong Nguyen, Viet-Anh Tran,\nRichard Baraniuk, Nhat Ho, and Stanley Osher.",
      "orig_title": "Improving transformers with probabilistic attention keys",
      "paper_id": "2110.08678v2"
    },
    {
      "index": 175,
      "title": "K-SAM: Sharpness-Aware Minimization at the Speed of SGD",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Renkun Ni, Ping-yeh Chiang, Jonas Geiping, Micah Goldblum, Andrew Gordon\nWilson, and Tom Goldstein.",
      "orig_title": "K-sam: Sharpness-aware minimization at the speed of sgd",
      "paper_id": "2210.12864v1"
    },
    {
      "index": 176,
      "title": "Deep model assembling",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yue Cao, and Gao Huang."
    },
    {
      "index": 177,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2022",
      "venue": "https://github.com/NVIDIA/apex",
      "authors": "NVIDIA.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 178,
      "title": "Machine Learning Systems: Design and Implementation",
      "abstract": "",
      "year": "2022",
      "venue": "GitHub",
      "authors": "Open Machine Learning Systems Community."
    },
    {
      "index": 179,
      "title": "Bmtrain, 2021",
      "abstract": "",
      "year": "2021",
      "venue": "URL https://github.com/OpenBMB/BMTrain",
      "authors": "OpenBMB."
    },
    {
      "index": 180,
      "title": "fairseq: A fast, extensible toolkit for sequence modeling",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT",
      "authors": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,\nDavid Grangier, and Michael Auli."
    },
    {
      "index": 181,
      "title": "Ia-red: Interpretability-aware redundancy reduction for vision transformers",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude\nOliva."
    },
    {
      "index": 182,
      "title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Rui Pan, Haishan Ye, and Tong Zhang.",
      "orig_title": "Eigencurve: Optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums",
      "paper_id": "2110.14109v3"
    },
    {
      "index": 183,
      "title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT",
      "abstract": "",
      "year": "",
      "venue": "URL https://arxiv.org/abs/",
      "authors": "Rui Pan, Shizhe Diao, Jianlin Chen, and Tong Zhang.",
      "orig_title": "Extremebert: A toolkit for accelerating pretraining of customized bert",
      "paper_id": "2211.17201v1"
    },
    {
      "index": 184,
      "title": "Budgeted training for vision transformer",
      "abstract": "",
      "year": "",
      "venue": "The Eleventh International Conference on Learning\nRepresentations",
      "authors": "Xuran Pan, Xuan Jin, Yuan He, Shiji Song, Gao Huang, et al."
    },
    {
      "index": 185,
      "title": "Fast Vision Transformers with HiLo Attention",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zizheng Pan, Jianfei Cai, and Bohan Zhuang.",
      "orig_title": "Fast vision transformers with hilo attention",
      "paper_id": "2205.13213v5"
    },
    {
      "index": 186,
      "title": "Simpletrack: Understanding and rethinking 3d multi-object tracking",
      "abstract": "",
      "year": "2023",
      "venue": "Computer Vision–ECCV 2022 Workshops: Tel Aviv, Israel,\nOctober 23–27",
      "authors": "Ziqi Pang, Zhichao Li, and Naiyan Wang."
    },
    {
      "index": 187,
      "title": "A survey on textual entailment based question answering",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of King Saud University-Computer and Information\nSciences, 34(10):",
      "authors": "Aarthi Paramasivam and S Jaya Nirmala."
    },
    {
      "index": 188,
      "title": "KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis",
      "authors": "J Gregory Pauloski, Qi Huang, Lei Huang, Shivaram Venkataraman, Kyle Chard, Ian\nFoster, and Zhao Zhang.",
      "orig_title": "Kaisa: an adaptive second-order optimizer framework for deep neural networks",
      "paper_id": "2107.01739v2"
    },
    {
      "index": 189,
      "title": "Random Feature Attention",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and\nLingpeng Kong.",
      "orig_title": "Random feature attention",
      "paper_id": "2103.02143v2"
    },
    {
      "index": 190,
      "title": "Shortformer: Better language modeling using shorter inputs",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers)",
      "authors": "Ofir Press, Noah A Smith, and Mike Lewis."
    },
    {
      "index": 191,
      "title": "SpeeChain: a speech toolkit for large-scale machine speech chain",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Heli Qi, Sashi Novitasari, Andros Tjandra, Sakriani Sakti, and Satoshi\nNakamura.",
      "orig_title": "Speechain: A speech toolkit for large-scale machine speech chain",
      "paper_id": "2301.02966v1"
    },
    {
      "index": 192,
      "title": "The Devil in Linear Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhen Qin, XiaoDong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and\nYiran Zhong.",
      "orig_title": "The devil in linear transformer",
      "paper_id": "2210.10340v1"
    },
    {
      "index": 193,
      "title": "Pre-trained Models for Natural Language Processing: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "Science China Technological Sciences, 63(10):",
      "authors": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.",
      "orig_title": "Pre-trained models for natural language processing: A survey",
      "paper_id": "2003.08271v4"
    },
    {
      "index": 194,
      "title": "Self-attention does not need o​(n2)𝑜superscript𝑛2o(n^{2}) memory",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Markus N Rabe and Charles Staats."
    },
    {
      "index": 195,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al."
    },
    {
      "index": 196,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog, 1(8):9",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al."
    },
    {
      "index": 197,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International conference on machine learning",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 198,
      "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,\nFrancis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,\net al.",
      "orig_title": "Scaling language models: Methods, analysis & insights from training gopher",
      "paper_id": "2112.11446v2"
    },
    {
      "index": 199,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "The Journal of Machine Learning Research, 21(1):",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 200,
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems, 30",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.",
      "orig_title": "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
      "paper_id": "1706.05806v2"
    },
    {
      "index": 201,
      "title": "Zero: Memory optimizations toward training trillion parameter models",
      "abstract": "",
      "year": "2020",
      "venue": "SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis",
      "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He."
    },
    {
      "index": 202,
      "title": "Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis",
      "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He."
    },
    {
      "index": 203,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 204,
      "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining",
      "authors": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He."
    },
    {
      "index": 205,
      "title": "Ray: A distributed framework for emerging ai applications",
      "abstract": "",
      "year": "2023",
      "venue": "https://github.com/ray-project/ray",
      "authors": "Ray Project."
    },
    {
      "index": 206,
      "title": "You Only Look Once: Unified, Real-Time Object Detection",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.",
      "orig_title": "You only look once: Unified, real-time object detection",
      "paper_id": "1506.02640v5"
    },
    {
      "index": 207,
      "title": "Combiner: Full Attention Transformer with Sparse Computation Cost",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale\nSchuurmans, and Bo Dai.",
      "orig_title": "Combiner: Full attention transformer with sparse computation cost",
      "paper_id": "2107.05768v2"
    },
    {
      "index": 208,
      "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training",
      "abstract": "",
      "year": "",
      "venue": "USENIX Annual Technical Conference, pages 551–564",
      "authors": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan\nYang, Minjia Zhang, Dong Li, and Yuxiong He.",
      "orig_title": "Zero-offload: Democratizing billion-scale model training",
      "paper_id": "2101.06840v1"
    },
    {
      "index": 209,
      "title": "A stochastic approximation method",
      "abstract": "",
      "year": "1951",
      "venue": "The annals of mathematical statistics",
      "authors": "Herbert Robbins and Sutton Monro."
    },
    {
      "index": 210,
      "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Computing Surveys, 55(10):1–45",
      "authors": "Anna Rogers, Matt Gardner, and Isabelle Augenstein.",
      "orig_title": "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension",
      "paper_id": "2107.12708v2"
    },
    {
      "index": 211,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International journal of computer vision, 115:211–252",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 212,
      "title": "Bitblade: Energy-efficient variable bit-precision hardware accelerator for quantized neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Journal of Solid-State Circuits, 57(6):",
      "authors": "Sungju Ryu, Hyungjun Kim, Wooseok Yi, Eunhwan Kim, Yulhwa Kim, Taesu Kim, and\nJae-Joon Kim."
    },
    {
      "index": 213,
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrew M Saxe, James L McClelland, and Surya Ganguli."
    },
    {
      "index": 214,
      "title": "What Language Model to Train if You Have One Million GPU Hours?",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman,\nM Saiful Bari, Stella Bideman, Hady Elsahar, Niklas Muennighoff, Jason Phang,\net al.",
      "orig_title": "What language model to train if you have one million gpu hours?",
      "paper_id": "2210.15424v2"
    },
    {
      "index": 215,
      "title": "Regularized nonlinear acceleration",
      "abstract": "",
      "year": "2016",
      "venue": "Advances In Neural Information Processing Systems, 29",
      "authors": "Damien Scieur, Alexandre d’Aspremont, and Francis Bach."
    },
    {
      "index": 216,
      "title": "Nonlinear acceleration of stochastic algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems, 30",
      "authors": "Damien Scieur, Francis Bach, and Alexandre d’Aspremont."
    },
    {
      "index": 217,
      "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns",
      "abstract": "",
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech\ncommunication association",
      "authors": "Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu."
    },
    {
      "index": 218,
      "title": "Improving neural machine translation models with monolingual data",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rico Sennrich, Barry Haddow, and Alexandra Birch."
    },
    {
      "index": 219,
      "title": "Horovod: fast and easy distributed deep learning in TensorFlow",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander Sergeev and Mike Del Balso.",
      "orig_title": "Horovod: fast and easy distributed deep learning in TensorFlow",
      "paper_id": "1802.05799v3"
    },
    {
      "index": 220,
      "title": "GLU Variants Improve Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Noam Shazeer.",
      "orig_title": "Glu variants improve transformer",
      "paper_id": "2002.05202v1"
    },
    {
      "index": 221,
      "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro.",
      "orig_title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "paper_id": "1909.08053v4"
    },
    {
      "index": 222,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:",
      "authors": "Karen Simonyan and Andrew Zisserman.",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 223,
      "title": "Feature wise normalization: An effective way of normalizing data",
      "abstract": "",
      "year": "2022",
      "venue": "Pattern Recognition, 122:",
      "authors": "Dalwinder Singh and Birmohan Singh."
    },
    {
      "index": 224,
      "title": "Cyclical learning rates for training neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE winter conference on applications of computer\nvision (WACV)",
      "authors": "Leslie N Smith."
    },
    {
      "index": 225,
      "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates",
      "abstract": "",
      "year": "2019",
      "venue": "Artificial intelligence and machine learning for\nmulti-domain operations applications, volume 11006, pages 369–386. SPIE",
      "authors": "Leslie N Smith and Nicholay Topin.",
      "orig_title": "Super-convergence: Very fast training of neural networks using large learning rates",
      "paper_id": "1708.07120v3"
    },
    {
      "index": 226,
      "title": "Gradual dropin of layers to train very deep neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition",
      "authors": "Leslie N Smith, Emily M Hand, and Timothy Doster."
    },
    {
      "index": 227,
      "title": "Don’t Decay the Learning Rate, Increase the Batch Size",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le.",
      "orig_title": "Don’t decay the learning rate, increase the batch size",
      "paper_id": "1711.00489v2"
    },
    {
      "index": 228,
      "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam\nRajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,\nVijay Korthikanti, et al."
    },
    {
      "index": 229,
      "title": "DNN Training Acceleration via Exploring GPGPU Friendly Sparsity",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhuoran Song, Yihong Xu, Han Li, Naifeng Jing, Xiaoyao Liang, and Li Jiang.",
      "orig_title": "Dnn training acceleration via exploring gpgpu friendly sparsity",
      "paper_id": "2203.05705v1"
    },
    {
      "index": 230,
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob\nUszkoreit, and Lucas Beyer.",
      "orig_title": "How to train your vit? data, augmentation, and regularization in vision transformers",
      "paper_id": "2106.10270v2"
    },
    {
      "index": 231,
      "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot.",
      "orig_title": "A monolingual approach to contextualized word embeddings for mid-resource languages",
      "paper_id": "2006.06202v2"
    },
    {
      "index": 232,
      "title": "Prioritizing Samples in Reinforcement Learning with Reducible Loss",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shivakanth Sujit, Somjit Nath, Pedro HM Braga, and Samira Ebrahimi Kahou.",
      "orig_title": "Prioritizing samples in reinforcement learning with reducible loss",
      "paper_id": "2208.10483v3"
    },
    {
      "index": 233,
      "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer\nvision",
      "authors": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.",
      "orig_title": "Revisiting unreasonable effectiveness of data in deep learning era",
      "paper_id": "1707.02968v2"
    },
    {
      "index": 234,
      "title": "ERNIE: Enhanced Representation through Knowledge Integration",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian,\nDanxiang Zhu, Hao Tian, and Hua Wu.",
      "orig_title": "Ernie: Enhanced representation through knowledge integration",
      "paper_id": "1904.09223v1"
    },
    {
      "index": 235,
      "title": "Random Walk Initialization for Training Very Deep Feedforward Networks",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:",
      "authors": "David Sussillo and LF Abbott.",
      "orig_title": "Random walk initialization for training very deep feedforward networks",
      "paper_id": "1412.6558v3"
    },
    {
      "index": 236,
      "title": "On the importance of initialization and momentum in deep learning",
      "abstract": "",
      "year": "2013",
      "venue": "International conference on machine learning",
      "authors": "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton."
    },
    {
      "index": 237,
      "title": "Profiling and Improving the PyTorch Dataloader for high-latency Storage * A Technical Report",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ivan Svogor, Christian Eichenberger, Markus Spanring, Moritz Neun, and Michael\nKopp.",
      "orig_title": "Profiling and improving the pytorch dataloader for high-latency storage: A technical report",
      "paper_id": "2211.04908v2"
    },
    {
      "index": 238,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 239,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\nWojna."
    },
    {
      "index": 240,
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial\nintelligence, volume 31",
      "authors": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.",
      "orig_title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "paper_id": "1602.07261v2"
    },
    {
      "index": 241,
      "title": "Convolutional Neural Networks With Low-rank Regularization",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al.",
      "orig_title": "Convolutional neural networks with low-rank regularization",
      "paper_id": "1511.06067v3"
    },
    {
      "index": 242,
      "title": "Adversarial attack and defense strategies of speaker recognition systems: A survey",
      "abstract": "",
      "year": "2022",
      "venue": "Electronics, 11(14):",
      "authors": "Hao Tan, Le Wang, Huan Zhang, Junjian Zhang, Muhammad Shafiq, and Zhaoquan Gu."
    },
    {
      "index": 243,
      "title": "Efficientnetv2: Smaller models and faster training",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Mingxing Tan and Quoc Le."
    },
    {
      "index": 244,
      "title": "1-bit adam: Communication efficient large-scale training with adam’s convergence speed",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li,\nXiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He."
    },
    {
      "index": 245,
      "title": "Efficient Transformers: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Computing Surveys, 55(6):1–28",
      "authors": "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.",
      "orig_title": "Efficient transformers: A survey",
      "paper_id": "2009.06732v3"
    },
    {
      "index": 246,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2021",
      "venue": "https://github.com/mosaicml/composer/",
      "authors": "The Mosaic ML Team.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 247,
      "title": "What’s in my ai? a comprehensive analysis of datasets used to train gpt-1, gpt-2, gpt-3, gpt-neox-20b, megatron-11b, mt-nlg, and gopher, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "URL https://lifearchitect.ai/whats-in-my-ai/",
      "authors": "Alan D. Thompson."
    },
    {
      "index": 248,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2012",
      "venue": "COURSERA: Neural networks for machine learning, 4(2):26–31",
      "authors": "Tijmen Tieleman, Geoffrey Hinton, et al.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 249,
      "title": "MLP-Mixer: An all-MLP Architecture for Vision",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in neural information processing systems,\n34:",
      "authors": "Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua\nZhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,\nJakob Uszkoreit, et al.",
      "orig_title": "Mlp-mixer: An all-mlp architecture for vision",
      "paper_id": "2105.01601v4"
    },
    {
      "index": 250,
      "title": "Fixing the train-test resolution discrepancy",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems, 32",
      "authors": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou.",
      "orig_title": "Fixing the train-test resolution discrepancy",
      "paper_id": "1906.06423v4"
    },
    {
      "index": 251,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "International conference on machine learning",
      "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and Hervé Jégou.",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 252,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, et al.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 253,
      "title": "Efficient Methods for Natural Language Processing: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Marcos Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel R\nCiosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Pedro H Martins,\net al.",
      "orig_title": "Efficient methods for natural language processing: a survey",
      "paper_id": "2209.00099v2"
    },
    {
      "index": 254,
      "title": "Saliencymix: A saliency guided data augmentation strategy for better regularization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "AFM Uddin, Mst Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae, et al."
    },
    {
      "index": 255,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems, 30",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 256,
      "title": "Manifold Mixup: Better Representations by Interpolating Hidden States",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,\nDavid Lopez-Paz, and Yoshua Bengio.",
      "orig_title": "Manifold mixup: Better representations by interpolating hidden states",
      "paper_id": "1806.05236v7"
    },
    {
      "index": 257,
      "title": "Anderson acceleration for fixed-point iterations",
      "abstract": "",
      "year": "2011",
      "venue": "SIAM Journal on Numerical Analysis, 49(4):",
      "authors": "Homer F Walker and Peng Ni."
    },
    {
      "index": 258,
      "title": "Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX",
      "abstract": "",
      "year": "2021",
      "venue": "https://github.com/kingoflolz/mesh-transformer-jax, May",
      "authors": "Ben Wang."
    },
    {
      "index": 259,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "arXiv e-prints",
      "authors": "Boxiang Wang, Qifan Xu, Zhengda Bian, and Yang You.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 260,
      "title": "Deep growing learning",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer\nVision",
      "authors": "Guangcong Wang, Xiaohua Xie, Jianhuang Lai, and Jiaxuan Zhuo."
    },
    {
      "index": 261,
      "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.",
      "orig_title": "Haq: Hardware-aware automated quantization with mixed precision",
      "paper_id": "1811.08886v3"
    },
    {
      "index": 262,
      "title": "Bfloat16: The secret to high performance on cloud tpus — google cloud blog, 2020",
      "abstract": "",
      "year": "2020",
      "venue": "URL\nhttps://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus",
      "authors": "Shibo Wang and Pankaj Kanwar."
    },
    {
      "index": 263,
      "title": "Linformer: Self-Attention with Linear Complexity",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.",
      "orig_title": "Linformer: Self-attention with linear complexity",
      "paper_id": "2006.04768v3"
    },
    {
      "index": 264,
      "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti\nAggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al.",
      "orig_title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "paper_id": "2208.10442v2"
    },
    {
      "index": 265,
      "title": "That’s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in\nnatural language processing",
      "authors": "William Yang Wang and Diyi Yang."
    },
    {
      "index": 266,
      "title": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaohui Wang, Ying Xiong, Xian Qian, Yang Wei, Lei Li, and Mingxuan Wang.",
      "orig_title": "Lightseq2: Accelerated training for transformer-based models on gpus",
      "paper_id": "2110.05722v3"
    },
    {
      "index": 267,
      "title": "LightSeq: A High Performance Inference Library for Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies: Industry Papers (NAACL-HLT), pages 113–120. Association for\nComputational Linguistics, June",
      "authors": "Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei Li.",
      "orig_title": "LightSeq: A high performance inference library for transformers",
      "paper_id": "2010.13887v4"
    },
    {
      "index": 268,
      "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and Gao\nHuang.",
      "orig_title": "Efficienttrain: Exploring generalized curriculum learning for training visual backbones",
      "paper_id": "2211.09703v3"
    },
    {
      "index": 269,
      "title": "One-dimensional deep low-rank and sparse network for accelerated mri",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Medical Imaging, 42(1):79–90",
      "authors": "Zi Wang, Chen Qian, Di Guo, Hongwei Sun, Rushuai Li, Bo Zhao, and Xiaobo Qu."
    },
    {
      "index": 270,
      "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jason Wei and Kai Zou.",
      "orig_title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "paper_id": "1901.11196v2"
    },
    {
      "index": 271,
      "title": "Emergent Abilities of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.",
      "orig_title": "Emergent abilities of large language models",
      "paper_id": "2206.07682v2"
    },
    {
      "index": 272,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "lilianweng.github.io, Jan",
      "authors": "Lilian Weng.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 273,
      "title": "Fastformer: Additive Attention Can Be All You Need",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie.",
      "orig_title": "Fastformer: Additive attention can be all you need",
      "paper_id": "2108.09084v6"
    },
    {
      "index": 274,
      "title": "When Do Curricula Work?",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.",
      "orig_title": "When do curricula work?",
      "paper_id": "2012.03107v3"
    },
    {
      "index": 275,
      "title": "Cross-domain collaborative normalization via structural knowledge",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Haifeng Xia and Zhengming Ding."
    },
    {
      "index": 276,
      "title": "Fast deep learning training through intelligently freezing layers",
      "abstract": "",
      "year": "2019",
      "venue": "2019 International Conference on Internet of Things\n(iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE\nCyber, Physical and Social Computing (CPSCom) and IEEE Smart Data\n(SmartData)",
      "authors": "Xueli Xiao, Thosini Bamunu Mudiyanselage, Chunyan Ji, Jie Hu, and Yi Pan."
    },
    {
      "index": 277,
      "title": "A deep learning dataloader with shared data preparation",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jian Xie, Jingwei Xu, Guochang Wang, Yuan Yao, Zenan Li, Chun Cao, and Hanghang\nTong."
    },
    {
      "index": 278,
      "title": "Data Selection for Language Models via Importance Resampling",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.",
      "orig_title": "Data selection for language models via importance resampling",
      "paper_id": "2302.03169v3"
    },
    {
      "index": 279,
      "title": "Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models, 2022b",
      "abstract": "",
      "year": "",
      "venue": "URL https://arxiv.org/abs/",
      "authors": "Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan."
    },
    {
      "index": 280,
      "title": "Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35",
      "authors": "Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung,\nYin Li, and Vikas Singh.",
      "orig_title": "Nyströmformer: A nyström-based algorithm for approximating self-attention",
      "paper_id": "2102.03902v3"
    },
    {
      "index": 281,
      "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Qifan Xu, Shenggui Li, Chaoyu Gong, and Yang You.",
      "orig_title": "An efficient 2d method for training super-large deep learning models",
      "paper_id": "2104.05343v1"
    },
    {
      "index": 282,
      "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao.",
      "orig_title": "Vitae: Vision transformer advanced by exploring intrinsic inductive bias",
      "paper_id": "2106.03348v4"
    },
    {
      "index": 283,
      "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\nSiddhant, Aditya Barua, and Colin Raffel.",
      "orig_title": "mt5: A massively multilingual pre-trained text-to-text transformer",
      "paper_id": "2010.11934v3"
    },
    {
      "index": 284,
      "title": "Large-batch Optimization for Dense Visual Predictions",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zeyue Xue, Jianming Liang, Guanglu Song, Zhuofan Zong, Liang Chen, Yu Liu, and\nPing Luo.",
      "orig_title": "Large-batch optimization for dense visual predictions",
      "paper_id": "2210.11078v1"
    },
    {
      "index": 285,
      "title": "Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, and\nYuxiong He.",
      "orig_title": "Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers",
      "paper_id": "2211.11586v1"
    },
    {
      "index": 286,
      "title": "Large Batch Training of Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yang You, Igor Gitman, and Boris Ginsburg.",
      "orig_title": "Large batch training of convolutional networks",
      "paper_id": "1708.03888v3"
    },
    {
      "index": 287,
      "title": "ImageNet Training in Minutes",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 47th International Conference on Parallel\nProcessing",
      "authors": "Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.",
      "orig_title": "Imagenet training in minutes",
      "paper_id": "1709.05011v10"
    },
    {
      "index": 288,
      "title": "Decentralized Training of Foundation Models in Heterogeneous Environments",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Binhang Yuan, Yongjun He, Jared Quincy Davis, Tianyi Zhang, Tri Dao, Beidi\nChen, Percy Liang, Christopher Re, and Ce Zhang.",
      "orig_title": "Decentralized training of foundation models in heterogeneous environments",
      "paper_id": "2206.01288v4"
    },
    {
      "index": 289,
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on\ncomputer vision",
      "authors": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo."
    },
    {
      "index": 290,
      "title": "Adadelta: an adaptive learning rate method",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:",
      "authors": "Matthew D Zeiler."
    },
    {
      "index": 291,
      "title": "Scaling Vision Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.",
      "orig_title": "Scaling vision transformers",
      "paper_id": "2106.04560v2"
    },
    {
      "index": 292,
      "title": "Root Mean Square Layer Normalization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems, 32",
      "authors": "Biao Zhang and Rico Sennrich.",
      "orig_title": "Root mean square layer normalization",
      "paper_id": "1910.07467v1"
    },
    {
      "index": 293,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 294,
      "title": "Residual learning without normalization via better initialization",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations,\nvolume 3",
      "authors": "Hongyi Zhang, Yann N Dauphin, and Tengyu Ma."
    },
    {
      "index": 295,
      "title": "AutoAssist: A Framework to Accelerate Training of Deep Neural Networks",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems, 32",
      "authors": "Jiong Zhang, Hsiang-Fu Yu, and Inderjit S Dhillon.",
      "orig_title": "Autoassist: A framework to accelerate training of deep neural networks",
      "paper_id": "1905.03381v1"
    },
    {
      "index": 296,
      "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems,\n33:",
      "authors": "Minjia Zhang and Yuxiong He.",
      "orig_title": "Accelerating training of transformer-based language models with progressive layer dropping",
      "paper_id": "2010.13369v1"
    },
    {
      "index": 297,
      "title": "Character-level convolutional networks for text classification",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems, 28",
      "authors": "Xiang Zhang, Junbo Zhao, and Yann LeCun."
    },
    {
      "index": 298,
      "title": "Adversarial AutoAugment",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong.",
      "orig_title": "Adversarial autoaugment",
      "paper_id": "1912.11188v1"
    },
    {
      "index": 299,
      "title": "Mixhead: Breaking the low-rank bottleneck in multi-head attention language models",
      "abstract": "",
      "year": "2022",
      "venue": "Knowledge-Based Systems, 240:",
      "authors": "Zhong Zhang, Nian Shao, Chongming Gao, Rui Miao, Qinli Yang, and Junming Shao."
    },
    {
      "index": 300,
      "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jiawei Zhao, Florian Schäfer, and Anima Anandkumar.",
      "orig_title": "Zero initialization: Initializing neural networks with only zeros and ones",
      "paper_id": "2110.12661v3"
    },
    {
      "index": 301,
      "title": "Differentiable Augmentation for Data-Efficient GAN Training",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems,\n33:",
      "authors": "Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han.",
      "orig_title": "Differentiable augmentation for data-efficient gan training",
      "paper_id": "2006.10738v4"
    },
    {
      "index": 302,
      "title": "A faster algorithm for reducing the computational complexity of convolutional neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "Algorithms, 11(10):159",
      "authors": "Yulin Zhao, Donghui Wang, Leiou Wang, and Peng Liu."
    },
    {
      "index": 303,
      "title": "Random Erasing Data Augmentation",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial\nintelligence, volume 34",
      "authors": "Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.",
      "orig_title": "Random erasing data augmentation",
      "paper_id": "1708.04896v2"
    },
    {
      "index": 304,
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng\nJi, Qiben Yan, Lifang He, et al."
    },
    {
      "index": 305,
      "title": "GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein.",
      "orig_title": "Gradinit: Learning to initialize neural networks for stable and efficient training",
      "paper_id": "2102.08098v3"
    },
    {
      "index": 306,
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "abstract": "",
      "year": "2015",
      "venue": "The IEEE International Conference on Computer Vision\n(ICCV), December",
      "authors": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\nAntonio Torralba, and Sanja Fidler."
    },
    {
      "index": 307,
      "title": "A Survey on Efficient Training of Transformers",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen.",
      "orig_title": "A survey on efficient training of transformers",
      "paper_id": "2302.01107v3"
    }
  ]
}