{
  "paper_id": "2206.06661v2",
  "title": "SoTeacher: A Student-oriented Teacher Network Training Framework for Knowledge Distillation",
  "abstract": "Abstract\nHow to train an ideal teacher for knowledge distillation is still an open problem.\nIt has been widely observed that a teacher minimizing the empirical risk not necessarily yields the best performing student, suggesting a fundamental discrepancy between the common practice in teacher network training and the distillation objective.\nTo fill this gap, we propose a novel student-oriented teacher network training framework SoTeacher, inspired by recent findings that student performance hinges on teacher’s capability to approximate the true label distribution of training samples.\nWe theoretically established that (1) the empirical risk minimizer with proper scoring rules as loss function can provably approximate the true label distribution of training data if the hypothesis function is locally Lipschitz continuous around training samples; and (2) when data augmentation is employed for training, an additional constraint is required that the minimizer has to produce consistent predictions across augmented views of the same training input.\nIn light of our theory, SoTeacher renovates the empirical risk minimization by incorporating Lipschitz regularization and consistency regularization.\nIt is worth mentioning that SoTeacher is applicable to almost all teacher-student architecture pairs, requires no prior knowledge of the student upon teacher’s training, and induces almost no computation overhead.\nExperiments on two benchmark datasets confirm that SoTeacher can improve student performance significantly and consistently across various knowledge distillation algorithms and teacher-student pairs.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Tiny imagenet",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Variational Information Distillation for Knowledge Transfer",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Ahn, S., Hu, S. X., Damianou, A. C., Lawrence, N. D., and Dai, Z.",
      "orig_title": "Variational information distillation for knowledge transfer",
      "paper_id": "1904.05835v1"
    },
    {
      "index": 2,
      "title": "Spectrally-normalized margin bounds for neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Bartlett, P. L., Foster, D. J., and Telgarsky, M.",
      "orig_title": "Spectrally-normalized margin bounds for neural networks",
      "paper_id": "1706.08498v2"
    },
    {
      "index": 3,
      "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Berthelot, D., Carlini, N., Cubuk, E. D., Kurakin, A., Sohn, K., Zhang, H., and Raffel, C.",
      "orig_title": "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
      "paper_id": "1911.09785v2"
    },
    {
      "index": 4,
      "title": "Verification of forecasts expressed in terms of probability",
      "abstract": "",
      "year": "1950",
      "venue": "Monthly Weather Review",
      "authors": "Brier, G. W."
    },
    {
      "index": 5,
      "title": "Model compression",
      "abstract": "",
      "year": "2006",
      "venue": "12th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "authors": "Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A."
    },
    {
      "index": 6,
      "title": "On the Efficacy of Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Cho, J. H. and Hariharan, B.",
      "orig_title": "On the efficacy of knowledge distillation",
      "paper_id": "1910.01348v1"
    },
    {
      "index": 7,
      "title": "Knowledge Distillation as Semiparametric Inference",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Dao, T., Kamath, G. M., Syrgkanis, V., and Mackey, L. W.",
      "orig_title": "Knowledge distillation as semiparametric inference",
      "paper_id": "2104.09732v1"
    },
    {
      "index": 8,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "CVPR",
      "authors": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L."
    },
    {
      "index": 9,
      "title": "Double descent in adversarial training: An implicit label noise perspective",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Dong, C., Liu, L., and Shang, J."
    },
    {
      "index": 10,
      "title": "Born-Again Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L., and Anandkumar, A.",
      "orig_title": "Born again neural networks",
      "paper_id": "1805.04770v2"
    },
    {
      "index": 11,
      "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Geifman, Y., Uziel, G., and El-Yaniv, R.",
      "orig_title": "Bias-reduced uncertainty estimation for deep neural classifiers",
      "paper_id": "1805.08206v4"
    },
    {
      "index": 12,
      "title": "Strictly proper scoring rules, prediction, and estimation",
      "abstract": "",
      "year": "2007",
      "venue": "Journal of the American Statistical Association",
      "authors": "Gneiting, T. and Raftery, A. E."
    },
    {
      "index": 13,
      "title": "Regularisation of Neural Networks by Enforcing Lipschitz Continuity",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning",
      "authors": "Gouk, H., Frank, E., Pfahringer, B., and Cree, M. J.",
      "orig_title": "Regularisation of neural networks by enforcing lipschitz continuity",
      "paper_id": "1804.04368v3"
    },
    {
      "index": 14,
      "title": "On Calibration of Modern Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q.",
      "orig_title": "On calibration of modern neural networks",
      "paper_id": "1706.04599v2"
    },
    {
      "index": 15,
      "title": "Asgn: An active semi-supervised graph neural network for molecular property prediction",
      "abstract": "",
      "year": "2020",
      "venue": "26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "Hao, Z., Lu, C., Hu, Z., Wang, H., Huang, Z., Liu, Q., Chen, E., and Lee, C."
    },
    {
      "index": 16,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "He, K., Zhang, X., Ren, S., and Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 17,
      "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B.",
      "orig_title": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "paper_id": "1912.02781v2"
    },
    {
      "index": 18,
      "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Heo, B., Lee, M., Yun, S., and Choi, J. Y.",
      "orig_title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
      "paper_id": "1811.03233v2"
    },
    {
      "index": 19,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "ArXiv",
      "authors": "Hinton, G. E., Vinyals, O., and Dean, J.",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 20,
      "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Huang, Z. and Wang, N.",
      "orig_title": "Like what you like: Knowledge distill via neuron selectivity transfer",
      "paper_id": "1707.01219v2"
    },
    {
      "index": 21,
      "title": "Knowledge Distillation via Route Constrained Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J., and Hu, X.",
      "orig_title": "Knowledge distillation via route constrained optimization",
      "paper_id": "1904.09149v1"
    },
    {
      "index": 22,
      "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Kim, J., Park, S., and Kwak, N.",
      "orig_title": "Paraphrasing complex network: Network compression via factor transfer",
      "paper_id": "1802.04977v3"
    },
    {
      "index": 23,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Krizhevsky, A."
    },
    {
      "index": 24,
      "title": "Temporal Ensembling for Semi-Supervised Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Laine, S. and Aila, T.",
      "orig_title": "Temporal ensembling for semi-supervised learning",
      "paper_id": "1610.02242v3"
    },
    {
      "index": 25,
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Lakshminarayanan, B., Pritzel, A., and Blundell, C.",
      "orig_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "paper_id": "1612.01474v3"
    },
    {
      "index": 26,
      "title": "Cross-lingual machine reading comprehension with language branch knowledge distillation",
      "abstract": "",
      "year": "2020",
      "venue": "COLING",
      "authors": "Liu, J., Shou, L., Pei, J., Gong, M., Yang, M., and Jiang, D."
    },
    {
      "index": 27,
      "title": "torchdistill: A modular, configuration-driven framework for knowledge distillation",
      "abstract": "",
      "year": "2021",
      "venue": "International Workshop on Reproducible Research in Pattern Recognition",
      "authors": "Matsubara, Y."
    },
    {
      "index": 28,
      "title": "A statistical perspective on distillation",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Menon, A. K., Rawat, A. S., Reddi, S. J., Kim, S., and Kumar, S."
    },
    {
      "index": 29,
      "title": "Spectral Normalization for Generative Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.",
      "orig_title": "Spectral normalization for generative adversarial networks",
      "paper_id": "1802.05957v1"
    },
    {
      "index": 30,
      "title": "Confidence-Aware Learning for Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Moon, J., Kim, J., Shin, Y., and Hwang, S.",
      "orig_title": "Confidence-aware learning for deep neural networks",
      "paper_id": "2007.01458v3"
    },
    {
      "index": 31,
      "title": "When Does Label Smoothing Help?",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Müller, R., Kornblith, S., and Hinton, G. E.",
      "orig_title": "When does label smoothing help?",
      "paper_id": "1906.02629v3"
    },
    {
      "index": 32,
      "title": "Obtaining well calibrated probabilities using bayesian binning",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Naeini, M. P., Cooper, G. F., and Hauskrecht, M."
    },
    {
      "index": 33,
      "title": "Pattern recognition and machine learning",
      "abstract": "",
      "year": "2007",
      "venue": "Technometrics",
      "authors": "Neal, R. M."
    },
    {
      "index": 34,
      "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Park, D. Y., Cha, M., Jeong, C., Kim, D., and Han, B.",
      "orig_title": "Learning student-friendly teacher networks for knowledge distillation",
      "paper_id": "2102.07650v4"
    },
    {
      "index": 35,
      "title": "Relational Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Park, W., Kim, D., Lu, Y., and Cho, M.",
      "orig_title": "Relational knowledge distillation",
      "paper_id": "1904.05068v2"
    },
    {
      "index": 36,
      "title": "Learning Deep Representations with Probabilistic Knowledge Transfer",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Passalis, N. and Tefas, A.",
      "orig_title": "Learning deep representations with probabilistic knowledge transfer",
      "paper_id": "1803.10837v3"
    },
    {
      "index": 37,
      "title": "Correlation Congruence for Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Peng, B., Jin, X., Liu, J., Zhou, S., Wu, Y., Liu, Y., Li, D., and Zhang, Z.",
      "orig_title": "Correlation congruence for knowledge distillation",
      "paper_id": "1904.01802v1"
    },
    {
      "index": 38,
      "title": "Concentration inequalities for multinoulli random variables",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Qian, J., Fruit, R., Pirotta, M., and Lazaric, A."
    },
    {
      "index": 39,
      "title": "FitNets: Hints for Thin Deep Nets",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y.",
      "orig_title": "Fitnets: Hints for thin deep nets",
      "paper_id": "1412.6550v4"
    },
    {
      "index": 40,
      "title": "KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Shah, H., Khare, A., Shah, N., and Siddiqui, K.",
      "orig_title": "Kd-lib: A pytorch library for knowledge distillation, pruning and quantization",
      "paper_id": "2011.14691v1"
    },
    {
      "index": 41,
      "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C."
    },
    {
      "index": 42,
      "title": "Robust large margin deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R. D."
    },
    {
      "index": 43,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z."
    },
    {
      "index": 44,
      "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Tan, M., Chen, B., Pang, R., Vasudevan, V., and Le, Q. V.",
      "orig_title": "Mnasnet: Platform-aware neural architecture search for mobile",
      "paper_id": "1807.11626v3"
    },
    {
      "index": 45,
      "title": "Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System",
      "abstract": "",
      "year": "2018",
      "venue": "24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "Tang, J. and Wang, K.",
      "orig_title": "Ranking distillation: Learning compact ranking models with high performance for recommender system",
      "paper_id": "1809.07428v1"
    },
    {
      "index": 46,
      "title": "On mixup training: Improved calibration and predictive uncertainty for deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Thulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya, T., and Michalak, S. E."
    },
    {
      "index": 47,
      "title": "Contrastive Representation Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Tian, Y., Krishnan, D., and Isola, P.",
      "orig_title": "Contrastive representation distillation",
      "paper_id": "1910.10699v3"
    },
    {
      "index": 48,
      "title": "Similarity-Preserving Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Tung, F. and Mori, G.",
      "orig_title": "Similarity-preserving knowledge distillation",
      "paper_id": "1907.09682v2"
    },
    {
      "index": 49,
      "title": "Learning to extract attribute value from product via question answering: A multi-task approach",
      "abstract": "",
      "year": "2020",
      "venue": "26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "Wang, Q., Yang, L., Kanagal, B., Sanghai, S. K., Sivakumar, D., Shu, B., Yu, Z., and Elsas, J. L."
    },
    {
      "index": 50,
      "title": "Inequalities for the l1 deviation of the empirical distribution",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Weissman, T., Ordentlich, E., Seroussi, G., Verdú, S., and Weinberger, M. J."
    },
    {
      "index": 51,
      "title": "Unsupervised data augmentation for consistency training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv: Learning",
      "authors": "Xie, Q., Dai, Z., Hovy, E. H., Luong, M.-T., and Le, Q. V."
    },
    {
      "index": 52,
      "title": "Knowledge Distillation Meets Self-Supervision",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Xu, G., Liu, Z., Li, X., and Loy, C. C.",
      "orig_title": "Knowledge distillation meets self-supervision",
      "paper_id": "2006.07114v2"
    },
    {
      "index": 53,
      "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Yang, C., Xie, L., Qiao, S., and Yuille, A. L.",
      "orig_title": "Training deep neural networks in generations: A more tolerant teacher educates better students",
      "paper_id": "1805.05551v2"
    },
    {
      "index": 54,
      "title": "Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System",
      "abstract": "",
      "year": "2020",
      "venue": "13th International Conference on Web Search and Data Mining",
      "authors": "Yang, Z., Shou, L., Gong, M., Lin, W., and Jiang, D.",
      "orig_title": "Model compression with two-stage multi-teacher knowledge distillation for web question answering system",
      "paper_id": "1910.08381v1"
    },
    {
      "index": 55,
      "title": "Spectral Norm Regularization for Improving the Generalizability of Deep Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Yoshida, Y. and Miyato, T.",
      "orig_title": "Spectral norm regularization for improving the generalizability of deep learning",
      "paper_id": "1705.10941v1"
    },
    {
      "index": 56,
      "title": "Wide Residual Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ArXiv",
      "authors": "Zagoruyko, S. and Komodakis, N.",
      "orig_title": "Wide residual networks",
      "paper_id": "1605.07146v4"
    },
    {
      "index": 57,
      "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Zagoruyko, S. and Komodakis, N.",
      "orig_title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "paper_id": "1612.03928v3"
    },
    {
      "index": 58,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2018a",
      "venue": "ArXiv",
      "authors": "Zhang, H., Cissé, M., Dauphin, Y., and Lopez-Paz, D.",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 59,
      "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
      "abstract": "",
      "year": "2018b",
      "venue": "CVPR",
      "authors": "Zhang, X., Zhou, X., Lin, M., and Sun, J.",
      "orig_title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "paper_id": "1707.01083v2"
    },
    {
      "index": 60,
      "title": "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation",
      "abstract": "",
      "year": "2020",
      "venue": "13th International Conference on Web Search and Data Mining",
      "authors": "Zhang, Y., Xu, X., Zhou, H., and Zhang, Y.",
      "orig_title": "Distilling structured knowledge into embeddings for explainable and accurate recommendation",
      "paper_id": "1912.08422v1"
    }
  ]
}