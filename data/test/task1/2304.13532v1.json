{
  "paper_id": "2304.13532v1",
  "title": "SCV-GNN: Sparse Compressed Vector-based Graph Neural Network Aggregation",
  "abstract": "Abstract\nGraph neural networks (GNNs) have emerged as a powerful tool to process graph-based data in fields like communication networks, molecular interactions, chemistry, social networks, and neuroscience. GNNs are characterized by the ultra-sparse nature of their adjacency matrix that necessitates the development of dedicated hardware beyond general-purpose sparse matrix multipliers. While there has been extensive research on designing dedicated hardware accelerators for GNNs, few have extensively explored the impact of the sparse storage format on the efficiency of the GNN accelerators. This paper proposes SCV-GNN with the novel sparse compressed vectors (SCV) format optimized for the aggregation operation. We use Z-Morton ordering to derive a data-locality-based computation ordering and partitioning scheme. The paper also presents how the proposed SCV-GNN is scalable on a vector processing system. Experimental results over various datasets show that the proposed method achieves a geometric mean speedup of 7.96×7.96\\times and 7.04×7.04\\times over CSC and CSR aggregation operations, respectively. The proposed method also reduces the memory traffic by a factor of 3.29×3.29\\times and 4.37×4.37\\times over compressed sparse column (CSC) and compressed sparse row (CSR), respectively. Thus, the proposed novel aggregation format reduces the latency and memory access for GNN inference.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Brain-inspired computing: Models and architectures",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Open Journal of Circuits and Systems",
      "authors": "K. K. Parhi and N. K. Unnikrishnan"
    },
    {
      "index": 1,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 2,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 3,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "K. He et al.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 4,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "C. Szegedy et al.",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 5,
      "title": "Audio-visual speech recognition using deep learning",
      "abstract": "",
      "year": "2015",
      "venue": "Applied Intelligence",
      "authors": "K. Noda et al."
    },
    {
      "index": 6,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "J. Devlin et al.",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 7,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu et al.",
      "orig_title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 8,
      "title": "Deep learning for image-based cancer detection and diagnosis - A survey",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "Z. Hu et al."
    },
    {
      "index": 9,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "T. N. Kipf and M. Welling",
      "orig_title": "Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 10,
      "title": "Graph Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "P. Veličković et al.",
      "orig_title": "Graph attention networks",
      "paper_id": "1710.10903v3"
    },
    {
      "index": 11,
      "title": "How Powerful are Graph Neural Networks?",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "K. Xu et al.",
      "orig_title": "How powerful are graph neural networks?",
      "paper_id": "1810.00826v3"
    },
    {
      "index": 12,
      "title": "Inductive Representation Learning on Large Graphs",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "W. Hamilton, Z. Ying, and J. Leskovec",
      "orig_title": "Inductive representation learning on large graphs",
      "paper_id": "1706.02216v4"
    },
    {
      "index": 13,
      "title": "Computing Graph Neural Networks: A Survey from Algorithms to Accelerators",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Computing Survey",
      "authors": "S. Abadal et al.",
      "orig_title": "Computing graph neural networks: A survey from algorithms to accelerators",
      "paper_id": "2010.00130v3"
    },
    {
      "index": 14,
      "title": "RouteNet: Leveraging Graph Neural Networks for network modeling and optimization in SDN",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Journal on Selected Areas in Communications",
      "authors": "K. Rusek et al.",
      "orig_title": "RouteNet: Leveraging graph neural networks for network modeling and optimization in sdn",
      "paper_id": "1910.01508v2"
    },
    {
      "index": 15,
      "title": "Neural Message Passing for Quantum Chemistry",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "J. Gilmer et al.",
      "orig_title": "Neural message passing for quantum chemistry",
      "paper_id": "1704.01212v2"
    },
    {
      "index": 16,
      "title": "Fake news detection on social media using geometric deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.06673",
      "authors": "F. Monti et al."
    },
    {
      "index": 17,
      "title": "Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis",
      "abstract": "",
      "year": "2020",
      "venue": "Medical Image Computing and Computer Assisted Intervention",
      "authors": "X. Li et al.",
      "orig_title": "Pooling regularized graph neural network for FMRI biomarker analysis",
      "paper_id": "2007.14589v1"
    },
    {
      "index": 18,
      "title": "NVIDIA A100 Tensor Core GPU Architecture",
      "abstract": "",
      "year": "2020",
      "venue": "Tech. Rep.",
      "authors": "NVIDIA"
    },
    {
      "index": 19,
      "title": "In-datacenter performance analysis of a tensor processing unit",
      "abstract": "",
      "year": "2017",
      "venue": "ACM/IEEE International Symposium on Computer Architecture (ISCA)",
      "authors": "N. P. Jouppi et al."
    },
    {
      "index": 20,
      "title": "Ten lessons from three generations shaped Google’s TPUv4i : Industrial product",
      "abstract": "",
      "year": "2021",
      "venue": "ACM/IEEE International Symposium on Computer Architecture (ISCA)",
      "authors": "N. P. Jouppi et al."
    },
    {
      "index": 21,
      "title": "PipeDream: Generalized pipeline parallelism for DNN training",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Symposium on Operating Systems Principles",
      "authors": "D. Narayanan et al."
    },
    {
      "index": 22,
      "title": "GPipe: Efficient training of giant neural networks using pipeline parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Huang et al."
    },
    {
      "index": 23,
      "title": "LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and Inter-Layer Gradient Pipelining and Multiprocessor Scheduling",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD)",
      "authors": "N. K. Unnikrishnan and K. K. Parhi",
      "orig_title": "LayerPipe: Accelerating deep neural network training by intra-layer and inter-layer gradient pipelining and multiprocessor scheduling",
      "paper_id": "2108.06629v1"
    },
    {
      "index": 24,
      "title": "PermDNN: Efficient compressed DNN architecture with permuted diagonal matrices",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE/ACM International Symposium on Microarchitecture (MICRO)",
      "authors": "C. Deng et al."
    },
    {
      "index": 25,
      "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": "Y. Chen et al.",
      "orig_title": "Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices",
      "paper_id": "1807.07928v2"
    },
    {
      "index": 26,
      "title": "EIE: Efficient inference engine on compressed deep neural network",
      "abstract": "",
      "year": "2016",
      "venue": "isca",
      "authors": "S. Han et al."
    },
    {
      "index": 27,
      "title": "A gradient-interleaved scheduler for energy-efficient backpropagation for training neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Symposium on Circuits and Systems (ISCAS)",
      "authors": "N. Unnikrishnan and K. K. Parhi"
    },
    {
      "index": 28,
      "title": "SIGMA: A sparse and irregular GEMM accelerator with flexible interconnects for DNN training",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Symposium on High Performance Computer Architecture (HPCA)",
      "authors": "E. Qin et al."
    },
    {
      "index": 29,
      "title": "FPGA-based training accelerator utilizing sparseness of convolutional neural network",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Field Programmable Logic and Applications (FPL)",
      "authors": "H. Nakahara et al."
    },
    {
      "index": 30,
      "title": "SPRING: A sparsity-aware reduced-precision monolithic 3D CNN accelerator architecture for training and inference",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": "Y. Yu and N. K. Jha"
    },
    {
      "index": 31,
      "title": "ExTensor: An accelerator for sparse tensor algebra",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/ACM International Symposium on Microarchitecture (MICRO)",
      "authors": "K. Hegde et al."
    },
    {
      "index": 32,
      "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.01315",
      "authors": "M. Wang et al.",
      "orig_title": "Deep graph library: A graph-centric, highly-performant package for graph neural networks",
      "paper_id": "1909.01315v2"
    },
    {
      "index": 33,
      "title": "Fast Graph Representation Learning with PyTorch Geometric",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR Workshop on Representation Learning on Graphs and Manifolds",
      "authors": "M. Fey and J. E. Lenssen",
      "orig_title": "Fast graph representation learning with PyTorch Geometric",
      "paper_id": "1903.02428v3"
    },
    {
      "index": 34,
      "title": "GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.08475",
      "authors": "S. Abi-Karam et al.",
      "orig_title": "GenGNN: A generic FPGA framework for graph neural network acceleration",
      "paper_id": "2201.08475v1"
    },
    {
      "index": 35,
      "title": "GNNerator: A hardware/software framework for accelerating graph neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "ACM/IEEE Design Automation Conference (DAC)",
      "authors": "J. R. Stevens et al."
    },
    {
      "index": 36,
      "title": "Hardware acceleration of graph neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "ACM/IEEE Design Automation Conference (DAC)",
      "authors": "A. Auten, M. Tomei, and R. Kumar"
    },
    {
      "index": 37,
      "title": "VersaGNN: a Versatile accelerator for Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.01280",
      "authors": "F. Shi, A. Y. Jin, and S.-C. Zhu",
      "orig_title": "VersaGNN: a versatile accelerator for graph neural networks",
      "paper_id": "2105.01280v1"
    },
    {
      "index": 38,
      "title": "Rubik: A hierarchical architecture for efficient graph neural network training",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": "X. Chen et al."
    },
    {
      "index": 39,
      "title": "Accelerating graph connected component computation with emerging processing-in-memory architecture",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": "——"
    },
    {
      "index": 40,
      "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE International Symposium on High Performance Computer Architecture (HPCA)",
      "authors": "H. You et al.",
      "orig_title": "GCoD: Graph convolutional network acceleration via dedicated algorithm and accelerator co-design",
      "paper_id": "2112.11594v3"
    },
    {
      "index": 41,
      "title": "GraphLily: Accelerating graph linear algebra on HBM-equipped FPGAs",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": "Y. Hu et al."
    },
    {
      "index": 42,
      "title": "ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:2107.08709",
      "authors": "Z. Zhang et al.",
      "orig_title": "ZIPPER: Exploiting tile- and operator-level parallelism for general and scalable graph neural network acceleration",
      "paper_id": "2107.08709v1"
    },
    {
      "index": 43,
      "title": "HyGCN: A GCN Accelerator with Hybrid Architecture",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Symposium on High Performance Computer Architecture (HPCA)",
      "authors": "M. Yan et al.",
      "orig_title": "HyGCN: A GCN accelerator with hybrid architecture",
      "paper_id": "2001.02514v1"
    },
    {
      "index": 44,
      "title": "Cambricon-G: A polyvalent energy-efficient accelerator for dynamic graph neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": "X. Song et al."
    },
    {
      "index": 45,
      "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching",
      "abstract": "",
      "year": "2022",
      "venue": "ACM/IEEE Design Automation Conference",
      "authors": "S. Mondal et al.",
      "orig_title": "GNNIE: GNN inference engine with load-balancing and graph-specific caching",
      "paper_id": "2105.10554v2"
    },
    {
      "index": 46,
      "title": "AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/ACM International Symposium on Microarchitecture (MICRO)",
      "authors": "T. Geng et al.",
      "orig_title": "AWB-GCN: A graph convolutional network accelerator with runtime workload rebalancing",
      "paper_id": "1908.10834v10"
    },
    {
      "index": 47,
      "title": "Efficient neighbor-sampling-based GNN training on CPU-FPGA heterogeneous platform",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE High Performance Extreme Computing Conference (HPEC)",
      "authors": "B. Zhang et al."
    },
    {
      "index": 48,
      "title": "A computer oriented geodetic data base; and a new technique in file sequencing",
      "abstract": "",
      "year": "1966",
      "venue": "IBM, Tech. Rep.",
      "authors": "G. M. Morton"
    },
    {
      "index": 49,
      "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE",
      "authors": "V. Sze et al.",
      "orig_title": "Efficient processing of deep neural networks: A tutorial and survey",
      "paper_id": "1703.09039v2"
    },
    {
      "index": 50,
      "title": "EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Computers",
      "authors": "S. Liang et al.",
      "orig_title": "EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks",
      "paper_id": "1909.00155v3"
    },
    {
      "index": 51,
      "title": "Efficient gather and scatter operations on graphics processors",
      "abstract": "",
      "year": "2007",
      "venue": "ACM/IEEE Conference on Supercomputing",
      "authors": "B. He et al."
    },
    {
      "index": 52,
      "title": "Revisiting multi-pass scatter and gather on GPUs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Parallel Processing",
      "authors": "Z. Lai, Q. Luo, and X. Jia"
    },
    {
      "index": 53,
      "title": "Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks",
      "abstract": "",
      "year": "2009",
      "venue": "Annual Symposium on Parallelism in Algorithms and Architectures",
      "authors": "A. Buluç et al."
    },
    {
      "index": 54,
      "title": "Designware multi-port memory",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Synopsys"
    },
    {
      "index": 55,
      "title": "Introduction to idt’s FourPort sram",
      "abstract": "",
      "year": "2020",
      "venue": "Renesas, Tech. Rep.",
      "authors": "J. R. Mick"
    },
    {
      "index": 56,
      "title": "Composing multi-ported memories on fpgas",
      "abstract": "",
      "year": "2014",
      "venue": "ACM Transactions on Reconfigurable Technology and Systems",
      "authors": "C. E. Laforest et al."
    },
    {
      "index": 57,
      "title": "Ramulator: A fast and extensible DRAM simulator",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Computer Architecture Letters",
      "authors": "Y. Kim, W. Yang, and O. Mutlu"
    },
    {
      "index": 58,
      "title": "Systolic arrays (for VLSI)",
      "abstract": "",
      "year": "1979",
      "venue": "Sparse Matrix Proceedings",
      "authors": "H. T. Kung and C. E. Leiserson"
    },
    {
      "index": 59,
      "title": "Accelerating matrix multiplication with block sparse format and nvidia tensor cores",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "T. Yamaguchi and F. Busato"
    },
    {
      "index": 60,
      "title": "GCNAX: A flexible and energy-efficient accelerator for graph convolutional neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Symposium on High Performance Computer Architecture (HPCA)",
      "authors": "J. Li et al."
    },
    {
      "index": 61,
      "title": "A survey of accelerator architectures for deep neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Engineering",
      "authors": "Y. Chen et al."
    },
    {
      "index": 62,
      "title": "A template-based design methodology for graph-parallel hardware accelerators",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": "A. Ayupov et al."
    },
    {
      "index": 63,
      "title": "I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/ACM International Symposium on Microarchitecture (MICRO)",
      "authors": "T. Geng et al.",
      "orig_title": "I-GCN: A graph convolutional network accelerator with runtime locality enhancement through islandization",
      "paper_id": "2203.03606v1"
    },
    {
      "index": 64,
      "title": "Rabbit order: Just-in-time parallel reordering for fast graph analysis",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE International Parallel and Distributed Processing Symposium (IPDPS)",
      "authors": "J. Arai et al."
    },
    {
      "index": 65,
      "title": "BoostGCN: A framework for optimizing gcn inference on fpga",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Symposium on Field-Programmable Custom Computing Machines (FCCM)",
      "authors": "B. Zhang, R. Kannan, and V. Prasanna"
    },
    {
      "index": 66,
      "title": "GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.00158",
      "authors": "R. Hwang et al.",
      "orig_title": "GROW: A row-stationary sparse-dense GEMM accelerator for memory-efficient graph convolutional neural networks",
      "paper_id": "2203.00158v4"
    },
    {
      "index": 67,
      "title": "Adaptive sparse tiling for sparse matrix multiplication",
      "abstract": "",
      "year": "2019",
      "venue": "Symposium on Principles and Practice of Parallel Programming",
      "authors": "C. Hong et al."
    },
    {
      "index": 68,
      "title": "Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE International Parallel and Distributed Processing Symposium (IPDPS)",
      "authors": "R. Garg et al.",
      "orig_title": "Understanding the design-space of sparse/dense multiphase GNN dataflows on spatial accelerators",
      "paper_id": "2103.07977v3"
    }
  ]
}