{
  "paper_id": "2402.07640v3",
  "title": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data",
  "abstract": "Abstract\nThe ability to generate sentiment-controlled feedback in response to multimodal inputs—comprising both text and images—addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment. A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability. Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics. It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Affective Feedback Synthesis Towards Multimodal Text and Image Data",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications",
      "authors": "P. Kumar et al.",
      "orig_title": "Affective Feedback Synthesis Towards Multimodal Text and Image Data",
      "paper_id": "2203.12692v2"
    },
    {
      "index": 1,
      "title": "MULTIMODAL EMOTION RECOGNITION WITH HIGH-LEVEL SPEECH AND TEXT FEATURES",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
      "authors": "M. R. Makiuchi, K. Uto et al.",
      "orig_title": "Multimodal Emotion Recognition with High-level Speech and Text Features",
      "paper_id": "2111.10202v1"
    },
    {
      "index": 2,
      "title": "Predicting User Reactions to Twitter Feed Content based on Personality Type and Social Cues",
      "abstract": "",
      "year": "2020",
      "venue": "Future Generation Computer Systems",
      "authors": "F. R. Gallo, G. I. Simari et al."
    },
    {
      "index": 3,
      "title": "Recognizing Induced Emotions of Movie Audiences from Multimodal Information",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "authors": "M. Muszynski et al."
    },
    {
      "index": 4,
      "title": "Multimodal Learning analytics and Education Data Mining: Using Computational Technologies to Measure Complex Learning Tasks",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Learning Analytics",
      "authors": "P. Blikstein and M. Worsley"
    },
    {
      "index": 5,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "A. Vaswani, N. Shazeer et al.",
      "orig_title": "Attention Is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 6,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "S. Ren, K. He et al.",
      "orig_title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 7,
      "title": "From Standard Summarization to New Tasks and Beyond: Summarization with Manifold Information",
      "abstract": "",
      "year": "2020",
      "venue": "The 29th International Joint Conference on Artificial Intelligence (IJCAI)",
      "authors": "S. Gao, X. Chen et al.",
      "orig_title": "From Standard Summarization to New Tasks and Beyond: Summarization With Manifold Information",
      "paper_id": "2005.04684v1"
    },
    {
      "index": 8,
      "title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder",
      "abstract": "",
      "year": "2019",
      "venue": "The International Conference on Learning Representations (ICLR)",
      "authors": "X. Gu, K. Cho et al.",
      "orig_title": "DialogWAE: Multimodal Response Generation With Conditional Wasserstein Auto Encoder",
      "paper_id": "1805.12352v2"
    },
    {
      "index": 9,
      "title": "VQA: Visual Question Answering",
      "abstract": "",
      "year": "2015",
      "venue": "The 19th IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "S. Antol, A. Agrawal et al."
    },
    {
      "index": 10,
      "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "The 21th IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "A. Das, S. Kottur et al.",
      "orig_title": "Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning",
      "paper_id": "1703.06585v2"
    },
    {
      "index": 11,
      "title": "Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "H. Zhou, M. Huang et al.",
      "orig_title": "Emotional Chatting Machine: Emotional Conversation Generation With Internal And External Memory",
      "paper_id": "1704.01074v4"
    },
    {
      "index": 12,
      "title": "Response Generation by Context-aware Prototype Editing",
      "abstract": "",
      "year": "2019",
      "venue": "The 33rd AAAI Conference on Artificial Intelligence (AAAI)",
      "authors": "Y. Wu, F. Wei et al.",
      "orig_title": "Response Generation by Context Aware Prototype Editing",
      "paper_id": "1806.07042v4"
    },
    {
      "index": 13,
      "title": "Improving Language Understanding by Generative Pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "OpenAI",
      "authors": "A. Radford, K. Narasimhan et al."
    },
    {
      "index": 14,
      "title": "Topic-Aware Video Summarization Using Multimodal Transformer",
      "abstract": "",
      "year": "2023",
      "venue": "Pattern Recognition",
      "authors": "Y. Zhu, W. Zhao et al."
    },
    {
      "index": 15,
      "title": "Abstractive Text-Image Summarization Using Multimodal Attention Hierarchical RNN",
      "abstract": "",
      "year": "2018",
      "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "J. Chen and H. Zhuge"
    },
    {
      "index": 16,
      "title": "MSMO: Multimodal Summarization With Multimodal Output",
      "abstract": "",
      "year": "2018",
      "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "J. Zhu, H. Li et al."
    },
    {
      "index": 17,
      "title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles",
      "abstract": "",
      "year": "2020",
      "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "R. Zellers et al.",
      "orig_title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles",
      "paper_id": "2010.05406v1"
    },
    {
      "index": 18,
      "title": "Multimodal Video Summarization Via Time-Aware Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "29th ACM International Conference on Multimedia",
      "authors": "X. Shang, Z. Yuan et al."
    },
    {
      "index": 19,
      "title": "AudioVisual Video Summarization",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "B. Zhao, M. Gong, and X. Li",
      "orig_title": "Audio-Visual Video Summarization",
      "paper_id": "2105.07667v1"
    },
    {
      "index": 20,
      "title": "Multimodal-Based And Aesthetic-Guided Narrative Video Summarization",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "J. Xie, X. Chen et al."
    },
    {
      "index": 21,
      "title": "Multimodal Multitask Emotion Recognition Using Images, texts and tags",
      "abstract": "",
      "year": "2019",
      "venue": "The ACM International Conference on Multimedia Retrieval (ICLR)",
      "authors": "M. Page Fortin and B. Chaib-draa"
    },
    {
      "index": 22,
      "title": "Multimodal Summarization With Guidance of Multimodal Reference",
      "abstract": "",
      "year": "2020",
      "venue": "The 34th AAAI Conference on Artificial Intelligence (AAAI)",
      "authors": "J. Zhu, Y. Zhou et al."
    },
    {
      "index": 23,
      "title": "Graph Convolutional Net For Difficulty-Controllable Visual Ques. Generation",
      "abstract": "",
      "year": "2023",
      "venue": "World Wide Web",
      "authors": "F. Chen, J. Xie et al."
    },
    {
      "index": 24,
      "title": "FVQA: Fact-based Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "P. Wang, Q. Wu et al.",
      "orig_title": "FVQA: Fact-Based Visual Question Answering",
      "paper_id": "1606.05433v4"
    },
    {
      "index": 25,
      "title": "SimVQA: Exploring Simulated Environments for Visual Question Answering",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "P. Cascante-Bonilla, H. Wu et al."
    },
    {
      "index": 26,
      "title": "RUArt: A Novel Text-Centered Text-Based Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "Z.-X. Jin, H. Wu et al."
    },
    {
      "index": 27,
      "title": "A Universal Quaternion Hypergraph For Multimodal VQA",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "Z. Guo, J. Zhao et al."
    },
    {
      "index": 28,
      "title": "Memory-Aware Attentive Control For Community Question Answering With Knowledge-Based Dual Refinement",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
      "authors": "J. Wu, T. Mu et al."
    },
    {
      "index": 29,
      "title": "Language Models As Controlled Natural Language Semantic Parsers For Knowledge Graph Question Answering",
      "abstract": "",
      "year": "2023",
      "venue": "European Conference on Artificial Intelligence (ECAI)",
      "authors": "J. Lehmann et al."
    },
    {
      "index": 30,
      "title": "Dual Attention Networks for Visual Reference Resolution in Visual Dialog",
      "abstract": "",
      "year": "2019",
      "venue": "The Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "G.-C. Kang, J. Lim et al."
    },
    {
      "index": 31,
      "title": "DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue",
      "abstract": "",
      "year": "2020",
      "venue": "The 34th AAAI Conference on Artificial Intelligence (AAAI)",
      "authors": "X. Jiang et al.",
      "orig_title": "DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue",
      "paper_id": "1911.07251v1"
    },
    {
      "index": 32,
      "title": "Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1809.06873",
      "authors": "X. Xu, O. Dušek et al.",
      "orig_title": "Better Conversations By Modeling, Filtering, And Optimizing For Coherence And Diversity",
      "paper_id": "1809.06873v1"
    },
    {
      "index": 33,
      "title": "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders",
      "abstract": "",
      "year": "2017",
      "venue": "The 55th Annual Meeting of Association for Comp. Linguistics (ACL)",
      "authors": "T. Zhao et al.",
      "orig_title": "Learning Discourse-Level Diversity For Neural Dialog Models Using Conditional Variational Autoencoders",
      "paper_id": "1703.10960v3"
    },
    {
      "index": 34,
      "title": "Improving Cross-Modal Understanding In Visual Dialog Via Contrastive Learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "F. Chen, X. Chen et al."
    },
    {
      "index": 35,
      "title": "Unified Multimodal Model with Unlikelihood Training for Visual Dialog",
      "abstract": "",
      "year": "2022",
      "venue": "The 30th ACM International Conference on Multimedia",
      "authors": "Z. Wang, J. Wang et al.",
      "orig_title": "Unified Multimodal Model With Unlikelihood Training For Visual Dialog",
      "paper_id": "2211.13235v1"
    },
    {
      "index": 36,
      "title": "The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "G.-C. Kang, S. Kim et al.",
      "orig_title": "The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training",
      "paper_id": "2205.12502v2"
    },
    {
      "index": 37,
      "title": "Closed-Loop Reasoning With Graph-Aware Dense Interaction For Visual Dialog",
      "abstract": "",
      "year": "2022",
      "venue": "Multimedia Systems",
      "authors": "A.-A. Liu, G. Zhang et al."
    },
    {
      "index": 38,
      "title": "Counterfactual Visual Dialog: Robust Commonsense Knowledge Learning From Unbiased Training",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia",
      "authors": "A.-A. Liu, C. Huang et al."
    },
    {
      "index": 39,
      "title": "Sentiment Adaptive End-to-End Dialog Systems",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "authors": "W. Shi and Z. Yu",
      "orig_title": "Sentiment Adaptive End-to-End Dialog Systems",
      "paper_id": "1804.10731v3"
    },
    {
      "index": 40,
      "title": "An Adversarial Approach To Sentiment-Controlled Neural Dialogue Generation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.07129",
      "authors": "X. Kong, B. Li et al."
    },
    {
      "index": 41,
      "title": "EmoSen: Generating Sentiment And Emotion Controlled Responses In A Multimodal Dialogue System",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "authors": "M. Firdaus, H. Chauhan et al."
    },
    {
      "index": 42,
      "title": "SEPRG: Sentiment Aware Emotion Controlled Personalized Response Generation",
      "abstract": "",
      "year": "2021",
      "venue": "14th International Conference on Natural Language Generation",
      "authors": "M. Firdaus, U. Jain et al."
    },
    {
      "index": 43,
      "title": "The Acoustically Emotion-Aware Conversational Agent With Speech Emotion And Empathetic Responses",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "authors": "J. Hu, Y. Huang et al."
    },
    {
      "index": 44,
      "title": "Towards Sentiment-Aware Multi-Modal Dialogue Policy Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Cognitive Computation",
      "authors": "T. Saha, S. Saha et al."
    },
    {
      "index": 45,
      "title": "SentiGAN: Generating Sentimental Texts Via Mixture Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
      "authors": "K. Wang and X. Wan"
    },
    {
      "index": 46,
      "title": "Automatic Dialogue Generation With Expressed Emotions",
      "abstract": "",
      "year": "2018",
      "venue": "The Conference of North American Chapter of the Association for Computational Linguistics (NAACL)",
      "authors": "C. Huang, O. R. Zaiane et al."
    },
    {
      "index": 47,
      "title": "An Affect-Rich Neural Conversational Model with Biased Attention and Weighted Cross-Entropy Loss",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "P. Zhong et al.",
      "orig_title": "An Affect-Rich Neural Conversational Model With Biased Attention And Weighted Cross-Entropy Loss",
      "paper_id": "1811.07078v1"
    },
    {
      "index": 48,
      "title": "Intersection Over Union (IoU) for Object Detection",
      "abstract": "",
      "year": "2016",
      "venue": "PyImageSearch.com",
      "authors": "A. Rosebrock"
    },
    {
      "index": 49,
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "authors": "N. Reimers and I. Gurevych",
      "orig_title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "paper_id": "1908.10084v1"
    },
    {
      "index": 50,
      "title": "Hybrid Fusion Based Interpretable Multimodal Emotion Recognition With Insufficient Labelled Data",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.11450",
      "authors": "P. Kumar, S. Malik et al."
    },
    {
      "index": 51,
      "title": "A Value for n-Person Games, Contributions to the Theory of Games II",
      "abstract": "",
      "year": "1953",
      "venue": "",
      "authors": "L. Shapley"
    },
    {
      "index": 52,
      "title": "FLAIR: An Easy-to-Use Framework For State-Of-The-Art NLP",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conf. of North American Chapter of Association for Comp. linguistics (NAACL)",
      "authors": "A. Akbik, T. Bergmann et al."
    },
    {
      "index": 53,
      "title": "Sentimentr Package for R Language",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "T. Rinker"
    },
    {
      "index": 54,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01108",
      "authors": "V. Sanh, L. Debut et al.",
      "orig_title": "DistilBERT, A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 55,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu, M. Ott et al.",
      "orig_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 56,
      "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
      "abstract": "",
      "year": "2002",
      "venue": "The 40th Annual Meeting on Association for Computational Linguistics (ACL)",
      "authors": "K. Papineni, S. Roukos et al."
    },
    {
      "index": 57,
      "title": "CIDEr: Consensus-based Image Description Evaluation",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "R. Vedantam, C. Lawrence Zitnick et al.",
      "orig_title": "CIDEr: Consensus-based Image Description Evaluation",
      "paper_id": "1411.5726v2"
    },
    {
      "index": 58,
      "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
      "abstract": "",
      "year": "2004",
      "venue": "Text Summarization Branches Out",
      "authors": "C.-Y. Lin"
    },
    {
      "index": 59,
      "title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "abstract": "",
      "year": "2016",
      "venue": "The European Conference on Computer Vision (ECCV)",
      "authors": "P. Anderson, B. Fernando et al.",
      "orig_title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "paper_id": "1607.08822v1"
    },
    {
      "index": 60,
      "title": "The METEOR Metric for Automatic Evaluation of Machine Translation",
      "abstract": "",
      "year": "2009",
      "venue": "Springer Machine Translation Journal",
      "authors": "A. Lavie and M. J. Denkowski"
    },
    {
      "index": 61,
      "title": "Detection of Duplicate Defect Reports using Natural Language Processing",
      "abstract": "",
      "year": "2007",
      "venue": "IEEE International Conference on Software Engineering",
      "authors": "P. Runeson, M. Alexandersson, and O. Nyholm"
    },
    {
      "index": 62,
      "title": "Mean Reciprocal Rank",
      "abstract": "",
      "year": "2009",
      "venue": "Encyclopedia of Database Systems",
      "authors": "N. Craswell"
    },
    {
      "index": 63,
      "title": "GPT2",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "OpenAI"
    },
    {
      "index": 64,
      "title": "Learning to Predict Crisp Boundaries",
      "abstract": "",
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)",
      "authors": "R. Deng, C. Shen et al.",
      "orig_title": "Learning To Predict Crisp Boundaries",
      "paper_id": "1807.10097v1"
    }
  ]
}