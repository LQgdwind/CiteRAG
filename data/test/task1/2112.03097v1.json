{
  "paper_id": "2112.03097v1",
  "title": "Flexible Option Learning",
  "abstract": "Abstract\nTemporal abstraction in reinforcement learning (RL), offers the promise of improving generalization and knowledge transfer in complex environments, by propagating information more efficiently over time. Although option learning was initially formulated in a way that allows updating many options simultaneously, using off-policy, intra-option learning (Sutton, Precup & Singh, 1999), many of the recent hierarchical reinforcement learning approaches only update a single option at a time: the option currently executing. We revisit and extend intra-option learning in the context of deep reinforcement learning, in order to enable updating all options consistent with current primitive action choices, without introducing any additional estimates. Our method can therefore be naturally adopted in most hierarchical RL frameworks. When we combine our approach with the option-critic algorithm for option discovery, we obtain significant improvements in performance and data-efficiency across a wide variety of domains.",
  "reference_labels": [
    {
      "index": 0,
      "title": "The Option-Critic Architecture",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Pierre-Luc Bacon, Jean Harb, and Doina Precup",
      "orig_title": "The option-critic architecture",
      "paper_id": "1609.05140v2"
    },
    {
      "index": 1,
      "title": "Option discovery using deep skill chaining",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Akhil Bagaria and George Konidaris"
    },
    {
      "index": 2,
      "title": "A Guide to Simulation (2nd Ed.)",
      "abstract": "",
      "year": "1987",
      "venue": "Springer-Verlag",
      "authors": "Paul Bratley, Bennett¬†L. Fox, and Linus¬†E. Schrage"
    },
    {
      "index": 3,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 4,
      "title": "gym-miniworld environment for openai gym",
      "abstract": "",
      "year": "2018",
      "venue": "GitHub",
      "authors": "Maxime Chevalier-Boisvert"
    },
    {
      "index": 5,
      "title": "Expected Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Kamil Ciosek and Shimon Whiteson",
      "orig_title": "Expected policy gradients",
      "paper_id": "1706.05374v6"
    },
    {
      "index": 6,
      "title": "Identifying useful subgoals in reinforcement learning by local graph partitioning",
      "abstract": "",
      "year": "2005",
      "venue": "International Conference on Machine Learning",
      "authors": "√ñzg√ºr ≈ûim≈üek, Alicia¬†P. Wolfe, and Andrew¬†G. Barto"
    },
    {
      "index": 7,
      "title": "Probabilistic inference for determining options in reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "Machine Learning",
      "authors": "Christian Daniel, Herke Van¬†Hoof, Jan Peters, and Gerhard Neumann"
    },
    {
      "index": 8,
      "title": "Feudal reinforcement learning",
      "abstract": "",
      "year": "1993",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Peter Dayan and Geoffrey¬†E Hinton"
    },
    {
      "index": 9,
      "title": "Openai baselines",
      "abstract": "",
      "year": "2017",
      "venue": "GitHub",
      "authors": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov"
    },
    {
      "index": 10,
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine",
      "orig_title": "Diversity is all you need: Learning skills without a reward function",
      "paper_id": "1802.06070v6"
    },
    {
      "index": 11,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Scott Fujimoto, Herke van Hoof, and David Meger",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 12,
      "title": "Variational Intrinsic Control",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Karol Gregor, Danilo¬†Jimenez Rezende, and Daan Wierstra",
      "orig_title": "Variational intrinsic control",
      "paper_id": "1611.07507v1"
    },
    {
      "index": 13,
      "title": "Consistent On-Line Off-Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Assaf Hallak and Shie Mannor",
      "orig_title": "Consistent on-line off-policy evaluation",
      "paper_id": "1702.07121v1"
    },
    {
      "index": 14,
      "title": "Conditional monte carlo",
      "abstract": "",
      "year": "1956",
      "venue": "J. ACM",
      "authors": "J.¬†M. Hammersley"
    },
    {
      "index": 15,
      "title": "When waiting is not an option : Learning options with a deliberation cost",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup"
    },
    {
      "index": 16,
      "title": "Q(ŒªùúÜ\\lambda) with off-policy corrections",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Anna Harutyunyan, Marc¬†G. Bellemare, Tom Stepleton, and Remi Munos"
    },
    {
      "index": 17,
      "title": "Learning with Options that Terminate Off-Policy",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, and Ann Nowe",
      "orig_title": "Learning with options that terminate off-policy",
      "paper_id": "1711.03817v2"
    },
    {
      "index": 18,
      "title": "Safe Option-Critic: Learning Safety in the Option-Critic Architecture",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Arushi Jain, Khimya Khetarpal, and Doina Precup",
      "orig_title": "Safe option-critic: Learning safety in the option-critic architecture",
      "paper_id": "1807.08060v2"
    },
    {
      "index": 19,
      "title": "Eligibility traces for options",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Ayush Jain and Doina Precup"
    },
    {
      "index": 20,
      "title": "Doubly robust off-policy value evaluation for reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Nan Jiang and Lihong Li"
    },
    {
      "index": 21,
      "title": "Discovering options for exploration by minimizing cover time",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Yuu Jinnai, Jee¬†Won Park, David Abel, and George Konidaris"
    },
    {
      "index": 22,
      "title": "Options of Interest: Temporal Abstraction with Interest Functions",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Khimya Khetarpal, Martin Klissarov, Maxime Chevalier-Boisvert, Pierre-Luc Bacon, and Doina Precup",
      "orig_title": "Options of interest: Temporal abstraction with interest functions",
      "paper_id": "2001.00271v1"
    },
    {
      "index": 23,
      "title": "Temporally Abstract Partial Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, and Doina Precup",
      "orig_title": "Temporally abstract partial models",
      "paper_id": "2108.03213v1"
    },
    {
      "index": 24,
      "title": "Reward propagation using graph convolutional networks",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Martin Klissarov and Doina Precup"
    },
    {
      "index": 25,
      "title": "Learnings Options End-to-End for Continuous Action Tasks",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Martin Klissarov, Pierre-Luc Bacon, Jean Harb, and Doina Precup",
      "orig_title": "Learnings options end-to-end for continuous action tasks",
      "paper_id": "1712.00004v1"
    },
    {
      "index": 26,
      "title": "Pytorch implementations of reinforcement learning algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "GitHub",
      "authors": "Ilya Kostrikov"
    },
    {
      "index": 27,
      "title": "Unified inter and intra options learning using policy gradient methods",
      "abstract": "",
      "year": "2011",
      "venue": "European Workshop on Reinforcement Learning",
      "authors": "Kfir¬†Y Levy and Nahum Shimkin"
    },
    {
      "index": 28,
      "title": "Toward minimax off-policy value estimation",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Lihong Li, Remi Munos, and Csaba Szepesvari"
    },
    {
      "index": 29,
      "title": "Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Yao Liu, Pierre-Luc Bacon, and Emma Brunskill",
      "orig_title": "Understanding the curse of horizon in off-policy evaluation via conditional importance sampling",
      "paper_id": "1910.06508v2"
    },
    {
      "index": 30,
      "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Marlos¬†C Machado, Marc¬†G Bellemare, and Michael Bowling",
      "orig_title": "A laplacian framework for option discovery in reinforcement learning",
      "paper_id": "1703.00956v2"
    },
    {
      "index": 31,
      "title": "Weighted importance sampling for off-policy learning with linear function approximation",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A.¬†Rupam Mahmood, Hado¬†P van Hasselt, and Richard¬†S Sutton"
    },
    {
      "index": 32,
      "title": "Emphatic temporal-difference learning",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "Ashique¬†Rupam Mahmood, Huizhen Yu, Martha White, and Richard¬†S. Sutton"
    },
    {
      "index": 33,
      "title": "Approximate value iteration with temporally extended actions",
      "abstract": "",
      "year": "2015",
      "venue": "J. Artif. Intell. Res.",
      "authors": "Timothy¬†A. Mann, Shie Mannor, and Doina Precup"
    },
    {
      "index": 34,
      "title": "Automatic discovery of subgoals in reinforcement learning using diverse density",
      "abstract": "",
      "year": "2001",
      "venue": "ICML",
      "authors": "Amy McGovern and Andrew¬†G Barto"
    },
    {
      "index": 35,
      "title": "Q-cut-dynamic discovery of sub-goals in reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "European Conference on Machine Learning",
      "authors": "Ishai Menache, Shie Mannor, and Nahum Shimkin"
    },
    {
      "index": 36,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "Volodymyr Mnih, Adri√†¬†Puigdom√®nech Badia, Mehdi Mirza, Alex Graves, Timothy¬†P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu"
    },
    {
      "index": 37,
      "title": "Efficient Memory-based Learning for Robot Control",
      "abstract": "",
      "year": "1991",
      "venue": "Carnegie Mellon University",
      "authors": "Andrew Moore"
    },
    {
      "index": 38,
      "title": "Data-Efficient Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine",
      "orig_title": "Data-efficient hierarchical reinforcement learning",
      "paper_id": "1805.08296v4"
    },
    {
      "index": 39,
      "title": "Self-Supervised Exploration via Disagreement",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta",
      "orig_title": "Self-supervised exploration via disagreement",
      "paper_id": "1906.04161v1"
    },
    {
      "index": 40,
      "title": "All-action policy gradient methods: A numerical integration approach",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Benjamin Petit, Loren Amdahl-Culleton, Yao Liu, Jimmy Smith, and Pierre-Luc Bacon"
    },
    {
      "index": 41,
      "title": "Temporal abstraction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Doina Precup"
    },
    {
      "index": 42,
      "title": "Eligibility traces for off-policy policy evaluation",
      "abstract": "",
      "year": "2000",
      "venue": "International Conference on Machine Learning",
      "authors": "Doina Precup, Richard¬†S. Sutton, and Satinder¬†P. Singh"
    },
    {
      "index": 43,
      "title": "Learning Abstract Options",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Matthew Riemer, Miao Liu, and Gerald Tesauro",
      "orig_title": "Learning abstract options",
      "paper_id": "1810.11583v4"
    },
    {
      "index": 44,
      "title": "Universal value function approximators",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Tom Schaul, Dan Horgan, Karol Gregor, and David Silver"
    },
    {
      "index": 45,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov"
    },
    {
      "index": 46,
      "title": "Information radius",
      "abstract": "",
      "year": "1969",
      "venue": "Zeitschrift f√ºr Wahrscheinlichkeitstheorie und verwandte Gebiete",
      "authors": "Robin Sibson"
    },
    {
      "index": 47,
      "title": "An inference-based policy gradient method for learning options",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Matthew Smith, Herke van Hoof, and Joelle Pineau"
    },
    {
      "index": 48,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "The MIT Press",
      "authors": "Richard¬†S. Sutton and Andrew¬†G. Barto"
    },
    {
      "index": 49,
      "title": "Comparing policy-gradient algorithms",
      "abstract": "",
      "year": "1983",
      "venue": "IEEE Trans. on Systems, Man, and Cybernetics",
      "authors": "Richard¬†S. Sutton, Satinder Singh, and David Mcallester"
    },
    {
      "index": 50,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "1999",
      "venue": "NIPS",
      "authors": "Richard¬†S. Sutton, David¬†A. McAllester, Satinder¬†P. Singh, and Yishay Mansour"
    },
    {
      "index": 51,
      "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial intelligence",
      "authors": "Richard¬†S Sutton, Doina Precup, and Satinder Singh"
    },
    {
      "index": 52,
      "title": "Natural Option Critic",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Saket Tiwari and Philip¬†S. Thomas",
      "orig_title": "Natural option critic",
      "paper_id": "1812.01488v1"
    },
    {
      "index": 53,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "IROS",
      "authors": "Emanuel Todorov, Tom Erez, and Yuval Tassa"
    },
    {
      "index": 54,
      "title": "A theoretical and empirical analysis of expected sarsa",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
      "authors": "Harm van Seijen, Hado van Hasselt, Shimon Whiteson, and Marco Wiering"
    },
    {
      "index": 55,
      "title": "FeUdal Networks for Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Alexander¬†Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu",
      "orig_title": "Feudal networks for hierarchical reinforcement learning",
      "paper_id": "1703.01161v2"
    },
    {
      "index": 56,
      "title": "Data-efficient Hindsight Off-policy Option Learning",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Markus Wulfmeier, Dushyant Rao, Roland Hafner, Thomas Lampe, Abbas Abdolmaleki, Tim Hertweck, Michael Neunert, Dhruva Tirumala, Noah¬†Y. Siegel, Nicolas Heess, and Martin¬†A. Riedmiller",
      "orig_title": "Data-efficient hindsight off-policy option learning",
      "paper_id": "2007.15588v2"
    },
    {
      "index": 57,
      "title": "Optimal off-policy evaluation for reinforcement learning with marginalized importance sampling",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Tengyang Xie, Yifei Ma, and Yu-Xiang Wang"
    },
    {
      "index": 58,
      "title": "GenDICE: Generalized Offline Estimation of Stationary Values",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Ruiyi Zhang, Bo¬†Dai, Lihong Li, and Dale Schuurmans",
      "orig_title": "Gendice: Generalized offline estimation of stationary values",
      "paper_id": "2002.09072v1"
    },
    {
      "index": 59,
      "title": "DAC: the double actor-critic architecture for learning options",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Shangtong Zhang and Shimon Whiteson"
    }
  ]
}