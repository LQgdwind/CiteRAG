{
  "paper_id": "2308.14919v2",
  "title": "On Reward Structures of Markov Decision Processes",
  "abstract": "Abstract",
  "reference_labels": [
    {
      "index": 0,
      "title": "Finite-Time Analysis of the Multiarmed Bandit Problem",
      "abstract": "",
      "year": "2002",
      "venue": "Machine Learning",
      "authors": "Peter Auer, Nicol√≤ Cesa-Bianchi and Paul Fischer"
    },
    {
      "index": 1,
      "title": "Reversible Markov chains and random walks on graphs",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": "David Aldous and James Fill"
    },
    {
      "index": 2,
      "title": "Potential-based Shaping in Model-based Reinforcement Learning.",
      "abstract": "",
      "year": "2008",
      "venue": "National Conference on Artificial Intelligence (AAAI)",
      "authors": "John Asmuth, Michael L Littman and Robert Zinkov"
    },
    {
      "index": 3,
      "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Felix Berkenkamp, Matteo Turchetta, Angela Schoellig and Andreas Krause",
      "orig_title": "Safe model-based reinforcement learning with stability guarantees",
      "paper_id": "1705.08551v3"
    },
    {
      "index": 4,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang and Wojciech Zaremba"
    },
    {
      "index": 5,
      "title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "Ronen I Brafman and Moshe Tennenholtz"
    },
    {
      "index": 6,
      "title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs",
      "abstract": "",
      "year": "2009",
      "venue": "Conference on Uncertainty in Artificial Intelligence (UAI)",
      "authors": "Peter L Bartlett and Ambuj Tewari"
    },
    {
      "index": 7,
      "title": "Neuro-dynamic programming",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Dimitri P Bertsekas and John N Tsitsiklis"
    },
    {
      "index": 8,
      "title": "Faulty Reward Functions in the Wild",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Jack Clark and Dario Amodei"
    },
    {
      "index": 9,
      "title": "On Integrating Apprentice Learning and Reinforcement Learning",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Jeffery Allen Clouse"
    },
    {
      "index": 10,
      "title": "Glyph-aware Embedding of Chinese Characters",
      "abstract": "",
      "year": "2017",
      "venue": "Empirical Methods of Natural Language Processing",
      "authors": "Falcon Z Dai and Zheng Cai"
    },
    {
      "index": 11,
      "title": "Towards Near-imperceptible Steganographic Text",
      "abstract": "",
      "year": "2019",
      "venue": "Association for Computational Linguistics",
      "authors": "Falcon Z Dai and Zheng Cai"
    },
    {
      "index": 12,
      "title": "Finite state Markovian decision processes",
      "abstract": "",
      "year": "1970",
      "venue": "",
      "authors": "Cyrus Derman"
    },
    {
      "index": 13,
      "title": "TD (ŒªùúÜ\\lambda) converges with probability 1",
      "abstract": "",
      "year": "1994",
      "venue": "Machine Learning",
      "authors": "Peter Dayan and Terrence J Sejnowski"
    },
    {
      "index": 14,
      "title": "Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Falcon Z Dai and Matthew R Walter",
      "orig_title": "Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards",
      "paper_id": "1907.02114v2"
    },
    {
      "index": 15,
      "title": "Loop Estimator for Discounted Values in Markov Reward Processes",
      "abstract": "",
      "year": "2021",
      "venue": "Association for the Advancement of Artificial Intelligence Conference",
      "authors": "Falcon Z Dai and Matthew R Walter",
      "orig_title": "Loop Estimator for Discounted Values in Markov Reward Processes",
      "paper_id": "2002.06299v3"
    },
    {
      "index": 16,
      "title": "Learning rates for Q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of machine learning Research",
      "authors": "Eyal Even-Dar and Yishay Mansour"
    },
    {
      "index": 17,
      "title": "Simulating discounted costs",
      "abstract": "",
      "year": "1989",
      "venue": "Management Science",
      "authors": "Bennett L Fox and Peter W Glynn"
    },
    {
      "index": 18,
      "title": "Scientific Applications: An Algorithm for Identifying the Ergodic Subchains and Transient States of a Stochastic Matrix",
      "abstract": "",
      "year": "1968",
      "venue": "Commun. ACM",
      "authors": "B.. Fox and D.. Landi"
    },
    {
      "index": 19,
      "title": "Exploration-Exploitation in Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Algorithmic Learning Theory conference",
      "authors": "Ronan Fruit, Alessandro Lazaric and Matteo Pirotta"
    },
    {
      "index": 20,
      "title": "Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "authors": "Ronan Fruit, Matteo Pirotta and Alessandro Lazaric",
      "orig_title": "Near optimal exploration-exploitation in non-communicating markov decision processes",
      "paper_id": "1807.02373v2"
    },
    {
      "index": 21,
      "title": "Improved Analysis of UCRL2 with Empirical Bernstein Inequality",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Ronan Fruit, Matteo Pirotta and Alessandro Lazaric",
      "orig_title": "Improved Analysis of UCRL2 with Empirical Bernstein Inequality",
      "paper_id": "2007.05456v1"
    },
    {
      "index": 22,
      "title": "Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric and Ronald Ortner",
      "orig_title": "Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning",
      "paper_id": "1802.04020v2"
    },
    {
      "index": 23,
      "title": "Characterization of optimal policies in vector-valued Markovian decision processes",
      "abstract": "",
      "year": "1980",
      "venue": "Mathematics of operations research",
      "authors": "Nagata Furukawa"
    },
    {
      "index": 24,
      "title": "Bounded-parameter Markov decision processes",
      "abstract": "",
      "year": "2000",
      "venue": "Artificial Intelligence",
      "authors": "Robert Givan, Sonia Leach and Thomas Dean"
    },
    {
      "index": 25,
      "title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model",
      "abstract": "",
      "year": "2013",
      "venue": "Machine Learning",
      "authors": "Mohammad Gheshlaghi Azar, R√©mi Munos and Hilbert J. Kappen"
    },
    {
      "index": 26,
      "title": "Reward shaping in episodic reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)",
      "authors": "Marek Grze≈õ"
    },
    {
      "index": 27,
      "title": "Dynamic programming and Markov processes.",
      "abstract": "",
      "year": "1960",
      "venue": "",
      "authors": "Ronald A Howard"
    },
    {
      "index": 28,
      "title": "Estimating the value of a discounted reward process",
      "abstract": "",
      "year": "1992",
      "venue": "Operations Research Letters",
      "authors": "Moshe Haviv and Martin L Puterman"
    },
    {
      "index": 29,
      "title": "Convergence of stochastic iterative dynamic programming algorithms",
      "abstract": "",
      "year": "1994",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tommi Jaakkola, Michael I Jordan and Satinder P Singh"
    },
    {
      "index": 30,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Machine Learning Research",
      "authors": "Thomas Jaksch, Ronald Ortner and Peter Auer"
    },
    {
      "index": 31,
      "title": "On the sample complexity of reinforcement learning",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Sham Machandranath Kakade"
    },
    {
      "index": 32,
      "title": "Reinforcement learning in robotics: A survey",
      "abstract": "",
      "year": "2013",
      "venue": "The International Journal of Robotics Research",
      "authors": "Jens Kober, J Andrew Bagnell and Jan Peters"
    },
    {
      "index": 33,
      "title": "Bias-Variance Error Bounds for Temporal Difference Updates",
      "abstract": "",
      "year": "2000",
      "venue": "Conference on Learning Theory",
      "authors": "Michael J Kearns and Satinder P Singh"
    },
    {
      "index": 34,
      "title": "Near-optimal reinforcement learning in polynomial time",
      "abstract": "",
      "year": "2002",
      "venue": "Machine learning",
      "authors": "Michael Kearns and Satinder Singh"
    },
    {
      "index": 35,
      "title": "Finite-sample convergence rates for Q-learning and indirect algorithms",
      "abstract": "",
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Michael J Kearns and Satinder P Singh"
    },
    {
      "index": 36,
      "title": "Approximating the Stationary Probability of a Single State in a Markov chain",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint",
      "authors": "Christina E Lee, Asuman Ozdaglar and Devavrat Shah"
    },
    {
      "index": 37,
      "title": "Markov chains and mixing times",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "David A Levin, Yuval Peres and Elizabeth L Wilmer"
    },
    {
      "index": 38,
      "title": "Safe exploration in Markov decision processes",
      "abstract": "",
      "year": "2012",
      "venue": "International Coference on International Conference on Machine Learning",
      "authors": "Teodor Mihai Moldovan and Pieter Abbeel"
    },
    {
      "index": 39,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland and Georg Ostrovski"
    },
    {
      "index": 40,
      "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
      "abstract": "",
      "year": "1999",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Andrew Y Ng, Daishi Harada and Stuart Russell"
    },
    {
      "index": 41,
      "title": "A value iteration method for undiscounted multichain Markov decision processes",
      "abstract": "",
      "year": "1988",
      "venue": "Zeitschrift f√ºr Operations Research",
      "authors": "K. Ohno"
    },
    {
      "index": 42,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama and Alex Ray"
    },
    {
      "index": 43,
      "title": "Iterated Prisoner‚Äôs Dilemma contains strategies that dominate any evolutionary opponent",
      "abstract": "",
      "year": "2012",
      "venue": "National Academy of Sciences",
      "authors": "William H. Press and Freeman J. Dyson"
    },
    {
      "index": 44,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": "Martin L Puterman"
    },
    {
      "index": 45,
      "title": "Value function estimation in Markov reward processes: Instance-dependent ‚Ñì‚àûsubscript‚Ñì\\ell_{\\infty}-bounds for policy evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Ashwin Pananjady and Martin J. Wainwright"
    },
    {
      "index": 46,
      "title": "On the possibility of learning in reactive environments with arbitrary dependence",
      "abstract": "",
      "year": "2008",
      "venue": "Theoretical Computer Science",
      "authors": "Daniil Ryabko and Marcus Hutter"
    },
    {
      "index": 47,
      "title": "Stochastic processes",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Sheldon M Ross"
    },
    {
      "index": 48,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Richard S Sutton and Andrew G Barto"
    },
    {
      "index": 49,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "Richard S Sutton and Andrew G Barto"
    },
    {
      "index": 50,
      "title": "A value-iteration scheme for undiscounted multichain Markov renewal programs",
      "abstract": "",
      "year": "1984",
      "venue": "Zeitschrift f√ºr Operations Research",
      "authors": "P.. Schweitzer"
    },
    {
      "index": 51,
      "title": "Towards Active Imitation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Learning from Demonstrations in High-Dimensional Feature Spaces workshop at Robotics: Science and Systems (RSS)",
      "authors": "Chip Schaff, Falcon Z Dai and Matthew R Walter"
    },
    {
      "index": 52,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam and Marc Lanctot"
    },
    {
      "index": 53,
      "title": "A theoretical analysis of model-based interval estimation",
      "abstract": "",
      "year": "2005",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Alexander L Strehl and Michael L Littman"
    },
    {
      "index": 54,
      "title": "An analysis of model-based interval estimation for Markov decision processes",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Computer and System Sciences",
      "authors": "Alexander L Strehl and Michael L Littman"
    },
    {
      "index": 55,
      "title": "Renewal Monte Carlo: Renewal theory based reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Jayakumar Subramanian and Aditya Mahajan",
      "orig_title": "Renewal Monte Carlo: Renewal theory based reinforcement learning",
      "paper_id": "1804.01116v1"
    },
    {
      "index": 56,
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial intelligence",
      "authors": "Richard S. Sutton, Doina Precup and Satinder Singh"
    },
    {
      "index": 57,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "Machine Learning",
      "authors": "Richard S Sutton"
    },
    {
      "index": 58,
      "title": "Bounded parameter Markov decision processes with average reward criterion",
      "abstract": "",
      "year": "2007",
      "venue": "International Conference on Computational Learning Theory (COLT)",
      "authors": "Ambuj Tewari and Peter L Bartlett"
    },
    {
      "index": 59,
      "title": "I.‚ÄîCOMPUTING MACHINERY AND INTELLIGENCE",
      "abstract": "",
      "year": "1950",
      "venue": "Mind",
      "authors": "Alan M Turing"
    },
    {
      "index": 60,
      "title": "Principled methods for advising reinforcement learning agents",
      "abstract": "",
      "year": "2003",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Eric Wiewiora, Garrison W Cottrell and Charles Elkan"
    },
    {
      "index": 61,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Christopher JCH Watkins and Peter Dayan"
    },
    {
      "index": 62,
      "title": "Potential-based shaping and Q-value initialization are equivalent",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Eric Wiewiora"
    }
  ]
}