{
  "paper_id": "2209.00606v1",
  "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation",
  "abstract": "Abstract\nAs its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence.\nDespite favorable performance, calculating pairwise correlations is prohibitively costly.\nWhile recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked.\nThis work addresses these constraints by architecting an accelerator, called Sprint 111Sprint: SParse attention acceleration with appRoximate IN-memory Token pruning, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner.\nOur design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling Sprint to fetch only a small subset of relevant data to on-chip memory.\nTo mitigate potential negative repercussions for model accuracy, Sprint re-computes the attention scores for the few fetched data in digital.\nThe combined in-memory pruning and on-chip recompute of the relevant attention scores enables Sprint to transform quadratic complexity to a merely linear one.\nIn addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches.\nWe evaluate our proposed technique on a wide range of state-of-the-art transformer models.\nOn average, Sprint yields 7.5×\\times speedup and 19.6×\\times energy reduction when total 16KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36%percent\\% degradation).",
  "reference_labels": [
    {
      "index": 0,
      "title": "Compute Caches",
      "abstract": "",
      "year": "2017",
      "venue": "HPCA",
      "authors": "S. Aga, S. Jeloka, A. Subramaniyan, S. Narayanasamy, D. Blaauw, and R. Das"
    },
    {
      "index": 1,
      "title": "Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Circuits and Systems I",
      "authors": "A. Agrawal, A. Jaiswal, D. Roy, B. Han, G. Srinivasan, A. Ankit, and K. Roy",
      "orig_title": "Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays",
      "paper_id": "1807.00343v2"
    },
    {
      "index": 2,
      "title": "A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing",
      "abstract": "",
      "year": "2015",
      "venue": "ISCA",
      "authors": "J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi"
    },
    {
      "index": 3,
      "title": "PIM-enabled Instructions: A Low-Overhead, Locality-Aware Processing-in-Memory Architecture",
      "abstract": "",
      "year": "2015",
      "venue": "ISCA",
      "authors": "J. Ahn, S. Yoo, O. Mutlu, and K. Choi"
    },
    {
      "index": 4,
      "title": "An Introduction to SDRAM and Memory Controllers",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "B. Akesson"
    },
    {
      "index": 5,
      "title": "Predator: A Predictable SDRAM Memory Controller",
      "abstract": "",
      "year": "2007",
      "venue": "CODES+ISSS",
      "authors": "B. Akesson, K. Goossens, and M. Ringhofer"
    },
    {
      "index": 6,
      "title": "SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ISCA",
      "authors": "V. Aklaghi, A. Yazdanbakhsh, K. Samadi, H. Esmaeilzadeh, and R. K. Gupta"
    },
    {
      "index": 7,
      "title": "Bit-Pragmatic Deep Neural Network Computing",
      "abstract": "",
      "year": "2017",
      "venue": "MICRO",
      "authors": "J. Albericio, A. Delmás, P. Judd, S. Sharify, G. O’Leary, R. Genov, and A. Moshovos"
    },
    {
      "index": 8,
      "title": "Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing",
      "abstract": "",
      "year": "2016",
      "venue": "ISCA",
      "authors": "J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos"
    },
    {
      "index": 9,
      "title": "ReDRAM: A Reconfigurable Processing-In-DRAM Platform for Accelerating Bulk Bit-Wise Operations",
      "abstract": "",
      "year": "2019",
      "venue": "ICCAD",
      "authors": "S. Angizi and D. Fan"
    },
    {
      "index": 10,
      "title": "DIMA: A Depthwise CNN In-Memory Accelerator",
      "abstract": "",
      "year": "2018",
      "venue": "ICCAD",
      "authors": "S. Angizi, Z. He, and D. Fan"
    },
    {
      "index": 11,
      "title": "ParaPIM: A Parallel Processing-in-memory Accelerator for Binary-Weight Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "ASPDAC",
      "authors": "S. Angizi, Z. He, and D. Fan"
    },
    {
      "index": 12,
      "title": "Artisan Memory Compilers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "ARM"
    },
    {
      "index": 13,
      "title": "Fafnir: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction",
      "abstract": "",
      "year": "2021",
      "venue": "HPCA",
      "authors": "B. Asgari, R. Hadidi, J. Cao, D. E. Shim, S.-K. Lim, and H. Kim"
    },
    {
      "index": 14,
      "title": "Multi-Level Control of Resistive RAM (RRAM) Using a Write Termination to Achieve 4 Bits/Cell in High Resistance State",
      "abstract": "",
      "year": "2021",
      "venue": "Electronics",
      "authors": "H. Aziza, S. Hamdioui, M. Fieback, M. Taouil, M. Moreau, P. Girard, A. Virazel, and K. Coulié"
    },
    {
      "index": 15,
      "title": "Longformer: The Long-Document Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.05150",
      "authors": "I. Beltagy, M. E. Peters, and A. Cohan",
      "orig_title": "Longformer: The Long-Document Transformer",
      "paper_id": "2004.05150v2"
    },
    {
      "index": 16,
      "title": "ReVAMP: ReRAM based VLIW Architecture for In-memory Computing",
      "abstract": "",
      "year": "2017",
      "venue": "DATE",
      "authors": "D. Bhattacharjee, R. Devadoss, and A. Chattopadhyay"
    },
    {
      "index": 17,
      "title": "PARDIS: A Programmable Memory Controller for the DDRx Interfacing Standards",
      "abstract": "",
      "year": "2012",
      "venue": "ISCA",
      "authors": "M. N. Bojnordi and E. Ipek"
    },
    {
      "index": 18,
      "title": "Genus Synthesis Solution",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Cadence"
    },
    {
      "index": 19,
      "title": "Innovus Implementation System",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Cadence"
    },
    {
      "index": 20,
      "title": "A Fully Integrated Reprogrammable Memristor–CMOS System for Efficient Multiply–Accumulate Operations",
      "abstract": "",
      "year": "2019",
      "venue": "Nature Electronics",
      "authors": "F. Cai, J. M. Correll, S. H. Lee, Y. Lim, V. Bothra, Z. Zhang, M. P. Flynn, and W. Lu"
    },
    {
      "index": 21,
      "title": "Impulse: Building a Smarter Memory Controller",
      "abstract": "",
      "year": "1999",
      "venue": "HPCA",
      "authors": "J. Carter, W. Hsieh, L. Stoller, M. Swanson, L. Zhang, E. Brunvand, A. Davis, C.-C. Kuo, R. Kuramkote, M. Parker, L. Schaelicke, and T. Tateyama"
    },
    {
      "index": 22,
      "title": "Circuit Design Challenges in Embedded Memory and Resistive RAM (RRAM) for Mobile SoC and 3D-IC",
      "abstract": "",
      "year": "2011",
      "venue": "ASP-DAC",
      "authors": "M.-F. Chang, P.-F. Chiu, and S.-S. Sheu"
    },
    {
      "index": 23,
      "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "T. Chen, Y. Cheng, Z. Gan, L. Yuan, L. Zhang, and Z. Wang",
      "orig_title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
      "paper_id": "2106.04533v3"
    },
    {
      "index": 24,
      "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning",
      "abstract": "",
      "year": "2014",
      "venue": "ASPLOS",
      "authors": "T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam"
    },
    {
      "index": 25,
      "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ISCA",
      "authors": "Y.-H. Chen, J. Emer, and V. Sze"
    },
    {
      "index": 26,
      "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices",
      "abstract": "",
      "year": "2019",
      "venue": "JETCAS",
      "authors": "Y.-H. Chen, T.-J. Yang, J. Emer, and V. Sze",
      "orig_title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices",
      "paper_id": "1807.07928v2"
    },
    {
      "index": 27,
      "title": "DaDianNao: A Machine-Learning Supercomputer",
      "abstract": "",
      "year": "2014",
      "venue": "MICRO",
      "authors": "Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu, N. Sun, and O. Temam"
    },
    {
      "index": 28,
      "title": "Prime: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-based Main Memory",
      "abstract": "",
      "year": "2016",
      "venue": "ISCA",
      "authors": "P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie"
    },
    {
      "index": 29,
      "title": "Generating Long Sequences with Sparse Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.10509",
      "authors": "R. Child, S. Gray, A. Radford, and I. Sutskever",
      "orig_title": "Generating Long Sequences with Sparse Transformers",
      "paper_id": "1904.10509v1"
    },
    {
      "index": 30,
      "title": "Multi-Stream Write SSD",
      "abstract": "",
      "year": "2016",
      "venue": "Flash Memory Summit",
      "authors": "C. Choi"
    },
    {
      "index": 31,
      "title": "Rethinking Attention with Performers",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.14794",
      "authors": "K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. Colwell, and A. Weller"
    },
    {
      "index": 32,
      "title": "Adaptively Sparse Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.00015",
      "authors": "G. M. Correia, V. Niculae, and A. F. Martins",
      "orig_title": "Adaptively Sparse Transformers",
      "paper_id": "1909.00015v2"
    },
    {
      "index": 33,
      "title": "Fine-Tune BERT with Sparse Self-Attention Mechanism",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "B. Cui, Y. Li, M. Chen, and Z. Zhang"
    },
    {
      "index": 34,
      "title": "Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "ASPLOS",
      "authors": "A. Delmas Lascorz, P. Judd, D. M. Stuart, Z. Poulos, M. Mahmoud, S. Sharify, M. Nikolic, K. Siu, and A. Moshovos"
    },
    {
      "index": 35,
      "title": "DrAcc: A DRAM based Accelerator for Accurate CNN Inference",
      "abstract": "",
      "year": "2018",
      "venue": "DAC",
      "authors": "Q. Deng, L. Jiang, Y. Zhang, M. Zhang, and J. Yang"
    },
    {
      "index": 36,
      "title": "Design of Ion-Implanted MOSFET’s with Very Small Physical Dimensions",
      "abstract": "",
      "year": "1974",
      "venue": "JSSC",
      "authors": "R. H. Dennard, F. H. Gaensslen, V. L. Rideout, E. Bassous, and A. R. LeBlanc"
    },
    {
      "index": 37,
      "title": "NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": "X. Dong, C. Xu, Y. Xie, and N. P. Jouppi"
    },
    {
      "index": 38,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby",
      "orig_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 39,
      "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ISCA",
      "authors": "C. Eckert, X. Wang, J. Wang, A. Subramaniyan, R. Iyer, D. Sylvester, D. Blaaauw, and R. Das",
      "orig_title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks",
      "paper_id": "1805.03718v1"
    },
    {
      "index": 40,
      "title": "“NDA: Near-DRAM Acceleration Architecture Leveraging Commodity DRAM Devices and Standard Memory Modules",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "“A Configurable Cloud-Scale DNN Processor for Real-Time AI",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "“Parallel Computing of Graph-based Functions in ReRAM",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 43,
      "title": "G. Tziantzioulis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 44,
      "title": "“TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 45,
      "title": "“Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 46,
      "title": "“DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 47,
      "title": "“Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 48,
      "title": "Mixed-Signal Charge-Domain Acceleration of Deep Neural Networks through Interleaved Bit-Partitioned Arithmetic",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic",
      "paper_id": "1906.11915v3"
    },
    {
      "index": 49,
      "title": "Bit-Parallel Vector Composability for Neural Acceleration",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Bit-Parallel Vector Composability for Neural Acceleration",
      "paper_id": "2004.05333v1"
    },
    {
      "index": 50,
      "title": "“Resistive RAM Endurance: Array-Level Characterization and Correction Techniques Targeting Deep Learning Applications",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "“AC-DIMM: Associative Computing with STT-MRAM",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 52,
      "title": "“ReRAM-based In-memory Computing for Search Engine and Neural Network Applications",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "A3: Accelerating Attention Mechanisms in Neural Networks with Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A3̂: Accelerating Attention Mechanisms in Neural Networks with Approximation",
      "paper_id": "2002.10941v1"
    },
    {
      "index": 54,
      "title": "“ELSA: Hardware-Software Co-design for Efficient",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "“EIE: Efficient Inference Engine on Compressed Deep Neural Network",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "“Simulating DRAM Controllers for Future System Architecture Exploration",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "“Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "“Noise Injection Adaption: End-to-end ReRAM Crossbar Mon-Ideal Effect Adaption for Meural Network Mapping",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "“Dot-product Engine for Neuromorphic Computing: Programming 1T1M Crossbar to Accelerate Matrix-Vector Multiplication",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "FastWave: Accelerating Autoregressive Convolutional Neural Networks on FPGA",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“FastWave: Accelerating Autoregressive Convolutional Neural Networks on FPGA",
      "paper_id": "2002.04971v1"
    },
    {
      "index": 61,
      "title": "“FloatPIM: In-Memory Acceleration of Deep Neural Network Training with High Precision",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "“XNOR-POP: A Processing-In-Memory Architecture for Binary Convolutional Neural Networks in Wide-IO2 DRAMs",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "“AccuReD: High Accuracy Training of CNNs on ReRAM/GPU Heterogeneous 3-D Architecture",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "R. Narayanaswami",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "“Stripes: Bit-serial Deep Neural Network Computing",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "“A Multi-functional In-memory Inference Processor using a Standard 6T SRAM Array",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "“An Energy-Efficient VLSI Architecture for Pattern Recognition via Deep Embedding of Computation in SRAM",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "“FlexRAM: Toward an Advanced Intelligent Memory System",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "Flat: An Optimized Dataflow for Mitigating Attention Bottlenecks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks",
      "paper_id": "2107.06419v7"
    },
    {
      "index": 70,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 71,
      "title": "“MViD: Sparse Matrix-Vector Multiplication in Mobile DRAM for Accelerating Recurrent Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent",
      "paper_id": "2102.07511v1"
    },
    {
      "index": 74,
      "title": "Learned Token Pruning for Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Learned Token Pruning for Transformers",
      "paper_id": "2107.00910v3"
    },
    {
      "index": 75,
      "title": "Reformer: The Efficient Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Reformer: The Efficient Transformer",
      "paper_id": "2001.04451v2"
    },
    {
      "index": 76,
      "title": "“EXECUBE-A New Architecture for Scaleable MPPs",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Learning Multiple Layers of Features from Tiny Images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "“Understanding Reuse",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "“MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "“TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "paper_id": "1909.11942v6"
    },
    {
      "index": 82,
      "title": "“ComPEND: Computation Pruning through Early Negative Detection for ReLU in a deep neural network accelerator",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "“Buffered Compares: Excavating the Hidden Parallelism Inside DRAM Architectures with Lightweight Logic",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "“UNPU: A 50.6 TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "“Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology: Industrial Product",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "“DRISA: A DRAM-based Reconfigurable In-Situ Accelerator",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "“Pinatubo: A Processing-in-Memory Architecture for Bulk Bitwise Operations in Emerging Non-Volatile Memories",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "“TIMELY: Pushing Data Movements and Interfaces in PIM Accelerators towards Local and in Time Domain",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Accelerating Attention through Gradient-Based Learned Runtime Pruning",
      "paper_id": "2204.03227v3"
    },
    {
      "index": 90,
      "title": "“PredictiveNet: An Energy-Efficient Convolutional Neural Network via Zero Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "“Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "Generating Wikipedia by Summarizing Long Sequences",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Generating Wikipedia by Summarizing Long Sequences",
      "paper_id": "1801.10198v1"
    },
    {
      "index": 93,
      "title": "“Cambricon: An Instruction Set Architecture for Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "“3DICT: A Reliable and QoS Capable Mobile Process-in-Memory Architecture for Lookup-Based CNNs in 3D XPoint ReRAMs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "“Smart Memories: A Modular Reconfigurable Architecture",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "“LerGAN: A Zero-free",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "“The WikiText Long Term Dependency Language Modeling Dataset",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "“Are Sixteen Heads Really Better than One?” arXiv preprint arXiv:1905.10650",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "“A Survey of ReRAM-based Architectures for Processing-In-Memory and Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "N. Muralimanohar",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "“Missing the Memory Wall: The Case for Processor/Memory Integration",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "“SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "“OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "“PyTorch: An Imperative Style",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "“A Case for Intelligent RAM",
      "abstract": "",
      "year": "1997",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "R. Balasubramonian",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "“SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "Blockwise Self-Attention for Long Document Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Blockwise Self-Attention for Long Document Understanding",
      "paper_id": "1911.02972v2"
    },
    {
      "index": 110,
      "title": "“Language Models are Unsupervised Multitask Learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "Compressive Transformers for Long-Range Sequence Modelling",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Compressive transformers for long-range sequence modelling",
      "paper_id": "1911.05507v1"
    },
    {
      "index": 112,
      "title": "000+ Questions for Machine Comprehension of Text",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "“Exploring the Impact of Memory Block Permutation on Performance of a Crossbar ReRAM Main Memory",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "“Minerva: Enabling Low-Power",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "“DeepFense: Online Accelerated Defense against Adversarial Deep Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient Content-based Sparse Attention with Routing Transformers",
      "paper_id": "2003.05997v5"
    },
    {
      "index": 117,
      "title": "“BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "“EncoDeep: Realizing Bit-Flexible Encoding for Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "R. Ausavarungnirun",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "“Ambit: In-Memory Accelerator for Bulk Bitwise Operations using Commodity DRAM Technology",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "“Gather-Scatter DRAM: In-DRAM Address Translation to Improve the Spatial Locality of Non-unit Strided Accesses",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "N. Muralimanohar",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "“Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "“Laconic Deep Learning Inference Acceleration",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "“Loom: Exploiting Weight and Activation Precisions to Accelerate Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "“From High-Level Deep Neural Models to FPGAs",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks",
      "paper_id": "1712.01507v2"
    },
    {
      "index": 128,
      "title": "“McDRAM: Low latency and energy-efficient matrix computations in DRAM",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight Convolutional Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Thanks for Nothing: Predicting Zero-valued Activations with Lightweight Convolutional Neural Networks",
      "paper_id": "1909.07636v3"
    },
    {
      "index": 130,
      "title": "“PipeLayer: A Pipelined ReRAM-based Accelerator for Deep Learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 131,
      "title": "GraphR: Accelerating Graph Processing Using ReRAM",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“GraphR: Accelerating Graph Processing using ReRAM",
      "paper_id": "1708.06248v4"
    },
    {
      "index": 132,
      "title": "“General-Purpose Code Acceleration with Limited-Precision Analog Computation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "“Memristor Crossbar Architecture for Synchronous Neural Networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "“Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "“Flash ADC architecture",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "D. Brooks et al",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "Long Range Arena: A Benchmark for Efficient Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Long Range Arena: A Benchmark for Efficient Transformers",
      "paper_id": "2011.04006v1"
    },
    {
      "index": 138,
      "title": "“ADC performance survey 1997-2016",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 140,
      "title": "“A 74 TMACS/W CMOS-RRAM Neurosynaptic Core with Dynamically Reconfigurable Dataflow and In-situ Transposable Weights for Probabilistic Graphical Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“Edge AI without Compromise: Efficient",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "paper_id": "1804.07461v3"
    },
    {
      "index": 143,
      "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning",
      "paper_id": "2012.09852v3"
    },
    {
      "index": 144,
      "title": "“Rerec: In-ReRAM Acceleration with Access-Aware Mapping for Personalized Recommendation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "Structured Pruning of Large Language Models",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Structured Pruning of Large Language Models",
      "paper_id": "1910.04732v2"
    },
    {
      "index": 146,
      "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning Intrinsic Sparse Structures within Long Short-Term Memory",
      "paper_id": "1709.05027v7"
    },
    {
      "index": 147,
      "title": "M. Funtowicz et al",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "“A Study on Low-Power",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“PIMGCN: A ReRAM-Based PIM Design for Graph Convolutional Network Acceleration",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“Face Classification Using Electronic Synapses",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "“FlexiGAN: An End-to-End Solution for FPGA Acceleration of Generative Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "“GANAX: A Unified SIMD-MIMD Acceleration for Generative Adversarial Network",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "“Neural Acceleration for GPU Throughput Processors",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks",
      "paper_id": "2102.10423v2"
    },
    {
      "index": 155,
      "title": "“In-DRAM Near-Data Approximate Acceleration for GPUs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference",
      "paper_id": "2105.11618v1"
    },
    {
      "index": 157,
      "title": "“BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 158,
      "title": "“XNOR-SRAM: In-Memory Computing SRAM Macro for Binary/Ternary Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Investigating the Switching Dynamics and Multilevel Capability of Bipolar Metal Oxide Resistive Switching Memory",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator",
      "paper_id": "2106.09144v1"
    },
    {
      "index": 161,
      "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“GOBO: Quantizing Attention-based NLP Models for Low Latency and Energy Efficient Inference",
      "paper_id": "2005.03842v2"
    },
    {
      "index": 162,
      "title": "“Big Bird: Transformers for Longer Sequences",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "“TOP-PIM: Throughput-Oriented Programmable Processing in Memory",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "“SpongeDirectory: Flexible Sparse Directories Utilizing Multi-Level Memristors",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "“GraphP: Reducing Communication for PIM-based Graph Processing with Efficient Data Partition",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "“Cambricon-X: An Accelerator for Sparse Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Explicit Sparse Transformer: Concentrated Attention through Explicit Selection",
      "paper_id": "1912.11637v1"
    },
    {
      "index": 168,
      "title": "“GraphQ: Scalable PIM-based Graph Processing",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    }
  ]
}