{
  "paper_id": "2112.05848v3",
  "title": "Faster Deep Reinforcement Learning with Slower Online Network",
  "abstract": "Abstract\nDeep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Wasserstein generative adversarial networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Arjovsky, M., Chintala, S., and Bottou, L."
    },
    {
      "index": 1,
      "title": "Lipschitz Continuity in Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Asadi et al.",
      "orig_title": "Lipschitz continuity in model-based reinforcement learning",
      "paper_id": "1804.07193v3"
    },
    {
      "index": 2,
      "title": "Deep radial-basis value functions for continuous control",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Asadi, K., Parikh, N., Parr, R. E., Konidaris, G. D., and Littman, M. L."
    },
    {
      "index": 3,
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences",
      "authors": "Beck, A. and Teboulle, M."
    },
    {
      "index": 4,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M."
    },
    {
      "index": 5,
      "title": "Dynamic Programming",
      "abstract": "",
      "year": "1957",
      "venue": "",
      "authors": "Bellman, R. E."
    },
    {
      "index": 6,
      "title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
      "abstract": "",
      "year": "2011",
      "venue": "Optimization for Machine Learning",
      "authors": "Bertsekas, D. P."
    },
    {
      "index": 7,
      "title": "Incremental proximal methods for large scale convex optimization",
      "abstract": "",
      "year": "2011",
      "venue": "Mathematical Programming",
      "authors": "Bertsekas, D. P."
    },
    {
      "index": 8,
      "title": "Neuro-dynamic programming",
      "abstract": "",
      "year": "1996",
      "venue": "Athena Scientific",
      "authors": "Bertsekas, D. P. and Tsitsiklis, J. N."
    },
    {
      "index": 9,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 10,
      "title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G.",
      "orig_title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "paper_id": "1812.06110v1"
    },
    {
      "index": 11,
      "title": "Proximal Splitting Methods in Signal Processing",
      "abstract": "",
      "year": "2009",
      "venue": "Fixed-point algorithms for inverse problems in science and engineering",
      "authors": "Combettes, P. L. and Pesquet, J.-C."
    },
    {
      "index": 12,
      "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Dabney, W., Ostrovski, G., Silver, D., and Munos, R.",
      "orig_title": "Implicit quantile networks for distributional reinforcement learning",
      "paper_id": "1806.06923v1"
    },
    {
      "index": 13,
      "title": "P3O: Policy-on Policy-off Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": "Fakoor, R., Chaudhari, P., and Smola, A. J.",
      "orig_title": "P3O: Policy-on policy-off policy optimization",
      "paper_id": "1905.01756v2"
    },
    {
      "index": 14,
      "title": "Meta-Q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Fakoor, R., Chaudhari, P., Soatto, S., and Smola, A. J."
    },
    {
      "index": 15,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Fujimoto, S., Hoof, H., and Meger, D.",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 16,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D.",
      "orig_title": "Rainbow: Combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 17,
      "title": "Dynamic programming and markov processes",
      "abstract": "",
      "year": "1960",
      "venue": "",
      "authors": "Howard, R. A."
    },
    {
      "index": 18,
      "title": "Deepmellow: removing the need for a target network in deep q-learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Kim, S., Asadi, K., Littman, M., and Konidaris, G."
    },
    {
      "index": 19,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Kingma, D. P. and Ba, J."
    },
    {
      "index": 20,
      "title": "Natural Gradient Deep Q-learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.07482",
      "authors": "Knight, E. and Lerner, O.",
      "orig_title": "Natural gradient deep q-learning",
      "paper_id": "1803.07482v2"
    },
    {
      "index": 21,
      "title": "Reinforcement learning in robotics: A survey",
      "abstract": "",
      "year": "2013",
      "venue": "International Journal of Robotics Research",
      "authors": "Kober, J., Bagnell, J. A., and Peters, J."
    },
    {
      "index": 22,
      "title": "Target-Based Temporal-Difference Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Lee, D. and He, N.",
      "orig_title": "Target-based temporal-difference learning",
      "paper_id": "1904.10945v3"
    },
    {
      "index": 23,
      "title": "Accelerated proximal gradient methods for nonconvex programming",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Li, H. and Lin, Z."
    },
    {
      "index": 24,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D."
    },
    {
      "index": 25,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Lin, L.-J."
    },
    {
      "index": 26,
      "title": "A generalized reinforcement-learning model: Convergence and applications",
      "abstract": "",
      "year": "1996",
      "venue": "ICML",
      "authors": "Littman, M. L. and Szepesvári, C."
    },
    {
      "index": 27,
      "title": "Finite-Sample Analysis of Proximal Gradient TD Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": "Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M.",
      "orig_title": "Finite-sample analysis of proximal gradient TD algorithms",
      "paper_id": "2006.14364v2"
    },
    {
      "index": 28,
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M.",
      "orig_title": "Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents",
      "paper_id": "1709.06009v2"
    },
    {
      "index": 29,
      "title": "Proximal Deterministic Policy Gradient",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Intelligent Robots and Systems",
      "authors": "Maggipinto, M., Susto, G. A., and Chaudhari, P.",
      "orig_title": "Proximal deterministic policy gradient",
      "paper_id": "2008.00759v1"
    },
    {
      "index": 30,
      "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv",
      "authors": "Mahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., and Liu, J."
    },
    {
      "index": 31,
      "title": "Regularisation, d’inéquations variationelles par approximations succesives",
      "abstract": "",
      "year": "1970",
      "venue": "Revue Francaise d’informatique et de Recherche operationelle",
      "authors": "Martinet, B."
    },
    {
      "index": 32,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al."
    },
    {
      "index": 33,
      "title": "Fonctions convexes duales et points proximaux dans un espace hilbertien",
      "abstract": "",
      "year": "1962",
      "venue": "Comptes rendus hebdomadaires des séances de l’Académie des sciences",
      "authors": "Moreau, J. J."
    },
    {
      "index": 34,
      "title": "Proximité et dualité dans un espace hilbertien",
      "abstract": "",
      "year": "1965",
      "venue": "Bulletin de la Société Mathématique de France",
      "authors": "Moreau, J. J."
    },
    {
      "index": 35,
      "title": "Massively Parallel Methods for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1507.04296",
      "authors": "Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., et al.",
      "orig_title": "Massively parallel methods for deep reinforcement learning",
      "paper_id": "1507.04296v2"
    },
    {
      "index": 36,
      "title": "A method for unconstrained convex minimization problem with the rate of convergence o (1/k^ 2)",
      "abstract": "",
      "year": "1983",
      "venue": "Doklady an USSR",
      "authors": "Nesterov, Y."
    },
    {
      "index": 37,
      "title": "Norm-based capacity control in neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "COLT",
      "authors": "Neyshabur et al."
    },
    {
      "index": 38,
      "title": "proximal algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "Foundations and Trends in optimization",
      "authors": "Parikh, N. and Boyd, S."
    },
    {
      "index": 39,
      "title": "Proximal algorithms in statistics and machine learning",
      "abstract": "",
      "year": "2015",
      "venue": "Statistical Science",
      "authors": "Polson, N. G., Scott, J. G., and Willard, B. T."
    },
    {
      "index": 40,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": "Puterman, M. L."
    },
    {
      "index": 41,
      "title": "Doubly robust covariate shift correction",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Reddi, S., Poczos, B., and Smola, A."
    },
    {
      "index": 42,
      "title": "Monotone operators and the proximal point algorithm",
      "abstract": "",
      "year": "1976",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Rockafellar, R. T."
    },
    {
      "index": 43,
      "title": "Approximate modified policy iteration and its application to the game of tetris",
      "abstract": "",
      "year": "2015",
      "venue": "J. Mach. Learn. Res.",
      "authors": "Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M."
    },
    {
      "index": 44,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 45,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O."
    },
    {
      "index": 46,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al."
    },
    {
      "index": 47,
      "title": "On the convergence of smooth regularized approximate value iteration schemes",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Smirnova, E. and Dohmatob, E."
    },
    {
      "index": 48,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "Machine learning",
      "authors": "Sutton, R. S."
    },
    {
      "index": 49,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Sutton, R. S. and Barto, A. G."
    },
    {
      "index": 50,
      "title": "A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sutton, R. S., Szepesvári, C., and Maei, H. R."
    },
    {
      "index": 51,
      "title": "TD-gammon, a self-teaching backgammon program, achieves master-level play",
      "abstract": "",
      "year": "1994",
      "venue": "Neural computation",
      "authors": "Tesauro, G."
    },
    {
      "index": 52,
      "title": "Mirror descent policy optimization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M."
    },
    {
      "index": 53,
      "title": "Deep Reinforcement Learning and the Deadly Triad",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J.",
      "orig_title": "Deep reinforcement learning and the deadly triad",
      "paper_id": "1812.02648v1"
    },
    {
      "index": 54,
      "title": "Using a logarithmic mapping to enable lower discount factors in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "van Seijen, H., Fatemi, M., and Tavakoli, A."
    },
    {
      "index": 55,
      "title": "Trust region-guided proximal policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wang, Y., He, H., Tan, X., and Gan, Y."
    },
    {
      "index": 56,
      "title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N.",
      "orig_title": "Dueling network architectures for deep reinforcement learning",
      "paper_id": "1511.06581v3"
    },
    {
      "index": 57,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Watkins, C. J. and Dayan, P."
    },
    {
      "index": 58,
      "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Association for Computational Linguistics",
      "authors": "Williams, J. D., Asadi, K., and Zweig, G.",
      "orig_title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
      "paper_id": "1702.03274v2"
    },
    {
      "index": 59,
      "title": "Breaking the deadly triad with a target network",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Zhang, S., Yao, H., and Whiteson, S."
    }
  ]
}