{
  "paper_id": "2401.06805v2",
  "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
  "abstract": "Abstract\nStrong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI.\nRecent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications.\nParticularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks.\nThese studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs.\nHowever, the reasoning abilities of MLLMs have not been systematically investigated.\nIn this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions.\nWe believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Minds, brains, and programs",
      "abstract": "",
      "year": "1980",
      "venue": "Behavioral and brain sciences",
      "authors": "John R Searle"
    },
    {
      "index": 1,
      "title": "Artificial general intelligence, volume 2",
      "abstract": "",
      "year": "2007",
      "venue": "Springer",
      "authors": "Ben Goertzel and Cassio Pennachin"
    },
    {
      "index": 2,
      "title": "In two minds: dual-process accounts of reasoning",
      "abstract": "",
      "year": "2003",
      "venue": "Trends in cognitive sciences",
      "authors": "Jonathan St BT Evans"
    },
    {
      "index": 3,
      "title": "Dual-process theories of higher cognition: Advancing the debate",
      "abstract": "",
      "year": "2013",
      "venue": "Perspectives on psychological science",
      "authors": "Jonathan St BT Evans and Keith E Stanovich"
    },
    {
      "index": 4,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 5,
      "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/cvf conference on computer vision and pattern recognition",
      "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi",
      "orig_title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
      "paper_id": "1906.00067v2"
    },
    {
      "index": 6,
      "title": "Scienceqa: A novel resource for question answering on scholarly articles",
      "abstract": "",
      "year": "2022",
      "venue": "International Journal on Digital Libraries",
      "authors": "Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya"
    },
    {
      "index": 7,
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "Drew A Hudson and Christopher D Manning"
    },
    {
      "index": 8,
      "title": "TouchStone: Evaluating Vision-Language Models by Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jingren Zhou",
      "orig_title": "Touchstone: Evaluating vision-language models by language models",
      "paper_id": "2308.16890v2"
    },
    {
      "index": 9,
      "title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Shengzhi Li and Nima Tajbakhsh",
      "orig_title": "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
      "paper_id": "2308.03349v1"
    },
    {
      "index": 10,
      "title": "Sparkles: Unlocking chats across multiple images for multimodal instruction-following models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2308.16463",
      "authors": "Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Collier Nigel, and Yutong Lu"
    },
    {
      "index": 11,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan"
    },
    {
      "index": 12,
      "title": "Visual Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee",
      "orig_title": "Visual instruction tuning",
      "paper_id": "2304.08485v2"
    },
    {
      "index": 13,
      "title": "Empowering vision-language models to follow interleaved vision-language instructions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2308.04152",
      "authors": "Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang"
    },
    {
      "index": 14,
      "title": "Multimodal Few-Shot Learning with Frozen Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill",
      "orig_title": "Multimodal few-shot learning with frozen language models",
      "paper_id": "2106.13884v2"
    },
    {
      "index": 15,
      "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.00598",
      "authors": "Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al."
    },
    {
      "index": 16,
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.15818",
      "authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al.",
      "orig_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "paper_id": "2307.15818v1"
    },
    {
      "index": 17,
      "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.18565",
      "authors": "Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al.",
      "orig_title": "Pali-x: On scaling up a multilingual vision and language model",
      "paper_id": "2305.18565v1"
    },
    {
      "index": 18,
      "title": "Palm-e: An embodied multimodal language model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence"
    },
    {
      "index": 19,
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan",
      "orig_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "paper_id": "2303.04671v1"
    },
    {
      "index": 20,
      "title": "MM-ReAct : Prompting ChatGPT for Multimodal Reasoning and Action",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang",
      "orig_title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "paper_id": "2303.11381v1"
    },
    {
      "index": 21,
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi",
      "orig_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "paper_id": "2301.12597v3"
    },
    {
      "index": 22,
      "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao",
      "orig_title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "paper_id": "2303.16199v3"
    },
    {
      "index": 23,
      "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei",
      "orig_title": "Kosmos-2: Grounding multimodal large language models to the world",
      "paper_id": "2306.14824v3"
    },
    {
      "index": 24,
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang"
    },
    {
      "index": 25,
      "title": "Mathematical Capabilities of ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.13867",
      "authors": "Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner",
      "orig_title": "Mathematical capabilities of chatgpt",
      "paper_id": "2301.13867v2"
    },
    {
      "index": 26,
      "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.04023",
      "authors": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.",
      "orig_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "paper_id": "2302.04023v4"
    },
    {
      "index": 27,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "authors": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung",
      "orig_title": "Survey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 28,
      "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.08896",
      "authors": "Potsawee Manakul, Adian Liusie, and Mark JF Gales",
      "orig_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "paper_id": "2303.08896v3"
    },
    {
      "index": 29,
      "title": "Natural Language Reasoning, A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.14725",
      "authors": "Fei Yu, Hongbo Zhang, and Benyou Wang",
      "orig_title": "Nature language reasoning, a survey",
      "paper_id": "2303.14725v2"
    },
    {
      "index": 30,
      "title": "Towards Reasoning in Large Language Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.10403",
      "authors": "Jie Huang and Kevin Chen-Chuan Chang",
      "orig_title": "Towards reasoning in large language models: A survey",
      "paper_id": "2212.10403v2"
    },
    {
      "index": 31,
      "title": "What is reasoning? what is an argument?",
      "abstract": "",
      "year": "1990",
      "venue": "The journal of Philosophy",
      "authors": "Douglas N Walton"
    },
    {
      "index": 32,
      "title": "A concise introduction to logic",
      "abstract": "",
      "year": "2017",
      "venue": "Open SUNY Textbooks",
      "authors": "Craig DeLancey"
    },
    {
      "index": 33,
      "title": "Informal logic and the theory of reasoning",
      "abstract": "",
      "year": "1984",
      "venue": "Informal Logic",
      "authors": "Maurice A Finocchiaro"
    },
    {
      "index": 34,
      "title": "Reasoning and the logic of things: The cambridge conferences lectures of 1898",
      "abstract": "",
      "year": "1994",
      "venue": "The Philosophical Review",
      "authors": "Edward H Madden"
    },
    {
      "index": 35,
      "title": "Logical reasoning in formal and everyday reasoning tasks",
      "abstract": "",
      "year": "2020",
      "venue": "International Journal of Science and Mathematics Education",
      "authors": "Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, and Martin Goedhart"
    },
    {
      "index": 36,
      "title": "Logical reasoning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Bradley H Dowden"
    },
    {
      "index": 37,
      "title": "Admissibility of logical inference rules",
      "abstract": "",
      "year": "1997",
      "venue": "Elsevier",
      "authors": "Vladimir V Rybakov"
    },
    {
      "index": 38,
      "title": "Belief rule-base inference methodology using the evidential reasoning approach-rimer",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Transactions on systems, Man, and Cybernetics-part A: Systems and Humans",
      "authors": "Jian-Bo Yang, Jun Liu, Jin Wang, How-Sing Sii, and Hong-Wei Wang"
    },
    {
      "index": 39,
      "title": "Deductive reasoning",
      "abstract": "",
      "year": "1999",
      "venue": "Annual review of psychology",
      "authors": "Philip N Johnson-Laird"
    },
    {
      "index": 40,
      "title": "Inductive reasoning and kolmogorov complexity",
      "abstract": "",
      "year": "1992",
      "venue": "Journal of Computer and System Sciences",
      "authors": "Ming Li and Paul MB Vitanyi"
    },
    {
      "index": 41,
      "title": "Abduction",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Igor Douven"
    },
    {
      "index": 42,
      "title": "Analogical reasoning: What develops? a review of research and theory",
      "abstract": "",
      "year": "1991",
      "venue": "Child development",
      "authors": "Usha Goswami"
    },
    {
      "index": 43,
      "title": "Analogy and analogical reasoning",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Paul Bartha"
    },
    {
      "index": 44,
      "title": "Introduction to logic and critical thinking",
      "abstract": "",
      "year": "2012",
      "venue": "Cengage Learning",
      "authors": "Merrilee H Salmon"
    },
    {
      "index": 45,
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.14168",
      "authors": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.",
      "orig_title": "Training verifiers to solve math word problems",
      "paper_id": "2110.14168v2"
    },
    {
      "index": 46,
      "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.07191",
      "authors": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal",
      "orig_title": "Are nlp models really able to solve simple math word problems?",
      "paper_id": "2103.07191v2"
    },
    {
      "index": 47,
      "title": "A diverse corpus for evaluating and developing English math word problem solvers",
      "abstract": "",
      "year": "2020",
      "venue": "58th Annual Meeting of the Association for Computational Linguistics",
      "authors": "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su"
    },
    {
      "index": 48,
      "title": "MAWPS: A math word problem repository",
      "abstract": "",
      "year": "2016",
      "venue": "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi"
    },
    {
      "index": 49,
      "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1905.13319",
      "authors": "Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi",
      "orig_title": "Mathqa: Towards interpretable math word problem solving with operation-based formalisms",
      "paper_id": "1905.13319v1"
    },
    {
      "index": 50,
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.",
      "orig_title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "paper_id": "1705.04146v3"
    },
    {
      "index": 51,
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric\nTang, Dawn Song, and Jacob Steinhardt.",
      "orig_title": "Measuring mathematical problem solving with the math dataset",
      "paper_id": "2103.03874v2"
    },
    {
      "index": 52,
      "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics",
      "authors": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",
      "orig_title": "Hellaswag: Can a machine really finish your sentence?",
      "paper_id": "1905.07830v1"
    },
    {
      "index": 53,
      "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34",
      "authors": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.",
      "orig_title": "Winogrande: An adversarial winograd schema challenge at scale",
      "paper_id": "1907.10641v2"
    },
    {
      "index": 54,
      "title": "Social iqa: Commonsense reasoning about social interactions",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP)",
      "authors": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi."
    },
    {
      "index": 55,
      "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial\nintelligence, volume 34",
      "authors": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.",
      "orig_title": "Piqa: Reasoning about physical commonsense in natural language",
      "paper_id": "1911.11641v1"
    },
    {
      "index": 56,
      "title": "Commongen: A constrained text generation challenge for generative commonsense reasoning",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics:\nEMNLP",
      "authors": "Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula,\nYejin Choi, and Xiang Ren."
    },
    {
      "index": 57,
      "title": "Cosmos qa: Machine reading comprehension with contextual commonsense reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP)",
      "authors": "Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi."
    },
    {
      "index": 58,
      "title": "Abductive Commonsense Reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari\nHoltzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi.",
      "orig_title": "Abductive commonsense reasoning",
      "paper_id": "1908.05739v2"
    },
    {
      "index": 59,
      "title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
      "abstract": "",
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning\nRepresentations",
      "authors": "Abulhair Saparov and He He."
    },
    {
      "index": 60,
      "title": "On the Paradox of Learning to Reason from Data",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den\nBroeck.",
      "orig_title": "On the paradox of learning to reason from data",
      "paper_id": "2205.11502v2"
    },
    {
      "index": 61,
      "title": "Folio: Natural language reasoning with first-order logic",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke\nBenson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al."
    },
    {
      "index": 62,
      "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
      "abstract": "",
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics:\nACL-IJCNLP",
      "authors": "Oyvind Tafjord, Bhavana Dalvi, and Peter Clark.",
      "orig_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
      "paper_id": "2012.13048v2"
    },
    {
      "index": 63,
      "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay,\nHyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al."
    },
    {
      "index": 64,
      "title": "TextWorld: A Learning Environment for Text-based Games",
      "abstract": "",
      "year": "2019",
      "venue": "Computer Games: 7th Workshop, CGW 2018, Held in Conjunction\nwith the 27th International Conference on Artificial Intelligence, IJCAI",
      "authors": "Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,\nTavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri,\nMahmoud Adada, et al.",
      "orig_title": "Textworld: A learning environment for text-based games",
      "paper_id": "1806.11532v2"
    },
    {
      "index": 65,
      "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam\nTrischler, and Matthew Hausknecht.",
      "orig_title": "Alfworld: Aligning text and embodied environments for interactive learning",
      "paper_id": "2010.03768v2"
    },
    {
      "index": 66,
      "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.",
      "orig_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
      "paper_id": "2207.01206v4"
    },
    {
      "index": 67,
      "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,\nChitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio.",
      "orig_title": "Babyai: A platform to study the sample efficiency of grounded language learning",
      "paper_id": "1810.08272v4"
    },
    {
      "index": 68,
      "title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,\nAndrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar."
    },
    {
      "index": 69,
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron\nDavid, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,\net al.",
      "orig_title": "Do as i can, not as i say: Grounding language in robotic affordances",
      "paper_id": "2204.01691v2"
    },
    {
      "index": 70,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision–ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755.\nSpringer",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Dollár, and C Lawrence Zitnick.",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 71,
      "title": "Nocaps: Novel object captioning at scale",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on\ncomputer vision",
      "authors": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark\nJohnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson."
    },
    {
      "index": 72,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer\nvision",
      "authors": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia\nHockenmaier, and Svetlana Lazebnik.",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 73,
      "title": "Infimm-eval: Complex open-ended reasoning evaluation for multi-modal large language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil\nMrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, Heng Wang, and Hongxia\nYang."
    },
    {
      "index": 74,
      "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu,\nXinchao Wang, and Lijuan Wang."
    },
    {
      "index": 75,
      "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel\nStevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao\nHuang, Huan Sun, Yu Su, and Wenhu Chen."
    },
    {
      "index": 76,
      "title": "Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun\nWang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou."
    },
    {
      "index": 77,
      "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint Link",
      "authors": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh\nHajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.",
      "orig_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
      "paper_id": "2310.02255v3"
    },
    {
      "index": 78,
      "title": "mplug-docowl: Modularized multimodal large language model for document understanding, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao,\nGuohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang."
    },
    {
      "index": 79,
      "title": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv",
      "authors": "Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang,\nTianyu Liu, and Baobao Chang.",
      "orig_title": "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
      "paper_id": "2310.02071v4"
    },
    {
      "index": 80,
      "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong\nBing.",
      "orig_title": "M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
      "paper_id": "2306.05179v2"
    },
    {
      "index": 81,
      "title": "Mmbench: Is your multi-modal model an all-around player?, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike\nYuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin."
    },
    {
      "index": 82,
      "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin,\nZhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al.",
      "orig_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models",
      "paper_id": "2306.13394v4"
    },
    {
      "index": 83,
      "title": "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li,\nLu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang."
    },
    {
      "index": 84,
      "title": "Halle-switch: Controlling object hallucination in large vision language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li."
    },
    {
      "index": 85,
      "title": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas\nAwadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt."
    },
    {
      "index": 86,
      "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan."
    },
    {
      "index": 87,
      "title": "Tiny lvlm-ehub: Early multimodal experiments with bard, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu,\nSiyuan Huang, Hongsheng Li, Yu Qiao, and Ping Luo."
    },
    {
      "index": 88,
      "title": "What matters in training a gpt4-style language model with multimodal inputs?, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei,\nYuchen Zhang, and Tao Kong."
    },
    {
      "index": 89,
      "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,\nOyvind Tafjord, Peter Clark, and Ashwin Kalyan.",
      "orig_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "paper_id": "2209.09513v2"
    },
    {
      "index": 90,
      "title": "Winoground: Probing vision and language models for visio-linguistic compositionality, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe\nKiela, and Candace Ross."
    },
    {
      "index": 91,
      "title": "RAVEN: A Dataset for Relational and Analogical Visual rEasoNing",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR)",
      "authors": "Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.",
      "orig_title": "Raven: A dataset for relational and analogical visual reasoning",
      "paper_id": "1903.02741v1"
    },
    {
      "index": 92,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern\nRecognition",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei."
    },
    {
      "index": 93,
      "title": "Microsoft coco: Common objects in context, 2015",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,\nJames Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr\nDollár."
    },
    {
      "index": 94,
      "title": "Lvis: A dataset for large vocabulary instance segmentation, 2019",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Agrim Gupta, Piotr Dollár, and Ross Girshick."
    },
    {
      "index": 95,
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for\nComputational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA,\nJuly",
      "authors": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu."
    },
    {
      "index": 96,
      "title": "Cider: Consensus-based image description evaluation, 2015",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh."
    },
    {
      "index": 97,
      "title": "ROUGE: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "Text Summarization Branches Out, pages 74–81, Barcelona,\nSpain, July",
      "authors": "Chin-Yew Lin."
    },
    {
      "index": 98,
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.\nGonzalez, and Ion Stoica."
    },
    {
      "index": 99,
      "title": "Gpt-4 technical report, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI."
    },
    {
      "index": 100,
      "title": "Fool your (vision and) language model with embarrassingly simple permutations, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, and Timothy\nHospedales."
    },
    {
      "index": 101,
      "title": "Solving Quantitative Reasoning Problems with Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\nMichalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al.",
      "orig_title": "Solving quantitative reasoning problems with language models",
      "paper_id": "2206.14858v2"
    },
    {
      "index": 102,
      "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.",
      "orig_title": "Explain yourself! leveraging language models for commonsense reasoning",
      "paper_id": "1906.02361v1"
    },
    {
      "index": 103,
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\nLe, Denny Zhou, et al."
    },
    {
      "index": 104,
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi\nWang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.",
      "orig_title": "Least-to-most prompting enables complex reasoning in large language models",
      "paper_id": "2205.10625v3"
    },
    {
      "index": 105,
      "title": "Large Language Models are Zero-Shot Reasoners",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in neural information processing systems,\n35:",
      "authors": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa.",
      "orig_title": "Large language models are zero-shot reasoners",
      "paper_id": "2205.11916v4"
    },
    {
      "index": 106,
      "title": "Complexity-Based Prompting for Multi-step Reasoning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot.",
      "orig_title": "Complexity-based prompting for multi-step reasoning",
      "paper_id": "2210.00720v2"
    },
    {
      "index": 107,
      "title": "Automatic chain of thought prompting in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola."
    },
    {
      "index": 108,
      "title": "Pal: Program-aided language models",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie\nCallan, and Graham Neubig."
    },
    {
      "index": 109,
      "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen."
    },
    {
      "index": 110,
      "title": "React: Synergizing reasoning and acting in language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao."
    },
    {
      "index": 111,
      "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang\nLin, Chang Zhou, and Jingren Zhou."
    },
    {
      "index": 112,
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
      "orig_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "paper_id": "2304.10592v2"
    },
    {
      "index": 113,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33",
      "authors": "",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 114,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al."
    },
    {
      "index": 115,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 116,
      "title": "Bloom: A 176b-parameter open-access multilingual language model, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,\nDaniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al."
    },
    {
      "index": 117,
      "title": "Training Compute-Optimal Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al.",
      "orig_title": "Training compute-optimal large language models",
      "paper_id": "2203.15556v1"
    },
    {
      "index": 118,
      "title": "Llama: Open and efficient foundation language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and\nGuillaume Lample."
    },
    {
      "index": 119,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.",
      "orig_title": "Opt: Open pre-trained transformer language models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 120,
      "title": "GLM: General language model pretraining with autoregressive blank infilling",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 320–335, Dublin,\nIreland, May",
      "authors": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang."
    },
    {
      "index": 121,
      "title": "Stanford alpaca: An instruction-following llama model",
      "abstract": "",
      "year": "2023",
      "venue": "https://github.com/tatsu-lab/stanford_alpaca",
      "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto."
    },
    {
      "index": 122,
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing."
    },
    {
      "index": 123,
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi."
    },
    {
      "index": 124,
      "title": "Improved Baselines with Visual Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.",
      "orig_title": "Improved baselines with visual instruction tuning",
      "paper_id": "2310.03744v2"
    },
    {
      "index": 125,
      "title": "Otter: A multi-modal model with in-context instruction tuning, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu."
    },
    {
      "index": 126,
      "title": "Cogvlm: Visual expert for pretrained language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji,\nZhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao\nDong, Ming Ding, and Jie Tang."
    },
    {
      "index": 127,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems,\n33:",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 128,
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.",
      "orig_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "paper_id": "2204.05862v1"
    },
    {
      "index": 129,
      "title": "Evaluating Large Language Models Trained on Code",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al.",
      "orig_title": "Evaluating large language models trained on code",
      "paper_id": "2107.03374v2"
    },
    {
      "index": 130,
      "title": "Finetuned Language Models Are Zero-Shot Learners",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le.",
      "orig_title": "Finetuned language models are zero-shot learners",
      "paper_id": "2109.01652v5"
    },
    {
      "index": 131,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International conference on machine learning",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 132,
      "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross\nWightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, et al.",
      "orig_title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
      "paper_id": "2210.08402v1"
    },
    {
      "index": 133,
      "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex\nFang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi.",
      "orig_title": "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
      "paper_id": "2304.06939v3"
    },
    {
      "index": 134,
      "title": "Instruction Tuning with GPT-4",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.",
      "orig_title": "Instruction tuning with gpt-4",
      "paper_id": "2304.03277v1"
    },
    {
      "index": 135,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)",
      "authors": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut."
    },
    {
      "index": 136,
      "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.",
      "orig_title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
      "paper_id": "2102.08981v2"
    },
    {
      "index": 137,
      "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,\nClayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran\nKomatsuzaki."
    },
    {
      "index": 138,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "International journal of computer vision, 123:32–73",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua\nKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al."
    },
    {
      "index": 139,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 140,
      "title": "A-okvqa: A benchmark for visual question answering using world knowledge",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision, pages 146–162.\nSpringer",
      "authors": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and\nRoozbeh Mottaghi."
    },
    {
      "index": 141,
      "title": "Ocr-vqa: Visual question answering by reading text in images",
      "abstract": "",
      "year": "2019",
      "venue": "2019 international conference on document analysis and\nrecognition (ICDAR)",
      "authors": "Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty."
    },
    {
      "index": 142,
      "title": "Textcaps: a dataset for image captioning with reading comprehension",
      "abstract": "",
      "year": "2020",
      "venue": "Computer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758.\nSpringer",
      "authors": "Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh."
    },
    {
      "index": 143,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 144,
      "title": "Coyo-700m: Image-text pair dataset",
      "abstract": "",
      "year": "2022",
      "venue": "https://github.com/kakaobrain/coyo-dataset",
      "authors": "Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and\nSaehoon Kim."
    },
    {
      "index": 145,
      "title": "DataComp: In search of the next generation of multimodal datasets",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios\nSmyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu\nZhang, et al.",
      "orig_title": "Datacomp: In search of the next generation of multimodal datasets",
      "paper_id": "2304.14108v5"
    },
    {
      "index": 146,
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP)",
      "authors": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg."
    },
    {
      "index": 147,
      "title": "Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet\nVong, and \"Teknium\"."
    },
    {
      "index": 148,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in neural information processing systems, 24",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg."
    },
    {
      "index": 149,
      "title": "Generation and comprehension of unambiguous object descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and\nKevin Murphy."
    },
    {
      "index": 150,
      "title": "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and\nTong Sun."
    },
    {
      "index": 151,
      "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.",
      "orig_title": "Mitigating hallucination in large multi-modal models via robust instruction tuning",
      "paper_id": "2306.14565v4"
    },
    {
      "index": 152,
      "title": "DVQA: Understanding Data Visualizations via Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.",
      "orig_title": "Dvqa: Understanding data visualizations via question answering",
      "paper_id": "1801.08163v2"
    },
    {
      "index": 153,
      "title": "DocVQA: A Dataset for VQA on Document Images",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications\nof computer vision",
      "authors": "Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.",
      "orig_title": "Docvqa: A dataset for vqa on document images",
      "paper_id": "2007.00398v3"
    },
    {
      "index": 154,
      "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.",
      "orig_title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
      "paper_id": "2203.10244v1"
    },
    {
      "index": 155,
      "title": "Ai2d-rst: A multimodal corpus of 1000 primary school science diagrams",
      "abstract": "",
      "year": "2021",
      "venue": "Language Resources and Evaluation, 55:661–688",
      "authors": "Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya\nLogacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John A\nBateman."
    },
    {
      "index": 156,
      "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu,\nKalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev,\nSimon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig\nSchmidt.",
      "orig_title": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
      "paper_id": "2308.01390v2"
    },
    {
      "index": 157,
      "title": "Introducing idefics: An open reproduction of state-of-the-art visual language model, August 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier,\nThomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine\nJernite, and Victor Sanh."
    },
    {
      "index": 158,
      "title": "Mimic-it: Multi-modal in-context instruction tuning, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang,\nChunyuan Li, and Ziwei Liu."
    },
    {
      "index": 159,
      "title": "Mmicl: Empowering vision-language model with multi-modal in-context learning, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan\nLiu, Sheng Wang, Wenjuan Han, and Baobao Chang."
    },
    {
      "index": 160,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of learning and motivation, volume 24",
      "authors": "Michael McCloskey and Neal J Cohen."
    },
    {
      "index": 161,
      "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
      "abstract": "",
      "year": "2023",
      "venue": "ICML",
      "authors": "Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.",
      "orig_title": "Grounding language models to images for multimodal inputs and outputs",
      "paper_id": "2301.13823v4"
    },
    {
      "index": 162,
      "title": "A survey for in-context learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\nJingjing Xu, and Zhifang Sui."
    },
    {
      "index": 163,
      "title": "What Makes Good In-Context Examples for GPT-3?",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The\n3rd Workshop on Knowledge Extraction and Integration for Deep Learning\nArchitectures, pages 100–114, Dublin, Ireland and Online, May 2022.\nAssociation for Computational Linguistics",
      "authors": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu\nChen.",
      "orig_title": "What makes good in-context examples for GPT-3?",
      "paper_id": "2101.06804v1"
    },
    {
      "index": 164,
      "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin,\nIreland, May",
      "authors": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.",
      "orig_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "paper_id": "2104.08786v2"
    },
    {
      "index": 165,
      "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1423–1436,\nToronto, Canada, July",
      "authors": "Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.",
      "orig_title": "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
      "paper_id": "2212.10375v2"
    },
    {
      "index": 166,
      "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng\nTao, and Steven Hoi.",
      "orig_title": "From images to textual prompts: Zero-shot visual question answering with frozen large language models",
      "paper_id": "2212.10846v3"
    },
    {
      "index": 167,
      "title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin,\nShuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al.",
      "orig_title": "Language models with image descriptors are strong few-shot video-language learners",
      "paper_id": "2205.10747v4"
    },
    {
      "index": 168,
      "title": "A Survey of Embodied AI: From Simulators to Research Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Emerging Topics in Computational\nIntelligence, 6(2):230–244",
      "authors": "Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan.",
      "orig_title": "A survey of embodied ai: From simulators to research tasks",
      "paper_id": "2103.04918v8"
    },
    {
      "index": 169,
      "title": "Habitat: A Platform for Embodied AI Research",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on\ncomputer vision",
      "authors": "Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,\nBhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al.",
      "orig_title": "Habitat: A platform for embodied ai research",
      "paper_id": "1904.01201v2"
    },
    {
      "index": 170,
      "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Robot Learning",
      "authors": "Mohit Shridhar, Lucas Manuelli, and Dieter Fox.",
      "orig_title": "Cliport: What and where pathways for robotic manipulation",
      "paper_id": "2109.12098v1"
    },
    {
      "index": 171,
      "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",
      "abstract": "",
      "year": "2023",
      "venue": "Conference on Robot Learning",
      "authors": "Dhruv Shah, Błażej Osiński, Sergey Levine, et al.",
      "orig_title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
      "paper_id": "2207.04429v2"
    },
    {
      "index": 172,
      "title": "Collaborating with language models for embodied reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila\nBabayan, Felix Hill, and Rob Fergus.",
      "orig_title": "Collaborating with language models for embodied reasoning",
      "paper_id": "2302.00763v1"
    },
    {
      "index": 173,
      "title": "Statler: State-Maintaining Language Models for Embodied Reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie\nLin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew R Walter.",
      "orig_title": "Statler: State-maintaining language models for embodied reasoning",
      "paper_id": "2306.17840v4"
    },
    {
      "index": 174,
      "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "Conference on Robot Learning",
      "authors": "Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy\nZeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.",
      "orig_title": "Inner monologue: Embodied reasoning through planning with language models",
      "paper_id": "2207.05608v1"
    },
    {
      "index": 175,
      "title": "Code as policies: Language model programs for embodied control",
      "abstract": "",
      "year": "2023",
      "venue": "2023 IEEE International Conference on Robotics and Automation\n(ICRA)",
      "authors": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete\nFlorence, and Andy Zeng."
    },
    {
      "index": 176,
      "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol\nHausman, Sergey Levine, and Jonathan Tompson.",
      "orig_title": "Robotic skill acquisition via instruction augmentation with vision-language models",
      "paper_id": "2211.11736v3"
    },
    {
      "index": 177,
      "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta.",
      "orig_title": "Distilling internet-scale vision-language models into embodied agents",
      "paper_id": "2301.12507v2"
    },
    {
      "index": 178,
      "title": "Object Scene Representation Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Mehdi SM Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste,\nFilip Pavetic, Mario Lucic, Leonidas J Guibas, Klaus Greff, and Thomas Kipf.",
      "orig_title": "Object scene representation transformer",
      "paper_id": "2206.06922v2"
    },
    {
      "index": 179,
      "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine\nHsu, et al.",
      "orig_title": "Rt-1: Robotics transformer for real-world control at scale",
      "paper_id": "2212.06817v2"
    },
    {
      "index": 180,
      "title": "TALM: Tool Augmented Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Aaron Parisi, Yao Zhao, and Noah Fiedel.",
      "orig_title": "Talm: Tool augmented language models",
      "paper_id": "2205.12255v1"
    },
    {
      "index": 181,
      "title": "Toolformer: Language models can teach themselves to use tools, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria\nLomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom."
    },
    {
      "index": 182,
      "title": "Art: Automatic multi-step reasoning and tool-use for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke\nZettlemoyer, and Marco Tulio Ribeiro."
    },
    {
      "index": 183,
      "title": "Tool learning with foundation models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "Augmented language models: a survey, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,\nRam Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom."
    },
    {
      "index": 185,
      "title": "Foundation models for decision making: Problems, methods, and opportunities, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale\nSchuurmans."
    },
    {
      "index": 186,
      "title": "Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie\nZhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun."
    },
    {
      "index": 187,
      "title": "Creator: Disentangling abstract and concrete reasonings of large language models through tool creation, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji."
    },
    {
      "index": 188,
      "title": "Webgpt: Browser-assisted question-answering with human feedback, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew\nKnight, Benjamin Chess, and John Schulman."
    },
    {
      "index": 189,
      "title": "Chameleon: Plug-and-play compositional reasoning with large language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, and Jianfeng Gao."
    },
    {
      "index": 190,
      "title": "Tool documentation enables zero-shot tool-usage with large language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner,\nChen-Yu Lee, Ranjay Krishna, and Tomas Pfister."
    },
    {
      "index": 191,
      "title": "Lamda: Language models for dialog applications, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "Tree of thoughts: Deliberate problem solving with large language models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan\nCao, and Karthik Narasimhan."
    },
    {
      "index": 193,
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez.",
      "orig_title": "Gorilla: Large language model connected with massive apis",
      "paper_id": "2305.15334v1"
    },
    {
      "index": 194,
      "title": "Visual programming: Compositional visual reasoning without training",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Tanmay Gupta and Aniruddha Kembhavi."
    },
    {
      "index": 195,
      "title": "MM-ReAct : Prompting ChatGPT for Multimodal Reasoning and Action",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal\nAhmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.",
      "orig_title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "paper_id": "2303.11381v1"
    },
    {
      "index": 196,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai\nLu, Lei Ji, Shaoguang Mao, et al.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 197,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy,\nA Mahendran, A Arnab, M Dehghani, Z Shen, et al.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 198,
      "title": "DSFD: Dual Shot Face Detector",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie\nWang, Jilin Li, and Feiyue Huang.",
      "orig_title": "Dsfd: dual shot face detector",
      "paper_id": "1810.10220v3"
    },
    {
      "index": 199,
      "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Bowen Cheng, Alex Schwing, and Alexander Kirillov.",
      "orig_title": "Per-pixel classification is not all you need for semantic segmentation",
      "paper_id": "2107.06278v2"
    },
    {
      "index": 200,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Wonjae Kim, Bokyung Son, and Ildoo Kim.",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 201,
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\nOmmer.",
      "orig_title": "High-resolution image synthesis with latent diffusion models",
      "paper_id": "2112.10752v2"
    },
    {
      "index": 202,
      "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on\nComputer Vision",
      "authors": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.",
      "orig_title": "Adding conditional control to text-to-image diffusion models",
      "paper_id": "2302.05543v3"
    },
    {
      "index": 203,
      "title": "Llama-adapter v2: Parameter-efficient visual instruction model, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei\nZhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao."
    },
    {
      "index": 204,
      "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian,\nJi Zhang, Fei Huang, and Jingren Zhou."
    },
    {
      "index": 205,
      "title": "Infimm:, 2024",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "InfiMM Team."
    },
    {
      "index": 206,
      "title": "Gemini: a family of highly capable multimodal models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,\nJiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al."
    },
    {
      "index": 207,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 208,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 209,
      "title": "Introducing our multimodal models, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena,\nArushi Somani, and Sağnak Taşırlar."
    },
    {
      "index": 210,
      "title": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan\nZhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan,\nXinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang,\nYu Qiao, Dahua Lin, and Jiaqi Wang."
    },
    {
      "index": 211,
      "title": "Scaling instruction-finetuned language models, 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet\nSingh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush,\nDouwe Kiela, et al.",
      "orig_title": "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
      "paper_id": "2306.16527v2"
    },
    {
      "index": 213,
      "title": "Generative Pretraining in Multimodality",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang,\nHongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.",
      "orig_title": "Generative pretraining in multimodality",
      "paper_id": "2307.05222v2"
    },
    {
      "index": 214,
      "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu,\nChen Lin, Wenqi Shao, Keqin Chen, et al.",
      "orig_title": "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models",
      "paper_id": "2311.07575v1"
    },
    {
      "index": 215,
      "title": "Vx2text: End-to-end learning of video-based text generation from multimodal inputs",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, and Lorenzo\nTorresani."
    },
    {
      "index": 216,
      "title": "Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition",
      "authors": "Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, Mike Zheng Shou, Heng Ji,\nand Shih-Fu Chang.",
      "orig_title": "Towards fast adaptation of pretrained contrastive models for multi-channel video-language retrieval",
      "paper_id": "2206.02082v4"
    },
    {
      "index": 217,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov."
    },
    {
      "index": 218,
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D\nManning, and Chelsea Finn.",
      "orig_title": "Direct preference optimization: Your language model is secretly a reward model",
      "paper_id": "2305.18290v3"
    },
    {
      "index": 219,
      "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen,\nChuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al.",
      "orig_title": "Aligning large multimodal models with factually augmented rlhf",
      "paper_id": "2309.14525v1"
    }
  ]
}