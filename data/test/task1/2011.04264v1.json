{
  "paper_id": "2011.04264v1",
  "title": "CapWAP: Captioning with a Purpose",
  "abstract": "Abstract\nThe traditional image captioning task uses generic reference captions to provide textual information about images.\nDifferent user populations, however, will care about different visual aspects of images.\nIn this paper, we propose a new task, Captioning with a Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image.\nIn this task, we use question-answer (QA) pairs—a natural expression of information need—from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions.\nWe convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.",
  "reference_labels": [
    {
      "index": 0,
      "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "abstract": "",
      "year": "2015",
      "venue": "tensorflow.org",
      "authors": "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng"
    },
    {
      "index": 1,
      "title": "Synthetic QA Corpora Generation with Roundtrip Consistency",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins",
      "orig_title": "Synthetic QA corpora generation with roundtrip consistency",
      "paper_id": "1906.05416v1"
    },
    {
      "index": 2,
      "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search",
      "abstract": "",
      "year": "2016",
      "venue": "EMNLP",
      "authors": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould",
      "orig_title": "Guided open vocabulary image captioning with constrained beam search",
      "paper_id": "1612.00576v2"
    },
    {
      "index": 3,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang",
      "orig_title": "Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 4,
      "title": "Guiding Extractive Summarization with Question-Answering Rewards",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Kristjan Arumae and Fei Liu",
      "orig_title": "Guiding extractive summarization with question-answering rewards",
      "paper_id": "1904.02321v1"
    },
    {
      "index": 5,
      "title": "VizWiz: Nearly real-time answers to visual questions",
      "abstract": "",
      "year": "2010",
      "venue": "ACM Symposium on User Interface Software and Technology",
      "authors": "Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C. Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh"
    },
    {
      "index": 6,
      "title": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "Florian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych",
      "orig_title": "Better rewards yield better summaries: Learning to summarise without references",
      "paper_id": "1909.01214v1"
    },
    {
      "index": 7,
      "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin",
      "orig_title": "Towards diverse and natural image descriptions via a conditional gan",
      "paper_id": "1703.06029v3"
    },
    {
      "index": 8,
      "title": "Contrastive Learning for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Bo Dai and Dahua Lin",
      "orig_title": "Contrastive learning for image captioning",
      "paper_id": "1710.02534v1"
    },
    {
      "index": 9,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 10,
      "title": "New methods in automatic extracting",
      "abstract": "",
      "year": "1969",
      "venue": "J. ACM",
      "authors": "H. P. Edmundson"
    },
    {
      "index": 11,
      "title": "Question Answering as an Automatic Evaluation Metric for News Article Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Matan Eyal, Tal Baumel, and Michael Elhadad",
      "orig_title": "Question answering as an automatic evaluation metric for news article summarization",
      "paper_id": "1906.00318v1"
    },
    {
      "index": 12,
      "title": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
      "abstract": "",
      "year": "2019",
      "venue": "IJCAI",
      "authors": "Yang Gao, Christian M. Meyer, Mohsen Mesgar, and Iryna Gurevych",
      "orig_title": "Reward learning for efficient reinforcement learning in extractive document summarisation",
      "paper_id": "1907.12894v1"
    },
    {
      "index": 13,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh",
      "orig_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 14,
      "title": "Improving Reinforcement Learning Based Image Captioning with Natural Language Prior",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Tszhang Guo, Shiyu Chang, Mo Yu, and Kun Bai",
      "orig_title": "Improving reinforcement learning based image captioning with natural language prior",
      "paper_id": "1809.06227v1"
    },
    {
      "index": 15,
      "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham",
      "orig_title": "Vizwiz grand challenge: Answering visual questions from blind people",
      "paper_id": "1802.08218v4"
    },
    {
      "index": 16,
      "title": "Teaching machines to read and comprehend",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom"
    },
    {
      "index": 17,
      "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
      "abstract": "",
      "year": "2017",
      "venue": "ACL",
      "authors": "Chris Hokamp and Qun Liu",
      "orig_title": "Lexically constrained decoding for sequence generation using grid beam search",
      "paper_id": "1704.07138v2"
    },
    {
      "index": 18,
      "title": "Attention on attention for image captioning",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei"
    },
    {
      "index": 19,
      "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Drew A. Hudson and Christopher D. Manning"
    },
    {
      "index": 20,
      "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy",
      "orig_title": "Spanbert: Improving pre-training by representing and predicting spans",
      "paper_id": "1907.10529v3"
    },
    {
      "index": 21,
      "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Andrej Karpathy and Fei-Fei Li",
      "orig_title": "Deep visual-semantic alignments for generating image descriptions",
      "paper_id": "1412.2306v2"
    },
    {
      "index": 22,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Diederik P. Kingma and Jimmy Ba"
    },
    {
      "index": 23,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei"
    },
    {
      "index": 24,
      "title": "Guided policy search",
      "abstract": "",
      "year": "2013",
      "venue": "ICML",
      "authors": "Sergey Levine and Vladlen Koltun"
    },
    {
      "index": 25,
      "title": "Learning through dialogue interactions by asking questions",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Jiwei Li, Alexander H. Miller, S. Chopra, Marc’Aurelio Ranzato, and Jason Weston"
    },
    {
      "index": 26,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 27,
      "title": "Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, and Jiebo Luo",
      "orig_title": "Tell-and-answer: Towards explainable visual question answering using attributes and captions",
      "paper_id": "1801.09041v1"
    },
    {
      "index": 28,
      "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "abstract": "",
      "year": "2003",
      "venue": "NAACL",
      "authors": "Chin-Yew Lin and Eduard Hovy"
    },
    {
      "index": 29,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick",
      "orig_title": "Microsoft COCO: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 30,
      "title": "Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xiaogang Wang",
      "orig_title": "Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data",
      "paper_id": "1803.08314v3"
    },
    {
      "index": 31,
      "title": "Nltk: The natural language toolkit",
      "abstract": "",
      "year": "2002",
      "venue": "ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics",
      "authors": "Edward Loper and Steven Bird"
    },
    {
      "index": 32,
      "title": "Automatically assessing machine summary content without a gold standard",
      "abstract": "",
      "year": "2013",
      "venue": "Computational Linguistics",
      "authors": "Annie Louis and Ani Nenkova"
    },
    {
      "index": 33,
      "title": "Discriminability objective for training descriptive captions",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Ruotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich",
      "orig_title": "Discriminability objective for training descriptive captions",
      "paper_id": "1803.04376v2"
    },
    {
      "index": 34,
      "title": "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL",
      "authors": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata",
      "orig_title": "Ranking sentences for extractive summarization with reinforcement learning",
      "paper_id": "1802.08636v2"
    },
    {
      "index": 35,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "NeurIPS",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg"
    },
    {
      "index": 36,
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Romain Paulus, Caiming Xiong, and Richard Socher",
      "orig_title": "A deep reinforced model for abstractive summarization",
      "paper_id": "1705.04304v3"
    },
    {
      "index": 37,
      "title": "A Simple Theoretical Model of Importance for Summarization",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Maxime Peyrard",
      "orig_title": "A simple theoretical model of importance for summarization",
      "paper_id": "1801.08991v2"
    },
    {
      "index": 38,
      "title": "Objective function learning to match human judgements for optimization-based summarization",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL",
      "authors": "Maxime Peyrard and Iryna Gurevych"
    },
    {
      "index": 39,
      "title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Pranav Rajpurkar, Robin Jia, and Percy Liang",
      "orig_title": "Know what you don’t know: Unanswerable questions for SQuAD",
      "paper_id": "1806.03822v1"
    },
    {
      "index": 40,
      "title": "SQuAD: 100,000+ questions for machine comprehension of text",
      "abstract": "",
      "year": "2016",
      "venue": "EMNLP",
      "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang"
    },
    {
      "index": 41,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba",
      "orig_title": "Sequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 42,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun",
      "orig_title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 43,
      "title": "Self-critical Sequence Training for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel",
      "orig_title": "Self-critical sequence training for image captioning",
      "paper_id": "1612.00563v2"
    },
    {
      "index": 44,
      "title": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "authors": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano",
      "orig_title": "Answers unite! unsupervised metrics for reinforced summarization models",
      "paper_id": "1909.01610v1"
    },
    {
      "index": 45,
      "title": "Learning to Caption Images through a Lifetime by Asking Questions",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Tingke Shen, Amlan Kar, and Sanja Fidler",
      "orig_title": "Learning to caption images through a lifetime by asking questions",
      "paper_id": "1812.00235v3"
    },
    {
      "index": 46,
      "title": "Towards better nlp system evaluation",
      "abstract": "",
      "year": "1994",
      "venue": "Human Language Technologies Workshop",
      "authors": "Karen Spärck Jones"
    },
    {
      "index": 47,
      "title": "Automatic summarizing: Factors and directions",
      "abstract": "",
      "year": "1999",
      "venue": "Advances in Automatic Text Summarization",
      "authors": "Karen Spärck Jones"
    },
    {
      "index": 48,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Hao Tan and Mohit Bansal",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 49,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 50,
      "title": "CIDEr: Consensus-based Image Description Evaluation",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh",
      "orig_title": "CIDEr: Consensus-based image description evaluation",
      "paper_id": "1411.5726v2"
    },
    {
      "index": 51,
      "title": "Show and Tell: A Neural Image Caption Generator",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan",
      "orig_title": "Show and tell: A neural image caption generator",
      "paper_id": "1411.4555v2"
    },
    {
      "index": 52,
      "title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "Alex Wang, Kyunghyun Cho, and Mike Lewis",
      "orig_title": "Asking and answering questions to evaluate the factual consistency of summaries",
      "paper_id": "2004.04228v1"
    },
    {
      "index": 53,
      "title": "Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Liwei Wang, Alexander Schwing, and Svetlana Lazebnik",
      "orig_title": "Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space",
      "paper_id": "1711.07068v1"
    },
    {
      "index": 54,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "Ronald J. Williams"
    },
    {
      "index": 55,
      "title": "Generating question relevant captions to aid visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Jialin Wu, Zeyuan Hu, and Raymond Mooney"
    },
    {
      "index": 56,
      "title": "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.",
      "orig_title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 57,
      "title": "Image captioning by asking questions",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl.",
      "authors": "Xiaoshan Yang and Changsheng Xu"
    },
    {
      "index": 58,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 59,
      "title": "Visual7W: Grounded question answering in images",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei"
    }
  ]
}