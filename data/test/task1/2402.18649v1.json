{
  "paper_id": "2402.18649v1",
  "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems Warning: This paper may contain content that has the potential to be offensive and harmful",
  "abstract": "Abstract\nLarge Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on.\nAlong with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems.\nHowever, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on).\nIn this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints.\nTo ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4.\nOur investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers.\nTo further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user’s chat history, all without the need to manipulate the user’s input or gain direct access to OpenAI GPT4.\nWe have reported the discovered vulnerabilities to OpenAI and our project demo is placed in the following link:\nhttps://fzwark.github.io/LLM-System-Attack-Demo/",
  "reference_labels": [
    {
      "index": 0,
      "title": "Bing Chat",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Canva GPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 2,
      "title": "DocMaker ChatGPT Plugin",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 3,
      "title": "Github Copilot - Your AI pair programmer",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 4,
      "title": "Google images",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 5,
      "title": "GPTs Statistic Data",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 6,
      "title": "GPTs Store",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 7,
      "title": "Introducting ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 8,
      "title": "OpenAI Dev Day",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 9,
      "title": "OpenAI Plugins",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 10,
      "title": "Third-Party GPTs Store",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 11,
      "title": "WebPilot ChatGPT Plugin",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 12,
      "title": "Secure computer system: Unified exposition and multics interpretation",
      "abstract": "",
      "year": "1976",
      "venue": "",
      "authors": "David E Bell, Leonard J La Padula, et al."
    },
    {
      "index": 13,
      "title": "Sard: A software assurance reference dataset",
      "abstract": "",
      "year": "1970",
      "venue": "",
      "authors": "Paul Black"
    },
    {
      "index": 14,
      "title": "Compositional security for reentrant applications",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "Ethan Cecchetti, Siqiu Yao, Haobin Ni, and Andrew C Myers"
    },
    {
      "index": 15,
      "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2310.08419",
      "authors": "Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong",
      "orig_title": "Jailbreaking black box large language models in twenty queries",
      "paper_id": "2310.08419v4"
    },
    {
      "index": 16,
      "title": "Evaluation of chatgpt model for vulnerability detection",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Anton Cheshkov, Pavel Zadorozhny, and Rodion Levichev"
    },
    {
      "index": 17,
      "title": "A lattice model of secure information flow",
      "abstract": "",
      "year": "1976",
      "venue": "Communications of the ACM",
      "authors": "Dorothy E Denning"
    },
    {
      "index": 18,
      "title": "Mathematical Capabilities of ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.13867",
      "authors": "Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner",
      "orig_title": "Mathematical capabilities of chatgpt",
      "paper_id": "2301.13867v2"
    },
    {
      "index": 19,
      "title": "Security policies and security models",
      "abstract": "",
      "year": "1982",
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": "Joseph A Goguen and José Meseguer"
    },
    {
      "index": 20,
      "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.04655",
      "authors": "Roberto Gozalo-Brizuela and Eduardo C Garrido-Merchan",
      "orig_title": "Chatgpt is not all you need. a state of the art review of large generative ai models",
      "paper_id": "2301.04655v1"
    },
    {
      "index": 21,
      "title": "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.12173",
      "authors": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz",
      "orig_title": "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
      "paper_id": "2302.12173v2"
    },
    {
      "index": 22,
      "title": "Markdown: Syntax",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "John Gruber"
    },
    {
      "index": 23,
      "title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Access",
      "authors": "Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj",
      "orig_title": "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
      "paper_id": "2307.00691v1"
    },
    {
      "index": 24,
      "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.10236",
      "authors": "Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, and Lei Ma",
      "orig_title": "Look before you leap: An exploratory study of uncertainty measurement for large language models",
      "paper_id": "2307.10236v4"
    },
    {
      "index": 25,
      "title": "Llm platform security: Applying a systematic evaluation framework to openai’s chatgpt plugins",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2309.10254",
      "authors": "Umar Iqbal, Tadayoshi Kohno, and Franziska Roesner"
    },
    {
      "index": 26,
      "title": "Could an artificial-intelligence agent pass an introductory physics course?",
      "abstract": "",
      "year": "2023",
      "venue": "Physical Review Physics Education Research",
      "authors": "Gerd Kortemeyer"
    },
    {
      "index": 27,
      "title": "AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.08155",
      "authors": "Kay Lehnert",
      "orig_title": "Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt",
      "paper_id": "2301.08155v1"
    },
    {
      "index": 28,
      "title": "Stolen memories: Leveraging model memorization for calibrated {{\\{White-Box}}} membership inference",
      "abstract": "",
      "year": "2020",
      "venue": "USENIX security symposium (USENIX Security 20)",
      "authors": "Klas Leino and Matt Fredrikson"
    },
    {
      "index": 29,
      "title": "Prompt Injection attack against LLM-integrated Applications, June 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2306.05499 [cs]",
      "authors": "Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu"
    },
    {
      "index": 30,
      "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications, October 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2310.12815 [cs]",
      "authors": "Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong"
    },
    {
      "index": 31,
      "title": "X. 509 internet public key infrastructure online certificate status protocol-ocsp",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": "Michael Myers, Rich Ankney, Ambarish Malpani, Slava Galperin, and Carlisle Adams"
    },
    {
      "index": 32,
      "title": "Department of Defense Trusted Computer System Evaluation Criteria, volume 83",
      "abstract": "",
      "year": "1987",
      "venue": "Department of Defense",
      "authors": "United States. Department of Defense"
    },
    {
      "index": 33,
      "title": "Asleep at the keyboard? assessing the security of github copilot’s code contributions",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri"
    },
    {
      "index": 34,
      "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "authors": "Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt",
      "orig_title": "Examining zero-shot vulnerability repair with large language models",
      "paper_id": "2112.02125v3"
    },
    {
      "index": 35,
      "title": "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application, August 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2308.01990 [cs]",
      "authors": "Rodrigo Pedro, Daniel Castro, Paulo Carreira, and Nuno Santos"
    },
    {
      "index": 36,
      "title": "Ignore Previous Prompt: Attack Techniques For Language Models, November 2022",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2211.09527 [cs]",
      "authors": "Fábio Perez and Ian Ribeiro"
    },
    {
      "index": 37,
      "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning, January 2024",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv:2312.17673 [cs]",
      "authors": "Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner"
    },
    {
      "index": 38,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al."
    },
    {
      "index": 39,
      "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas",
      "orig_title": "Smoothllm: Defending large language models against jailbreaking attacks",
      "paper_id": "2310.03684v4"
    },
    {
      "index": 40,
      "title": "Language-based information-flow security",
      "abstract": "",
      "year": "2003",
      "venue": "IEEE Journal on selected areas in communications",
      "authors": "Andrei Sabelfeld and Andrew C Myers"
    },
    {
      "index": 41,
      "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks, December 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2312.11513 [cs]",
      "authors": "Ahmed Salem, Andrew Paverd, and Boris Köpf"
    },
    {
      "index": 42,
      "title": "Access control: principle and practice",
      "abstract": "",
      "year": "1994",
      "venue": "IEEE communications magazine",
      "authors": "Ravi S Sandhu and Pierangela Samarati"
    },
    {
      "index": 43,
      "title": "Privacy and data protection in chatgpt and other ai chatbots: Strategies for securing user information",
      "abstract": "",
      "year": "2023",
      "venue": "Available at SSRN 4454761",
      "authors": "Glorin Sebastian"
    },
    {
      "index": 44,
      "title": "An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.13814",
      "authors": "Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu",
      "orig_title": "An independent evaluation of chatgpt on mathematical word problems (mwp)",
      "paper_id": "2302.13814v2"
    },
    {
      "index": 45,
      "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2310.10844",
      "authors": "Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh",
      "orig_title": "Survey of vulnerabilities in large language models revealed by adversarial attacks",
      "paper_id": "2310.10844v1"
    },
    {
      "index": 46,
      "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications, January 2024",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv:2401.07612 [cs]",
      "authors": "Xuchen Suo"
    },
    {
      "index": 47,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.13971",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 48,
      "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game, November 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2311.01011 [cs]",
      "authors": "Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell"
    },
    {
      "index": 49,
      "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li",
      "orig_title": "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
      "paper_id": "2306.11698v5"
    },
    {
      "index": 50,
      "title": "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection, June 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2306.08833 [cs]",
      "authors": "Chaofan Wang, Samuel Kernan Freire, Mo Zhang, Jing Wei, Jorge Goncalves, Vassilis Kostakos, Zhanna Sarsenbayeva, Christina Schneegass, Alessandro Bozzon, and Evangelos Niforatos"
    },
    {
      "index": 51,
      "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Zeming Wei, Yifei Wang, and Yisen Wang",
      "orig_title": "Jailbreak and guard aligned language models with only few in-context demonstrations",
      "paper_id": "2310.06387v3"
    },
    {
      "index": 52,
      "title": "Unveiling Security, Privacy, and Ethical Concerns of ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Information and Intelligence",
      "authors": "Xiaodong Wu, Ran Duan, and Jianbing Ni",
      "orig_title": "Unveiling security, privacy, and ethical concerns of chatgpt",
      "paper_id": "2307.14192v1"
    },
    {
      "index": 53,
      "title": "A language for automatically enforcing privacy policies",
      "abstract": "",
      "year": "2012",
      "venue": "ACM SIGPLAN Notices",
      "authors": "Jean Yang, Kuat Yessenov, and Armando Solar-Lezama"
    },
    {
      "index": 54,
      "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2312.14197",
      "authors": "Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu",
      "orig_title": "Benchmarking and defending against indirect prompt injection attacks on large language models",
      "paper_id": "2312.14197v4"
    },
    {
      "index": 55,
      "title": "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models, January 2024",
      "abstract": "",
      "year": "2024",
      "venue": "arXiv:2401.00991 [cs]",
      "authors": "Daniel Wankit Yip, Aysan Esmradi, and Chun Fai Chan"
    },
    {
      "index": 56,
      "title": "Assessing Prompt Injection Risks in 200+ Custom GPTs, November 2023",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:2311.11538 [cs]",
      "authors": "Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, and Xinyu Xing"
    },
    {
      "index": 57,
      "title": "Secure program partitioning",
      "abstract": "",
      "year": "2002",
      "venue": "ACM Transactions on Computer Systems (TOCS)",
      "authors": "Steve Zdancewic, Lantian Zheng, Nathaniel Nystrom, and Andrew C Myers"
    },
    {
      "index": 58,
      "title": "Using replication and partitioning to build secure distributed systems",
      "abstract": "",
      "year": "2003",
      "venue": "Symposium on Security and Privacy",
      "authors": "Lantian Zheng, Stephen Chong, Andrew C Myers, and Steve Zdancewic"
    },
    {
      "index": 59,
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.15043",
      "authors": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson",
      "orig_title": "Universal and transferable adversarial attacks on aligned language models",
      "paper_id": "2307.15043v2"
    }
  ]
}