{
  "paper_id": "2303.11098v5",
  "title": "Understanding the Role of the Projector in Knowledge Distillation",
  "abstract": "Abstract\nIn this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a 77.277.277.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are publicly available.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Allen-Zhu, Z.; and Li, Y.",
      "orig_title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning",
      "paper_id": "2012.09816v3"
    },
    {
      "index": 1,
      "title": "VICReg: Variance-Invariance-Covariance Re-gularization for Self-Supervised Learning",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Bardes, A.; Ponce, J.; and LeCun, Y.",
      "orig_title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
      "paper_id": "2105.04906v3"
    },
    {
      "index": 2,
      "title": "VICRegL: Self-Supervised Learning of Local Visual Features",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Bardes, A.; Ponce, J.; and LeCun, Y.",
      "orig_title": "VICRegL: Self-Supervised Learning of Local Visual Features",
      "paper_id": "2210.01571v1"
    },
    {
      "index": 3,
      "title": "Knowledge distillation: A good teacher is patient and consistent",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Beyer, L.; Zhai, X.; Royer, A.; Markeeva, L.; Anil, R.; and Kolesnikov, A.",
      "orig_title": "Knowledge distillation: A good teacher is patient and consistent",
      "paper_id": "2106.05237v2"
    },
    {
      "index": 4,
      "title": "Domain Generalization by Solving Jigsaw Puzzles",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Carlucci, F. M.; D’Innocente, A.; Bucci, S.; Caputo, B.; and Tommasi, T.",
      "orig_title": "Domain Generalization by Solving Jigsaw Puzzles",
      "paper_id": "1903.06864v2"
    },
    {
      "index": 5,
      "title": "Knowledge Distillation with the Reused Teacher Classifier",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Chen, D.; Mei, J.-P.; Zhang, H.; Wang, C.; Feng, Y.; and Chen, C.",
      "orig_title": "Knowledge Distillation with the Reused Teacher Classifier",
      "paper_id": "2203.14001v1"
    },
    {
      "index": 6,
      "title": "Wasserstein Contrastive Representation Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Chen, L.; Wang, D.; Gan, Z.; Liu, J.; Henao, R.; and Carin, L."
    },
    {
      "index": 7,
      "title": "Distilling Knowledge via Knowledge Review",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Chen, P.; Liu, S.; Zhao, H.; and Jia, J.",
      "orig_title": "Distilling Knowledge via Knowledge Review",
      "paper_id": "2104.09044v1"
    },
    {
      "index": 8,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G.",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 9,
      "title": "Improved Baselines with Momentum Contrastive Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Chen, X.; Fan, H.; Girshick, R.; and He, K.",
      "orig_title": "Improved Baselines with Momentum Contrastive Learning",
      "paper_id": "2003.04297v1"
    },
    {
      "index": 10,
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Chen, X.; Xie, S.; and He, K.",
      "orig_title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "paper_id": "2104.02057v4"
    },
    {
      "index": 11,
      "title": "On Self-Distilling Graph Neural Network",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chen, Y.; Bian, Y.; Xiao, X.; Rong, Y.; Xu, T.; and Huang, J.",
      "orig_title": "On Self-Distilling Graph Neural Network",
      "paper_id": "2011.02255v2"
    },
    {
      "index": 12,
      "title": "Improved Feature Distillation via Projector Ensemble",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Chen, Y.; Wang, S.; Liu, J.; Xu, X.; de Hoog, F.; and Huang, Z.",
      "orig_title": "Improved Feature Distillation via Projector Ensemble",
      "paper_id": "2210.15274v2"
    },
    {
      "index": 13,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR Workshop",
      "authors": "Cubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V."
    },
    {
      "index": 14,
      "title": "Unsupervised Visual Representation Learning by Context Prediction",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Doersch, C.; Gupta, A.; and Efros, A. A.",
      "orig_title": "Unsupervised Visual Representation Learning by Context Prediction",
      "paper_id": "1505.05192v3"
    },
    {
      "index": 15,
      "title": "Whitening for Self-Supervised Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Ermolov, A.; Siarohin, A.; Sangineto, E.; and Sebe, N.",
      "orig_title": "Whitening for Self-Supervised Representation Learning",
      "paper_id": "2007.06346v5"
    },
    {
      "index": 16,
      "title": "The pascal visual object classes (voc) challenge",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Everingham, M.; Gool, L. V.; Williams, C. K. I.; Winn, J.; and Zisserman., A."
    },
    {
      "index": 17,
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Gidaris, S.; Singh, P.; and Komodakis, N.",
      "orig_title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "paper_id": "1803.07728v1"
    },
    {
      "index": 18,
      "title": "Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Guo, J.; Chen, M.; Hu, Y.; Zhu, C.; He, X.; and Cai, D.",
      "orig_title": "Reducing the Teacher-Student Gap via Spherical Knowledge Distillation",
      "paper_id": "2010.07485v5"
    },
    {
      "index": 19,
      "title": "Feature Kernel Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "He, B.; and Ozay, M."
    },
    {
      "index": 20,
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "He, K.; Chen, X.; Xie, S.; Li, Y.; Dollár, P.; and Girshick, R."
    },
    {
      "index": 21,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R.",
      "orig_title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 22,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Hinton, G.; Vinyals, O.; and Dean, J.",
      "orig_title": "Distilling the Knowledge in a Neural Network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 23,
      "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Huang, Z.; and Wang, N.",
      "orig_title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer",
      "paper_id": "1707.01219v2"
    },
    {
      "index": 24,
      "title": "On Representation Knowledge Distillation for Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Joshi, C. K.; Liu, F.; Xun, X.; Lin, J.; and Foo, C.-S.",
      "orig_title": "On Representation Knowledge Distillation for Graph Neural Networks",
      "paper_id": "2111.04964v4"
    },
    {
      "index": 25,
      "title": "Learning Multiple Layers of Features from Tiny Images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Krizhevsky, A."
    },
    {
      "index": 26,
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "abstract": "",
      "year": "2012",
      "venue": "NeurIPS",
      "authors": "Krizhevsky, A.; Sutskever, I.; and Hinton, G. E."
    },
    {
      "index": 27,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Lin, T. Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollár, P.; and Zitnick, C. L.",
      "orig_title": "Microsoft COCO: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 28,
      "title": "Exploring Inter-Channel Correlation for Diversity-preserved Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "ICCV",
      "authors": "Liu, L.; Huang, Q.; Lin, S.; Xie, H.; Wang, B.; Chang, X.; and Liang, X.",
      "orig_title": "Exploring Inter-Channel Correlation for Diversity-preserved Knowledge Distillation",
      "paper_id": "2202.03680v1"
    },
    {
      "index": 29,
      "title": "Structured Knowledge Distillation for Semantic Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Liu, Y.; Chen, K.; Liu, C.; Qin, Z.; Luo, Z.; and Wang, J."
    },
    {
      "index": 30,
      "title": "Distilling Knowledge from Self-Supervised Teacher by Embedding Graph Alignment",
      "abstract": "",
      "year": "2022",
      "venue": "BMVC",
      "authors": "Ma, Y.; Chen, Y.; and Akata, Z.",
      "orig_title": "Distilling Knowledge from Self-Supervised Teacher by Embedding Graph Alignment",
      "paper_id": "2211.13264v1"
    },
    {
      "index": 31,
      "title": "torchdistill : A Modular, Configuration-Driven Framework for Knowledge Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Matsubara, Y."
    },
    {
      "index": 32,
      "title": "Cascaded channel pruning using hierarchical self-distillation",
      "abstract": "",
      "year": "2020",
      "venue": "BMVC",
      "authors": "Miles, R.; and Mikolajczyk, K.",
      "orig_title": "Cascaded channel pruning using hierarchical self-distillation",
      "paper_id": "2008.06814v1"
    },
    {
      "index": 33,
      "title": "Information Theoretic Representation Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "BMVC",
      "authors": "Miles, R.; Rodriguez, A. L.; and Mikolajczyk, K.",
      "orig_title": "Information Theoretic Representation Distillation",
      "paper_id": "2112.00459v3"
    },
    {
      "index": 34,
      "title": "MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation",
      "abstract": "",
      "year": "2023",
      "venue": "CVPR",
      "authors": "Miles, R.; Yucel, M. K.; Manganelli, B.; and Saa-Garriga, A.",
      "orig_title": "MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation",
      "paper_id": "2303.07815v1"
    },
    {
      "index": 35,
      "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Mobahi, H.; Farajtabar, M.; and Bartlett, P. L."
    },
    {
      "index": 36,
      "title": "SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "BMVC",
      "authors": "Navaneet, K. L.; Koohpayegani, S. A.; Tejankar, A.; and Pirsiavash, H.",
      "orig_title": "SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation",
      "paper_id": "2201.05131v1"
    },
    {
      "index": 37,
      "title": "Relational Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Park, W.; Corp, K.; Kim, D.; and Lu, Y.",
      "orig_title": "Relational Knowledge Distillation",
      "paper_id": "1904.05068v2"
    },
    {
      "index": 38,
      "title": "Correlation Congruence for Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Peng, B.; Jin, X.; Li, D.; Zhou, S.; Wu, Y.; Liu, J.; Zhang, Z.; and Liu, Y.",
      "orig_title": "Correlation congruence for knowledge distillation",
      "paper_id": "1904.01802v1"
    },
    {
      "index": 39,
      "title": "Co-advise: Cross Inductive Bias Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Ren, S.; Gao, Z.; Hua, T.; Xue, Z.; Tian, Y.; He, S.; and Zhao, H.",
      "orig_title": "Co-advise: Cross Inductive Bias Distillation",
      "paper_id": "2106.12378v1"
    },
    {
      "index": 40,
      "title": "FitNets: Hints for Thin Deep Nets",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "Romero, A.; Ballas, N.; Ebrahimi Kahou, S.; Chassang, A.; Gatta, C.; and Bengio, Y.",
      "orig_title": "FitNets: Hints For Thin Deep Nets",
      "paper_id": "1412.6550v4"
    },
    {
      "index": 41,
      "title": "S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Roth, K.; Milbich, T.; Ommer, B.; Cohen, J. P.; and Ghassemi, M."
    },
    {
      "index": 42,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2014",
      "venue": "IJCV",
      "authors": "Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L.",
      "orig_title": "ImageNet Large Scale Visual Recognition Challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 43,
      "title": "Contrastive Representation Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Tian, Y.; Krishnan, D.; and Isola, P.",
      "orig_title": "Contrastive representation distillation",
      "paper_id": "1910.10699v3"
    },
    {
      "index": 44,
      "title": "Deep Learning and the Information Bottleneck Principle",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Information Theory Workshop (ITW)",
      "authors": "Tishby, N.",
      "orig_title": "Deep Learning and the Information Bottleneck Principle",
      "paper_id": "1503.02406v1"
    },
    {
      "index": 45,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "PMLR",
      "authors": "Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; and Jégou, H.",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 46,
      "title": "Similarity-Preserving Knowledge Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Tung, F.; and Mori, G.",
      "orig_title": "Similarity-preserving knowledge distillation",
      "paper_id": "1907.09682v2"
    },
    {
      "index": 47,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I.",
      "orig_title": "Attention Is All You Need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 48,
      "title": "Distilling Object Detectors with Fine-grained Feature Imitation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Wang, T.; Yuan, L.; Zhang, X.; and Feng, J.",
      "orig_title": "Distilling Object Detectors with Fine-grained Feature Imitation",
      "paper_id": "1906.03609v1"
    },
    {
      "index": 49,
      "title": "Knowledge Distillation Meets Self-Supervision",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Xu, G.; Liu, Z.; Li, X.; and Loy, C. C.",
      "orig_title": "Knowledge Distillation Meets Self-supervision",
      "paper_id": "2006.07114v2"
    },
    {
      "index": 50,
      "title": "Hierarchical Self-supervised Augmented Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "IJCAI",
      "authors": "Yang, C.; An, Z.; Cai, L.; and Xu, Y."
    },
    {
      "index": 51,
      "title": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Yim, J."
    },
    {
      "index": 52,
      "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Zagoruyko, S.; and Komodakis, N.",
      "orig_title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "paper_id": "1612.03928v3"
    },
    {
      "index": 53,
      "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Zbontar, J.; Jing, L.; Misra, I.; LeCun, Y.; and Deny, S."
    },
    {
      "index": 54,
      "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Zhang, L.; Song, J.; Gao, A.; Chen, J.; Bao, C.; and Ma, K.",
      "orig_title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
      "paper_id": "1905.08094v1"
    },
    {
      "index": 55,
      "title": "Colorful Image Colorization",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Zhang, R.; Isola, P.; and Efros, A. A.",
      "orig_title": "Colorful Image Colorization",
      "paper_id": "1603.08511v5"
    },
    {
      "index": 56,
      "title": "Prime-Aware Adaptive Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Zhang, Y.; Lan, Z.; Dai, Y.; Zeng, F.; and Bai, Y."
    },
    {
      "index": 57,
      "title": "Self-Distillation as Instance-Specific Label Smoothing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Zhang, Z.; and Sabuncu, M. R."
    },
    {
      "index": 58,
      "title": "Decoupled Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "CVPR",
      "authors": "Zhao, B.; Song, R.; and Qiu, Y.",
      "orig_title": "Decoupled Knowledge Distillation",
      "paper_id": "2203.08679v2"
    },
    {
      "index": 59,
      "title": "Complementary Relation Contrastive Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Zhu, J.; Tang, S.; Chen, D.; and Yu, S.",
      "orig_title": "Complementary Relation Contrastive Distillation",
      "paper_id": "2103.16367v1"
    },
    {
      "index": 60,
      "title": "TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios",
      "abstract": "",
      "year": "2021",
      "venue": "VisDrone ICCV workshop",
      "authors": "Zhu, X.; Lyu, S.; Wang, X.; and Zhao, Q."
    }
  ]
}