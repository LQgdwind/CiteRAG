{
  "paper_id": "2206.07376v3",
  "title": "Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning",
  "abstract": "Abstract\nKeeping risk under control is often more crucial than maximizing expected rewards in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. The most natural choice of risk measures is variance, which penalizes the upside volatility as much as the downside part. Instead, the (downside) semivariance, which captures the negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement learning w.r.t. steady reward distribution. Since semivariance is time-inconsistent and does not satisfy the standard Bellman equation, the traditional dynamic programming methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to Perturbation Analysis (PA) theory and establish the performance difference formula for MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of RL problems with a policy-dependent reward function. Further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. Finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in MuJoCo, which demonstrate the effectiveness of our proposed methods.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Maximum a Posteriori Policy Optimisation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. A.",
      "orig_title": "Maximum a posteriori policy optimisation",
      "paper_id": "1806.06920v1"
    },
    {
      "index": 1,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Achiam, J., Held, D., Tamar, A., and Abbeel, P.",
      "orig_title": "Constrained policy optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 2,
      "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv preprint",
      "authors": "Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al.",
      "orig_title": "Dota 2 with large scale deep reinforcement learning",
      "paper_id": "1912.06680v1"
    },
    {
      "index": 3,
      "title": "Risk-Averse Trust Region Optimization for Reward-Volatility Reduction",
      "abstract": "",
      "year": "2020",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Bisi, L., Sabbioni, L., Vittori, E., Papini, M., and Restelli, M.",
      "orig_title": "Risk-averse trust region optimization for reward-volatility reduction",
      "paper_id": "1912.03193v1"
    },
    {
      "index": 4,
      "title": "Good volatility, bad volatility, and the cross section of stock returns",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Financial and Quantitative Analysis",
      "authors": "Bollerslev, T., Li, S. Z., and Zhao, B."
    },
    {
      "index": 5,
      "title": "Risk-sensitive optimal control for Markov decision processes with monotone cost",
      "abstract": "",
      "year": "2002",
      "venue": "Mathematics of Operations Research",
      "authors": "Borkar, V. S., and Meyn, S. P."
    },
    {
      "index": 6,
      "title": "Multi-horizon markowitz portfolio performance appraisals: A general approach",
      "abstract": "",
      "year": "2009",
      "venue": "Omega",
      "authors": "Briec, W., and Kerstens, K."
    },
    {
      "index": 7,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "ArXiv preprint",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 8,
      "title": "Stochastic Learning and Optimization: A Sensitivity-Based Approach",
      "abstract": "",
      "year": "2007",
      "venue": "Springer",
      "authors": "Cao, X.-R."
    },
    {
      "index": 9,
      "title": "Policy gradients with variance related risk criteria",
      "abstract": "",
      "year": "2012",
      "venue": "International Conference on Machine Learning",
      "authors": "Castro, D. D., Tamar, A., and Mannor, S."
    },
    {
      "index": 10,
      "title": "Multi-period mean–semivariance portfolio optimization based on uncertain measure",
      "abstract": "",
      "year": "2019",
      "venue": "Soft Computing",
      "authors": "Chen, W., Li, D., Lu, S., and Liu, W."
    },
    {
      "index": 11,
      "title": "A simple approximation for semivariance",
      "abstract": "",
      "year": "1986",
      "venue": "European Journal of Operational Research",
      "authors": "Choobineh, F., and Branting, D."
    },
    {
      "index": 12,
      "title": "Algorithms for CVaR Optimization in MDPs",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chow, Y., and Ghavamzadeh, M.",
      "orig_title": "Algorithms for CVaR optimization in MDPs",
      "paper_id": "1406.3339v3"
    },
    {
      "index": 13,
      "title": "Risk-constrained reinforcement learning with percentile risk criteria",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Machine Learning Research",
      "authors": "Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M."
    },
    {
      "index": 14,
      "title": "Risk-sensitive and robust decision-making: a CVaR optimization approach",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chow, Y., Tamar, A., Mannor, S., and Pavone, M."
    },
    {
      "index": 15,
      "title": "Mean-variance tradeoffs in an undiscounted MDP: the unichain case",
      "abstract": "",
      "year": "1994",
      "venue": "Operations Research",
      "authors": "Chung, K.-J."
    },
    {
      "index": 16,
      "title": "“Dice”-sion–making under uncertainty: When can a random decision reduce risk?",
      "abstract": "",
      "year": "2019",
      "venue": "Management Science",
      "authors": "Delage, E., Kuhn, D., and Wiesemann, W."
    },
    {
      "index": 17,
      "title": "Challenges of Real-World Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv preprint",
      "authors": "Dulac-Arnold, G., Mankowitz, D., and Hester, T.",
      "orig_title": "Challenges of real-world reinforcement learning",
      "paper_id": "1904.12901v1"
    },
    {
      "index": 18,
      "title": "Mean-semivariance behavior: Downside risk and capital asset pricing",
      "abstract": "",
      "year": "2007",
      "venue": "International Review of Economics & Finance",
      "authors": "Estrada, J."
    },
    {
      "index": 19,
      "title": "Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Fei, Y., Yang, Z., Chen, Y., Wang, Z., and Xie, Q.",
      "orig_title": "Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret",
      "paper_id": "2006.13827v1"
    },
    {
      "index": 20,
      "title": "Variance-penalized Markov decision processes",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of Operations Research",
      "authors": "Filar, J. A., Kallenberg, L. C., and Lee, H.-M."
    },
    {
      "index": 21,
      "title": "A comprehensive survey on safe reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Machine Learning Research",
      "authors": "Garcıa, J., and Fernández, F."
    },
    {
      "index": 22,
      "title": "Variance-penalized Markov decision processes: Dynamic programming and reinforcement learning techniques",
      "abstract": "",
      "year": "2014",
      "venue": "International Journal of General Systems",
      "authors": "Gosavi, A."
    },
    {
      "index": 23,
      "title": "Toward the development of an equilibrium capital-market model based on semivariance",
      "abstract": "",
      "year": "1974",
      "venue": "Journal of Financial and Quantitative Analysis",
      "authors": "Hogan, W. W., and Warren, J. M."
    },
    {
      "index": 24,
      "title": "Risk-sensitive Markov decision processes",
      "abstract": "",
      "year": "1972",
      "venue": "Management science",
      "authors": "Howard, R. A., and Matheson, J. E."
    },
    {
      "index": 25,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Janner, M., Fu, J., Zhang, M., and Levine, S."
    },
    {
      "index": 26,
      "title": "Optimal dynamic portfolio selection: Multiperiod mean-variance formulation",
      "abstract": "",
      "year": "2000",
      "venue": "Mathematical Finance",
      "authors": "Li, D., and Ng, W.-L."
    },
    {
      "index": 27,
      "title": "A multi-period fuzzy portfolio optimization model with minimum transaction lots",
      "abstract": "",
      "year": "2015",
      "venue": "European Journal of Operational Research",
      "authors": "Liu, Y.-J., and Zhang, W.-G."
    },
    {
      "index": 28,
      "title": "An optimistic value iteration for mean–variance optimization in discounted markov decision processes",
      "abstract": "",
      "year": "2022",
      "venue": "Results in Control and Optimization",
      "authors": "Ma, S., Ma, X., and Xia, L."
    },
    {
      "index": 29,
      "title": "A unified algorithm framework for mean-variance optimization in discounted Markov decision processes",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv preprint",
      "authors": "Ma, S., Ma, X., and Xia, L.",
      "orig_title": "A unified algorithm framework for mean-variance optimization in discounted Markov decision processes",
      "paper_id": "2201.05737v1"
    },
    {
      "index": 30,
      "title": "Average-Reward Reinforcement Learning with Trust Region Methods",
      "abstract": "",
      "year": "2021",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Ma, X., Tang, X., Xia, L., Yang, J., and Zhao, Q.",
      "orig_title": "Average-reward reinforcement learning with trust region methods",
      "paper_id": "2106.03442v2"
    },
    {
      "index": 31,
      "title": "DSAC: Distributional Soft Actor Critic for Risk-Sensitive Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv preprint",
      "authors": "Ma, X., Xia, L., Zhou, Z., Yang, J., and Zhao, Q.",
      "orig_title": "Dsac: distributional soft actor critic for risk-sensitive reinforcement learning",
      "paper_id": "2004.14547v3"
    },
    {
      "index": 32,
      "title": "Computation of mean-semivariance efficient sets by the critical line algorithm",
      "abstract": "",
      "year": "1993",
      "venue": "Annals of Operations Research",
      "authors": "Markowitz, H., Todd, P., Xu, G., and Yamane, Y."
    },
    {
      "index": 33,
      "title": "Portfolio selection",
      "abstract": "",
      "year": "1952",
      "venue": "Journal of Finance",
      "authors": "Markowitz, H. M."
    },
    {
      "index": 34,
      "title": "Portfolio Selection: Efficient Diversification of Investments",
      "abstract": "",
      "year": "1959",
      "venue": "John Wiley & Sons, New York",
      "authors": "Markowitz, H. M."
    },
    {
      "index": 35,
      "title": "Distributional reinforcement learning for efficient exploration",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Mavrin, B., Yao, H., Kong, L., Wu, K., and Yu, Y."
    },
    {
      "index": 36,
      "title": "Deep Dynamics Models for Learning Dexterous Manipulation",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Robot Learning",
      "authors": "Nagabandi, A., Konolige, K., Levine, S., and Kumar, V.",
      "orig_title": "Deep dynamics models for learning dexterous manipulation",
      "paper_id": "1909.11652v1"
    },
    {
      "index": 37,
      "title": "Convex approximations of chance constrained programs",
      "abstract": "",
      "year": "2007",
      "venue": "SIAM Journal on Optimization",
      "authors": "Nemirovski, A., and Shapiro, A."
    },
    {
      "index": 38,
      "title": "Variance-constrained actor-critic algorithms for discounted and average reward MDPs",
      "abstract": "",
      "year": "2016",
      "venue": "Machine Learning",
      "authors": "Prashanth, L., and Ghavamzadeh, M."
    },
    {
      "index": 39,
      "title": "Risk-averse dynamic programming for Markov decision processes",
      "abstract": "",
      "year": "2010",
      "venue": "Mathematical programming",
      "authors": "Ruszczyński, A."
    },
    {
      "index": 40,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P.",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 41,
      "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P.",
      "orig_title": "High-dimensional continuous control using generalized advantage estimation",
      "paper_id": "1506.02438v6"
    },
    {
      "index": 42,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv preprint",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O."
    },
    {
      "index": 43,
      "title": "Lectures on stochastic programming: modeling and theory",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM",
      "authors": "Shapiro, A., Dentcheva, D., and Ruszczynski, A."
    },
    {
      "index": 44,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al."
    },
    {
      "index": 45,
      "title": "The variance of discounted Markov decision processes",
      "abstract": "",
      "year": "1982",
      "venue": "Journal of Applied Probability",
      "authors": "Sobel, M. J."
    },
    {
      "index": 46,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Sutton, R. S., and Barto, A. G."
    },
    {
      "index": 47,
      "title": "Sequential decision making with coherent risk",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Tamar, A., Chow, Y., Ghavamzadeh, M., and Mannor, S."
    },
    {
      "index": 48,
      "title": "Optimizing the CVaR via sampling",
      "abstract": "",
      "year": "2015",
      "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "authors": "Tamar, A., Glassner, Y., and Mannor, S."
    },
    {
      "index": 49,
      "title": "Mujoco: A physics engine for model-based control",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "authors": "Todorov, E., Erez, T., and Tassa, Y."
    },
    {
      "index": 50,
      "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Nature",
      "authors": "Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al."
    },
    {
      "index": 51,
      "title": "Mean–semivariance optimality for continuous-time Markov decision processes",
      "abstract": "",
      "year": "2019",
      "venue": "Systems & Control Letters",
      "authors": "Wei, Q."
    },
    {
      "index": 52,
      "title": "Optimization of Markov decision processes under the variance criterion",
      "abstract": "",
      "year": "2016",
      "venue": "Automatica",
      "authors": "Xia, L."
    },
    {
      "index": 53,
      "title": "Risk-sensitive Markov decision processes with combined metrics of mean and variance",
      "abstract": "",
      "year": "2020",
      "venue": "Production and Operations Management",
      "authors": "Xia, L."
    },
    {
      "index": 54,
      "title": "A Block Coordinate Ascent Algorithm for Mean-Variance Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Xie, T., Liu, B., Xu, Y., Ghavamzadeh, M., Chow, Y., Lyu, D., and Yoon, D.",
      "orig_title": "A block coordinate ascent algorithm for mean-variance optimization",
      "paper_id": "1809.02292v3"
    },
    {
      "index": 55,
      "title": "Multi-period semi-variance portfolio selection: Model and numerical solution",
      "abstract": "",
      "year": "2007",
      "venue": "Applied Mathematics and Computation",
      "authors": "Yan, W., Miao, R., and Li, S."
    },
    {
      "index": 56,
      "title": "Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Zhang, S., Liu, B., and Whiteson, S.",
      "orig_title": "Mean-variance policy iteration for risk-averse reinforcement learning",
      "paper_id": "2004.10888v6"
    },
    {
      "index": 57,
      "title": "A possibilistic mean-semivariance-entropy model for multi-period portfolio selection with transaction costs",
      "abstract": "",
      "year": "2012",
      "venue": "European Journal of Operational Research",
      "authors": "Zhang, W.-G., Liu, Y.-J., and Xu, W.-J."
    },
    {
      "index": 58,
      "title": "On-Policy Deep Reinforcement Learning for the Average-Reward Criterion",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhang, Y., and Ross, K. W.",
      "orig_title": "On-policy deep reinforcement learning for the average-reward criterion",
      "paper_id": "2106.07329v1"
    },
    {
      "index": 59,
      "title": "Non-crossing quantile regression for distributional reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhou, F., Wang, J., and Feng, X."
    }
  ]
}