{
  "paper_id": "2211.08416v3",
  "title": "Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment",
  "abstract": "Abstract\nWith the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy’s performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human trust and optimizing the policies with weighted behavioral cloning. We evaluate Sirius in simulation and on real hardware, showing that Sirius consistently outperforms baselines over a collection of contact-rich manipulation tasks, achieving an 8% boost in simulation and 27% on real hardware than the state-of-the-art methods in policy success rate, with twice faster convergence and 85% memory size reduction. Videos and more details are available at https://ut-austin-rpl.github.io/sirius/",
  "reference_labels": [
    {
      "index": 0,
      "title": "Opal: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Anurag Ajay et al."
    },
    {
      "index": 1,
      "title": "Learning Dexterous In-Hand Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "IJRR",
      "authors": "Marcin Andrychowicz et al.",
      "orig_title": "Learning Dexterous In-hand Manipulation",
      "paper_id": "1808.00177v5"
    },
    {
      "index": 2,
      "title": "Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences",
      "abstract": "",
      "year": "2022",
      "venue": "IJRR",
      "authors": "Erdem Bıyık et al.",
      "orig_title": "Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences",
      "paper_id": "2006.14091v2"
    },
    {
      "index": 3,
      "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan and Scott Niekum",
      "orig_title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
      "paper_id": "1904.06387v5"
    },
    {
      "index": 4,
      "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.12200",
      "authors": "Serkan Cabi et al.",
      "orig_title": "Scaling Data-driven Robotics With Reward Sketching and Batch Reinforcement Learning",
      "paper_id": "1909.12200v3"
    },
    {
      "index": 5,
      "title": "Interactive Imitation Learning in Robotics: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "Foundations and Trends® in Robotics",
      "authors": "Carlos Celemin et al.",
      "orig_title": "Interactive Imitation Learning in Robotics: A Survey",
      "paper_id": "2211.00600v1"
    },
    {
      "index": 6,
      "title": "Correct Me If I Am Wrong: Interactive Learning for Robotic Manipulation",
      "abstract": "",
      "year": "2021",
      "venue": "RAL",
      "authors": "Eugenio Chisari et al."
    },
    {
      "index": 7,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Paul Christiano et al.",
      "orig_title": "Deep Reinforcement Learning from Human Preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 8,
      "title": "A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges",
      "abstract": "",
      "year": "2020",
      "venue": "DIS",
      "authors": "Christian Arzate Cruz and Takeo Igarashi"
    },
    {
      "index": 9,
      "title": "Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IJCAI",
      "authors": "Yuchen Cui et al."
    },
    {
      "index": 10,
      "title": "Active Reward Learning",
      "abstract": "",
      "year": "2014",
      "venue": "RSS",
      "authors": "Christian Daniel et al."
    },
    {
      "index": 11,
      "title": "Formalizing Assistive Teleoperation",
      "abstract": "",
      "year": "2012",
      "venue": "RSS",
      "authors": "Anca Dragan and Siddhartha Srinivasa"
    },
    {
      "index": 12,
      "title": "A Policy-Blending Formalism for Shared Control",
      "abstract": "",
      "year": "2013",
      "venue": "The International Journal of Robotics Research",
      "authors": "Anca Dragan and Siddhartha Srinivasa"
    },
    {
      "index": 13,
      "title": "Implicit Behavioral Cloning",
      "abstract": "",
      "year": "2021",
      "venue": "CoRL",
      "authors": "Pete Florence et al.",
      "orig_title": "Implicit Behavioral Cloning",
      "paper_id": "2109.00137v1"
    },
    {
      "index": 14,
      "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:1802.01744",
      "authors": "Justin Fu et al.",
      "orig_title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
      "paper_id": "2004.07219v4"
    },
    {
      "index": 15,
      "title": "Off-Policy Deep Reinforcement Learning without Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Scott Fujimoto, David Meger and Doina Precup",
      "orig_title": "Off-policy Deep Reinforcement Learning Without Exploration",
      "paper_id": "1812.02900v3"
    },
    {
      "index": 16,
      "title": "Eliciting Compatible Demonstrations for Multi-Human Imitation Learning",
      "abstract": "",
      "year": "2022",
      "venue": "CoRL",
      "authors": "Kanishk Gandhi, Siddharth Karamcheti, Madeline Liao and Dorsa Sadigh"
    },
    {
      "index": 17,
      "title": "Human-in-the-Loop Optimization of Shared Autonomy in Assistive Robotics",
      "abstract": "",
      "year": "2017",
      "venue": "RAL",
      "authors": "Deepak Gopinath, Siddarth Jain and Brenna D. Argall"
    },
    {
      "index": 18,
      "title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning",
      "abstract": "",
      "year": "2013",
      "venue": "NeurIPS",
      "authors": "S. Griffith et al."
    },
    {
      "index": 19,
      "title": "RL Unplugged: Benchmarks for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Caglar Gulcehre et al."
    },
    {
      "index": 20,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun",
      "orig_title": "Deep Residual Learning for Image Recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 21,
      "title": "ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "CoRL",
      "authors": "Ryan Hoque et al.",
      "orig_title": "ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning",
      "paper_id": "2109.08273v1"
    },
    {
      "index": 22,
      "title": "Shared Autonomy via Hindsight Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "RSS",
      "authors": "Shervin Javdani, Siddhartha S. Srinivasa and J. Bagnell"
    },
    {
      "index": 23,
      "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "CoRL",
      "authors": "Dmitry Kalashnikov et al.",
      "orig_title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-based Robotic Manipulation",
      "paper_id": "1806.10293v3"
    },
    {
      "index": 24,
      "title": "HG-DAgger: Interactive Imitation Learning with Human Experts",
      "abstract": "",
      "year": "2019",
      "venue": "ICRA",
      "authors": "Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell and Mykel J Kochenderfer",
      "orig_title": "HG-DAgger: Interactive Imitation Learning with Human Experts",
      "paper_id": "1810.02890v2"
    },
    {
      "index": 25,
      "title": "Morel: Model-based Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli and Thorsten Joachims"
    },
    {
      "index": 26,
      "title": "Interactively Shaping Agents via Human Reinforcement: The TAMER Framework",
      "abstract": "",
      "year": "2009",
      "venue": "K-CAP",
      "authors": "W. Knox and Peter Stone"
    },
    {
      "index": 27,
      "title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Ilya Kostrikov, Ashvin Nair and Sergey Levine",
      "orig_title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "paper_id": "2110.06169v1"
    },
    {
      "index": 28,
      "title": "When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Aviral Kumar, Joey Hong, Anikait Singh and Sergey Levine",
      "orig_title": "When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?",
      "paper_id": "2204.05618v1"
    },
    {
      "index": 29,
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Aviral Kumar, Aurick Zhou, G. Tucker and Sergey Levine",
      "orig_title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "paper_id": "2006.04779v3"
    },
    {
      "index": 30,
      "title": "Learning Quadrupedal Locomotion over Challenging Terrain",
      "abstract": "",
      "year": "2020",
      "venue": "Science robotics",
      "authors": "Joonho Lee et al.",
      "orig_title": "Learning Quadrupedal Locomotion over Challenging Terrain",
      "paper_id": "2010.11251v1"
    },
    {
      "index": 31,
      "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Kimin Lee, Laura Smith and P. Abbeel",
      "orig_title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
      "paper_id": "2106.05091v1"
    },
    {
      "index": 32,
      "title": "Scalable agent alignment via reward modeling: a research direction",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.07871",
      "authors": "Jan Leike et al.",
      "orig_title": "Scalable Agent Alignment via Reward Modeling: A Research Direction",
      "paper_id": "1811.07871v1"
    },
    {
      "index": 33,
      "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.01643",
      "authors": "Sergey Levine, Aviral Kumar, George Tucker and Justin Fu",
      "orig_title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "paper_id": "2005.01643v3"
    },
    {
      "index": 34,
      "title": "Interactive Learning from Policy-Dependent Human Feedback",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "James MacGlashan et al.",
      "orig_title": "Interactive Learning from Policy-dependent Human Feedback",
      "paper_id": "1701.06049v2"
    },
    {
      "index": 35,
      "title": "Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations",
      "abstract": "",
      "year": "2020",
      "venue": "RSS",
      "authors": "Ajay Mandlekar et al."
    },
    {
      "index": 36,
      "title": "Human-in-the-Loop Imitation Learning using Remote Teleoperation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.06733",
      "authors": "Ajay Mandlekar et al.",
      "orig_title": "Human-in-the-Loop Imitation Learning using Remote Teleoperation",
      "paper_id": "2012.06733v1"
    },
    {
      "index": 37,
      "title": "IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data",
      "abstract": "",
      "year": "2020",
      "venue": "ICRA",
      "authors": "Ajay Mandlekar et al.",
      "orig_title": "IRIS: Implicit Reinforcement Without Interaction at Scale for Learning Control from Offline Robot Manipulation Data",
      "paper_id": "1911.05321v2"
    },
    {
      "index": 38,
      "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
      "abstract": "",
      "year": "2021",
      "venue": "CoRL",
      "authors": "Ajay Mandlekar et al.",
      "orig_title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
      "paper_id": "2108.03298v2"
    },
    {
      "index": 39,
      "title": "EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IROS",
      "authors": "Kunal Menda, Katherine Driggs-Campbell and Mykel J. Kochenderfer"
    },
    {
      "index": 40,
      "title": "Autonomy Infused Teleoperation with Application to BCI Manipulation",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.05451",
      "authors": "Katharina Muelling et al."
    },
    {
      "index": 41,
      "title": "AWAC: Accelerating Online Reinforcement Learning with Offline Datasets",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2006.09359",
      "authors": "Ashvin Nair, Abhishek Gupta, Murtaza Dalal and Sergey Levine"
    },
    {
      "index": 42,
      "title": "Fast Target Prediction of Human Reaching Motion for Cooperative Human-robot Manipulation Tasks Using Time Series Classification",
      "abstract": "",
      "year": "2015",
      "venue": "ICRA",
      "authors": "Claudia Perez-D’Arpino and Julie A. Shah"
    },
    {
      "index": 43,
      "title": "Alvinn: An Autonomous Land Vehicle in a Neural Network",
      "abstract": "",
      "year": "1989",
      "venue": "NeurIPS",
      "authors": "Dean A Pomerleau"
    },
    {
      "index": 44,
      "title": "Shared Autonomy via Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "RSS",
      "authors": "Siddharth Reddy, Sergey Levine and Anca D. Dragan",
      "orig_title": "Shared Autonomy via Deep Reinforcement Learning",
      "paper_id": "1802.01744v2"
    },
    {
      "index": 45,
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-regret Online Learning",
      "abstract": "",
      "year": "2011",
      "venue": "AISTATS",
      "authors": "Stéphane Ross, Geoffrey Gordon and Drew Bagnell"
    },
    {
      "index": 46,
      "title": "Behavioral Cloning from Noisy Demonstrations",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Fumihiro Sasaki and Ryota Yamashina"
    },
    {
      "index": 47,
      "title": "Continuous Integration, Delivery and Deployment: A Systematic Review on Approaches, Tools, Challenges and Practices",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Access",
      "authors": "Mojtaba Shahin, Muhammad Ali Babar and Liming Zhu"
    },
    {
      "index": 48,
      "title": "COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "CoRL",
      "authors": "Avi Singh et al."
    },
    {
      "index": 49,
      "title": "Learning from Interventions: Human-robot interaction as both explicit and implicit feedback",
      "abstract": "",
      "year": "2020",
      "venue": "RSS",
      "authors": "Jonathan Spencer et al."
    },
    {
      "index": 50,
      "title": "Modeling Human Response to Robot Errors for Timely Error Detection",
      "abstract": "",
      "year": "2022",
      "venue": "IROS",
      "authors": "Maia Stiber, Russell Taylor and Chien-Ming Huang",
      "orig_title": "Modeling Human Response to Robot Errors for Timely Error Detection",
      "paper_id": "2208.00565v1"
    },
    {
      "index": 51,
      "title": "Intervention Aware Shared Autonomy",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Weihao Tan et al."
    },
    {
      "index": 52,
      "title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "CoRL",
      "authors": "Xiaofei Wang et al.",
      "orig_title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback",
      "paper_id": "2108.05382v1"
    },
    {
      "index": 53,
      "title": "Critic Regularized Regression",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Ziyu Wang et al.",
      "orig_title": "Critic Regularized Regression",
      "paper_id": "2006.15134v3"
    },
    {
      "index": 54,
      "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Garrett Warnell, Nicholas Waytowich, Vernon Lawhern and Peter Stone",
      "orig_title": "Deep Tamer: Interactive Agent Shaping in High-Dimensional State Spaces",
      "paper_id": "1709.10163v2"
    },
    {
      "index": 55,
      "title": "Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Haoran Xu, Xianyuan Zhan, Honglei Yin and Huiling Qin",
      "orig_title": "Discriminator-weighted Offline Imitation Learning from Suboptimal Demonstrations",
      "paper_id": "2207.10050v1"
    },
    {
      "index": 56,
      "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tianhe Yu et al.",
      "orig_title": "Combo: Conservative Offline Model-Based Policy Optimization",
      "paper_id": "2102.08363v2"
    },
    {
      "index": 57,
      "title": "MOPO: Model-based Offline Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tianhe Yu et al.",
      "orig_title": "Mopo: Model-based Offline Oolicy Optimization",
      "paper_id": "2005.13239v6"
    },
    {
      "index": 58,
      "title": "Leveraging Human Guidance for Deep Reinforcement Learning Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "IJCAI",
      "authors": "Ruohan Zhang et al."
    },
    {
      "index": 59,
      "title": "Human Gaze Assisted Artificial Intelligence: A Review Survey track",
      "abstract": "",
      "year": "2020",
      "venue": "IJCAI",
      "authors": "Ruohan Zhang et al."
    },
    {
      "index": 60,
      "title": "Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation",
      "abstract": "",
      "year": "2018",
      "venue": "ICRA",
      "authors": "Tianhao Zhang et al."
    },
    {
      "index": 61,
      "title": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.12293",
      "authors": "Yuke Zhu, Josiah Wong, Ajay Mandlekar and Roberto Martı́n-Martı́n",
      "orig_title": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
      "paper_id": "2009.12293v3"
    },
    {
      "index": 62,
      "title": "Offline Learning from Demonstrations and Unlabeled Experience",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Konrad Zolna et al.",
      "orig_title": "Offline Learning from Demonstrations and Unlabeled Experience",
      "paper_id": "2011.13885v1"
    }
  ]
}