{
  "paper_id": "2210.06692v2",
  "title": "Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief",
  "abstract": "Abstract\nModel-based offline reinforcement learning (RL) aims to find highly rewarding policy, by leveraging a previously collected static dataset and a dynamics model. While the dynamics model learned through reuse of the static dataset, its generalization ability hopefully promotes policy learning if properly utilized. To that end, several works propose to quantify the uncertainty of predicted dynamics, and explicitly apply it to penalize reward. However, as the dynamics and the reward are intrinsically different factors in context of MDP, characterizing the impact of dynamics uncertainty through reward penalty may incur unexpected tradeoff between model utilization and risk avoidance. In this work, we instead maintain a belief distribution over dynamics, and evaluate/optimize policy through biased sampling from the belief. The sampling procedure, biased towards pessimism, is derived based on an alternating Markov game formulation of offline RL. We formally show that the biased sampling naturally induces an updated dynamics belief with policy-dependent reweighting factor, termed Pessimism-Modulated Dynamics Belief. To improve policy, we devise an iterative regularized policy optimization algorithm for the game, with guarantee of monotonous improvement under certain condition. To make practical, we further devise an offline RL algorithm to approximately find the solution. Empirical results show that the proposed approach achieves state-of-the-art performance on a wide range of benchmark tasks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE ICRA",
      "authors": "Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine."
    },
    {
      "index": 1,
      "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE ICRA",
      "authors": "Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine"
    },
    {
      "index": 2,
      "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE TITS",
      "authors": "B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Pérez",
      "orig_title": "Deep reinforcement learning for autonomous driving: A survey",
      "paper_id": "2002.00444v2"
    },
    {
      "index": 3,
      "title": "Reinforcement learning in healthcare: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Computing Surveys",
      "authors": "Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin"
    },
    {
      "index": 4,
      "title": "Off-Policy Deep Reinforcement Learning without Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Scott Fujimoto, David Meger, and Doina Precup",
      "orig_title": "Off-policy deep reinforcement learning without exploration",
      "paper_id": "1812.02900v3"
    },
    {
      "index": 5,
      "title": "Stabilizing off-policy Q-learning via bootstrapping error reduction",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine"
    },
    {
      "index": 6,
      "title": "MOReL: Model-based offline reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims"
    },
    {
      "index": 7,
      "title": "Behavior Regularized Offline Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.11361",
      "authors": "Yifan Wu, George Tucker, and Ofir Nachum",
      "orig_title": "Behavior regularized offline reinforcement learning",
      "paper_id": "1911.11361v1"
    },
    {
      "index": 8,
      "title": "EMaQ: Expected-max Q-learning operator for simple yet effective offline and online RL",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu"
    },
    {
      "index": 9,
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine",
      "orig_title": "Conservative Q-learning for offline reinforcement learning",
      "paper_id": "2006.04779v3"
    },
    {
      "index": 10,
      "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum",
      "orig_title": "Offline reinforcement learning with fisher divergence critic regularization",
      "paper_id": "2103.08050v1"
    },
    {
      "index": 11,
      "title": "AlgaeDICE: Policy Gradient from Arbitrary Experience",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.02074",
      "authors": "Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans",
      "orig_title": "AlgaeDICE: Policy gradient from arbitrary experience",
      "paper_id": "1912.02074v1"
    },
    {
      "index": 12,
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell",
      "orig_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "paper_id": "1612.01474v3"
    },
    {
      "index": 13,
      "title": "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song",
      "orig_title": "Uncertainty-based offline reinforcement learning with diversified Q-ensemble",
      "paper_id": "2110.01548v2"
    },
    {
      "index": 14,
      "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.11566",
      "authors": "Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang",
      "orig_title": "Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning",
      "paper_id": "2202.11566v1"
    },
    {
      "index": 15,
      "title": "Bayesian deep learning and a probabilistic perspective of generalization",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Andrew G Wilson and Pavel Izmailov"
    },
    {
      "index": 16,
      "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson",
      "orig_title": "A simple baseline for Bayesian uncertainty in deep learning",
      "paper_id": "1902.02476v2"
    },
    {
      "index": 17,
      "title": "TossingBot: Learning to Throw Arbitrary Objects with Residual Physics",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE T-RO",
      "authors": "Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser",
      "orig_title": "Tossingbot: Learning to throw arbitrary objects with residual physics",
      "paper_id": "1903.11239v3"
    },
    {
      "index": 18,
      "title": "Invariant Causal Prediction for Block MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup",
      "orig_title": "Invariant causal prediction for block MDPs",
      "paper_id": "2003.06016v2"
    },
    {
      "index": 19,
      "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine",
      "orig_title": "Learning invariant representations for reinforcement learning without reconstruction",
      "paper_id": "2006.10742v2"
    },
    {
      "index": 20,
      "title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Philip J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts",
      "orig_title": "Augmented world models facilitate zero-shot dynamics generalization from a single offline environment",
      "paper_id": "2104.05632v3"
    },
    {
      "index": 21,
      "title": "MOPO: Model-based Offline Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma",
      "orig_title": "MOPO: Model-based offline policy optimization",
      "paper_id": "2005.13239v6"
    },
    {
      "index": 22,
      "title": "Revisiting design choices in model-based offline reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Cong Lu, Philip J. Ball, Jack Parker-Holder, Michael A. Osborne, and Stephen J. Roberts"
    },
    {
      "index": 23,
      "title": "Robust control of Markov decision processes with uncertain transition matrices",
      "abstract": "",
      "year": "2005",
      "venue": "Operations Research",
      "authors": "Arnab Nilim and Laurent El Ghaoui"
    },
    {
      "index": 24,
      "title": "Robust Markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "Mathematics of Operations Research",
      "authors": "Wolfram Wiesemann, Daniel Kuhn, and Berç Rustem"
    },
    {
      "index": 25,
      "title": "Soft-robust algorithms for batch reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.14495",
      "authors": "Elita A. Lobo, Mohammad Ghavamzadeh, and Marek Petrik"
    },
    {
      "index": 26,
      "title": "Soft-robust actor-critic policy-gradient",
      "abstract": "",
      "year": "2018",
      "venue": "UAI",
      "authors": "Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor"
    },
    {
      "index": 27,
      "title": "Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Marek Petrik and Reazul Hasan Russel",
      "orig_title": "Beyond confidence regions: Tight Bayesian ambiguity sets for robust MDPs",
      "paper_id": "1902.07605v1"
    },
    {
      "index": 28,
      "title": "Optimizing percentile criterion using robust MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "AISTATS",
      "authors": "Bahram Behzadian, Reazul Hasan Russel, Marek Petrik, and Chin Pang Ho"
    },
    {
      "index": 29,
      "title": "Safe Policy Improvement with Baseline Bootstrapping",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes",
      "orig_title": "Safe policy improvement with baseline bootstrapping",
      "paper_id": "1712.06924v5"
    },
    {
      "index": 30,
      "title": "High-confidence off-policy evaluation",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI",
      "authors": "Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh"
    },
    {
      "index": 31,
      "title": "Fast Bellman updates for robust MDPs",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann"
    },
    {
      "index": 32,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "1999",
      "venue": "NeurIPS",
      "authors": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour"
    },
    {
      "index": 33,
      "title": "Deterministic policy gradient algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller"
    },
    {
      "index": 34,
      "title": "Reinforcement Learning with Deep Energy-Based Policies",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Reinforcement learning with deep energy-based policies",
      "paper_id": "1702.08165v2"
    },
    {
      "index": 35,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 36,
      "title": "A Theory of Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Matthieu Geist, Bruno Scherrer, and Olivier Pietquin",
      "orig_title": "A theory of regularized Markov decision processes",
      "paper_id": "1901.11275v2"
    },
    {
      "index": 37,
      "title": "Information asymmetry in KL-regularized RL",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Alexandre Galashov, Siddhant M. Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess",
      "orig_title": "Information asymmetry in KL-regularized RL",
      "paper_id": "1905.01240v1"
    },
    {
      "index": 38,
      "title": "Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin A. Riedmiller",
      "orig_title": "Keep doing what worked: Behavioral modelling priors for offline reinforcement learning",
      "paper_id": "2002.08396v3"
    },
    {
      "index": 39,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al."
    },
    {
      "index": 40,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra"
    },
    {
      "index": 41,
      "title": "On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.09168",
      "authors": "Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius"
    },
    {
      "index": 42,
      "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.07219",
      "authors": "Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine",
      "orig_title": "D4RL: Datasets for deep data-driven reinforcement learning",
      "paper_id": "2004.07219v4"
    },
    {
      "index": 43,
      "title": "When to trust your model: Model-based policy optimization",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine"
    },
    {
      "index": 44,
      "title": "Representation balancing offline model-based reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Byung-Jun Lee, Jongmin Lee, and Kim Kee-Eung"
    },
    {
      "index": 45,
      "title": "Weighted model estimation for offline model-based reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Toru Hishinuma and Kei Senda"
    },
    {
      "index": 46,
      "title": "Model-Based Offline Planning",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Arthur Argenson and Gabriel Dulac-Arnold",
      "orig_title": "Model-based offline planning",
      "paper_id": "2008.05556v3"
    },
    {
      "index": 47,
      "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu",
      "orig_title": "Deployment-efficient reinforcement learning via model-based offline optimization",
      "paper_id": "2006.03647v2"
    },
    {
      "index": 48,
      "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn",
      "orig_title": "COMBO: Conservative offline model-based policy optimization",
      "paper_id": "2102.08363v2"
    },
    {
      "index": 49,
      "title": "Optimization of conditional value-at-risk",
      "abstract": "",
      "year": "2000",
      "venue": "Journal of Risk",
      "authors": "RT Rockafellar and Stanislav Uryasev"
    },
    {
      "index": 50,
      "title": "Scaling up robust MDPs by reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1306.6189",
      "authors": "Aviv Tamar, Huan Xu, and Shie Mannor"
    },
    {
      "index": 51,
      "title": "Safe Policy Improvement with Baseline Bootstrapping",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes",
      "orig_title": "Safe policy improvement with baseline bootstrapping",
      "paper_id": "1712.06924v5"
    },
    {
      "index": 52,
      "title": "Robust Reinforcement Learning for Continuous Control with Model Misspecification",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Daniel J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy A. Mann, Todd Hester, and Martin A. Riedmiller",
      "orig_title": "Robust reinforcement learning for continuous control with model misspecification",
      "paper_id": "1906.07516v2"
    },
    {
      "index": 53,
      "title": "Robust Adversarial Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta",
      "orig_title": "Robust adversarial reinforcement learning",
      "paper_id": "1703.02702v1"
    },
    {
      "index": 54,
      "title": "Action Robust Reinforcement Learning and Applications in Continuous Control",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Chen Tessler, Yonathan Efroni, and Shie Mannor",
      "orig_title": "Action robust reinforcement learning and applications in continuous control",
      "paper_id": "1901.09184v2"
    },
    {
      "index": 55,
      "title": "Distributionally Robust Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ICML Workshop",
      "authors": "Elena Smirnova, Elvis Dohmatob, and Jérémie Mary",
      "orig_title": "Distributionally robust reinforcement learning",
      "paper_id": "1902.08708v2"
    },
    {
      "index": 56,
      "title": "Planning for Risk-Aversion and Expected Value in MDPs",
      "abstract": "",
      "year": "2022",
      "venue": "ICAPS",
      "authors": "Marc Rigter, Paul Duckworth, Bruno Lacerda, and Nick Hawes",
      "orig_title": "Planning for risk-aversion and expected value in MDPs",
      "paper_id": "2110.12746v2"
    },
    {
      "index": 57,
      "title": "A Bayesian approach to robust reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence Conference",
      "authors": "Esther Derman, Daniel Mankowitz, Timothy Mann, and Shie Mannor"
    },
    {
      "index": 58,
      "title": "Risk-Averse Bayes-Adaptive Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Marc Rigter, Bruno Lacerda, and Nick Hawes",
      "orig_title": "Risk-averse Bayes-adaptive reinforcement learning",
      "paper_id": "2102.05762v2"
    },
    {
      "index": 59,
      "title": "Order Statistics",
      "abstract": "",
      "year": "2004",
      "venue": "Wiley",
      "authors": "H.A. David and H.N. Nagaraja"
    },
    {
      "index": 60,
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1805.00909",
      "authors": "Sergey Levine",
      "orig_title": "Reinforcement learning and control as probabilistic inference: Tutorial and review",
      "paper_id": "1805.00909v3"
    }
  ]
}