{
  "paper_id": "2011.00583v4",
  "title": "An Overview of Multi-agent Reinforcement Learning from Game Theoretical Perspective",
  "abstract": "Abstract\nFollowing the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques.\nMARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously.\nIt is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation.\nAlthough MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010.\nIn this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier.\nOur work is separated into two parts. From §§\\S1 to §§\\S4, we present the self-contained fundamental knowledge of MARL, including problem formulations, basic solutions, and existing challenges.\nSpecifically, we present the MARL formulations through two representative frameworks, namely, stochastic games and extensive-form games, along with different variations of games that can be addressed. The goal of this part is to enable the readers, even those with minimal related background, to grasp the key ideas in MARL research.\nFrom §§\\S5 to §§\\S9, we present an overview of recent developments of MARL algorithms. Starting from new taxonomies for MARL methods, we conduct a survey of previous survey papers.\nIn later sections, we highlight several modern topics in MARL research, including Q-function factorisation, multi-agent soft learning, networked multi-agent MDP, stochastic potential games, zero-sum continuous games, online MDP, turn-based stochastic games, policy space response oracle, approximation methods in general-sum games, and mean-field type learning in games with infinite agents.\nWithin each topic, we select both the most fundamental and cutting-edge algorithms.\nThe goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective.\nWe expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Politex: Regret bounds for policy iteration using expert prediction",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and Weisz, G."
    },
    {
      "index": 1,
      "title": "A multiagent reinforcement learning algorithm with non-linear dynamics",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Abdallah, S. and Lesser, V."
    },
    {
      "index": 2,
      "title": "Wasserstein robust reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.13196",
      "authors": "Abdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V., Luo, R., Zhang, M., and Wang, J."
    },
    {
      "index": 3,
      "title": "The equivalence of linear programs and zero-sum games",
      "abstract": "",
      "year": "2013",
      "venue": "International Journal of Game Theory",
      "authors": "Adler, I."
    },
    {
      "index": 4,
      "title": "A cooperative multi-agent transportation management and route guidance system",
      "abstract": "",
      "year": "2002",
      "venue": "Transportation Research Part C: Emerging Technologies",
      "authors": "Adler, J. L. and Blue, V. J."
    },
    {
      "index": 5,
      "title": "Local saddle point optimization: A curvature exploitation approach",
      "abstract": "",
      "year": "2019",
      "venue": "The 22nd International Conference on Artificial Intelligence and Statistics",
      "authors": "Adolphs, L., Daneshmand, H., Lucchi, A., and Hofmann, T."
    },
    {
      "index": 6,
      "title": "Model-free q-learning designs for linear discrete-time zero-sum games with application to h-infinity control",
      "abstract": "",
      "year": "2007",
      "venue": "Automatica",
      "authors": "Al-Tamimi, A., Lewis, F. L., and Abu-Khalaf, M."
    },
    {
      "index": 7,
      "title": "Optimizing fixed-size stochastic controllers for pomdps and decentralized pomdps",
      "abstract": "",
      "year": "2010",
      "venue": "Autonomous Agents and Multi-Agent Systems",
      "authors": "Amato, C., Bernstein, D. S., and Zilberstein, S."
    },
    {
      "index": 8,
      "title": "Fitted q-learning in mean-field games",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.13309",
      "authors": "Anahtarcı, B., Karıksız, C. D., and Saldi, N."
    },
    {
      "index": 9,
      "title": "Q-learning in regularized mean-field games",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.12151",
      "authors": "Anahtarci, B., Kariksiz, C. D., and Saldi, N."
    },
    {
      "index": 10,
      "title": "A maximum principle for sdes of mean-field type",
      "abstract": "",
      "year": "2011",
      "venue": "Applied Mathematics & Optimization",
      "authors": "Andersson, D. and Djehiche, B."
    },
    {
      "index": 11,
      "title": "Decentralized q-learning for stochastic teams and games",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Arslan, G. and Yüksel, S."
    },
    {
      "index": 12,
      "title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem",
      "abstract": "",
      "year": "1995",
      "venue": "IEEE 36th Annual Foundations of Computer Science",
      "authors": "Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E."
    },
    {
      "index": 13,
      "title": "The nonstochastic multiarmed bandit problem",
      "abstract": "",
      "year": "2002",
      "venue": "SIAM journal on computing",
      "authors": "Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E."
    },
    {
      "index": 14,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in neural information processing systems",
      "authors": "Auer, P., Jaksch, T., and Ortner, R."
    },
    {
      "index": 15,
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Azar, M. G., Osband, I., and Munos, R.",
      "orig_title": "Minimax regret bounds for reinforcement learning",
      "paper_id": "1703.05449v2"
    },
    {
      "index": 16,
      "title": "Emergent tool use from multi-agent autocurricula",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and\nMordatch, I. (2019a)."
    },
    {
      "index": 17,
      "title": "Emergent tool use from multi-agent autocurricula",
      "abstract": "",
      "year": "1909",
      "venue": "CoRR, abs/",
      "authors": "Baker, B., Kanitscheider, I., Markov, T. M., Wu, Y., Powell, G., McGrew, B.,\nand Mordatch, I. (2019b)."
    },
    {
      "index": 18,
      "title": "Open-ended Learning in Symmetric Zero-sum Games",
      "abstract": "",
      "year": "2019",
      "venue": "ICML, volume 97",
      "authors": "Balduzzi, D., Garnelo, M., Bachrach, Y., Czarnecki, W., Pérolat, J.,\nJaderberg, M., and Graepel, T. (2019).",
      "orig_title": "Open-ended learning in symmetric zero-sum games",
      "paper_id": "1901.08106v2"
    },
    {
      "index": 19,
      "title": "The mechanics of n-player differentiable games",
      "abstract": "",
      "year": "",
      "venue": "ICML, volume 80",
      "authors": "Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., and Graepel,\nT. (2018a)."
    },
    {
      "index": 20,
      "title": "Re-evaluating Evaluation",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Balduzzi, D., Tuyls, K., Perolat, J., and Graepel, T. (2018b).",
      "orig_title": "Re-evaluating evaluation",
      "paper_id": "1806.02643v2"
    },
    {
      "index": 21,
      "title": "On the theory of dynamic programming",
      "abstract": "",
      "year": "1952",
      "venue": "Proceedings of the National Academy of Sciences of the United\nStates of America, 38(8):716",
      "authors": "Bellman, R. (1952)."
    },
    {
      "index": 22,
      "title": "Consistency of vanishingly smooth fictitious play",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Benaïm, M. and Faure, M. (2013)."
    },
    {
      "index": 23,
      "title": "Mixed equilibria and dynamical systems arising from fictitious play in perturbed games",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": "Benaım, M. and Hirsch, M. W. (1999)."
    },
    {
      "index": 24,
      "title": "Partitioning procedures for solving mixed-variable program-ming problems, numerische matkematic 4",
      "abstract": "",
      "year": "1962",
      "venue": "",
      "authors": "Benders, J. (1962)."
    },
    {
      "index": 25,
      "title": "Learning deep architectures for AI",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Bengio, Y. (2009)."
    },
    {
      "index": 26,
      "title": "Mean field games and mean field type control theory, volume 101",
      "abstract": "",
      "year": "2013",
      "venue": "Springer",
      "authors": "Bensoussan, A., Frehse, J., Yam, P., et al. (2013)."
    },
    {
      "index": 27,
      "title": "Brown’s original fictitious play",
      "abstract": "",
      "year": "2007",
      "venue": "Journal of Economic Theory, 135(1):572–578",
      "authors": "Berger, U. (2007)."
    },
    {
      "index": 28,
      "title": "Policy iteration for decentralized control of markov decision processes",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Artificial Intelligence Research, 34:89–132",
      "authors": "Bernstein, D. S., Amato, C., Hansen, E. A., and Zilberstein, S. (2009)."
    },
    {
      "index": 29,
      "title": "The complexity of decentralized control of markov decision processes",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": "Bernstein, D. S., Givan, R., Immerman, N., and Zilberstein, S. (2002)."
    },
    {
      "index": 30,
      "title": "The dynamic programming algorithm",
      "abstract": "",
      "year": "2005",
      "venue": "Dynamic Programming and Optimal Control; Athena Scientific:\nNashua, NH, USA",
      "authors": "Bertsekas, D. P. (2005)."
    },
    {
      "index": 31,
      "title": "Neuro-dynamic programming",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Bertsekas, D. P. and Tsitsiklis, J. N. (1996)."
    },
    {
      "index": 32,
      "title": "Approximating game-theoretic optimal strategies for full-scale poker",
      "abstract": "",
      "year": "2003",
      "venue": "IJCAI, volume 3",
      "authors": "Billings, D., Burch, N., Davidson, A., Holte, R., Schaeffer, J., Schauenberg,\nT., and Szafron, D. (2003)."
    },
    {
      "index": 33,
      "title": "An analog of the minimax theorem for vector payoffs",
      "abstract": "",
      "year": "1956",
      "venue": "Pacific Journal of Mathematics, 6(1):1–8",
      "authors": "Blackwell, D. et al. (1956)."
    },
    {
      "index": 34,
      "title": "Variational Inference: A Review for Statisticians",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of the American statistical Association,\n112(518):859–877",
      "authors": "Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017).",
      "orig_title": "Variational inference: A review for statisticians",
      "paper_id": "1601.00670v9"
    },
    {
      "index": 35,
      "title": "Evolutionary dynamics of multi-agent learning: A survey",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Artificial Intelligence Research, 53:659–697",
      "authors": "Bloembergen, D., Tuyls, K., Hennes, D., and Kaisers, M. (2015)."
    },
    {
      "index": 36,
      "title": "The statistical mechanics of strategic interaction",
      "abstract": "",
      "year": "1993",
      "venue": "",
      "authors": "Blume, L. E. (1993)."
    },
    {
      "index": 37,
      "title": "Stochastic approximation with two time scales",
      "abstract": "",
      "year": "1997",
      "venue": "",
      "authors": "Borkar, V. S. (1997)."
    },
    {
      "index": 38,
      "title": "Reinforcement learning in markovian evolutionary games",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": "Borkar, V. S. (2002)."
    },
    {
      "index": 39,
      "title": "Decision-theoretic planning: Structural assumptions and computational leverage",
      "abstract": "",
      "year": "1999",
      "venue": "Journal of Artificial Intelligence Research, 11:1–94",
      "authors": "Boutilier, C., Dean, T., and Hanks, S. (1999)."
    },
    {
      "index": 40,
      "title": "Convergence problems of general-sum multiagent reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "ICML",
      "authors": "Bowling, M. (2000)."
    },
    {
      "index": 41,
      "title": "Convergence and no-regret in multiagent learning",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Bowling, M. (2005)."
    },
    {
      "index": 42,
      "title": "An analysis of stochastic game theory for multiagent reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": "Bowling, M. and Veloso, M. (2000)."
    },
    {
      "index": 43,
      "title": "Rational and convergent learning in stochastic games",
      "abstract": "",
      "year": "2001",
      "venue": "International joint conference on artificial intelligence,\nvolume 17",
      "authors": "Bowling, M. and Veloso, M. (2001)."
    },
    {
      "index": 44,
      "title": "Multiagent learning using a variable learning rate",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": "Bowling, M. and Veloso, M. (2002)."
    },
    {
      "index": 45,
      "title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research, 3(Oct):213–231",
      "authors": "Brafman, R. I. and Tennenholtz, M. (2002)."
    },
    {
      "index": 46,
      "title": "On the computation of equilibria in discounted stochastic dynamic games",
      "abstract": "",
      "year": "1986",
      "venue": "Dynamic games and applications in economics, pages 64–87.\nSpringer",
      "authors": "Breton, M., Filar, J. A., Haurle, A., and Schultz, T. A. (1986)."
    },
    {
      "index": 47,
      "title": "Dynamic thresholding and pruning for regret minimization",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "Brown, N., Kroer, C., and Sandholm, T. (2017)."
    },
    {
      "index": 48,
      "title": "Deep Counterfactual Regret Minimization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Brown, N., Lerer, A., Gross, S., and Sandholm, T. (2019).",
      "orig_title": "Deep counterfactual regret minimization",
      "paper_id": "1811.00164v3"
    },
    {
      "index": 49,
      "title": "Regret-based pruning in extensive-form games",
      "abstract": "",
      "year": "1980",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Brown, N. and Sandholm, T. (2015)."
    },
    {
      "index": 50,
      "title": "Reduced space and faster convergence in imperfect-information games via pruning",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "Brown, N. and Sandholm, T. (2017)."
    },
    {
      "index": 51,
      "title": "Superhuman ai for heads-up no-limit poker: Libratus beats top professionals",
      "abstract": "",
      "year": "2018",
      "venue": "Science, 359(",
      "authors": "Brown, N. and Sandholm, T. (2018)."
    },
    {
      "index": 52,
      "title": "Superhuman ai for multiplayer poker",
      "abstract": "",
      "year": "2019",
      "venue": "Science, 365(",
      "authors": "Brown, N. and Sandholm, T. (2019)."
    },
    {
      "index": 53,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020).",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 54,
      "title": "A survey of monte carlo tree search methods",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in\ngames, 4(1):1–43",
      "authors": "Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I.,\nRohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S.\n(2012)."
    },
    {
      "index": 55,
      "title": "Global Convergence of Policy Gradient for Sequential Zero-Sum Linear Quadratic Dynamic Games",
      "abstract": "",
      "year": "1911",
      "venue": "arXiv",
      "authors": "Bu, J., Ratliff, L. J., and Mesbahi, M. (2019).",
      "orig_title": "Global convergence of policy gradient for sequential zero-sum linear quadratic dynamic games",
      "paper_id": "1911.04672v1"
    },
    {
      "index": 56,
      "title": "A general stochastic maximum principle for sdes of mean-field type",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Buckdahn, R., Djehiche, B., and Li, J. (2011)."
    },
    {
      "index": 57,
      "title": "Efficient monte carlo counterfactual regret minimization in games with many player actions",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Burch, N., Lanctot, M., Szafron, D., and Gibson, R. G. (2012)."
    },
    {
      "index": 58,
      "title": "Multi-agent reinforcement learning: An overview",
      "abstract": "",
      "year": "2010",
      "venue": "Innovations in multi-agent systems and applications-1",
      "authors": "Buşoniu, L., Babuška, R., and De Schutter, B. (2010)."
    },
    {
      "index": 59,
      "title": "Provably Efficient Exploration in Policy Optimization",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2019a).",
      "orig_title": "Provably efficient exploration in policy optimization",
      "paper_id": "1912.05830v4"
    },
    {
      "index": 60,
      "title": "Neural temporal-difference learning converges to global optima",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Cai, Q., Yang, Z., Lee, J. D., and Wang, Z. (2019b)."
    },
    {
      "index": 61,
      "title": "Sophisticated experience-weighted attraction learning and strategic teaching in repeated games",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Economic theory, 104(1):137–188",
      "authors": "Camerer, C. F., Ho, T.-H., and Chong, J.-K. (2002)."
    },
    {
      "index": 62,
      "title": "A cognitive hierarchy model of games",
      "abstract": "",
      "year": "2004",
      "venue": "The Quarterly Journal of Economics",
      "authors": "Camerer, C. F., Ho, T.-H., and Chong, J.-K. (2004)."
    },
    {
      "index": 63,
      "title": "Multiagent systems in automotive applications",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Campos-Rodriguez, R., Gonzalez-Jimenez, L., Cervantes-Alvarez, F.,\nAmezcua-Garcia, F., and Fernandez-Garcia, M. (2017)."
    },
    {
      "index": 64,
      "title": "Flows and decompositions of games: Harmonic and potential games",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Candogan, O., Menache, I., Ozdaglar, A., and Parrilo, P. A. (2011)."
    },
    {
      "index": 65,
      "title": "Learning to rank: from pairwise approach to listwise approach",
      "abstract": "",
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine\nlearning",
      "authors": "Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, H. (2007)."
    },
    {
      "index": 66,
      "title": "The master equation and the convergence problem in mean field games",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Cardaliaguet, P., Delarue, F., Lasry, J.-M., and Lions, P.-L. (2015)."
    },
    {
      "index": 67,
      "title": "Learning in mean field games: the fictitious play",
      "abstract": "",
      "year": "2017",
      "venue": "ESAIM: Control, Optimisation and Calculus of Variations,\n23(2):569–591",
      "authors": "Cardaliaguet, P. and Hadikhanloo, S. (2017)."
    },
    {
      "index": 68,
      "title": "Mean field game of controls and an application to trade crowding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Cardaliaguet, P. and Lehalle, C.-A. (2018)."
    },
    {
      "index": 69,
      "title": "Probabilistic analysis of mean-field games",
      "abstract": "",
      "year": "2013",
      "venue": "SIAM Journal on Control and Optimization, 51(4):",
      "authors": "Carmona, R. and Delarue, F. (2013)."
    },
    {
      "index": 70,
      "title": "Forward–backward stochastic differential equations and controlled mckean–vlasov dynamics",
      "abstract": "",
      "year": "",
      "venue": "The Annals of Probability, 43(5):",
      "authors": "Carmona, R., Delarue, F., et al. (2015a)."
    },
    {
      "index": 71,
      "title": "Probabilistic Theory of Mean Field Games with Applications I-II",
      "abstract": "",
      "year": "2018",
      "venue": "Springer",
      "authors": "Carmona, R., Delarue, F., et al. (2018)."
    },
    {
      "index": 72,
      "title": "Control of mckean–vlasov dynamics versus mean field games",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Carmona, R., Delarue, F., and Lachapelle, A. (2013)."
    },
    {
      "index": 73,
      "title": "Mean field games with common noise",
      "abstract": "",
      "year": "2016",
      "venue": "The Annals of Probability, 44(6):",
      "authors": "Carmona, R., Delarue, F., Lacker, D., et al. (2016)."
    },
    {
      "index": 74,
      "title": "A probabilistic weak formulation of mean field games and applications",
      "abstract": "",
      "year": "",
      "venue": "The Annals of Applied Probability, 25(3):",
      "authors": "Carmona, R., Lacker, D., et al. (2015b)."
    },
    {
      "index": 75,
      "title": "Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods",
      "abstract": "",
      "year": "1910",
      "venue": "arXiv preprint arXiv:",
      "authors": "Carmona, R., Laurière, M., and Tan, Z. (2019a)."
    },
    {
      "index": 76,
      "title": "Model-free mean-field reinforcement learning: mean-field mdp and mean-field q-learning",
      "abstract": "",
      "year": "1910",
      "venue": "arXiv preprint arXiv:",
      "authors": "Carmona, R., Laurière, M., and Tan, Z. (2019b)."
    },
    {
      "index": 77,
      "title": "On the convergence problem in mean field games: a two state model without uniqueness",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Control and Optimization, 57(4):",
      "authors": "Cecchin, A., Pra, P. D., Fischer, M., and Pelino, G. (2019)."
    },
    {
      "index": 78,
      "title": "Prediction, learning, and games",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Cesa-Bianchi, N. and Lugosi, G. (2006)."
    },
    {
      "index": 79,
      "title": "Is reinforcement learning worth the hype? 2020",
      "abstract": "",
      "year": "2020",
      "venue": "URL\nhttps://www.capgemini.com/gb-en/",
      "authors": "Chalmers, C. (2020)."
    },
    {
      "index": 80,
      "title": "Convergence Analysis of Gradient-Based Learning with Non-Uniform Learning Rates in Non-Cooperative Multi-Agent Settings",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chasnov, B., Ratliff, L. J., Mazumdar, E., and Burden, S. A. (2019).",
      "orig_title": "Convergence analysis of gradient-based learning with non-uniform learning rates in non-cooperative multi-agent settings",
      "paper_id": "1906.00731v1"
    },
    {
      "index": 81,
      "title": "Settling the complexity of two-player nash equilibrium",
      "abstract": "",
      "year": "2006",
      "venue": "2006 47th Annual IEEE Symposium on Foundations of Computer\nScience (FOCS’06)",
      "authors": "Chen, X. and Deng, X. (2006)."
    },
    {
      "index": 82,
      "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Cheung, W. C., Simchi-Levi, D., and Zhu, R. (2020).",
      "orig_title": "Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism",
      "paper_id": "2006.14389v1"
    },
    {
      "index": 83,
      "title": "The dynamics of reinforcement learning in cooperative multiagent systems",
      "abstract": "",
      "year": "1998",
      "venue": "AAAI/IAAI",
      "authors": "Claus, C. and Boutilier, C. (1998a)."
    },
    {
      "index": 84,
      "title": "The dynamics of reinforcement learning in cooperative multiagent systems",
      "abstract": "",
      "year": "1998",
      "venue": "AAAI/IAAI",
      "authors": "Claus, C. and Boutilier, C. (1998b)."
    },
    {
      "index": 85,
      "title": "Complexity results about nash equilibria",
      "abstract": "",
      "year": "2002",
      "venue": "arXiv preprint cs/",
      "authors": "Conitzer, V. and Sandholm, T. (2002)."
    },
    {
      "index": 86,
      "title": "Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Conitzer, V. and Sandholm, T. (2007)."
    },
    {
      "index": 87,
      "title": "New complexity results about nash equilibria",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Conitzer, V. and Sandholm, T. (2008)."
    },
    {
      "index": 88,
      "title": "Neural correlates of depth of strategic reasoning in medial prefrontal cortex",
      "abstract": "",
      "year": "2009",
      "venue": "Proceedings of the National Academy of Sciences,\n106(23):",
      "authors": "Coricelli, G. and Nagel, R. (2009)."
    },
    {
      "index": 89,
      "title": "Information set monte carlo tree search",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in\nGames, 4(2):120–143",
      "authors": "Cowling, P. I., Powley, E. J., and Whitehouse, D. (2012)."
    },
    {
      "index": 90,
      "title": "A survey on transfer learning for multiagent reinforcement learning systems",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Artificial Intelligence Research, 64:645–703",
      "authors": "Da Silva, F. L. and Costa, A. H. R. (2019)."
    },
    {
      "index": 91,
      "title": "Distributed optimal power flow for smart microgrids",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Smart Grid, 4(3):",
      "authors": "Dall’Anese, E., Zhu, H., and Giannakis, G. B. (2013)."
    },
    {
      "index": 92,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "1951",
      "venue": "",
      "authors": "Dantzig, G. (1951).",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 93,
      "title": "The complexity of computing a nash equilibrium",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Computing, 39(1):195–259",
      "authors": "Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. (2009)."
    },
    {
      "index": 94,
      "title": "Training GANs with Optimism",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H. (2017).",
      "orig_title": "Training gans with optimism",
      "paper_id": "1711.00141v2"
    },
    {
      "index": 95,
      "title": "The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Daskalakis, C. and Panageas, I. (2018).",
      "orig_title": "The limit points of (optimistic) gradient descent in min-max optimization",
      "paper_id": "1807.03907v1"
    },
    {
      "index": 96,
      "title": "Three-player games are hard",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Daskalakis, C. and Papadimitriou, C. H. (2005)."
    },
    {
      "index": 97,
      "title": "Pilco: A model-based and data-efficient approach to policy search",
      "abstract": "",
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on machine\nlearning (ICML-11)",
      "authors": "Deisenroth, M. and Rasmussen, C. E. (2011)."
    },
    {
      "index": 98,
      "title": "Selection of equilibria in a linear quadratic mean-field game",
      "abstract": "",
      "year": "2020",
      "venue": "Stochastic Processes and their Applications, 130(2):",
      "authors": "Delarue, F. and Tchuendom, R. F. (2020)."
    },
    {
      "index": 99,
      "title": "A review on the applications of multiagent systems in wireless sensor networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Journal of Distributed Sensor Networks,\n15(5):",
      "authors": "Derakhshan, F. and Yousefi, S. (2019)."
    },
    {
      "index": 100,
      "title": "Solving stochastic games",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dermed, L. M. and Isbell, C. L. (2009)."
    },
    {
      "index": 101,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 102,
      "title": "Learning to act in decentralized partially observable mdps",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Dibangoye, J. and Buffet, O. (2018)."
    },
    {
      "index": 103,
      "title": "Optimally solving dec-pomdps as continuous-state mdps",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Artificial Intelligence Research, 55:443–497",
      "authors": "Dibangoye, J. S., Amato, C., Buffet, O., and Charpillet, F. (2016)."
    },
    {
      "index": 104,
      "title": "Online learning in markov decision processes with changing cost sequences",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Dick, T., Gyorgy, A., and Szepesvari, C. (2014)."
    },
    {
      "index": 105,
      "title": "Mckean-vlasov optimal control: the dynamic programming principle",
      "abstract": "",
      "year": "1907",
      "venue": "arXiv preprint arXiv:",
      "authors": "Djete, M. F., Possamaï, D., and Tan, X. (2019)."
    },
    {
      "index": 106,
      "title": "Approximate fictitious play for mean field games",
      "abstract": "",
      "year": "1907",
      "venue": "arXiv preprint arXiv:",
      "authors": "Elie, R., Pérolat, J., Laurière, M., Geist, M., and Pietquin, O.\n(2019)."
    },
    {
      "index": 107,
      "title": "On the Convergence of Model Free Learning in Mean Field Games",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Elie, R., Pérolat, J., Laurière, M., Geist, M., and Pietquin, O.\n(2020).",
      "orig_title": "On the convergence of model free learning in mean field games",
      "paper_id": "1907.02633v3"
    },
    {
      "index": 108,
      "title": "Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria",
      "abstract": "",
      "year": "1998",
      "venue": "American economic review",
      "authors": "Erev, I. and Roth, A. E. (1998)."
    },
    {
      "index": 109,
      "title": "Experts in a markov decision process",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Even-Dar, E., Kakade, S. M., and Mansour, Y. (2005)."
    },
    {
      "index": 110,
      "title": "Online markov decision processes",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Even-Dar, E., Kakade, S. M., and Mansour, Y. (2009)."
    },
    {
      "index": 111,
      "title": "Learning rates for q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of machine learning Research, 5(Dec):1–25",
      "authors": "Even-Dar, E. and Mansour, Y. (2003)."
    },
    {
      "index": 112,
      "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Fazel, M., Ge, R., Kakade, S., and Mesbahi, M. (2018).",
      "orig_title": "Global convergence of policy gradient methods for the linear quadratic regulator",
      "paper_id": "1801.05039v3"
    },
    {
      "index": 113,
      "title": "Total expected discounted reward mdps: existence of optimal policies",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Feinberg, E. A. (2010)."
    },
    {
      "index": 114,
      "title": "Convergence of Learning Dynamics in Stackelberg Games",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv",
      "authors": "Fiez, T., Chasnov, B., and Ratliff, L. J. (2019).",
      "orig_title": "Convergence of learning dynamics in stackelberg games",
      "paper_id": "1906.01217v3"
    },
    {
      "index": 115,
      "title": "Competitive Markov decision processes",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "Filar, J. and Vrieze, K. (2012)."
    },
    {
      "index": 116,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Finn, C., Abbeel, P., and Levine, S. (2017).",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 117,
      "title": "On the connection between symmetric n𝑛n-player games and mean field games",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Fischer, M. et al. (2017)."
    },
    {
      "index": 118,
      "title": "Learning with Opponent-Learning Awareness",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 17th International Conference on\nAutonomous Agents and MultiAgent Systems",
      "authors": "Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and\nMordatch, I. (2018a).",
      "orig_title": "Learning with opponent-learning awareness",
      "paper_id": "1709.04326v4"
    },
    {
      "index": 119,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n(2017a).",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 120,
      "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 34th International Conference on Machine\nLearning-Volume 70",
      "authors": "Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P.,\nand Whiteson, S. (2017b).",
      "orig_title": "Stabilising experience replay for deep multi-agent reinforcement learning",
      "paper_id": "1702.08887v3"
    },
    {
      "index": 121,
      "title": "Counterfactual Multi-Agent Policy Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "McIlraith, S. A. and Weinberger, K. Q., editors, Proceedings\nof the Thirty-Second AAAI Conference on Artificial Intelligence, New\nOrleans, Louisiana, USA, February 2-7",
      "authors": "Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n(2018b).",
      "orig_title": "Counterfactual multi-agent policy gradients",
      "paper_id": "1705.08926v3"
    },
    {
      "index": 122,
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "abstract": "",
      "year": "1997",
      "venue": "Journal of computer and system sciences, 55(1):119–139",
      "authors": "Freund, Y. and Schapire, R. E. (1997)."
    },
    {
      "index": 123,
      "title": "Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games",
      "abstract": "",
      "year": "1910",
      "venue": "arXiv preprint arXiv:",
      "authors": "Fu, Z., Yang, Z., Chen, Y., and Wang, Z. (2019).",
      "orig_title": "Actor-critic provably finds nash equilibria of linear-quadratic mean-field games",
      "paper_id": "1910.07498v1"
    },
    {
      "index": 124,
      "title": "The theory of learning in games, volume 2",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "Fudenberg, D., Drew, F., Levine, D. K., and Levine, D. K. (1998)."
    },
    {
      "index": 125,
      "title": "Learning mixed equilibria",
      "abstract": "",
      "year": "1993",
      "venue": "",
      "authors": "Fudenberg, D. and Kreps, D. M. (1993)."
    },
    {
      "index": 126,
      "title": "Consistency and cautious fictitious play",
      "abstract": "",
      "year": "1995",
      "venue": "Journal of Economic Dynamics and Control",
      "authors": "Fudenberg, D. and Levine, D. (1995)."
    },
    {
      "index": 127,
      "title": "Partially observable mean field reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv e-prints",
      "authors": "Ganapathi Subramanian, S., Taylor, M. E., Crowley, M., and Poupart, P. (2020)."
    },
    {
      "index": 128,
      "title": "A comprehensive survey on safe reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Machine Learning Research, 16(1):",
      "authors": "Garcıa, J. and Fernández, F. (2015)."
    },
    {
      "index": 129,
      "title": "On the mckean-vlasov limit for interacting diffusions",
      "abstract": "",
      "year": "1988",
      "venue": "",
      "authors": "Gärtner, J. (1988)."
    },
    {
      "index": 130,
      "title": "Distributed Artificial Intelligence: Volume II, volume 2",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Gasser, R. and Huhns, M. N. (2014)."
    },
    {
      "index": 131,
      "title": "Generalized sampling and variance in counterfactual regret minimization",
      "abstract": "",
      "year": "2012",
      "venue": "AAAI",
      "authors": "Gibson, R. G., Lanctot, M., Burch, N., Szafron, D., and Bowling, M. (2012)."
    },
    {
      "index": 132,
      "title": "Bounded rationality: The adaptive toolbox",
      "abstract": "",
      "year": "2002",
      "venue": "MIT press",
      "authors": "Gigerenzer, G. and Selten, R. (2002)."
    },
    {
      "index": 133,
      "title": "Gradient-based algorithms for finding nash equilibria in extensive form games",
      "abstract": "",
      "year": "2007",
      "venue": "International Workshop on Web and Internet Economics",
      "authors": "Gilpin, A., Hoda, S., Pena, J., and Sandholm, T. (2007)."
    },
    {
      "index": 134,
      "title": "Finding equilibria in large sequential games of imperfect information",
      "abstract": "",
      "year": "2006",
      "venue": "Proceedings of the 7th ACM conference on Electronic\ncommerce",
      "authors": "Gilpin, A. and Sandholm, T. (2006)."
    },
    {
      "index": 135,
      "title": "Discrete–time stochastic control and dynamic potential games: the Euler–Equation approach",
      "abstract": "",
      "year": "2013",
      "venue": "Springer Science & Business Media",
      "authors": "González-Sánchez, D. and Hernández-Lerma, O. (2013)."
    },
    {
      "index": 136,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "",
      "venue": "NIPS",
      "authors": "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., and Bengio, Y. (2014a)."
    },
    {
      "index": 137,
      "title": "Explaining and Harnessing Adversarial Examples",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014b).",
      "orig_title": "Explaining and harnessing adversarial examples",
      "paper_id": "1412.6572v3"
    },
    {
      "index": 138,
      "title": "No-regret algorithms for online convex programs",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Gordon, G. J. (2007)."
    },
    {
      "index": 139,
      "title": "Balancing Two-Player Stochastic Games with Soft Q-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "Grau-Moya, J., Leibfried, F., and Bou-Ammar, H. (2018).",
      "orig_title": "Balancing two-player stochastic games with soft q-learning",
      "paper_id": "1802.03216v2"
    },
    {
      "index": 140,
      "title": "Correlated q-learning",
      "abstract": "",
      "year": "2003",
      "venue": "ICML, volume 20",
      "authors": "Greenwald, A., Hall, K., and Serrano, R. (2003)."
    },
    {
      "index": 141,
      "title": "Dynamic programming principles for learning mfcs",
      "abstract": "",
      "year": "1911",
      "venue": "arXiv preprint arXiv:",
      "authors": "Gu, H., Guo, X., Wei, X., and Xu, R. (2019)."
    },
    {
      "index": 142,
      "title": "Q-learning for mean-field controls",
      "abstract": "",
      "year": "2002",
      "venue": "arXiv preprint arXiv:",
      "authors": "Gu, H., Guo, X., Wei, X., and Xu, R. (2020)."
    },
    {
      "index": 143,
      "title": "Mean field games and applications",
      "abstract": "",
      "year": "2010",
      "venue": "Paris-Princeton lectures on mathematical finance",
      "authors": "Guéant, O., Lasry, J.-M., and Lions, P.-L. (2011)."
    },
    {
      "index": 144,
      "title": "Multiagent planning with factored mdps",
      "abstract": "",
      "year": "",
      "venue": "Advances in neural information processing systems",
      "authors": "Guestrin, C., Koller, D., and Parr, R. (2002a)."
    },
    {
      "index": 145,
      "title": "Coordinated reinforcement learning",
      "abstract": "",
      "year": "",
      "venue": "ICML, volume 2",
      "authors": "Guestrin, C., Lagoudakis, M., and Parr, R. (2002b)."
    },
    {
      "index": 146,
      "title": "Learning mean-field games",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Guo, X., Hu, A., Xu, R., and Zhang, J. (2019)."
    },
    {
      "index": 147,
      "title": "A general framework for learning mean-field games",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Guo, X., Hu, A., Xu, R., and Zhang, J. (2020)."
    },
    {
      "index": 148,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 149,
      "title": "Finite mean field games: fictitious play and convergence to a first order continuous mean field game",
      "abstract": "",
      "year": "2019",
      "venue": "Journal de Mathématiques Pures et Appliquées,\n132:369–397",
      "authors": "Hadikhanloo, S. and Silva, F. J. (2019)."
    },
    {
      "index": 150,
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019a).",
      "orig_title": "Dream to control: Learning behaviors by latent imagination",
      "paper_id": "1912.01603v3"
    },
    {
      "index": 151,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and\nDavidson, J. (2019b).",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 152,
      "title": "Approximation to bayes risk in repeated play",
      "abstract": "",
      "year": "1957",
      "venue": "",
      "authors": "Hannan, J. (1957)."
    },
    {
      "index": 153,
      "title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Hansen, N., Müller, S. D., and Koumoutsakos, P. (2003)."
    },
    {
      "index": 154,
      "title": "Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of the ACM (JACM), 60(1):1–16",
      "authors": "Hansen, T. D., Miltersen, P. B., and Zwick, U. (2013)."
    },
    {
      "index": 155,
      "title": "Simple adaptive strategies: from regret-matching to uncoupled dynamics, volume 4",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Hart, S. (2013)."
    },
    {
      "index": 156,
      "title": "A reinforcement procedure leading to correlated equilibrium",
      "abstract": "",
      "year": "2001",
      "venue": "Economics Essays",
      "authors": "Hart, S. and Mas-Colell, A. (2001)."
    },
    {
      "index": 157,
      "title": "Fictitious self-play in extensive-form games",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Heinrich, J., Lanctot, M., and Silver, D. (2015)."
    },
    {
      "index": 158,
      "title": "Deep reinforcement learning from self-play in imperfect-information games",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Heinrich, J. and Silver, D. (2016)."
    },
    {
      "index": 159,
      "title": "Neural replicator dynamics",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hennes, D., Morrill, D., Omidshafiei, S., Munos, R., Perolat, J., Lanctot, M.,\nGruslys, A., Lespiau, J.-B., Parmas, P., Duenez-Guzman, E., et al. (2019)."
    },
    {
      "index": 160,
      "title": "Homotopy methods to compute equilibria in game theory",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Herings, P. J.-J. and Peeters, R. (2010)."
    },
    {
      "index": 161,
      "title": "Stationary equilibria in stochastic games: Structure, selection, and computation",
      "abstract": "",
      "year": "2004",
      "venue": "Journal of Economic Theory, 118(1):32–60",
      "authors": "Herings, P. J.-J., Peeters, R. J., et al. (2004)."
    },
    {
      "index": 162,
      "title": "A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hernandez-Leal, P., Kaisers, M., Baarslag, T., and de Cote, E. M. (2017).",
      "orig_title": "A survey of learning in multiagent environments: Dealing with non-stationarity",
      "paper_id": "1707.09183v2"
    },
    {
      "index": 163,
      "title": "A survey and critique of multiagent deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Hernandez-Leal, P., Kartal, B., and Taylor, M. E. (2019)."
    },
    {
      "index": 164,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.\n(2017).",
      "orig_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 165,
      "title": "Differential topology, volume 33",
      "abstract": "",
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "authors": "Hirsch, M. W. (2012)."
    },
    {
      "index": 166,
      "title": "On the global convergence of stochastic fictitious play",
      "abstract": "",
      "year": "2002",
      "venue": "Econometrica, 70(6):",
      "authors": "Hofbauer, J. and Sandholm, W. H. (2002)."
    },
    {
      "index": 167,
      "title": "Nash q-learning for general-sum stochastic games",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of Machine learning research, 4(Nov):",
      "authors": "Hu, J. and Wellman, M. P. (2003)."
    },
    {
      "index": 168,
      "title": "Multiagent reinforcement learning: theoretical framework and an algorithm",
      "abstract": "",
      "year": "1998",
      "venue": "ICML, volume 98",
      "authors": "Hu, J., Wellman, M. P., et al. (1998)."
    },
    {
      "index": 169,
      "title": "Mean-field langevin dynamics and energy landscape of neural networks",
      "abstract": "",
      "year": "1905",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hu, K., Ren, Z., Siska, D., and Szpruch, L. (2019)."
    },
    {
      "index": 170,
      "title": "Large population stochastic dynamic games: closed-loop mckean-vlasov systems and the nash certainty equivalence principle",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Huang, M., Malhamé, R. P., Caines, P. E., et al. (2006)."
    },
    {
      "index": 171,
      "title": "Distributed Artificial Intelligence: Volume I, volume 1",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Huhns, M. N. (2012)."
    },
    {
      "index": 172,
      "title": "Mean field equilibria of dynamic auctions with learning",
      "abstract": "",
      "year": "2014",
      "venue": "Management Science, 60(12):",
      "authors": "Iyer, K., Johari, R., and Sundararajan, M. (2014)."
    },
    {
      "index": 173,
      "title": "Human-level performance in 3d multiplayer games with population-based reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Science, 364(",
      "authors": "Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda,\nA. G., Beattie, C., Rabinowitz, N. C., Morcos, A. S., Ruderman, A., et al.\n(2019)."
    },
    {
      "index": 174,
      "title": "An overview of cooperative and competitive multiagent learning",
      "abstract": "",
      "year": "2005",
      "venue": "International Workshop on Learning and Adaption in\nMulti-Agent Systems",
      "authors": "Jan’t Hoen, P., Tuyls, K., Panait, L., Luke, S., and La Poutre, J. A. (2005)."
    },
    {
      "index": 175,
      "title": "Feature-Based Q-Learning for Two-Player Stochastic Games",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv",
      "authors": "Jia, Z., Yang, L. F., and Wang, M. (2019).",
      "orig_title": "Feature-based q-learning for two-player stochastic games",
      "paper_id": "1906.00423v1"
    },
    {
      "index": 176,
      "title": "What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?",
      "abstract": "",
      "year": "1902",
      "venue": "arXiv",
      "authors": "Jin, C., Netrapalli, P., and Jordan, M. I. (2019).",
      "orig_title": "What is local optimality in nonconvex-nonconcave minimax optimization?",
      "paper_id": "1902.00618v3"
    },
    {
      "index": 177,
      "title": "Regret Minimization for Partially Observable Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Jin, P., Keutzer, K., and Levine, S. (2018).",
      "orig_title": "Regret minimization for partially observable deep reinforcement learning",
      "paper_id": "1710.11424v2"
    },
    {
      "index": 178,
      "title": "Finding optimal abstract strategies in extensive-form games",
      "abstract": "",
      "year": "",
      "venue": "AAAI. Citeseer",
      "authors": "Johanson, M., Bard, N., Burch, N., and Bowling, M. (2012a)."
    },
    {
      "index": 179,
      "title": "Efficient nash equilibrium approximation through monte carlo counterfactual regret minimization",
      "abstract": "",
      "year": "",
      "venue": "AAMAS",
      "authors": "Johanson, M., Bard, N., Lanctot, M., Gibson, R. G., and Bowling, M. (2012b)."
    },
    {
      "index": 180,
      "title": "Anonymous sequential games",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Mathematical Economics, 17(1):77–87",
      "authors": "Jovanovic, B. and Rosenthal, R. W. (1988)."
    },
    {
      "index": 181,
      "title": "More is the same; phase transitions and mean field theories",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Statistical Physics, 137(5-6):777",
      "authors": "Kadanoff, L. P. (2009)."
    },
    {
      "index": 182,
      "title": "Reinforcement learning: A survey",
      "abstract": "",
      "year": "1996",
      "venue": "Journal of artificial intelligence research, 4:237–285",
      "authors": "Kaelbling, L. P., Littman, M. L., and Moore, A. W. (1996)."
    },
    {
      "index": 183,
      "title": "Learning dynamics in games with stochastic perturbations",
      "abstract": "",
      "year": "1995",
      "venue": "",
      "authors": "Kaniovski, Y. M. and Young, H. P. (1995)."
    },
    {
      "index": 184,
      "title": "Graphical games",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Kearns, M. (2007)."
    },
    {
      "index": 185,
      "title": "Graphical models for game theory",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kearns, M., Littman, M. L., and Singh, S. (2013)."
    },
    {
      "index": 186,
      "title": "Swarm intelligence",
      "abstract": "",
      "year": "2006",
      "venue": "Handbook of nature-inspired and innovative computing",
      "authors": "Kennedy, J. (2006)."
    },
    {
      "index": 187,
      "title": "The General Theory of Employment, Interest and Money",
      "abstract": "",
      "year": "1973",
      "venue": "Macmillan",
      "authors": "Keynes, J. M. (1936)."
    },
    {
      "index": 188,
      "title": "Brain function and adaptive systems: a heterostatic theory",
      "abstract": "",
      "year": "1972",
      "venue": "",
      "authors": "Klopf, A. H. (1972)."
    },
    {
      "index": 189,
      "title": "Reinforcement learning in robotics: A survey",
      "abstract": "",
      "year": "2013",
      "venue": "The International Journal of Robotics Research,\n32(11):",
      "authors": "Kober, J., Bagnell, J. A., and Peters, J. (2013)."
    },
    {
      "index": 190,
      "title": "Bandit based monte-carlo planning",
      "abstract": "",
      "year": "2006",
      "venue": "European conference on machine learning, pages 282–293.\nSpringer",
      "authors": "Kocsis, L. and Szepesvári, C. (2006)."
    },
    {
      "index": 191,
      "title": "Sparse cooperative q-learning",
      "abstract": "",
      "year": "2004",
      "venue": "Proceedings of the twenty-first international conference on\nMachine learning",
      "authors": "Kok, J. R. and Vlassis, N. (2004)."
    },
    {
      "index": 192,
      "title": "The complexity of two-person zero-sum games in extensive form",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": "Koller, D. and Megiddo, N. (1992)."
    },
    {
      "index": 193,
      "title": "Finding mixed strategies with small supports in extensive form games",
      "abstract": "",
      "year": "1996",
      "venue": "International Journal of Game Theory, 25(1):73–92",
      "authors": "Koller, D. and Megiddo, N. (1996)."
    },
    {
      "index": 194,
      "title": "Actor-critic algorithms",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "Konda, V. R. and Tsitsiklis, J. N. (2000)."
    },
    {
      "index": 195,
      "title": "An accelerated inexact proximal point method for solving nonconvex-concave min-max problems",
      "abstract": "",
      "year": "1905",
      "venue": "arXiv",
      "authors": "Kong, W. and Monteiro, R. D. (2019)."
    },
    {
      "index": 196,
      "title": "Rethinking Formal Models of Partially Observable Multiagent Decision Making",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kovařík, V., Schmid, M., Burch, N., Bowling, M., and Lisỳ, V.\n(2019).",
      "orig_title": "Rethinking formal models of partially observable multiagent decision making",
      "paper_id": "1906.11110v4"
    },
    {
      "index": 197,
      "title": "Reputation and imperfect information",
      "abstract": "",
      "year": "1982",
      "venue": "Journal of economic theory, 27(2):253–279",
      "authors": "Kreps, D. M. and Wilson, R. (1982)."
    },
    {
      "index": 198,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012)."
    },
    {
      "index": 199,
      "title": "Extensive games",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the National Academy of Sciences of the United\nStates of America, 36(10):570",
      "authors": "Kuhn, H. W. (1950a)."
    },
    {
      "index": 200,
      "title": "A simplified two-person poker",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Kuhn, H. W. (1950b)."
    },
    {
      "index": 201,
      "title": "Determinantal point processes for machine learning",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Kulesza, A., Taskar, B., et al. (2012)."
    },
    {
      "index": 202,
      "title": "Potential Game Theory",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Lã, Q. D., Chew, Y. H., and Soong, B.-H. (2016)."
    },
    {
      "index": 203,
      "title": "Mean field games via controlled martingale problems: existence of markovian equilibria",
      "abstract": "",
      "year": "2015",
      "venue": "Stochastic Processes and their Applications, 125(7):",
      "authors": "Lacker, D. (2015)."
    },
    {
      "index": 204,
      "title": "Limit theory for controlled mckean–vlasov dynamics",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM Journal on Control and Optimization, 55(3):",
      "authors": "Lacker, D. (2017)."
    },
    {
      "index": 205,
      "title": "On the convergence of closed-loop nash equilibria to the mean field game limit",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lacker, D. (2018)."
    },
    {
      "index": 206,
      "title": "Mean field and n-agent games for optimal investment under relative performance criteria",
      "abstract": "",
      "year": "2019",
      "venue": "Mathematical Finance, 29(4):",
      "authors": "Lacker, D. and Zariphopoulou, T. (2019)."
    },
    {
      "index": 207,
      "title": "Learning in zero-sum team markov games using factored value functions",
      "abstract": "",
      "year": "2003",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Lagoudakis, M. G. and Parr, R. (2003)."
    },
    {
      "index": 208,
      "title": "Monte carlo sampling for regret minimization in extensive games",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in neural information processing systems",
      "authors": "Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M. (2009)."
    },
    {
      "index": 209,
      "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat,\nJ., Silver, D., and Graepel, T. (2017).",
      "orig_title": "A unified game-theoretic approach to multiagent reinforcement learning",
      "paper_id": "1711.00832v2"
    },
    {
      "index": 210,
      "title": "Batch reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "Reinforcement learning",
      "authors": "Lange, S., Gabel, T., and Riedmiller, M. (2012)."
    },
    {
      "index": 211,
      "title": "Mean field games",
      "abstract": "",
      "year": "2007",
      "venue": "Japanese journal of mathematics, 2(1):229–260",
      "authors": "Lasry, J.-M. and Lions, P.-L. (2007)."
    },
    {
      "index": 212,
      "title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems",
      "abstract": "",
      "year": "2000",
      "venue": "ICML. Citeseer",
      "authors": "Lauer, M. and Riedmiller, M. (2000)."
    },
    {
      "index": 213,
      "title": "Dynamic programming for mean-field type control",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Laurière, M. and Pironneau, O. (2014)."
    },
    {
      "index": 214,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature, 521(",
      "authors": "LeCun, Y., Bengio, Y., and Hinton, G. (2015).",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 215,
      "title": "A mean field game of portfolio trading and its consequences on perceived correlations",
      "abstract": "",
      "year": "1902",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lehalle, C.-A. and Mouzouni, C. (2019)."
    },
    {
      "index": 216,
      "title": "Equilibrium points of bimatrix games",
      "abstract": "",
      "year": "1964",
      "venue": "Journal of the Society for Industrial and Applied Mathematics,\n12(2):413–423",
      "authors": "Lemke, C. E. and Howson, Jr, J. T. (1964)."
    },
    {
      "index": 217,
      "title": "Convergent multiple-timescales reinforcement learning algorithms in normal form games",
      "abstract": "",
      "year": "2003",
      "venue": "The Annals of Applied Probability, 13(4):",
      "authors": "Leslie, D. S., Collins, E., et al. (2003)."
    },
    {
      "index": 218,
      "title": "Individual q-learning in normal form games",
      "abstract": "",
      "year": "2005",
      "venue": "SIAM Journal on Control and Optimization, 44(2):495–514",
      "authors": "Leslie, D. S. and Collins, E. J. (2005)."
    },
    {
      "index": 219,
      "title": "Generalised weakened fictitious play",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Leslie, D. S. and Collins, E. J. (2006)."
    },
    {
      "index": 220,
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Levine, S. (2018).",
      "orig_title": "Reinforcement learning and control as probabilistic inference: Tutorial and review",
      "paper_id": "1805.00909v3"
    },
    {
      "index": 221,
      "title": "Local-effect games",
      "abstract": "",
      "year": "2005",
      "venue": "Dagstuhl Seminar Proceedings. Schloss\nDagstuhl-Leibniz-Zentrum für Informatik",
      "authors": "Leyton-Brown, K. and Tennenholtz, M. (2005)."
    },
    {
      "index": 222,
      "title": "Efficient Ridesharing Order Dispatching with Mean Field Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "The World Wide Web Conference",
      "authors": "Li, M., Qin, Z., Jiao, Y., Yang, Y., Wang, J., Wang, C., Wu, G., and Ye, J.\n(2019a).",
      "orig_title": "Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning",
      "paper_id": "1901.11454v1"
    },
    {
      "index": 223,
      "title": "Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33",
      "authors": "Li, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019b)."
    },
    {
      "index": 224,
      "title": "Deep Reinforcement Learning: An Overview",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Li, Y. (2017).",
      "orig_title": "Deep reinforcement learning: An overview",
      "paper_id": "1806.08894v1"
    },
    {
      "index": 225,
      "title": "Sampled Fictitious Play is Hannan Consistent",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Li, Z. and Tewari, A. (2018).",
      "orig_title": "Sampled fictitious play is hannan consistent",
      "paper_id": "1610.01687v2"
    },
    {
      "index": 226,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\nSilver, D., and Wierstra, D. (2015)."
    },
    {
      "index": 227,
      "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lin, T., Jin, C., and Jordan, M. I. (2019).",
      "orig_title": "On gradient descent ascent for nonconvex-concave minimax problems",
      "paper_id": "1906.00331v10"
    },
    {
      "index": 228,
      "title": "Convergence of monte carlo tree search in simultaneous move games",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Lisy, V., Kovarik, V., Lanctot, M., and Bosansky, B. (2013)."
    },
    {
      "index": 229,
      "title": "The weighted majority algorithm",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": "Littlestone, N. and Warmuth, M. K. (1994)."
    },
    {
      "index": 230,
      "title": "Markov games as a framework for multi-agent reinforcement learning",
      "abstract": "",
      "year": "1994",
      "venue": "Proceedings of the eleventh international conference on\nmachine learning, volume 157",
      "authors": "Littman, M. L. (1994)."
    },
    {
      "index": 231,
      "title": "Friend-or-foe q-learning in general-sum games",
      "abstract": "",
      "year": "",
      "venue": "ICML, volume 1",
      "authors": "Littman, M. L. (2001a)."
    },
    {
      "index": 232,
      "title": "Value-function reinforcement learning in markov games",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Littman, M. L. (2001b)."
    },
    {
      "index": 233,
      "title": "Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy",
      "abstract": "",
      "year": "1906",
      "venue": "arXiv preprint arXiv:",
      "authors": "Liu, B., Cai, Q., Yang, Z., and Wang, Z. (2019).",
      "orig_title": "Neural proximal/trust region policy optimization attains globally optimal policy",
      "paper_id": "1906.10306v3"
    },
    {
      "index": 234,
      "title": "Computing approximate equilibria in sequential adversarial games by exploitability descent",
      "abstract": "",
      "year": "1903",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lockhart, E., Lanctot, M., Pérolat, J., Lespiau, J.-B., Morrill, D.,\nTimbers, F., and Tuyls, K. (2019)."
    },
    {
      "index": 235,
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. (2017).",
      "orig_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
      "paper_id": "1706.02275v4"
    },
    {
      "index": 236,
      "title": "Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Signal Processing",
      "authors": "Lu, S., Tsaknakis, I., Hong, M., and Chen, Y. (2020a)."
    },
    {
      "index": 237,
      "title": "A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lu, Y., Ma, C., Lu, Y., Lu, J., and Ying, L. (2020b).",
      "orig_title": "A mean-field analysis of deep resnet and beyond: Towards provable optimization via overparameterization from depth",
      "paper_id": "2003.05508v2"
    },
    {
      "index": 238,
      "title": "Natural Actor-Critic Converges Globally for Hierarchical Linear Quadratic Regulator",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Luo, Y., Yang, Z., Wang, Z., and Kolar, M. (2019).",
      "orig_title": "Natural actor-critic converges globally for hierarchical linear quadratic regulator",
      "paper_id": "1912.06875v2"
    },
    {
      "index": 239,
      "title": "Learning parametric closed-loop policies for markov potential games",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Macua, S. V., Zazo, J., and Zazo, S. (2018)."
    },
    {
      "index": 240,
      "title": "Average reward reinforcement learning: Foundations, algorithms, and empirical results",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Mahadevan, S. (1996)."
    },
    {
      "index": 241,
      "title": "The empirical bayes envelope and regret minimization in competitive markov decision processes",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Mannor, S. and Shimkin, N. (2003)."
    },
    {
      "index": 242,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2001",
      "venue": "Journal of Economic Theory, 100(2):191–219",
      "authors": "Maskin, E. and Tirole, J. (2001).",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 243,
      "title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Matignon, L., Laurent, G. J., and Le Fort-Piat, N. (2012)."
    },
    {
      "index": 244,
      "title": "Understanding and using linear programming",
      "abstract": "",
      "year": "2007",
      "venue": "Springer Science & Business Media",
      "authors": "Matousek, J. and Gärtner, B. (2007)."
    },
    {
      "index": 245,
      "title": "On evolution",
      "abstract": "",
      "year": "1972",
      "venue": "",
      "authors": "Maynard Smith, J. (1972)."
    },
    {
      "index": 246,
      "title": "On the convergence of gradient-based learning in continuous games",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Mazumdar, E. and Ratliff, L. J. (2018)."
    },
    {
      "index": 247,
      "title": "Policy gradient in linear quadratic dynamic games has no convergence guarantees",
      "abstract": "",
      "year": "",
      "venue": "Smooth Games Optimization and Machine Learning Workshop: Bridging\nGame …",
      "authors": "Mazumdar, E., Ratliff, L. J., Sastry, S., and Jordan, M. I. (2019a)."
    },
    {
      "index": 248,
      "title": "On finding local nash equilibria (and only local nash equilibria) in zero-sum games",
      "abstract": "",
      "year": "1901",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mazumdar, E. V., Jordan, M. I., and Sastry, S. S. (2019b)."
    },
    {
      "index": 249,
      "title": "Propagation of chaos for a class of non-linear parabolic equations",
      "abstract": "",
      "year": "1967",
      "venue": "Stochastic Differential Equations (Lecture Series in\nDifferential Equations, Session 7, Catholic Univ",
      "authors": "McKean, H. P. (1967)."
    },
    {
      "index": 250,
      "title": "Planning in the presence of cost functions controlled by an adversary",
      "abstract": "",
      "year": "2003",
      "venue": "Proceedings of the 20th International Conference on Machine\nLearning (ICML-03)",
      "authors": "McMahan, H. B., Gordon, G. J., and Blum, A. (2003)."
    },
    {
      "index": 251,
      "title": "Optimistic Mirror Descent in Saddle-Point Problems: Going the Extra (Gradient) Mile",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and\nPiliouras, G. (2018).",
      "orig_title": "Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile",
      "paper_id": "1807.02629v2"
    },
    {
      "index": 252,
      "title": "Learning in games with continuous action sets and unknown payoff functions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Mertikopoulos, P. and Zhou, Z. (2019)."
    },
    {
      "index": 253,
      "title": "Which training methods for gans do actually converge?",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mescheder, L., Geiger, A., and Nowozin, S. (2018)."
    },
    {
      "index": 254,
      "title": "The Numerics of GANs",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Mescheder, L., Nowozin, S., and Geiger, A. (2017).",
      "orig_title": "The numerics of gans",
      "paper_id": "1705.10461v3"
    },
    {
      "index": 255,
      "title": "Stochastic potential games",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mguni, D. (2020)."
    },
    {
      "index": 256,
      "title": "Algorithmic game theory lecture notes",
      "abstract": "",
      "year": "2020",
      "venue": "http://www.cs.jhu.edu/ mdinitz/classes/AGT/Spring",
      "authors": "Michael, D. (2020)."
    },
    {
      "index": 257,
      "title": "Steps toward artificial intelligence",
      "abstract": "",
      "year": "1961",
      "venue": "Proceedings of the IRE, 49(1):8–30",
      "authors": "Minsky, M. (1961)."
    },
    {
      "index": 258,
      "title": "Theory of neural-analog reinforcement systems and its application to the brain model problem",
      "abstract": "",
      "year": "1954",
      "venue": "",
      "authors": "Minsky, M. L. (1954)."
    },
    {
      "index": 259,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature, 518(",
      "authors": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,\nM. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al.\n(2015)."
    },
    {
      "index": 260,
      "title": "Potential games",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": "Monderer, D. and Shapley, L. S. (1996)."
    },
    {
      "index": 261,
      "title": "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker",
      "abstract": "",
      "year": "2017",
      "venue": "Science, 356(",
      "authors": "Moravčík, M., Schmid, M., Burch, N., Lisỳ, V., Morrill, D.,\nBard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. (2017)."
    },
    {
      "index": 262,
      "title": "Robust reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Morimoto, J. and Doya, K. (2005)."
    },
    {
      "index": 263,
      "title": "Mean-field markov decision processes with common noise and open-loop controls",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Motte, M. and Pham, H. (2019)."
    },
    {
      "index": 264,
      "title": "Application impact of multi-agent systems and technologies: A survey",
      "abstract": "",
      "year": "2014",
      "venue": "Agent-oriented software engineering",
      "authors": "Müller, J. P. and Fischer, K. (2014)."
    },
    {
      "index": 265,
      "title": "A Generalized Training Approach for Multiagent Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S.,\nHennes, D., Marris, L., Lanctot, M., Hughes, E., et al. (2019).",
      "orig_title": "A generalized training approach for multiagent learning",
      "paper_id": "1909.12823v2"
    },
    {
      "index": 266,
      "title": "Finite-time bounds for fitted value iteration",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Machine Learning Research, 9(May):815–857",
      "authors": "Munos, R. and Szepesvári, C. (2008)."
    },
    {
      "index": 267,
      "title": "Gradient descent GAN optimization is locally stable",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Nagarajan, V. and Kolter, J. Z. (2017).",
      "orig_title": "Gradient descent gan optimization is locally stable",
      "paper_id": "1706.04156v3"
    },
    {
      "index": 268,
      "title": "Non-cooperative games",
      "abstract": "",
      "year": "1951",
      "venue": "",
      "authors": "Nash, J. (1951)."
    },
    {
      "index": 269,
      "title": "Decentralized stochastic control with partial history sharing: A common information approach",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Transactions on Automatic Control, 58(7):",
      "authors": "Nayyar, A., Mahajan, A., and Teneketzis, D. (2013)."
    },
    {
      "index": 270,
      "title": "Achieving geometric convergence for distributed optimization over time-varying graphs",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM Journal on Optimization, 27(4):",
      "authors": "Nedic, A., Olshevsky, A., and Shi, W. (2017)."
    },
    {
      "index": 271,
      "title": "Distributed subgradient methods for multi-agent optimization",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Automatic Control, 54(1):48–61",
      "authors": "Nedic, A. and Ozdaglar, A. (2009)."
    },
    {
      "index": 272,
      "title": "Problem complexity and method efficiency in optimization",
      "abstract": "",
      "year": "1983",
      "venue": "",
      "authors": "Nemirovsky, A. S. and Yudin, D. B. (1983)."
    },
    {
      "index": 273,
      "title": "Online markov decision processes under bandit feedback",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Neu, G., Antos, A., György, A., and Szepesvári, C. (2010)."
    },
    {
      "index": 274,
      "title": "Online markov decision processes under bandit feedback",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Transactions on Automatic Control, 3(59):676–691",
      "authors": "Neu, G., Gyorgy, A., Szepesvari, C., and Antos, A. (2014)."
    },
    {
      "index": 275,
      "title": "A Unified View of Entropy-Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Neu, G., Jonsson, A., and Gómez, V. (2017).",
      "orig_title": "A unified view of entropy-regularized markov decision processes",
      "paper_id": "1705.07798v1"
    },
    {
      "index": 276,
      "title": "Zur theorie der gesellschaftsspiele",
      "abstract": "",
      "year": "1928",
      "venue": "",
      "authors": "Neumann, J. v. (1928)."
    },
    {
      "index": 277,
      "title": "Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE transactions on cybernetics",
      "authors": "Nguyen, T. T., Nguyen, N. D., and Nahavandi, S. (2020)."
    },
    {
      "index": 278,
      "title": "Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Razaviyayn, M. (2019).",
      "orig_title": "Solving a class of non-convex min-max games using iterative first order methods",
      "paper_id": "1902.08297v3"
    },
    {
      "index": 279,
      "title": "Game theory and multi-agent reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "Reinforcement Learning",
      "authors": "Nowé, A., Vrancx, P., and De Hauwere, Y.-M. (2012)."
    },
    {
      "index": 280,
      "title": "Convergence to the mean field game limit: a case study",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Nutz, M., San Martin, J., Tan, X., et al. (2020)."
    },
    {
      "index": 281,
      "title": "A concise introduction to decentralized POMDPs, volume 1",
      "abstract": "",
      "year": "2016",
      "venue": "Springer",
      "authors": "Oliehoek, F. A., Amato, C., et al. (2016)."
    },
    {
      "index": 282,
      "title": "α𝛼\\alpha-rank: Multi-agent evaluation by evolution",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Omidshafiei, S., Papadimitriou, C., Piliouras, G., Tuyls, K., Rowland, M.,\nLespiau, J.-B., Czarnecki, W. M., Lanctot, M., Perolat, J., and Munos, R.\n(2019)."
    },
    {
      "index": 283,
      "title": "Navigating the landscape of games",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Omidshafiei, S., Tuyls, K., Czarnecki, W. M., Santos, F. C., Rowland, M.,\nConnor, J., Hennes, D., Muller, P., Perolat, J., De Vylder, B., et al.\n(2020)."
    },
    {
      "index": 284,
      "title": "A Review of Cooperative Multi-Agent Deep Reinforcement Learning",
      "abstract": "",
      "year": "1908",
      "venue": "arXiv preprint arXiv:",
      "authors": "OroojlooyJadid, A. and Hajinezhad, D. (2019).",
      "orig_title": "A review of cooperative multi-agent deep reinforcement learning",
      "paper_id": "1908.03963v4"
    },
    {
      "index": 285,
      "title": "Variational Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Ortner, R., Gajane, P., and Auer, P. (2020).",
      "orig_title": "Variational regret bounds for reinforcement learning",
      "paper_id": "1905.05857v3"
    },
    {
      "index": 286,
      "title": "A course in game theory",
      "abstract": "",
      "year": "1994",
      "venue": "MIT press",
      "authors": "Osborne, M. J. and Rubinstein, A. (1994)."
    },
    {
      "index": 287,
      "title": "Openai five, 2018",
      "abstract": "",
      "year": "2018",
      "venue": "URL https://blog. openai. com/openai-five",
      "authors": "Pachocki, J., Brockman, G., Raiman, J., Zhang, S., Pondé, H., Tang, J.,\nWolski, F., Dennison, C., Jozefowicz, R., Debiak, P., et al. (2018)."
    },
    {
      "index": 288,
      "title": "Cooperative multi-agent learning: The state of the art",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Panait, L. and Luke, S. (2005)."
    },
    {
      "index": 289,
      "title": "The complexity of markov decision processes",
      "abstract": "",
      "year": "1987",
      "venue": "",
      "authors": "Papadimitriou, C. H. and Tsitsiklis, J. N. (1987)."
    },
    {
      "index": 290,
      "title": "Comparative evaluation of multi-agent deep reinforcement learning algorithms",
      "abstract": "",
      "year": "2006",
      "venue": "arXiv preprint arXiv:",
      "authors": "Papoudakis, G., Christianos, F., Schäfer, L., and Albrecht, S. V. (2020)."
    },
    {
      "index": 291,
      "title": "Mixed-strategy learning with continuous action sets",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Transactions on Automatic Control, 62(1):379–384",
      "authors": "Perkins, S., Mertikopoulos, P., and Leslie, D. S. (2015)."
    },
    {
      "index": 292,
      "title": "Actor-critic fictitious play in simultaneous move multistage games",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Artificial Intelligence and\nStatistics",
      "authors": "Perolat, J., Piot, B., and Pietquin, O. (2018)."
    },
    {
      "index": 293,
      "title": "Natural actor-critic",
      "abstract": "",
      "year": "2008",
      "venue": "Neurocomputing, 71(7-9):",
      "authors": "Peters, J. and Schaal, S. (2008)."
    },
    {
      "index": 294,
      "title": "Discrete time mckean–vlasov control problem: a dynamic programming approach",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Pham, H. and Wei, X. (2016)."
    },
    {
      "index": 295,
      "title": "Dynamic programming for optimal control of stochastic mckean–vlasov dynamics",
      "abstract": "",
      "year": "2017",
      "venue": "SIAM Journal on Control and Optimization, 55(2):",
      "authors": "Pham, H. and Wei, X. (2017)."
    },
    {
      "index": 296,
      "title": "Bellman equation and viscosity solutions for mean-field stochastic control problem",
      "abstract": "",
      "year": "2018",
      "venue": "ESAIM: Control, Optimisation and Calculus of Variations,\n24(1):437–461",
      "authors": "Pham, H. and Wei, X. (2018)."
    },
    {
      "index": 297,
      "title": "Learning against opponents with bounded memory",
      "abstract": "",
      "year": "",
      "venue": "IJCAI, volume 5",
      "authors": "Powers, R. and Shoham, Y. (2005a)."
    },
    {
      "index": 298,
      "title": "New criteria and a new algorithm for learning in multi-agent systems",
      "abstract": "",
      "year": "",
      "venue": "Advances in neural information processing systems",
      "authors": "Powers, R. and Shoham, Y. (2005b)."
    },
    {
      "index": 299,
      "title": "Two-timescale algorithms for learning nash equilibria in general-sum stochastic games",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the 2015 International Conference on\nAutonomous Agents and Multiagent Systems",
      "authors": "Prasad, H., LA, P., and Bhatnagar, S. (2015)."
    },
    {
      "index": 300,
      "title": "Non-convex min-max optimization: Provable algorithms and applications in machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Rafique, H., Liu, M., Lin, Q., and Yang, T. (2018)."
    },
    {
      "index": 301,
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\nWhiteson, S. (2018).",
      "orig_title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
      "paper_id": "1803.11485v2"
    },
    {
      "index": 302,
      "title": "Characterization and computation of local nash equilibria in continuous games",
      "abstract": "",
      "year": "2013",
      "venue": "2013 51st Annual Allerton Conference on Communication,\nControl, and Computing (Allerton)",
      "authors": "Ratliff, L. J., Burden, S. A., and Sastry, S. S. (2013)."
    },
    {
      "index": 303,
      "title": "Genericity and structural stability of non-degenerate differential nash equilibria",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Ratliff, L. J., Burden, S. A., and Sastry, S. S. (2014)."
    },
    {
      "index": 304,
      "title": "Factorization machines",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Rendle, S. (2010)."
    },
    {
      "index": 305,
      "title": "Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method",
      "abstract": "",
      "year": "2005",
      "venue": "European Conference on Machine Learning, pages 317–328.\nSpringer",
      "authors": "Riedmiller, M. (2005)."
    },
    {
      "index": 306,
      "title": "A survey of multi-objective sequential decision-making",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research, 48:67–113",
      "authors": "Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. (2013)."
    },
    {
      "index": 307,
      "title": "Online convex optimization in adversarial markov decision processes",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Rosenberg, A. and Mansour, Y. (2019)."
    },
    {
      "index": 308,
      "title": "ProMP: Proximal Meta-Policy Search",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Rothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P. (2018).",
      "orig_title": "Promp: Proximal meta-policy search",
      "paper_id": "1810.06784v4"
    },
    {
      "index": 309,
      "title": "Markov–nash equilibria in mean-field games with discounted cost",
      "abstract": "",
      "year": "2018",
      "venue": "SIAM Journal on Control and Optimization, 56(6):",
      "authors": "Saldi, N., Basar, T., and Raginsky, M. (2018)."
    },
    {
      "index": 310,
      "title": "Approximate nash equilibria in partially observed stochastic games with mean-field interactions",
      "abstract": "",
      "year": "2019",
      "venue": "Mathematics of Operations Research, 44(3):",
      "authors": "Saldi, N., Başar, T., and Raginsky, M. (2019)."
    },
    {
      "index": 311,
      "title": "Comparing uct versus cfr in simultaneous games",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Schaeffer, M. S. N. S. J., Shafiei, N., et al. (2009)."
    },
    {
      "index": 312,
      "title": "Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33",
      "authors": "Schmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec, R., and Bowling, M.\n(2019)."
    },
    {
      "index": 313,
      "title": "Deep learning in neural networks: An overview",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Schmidhuber, J. (2015)."
    },
    {
      "index": 314,
      "title": "Experiments on decisions under risk: The expected utility hypothesis",
      "abstract": "",
      "year": "2013",
      "venue": "Springer Science & Business Media",
      "authors": "Schoemaker, P. J. (2013)."
    },
    {
      "index": 315,
      "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
      "abstract": "",
      "year": "2020",
      "venue": "Nature, 588(",
      "authors": "Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,\nSchmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al.\n(2020).",
      "orig_title": "Mastering atari, go, chess and shogi by planning with a learned model",
      "paper_id": "1911.08265v2"
    },
    {
      "index": 316,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015).",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 317,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017)."
    },
    {
      "index": 318,
      "title": "Spieltheoretische behandlung eines oligopolmodells mit nachfrageträgheit: Teil i: Bestimmung des dynamischen preisgleichgewichts",
      "abstract": "",
      "year": "1965",
      "venue": "Zeitschrift für die gesamte Staatswissenschaft/Journal of\nInstitutional and Theoretical Economics, (H. 2):301–324",
      "authors": "Selten, R. (1965)."
    },
    {
      "index": 319,
      "title": "Multi-agent system applications in healthcare: current technology and future roadmap",
      "abstract": "",
      "year": "2015",
      "venue": "ANT/SEIT",
      "authors": "Shakshuki, E. M. and Reid, M. (2015)."
    },
    {
      "index": 320,
      "title": "Understanding machine learning: From theory to algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Shalev-Shwartz, S. and Ben-David, S. (2014)."
    },
    {
      "index": 321,
      "title": "Online learning and online convex optimization",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Shalev-Shwartz, S. et al. (2011)."
    },
    {
      "index": 322,
      "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shalev-Shwartz, S., Shammah, S., and Shashua, A. (2016).",
      "orig_title": "Safe, multi-agent, reinforcement learning for autonomous driving",
      "paper_id": "1610.03295v1"
    },
    {
      "index": 323,
      "title": "Stochastic games",
      "abstract": "",
      "year": "1953",
      "venue": "Proceedings of the national academy of sciences,\n39(10):",
      "authors": "Shapley, L. S. (1953)."
    },
    {
      "index": 324,
      "title": "A note on the lemke-howson algorithm",
      "abstract": "",
      "year": "1974",
      "venue": "Pivoting and Extension",
      "authors": "Shapley, L. S. (1974)."
    },
    {
      "index": 325,
      "title": "Soft policy gradient method for maximum entropy deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on\nArtificial Intelligence",
      "authors": "Shi, W., Song, S., and Wu, C. (2019)."
    },
    {
      "index": 326,
      "title": "Multiagent systems: Algorithmic, game-theoretic, and logical foundations",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Shoham, Y. and Leyton-Brown, K. (2008)."
    },
    {
      "index": 327,
      "title": "If multi-agent learning is the answer, what is the question?",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Shoham, Y., Powers, R., and Grenager, T. (2007)."
    },
    {
      "index": 328,
      "title": "Global stability of dynamical systems",
      "abstract": "",
      "year": "2013",
      "venue": "Springer Science & Business Media",
      "authors": "Shub, M. (2013)."
    },
    {
      "index": 329,
      "title": "Near-optimal time and sample complexities for solving markov decision processes with a generative model",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018)."
    },
    {
      "index": 330,
      "title": "Solving Discounted Stochastic Two-Player Games with Near-Optimal Time and Sample Complexity",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and\nStatistics",
      "authors": "Sidford, A., Wang, M., Yang, L., and Ye, Y. (2020).",
      "orig_title": "Solving discounted stochastic two-player games with near-optimal time and sample complexity",
      "paper_id": "1908.11071v1"
    },
    {
      "index": 331,
      "title": "Mastering the game of go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "nature, 529(",
      "authors": "Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche,\nG., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,\net al. (2016)."
    },
    {
      "index": 332,
      "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play",
      "abstract": "",
      "year": "2018",
      "venue": "Science, 362(",
      "authors": "Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,\nLanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. (2018)."
    },
    {
      "index": 333,
      "title": "Deterministic policy gradient algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "ICML",
      "authors": "Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n(2014)."
    },
    {
      "index": 334,
      "title": "Mastering the game of go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature, 550(",
      "authors": "Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,\nA., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017)."
    },
    {
      "index": 335,
      "title": "Theories of bounded rationality",
      "abstract": "",
      "year": "1972",
      "venue": "",
      "authors": "Simon, H. A. (1972)."
    },
    {
      "index": 336,
      "title": "Nash convergence of gradient dynamics in general-sum games",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": "Singh, S. P., Kearns, M. J., and Mansour, Y. (2000)."
    },
    {
      "index": 337,
      "title": "Mean field analysis of neural networks: A law of large numbers",
      "abstract": "",
      "year": "2020",
      "venue": "SIAM Journal on Applied Mathematics, 80(2):725–752",
      "authors": "Sirignano, J. and Spiliopoulos, K. (2020)."
    },
    {
      "index": 338,
      "title": "The logic of animal conflict",
      "abstract": "",
      "year": "1973",
      "venue": "Nature, 246(",
      "authors": "Smith, J. M. and Price, G. R. (1973)."
    },
    {
      "index": 339,
      "title": "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Son, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y. (2019).",
      "orig_title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
      "paper_id": "1905.05408v1"
    },
    {
      "index": 340,
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the National Academy of Sciences,\n115:E",
      "authors": "Song, M., Montanari, A., and Nguyen, P. (2018).",
      "orig_title": "A mean field view of the landscape of two-layers neural networks",
      "paper_id": "1804.06561v2"
    },
    {
      "index": 341,
      "title": "On the universality of online mirror descent",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in neural information processing systems",
      "authors": "Srebro, N., Sridharan, K., and Tewari, A. (2011)."
    },
    {
      "index": 342,
      "title": "Actor-Critic Policy Optimization in Partially Observable Multiagent Environments",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Srinivasan, S., Lanctot, M., Zambaldi, V., Pérolat, J., Tuyls, K., Munos,\nR., and Bowling, M. (2018).",
      "orig_title": "Actor-critic policy optimization in partially observable multiagent environments",
      "paper_id": "1810.09026v5"
    },
    {
      "index": 343,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Stone, P. (2007).",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 344,
      "title": "Multiagent systems: A survey from a machine learning perspective",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": "Stone, P. and Veloso, M. (2000)."
    },
    {
      "index": 345,
      "title": "Reinforcement learning in stationary mean-field games",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 18th International Conference on\nAutonomous Agents and MultiAgent Systems",
      "authors": "Subramanian, J. and Mahajan, A. (2019)."
    },
    {
      "index": 346,
      "title": "Multi Type Mean Field Reinforcement Learning",
      "abstract": "",
      "year": "2002",
      "venue": "arXiv preprint arXiv:",
      "authors": "Subramanian, S. G., Poupart, P., Taylor, M. E., and Hegde, N. (2020).",
      "orig_title": "Multi type mean field reinforcement learning",
      "paper_id": "2002.02513v7"
    },
    {
      "index": 347,
      "title": "Value-decomposition networks for cooperative multi-agent learning based on team reward",
      "abstract": "",
      "year": "2087",
      "venue": "AAMAS",
      "authors": "Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V. F.,\nJaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al.\n(2018)."
    },
    {
      "index": 348,
      "title": "A Multi-Agent Off-Policy Actor-Critic Algorithm for Distributed Reinforcement Learning",
      "abstract": "",
      "year": "1903",
      "venue": "arXiv preprint arXiv:",
      "authors": "Suttle, W., Yang, Z., Zhang, K., Wang, Z., Basar, T., and Liu, J. (2019).",
      "orig_title": "A multi-agent off-policy actor-critic algorithm for distributed reinforcement learning",
      "paper_id": "1903.06372v3"
    },
    {
      "index": 349,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "",
      "authors": "Sutton, R. S. (1988)."
    },
    {
      "index": 350,
      "title": "Reinforcement learning: An introduction, volume 1",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press Cambridge",
      "authors": "Sutton, R. S. and Barto, A. G. (1998)."
    },
    {
      "index": 351,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000)."
    },
    {
      "index": 352,
      "title": "Smooth fictitious play in n×\\times 2 potential games",
      "abstract": "",
      "year": "2019",
      "venue": "2019 53rd Asilomar Conference on Signals, Systems, and\nComputers",
      "authors": "Swenson, B. and Poor, H. V. (2019)."
    },
    {
      "index": 353,
      "title": "Apprenticeship learning using linear programming",
      "abstract": "",
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine\nlearning",
      "authors": "Syed, U., Bowling, M., and Schapire, R. E. (2008)."
    },
    {
      "index": 354,
      "title": "Algorithms for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Szepesvári, C. (2010)."
    },
    {
      "index": 355,
      "title": "A unified analysis of value-function-based reinforcement-learning algorithms",
      "abstract": "",
      "year": "2060",
      "venue": "Neural computation, 11(8):",
      "authors": "Szepesvári, C. and Littman, M. L. (1999)."
    },
    {
      "index": 356,
      "title": "Maa*: A heuristic search algorithm for solving decentralized pomdps",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Szer, D., Charpillet, F., and Zilberstein, S. (2005)."
    },
    {
      "index": 357,
      "title": "Topics in propagation of chaos",
      "abstract": "",
      "year": "1989",
      "venue": "Ecole d’été de probabilités de Saint-Flour\nXIX—",
      "authors": "Sznitman, A.-S. (1991)."
    },
    {
      "index": 358,
      "title": "Solving heads-up limit texas hold’em",
      "abstract": "",
      "year": "2015",
      "venue": "Twenty-Fourth International Joint Conference on Artificial\nIntelligence",
      "authors": "Tammelin, O., Burch, N., Johanson, M., and Bowling, M. (2015)."
    },
    {
      "index": 359,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "1993",
      "venue": "Proceedings of the tenth international conference on machine\nlearning",
      "authors": "Tan, M. (1993).",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 360,
      "title": "Transfer learning for reinforcement learning domains: A survey",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research, 10(7)",
      "authors": "Taylor, M. E. and Stone, P. (2009)."
    },
    {
      "index": 361,
      "title": "Pettingzoo: Gym for multi-agent reinforcement learning",
      "abstract": "",
      "year": "2009",
      "venue": "arXiv preprint arXiv:",
      "authors": "Terry, J. K., Black, B., Jayakumar, M., Hari, A., Santos, L., Dieffendahl, C.,\nWilliams, N. L., Lokesh, Y., Sullivan, R., Horsch, C., and Ravi, P. (2020)."
    },
    {
      "index": 362,
      "title": "Temporal difference learning and td-gammon",
      "abstract": "",
      "year": "1995",
      "venue": "Communications of the ACM, 38(3):58–68",
      "authors": "Tesauro, G. (1995)."
    },
    {
      "index": 363,
      "title": "Efficient Algorithms for Smooth Minimax Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Thekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S. (2019).",
      "orig_title": "Efficient algorithms for smooth minimax optimization",
      "paper_id": "1907.01543v1"
    },
    {
      "index": 364,
      "title": "Animal intelligence: an experimental study of the associative processes in animals",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Thorndike, E. L. (1898)."
    },
    {
      "index": 365,
      "title": "A regularized opponent model with maximum entropy objective",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on\nArtificial Intelligence",
      "authors": "Tian, Z., Wen, Y., Gong, Z., Punakkath, F., Zou, S., and Wang, J. (2019)."
    },
    {
      "index": 366,
      "title": "Hierarchical pomdp controller optimization by likelihood maximization",
      "abstract": "",
      "year": "2008",
      "venue": "UAI, volume 24",
      "authors": "Toussaint, M., Charlin, L., and Poupart, P. (2008)."
    },
    {
      "index": 367,
      "title": "An optimization approach for approximate nash equilibria",
      "abstract": "",
      "year": "2007",
      "venue": "International Workshop on Web and Internet Economics",
      "authors": "Tsaknakis, H. and Spirakis, P. G. (2007)."
    },
    {
      "index": 368,
      "title": "Evolutionary game theory and multi-agent reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Tuyls, K. and Nowé, A. (2005)."
    },
    {
      "index": 369,
      "title": "What evolutionary game theory tells us about multiagent learning",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Tuyls, K. and Parsons, S. (2007)."
    },
    {
      "index": 370,
      "title": "A Generalised Method for Empirical Game Theoretic Analysis",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 17th International Conference on\nAutonomous Agents and MultiAgent Systems",
      "authors": "Tuyls, K., Perolat, J., Lanctot, M., Leibo, J. Z., and Graepel, T. (2018).",
      "orig_title": "A generalised method for empirical game theoretic analysis",
      "paper_id": "1803.06376v1"
    },
    {
      "index": 371,
      "title": "Multiagent learning: Basics, challenges, and prospects",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Tuyls, K. and Weiss, G. (2012)."
    },
    {
      "index": 372,
      "title": "Approximate Equilibrium Computation for Discrete-Time Linear-Quadratic Mean-Field Games",
      "abstract": "",
      "year": "2020",
      "venue": "2020 American Control Conference (ACC), pages 333–339.\nIEEE",
      "authors": "uz Zaman, M. A., Zhang, K., Miehling, E., and Başar, T. (2020).",
      "orig_title": "Approximate equilibrium computation for discrete-time linear-quadratic mean-field games",
      "paper_id": "2003.13195v2"
    },
    {
      "index": 373,
      "title": "Reinforcement learning and markov decision processes",
      "abstract": "",
      "year": "2012",
      "venue": "Reinforcement Learning",
      "authors": "Van Otterlo, M. and Wiering, M. (2012)."
    },
    {
      "index": 374,
      "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
      "abstract": "",
      "year": "",
      "venue": "Nature, 575(",
      "authors": "Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung,\nJ., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019a)."
    },
    {
      "index": 375,
      "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
      "abstract": "",
      "year": "",
      "venue": "Nature, 575(",
      "authors": "Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung,\nJ., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019b)."
    },
    {
      "index": 376,
      "title": "StarCraft II: A New Challenge for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo,\nM., Makhzani, A., Küttler, H., Agapiou, J., Schrittwieser, J., et al.\n(2017).",
      "orig_title": "Starcraft ii: A new challenge for reinforcement learning",
      "paper_id": "1708.04782v1"
    },
    {
      "index": 377,
      "title": "No-regret dynamics and fictitious play",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Economic Theory, 148(2):825–842",
      "authors": "Viossat, Y. and Zapechelnyuk, A. (2013)."
    },
    {
      "index": 378,
      "title": "Theory of games and economic behavior",
      "abstract": "",
      "year": "1945",
      "venue": "Princeton University Press Princeton, NJ",
      "authors": "Von Neumann, J. and Morgenstern, O. (1945)."
    },
    {
      "index": 379,
      "title": "Theory of games and economic behavior (commemorative edition)",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "Von Neumann, J. and Morgenstern, O. (2007)."
    },
    {
      "index": 380,
      "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Wang, L., Cai, Q., Yang, Z., and Wang, Z. (2019).",
      "orig_title": "Neural policy gradient methods: Global optimality and rates of convergence",
      "paper_id": "1909.01150v3"
    },
    {
      "index": 381,
      "title": "Reinforcement learning to play an optimal nash equilibrium in team markov games",
      "abstract": "",
      "year": "2003",
      "venue": "Advances in neural information processing systems",
      "authors": "Wang, X. and Sandholm, T. (2003)."
    },
    {
      "index": 382,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": "Watkins, C. J. and Dayan, P. (1992).",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 383,
      "title": "Solving games with functional regret estimation",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:",
      "authors": "Waugh, K., Morrill, D., Bagnell, J. A., and Bowling, M. (2014)."
    },
    {
      "index": 384,
      "title": "The optimal reward baseline for gradient-based reinforcement learning",
      "abstract": "",
      "year": "2001",
      "venue": "Proceedings of the Seventeenth conference on Uncertainty in\nartificial intelligence",
      "authors": "Weaver, L. and Tao, N. (2001)."
    },
    {
      "index": 385,
      "title": "Online reinforcement learning in stochastic games",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wei, C.-Y., Hong, Y.-T., and Lu, C.-J. (2017)."
    },
    {
      "index": 386,
      "title": "Multiagent systems: a modern approach to distributed artificial intelligence",
      "abstract": "",
      "year": "1999",
      "venue": "MIT press",
      "authors": "Weiss, G. (1999)."
    },
    {
      "index": 387,
      "title": "L’hypothèse du champ moléculaire et la propriété ferromagnétique",
      "abstract": "",
      "year": "1907",
      "venue": "",
      "authors": "Weiss, P. (1907)."
    },
    {
      "index": 388,
      "title": "Methods for empirical game-theoretic analysis",
      "abstract": "",
      "year": "2006",
      "venue": "AAAI",
      "authors": "Wellman, M. P. (2006)."
    },
    {
      "index": 389,
      "title": "Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning",
      "abstract": "",
      "year": "1901",
      "venue": "IJCAI",
      "authors": "Wen, Y., Yang, Y., Luo, R., and Wang, J. (2019)."
    },
    {
      "index": 390,
      "title": "Probabilistic recursive reasoning for multi-agent reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Wen, Y., Yang, Y., Luo, R., Wang, J., and Pan, W. (2018)."
    },
    {
      "index": 391,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": "Williams, R. J. (1992)."
    },
    {
      "index": 392,
      "title": "An introduction to multiagent systems",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Wooldridge, M. (2009)."
    },
    {
      "index": 393,
      "title": "Rollout sampling policy iteration for decentralized pomdps",
      "abstract": "",
      "year": "2010",
      "venue": "Proceedings of the Twenty-Sixth Conference on Uncertainty in\nArtificial Intelligence",
      "authors": "Wu, F., Zilberstein, S., and Chen, X. (2010)."
    },
    {
      "index": 394,
      "title": "Monte-carlo expectation maximization for decentralized pomdps",
      "abstract": "",
      "year": "2013",
      "venue": "Twenty-Third International Joint Conference on Artificial\nIntelligence",
      "authors": "Wu, F., Zilberstein, S., and Jennings, N. R. (2013)."
    },
    {
      "index": 395,
      "title": "Multiagent planning with trembling-hand perfect equilibrium in multiagent pomdps",
      "abstract": "",
      "year": "2007",
      "venue": "Pacific Rim International Conference on Multi-Agents",
      "authors": "Yabu, Y., Yokoo, M., and Iwasaki, A. (2007)."
    },
    {
      "index": 396,
      "title": "Online learning in markov decision processes with adversarially chosen transition probability distributions",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in neural information processing systems",
      "authors": "Yadkori, Y. A., Bartlett, P. L., Kanade, V., Seldin, Y., and Szepesvári, C.\n(2013)."
    },
    {
      "index": 397,
      "title": "Learning Deep Mean Field Games for Modeling Large Population Behavior",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Yang, J., Ye, X., Trivedi, R., Xu, H., and Zha, H. (2018a).",
      "orig_title": "Learning deep mean field games for modeling large population behavior",
      "paper_id": "1711.03156v2"
    },
    {
      "index": 398,
      "title": "Mean Field Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang, J. (2018b).",
      "orig_title": "Mean field multi-agent reinforcement learning",
      "paper_id": "1802.05438v5"
    },
    {
      "index": 399,
      "title": "Alpha-alpha-rank: Scalable multi-agent evaluation through evolution",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yang, Y., Tutunov, R., Sakulwongtana, P., Ammar, H. B., and Wang, J. (2019a)."
    },
    {
      "index": 400,
      "title": "Multi-Agent Determinantal Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Yang, Y., Wen, Y., Chen, L., Wang, J., Shao, K., Mguni, D., and Zhang, W.\n(2020).",
      "orig_title": "Multi-agent determinantal q-learning",
      "paper_id": "2006.01482v4"
    },
    {
      "index": 401,
      "title": "Provably global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yang, Z., Chen, Y., Hong, M., and Wang, Z. (2019b)."
    },
    {
      "index": 402,
      "title": "A Theoretical Analysis of Deep Q-Learning",
      "abstract": "",
      "year": "1901",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yang, Z., Xie, Y., and Wang, Z. (2019c).",
      "orig_title": "A theoretical analysis of deep q-learning",
      "paper_id": "1901.00137v3"
    },
    {
      "index": 403,
      "title": "A new complexity result on solving the markov decision problem",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Ye, Y. (2005)."
    },
    {
      "index": 404,
      "title": "The simplex method is strongly polynomial for the markov decision problem with a fixed discount rate",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Ye, Y. (2010)."
    },
    {
      "index": 405,
      "title": "Learning team-optimality for decentralized stochastic control and dynamic games",
      "abstract": "",
      "year": "1903",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yongacoglu, B., Arslan, G., and Yüksel, S. (2019)."
    },
    {
      "index": 406,
      "title": "The evolution of conventions",
      "abstract": "",
      "year": "1993",
      "venue": "Econometrica: Journal of the Econometric Society",
      "authors": "Young, H. P. (1993)."
    },
    {
      "index": 407,
      "title": "Markov decision processes with arbitrary reward processes",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Yu, J. Y., Mannor, S., and Shimkin, N. (2009)."
    },
    {
      "index": 408,
      "title": "Dynamic potential games in communications: Fundamentals and applications",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "Zazo, S., Valcarcel Macua, S., Sánchez-Fernández, M., and Zazo, J.\n(2015)."
    },
    {
      "index": 409,
      "title": "On an application of set theory to the theory of the game of chess",
      "abstract": "",
      "year": "1913",
      "venue": "Congress of Mathematicians",
      "authors": "Zermelo, E. and Borel, E. (1913)."
    },
    {
      "index": 410,
      "title": "Multi-agent learning with policy prediction",
      "abstract": "",
      "year": "2010",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 24",
      "authors": "Zhang, C. and Lesser, V. (2010)."
    },
    {
      "index": 411,
      "title": "Convergence of Gradient Methods on Bilinear Zero-Sum Games",
      "abstract": "",
      "year": "1908",
      "venue": "arXiv e-prints",
      "authors": "Zhang, G. and Yu, Y. (2019).",
      "orig_title": "Convergence of gradient methods on bilinear zero-sum games",
      "paper_id": "1908.05699v4"
    },
    {
      "index": 412,
      "title": "Bi-level actor-critic for multi-agent coordination",
      "abstract": "",
      "year": "1909",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhang, H., Chen, W., Huang, Z., Li, M., Yang, Y., Zhang, W., and Wang, J.\n(2019a)."
    },
    {
      "index": 413,
      "title": "Robust multi-agent reinforcement learning with model uncertainty",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Zhang, K., Sun, T., Tao, Y., Genc, S., Mallya, S., and Basar, T. (2020)."
    },
    {
      "index": 414,
      "title": "Networked multi-agent reinforcement learning in continuous spaces",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Zhang, K., Yang, Z., and Basar, T. (2018a)."
    },
    {
      "index": 415,
      "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
      "abstract": "",
      "year": "1911",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhang, K., Yang, Z., and Başar, T. (2019b).",
      "orig_title": "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
      "paper_id": "1911.10635v2"
    },
    {
      "index": 416,
      "title": "Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhang, K., Yang, Z., and Basar, T. (2019c).",
      "orig_title": "Policy optimization provably converges to nash equilibria in zero-sum linear quadratic games",
      "paper_id": "1906.00729v4"
    },
    {
      "index": 417,
      "title": "Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement Learning With Networked Agents",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhang, K., Yang, Z., Liu, H., Zhang, T., and Başar, T. (2018b).",
      "orig_title": "Finite-sample analysis for decentralized batch multi-agent reinforcement learning with networked agents",
      "paper_id": "1812.02783v8"
    },
    {
      "index": 418,
      "title": "Fully decentralized multi-agent reinforcement learning with networked agents",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018c)."
    },
    {
      "index": 419,
      "title": "Distributed off-Policy Actor-Critic Reinforcement Learning with Policy Consensus",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Zhang, Y. and Zavlanos, M. M. (2019).",
      "orig_title": "Distributed off-policy actor-critic reinforcement learning with policy consensus",
      "paper_id": "1903.09255v1"
    },
    {
      "index": 420,
      "title": "Analysis and improvement of policy gradient estimation",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M. (2011)."
    },
    {
      "index": 421,
      "title": "Factorized Q-Learning for Large-Scale Multi-Agent Systems",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the First International Conference on\nDistributed Artificial Intelligence",
      "authors": "Zhou, M., Chen, Y., Wen, Y., Yang, Y., Su, Y., Zhang, W., Zhang, D., and Wang,\nJ. (2019).",
      "orig_title": "Factorized q-learning for large-scale multi-agent systems",
      "paper_id": "1809.03738v4"
    },
    {
      "index": 422,
      "title": "Online learning in episodic markovian decision processes by relative entropy policy search",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in neural information processing systems",
      "authors": "Zimin, A. and Neu, G. (2013)."
    },
    {
      "index": 423,
      "title": "Online convex programming and generalized infinitesimal gradient ascent",
      "abstract": "",
      "year": "2003",
      "venue": "Proceedings of the 20th international conference on machine\nlearning (icml-03)",
      "authors": "Zinkevich, M. (2003)."
    },
    {
      "index": 424,
      "title": "Cyclic equilibria in markov games",
      "abstract": "",
      "year": "2006",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zinkevich, M., Greenwald, A., and Littman, M. L. (2006)."
    },
    {
      "index": 425,
      "title": "Regret minimization in games with incomplete information",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in neural information processing systems",
      "authors": "Zinkevich, M., Johanson, M., Bowling, M., and Piccione, C. (2008)."
    }
  ]
}