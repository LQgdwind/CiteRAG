{
  "paper_id": "2210.17049v2",
  "title": "Modular Hybrid Autoregressive Transducer",
  "abstract": "Abstract\nText-only adaptation of a transducer model remains challenging for end-to-end speech recognition since the transducer has no clearly separated acoustic model (AM), language model (LM) or blank model. In this work, we propose a modular hybrid autoregressive transducer (MHAT) that has structurally separated label and blank decoders to predict label and blank distributions, respectively, along with a shared acoustic encoder. The encoder and label decoder outputs are directly projected to AM and internal LM scores and then added to compute label posteriors. We train MHAT with an internal LM loss and a HAT loss to ensure that its internal LM becomes a standalone neural LM that can be effectively adapted to text. Moreover, text adaptation of MHAT fosters a much better LM fusion than internal LM subtraction-based methods. On Google’s large-scale production data, a multi-domain MHAT adapted with 100B sentences achieves relative WER reductions of up to 12.4% without LM fusion and 21.5% with LM fusion from 400K-hour trained HAT.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "abstract": "",
      "year": "2006",
      "venue": "ICML. ACM",
      "authors": "A. Graves, S. Fernández, F. Gomez, et al."
    },
    {
      "index": 1,
      "title": "Deep Speech: Scaling up end-to-end speech recognition",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.5567",
      "authors": "A. Hannun, C. Case, et al.",
      "orig_title": "Deep speech: Scaling up end-to-end speech recognition",
      "paper_id": "1412.5567v2"
    },
    {
      "index": 2,
      "title": "Sequence transduction with recurrent neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1211.3711",
      "authors": "A. Graves"
    },
    {
      "index": 3,
      "title": "RNN-T for Latency Controlled ASR WITH IMPROVED BEAM SEARCH",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.01629",
      "authors": "M. Jain, K. Schubert, J. Mahadeokar, et al.",
      "orig_title": "RNN-T for latency controlled ASR with improved beam search",
      "paper_id": "1911.01629v2"
    },
    {
      "index": 4,
      "title": "A Streaming On-Device End-to-End Model Surpassing Server-Side Conventional Model Quality and Latency",
      "abstract": "",
      "year": "2020",
      "venue": "Proc. ICASSP",
      "authors": "T. Sainath, Y. He, B. Li, et al.",
      "orig_title": "A streaming on-device end-to-end model surpassing server-side conventional model quality and latency",
      "paper_id": "2003.12710v2"
    },
    {
      "index": 5,
      "title": "Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability",
      "abstract": "",
      "year": "2020",
      "venue": "Interspeech",
      "authors": "J. Li, R. Zhao, Z. Meng, et al.",
      "orig_title": "Developing RNN-T models surpassing high-performance hybrid models with customization capability",
      "paper_id": "2007.15188v1"
    },
    {
      "index": 6,
      "title": "Attention-based models for speech recognition",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "J. K Chorowski, D. Bahdanau, D. Serdyuk, et al."
    },
    {
      "index": 7,
      "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "abstract": "",
      "year": "2016",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "W. Chan, N. Jaitly, Q. Le, and O. Vinyals"
    },
    {
      "index": 8,
      "title": "State-of-the-art speech recognition with sequence-to-sequence models",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. ICASSP",
      "authors": "C.-C. Chiu, T. Sainath, et al."
    },
    {
      "index": 9,
      "title": "Kl-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition",
      "abstract": "",
      "year": "2013",
      "venue": "Proc. ICASSP",
      "authors": "D. Yu, K. Yao, H. Su, et al."
    },
    {
      "index": 10,
      "title": "Adversarial Speaker Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. ICASSP",
      "authors": "Z. Meng, J. Li, and Y. Gong",
      "orig_title": "Adversarial speaker adaptation",
      "paper_id": "1904.12407v1"
    },
    {
      "index": 11,
      "title": "Speaker adaptation of context dependent deep neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "Proc. ICASSP",
      "authors": "H. Liao"
    },
    {
      "index": 12,
      "title": "L-Vector: Neural Label Embedding for Domain Adaptation",
      "abstract": "",
      "year": "2020",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "Z. Meng, H. Hu, J. Li, et al.",
      "orig_title": "L-vector: Neural label embedding for domain adaptation",
      "paper_id": "2004.13480v1"
    },
    {
      "index": 13,
      "title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models",
      "abstract": "",
      "year": "2014",
      "venue": "Proc. SLT. IEEE",
      "authors": "P. Swietojanski and S. Renals"
    },
    {
      "index": 14,
      "title": "Learning small-size DNN with output-distribution-based criteria.",
      "abstract": "",
      "year": "2014",
      "venue": "INTERSPEECH",
      "authors": "J. Li, R. Zhao, et al."
    },
    {
      "index": 15,
      "title": "Adversarial Teacher-Student Learning for Unsupervised Domain Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. ICASSP",
      "authors": "Z. Meng, J. Li, Y. Gong, et al.",
      "orig_title": "Adversarial teacher-student learning for unsupervised domain adaptation",
      "paper_id": "1804.00644v2"
    },
    {
      "index": 16,
      "title": "A teacher-student learning approach for unsupervised domain adaptation of sequence-trained ASR models",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. SLT. IEEE",
      "authors": "V. Manohar, P. Ghahremani, D. Povey, et al."
    },
    {
      "index": 17,
      "title": "Conditional Teacher-Student Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. ICASSP",
      "authors": "Z. Meng, J. Li, Y. Zhao, and Y. Gong",
      "orig_title": "Conditional teacher-student learning",
      "paper_id": "1904.12399v1"
    },
    {
      "index": 18,
      "title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models",
      "abstract": "",
      "year": "2007",
      "venue": "Speech Communication",
      "authors": "R. Gemello, F. Mana, S. Scanzio, et al."
    },
    {
      "index": 19,
      "title": "Cluster adaptive training for deep neural network",
      "abstract": "",
      "year": "2015",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "T. Tan, Y. Qian, M. Yin, et al."
    },
    {
      "index": 20,
      "title": "Fast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code",
      "abstract": "",
      "year": "2013",
      "venue": "Proc. ICASSP",
      "authors": "O. Abdel-Hamid and H. Jiang"
    },
    {
      "index": 21,
      "title": "Adversarial multi-task learning of deep neural networks for robust speech recognition",
      "abstract": "",
      "year": "2016",
      "venue": "INTERSPEECH",
      "authors": "Y. Shinohara"
    },
    {
      "index": 22,
      "title": "Speaker-Invariant Training via Adversarial Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. ICASSP",
      "authors": "Z. Meng, J. Li, Z. Chen, et al.",
      "orig_title": "Speaker-invariant training via adversarial learning",
      "paper_id": "1804.00732v3"
    },
    {
      "index": 23,
      "title": "Invariant Representations for Noisy Speech Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS Workshop",
      "authors": "D. Serdyuk, K. Audhkhasi, et al.",
      "orig_title": "Invariant representations for noisy speech recognition",
      "paper_id": "1612.01928v1"
    },
    {
      "index": 24,
      "title": "Unsupervised Adaptation with Domain Separation Networks for Robust Speech Recognition",
      "abstract": "",
      "year": "2017",
      "venue": "Proc. ASRU",
      "authors": "Z. Meng, Z. Chen, V. Mazalov, J. Li, and Y. Gong",
      "orig_title": "Unsupervised adaptation with domain separation networks for robust speech recognition",
      "paper_id": "1711.08010v2"
    },
    {
      "index": 25,
      "title": "Speaker adaptation for multichannel end-to-end speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. ICASSP",
      "authors": "T. Ochiai, S. Watanabe, et al."
    },
    {
      "index": 26,
      "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. Interspeech",
      "authors": "Z. Meng, Y. Gaur, J. Li, et al.",
      "orig_title": "Speaker adaptation for attention-based end-to-end speech recognition",
      "paper_id": "1911.03762v1"
    },
    {
      "index": 27,
      "title": "Domain Adaptation via Teacher-Student Learning for End-to-End Speech Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. ASRU. IEEE",
      "authors": "Z. Meng, J. Li, Y. Gaur, and Y. Gong",
      "orig_title": "Domain adaptation via teacher-student learning for end-to-end speech recognition",
      "paper_id": "2001.01798v1"
    },
    {
      "index": 28,
      "title": "On using monolingual corpora in neural machine translation",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "C. Gulcehre, O. Firat, K. Xu, et al."
    },
    {
      "index": 29,
      "title": "A DENSITY RATIO APPROACH TO LANGUAGE MODEL FUSION IN END-TO-END AUTOMATIC SPEECH RECOGNITION",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. ASRU. IEEE",
      "authors": "E. McDermott, H. Sak, and E. Variani",
      "orig_title": "A density ratio approach to language model fusion in end-to-end automatic speech recognition",
      "paper_id": "2002.11268v3"
    },
    {
      "index": 30,
      "title": "Maximum a posteriori based decoding for CTC acoustic models.",
      "abstract": "",
      "year": "2016",
      "venue": "Interspeech",
      "authors": "N. Kanda, X. Lu, and H. Kawai"
    },
    {
      "index": 31,
      "title": "A gaussian mixture model layer jointly optimized with discriminative features within a deep neural network architecture",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE",
      "authors": "E. Variani, E. McDermott, and G. Heigold"
    },
    {
      "index": 32,
      "title": "Hybrid Autoregressive Transducer (HAT)",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.07705",
      "authors": "E. Variani, D. Rybach, et al.",
      "orig_title": "Hybrid autoregressive transducer (hat)",
      "paper_id": "2003.07705v1"
    },
    {
      "index": 33,
      "title": "Internal language model estimation for domain-adaptive end-to-end speech recognition",
      "abstract": "",
      "year": "2021",
      "venue": "Proc. SLT. IEEE",
      "authors": "Z. Meng, S. Parthasarathy, E. Sun, et al."
    },
    {
      "index": 34,
      "title": "Librispeech transducer model with internal language model prior correction",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.03006",
      "authors": "A. Zeyer, A. Merboldt, W. Michel, et al."
    },
    {
      "index": 35,
      "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
      "abstract": "",
      "year": "2019",
      "venue": "INTERSPEECH",
      "authors": "C. Peyser, H. Zhang, T. N. Sainath, et al.",
      "orig_title": "Improving performance of end-to-end ASR on numeric sequences",
      "paper_id": "1907.01372v1"
    },
    {
      "index": 36,
      "title": "Rapid RNN-T adaptation using personalized speech synthesis and neural language generator.",
      "abstract": "",
      "year": "2020",
      "venue": "INTERSPEECH",
      "authors": "Y. Huang, J. Li, L. He, et al."
    },
    {
      "index": 37,
      "title": "Improving speech recognition using GAN-based speech synthesis and contrastive unspoken text selection.",
      "abstract": "",
      "year": "2020",
      "venue": "Interspeech",
      "authors": "Z. Chen, A. Rosenberg, Y. Zhang, et al."
    },
    {
      "index": 38,
      "title": "Fast text-only domain adaptation of RNN-transducer prediction network",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.11127",
      "authors": "J. Pylkkönen, A. Ukkonen, J. Kilpikoski, et al."
    },
    {
      "index": 39,
      "title": "Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "Interspeech",
      "authors": "Z. Meng, Y. Gaur, N. Kanda, et al.",
      "orig_title": "Internal language model adaptation with text-only data for end-to-end speech recognition",
      "paper_id": "2110.05354v5"
    },
    {
      "index": 40,
      "title": "Internal Language Model Training for Domain-Adaptive End-to-End Speech Recognition",
      "abstract": "",
      "year": "2021",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "Z. Meng, N. Kanda, Y. Gaur, et al.",
      "orig_title": "Internal language model training for domain-adaptive end-to-end speech recognition",
      "paper_id": "2102.01380v2"
    },
    {
      "index": 41,
      "title": "Factorized Neural Transducer for Efficient Language Model Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "X. Chen, Z. Meng, et al.",
      "orig_title": "Factorized neural transducer for efficient language model adaptation",
      "paper_id": "2110.01500v5"
    },
    {
      "index": 42,
      "title": "A new training pipeline for an improved neural transducer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.09319",
      "authors": "A. Zeyer, A. Merboldt, R. Schlüter, and H. Ney"
    },
    {
      "index": 43,
      "title": "On information and sufficiency",
      "abstract": "",
      "year": "1951",
      "venue": "The annals of mathematical statistics",
      "authors": "S. Kullback and R. A Leibler"
    },
    {
      "index": 44,
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "abstract": "",
      "year": "2015",
      "venue": "Proc. ICASSP",
      "authors": "V. Panayotov, G. Chen, et al."
    },
    {
      "index": 45,
      "title": "Ted-lium: an automatic speech recognition dedicated corpus.",
      "abstract": "",
      "year": "2012",
      "venue": "LREC",
      "authors": "A. Rousseau, P. Deléglise, and Y. Esteve"
    },
    {
      "index": 46,
      "title": "Common Voice: A Massively-Multilingual Speech Corpus",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.06670",
      "authors": "R. Ardila, M. Branson, K. Davis, et al.",
      "orig_title": "Common voice: A massively-multilingual speech corpus",
      "paper_id": "1912.06670v2"
    },
    {
      "index": 47,
      "title": "TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on speech and computer. Springer",
      "authors": "F. Hernandez, V. Nguyen, S. Ghannay, et al.",
      "orig_title": "Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation",
      "paper_id": "1805.04699v4"
    },
    {
      "index": 48,
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.08100",
      "authors": "A. Gulati, J. Qin, C. Chiu, et al.",
      "orig_title": "Conformer: Convolution-augmented transformer for speech recognition",
      "paper_id": "2005.08100v1"
    },
    {
      "index": 49,
      "title": "Tied & Reduced RNN-T Decoder",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.07513",
      "authors": "R. Botros, T. N Sainath, R. David, et al.",
      "orig_title": "Tied & reduced rnn-t decoder",
      "paper_id": "2109.07513v1"
    },
    {
      "index": 50,
      "title": "Japanese and korean voice search",
      "abstract": "",
      "year": "2012",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "M. Schuster and K. Nakajima"
    },
    {
      "index": 51,
      "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "abstract": "",
      "year": "2014",
      "venue": "Interspeech",
      "authors": "H. Sak, A. Senior, and F. Beaufays"
    },
    {
      "index": 52,
      "title": "Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition",
      "abstract": "",
      "year": "2017",
      "venue": "ICASSP. IEEE",
      "authors": "Z. Meng, S. Watanabe, J. R. Hershey, et al."
    },
    {
      "index": 53,
      "title": "Multi-channel speech recognition: LSTMs all the way through",
      "abstract": "",
      "year": "2016",
      "venue": "CHiME-4 workshop",
      "authors": "H. Erdogan, T. Hayashi, et al."
    },
    {
      "index": 54,
      "title": "Improving the latency and quality of cascaded encoders",
      "abstract": "",
      "year": "2022",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "T. N Sainath, Y. He, A. Narayanan, et al."
    },
    {
      "index": 55,
      "title": "Recognizing Long-Form Speech Using Streaming End-to-End Models",
      "abstract": "",
      "year": "2019",
      "venue": "Proc. ASRU. IEEE",
      "authors": "A. Narayanan, R. Prabhavalkar, C. Chiu, et al.",
      "orig_title": "Recognizing long-form speech using streaming end-to-end models",
      "paper_id": "1910.11455v1"
    },
    {
      "index": 56,
      "title": "Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription",
      "abstract": "",
      "year": "2013",
      "venue": "Proc. ASRU. IEEE",
      "authors": "H. Liao, E. McDermott, et al."
    },
    {
      "index": 57,
      "title": "Artificial intelligence at google: Our principles.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "C. Kim, A. Misra, K. Chin, et al."
    },
    {
      "index": 59,
      "title": "Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM",
      "abstract": "",
      "year": "2012",
      "venue": "Proc. SLT. IEEE",
      "authors": "J. Li, D. Yu, J.-T. Huang, et al."
    },
    {
      "index": 60,
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1904.08779",
      "authors": "D. S Park, W. Chan, Y. Zhang, et al.",
      "orig_title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "paper_id": "1904.08779v3"
    },
    {
      "index": 61,
      "title": "Two-pass end-to-end speech recognition",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.10992",
      "authors": "T. N Sainath, R. Pang, D. Rybach, et al."
    },
    {
      "index": 62,
      "title": "Cascaded encoders for unifying streaming and non-streaming asr",
      "abstract": "",
      "year": "2021",
      "venue": "Proc. ICASSP. IEEE",
      "authors": "A. Narayanan, T. N Sainath, R. Pang, et al."
    }
  ]
}