{
  "paper_id": "2303.03106v3",
  "title": "Rotation Invariant Quantization for Model Compression",
  "abstract": "Abstract\nPost-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources.\nIn this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates ×19.4absent19.4\\times 19.4 and ×52.9absent52.9\\times 52.9 compression ratios on pre-trained VGG dense and pruned models, respectively, with <0.4%absentpercent0.4<0.4\\% accuracy degradation.\nCode is available in github.com/ehaleva/RIQ.",
  "reference_labels": [
    {
      "index": 0,
      "title": "On the surprising behavior of distance metrics in high dimensional space",
      "abstract": "",
      "year": "2001",
      "venue": "Database Theory—ICDT 2001",
      "authors": "Aggarwal, C. C., Hinneburg, A., and Keim, D. A."
    },
    {
      "index": 1,
      "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Banner, R., Nahshan, Y., and Soudry, D.",
      "orig_title": "Post training 4-bit quantization of convolutional networks for rapid-deployment",
      "paper_id": "1810.05723v3"
    },
    {
      "index": 2,
      "title": "CAT: Compression-Aware Training for bandwidth reduction",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Baskin, C., Chmiel, B., Zheltonozhskii, E., Banner, R., Bronstein, A. M., and Mendelson, A.",
      "orig_title": "Cat: Compression-aware training for bandwidth reduction",
      "paper_id": "1909.11481v1"
    },
    {
      "index": 3,
      "title": "Uniq: Uniform noise injection for non-uniform quantization of neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Transactions on Computer Systems (TOCS)",
      "authors": "Baskin, C., Liss, N., Schwartz, E., Zheltonozhskii, E., Giryes, R., Bronstein, A. M., and Mendelson, A."
    },
    {
      "index": 4,
      "title": "LSQ+: Improving low-bit quantization through learnable offsets and better initialization",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Bhalgat, Y., Lee, J., Nagel, M., Blankevoort, T., and Kwak, N.",
      "orig_title": "Lsq+: Improving low-bit quantization through learnable offsets and better initialization",
      "paper_id": "2004.09576v1"
    },
    {
      "index": 5,
      "title": "Sqnr estimation of fixed-point dsp algorithms",
      "abstract": "",
      "year": "2010",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "authors": "Caffarena, G., Carreras, C., López, J. A., and Fernández, Á."
    },
    {
      "index": 6,
      "title": "How to Train your DNN: The Network Operator Edition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Chang, M. A., Bottini, D., Jian, L., Kumar, P., Panda, A., and Shenker, S.",
      "orig_title": "How to train your dnn: The network operator edition",
      "paper_id": "2004.10275v1"
    },
    {
      "index": 7,
      "title": "Low-bit quantization of neural networks for efficient inference",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision Workshop",
      "authors": "Choukroun, Y., Kravchik, E., Yang, F., and Kisilev, P."
    },
    {
      "index": 8,
      "title": "Elements of information theory",
      "abstract": "",
      "year": "2006",
      "venue": "Wiley-Interscience",
      "authors": "Cover, T. M. and Thomas, J. A."
    },
    {
      "index": 9,
      "title": "Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Daulton, S., Balandat, M., and Bakshy, E."
    },
    {
      "index": 10,
      "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Daulton, S., Balandat, M., and Bakshy, E.",
      "orig_title": "Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement",
      "paper_id": "2105.08195v2"
    },
    {
      "index": 11,
      "title": "Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Daulton, S., Eriksson, D., Balandat, M., and Bakshy, E.",
      "orig_title": "Multi-objective bayesian optimization over high-dimensional search spaces",
      "paper_id": "2109.10964v4"
    },
    {
      "index": 12,
      "title": "Differentiable Model Compression via Pseudo Quantization Noise",
      "abstract": "",
      "year": "2022",
      "venue": "TMLR",
      "authors": "Défossez, A., Adi, Y., and Synnaeve, G.",
      "orig_title": "Differentiable model compression via pseudo quantization noise",
      "paper_id": "2104.09987v3"
    },
    {
      "index": 13,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 14,
      "title": "Asymmetric numeral systems: entropy coding combining speed of huffman coding with compression rate of arithmetic coding",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Duda, J."
    },
    {
      "index": 15,
      "title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces",
      "abstract": "",
      "year": "2021",
      "venue": "UAI",
      "authors": "Eriksson, D. and Jankowiak, M.",
      "orig_title": "High-dimensional bayesian optimization with sparse axis-aligned subspaces",
      "paper_id": "2103.00349v2"
    },
    {
      "index": 16,
      "title": "Training with Quantization Noise for Extreme Model Compression",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R., Jégou, H., and Joulin, A.",
      "orig_title": "Training with quantization noise for extreme model compression",
      "paper_id": "2004.07320v3"
    },
    {
      "index": 17,
      "title": "Symmetric multivariate and related distributions",
      "abstract": "",
      "year": "2018",
      "venue": "Chapman and Hall/CRC",
      "authors": "Fang, K.-T., Kotz, S., and Ng, K. W."
    },
    {
      "index": 18,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Frankle, J. and Carbin, M.",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 19,
      "title": "A framework for few-shot language model evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A."
    },
    {
      "index": 20,
      "title": "Rate Distortion For Model Compression: From Theory To Practice",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Gao, W., Liu, Y.-H., Wang, C., and Oh, S.",
      "orig_title": "Rate distortion for model compression: From theory to practice",
      "paper_id": "1810.06401v2"
    },
    {
      "index": 21,
      "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Han, S., Mao, H., and Dally, W. J."
    },
    {
      "index": 22,
      "title": "Optimal brain surgeon and general network pruning",
      "abstract": "",
      "year": "1993",
      "venue": "IEEE international conference on neural networks",
      "authors": "Hassibi, B., Stork, D. G., and Wolff, G. J."
    },
    {
      "index": 23,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "He, K., Zhang, X., Ren, S., and Sun, J.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 24,
      "title": "Finding the rotation matrix in n-dimensions",
      "abstract": "",
      "year": "2016",
      "venue": "Mathematics Stack Exchange",
      "authors": "(https://stephenmontgomerysmith.github.io/), S. M.-S."
    },
    {
      "index": 25,
      "title": "Accurate post training quantization with small calibration sets",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D."
    },
    {
      "index": 26,
      "title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines",
      "abstract": "",
      "year": "1989",
      "venue": "Communications in Statistics - Simulation and Computation",
      "authors": "Hutchinson, M. F."
    },
    {
      "index": 27,
      "title": "Low-rank compression of neural nets: Learning the rank of each layer",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Idelbayev, Y. and Carreira-Perpinán, M. A."
    },
    {
      "index": 28,
      "title": "Optimal quantization using scaled codebook",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Idelbayev, Y., Molchanov, P., Shen, M., Yin, H., Carreira-Perpinán, M. A., and Alvarez, J. M."
    },
    {
      "index": 29,
      "title": "An information-theoretic justification for model pruning",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Isik, B., Weissman, T., and No, A."
    },
    {
      "index": 30,
      "title": "ultralytics/yolov5: v6.1 - TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jocher, G., Chaurasia, A., Stoken, A., Borovec, J., NanoCode012, Kwon, Y., TaoXie, Fang, J., imyhxy, Michael, K., Lorna, V, A., Montes, D., Nadar, J., Laughing, tkianai, yxNONG, Skalski, P., Wang, Z., Hogan, A., Fati, C., Mammana, L., AlexWang1900, Patel, D., Yiwei, D., You, F., Hajek, J., Diaconu, L., and Minh, M. T."
    },
    {
      "index": 31,
      "title": "Gaussian approximation of quantization error for estimation from compressed data",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Kipnis, A. and Reeves, G."
    },
    {
      "index": 32,
      "title": "Quantization with minimal entropy",
      "abstract": "",
      "year": "1963",
      "venue": "Probl. Pered. Inform",
      "authors": "Koshelev, V."
    },
    {
      "index": 33,
      "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Krishnamoorthi, R.",
      "orig_title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
      "paper_id": "1806.08342v1"
    },
    {
      "index": 34,
      "title": "Optimal brain damage",
      "abstract": "",
      "year": "1989",
      "venue": "Advances in neural information processing systems",
      "authors": "LeCun, Y., Denker, J., and Solla, S."
    },
    {
      "index": 35,
      "title": "Learning Low-Rank Approximation for CNNs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Lee, D., Kwon, S. J., Kim, B., and Wei, G.-Y.",
      "orig_title": "Learning low-rank approximation for CNNs",
      "paper_id": "1905.10145v1"
    },
    {
      "index": 36,
      "title": "Brecq: pushing the limit of post-training quantization by block reconstruction",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S.",
      "orig_title": "Brecq: Pushing the limit of post-training quantization by block reconstruction",
      "paper_id": "2102.05426v2"
    },
    {
      "index": 37,
      "title": "Holistic cnn compression via low-rank decomposition with knowledge transfer",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Lin, S., Ji, R., Chen, C., Tao, D., and Luo, J."
    },
    {
      "index": 38,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision – ECCV 2014",
      "authors": "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L.",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 39,
      "title": "The validity of the additive noise model for uniform scalar quantizers",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Marco, D. and Neuhoff, D."
    },
    {
      "index": 40,
      "title": "The algebra of random variables",
      "abstract": "",
      "year": "1979",
      "venue": "Wiley",
      "authors": "Melvin Dale, S."
    },
    {
      "index": 41,
      "title": "Up or Down? Adaptive Rounding for Post-Training Quantization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T.",
      "orig_title": "Up or down? adaptive rounding for post-training quantization",
      "paper_id": "2004.10568v2"
    },
    {
      "index": 42,
      "title": "Pytorch quantization - functionalities",
      "abstract": "",
      "year": "2021",
      "venue": "docs.nvidia.com",
      "authors": "NVIDIA."
    },
    {
      "index": 43,
      "title": "Scalable Model Compression by Entropy Penalized Reparameterization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Oktay, D., Ballé, J., Singh, S., and Shrivastava, A.",
      "orig_title": "Scalable model compression by entropy penalized reparameterization",
      "paper_id": "1906.06624v3"
    },
    {
      "index": 44,
      "title": "Lecture notes on information theory",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Polyanskiy, Y. and Wu, Y."
    },
    {
      "index": 45,
      "title": "Information theory: From coding to learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Polyanskiy, Y. and Wu, Y."
    },
    {
      "index": 46,
      "title": "Squad: 100,000+ questions for machine comprehension of text",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P."
    },
    {
      "index": 47,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Sanh, V., Debut, L., Chaumond, J., and Wolf, T.",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 48,
      "title": "Knowledge Distillation Beyond Model Compression",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Pattern Recognition",
      "authors": "Sarfraz, F., Arani, E., and Zonooz, B.",
      "orig_title": "Knowledge distillation beyond model compression",
      "paper_id": "2007.01922v1"
    },
    {
      "index": 49,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Simonyan, K. and Zisserman, A.",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 50,
      "title": "Online Ensemble Model Compression using Knowledge Distillation",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Computer Vision",
      "authors": "Walawalkar, D., Shen, Z., and Savvides, M.",
      "orig_title": "Online ensemble model compression using knowledge distillation",
      "paper_id": "2011.07449v1"
    },
    {
      "index": 51,
      "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S.",
      "orig_title": "Haq: Hardware-aware automated quantization with mixed precision",
      "paper_id": "1811.08886v3"
    },
    {
      "index": 52,
      "title": "Massively parallel ans decoding on gpus",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Parallel Processing",
      "authors": "Weißenberger, A. and Schmidt, B."
    },
    {
      "index": 53,
      "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wu, H., Judd, P., Zhang, X., Isaev, M., and Micikevicius, P.",
      "orig_title": "Integer quantization for deep learning inference: Principles and empirical evaluation",
      "paper_id": "2004.09602v1"
    },
    {
      "index": 54,
      "title": "DRGS: Low-precision full quantization of deep neural network with dynamic rounding and gradient scaling for object detection",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Data Mining and Big Data",
      "authors": "Wu, Q., Li, Y., Chen, S., and Kang, Y."
    },
    {
      "index": 55,
      "title": "Prune Once for All: Sparse Pre-Trained Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zafrir, O., Larey, A., Boudoukh, G., Shen, H., and Wasserblat, M.",
      "orig_title": "Prune once for all: Sparse pre-trained language models",
      "paper_id": "2111.05754v1"
    },
    {
      "index": 56,
      "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zhang, J., Peng, H., Wu, K., Liu, M., Xiao, B., Fu, J., and Yuan, L.",
      "orig_title": "Minivit: Compressing vision transformers with weight multiplexing",
      "paper_id": "2204.07154v1"
    },
    {
      "index": 57,
      "title": "Post-training Quantization for Neural Networks with Provable Guarantees",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhang, J., Zhou, Y., and Saab, R.",
      "orig_title": "Post-training quantization for neural networks with provable guarantees",
      "paper_id": "2201.11113v3"
    },
    {
      "index": 58,
      "title": "Exploration and estimation for model compression",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Zhang, Y., Gao, S., and Huang, H."
    },
    {
      "index": 59,
      "title": "Improving neural network quantization without retraining using outlier channel splitting",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhao, R., Hu, Y., Dotzel, J., De Sa, C., and Zhang, Z."
    }
  ]
}