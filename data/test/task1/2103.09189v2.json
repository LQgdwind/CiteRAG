{
  "paper_id": "2103.09189v2",
  "title": "Goal-constrained Sparse Reinforcement Learning for End-to-End Driving",
  "abstract": "Abstract\nDeep reinforcement Learning for end-to-end driving is limited by the need of complex reward engineering. Sparse rewards can circumvent this challenge but suffers from long training time and leads to sub-optimal policy. In this work, we explore full-control driving with only goal-constrained sparse reward and propose a curriculum learning approach for end-to-end driving using only navigation view maps that benefit from small virtual-to-real domain gap. To address the complexity of multiple driving policies, we learn concurrent individual policies selected at inference by a navigation system. We demonstrate the ability of our proposal to generalize on unseen road layout, and to drive significantly longer than in the training.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Learning to generalize from sparse and underspecified rewards",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "R. Agarwal, C. Liang, D. Schuurmans, and M. Norouzi"
    },
    {
      "index": 1,
      "title": "Curriculum learning",
      "abstract": "",
      "year": "2009",
      "venue": "ICML",
      "authors": "Y. Bengio, J. Louradour, R. Collobert, and J. Weston"
    },
    {
      "index": 2,
      "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, et al.",
      "orig_title": "Dota 2 with large scale deep reinforcement learning",
      "paper_id": "1912.06680v1"
    },
    {
      "index": 3,
      "title": "Robot navigation with map-based deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICNSC",
      "authors": "G. Chen, L. Pan, P. Xu, Z. Wang, P. Wu, J. Ji, X. Chen, et al."
    },
    {
      "index": 4,
      "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving",
      "abstract": "",
      "year": "2019",
      "venue": "ITSC",
      "authors": "J. Chen, B. Yuan, and M. Tomizuka",
      "orig_title": "Model-free deep reinforcement learning for urban autonomous driving",
      "paper_id": "1904.09503v2"
    },
    {
      "index": 5,
      "title": "Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "J.-Y. Chen, S. E. Li, and M. Tomizuka",
      "orig_title": "Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning",
      "paper_id": "2001.08726v3"
    },
    {
      "index": 6,
      "title": "Deriving subgoals autonomously to accelerate learning in sparse reward domains",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "M. Dann, F. Zambetta, and J. Thangarajah"
    },
    {
      "index": 7,
      "title": "Ctrl-Z: Recovering from Instability in Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "V. Dasagi, J. Bruce, T. Peynot, and J. Leitner",
      "orig_title": "Ctrl-z: Recovering from instability in reinforcement learning",
      "paper_id": "1910.03732v1"
    },
    {
      "index": 8,
      "title": "CARLA: An Open Urban Driving Simulator",
      "abstract": "",
      "year": "2017",
      "venue": "CoRL",
      "authors": "A. Dosovitskiy, G. Ros, F. Codevilla, A. López, and V. Koltun",
      "orig_title": "Carla: An open urban driving simulator",
      "paper_id": "1711.03938v1"
    },
    {
      "index": 9,
      "title": "Learning and development in neural networks: the importance of starting small",
      "abstract": "",
      "year": "1993",
      "venue": "Cognition",
      "authors": "J. Elman"
    },
    {
      "index": 10,
      "title": "Automated Curriculum Learning for Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu",
      "orig_title": "Automated curriculum learning for neural networks",
      "paper_id": "1704.03003v1"
    },
    {
      "index": 11,
      "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
      "abstract": "",
      "year": "2017",
      "venue": "ICRA",
      "authors": "S. Gu, E. Holly, T. Lillicrap, and S. Levine"
    },
    {
      "index": 12,
      "title": "World Models",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "D. R. Ha and J. Schmidhuber",
      "orig_title": "World models",
      "paper_id": "1803.10122v4"
    },
    {
      "index": 13,
      "title": "Dealing with sparse rewards in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "J. Hare"
    },
    {
      "index": 14,
      "title": "Dealing with sparse rewards in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "J. Hare"
    },
    {
      "index": 15,
      "title": "Learning Accurate and Human-Like Driving using Semantic Maps and Attention",
      "abstract": "",
      "year": "2020",
      "venue": "IROS",
      "authors": "S. Hecker, D. Dai, A. Liniger, M. Hahner, and L. Van Gool",
      "orig_title": "Learning accurate and human-like driving using semantic maps and attention",
      "paper_id": "2007.07218v1"
    },
    {
      "index": 16,
      "title": "Automatic Goal Generation for Reinforcement Learning Agents",
      "abstract": "",
      "year": "2018",
      "venue": "PMLR",
      "authors": "D. Held, X. Geng, C. Florensa, and P. Abbeel",
      "orig_title": "Automatic goal generation for reinforcement learning agents",
      "paper_id": "1705.06366v5"
    },
    {
      "index": 17,
      "title": "Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Y. Hu, W. Wang, H. Jia, Y. Wang, Y. Chen, J. Hao, F. Wu, and C. Fan",
      "orig_title": "Learning to utilize shaping rewards: A new approach of reward shaping",
      "paper_id": "2011.02669v1"
    },
    {
      "index": 18,
      "title": "End-to-End Race Driving with Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "ICRA",
      "authors": "M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi",
      "orig_title": "End-to-end race driving with deep reinforcement learning",
      "paper_id": "1807.02371v2"
    },
    {
      "index": 19,
      "title": "Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward",
      "abstract": "",
      "year": "2019",
      "venue": "Neurocomputing",
      "authors": "N. Jiang, S. Jin, and C. Zhang"
    },
    {
      "index": 20,
      "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine",
      "orig_title": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation",
      "paper_id": "1806.10293v3"
    },
    {
      "index": 21,
      "title": "Real-time motion planning methods for autonomous on-road driving: State-of-the-art and future research directions",
      "abstract": "",
      "year": "2015",
      "venue": "TRC",
      "authors": "C. Katrakazas, M. Quddus, W. hua Chen, and L. Deka"
    },
    {
      "index": 22,
      "title": "Learning to drive in a day",
      "abstract": "",
      "year": "2019",
      "venue": "ICRA",
      "authors": "A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam, et al."
    },
    {
      "index": 23,
      "title": "How do humans teach: On curriculum learning and teaching dimension",
      "abstract": "",
      "year": "2011",
      "venue": "NeurIPS",
      "authors": "F. Khan, X. Zhu, and B. Mutlu"
    },
    {
      "index": 24,
      "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "T-ITS",
      "authors": "B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Sallab, S. Yogamani, and P. Pérez",
      "orig_title": "Deep reinforcement learning for autonomous driving: A survey",
      "paper_id": "2002.00444v2"
    },
    {
      "index": 25,
      "title": "The influence of reward on the speed of reinforcement learning: An analysis of shaping",
      "abstract": "",
      "year": "2003",
      "venue": "ICML",
      "authors": "A. Laud and G. DeJong"
    },
    {
      "index": 26,
      "title": "Transfer of samples in batch reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "ICML",
      "authors": "A. Lazaric, M. Restelli, and A. Bonarini"
    },
    {
      "index": 27,
      "title": "Urban Driving with Multi-Objective Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "AAMAS",
      "authors": "C. Li and K. Czarnecki",
      "orig_title": "Urban driving with multi-objective deep reinforcement learning",
      "paper_id": "1811.08586v2"
    },
    {
      "index": 28,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra"
    },
    {
      "index": 29,
      "title": "Belief reward shaping in reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "O. Marom and B. Rosman"
    },
    {
      "index": 30,
      "title": "Teacher-Student Curriculum Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Transactions on Neural Networks and Learning Systems",
      "authors": "T. Matiisen, A. Oliver, T. Cohen, and J. Schulman",
      "orig_title": "Teacher–student curriculum learning",
      "paper_id": "1707.00183v2"
    },
    {
      "index": 31,
      "title": "Context and intention aware planning for urban driving",
      "abstract": "",
      "year": "2019",
      "venue": "IROS",
      "authors": "M. Meghjani, Y.-F. Luo, Q. H. Ho, P. Cai, S. Verma, D. Rus, and D. D. Hsu"
    },
    {
      "index": 32,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu"
    },
    {
      "index": 33,
      "title": "Playing atari with deep reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "ArXiv",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller"
    },
    {
      "index": 34,
      "title": "Cuckoo search algorithm for the mobile robot navigation",
      "abstract": "",
      "year": "2013",
      "venue": "SEMCCO",
      "authors": "P. K. Mohanty and D. Parhi"
    },
    {
      "index": 35,
      "title": "Autonomous Highway Driving using Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "SMC",
      "authors": "S. Nageshrao, H. E. Tseng, and D. Filev",
      "orig_title": "Autonomous highway driving using deep reinforcement learning",
      "paper_id": "1904.00035v1"
    },
    {
      "index": 36,
      "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
      "abstract": "",
      "year": "1999",
      "venue": "ICML",
      "authors": "A. Ng, D. Harada, and S. Russell"
    },
    {
      "index": 37,
      "title": "Learning situational driving",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "E. Ohn-Bar, A. Prakash, A. Behl, K. Chitta, and A. Geiger"
    },
    {
      "index": 38,
      "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
      "abstract": "",
      "year": "2017",
      "venue": "BMVC",
      "authors": "X. Pan, Y. You, Z. Wang, and C. Lu",
      "orig_title": "Virtual to real reinforcement learning for autonomous driving",
      "paper_id": "1704.03952v4"
    },
    {
      "index": 39,
      "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
      "abstract": "",
      "year": "2017",
      "venue": "BMVC",
      "authors": "X. Pan, Y. You, Z. Wang, and C. Lu",
      "orig_title": "Virtual to real reinforcement learning for autonomous driving",
      "paper_id": "1704.03952v4"
    },
    {
      "index": 40,
      "title": "Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments",
      "abstract": "",
      "year": "2019",
      "venue": "CoRL",
      "authors": "R. Portelas, C. Colas, K. Hofmann, and P.-Y. Oudeyer",
      "orig_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments",
      "paper_id": "1910.07224v1"
    },
    {
      "index": 41,
      "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "A. Prakash, K. Chitta, and A. Geiger",
      "orig_title": "Multi-modal fusion transformer for end-to-end autonomous driving",
      "paper_id": "2104.09224v1"
    },
    {
      "index": 42,
      "title": "Automated curricula through setter-solver interactions",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "S. Racanière, A. Lampinen, A. Santoro, D. P. Reichert, V. Firoiu, and T. Lillicrap",
      "orig_title": "Automated curricula through setter-solver interactions",
      "paper_id": "1909.12892v2"
    },
    {
      "index": 43,
      "title": "Learning sparse representations in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "J. Rafati and D. C. Noelle",
      "orig_title": "Learning sparse representations in reinforcement learning",
      "paper_id": "1909.01575v1"
    },
    {
      "index": 44,
      "title": "Learning to drive a bicycle using reinforcement learning and shaping",
      "abstract": "",
      "year": "1998",
      "venue": "ICML",
      "authors": "J. Randløv and P. Alstrøm"
    },
    {
      "index": 45,
      "title": "Learning by Playing – Solving Sparse Reward Tasks from Scratch",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg",
      "orig_title": "Learning by playing solving sparse reward tasks from scratch",
      "paper_id": "1802.10567v1"
    },
    {
      "index": 46,
      "title": "Language acquisition in the absence of explicit negative evidence: how important is starting small?",
      "abstract": "",
      "year": "1999",
      "venue": "Cognition",
      "authors": "D. L. T. Rohde and D. Plaut"
    },
    {
      "index": 47,
      "title": "Neural network learning control of robot manipulators using gradually increasing task difficulty",
      "abstract": "",
      "year": "1994",
      "venue": "T-RO",
      "authors": "T. Sanger"
    },
    {
      "index": 48,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 49,
      "title": "Mastering the game of go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, et al."
    },
    {
      "index": 50,
      "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus",
      "orig_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
      "paper_id": "1703.05407v5"
    },
    {
      "index": 51,
      "title": "A Survey of End-to-End Driving: Architectures and Training Methods",
      "abstract": "",
      "year": "2020",
      "venue": "Trans. on Neural Networks and Learning Systems",
      "authors": "A. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad",
      "orig_title": "A survey of end-to-end driving: Architectures and training methods",
      "paper_id": "2003.06404v2"
    },
    {
      "index": 52,
      "title": "Transfer learning for reinforcement learning domains: A survey",
      "abstract": "",
      "year": "2009",
      "venue": "J. Mach. Learn. Res.",
      "authors": "M. E. Taylor and P. Stone"
    },
    {
      "index": 53,
      "title": "Dynamic reward shaping: Training a robot by voice",
      "abstract": "",
      "year": "2010",
      "venue": "IBERAMIA",
      "authors": "A. C. Tenorio-González, E. Morales, and L. Pineda"
    },
    {
      "index": 54,
      "title": "Deep Reinforcement Learning for Autonomous Driving",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "M. Toromanoff, E. Wirbel, and F. Moutarde",
      "orig_title": "Deep reinforcement learning for autonomous driving",
      "paper_id": "1811.11329v3"
    },
    {
      "index": 55,
      "title": "End to End Vehicle Lateral Control Using a Single Fisheye Camera",
      "abstract": "",
      "year": "2018",
      "venue": "IROS",
      "authors": "M. Toromanoff, E. Wirbel, F. Wilhelm, C. Vejarano, X. Perrotton, and F. Moutarde",
      "orig_title": "End to end vehicle lateral control using a single fisheye camera",
      "paper_id": "1808.06940v1"
    },
    {
      "index": 56,
      "title": "Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "A. Trott, S. Zheng, C. Xiong, and R. Socher"
    },
    {
      "index": 57,
      "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, et al.",
      "orig_title": "Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards",
      "paper_id": "1707.08817v2"
    },
    {
      "index": 58,
      "title": "Deep-reinforcement-learning-based autonomous uav navigation with sparse rewards",
      "abstract": "",
      "year": "2020",
      "venue": "Internet of Things Journal",
      "authors": "C. Wang, J. Wang, J. Wang, and X. Zhang"
    },
    {
      "index": 59,
      "title": "Curriculum learning by transfer learning: Theory and experiments with deep networks",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "D. Weinshall, G. Cohen, and D. Amir"
    },
    {
      "index": 60,
      "title": "Learning how to drive in a real world simulation with deep q-networks",
      "abstract": "",
      "year": "2017",
      "venue": "IV",
      "authors": "P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Härtl, F. Dürr, and J. M. Zöllner"
    },
    {
      "index": 61,
      "title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu",
      "orig_title": "The surprising effectiveness of mappo in cooperative, multi-agent games",
      "paper_id": "2103.01955v4"
    }
  ]
}