{
  "paper_id": "2305.18703v7",
  "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
  "abstract": "Abstract.\nLarge language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song",
      "orig_title": "A convergence theory for deep learning via over-parameterization",
      "paper_id": "1811.03962v5"
    },
    {
      "index": 1,
      "title": "Ask Me Anything: A simple strategy for prompting language models",
      "abstract": "",
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "authors": "Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re"
    },
    {
      "index": 2,
      "title": "Attempt: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts",
      "abstract": "",
      "year": "2022",
      "venue": "2022 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Akari Asai, Mohammadreza Salehi, Matthew E Peters, and Hannaneh Hajishirzi",
      "orig_title": "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
      "paper_id": "2205.11961v2"
    },
    {
      "index": 3,
      "title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.16740",
      "authors": "Zhangir Azerbayev, Ansong Ni, Hailey Schoelkopf, and Dragomir Radev",
      "orig_title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation",
      "paper_id": "2211.16740v3"
    },
    {
      "index": 4,
      "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.04023",
      "authors": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.",
      "orig_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "paper_id": "2302.04023v4"
    },
    {
      "index": 5,
      "title": "Simple, Scalable Adaptation for Neural Machine Translation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.08478",
      "authors": "Ankur Bapna, Naveen Arivazhagan, and Orhan Firat",
      "orig_title": "Simple, scalable adaptation for neural machine translation",
      "paper_id": "1909.08478v1"
    },
    {
      "index": 6,
      "title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains",
      "abstract": "",
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Eyal Ben-David, Nadav Oved, and Roi Reichart",
      "orig_title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains",
      "paper_id": "2102.12206v4"
    },
    {
      "index": 7,
      "title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "Rishabh Bhardwaj, Amrita Saha, and Steven C. H. Hoi",
      "orig_title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding",
      "paper_id": "2205.11024v2"
    },
    {
      "index": 8,
      "title": "Potential Use of Chat GPT in Global Warming",
      "abstract": "",
      "year": "2023",
      "venue": "Annals of Biomedical Engineering",
      "authors": "Som S Biswas"
    },
    {
      "index": 9,
      "title": "Improving language models by retrieving from trillions of tokens",
      "abstract": "",
      "year": "2022",
      "venue": "International conference on machine learning",
      "authors": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.",
      "orig_title": "Improving language models by retrieving from trillions of tokens",
      "paper_id": "2112.04426v3"
    },
    {
      "index": 10,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 11,
      "title": "Multi-Head Adapter Routing for Data-Efficient Fine-Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.03831",
      "authors": "Lucas Caccia, Edoardo Ponti, Lucas Liu, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni"
    },
    {
      "index": 12,
      "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.04226",
      "authors": "Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and Lichao Sun",
      "orig_title": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "paper_id": "2303.04226v1"
    },
    {
      "index": 13,
      "title": "Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": "Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang"
    },
    {
      "index": 14,
      "title": "LEGAL-BERT: The Muppets straight out of Law School",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "authors": "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos"
    },
    {
      "index": 15,
      "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.12651",
      "authors": "Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu",
      "orig_title": "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
      "paper_id": "2004.12651v1"
    },
    {
      "index": 16,
      "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.12588",
      "authors": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen",
      "orig_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
      "paper_id": "2211.12588v4"
    },
    {
      "index": 17,
      "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Web Conference 2022",
      "authors": "Xiang Chen, Ningyu Zhang, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen"
    },
    {
      "index": 18,
      "title": "Binding Language Models in Symbolic Languages",
      "abstract": "",
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "authors": "Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu",
      "orig_title": "Binding Language Models in Symbolic Languages",
      "paper_id": "2210.02875v2"
    },
    {
      "index": 19,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.02311",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 20,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.02311\n(",
      "authors": "Aakanksha Chowdhery,\nSharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham,\nHyung¬†Won Chung, Charles Sutton,\nSebastian Gehrmann, et¬†al.\n2022.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 21,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing\nsystems 30 (",
      "authors": "Paul¬†F Christiano, Jan\nLeike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei.\n2017.",
      "orig_title": "Deep reinforcement learning from human preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 22,
      "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.07027\n(",
      "authors": "Alexandra Chronopoulou,\nMatthew¬†E Peters, Alexander Fraser, and\nJesse Dodge. 2023.",
      "orig_title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
      "paper_id": "2302.07027v3"
    },
    {
      "index": 23,
      "title": "Scaling Instruction-Finetuned Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.11416\n(",
      "authors": "Hyung¬†Won Chung, Le Hou,\nShayne Longpre, Barret Zoph,\nYi Tay, William Fedus,\nEric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma,\net¬†al. 2022.",
      "orig_title": "Scaling instruction-finetuned language models",
      "paper_id": "2210.11416v5"
    },
    {
      "index": 24,
      "title": "A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2306.04802\n(",
      "authors": "Hejie Cui, Jiaying Lu,\nShiyu Wang, Ran Xu,\nWenjing Ma, Shaojun Yu,\nYue Yu, Xuan Kan, Chen\nLing, Joyce Ho, et¬†al.\n2023."
    },
    {
      "index": 25,
      "title": "Knowledge Neurons in Pretrained Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.08696\n(",
      "authors": "Damai Dai, Li Dong,\nYaru Hao, Zhifang Sui,\nBaobao Chang, and Furu Wei.\n2021.",
      "orig_title": "Knowledge neurons in pretrained transformers",
      "paper_id": "2104.08696v2"
    },
    {
      "index": 26,
      "title": "Promptagator: Few-shot dense retrieval from 8 examples",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.11755\n(",
      "authors": "Zhuyun Dai, Vincent¬†Y\nZhao, Ji Ma, Yi Luan,\nJianmo Ni, Jing Lu,\nAnton Bakalov, Kelvin Guu,\nKeith¬†B Hall, and Ming-Wei Chang.\n2022."
    },
    {
      "index": 27,
      "title": "Christine Kaeser-Chen",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ishita Dasgupta, Christine\nKaeser-Chen, Kenneth Marino, Arun Ahuja,\nSheila Babayan, Felix Hill, and\nRob Fergus. 2023."
    },
    {
      "index": 28,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Nicola De¬†Cao, Wilker\nAziz, and Ivan Titov. 2021.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 29,
      "title": "Cheng-Ping Hsieh",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Mingkai Deng, Jianyu\nWang, Cheng-Ping Hsieh, Yihan Wang,\nHan Guo, Tianmin Shu,\nMeng Song, Eric¬†P. Xing, and\nZhiting Hu. 2022."
    },
    {
      "index": 30,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805\n(",
      "authors": "Jacob Devlin, Ming-Wei\nChang, Kenton Lee, and Kristina\nToutanova. 2018.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 31,
      "title": "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv (",
      "authors": "Ning Ding, Yujia Qin,\nGuang Yang, Fuchao Wei,\nZonghan Yang, Yusheng Su,\nShengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen,\net¬†al. 2022."
    },
    {
      "index": 32,
      "title": "Nathanael Sch√§rli",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Andrew Drozdov, Nathanael\nSch√§rli, Ekin Aky√ºrek, Nathan\nScales, Xinying Song, Xinyun Chen,\nOlivier Bousquet, and Denny Zhou.\n2023."
    },
    {
      "index": 33,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Dheeru Dua, Shivanshu\nGupta, Sameer Singh, and Matt\nGardner. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 34,
      "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.10650\n(",
      "authors": "Ali Edalati, Marzieh\nTahaei, Ivan Kobyzev, Vahid¬†Partovi Nia,\nJames¬†J Clark, and Mehdi\nRezagholizadeh. 2022.",
      "orig_title": "KronA: Parameter Efficient Tuning with Kronecker Adapter",
      "paper_id": "2212.10650v1"
    },
    {
      "index": 35,
      "title": "P{{\\{\\\\\\backslashO}}\\}DA: Prompt-driven Zero-shot Domain Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.03241\n(",
      "authors": "Mohammad Fahes, Tuan-Hung\nVu, Andrei Bursuc, Patrick P√©rez,\nand Raoul de Charette. 2022."
    },
    {
      "index": 36,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Yang Feng, Shiyue Zhang,\nAndi Zhang, Dong Wang, and\nAndrew Abel. 2017.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 37,
      "title": "PAL: Program-aided Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.10435\n(",
      "authors": "Luyu Gao, Aman Madaan,\nShuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang,\nJamie Callan, and Graham Neubig.\n2022."
    },
    {
      "index": 38,
      "title": "Domain Adaptation via Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.06687\n(",
      "authors": "Chunjiang Ge, Rui Huang,\nMixue Xie, Zihang Lai,\nShiji Song, Shuang Li, and\nGao Huang. 2022.",
      "orig_title": "Domain Adaptation via Prompt Learning",
      "paper_id": "2202.06687v1"
    },
    {
      "index": 39,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Edouard Grave, Armand\nJoulin, and Nicolas Usunier.\n2017.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 40,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yuxian Gu, Xu Han,\nZhiyuan Liu, and Minlie Huang.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 41,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Xu Guo, Boyang Li, and\nHan Yu. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 42,
      "title": "On the Domain Adaptation and Generalization of Pretrained Language Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv (",
      "authors": "Xu Guo and Han Yu.\n2022.",
      "orig_title": "On the Domain Adaptation and Generalization of Pretrained Language Models: A Survey",
      "paper_id": "2211.03154v1"
    },
    {
      "index": 43,
      "title": "Hrant Khachatrian",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Karen Hambardzumyan, Hrant\nKhachatrian, and Jonathan May.\n2021."
    },
    {
      "index": 44,
      "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2301.00303\n(",
      "authors": "Hangfeng He, Hongming\nZhang, and Dan Roth. 2022b."
    },
    {
      "index": 45,
      "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.04366\n(",
      "authors": "Junxian He, Chunting\nZhou, Xuezhe Ma, Taylor\nBerg-Kirkpatrick, and Graham Neubig.\n2021.",
      "orig_title": "Towards a unified view of parameter-efficient transfer learning",
      "paper_id": "2110.04366v3"
    },
    {
      "index": 46,
      "title": "ùíÆparseùíúdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.04284\n(",
      "authors": "Shwai He, Liang Ding,\nDaize Dong, Miao Zhang, and\nDacheng Tao. 2022a.",
      "orig_title": "Sparseadapter: An easy approach for improving the parameter-efficiency of adapters",
      "paper_id": "2210.04284v5"
    },
    {
      "index": 47,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.08415\n(",
      "authors": "Dan Hendrycks and Kevin\nGimpel. 2016.",
      "orig_title": "Gaussian error linear units (gelus)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 48,
      "title": "Measuring and Manipulating Knowledge Representations in Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.00740\n(",
      "authors": "Evan Hernandez, Belinda¬†Z\nLi, and Jacob Andreas. 2023."
    },
    {
      "index": 49,
      "title": "Stanislaw Jastrzebski",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Neil Houlsby, Andrei\nGiurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De¬†Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain\nGelly. 2019."
    },
    {
      "index": 50,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.06146\n(",
      "authors": "Jeremy Howard and\nSebastian Ruder. 2018.",
      "orig_title": "Universal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 51,
      "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.02301\n(",
      "authors": "Cheng-Yu Hsieh, Chun-Liang\nLi, Chih-Kuan Yeh, Hootan Nakhost,\nYasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and\nTomas Pfister. 2023.",
      "orig_title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
      "paper_id": "2305.02301v2"
    },
    {
      "index": 52,
      "title": "Lora: Low-rank adaptation of large language models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.09685\n(",
      "authors": "Edward¬†J Hu, Yelong Shen,\nPhillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen.\n2021."
    },
    {
      "index": 53,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Shengding Hu, Ning Ding,\nHuadong Wang, Zhiyuan Liu,\nJingang Wang, Juanzi Li,\nWei Wu, and Maosong Sun.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 54,
      "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.01933\n(",
      "authors": "Zhiqiang Hu, Yihuai Lan,\nLei Wang, Wanyu Xu,\nEe-Peng Lim, Roy Ka-Wei Lee,\nLidong Bing, and Soujanya Poria.\n2023.",
      "orig_title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
      "paper_id": "2304.01933v3"
    },
    {
      "index": 55,
      "title": "Large Language Models Can Self-Improve",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.11610\n(",
      "authors": "Jiaxin Huang,\nShixiang¬†Shane Gu, Le Hou,\nYuexin Wu, Xuezhi Wang,\nHongkun Yu, and Jiawei Han.\n2022.",
      "orig_title": "Large language models can self-improve",
      "paper_id": "2210.11610v2"
    },
    {
      "index": 56,
      "title": "Language Is Not All You Need: Aligning Perception with Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.14045\n(",
      "authors": "Shaohan Huang, Li Dong,\nWenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma,\nTengchao Lv, Lei Cui,\nOwais¬†Khan Mohammed, Qiang Liu,\net¬†al. 2023.",
      "orig_title": "Language is not all you need: Aligning perception with language models",
      "paper_id": "2302.14045v2"
    },
    {
      "index": 57,
      "title": "Few-shot learning with retrieval augmented language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.03299\n(",
      "authors": "Gautier Izacard, Patrick\nLewis, Maria Lomeli, Lucas Hosseini,\nFabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin,\nSebastian Riedel, and Edouard Grave.\n2022."
    },
    {
      "index": 58,
      "title": "ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.14882\n(",
      "authors": "Katharina Jeblick,\nBalthasar Schachtner, Jakob Dexl,\nAndreas Mittermeier, Anna¬†Theresa\nSt√ºber, Johanna Topalis, Tobias\nWeber, Philipp Wesp, Bastian Sabel,\nJens Ricke, et¬†al.\n2022."
    },
    {
      "index": 59,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "Comput. Surveys 55,\n12 (",
      "authors": "Ziwei Ji, Nayeon Lee,\nRita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko\nIshii, Ye¬†Jin Bang, Andrea Madotto,\nand Pascale Fung. 2023.",
      "orig_title": "Survey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 60,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Chen Jia and Yue\nZhang. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 61,
      "title": "Timothee Lacroix",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Albert¬†Qiaochu Jiang, Sean\nWelleck, Jin¬†Peng Zhou, Timothee\nLacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and\nYuhuai Wu. 2023."
    },
    {
      "index": 62,
      "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.03437\n(",
      "authors": "Haoming Jiang, Pengcheng\nHe, Weizhu Chen, Xiaodong Liu,\nJianfeng Gao, and Tuo Zhao.\n2019.",
      "orig_title": "Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization",
      "paper_id": "1911.03437v5"
    },
    {
      "index": 63,
      "title": "Instance-aware Prompt Learning for Language Understanding and Generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.07126\n(",
      "authors": "Feihu Jin, Jinliang Lu,\nJiajun Zhang, and Chengqing Zong.\n2022.",
      "orig_title": "Instance-aware prompt learning for language understanding and generation",
      "paper_id": "2201.07126v1"
    },
    {
      "index": 64,
      "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
      "abstract": "",
      "year": "",
      "venue": "ArXiv ([n.‚Äâd.])",
      "authors": "Qiao Jin, Yifan Yang,\nQingyu Chen, and Zhiyong Lu.\n[n.‚Äâd.].",
      "orig_title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
      "paper_id": "2304.09667v3"
    },
    {
      "index": 65,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv (",
      "authors": "Jared Kaplan, Sam\nMcCandlish, Tom Henighan, Tom¬†B Brown,\nBenjamin Chess, Rewon Child,\nScott Gray, Alec Radford,\nJeffrey Wu, and Dario Amodei.\n2020.",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 66,
      "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34 (2021)",
      "authors": "Rabeeh Karimi¬†Mahabadi,\nJames Henderson, and Sebastian Ruder.\n2021.",
      "orig_title": "Compacter: Efficient low-rank hypercomplex adapter layers",
      "paper_id": "2106.04647v2"
    },
    {
      "index": 67,
      "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.02406\n(",
      "authors": "Tushar Khot, Harsh\nTrivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and\nAshish Sabharwal. 2022.",
      "orig_title": "Decomposed prompting: A modular approach for solving complex tasks",
      "paper_id": "2210.02406v2"
    },
    {
      "index": 68,
      "title": "Shixiang Shane Gu",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Takeshi Kojima,\nShixiang¬†Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa.\n2022."
    },
    {
      "index": 69,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Mojtaba Komeili, Kurt\nShuster, and Jason Weston.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 70,
      "title": "Do We Still Need Clinical Language Models?",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.08091\n(",
      "authors": "Eric Lehman, Evan\nHernandez, Diwakar Mahajan, Jonas Wulff,\nMicah¬†J Smith, Zachary Ziegler,\nDaniel Nadler, Peter Szolovits,\nAlistair Johnson, and Emily Alsentzer.\n2023.",
      "orig_title": "Do We Still Need Clinical Language Models?",
      "paper_id": "2302.08091v1"
    },
    {
      "index": 71,
      "title": "Sentiment Spin: Attacking Financial Sentiment with GPT-3",
      "abstract": "",
      "year": "2023",
      "venue": "Available at SSRN (",
      "authors": "Markus Leippold.\n2023."
    },
    {
      "index": 72,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Brian Lester, Rami\nAl-Rfou, and Noah Constant.\n2021.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 73,
      "title": "Standing on the Shoulders of Giant Frozen Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv abs/2204.10019\n(",
      "authors": "Yoav Levine, Itay\nDalmedigos, Ori Ram, Yoel Zeldes,\nDaniel Jannai, Dor Muhlgay,\nYoni Osin, Opher Lieber,\nBarak Lenz, Shai Shalev-Shwartz,\nAmnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. 2022.",
      "orig_title": "Standing on the Shoulders of Giant Frozen Language Models",
      "paper_id": "2204.10019v1"
    },
    {
      "index": 74,
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing\nSystems 33 (2020)",
      "authors": "Patrick Lewis, Ethan\nPerez, Aleksandra Piktus, Fabio Petroni,\nVladimir Karpukhin, Naman Goyal,\nHeinrich K√ºttler, Mike Lewis,\nWen-tau Yih, Tim Rockt√§schel,\net¬†al. 2020.",
      "orig_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "paper_id": "2005.11401v4"
    },
    {
      "index": 75,
      "title": "Large Language Models with Controllable Working Memory",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.05110\n(",
      "authors": "Daliang Li, Ankit¬†Singh\nRawat, Manzil Zaheer, Xin Wang,\nMichal Lukasik, Andreas Veit,\nFelix Yu, and Sanjiv Kumar.\n2022b.",
      "orig_title": "Large Language Models with Controllable Working Memory",
      "paper_id": "2211.05110v1"
    },
    {
      "index": 76,
      "title": "CAMEL: Communicative Agents for‚Äù Mind‚Äù Exploration of Large Scale Language Model Society",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.17760\n(",
      "authors": "Guohao Li, Hasan Abed\nAl¬†Kader Hammoud, Hani Itani, Dmitrii\nKhizbullin, and Bernard Ghanem.\n2023a."
    },
    {
      "index": 77,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Haochen Li, Tong Mo,\nHongcheng Fan, Jingkun Wang,\nJiaxi Wang, Fuhao Zhang, and\nWeiping Li. 2022a.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 78,
      "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.03111\n(",
      "authors": "Jinyang Li, Binyuan Hui,\nGe Qu, Binhua Li, Jiaxi\nYang, Bowen Li, Bailin Wang,\nBowen Qin, Rongyu Cao,\nRuiying Geng, et¬†al.\n2023b.",
      "orig_title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
      "paper_id": "2305.03111v3"
    },
    {
      "index": 79,
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers)\nabs/",
      "authors": "Xiang¬†Lisa Li and Percy\nLiang. 2021.",
      "orig_title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "paper_id": "2101.00190v1"
    },
    {
      "index": 80,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jacky Liang, Wenlong\nHuang, Fei Xia, Peng Xu,\nKarol Hausman, Pete Florence,\nAndy Zeng, et¬†al.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 81,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.16434\n(",
      "authors": "Yaobo Liang, Chenfei Wu,\nTing Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang\nOu, Shuai Lu, Lei Ji,\nShaoguang Mao, et¬†al.\n2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 82,
      "title": "Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.01117\n(",
      "authors": "Hongzhan Lin, Pengyao Yi,\nJing Ma, Haiyun Jiang,\nZiyang Luo, Shuming Shi, and\nRuifang Liu. 2022."
    },
    {
      "index": 83,
      "title": "Relational Memory Augmented Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Transactions of the Association for\nComputational Linguistics 10 (2022),\n555‚Äì572",
      "authors": "Qi Liu, Dani Yogatama,\nand Phil Blunsom. 2022c.",
      "orig_title": "Relational Memory-Augmented Language Models",
      "paper_id": "2201.09680v1"
    },
    {
      "index": 84,
      "title": "Mind‚Äôs Eye: Grounded Language Model Reasoning through Simulation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.05359\n(",
      "authors": "Ruibo Liu, Jason Wei,\nShixiang¬†Shane Gu, Te-Yen Wu,\nSoroush Vosoughi, Claire Cui,\nDenny Zhou, and Andrew¬†M Dai.\n2022b.",
      "orig_title": "Mind‚Äôs Eye: Grounded Language Model Reasoning through Simulation",
      "paper_id": "2210.05359v1"
    },
    {
      "index": 85,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Xiangyang Liu, Tianxiang\nSun, Xuanjing Huang, and Xipeng Qiu.\n2022a.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 86,
      "title": "Summary of chatgpt/gpt-4 research and perspective towards the future of large language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.01852\n(",
      "authors": "Yiheng Liu, Tianle Han,\nSiyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian,\nHao He, Antong Li,\nMengshen He, Zhengliang Liu,\net¬†al. 2023."
    },
    {
      "index": 87,
      "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.07619\n(",
      "authors": "Alejandro Lopez-Lira and\nYuehua Tang. 2023."
    },
    {
      "index": 88,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jiaying Lu, Jiaming Shen,\nBo Xiong, Wengjing Ma,\nStaab Steffen, and Carl Yang.\n2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 89,
      "title": "Sebastian Riedel",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yao Lu, Max Bartolo,\nAlastair Moore, Sebastian Riedel, and\nPontus Stenetorp. 2022."
    },
    {
      "index": 90,
      "title": "Language Models of Code are Few-Shot Commonsense Learners",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.07128\n(",
      "authors": "Aman Madaan, Shuyan Zhou,\nUri Alon, Yiming Yang, and\nGraham Neubig. 2022.",
      "orig_title": "Language models of code are few-shot commonsense learners",
      "paper_id": "2210.07128v3"
    },
    {
      "index": 91,
      "title": "Designing Chemical Reaction Arrays using phactor and ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Babak Mahjour, Jillian\nHoffstadt, and Tim Cernak.\n2023."
    },
    {
      "index": 92,
      "title": "UDApter - Efficient Domain Adaptation Using Adapters",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.03194\n(",
      "authors": "Bhavitvya Malik,\nAbhinav¬†Ramesh Kashyap, Min-Yen Kan,\nand Soujanya Poria. 2023.",
      "orig_title": "UDApter‚ÄìEfficient Domain Adaptation Using Adapters",
      "paper_id": "2302.03194v2"
    },
    {
      "index": 93,
      "title": "Unipelt: A unified framework for parameter-efficient language model tuning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.07577\n(",
      "authors": "Yuning Mao, Lambert\nMathias, Rui Hou, Amjad Almahairi,\nHao Ma, Jiawei Han,\nWen-tau Yih, and Madian Khabsa.\n2021."
    },
    {
      "index": 94,
      "title": "What Language Reveals about Perception: Distilling Psychophysical Knowledge from Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.01308\n(",
      "authors": "Raja Marjieh, Ilia\nSucholutsky, Pol van Rijn, Nori Jacoby,\nand Thomas¬†L Griffiths. 2023."
    },
    {
      "index": 95,
      "title": "Locating and Editing Factual Associations in GPT",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing\nSystems 35 (2022)",
      "authors": "Kevin Meng, David Bau,\nAlex Andonian, and Yonatan Belinkov.\n2022a.",
      "orig_title": "Locating and editing factual associations in gpt",
      "paper_id": "2202.05262v5"
    },
    {
      "index": 96,
      "title": "Mass-Editing Memory in a Transformer",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.07229\n(",
      "authors": "Kevin Meng, Arnab¬†Sen\nSharma, Alex Andonian, Yonatan Belinkov,\nand David Bau. 2022b.",
      "orig_title": "Mass-editing memory in a transformer",
      "paper_id": "2210.07229v2"
    },
    {
      "index": 97,
      "title": "Teaching language models to support answers with verified quotes",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11147\n(",
      "authors": "Jacob Menick, Maja\nTrebacz, Vladimir Mikulik, John\nAslanides, Francis Song, Martin\nChadwick, Mia Glaese, Susannah Young,\nLucy Campbell-Gillingham, Geoffrey\nIrving, et¬†al. 2022a.",
      "orig_title": "Teaching language models to support answers with verified quotes",
      "paper_id": "2203.11147v1"
    },
    {
      "index": 98,
      "title": "Teaching language models to support answers with verified quotes",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.11147\n(",
      "authors": "Jacob Menick, Maja\nTrebacz, Vladimir Mikulik, John\nAslanides, Francis Song, Martin\nChadwick, Mia Glaese, Susannah Young,\nLucy Campbell-Gillingham, Geoffrey\nIrving, et¬†al. 2022b.",
      "orig_title": "Teaching language models to support answers with verified quotes",
      "paper_id": "2203.11147v1"
    },
    {
      "index": 99,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Stephen Merity, Caiming\nXiong, James Bradbury, and Richard\nSocher. 2016.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 100,
      "title": "Augmented Language Models: a Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.07842\n(",
      "authors": "Gr√©goire Mialon,\nRoberto Dess√¨, Maria Lomeli,\nChristoforos Nalmpantis, Ram Pasunuru,\nRoberta Raileanu, Baptiste Rozi√®re,\nTimo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et¬†al.\n2023.",
      "orig_title": "Augmented language models: a survey",
      "paper_id": "2302.07842v1"
    },
    {
      "index": 101,
      "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv (",
      "authors": "Bonan Min, Hayley Ross,\nElior Sulem, Amir Pouran¬†Ben Veyseh,\nThien¬†Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and\nDan Roth. 2021.",
      "orig_title": "Recent advances in natural language processing via large pre-trained language models: A survey",
      "paper_id": "2111.01243v1"
    },
    {
      "index": 102,
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.12837\n(",
      "authors": "Sewon Min, Xinxi Lyu,\nAri Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022."
    },
    {
      "index": 103,
      "title": "Fast model editing at scale",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.11309\n(",
      "authors": "Eric Mitchell, Charles\nLin, Antoine Bosselut, Chelsea Finn,\nand Christopher¬†D Manning.\n2021."
    },
    {
      "index": 104,
      "title": "Antoine Bosselut",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Eric Mitchell, Charles\nLin, Antoine Bosselut, Christopher¬†D\nManning, and Chelsea Finn.\n2022."
    },
    {
      "index": 105,
      "title": "ChatGPT: Can artificial intelligence language models be of value for cardiovascular nurses and allied health professionals",
      "abstract": "",
      "year": "2023",
      "venue": "European Journal of Cardiovascular Nursing\n(",
      "authors": "Philip Moons and Liesbet\nVan¬†Bulck. 2023."
    },
    {
      "index": 106,
      "title": "Santosh Rajagopalan",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Aashiq Muhamed, Iman\nKeivanloo, Sujan Perera, James Mracek,\nYi Xu, Qingjun Cui,\nSantosh Rajagopalan, Belinda Zeng, and\nTrishul Chilimbi. 2021."
    },
    {
      "index": 107,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Prasanth Murali, Ian\nSteenstra, Hye¬†Sun Yun, Ameneh Shamekhi,\nand Timothy Bickmore. 2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 108,
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2112.09332\n(",
      "authors": "Reiichiro Nakano, Jacob\nHilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim,\nChristopher Hesse, Shantanu Jain,\nVineet Kosaraju, William Saunders,\net¬†al. 2021.",
      "orig_title": "Webgpt: Browser-assisted question-answering with human feedback",
      "paper_id": "2112.09332v3"
    },
    {
      "index": 109,
      "title": "ChatGPT plugins",
      "abstract": "",
      "year": "2023",
      "venue": "Accessed:",
      "authors": "OpenAI.\n[n.‚Äâd.]."
    },
    {
      "index": 110,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:",
      "authors": "OpenAI. 2023.",
      "orig_title": "GPT-4 Technical Report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 111,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing\nSystems 35 (2022)",
      "authors": "Long Ouyang, Jeffrey Wu,\nXu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal,\nKatarina Slama, Alex Ray,\net¬†al. 2022."
    },
    {
      "index": 112,
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.03442\n(",
      "authors": "Joon¬†Sung Park, Joseph¬†C\nO‚ÄôBrien, Carrie¬†J Cai, Meredith¬†Ringel\nMorris, Percy Liang, and Michael¬†S\nBernstein. 2023.",
      "orig_title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "paper_id": "2304.03442v2"
    },
    {
      "index": 113,
      "title": "Instruction Tuning with GPT-4",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.03277\n(",
      "authors": "Baolin Peng, Chunyuan Li,\nPengcheng He, Michel Galley, and\nJianfeng Gao. 2023.",
      "orig_title": "Instruction tuning with gpt-4",
      "paper_id": "2304.03277v1"
    },
    {
      "index": 114,
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00247\n(",
      "authors": "Jonas Pfeiffer, Aishwarya\nKamath, Andreas R√ºckl√©, Kyunghyun\nCho, and Iryna Gurevych.\n2020a.",
      "orig_title": "AdapterFusion: Non-destructive task composition for transfer learning",
      "paper_id": "2005.00247v3"
    },
    {
      "index": 115,
      "title": "AdapterHub: A Framework for Adapting Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.07779\n(",
      "authors": "Jonas Pfeiffer, Andreas\nR√ºckl√©, Clifton Poth, Aishwarya\nKamath, Ivan Vuliƒá, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych.\n2020b.",
      "orig_title": "Adapterhub: A framework for adapting transformers",
      "paper_id": "2007.07779v3"
    },
    {
      "index": 116,
      "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00052\n(",
      "authors": "Jonas Pfeiffer, Ivan\nVuliƒá, Iryna Gurevych, and\nSebastian Ruder. 2020c.",
      "orig_title": "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
      "paper_id": "2005.00052v3"
    },
    {
      "index": 117,
      "title": "Combining Modular Skills in Multitask Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.13914\n(",
      "authors": "Edoardo¬†M Ponti,\nAlessandro Sordoni, and Siva Reddy.\n2022.",
      "orig_title": "Combining modular skills in multitask learning",
      "paper_id": "2202.13914v2"
    },
    {
      "index": 118,
      "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.11015\n(",
      "authors": "Mohammadreza Pourreza and\nDavood Rafiei. 2023.",
      "orig_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
      "paper_id": "2304.11015v3"
    },
    {
      "index": 119,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Archiki Prasad, Peter\nHase, Xiang Zhou, and Mohit Bansal.\n2022b.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 120,
      "title": "Mohand Boughanem",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Nishchal Prasad, Mohand\nBoughanem, and Taoufiq Dkaki.\n2022a."
    },
    {
      "index": 121,
      "title": "LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2110.07298\n(",
      "authors": "Chengwei Qin and Shafiq\nJoty. 2021."
    },
    {
      "index": 122,
      "title": "Tool Learning with Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.08354\n(",
      "authors": "Yujia Qin, Shengding Hu,\nYankai Lin, Weize Chen,\nNing Ding, Ganqu Cui,\nZheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, et¬†al.\n2023.",
      "orig_title": "Tool Learning with Foundation Models",
      "paper_id": "2304.08354v3"
    },
    {
      "index": 123,
      "title": "Pre-trained Models for Natural Language Processing: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "Science China Technological Sciences\n63, 10 (2020)",
      "authors": "Xipeng Qiu, Tianxiang\nSun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang.\n2020.",
      "orig_title": "Pre-trained models for natural language processing: A survey",
      "paper_id": "2003.08271v4"
    },
    {
      "index": 124,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Alec Radford, Karthik\nNarasimhan, Tim Salimans, Ilya\nSutskever, et¬†al. 2018."
    },
    {
      "index": 125,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "The Journal of Machine Learning Research\n21, 1 (2020)",
      "authors": "Colin Raffel, Noam\nShazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and\nPeter¬†J Liu. 2020.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 126,
      "title": "In-Context Retrieval-Augmented Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.00083\n(",
      "authors": "Ori Ram, Yoav Levine,\nItay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. 2023.",
      "orig_title": "In-Context Retrieval-Augmented Language Models",
      "paper_id": "2302.00083v3"
    },
    {
      "index": 127,
      "title": "Evaluating ChatGPT as an adjunct for radiologic decision-making",
      "abstract": "",
      "year": "2023",
      "venue": "medRxiv (2023)",
      "authors": "Arya Rao, John Kim,\nMeghana Kamineni, Michael Pang,\nWinston Lie, and Marc¬†D Succi.\n2023."
    },
    {
      "index": 128,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Anastasia Razdaibiedina,\nYuning Mao, Rui Hou,\nMadian Khabsa, Mike Lewis, and\nAmjad Almahairi. 2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 129,
      "title": "Learning multiple visual domains with residual adapters",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing\nsystems 30 (",
      "authors": "Sylvestre-Alvise Rebuffi,\nHakan Bilen, and Andrea Vedaldi.\n2017."
    },
    {
      "index": 130,
      "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.08910\n(",
      "authors": "Adam Roberts, Colin\nRaffel, and Noam Shazeer.\n2020.",
      "orig_title": "How much knowledge can you pack into the parameters of a language model?",
      "paper_id": "2002.08910v4"
    },
    {
      "index": 131,
      "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv (",
      "authors": "Joshua Robinson,\nChristopher¬†Michael Rytting, and David\nWingate. 2022.",
      "orig_title": "Leveraging Large Language Models for Multiple Choice Question Answering",
      "paper_id": "2210.12353v3"
    },
    {
      "index": 132,
      "title": "Large-scale chemical language representations capture molecular structure and properties",
      "abstract": "",
      "year": "2022",
      "venue": "Nature Machine Intelligence\n4, 12 (2022)",
      "authors": "Jerret Ross, Brian\nBelgodere, Vijil Chenthamarakshan, Inkit\nPadhi, Youssef Mroueh, and Payel Das.\n2022."
    },
    {
      "index": 133,
      "title": "The utility of ChatGPT as an example of large language models in healthcare education, research and practice: Systematic review on the future perspectives and potential limitations",
      "abstract": "",
      "year": "2023",
      "venue": "medRxiv (2023)",
      "authors": "Malik Sallam.\n2023."
    },
    {
      "index": 134,
      "title": "Lintang Sutawika",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.04761\n(",
      "authors": "Timo Schick, Jane\nDwivedi-Yu, Roberto Dess√¨, Roberta\nRaileanu, Maria Lomeli, Luke\nZettlemoyer, Nicola Cancedda, and\nThomas Scialom. 2023.",
      "orig_title": "Toolformer: Language models can teach themselves to use tools",
      "paper_id": "2302.04761v1"
    },
    {
      "index": 136,
      "title": "Exploiting cloze questions for few shot text classification and natural language inference",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.07676\n(",
      "authors": "Timo Schick and Hinrich\nSch√ºtze. 2020."
    },
    {
      "index": 137,
      "title": "Memory Augmented Large Language Models are Computationally Universal",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.04589\n(",
      "authors": "Dale Schuurmans.\n2023.",
      "orig_title": "Memory Augmented Large Language Models are Computationally Universal",
      "paper_id": "2301.04589v1"
    },
    {
      "index": 138,
      "title": "Tuhin Chakrabarty",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Thomas Scialom, Tuhin\nChakrabarty, and Smaranda Muresan.\n2022."
    },
    {
      "index": 139,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Shohreh Shaghaghian,\nLuna¬†Yue Feng, Borna Jafarpour, and\nNicolai Pogrebnyakov. 2020.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 140,
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv:",
      "authors": "Yongliang Shen, Kaitao\nSong, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang.\n2023.",
      "orig_title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace",
      "paper_id": "2303.17580v4"
    },
    {
      "index": 141,
      "title": "Continual Learning with Deep Generative Replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing\nsystems 30 (",
      "authors": "Hanul Shin, Jung¬†Kwon\nLee, Jaehong Kim, and Jiwon Kim.\n2017.",
      "orig_title": "Continual learning with deep generative replay",
      "paper_id": "1705.08690v3"
    },
    {
      "index": 142,
      "title": "Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.00193\n(",
      "authors": "Kumar Shridhar, Alessandro\nStolfo, and Mrinmaya Sachan.\n2022."
    },
    {
      "index": 143,
      "title": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing\nSystems 34 (2021)",
      "authors": "Devendra Singh, Siva\nReddy, Will Hamilton, Chris Dyer, and\nDani Yogatama. 2021.",
      "orig_title": "End-to-end training of multi-document reader and retriever for open-domain question answering",
      "paper_id": "2106.05346v2"
    },
    {
      "index": 144,
      "title": "Arsalan Mousavian",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Ishika Singh, Valts\nBlukis, Arsalan Mousavian, Ankit Goyal,\nDanfei Xu, Jonathan Tremblay,\nDieter Fox, Jesse Thomason, and\nAnimesh Garg. [n.‚Äâd.]."
    },
    {
      "index": 145,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yusheng Su, Xiaozhi Wang,\nYujia Qin, Chi-Min Chan,\nYankai Lin, Huadong Wang,\nKaiyue Wen, Zhiyuan Liu,\nPeng Li, Juanzi Li, et¬†al.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 146,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Tianxiang Sun, Yunfan\nShao, Hong Qian, Xuanjing Huang, and\nXipeng Qiu. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 147,
      "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2206.06522\n(",
      "authors": "Yi-Lin Sung, Jaemin Cho,\nand Mohit Bansal. 2022.",
      "orig_title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning",
      "paper_id": "2206.06522v2"
    },
    {
      "index": 148,
      "title": "Vipergpt: Visual inference via python execution for reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.08128\n(",
      "authors": "D√≠dac Sur√≠s,\nSachit Menon, and Carl Vondrick.\n2023."
    },
    {
      "index": 149,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.13971\n(",
      "authors": "Hugo Touvron, Thibaut\nLavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux,\nTimoth√©e Lacroix, Baptiste\nRozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et¬†al.\n2023.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 150,
      "title": "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.07558\n(",
      "authors": "Mojtaba Valipour, Mehdi\nRezagholizadeh, Ivan Kobyzev, and Ali\nGhodsi. 2022."
    },
    {
      "index": 151,
      "title": "On the Role of Negative Precedent in Legal Outcome Prediction",
      "abstract": "",
      "year": "2023",
      "venue": "Transactions of the Association for\nComputational Linguistics 11 (2023),\n34‚Äì48",
      "authors": "Josef Valvoda, Ryan\nCotterell, and Simone Teufel.\n2023.",
      "orig_title": "On the Role of Negative Precedent in Legal Outcome Prediction",
      "paper_id": "2208.08225v2"
    },
    {
      "index": 152,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing\nsystems 30 (",
      "authors": "Ashish Vaswani, Noam\nShazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan¬†N Gomez,\n≈Åukasz Kaiser, and Illia\nPolosukhin. 2017.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 153,
      "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv abs/2110.07904\n(",
      "authors": "Tu Vu, Brian Lester,\nNoah Constant, Rami Al-Rfou, and\nDaniel¬†Matthew Cer. 2021.",
      "orig_title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
      "paper_id": "2110.07904v2"
    },
    {
      "index": 154,
      "title": "Efficient Fine-Tuning of Compressed Language Models with Learners",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2208.02070\n(",
      "authors": "Danilo Vucetic,\nMohammadreza Tayaranian, Maryam\nZiaeefard, James¬†J Clark, Brett¬†H Meyer,\nand Warren¬†J Gross. 2022.",
      "orig_title": "Efficient Fine-Tuning of Compressed Language Models with Learners",
      "paper_id": "2208.02070v1"
    },
    {
      "index": 155,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhongwei Wan, Yichun Yin,\nWei Zhang, Jiaxin Shi,\nLifeng Shang, Guangyong Chen,\nXin Jiang, and Qun Liu.\n2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 156,
      "title": "Document-Level Machine Translation with Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.02210\n(",
      "authors": "Longyue Wang, Chenyang\nLyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and\nZhaopeng Tu. 2023b.",
      "orig_title": "Document-level machine translation with large language models",
      "paper_id": "2304.02210v2"
    },
    {
      "index": 157,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Ruijie Wang, Zheng Li,\nDachun Sun, Shengzhong Liu,\nJinning Li, Bing Yin, and\nTarek Abdelzaher. 2022b.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 158,
      "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.01808\n(",
      "authors": "Ruize Wang, Duyu Tang,\nNan Duan, Zhongyu Wei,\nXuanjing Huang, Guihong Cao,\nDaxin Jiang, Ming Zhou, et¬†al.\n2020.",
      "orig_title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "paper_id": "2002.01808v5"
    },
    {
      "index": 159,
      "title": "Rationale-Augmented Ensembles in Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2207.00747\n(",
      "authors": "Xuezhi Wang, Jason Wei,\nDale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou.\n2022e.",
      "orig_title": "Rationale-augmented ensembles in language models",
      "paper_id": "2207.00747v1"
    },
    {
      "index": 160,
      "title": "Aakanksha Chowdhery",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Xuezhi Wang, Jason Wei,\nDale Schuurmans, Quoc¬†V Le,\nEd¬†H. Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou.\n2023c."
    },
    {
      "index": 161,
      "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yizhong Wang, Yeganeh\nKordi, Swaroop Mishra, Alisa Liu,\nNoah¬†A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. 2022a."
    },
    {
      "index": 162,
      "title": "Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.12410\n(",
      "authors": "Yaqing Wang, Subhabrata\nMukherjee, Xiaodong Liu, Jing Gao,\nAhmed¬†Hassan Awadallah, and Jianfeng\nGao. 2022c."
    },
    {
      "index": 163,
      "title": "Preserving In-Context Learning ability in Large Language Model Fine-tuning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.00635\n(",
      "authors": "Yihan Wang, Si Si,\nDaliang Li, Michal Lukasik,\nFelix Yu, Cho-Jui Hsieh,\nInderjit¬†S Dhillon, and Sanjiv Kumar.\n2022d."
    },
    {
      "index": 164,
      "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.01560\n(",
      "authors": "Zihao Wang, Shaofei Cai,\nAnji Liu, Xiaojian Ma, and\nYitao Liang. 2023a."
    },
    {
      "index": 165,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Jason Wei, Maarten Bosma,\nVincent Zhao, Kelvin Guu,\nAdams¬†Wei Yu, Brian Lester,\nNan Du, Andrew¬†M Dai, and\nQuoc¬†V Le. 2022a.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 166,
      "title": "Emergent Abilities of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2206.07682\n(",
      "authors": "Jason Wei, Yi Tay,\nRishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler,\net¬†al. 2022b.",
      "orig_title": "Emergent abilities of large language models",
      "paper_id": "2206.07682v2"
    },
    {
      "index": 167,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Jason Wei, Xuezhi Wang,\nDale Schuurmans, Maarten Bosma,\nFei Xia, Ed¬†H Chi,\nQuoc¬†V Le, Denny Zhou, et¬†al.\n2022c.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 168,
      "title": "ChatGPT Gets Its ‚ÄúWolfram Superpowers‚Äù!",
      "abstract": "",
      "year": "2023",
      "venue": "https://writings.stephenwolfram.com/",
      "authors": "Stephen Wolfram.\n[n.‚Äâd.]."
    },
    {
      "index": 169,
      "title": "BloombergGPT: A Large Language Model for Finance",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.17564\n(",
      "authors": "Shijie Wu, Ozan Irsoy,\nSteven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann,\nPrabhanjan Kambadur, David Rosenberg,\nand Gideon Mann. 2023.",
      "orig_title": "Bloomberggpt: A large language model for finance",
      "paper_id": "2303.17564v3"
    },
    {
      "index": 170,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Zhuofeng Wu, Sinong Wang,\nJiatao Gu, Rui Hou,\nYuxiao Dong, V.¬†G.¬†Vinod Vydiswaran,\nand Hao Ma. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 171,
      "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.05687\n(",
      "authors": "Runxin Xu, Fuli Luo,\nZhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and\nFei Huang. 2021.",
      "orig_title": "Raise a child in large language model: Towards effective and generalizable fine-tuning",
      "paper_id": "2109.05687v1"
    },
    {
      "index": 172,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jingfeng Yang, Haoming\nJiang, Qingyu Yin, Danqing Zhang,\nBing Yin, and Diyi Yang.\n2022a.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 173,
      "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv (",
      "authors": "Jingfeng Yang, Hongye\nJin, Ruixiang Tang, Xiaotian Han,\nQizhang Feng, Haoming Jiang,\nBing Yin, and Xia Hu.\n2023b.",
      "orig_title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
      "paper_id": "2304.13712v2"
    },
    {
      "index": 174,
      "title": "Large language models can rate news outlet credibility",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.00228\n(",
      "authors": "Kai-Cheng Yang and\nFilippo Menczer. 2023."
    },
    {
      "index": 175,
      "title": "Dynamic Prompting: A Unified Framework for Prompt Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv abs/2303.02909\n(",
      "authors": "Xianjun Yang, Wei Cheng,\nXujiang Zhao, Linda Petzold, and\nHaifeng Chen. 2023a."
    },
    {
      "index": 176,
      "title": "Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.04492\n(",
      "authors": "Zonghan Yang, Xiaoyuan\nYi, Peng Li, Yang Liu, and\nXing Xie. 2022b.",
      "orig_title": "Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization",
      "paper_id": "2210.04492v2"
    },
    {
      "index": 177,
      "title": "Ontology-enhanced Prompt-tuning for Few-shot Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the ACM Web Conference 2022\n(",
      "authors": "Hongbin Ye, Ningyu Zhang,\nShumin Deng, Xiang Chen,\nHui Chen, Feiyu Xiong,\nXi Chen, and Huajun Chen.\n2022.",
      "orig_title": "Ontology-enhanced Prompt-tuning for Few-shot Learning",
      "paper_id": "2201.11332v1"
    },
    {
      "index": 178,
      "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.10199\n(",
      "authors": "Elad¬†Ben Zaken, Shauli\nRavfogel, and Yoav Goldberg.\n2021.",
      "orig_title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "paper_id": "2106.10199v5"
    },
    {
      "index": 179,
      "title": "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n1ùëõ1/n parameters",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.08597\n(",
      "authors": "Aston Zhang, Yi Tay,\nShuai Zhang, Alvin Chan,\nAnh¬†Tuan Luu, Siu¬†Cheung Hui, and\nJie Fu. 2021a."
    },
    {
      "index": 180,
      "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.11717\n(",
      "authors": "Chaoning Zhang, Chenshuang\nZhang, Sheng Zheng, Yu Qiao,\nChenghao Li, Mengchun Zhang,\nSumit¬†Kumar Dam, Chu¬†Myaet Thwal,\nYe¬†Lin Tun, Le¬†Luang Huy,\net¬†al. 2023b.",
      "orig_title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
      "paper_id": "2303.11717v1"
    },
    {
      "index": 181,
      "title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.01642\n(",
      "authors": "Haojie Zhang, Ge Li,\nJia Li, Zhongjin Zhang,\nYuqi Zhu, and Zhi Jin.\n2022a.",
      "orig_title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively",
      "paper_id": "2211.01642v1"
    },
    {
      "index": 182,
      "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.16199\n(",
      "authors": "Renrui Zhang, Jiaming\nHan, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu,\nHongsheng Li, Peng Gao, and\nYu Qiao. 2023a.",
      "orig_title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "paper_id": "2303.16199v3"
    },
    {
      "index": 183,
      "title": "Unsupervised Domain Adaptation with Adapter",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.00667\n(",
      "authors": "Rongsheng Zhang, Yinhe\nZheng, Xiaoxi Mao, and Minlie Huang.\n2021b.",
      "orig_title": "Unsupervised domain adaptation with adapter",
      "paper_id": "2111.00667v1"
    },
    {
      "index": 184,
      "title": "Automatic chain of thought prompting in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.03493\n(",
      "authors": "Zhuosheng Zhang, Aston\nZhang, Mu Li, and Alex Smola.\n2022b."
    },
    {
      "index": 185,
      "title": "Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.01979\n(",
      "authors": "Hongyu Zhao, Hao Tan,\nand Hongyuan Mei. 2022.",
      "orig_title": "Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters",
      "paper_id": "2211.01979v1"
    },
    {
      "index": 186,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.18223\n(",
      "authors": "Wayne¬†Xin Zhao, Kun Zhou,\nJunyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang,\nJunjie Zhang, Zican Dong,\net¬†al. 2023.",
      "orig_title": "A Survey of Large Language Models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 187,
      "title": "Nathanael Sch√§rli",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Denny Zhou, Nathanael\nSch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang,\nDale Schuurmans, Claire Cui,\nOlivier Bousquet, Quoc¬†V Le, and\nEd¬†H. Chi. 2023."
    },
    {
      "index": 188,
      "title": "ChatGPT and environmental research",
      "abstract": "",
      "year": "2023",
      "venue": "Environmental Science & Technology\n(",
      "authors": "Jun-Jie Zhu, Jinyue\nJiang, Meiqi Yang, and Zhiyong¬†Jason\nRen. 2023."
    },
    {
      "index": 189,
      "title": "Fine-Tuning Language Models from Human Preferences",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.08593\n(",
      "authors": "Daniel¬†M Ziegler, Nisan\nStiennon, Jeffrey Wu, Tom¬†B Brown,\nAlec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving.\n2019.",
      "orig_title": "Fine-tuning language models from human preferences",
      "paper_id": "1909.08593v2"
    }
  ]
}