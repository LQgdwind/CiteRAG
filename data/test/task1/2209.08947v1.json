{
  "paper_id": "2209.08947v1",
  "title": "Age of Semantics in Cooperative Communications: To Expedite Simulation Towards Real via Offline Reinforcement Learning",
  "abstract": "Abstract\nThe age of information metric fails to correctly describe the intrinsic semantics of a status update.\nIn an intelligent reflecting surface-aided cooperative relay communication system, we propose the age of semantics (AoS) for measuring semantics freshness of the status updates.\nSpecifically, we focus on the status updating from a source node (SN) to the destination, which is formulated as a Markov decision process (MDP).\nThe objective of the SN is to maximize the expected satisfaction of AoS and energy consumption under the maximum transmit power constraint.\nTo seek the optimal control policy, we first derive an online deep actor-critic (DAC) learning scheme under the on-policy temporal difference learning framework.\nHowever, implementing the online DAC in practice poses the key challenge in infinitely repeated interactions between the SN and the system, which can be dangerous particularly during the exploration.\nWe then put forward a novel offline DAC scheme, which estimates the optimal control policy from a previously collected dataset without any further interactions with the system.\nNumerical experiments verify the theoretical results and show that our offline DAC scheme significantly outperforms the online DAC scheme and the most representative baselines in terms of mean utility, demonstrating strong robustness to dataset quality.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Delay-aware two-hop cooperative relay communications via approximate MDP and stochastic learning",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Trans. Inf. Theory",
      "authors": "R. Wang and V. K. N. Lau"
    },
    {
      "index": 1,
      "title": "IRS-Aided Wireless Relaying: Deployment Strategy and Capacity Scaling",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Wireless Commun. Lett.",
      "authors": "Z. Kang, C. You, and R. Zhang",
      "orig_title": "IRS-aided wireless relaying: Deployment strategy and capacity scaling",
      "paper_id": "2105.08495v2"
    },
    {
      "index": 2,
      "title": "Intelligent reflecting surface versus decode-and-forward: How large surfaces are needed to beat relaying?",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Wireless Commun. Lett.",
      "authors": "E. Björnson, Ö. Özdogan, and E. G. Larsson"
    },
    {
      "index": 3,
      "title": "Towards Industry 5.0: Intelligent Reflecting Surface (IRS) in Smart Manufacturing",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Commun. Mag.",
      "authors": "M. Noor-A-Rahim, F. Firyaguna, J. John, M. O. Khyam, D. Pesch, E. Armstrong, H. Claussen, and H. V. Poor",
      "orig_title": "Towards Industry 5.0: Intelligent reflecting surface (IRS) in smart manufacturing",
      "paper_id": "2201.02214v3"
    },
    {
      "index": 4,
      "title": "Enabling edge-cloud video analytics for robotics applications",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE INFOCOM",
      "authors": "Y. Wang, W. Wang, D. Liu, X. Jin, J. Jiang, and K. Chen"
    },
    {
      "index": 5,
      "title": "Server-driven video streaming for deep learning inference",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGCOMM",
      "authors": "K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, and J. Jiang"
    },
    {
      "index": 6,
      "title": "Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Commun.",
      "authors": "X. Kang, B. Song, J. Guo, Z. Qin, and F. R. Yu",
      "orig_title": "Task-oriented image transmission for scene classification in unmanned aerial systems",
      "paper_id": "2112.10948v1"
    },
    {
      "index": 7,
      "title": "Real-time status: How often should one update?",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE INFOCOM",
      "authors": "S. Kaul, R. Yates, and M. Gruteser"
    },
    {
      "index": 8,
      "title": "Age of information aware radio resource management in vehicular networks: A proactive deep reinforcement learning perspective",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Trans. Wireless Commun.",
      "authors": "X. Chen, C. Wu, T. Chen, H. Zhang, Z. Liu, Y. Zhang, and M. Bennis"
    },
    {
      "index": 9,
      "title": "Information Freshness-Aware Task Offloading in Air-Ground Integrated Edge Computing Systems",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE J. Sel. Areas Commun.",
      "authors": "X. Chen, C. Wu, T. Chen, Z. Liu, H. Zhang, M. Bennis, H. Liu, and Y. Ji",
      "orig_title": "Information freshness-aware task offloading in air-ground integrated edge computing systems",
      "paper_id": "2007.10129v1"
    },
    {
      "index": 10,
      "title": "Minimizing age of information in multi-channel time-sensitive information update systems",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE INFOCOM",
      "authors": "Z. Qian, F. Wu, J. Pan, K. Srinivasan, and N. B. Shroff"
    },
    {
      "index": 11,
      "title": "A reinforcement learning framework for optimizing age of information in RF-powered communication systems",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Trans. Commun.",
      "authors": "M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas"
    },
    {
      "index": 12,
      "title": "Optimal Scheduling of Age-centric Caching: Tractability and Computation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Mobile Comput.",
      "authors": "G. Ahani, D. Yuan, and S. Sun",
      "orig_title": "Optimal scheduling of age-centric caching: Tractability and computation",
      "paper_id": "2005.00445v1"
    },
    {
      "index": 13,
      "title": "Age of Information: An Introduction and Survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE J. Sel. Areas Commun.",
      "authors": "R. D. Yates, Y. Sun, D. R. Brown, S. K. Kaul, E. Modiano, and S. Ulukus",
      "orig_title": "Age of information: An introduction and survey",
      "paper_id": "2007.08564v1"
    },
    {
      "index": 14,
      "title": "Minimizing age-of-information in multi-hop wireless networks",
      "abstract": "",
      "year": "2017",
      "venue": "Allerton",
      "authors": "R. Talak, S. Karaman, and E. Modiano"
    },
    {
      "index": 15,
      "title": "On the age of information in multi-source multi-hop wireless status update networks",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE SPAWC",
      "authors": "S. Farazi, A. G. Klein, J. A. McNeill, and D. Richard Brown"
    },
    {
      "index": 16,
      "title": "Optimizing information freshness in RF-powered multi-hop wireless networks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Wireless Commun.",
      "authors": "T. He, K.-W. Chin, Z. Zhang, T. Liu, and J. Wen"
    },
    {
      "index": 17,
      "title": "Age of information optimization in multi-channel based multi-hop wireless networks",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Mobile Comput.",
      "authors": "J. Lou, X. Yuan, P. Sigdel, X. Qin, S. Kompella, and N.-F. Tzeng"
    },
    {
      "index": 18,
      "title": "Minimizing AoI with throughput requirements in multi-path network communication",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/ACM Trans. Netw.",
      "authors": "Q. Liu, H. Zeng, and M. Chen"
    },
    {
      "index": 19,
      "title": "Optimized Computation Offloading Performance in Virtual Edge Computing Systems via Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Internet Things J.",
      "authors": "X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis",
      "orig_title": "Optimized computation offloading performance in virtual edge computing systems via deep reinforcement learning",
      "paper_id": "1805.06146v1"
    },
    {
      "index": 20,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT Press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 21,
      "title": "Optimizing information freshness in two-hop status update systems under a resource constraint",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE J. Sel. Areas Commun.",
      "authors": "Y. Gu, Q. Wang, H. Chen, Y. Li, and B. Vucetic"
    },
    {
      "index": 22,
      "title": "Information Freshness in Multi-Hop Wireless Networks",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "V. Tripathi, R. Talak, and E. Modiano",
      "orig_title": "Information freshness in multi-hop wireless networks",
      "paper_id": "2111.09217v1"
    },
    {
      "index": 23,
      "title": "Sampling of the wiener process for remote estimation over a channel with random delay",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Trans. Inf. Theory",
      "authors": "Y. Sun, Y. Polyanskiy, and E. Uysal"
    },
    {
      "index": 24,
      "title": "The age of incorrect information: A new performance metric for status updates",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/ACM Trans. Netw.",
      "authors": "A. Maatouk, S. Kriouile, M. Assaad, and A. Ephremides"
    },
    {
      "index": 25,
      "title": "PnP-DRL: A plug-and-play deep reinforcement learning approach for experience-driven networking",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE J. Sel. Areas Commun.",
      "authors": "Z. Xu, K. Wu, W. Zhang, J. Tang, Y. Wang, and G. Xue"
    },
    {
      "index": 26,
      "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "S. Levine, A. Kumar, G. Tucker, and J. Fu",
      "orig_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "paper_id": "2005.01643v3"
    },
    {
      "index": 27,
      "title": "Off-Policy Deep Reinforcement Learning without Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "S. Fujimoto, D. Meger, and D. Precup",
      "orig_title": "Off-policy deep reinforcement learning without exploration",
      "paper_id": "1812.02900v3"
    },
    {
      "index": 28,
      "title": "Stabilizing off-policy Q-learning via bootstrapping error reduction",
      "abstract": "",
      "year": "2019",
      "venue": "NIPS",
      "authors": "A. Kumar, J. Fu, G. Tucker, and S. Levine"
    },
    {
      "index": 29,
      "title": "Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "N. Siegel, J. T. Springenberg, F. Berkenkamp, A. Abdolmaleki, M. Neunert, T. Lampe, R. Hafner, N. Heess, and M. Riedmiller",
      "orig_title": "Keep doing what worked: Behavior modelling priors for offline reinforcement learning",
      "paper_id": "2002.08396v3"
    },
    {
      "index": 30,
      "title": "Critic Regularized Regression",
      "abstract": "",
      "year": "2020",
      "venue": "NIPS",
      "authors": "Z. Wang, A. Novikov, K. Żołna, J. T. Springenberg, S. Reed, B. Shahriari, N. Siegel, J. Merel, C. Gulcehre, N. Heess, and N. de Freitas",
      "orig_title": "Critic regularized regression",
      "paper_id": "2006.15134v3"
    },
    {
      "index": 31,
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NIPS",
      "authors": "A. Kumar, A. Zhou, G. Tucker, and S. Levine",
      "orig_title": "Conservative Q-learning for offline reinforcement learning",
      "paper_id": "2006.04779v3"
    },
    {
      "index": 32,
      "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI",
      "authors": "H. Xu, X. Zhan, and X. Zhu",
      "orig_title": "Constraints penalized Q-learning for safe offline reinforcement learning",
      "paper_id": "2107.09003v3"
    },
    {
      "index": 33,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu"
    },
    {
      "index": 34,
      "title": "Handover in multihop cellular networks",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Commun. Mag.",
      "authors": "S. Cho, E. W. Jang, and J. M. Cioffi"
    },
    {
      "index": 35,
      "title": "Buffer-aided relaying with adaptive link selection-fixed and mixed rate transmission",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Trans. Inf. Theory",
      "authors": "N. Zlatanov and R. Schober"
    },
    {
      "index": 36,
      "title": "Distributed Reinforcement Learning for Age of Information Minimization in Real-Time IoT Systems",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process.",
      "authors": "S. Wang, M. Chen, Z. Yang, C. Yin, W. Saad, S. Cui, and H. V. Poor",
      "orig_title": "Distributed reinforcement learning for age of information minimization in real-time IoT systems",
      "paper_id": "2104.01527v2"
    },
    {
      "index": 37,
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artif. Intell.",
      "authors": "R. S. Sutton, D. Precup, and S. Singh"
    },
    {
      "index": 38,
      "title": "A generic quantitative relationship between quality of experience and quality of service",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE Netw.",
      "authors": "M. Fiedler, T. Hossfeld, and P. Tran-Gia"
    },
    {
      "index": 39,
      "title": "Sensitive discount optimality: Unifying discounted and average reward reinforcement learning",
      "abstract": "",
      "year": "1996",
      "venue": "ICML",
      "authors": "S. Mahadevan"
    },
    {
      "index": 40,
      "title": "NP-hardness of checking the unichain condition in average cost MDPs",
      "abstract": "",
      "year": "2007",
      "venue": "Oper. Res. Lett.",
      "authors": "J. N. Tsitsiklis"
    },
    {
      "index": 41,
      "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "B. Eysenbach and S. Levine",
      "orig_title": "Maximum entropy RL (provably) solves some robust RL problems",
      "paper_id": "2103.06257v2"
    },
    {
      "index": 42,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 43,
      "title": "Dynamic Programming",
      "abstract": "",
      "year": "1957",
      "venue": "Princeton University Press",
      "authors": "R. Bellman"
    },
    {
      "index": 44,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis"
    },
    {
      "index": 45,
      "title": "Multi-Tenant Cross-Slice Resource Orchestration: A Deep Reinforcement Learning Approach",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE J. Sel. Areas Commun.",
      "authors": "X. Chen, Z. Zhao, C. Wu, M. Bennis, H. Liu, Y. Ji, and H. Zhang",
      "orig_title": "Multi-tenant cross-slice resource orchestration: A deep reinforcement learning approach",
      "paper_id": "1807.09350v2"
    },
    {
      "index": 46,
      "title": "Challenges of real-world reinforcement learning: Definitions, benchmarks and analysis",
      "abstract": "",
      "year": "2021",
      "venue": "Mach. Learn.",
      "authors": "G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru, S. Gowal, and T. Hester"
    },
    {
      "index": 47,
      "title": "Digital twin networks: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Internet Things J.",
      "authors": "Y. Wu, K. Zhang, and Y. Zhang"
    },
    {
      "index": 48,
      "title": "Maximum a Posteriori Policy Optimisation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "A. Abdolmaleki, J. T. Springenberg, N. Heess, Y. Tassa, and R. Munos",
      "orig_title": "Maximum a posteriori policy optimisation",
      "paper_id": "1806.06920v1"
    },
    {
      "index": 49,
      "title": "ConQUR: Mitigating Delusional Bias in Deep Q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "D. Su, J. Ooi, T. Lu, D. Schuurmans, and C. Boutilier",
      "orig_title": "ConQUR: Mitigating delusional bias in deep Q-learning",
      "paper_id": "2002.12399v1"
    },
    {
      "index": 50,
      "title": "Ranking policy gradient",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "K. Lin and J. Zhou"
    },
    {
      "index": 51,
      "title": "Offline Reinforcement Learning as Anti-Exploration",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI",
      "authors": "S. Rezaeifar, R. Dadashi, N. Vieillard, L. Hussenot, O. Bachem, O. Pietquin, and M. Geist",
      "orig_title": "Offline reinforcement learning as anti-exploration",
      "paper_id": "2106.06431v1"
    },
    {
      "index": 52,
      "title": "Variational Bayesian Reinforcement Learning with Regret Bounds",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "B. O’Donoghue",
      "orig_title": "Variational bayesian reinforcement learning with regret bounds",
      "paper_id": "1807.09647v4"
    },
    {
      "index": 53,
      "title": "Learning Stochastic Shortest Path with Linear Function Approximation",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Y. Min, J. He, T. Wang, and Q. Gu",
      "orig_title": "Learning stochastic shortest path with linear function approximation",
      "paper_id": "2110.12727v3"
    },
    {
      "index": 54,
      "title": "A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "A. Sharma, R. Ahmad, and C. Finn",
      "orig_title": "A state-distribution matching approach to non-Episodic reinforcement learning",
      "paper_id": "2205.05212v1"
    },
    {
      "index": 55,
      "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "O. Nachum, M. Norouzi, K. Xu, and D. Schuurmans",
      "orig_title": "Bridging the gap between value and policy based reinforcement learning",
      "paper_id": "1702.08892v3"
    },
    {
      "index": 56,
      "title": "Reinforcement Learning with Deep Energy-Based Policies",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "T. Haarnoja, H. Tang, P. Abbeel, and S. Levine",
      "orig_title": "Reinforcement learning with deep energy-based policies",
      "paper_id": "1702.08165v2"
    },
    {
      "index": 57,
      "title": "Rectified linear units improve restricted boltzmann machines",
      "abstract": "",
      "year": "2010",
      "venue": "ICML",
      "authors": "V. Nair and G. E. Hinton"
    },
    {
      "index": 58,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "MIT Press",
      "authors": "I. Goodfellow, Y. Bengio, and A. Courville",
      "orig_title": "Deep Learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 59,
      "title": "Adam: A Method for Stochastic Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "D. P. Kingma and J. Ba"
    }
  ]
}