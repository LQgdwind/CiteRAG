{
  "paper_id": "2110.09548v4",
  "title": "Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks",
  "abstract": "Abstract\nUnderstanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong global optimality guarantees for this algorithm. We also provide experiments corroborating our theory.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Norm-based capacity control in neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "The 28th Conference on Learning Theory",
      "authors": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro"
    },
    {
      "index": 1,
      "title": "Path-sgd: Path-normalized optimization in deep neural networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro"
    },
    {
      "index": 2,
      "title": "Convex geometry of two-layer relu networks: Implicit autoencoding and interpretable models",
      "abstract": "",
      "year": "2020",
      "venue": "The Twenty Third International Conference on Artificial Intelligence and Statistics",
      "authors": "Tolga Ergen and Mert Pilanci"
    },
    {
      "index": 3,
      "title": "Convex geometry and duality of over-parameterized neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Tolga Ergen and Mert Pilanci"
    },
    {
      "index": 4,
      "title": "Convex optimization for shallow neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",
      "authors": "T. Ergen and M. Pilanci"
    },
    {
      "index": 5,
      "title": "Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks",
      "abstract": "",
      "year": "2020",
      "venue": "37th International Conference on Machine Learning",
      "authors": "Mert Pilanci and Tolga Ergen",
      "orig_title": "Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks",
      "paper_id": "2002.10553v2"
    },
    {
      "index": 6,
      "title": "Failures of gradient-based deep learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.07950",
      "authors": "Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah"
    },
    {
      "index": 7,
      "title": "Deep learning, volume 1",
      "abstract": "",
      "year": "2016",
      "venue": "MIT press Cambridge",
      "authors": "Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio"
    },
    {
      "index": 8,
      "title": "Learning One-hidden-layer Neural Networks with Landscape Design",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Rong Ge, Jason D. Lee, and Tengyu Ma",
      "orig_title": "Learning one-hidden-layer neural networks with landscape design",
      "paper_id": "1711.00501v2"
    },
    {
      "index": 9,
      "title": "Spurious Local Minima are Common in Two-Layer ReLU Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Itay Safran and Ohad Shamir",
      "orig_title": "Spurious local minima are common in two-layer relu neural networks",
      "paper_id": "1712.08968v3"
    },
    {
      "index": 10,
      "title": "Efficient approaches for escaping higher order saddle points in non-convex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "Conference on learning theory",
      "authors": "Animashree Anandkumar and Rong Ge"
    },
    {
      "index": 11,
      "title": "On the complexity of training neural networks with continuous activation functions",
      "abstract": "",
      "year": "1995",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "Bhaskar DasGupta, Hava T Siegelmann, and Eduardo Sontag"
    },
    {
      "index": 12,
      "title": "Training a 3-node neural network is np-complete",
      "abstract": "",
      "year": "1989",
      "venue": "Advances in neural information processing systems",
      "authors": "Avrim Blum and Ronald L Rivest"
    },
    {
      "index": 13,
      "title": "Hardness results for neural network approximation problems",
      "abstract": "",
      "year": "1999",
      "venue": "European Conference on Computational Learning Theory",
      "authors": "Peter Bartlett and Shai Ben-David"
    },
    {
      "index": 14,
      "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz",
      "orig_title": "SGD learns over-parameterized networks that provably generalize on linearly separable data",
      "paper_id": "1710.10174v1"
    },
    {
      "index": 15,
      "title": "On the Power of Over-parametrization in Neural Networks with Quadratic Activation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.01206",
      "authors": "Simon S Du and Jason D Lee",
      "orig_title": "On the power of over-parametrization in neural networks with quadratic activation",
      "paper_id": "1803.01206v2"
    },
    {
      "index": 16,
      "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization",
      "abstract": "",
      "year": "2018",
      "venue": "35th International Conference on Machine Learning, ICML 2018",
      "authors": "Sanjeev Arora, Nadav Cohen, and Elad Hazan",
      "orig_title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
      "paper_id": "1802.06509v2"
    },
    {
      "index": 17,
      "title": "Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1805.12076",
      "authors": "Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro",
      "orig_title": "Towards understanding the role of over-parametrization in generalization of neural networks",
      "paper_id": "1805.12076v1"
    },
    {
      "index": 18,
      "title": "Parallel Deep Neural Networks Have Zero Duality Gap",
      "abstract": "",
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "authors": "Yifei Wang, Tolga Ergen, and Mert Pilanci",
      "orig_title": "Parallel deep neural networks have zero duality gap",
      "paper_id": "2110.06482v3"
    },
    {
      "index": 19,
      "title": "Revealing the Structure of Deep Neural Networks via Convex Duality",
      "abstract": "",
      "year": "2021",
      "venue": "38th International Conference on Machine Learning",
      "authors": "Tolga Ergen and Mert Pilanci",
      "orig_title": "Revealing the structure of deep neural networks via convex duality",
      "paper_id": "2002.09773v4"
    },
    {
      "index": 20,
      "title": "Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs",
      "abstract": "",
      "year": "2021",
      "venue": "38th International Conference on Machine Learning",
      "authors": "Tolga Ergen and Mert Pilanci",
      "orig_title": "Global optimality beyond two layers: Training deep relu networks via convex programs",
      "paper_id": "2110.05518v2"
    },
    {
      "index": 21,
      "title": "Deep neural networks with multi-branch architectures are intrinsically less non-convex",
      "abstract": "",
      "year": "2019",
      "venue": "22nd International Conference on Artificial Intelligence and Statistics",
      "authors": "Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov"
    },
    {
      "index": 22,
      "title": "Global optimality in neural network training",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Benjamin D Haeffele and René Vidal"
    },
    {
      "index": 23,
      "title": "Wide Residual Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1605.07146",
      "authors": "Sergey Zagoruyko and Nikos Komodakis",
      "orig_title": "Wide residual networks",
      "paper_id": "1605.07146v4"
    },
    {
      "index": 24,
      "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Andreas Veit, Michael J Wilber, and Serge Belongie",
      "orig_title": "Residual networks behave like ensembles of relatively shallow networks",
      "paper_id": "1605.06431v2"
    },
    {
      "index": 25,
      "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1602.07360",
      "authors": "Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer",
      "orig_title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size",
      "paper_id": "1602.07360v4"
    },
    {
      "index": 26,
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-first AAAI conference on artificial intelligence",
      "authors": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi",
      "orig_title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "paper_id": "1602.07261v2"
    },
    {
      "index": 27,
      "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "François Chollet",
      "orig_title": "Xception: Deep learning with depthwise separable convolutions",
      "paper_id": "1610.02357v3"
    },
    {
      "index": 28,
      "title": "Aggregated Residual Transformations for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He",
      "orig_title": "Aggregated residual transformations for deep neural networks",
      "paper_id": "1611.05431v2"
    },
    {
      "index": 29,
      "title": "Understanding Deep Neural Networks with Rectified Linear Units",
      "abstract": "",
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018",
      "authors": "Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee",
      "orig_title": "Understanding deep neural networks with rectified linear units",
      "paper_id": "1611.01491v6"
    },
    {
      "index": 30,
      "title": "Tight Hardness Results for Training Depth-2 ReLU Networks",
      "abstract": "",
      "year": "2021",
      "venue": "12th Innovations in Theoretical Computer Science Conference (ITCS 2021)",
      "authors": "Surbhi Goel, Adam Klivans, Pasin Manurangsi, and Daniel Reichman"
    },
    {
      "index": 31,
      "title": "The Computational Complexity of ReLU Network Training Parameterized by Data Dimensionality",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Vincent Froese, Christoph Hertrich, and Rolf Niedermeier",
      "orig_title": "The computational complexity of relu network training parameterized by data dimensionality",
      "paper_id": "2105.08675v3"
    },
    {
      "index": 32,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 33,
      "title": "Representation costs of linear neural networks: Analysis and design",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhen Dai, Mina Karzand, and Nathan Srebro"
    },
    {
      "index": 34,
      "title": "L1 regularization in infinite dimensional feature spaces",
      "abstract": "",
      "year": "2007",
      "venue": "International Conference on Computational Learning Theory",
      "authors": "Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu"
    },
    {
      "index": 35,
      "title": "Convex optimization",
      "abstract": "",
      "year": "2004",
      "venue": "Cambridge university press",
      "authors": "Stephen Boyd and Lieven Vandenberghe"
    },
    {
      "index": 36,
      "title": "Convex geometry and duality of over-parameterized neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Tolga Ergen and Mert Pilanci"
    },
    {
      "index": 37,
      "title": "Breaking the curse of dimensionality with convex neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Francis Bach"
    },
    {
      "index": 38,
      "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang",
      "orig_title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
      "paper_id": "1811.04918v6"
    },
    {
      "index": 39,
      "title": "Global convergence of three-layer neural networks in the mean field regime",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Huy Tuan Pham and Phan-Minh Nguyen"
    },
    {
      "index": 40,
      "title": "Greedy Layerwise Learning Can Scale to ImageNet",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon",
      "orig_title": "Greedy layerwise learning can scale to imagenet",
      "paper_id": "1812.11446v3"
    },
    {
      "index": 41,
      "title": "The CIFAR-10 dataset",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton"
    },
    {
      "index": 42,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Han Xiao, Kashif Rasul, and Roland Vollgraf",
      "orig_title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 43,
      "title": "Globally Optimal Training of Neural Networks with Threshold Activation Functions",
      "abstract": "",
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations",
      "authors": "Tolga Ergen, Halil I. Gulluk, Jonathan Lacotte, and Mert Pilanci",
      "orig_title": "Globally optimal training of neural networks with threshold activation functions",
      "paper_id": "2303.03382v1"
    },
    {
      "index": 44,
      "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Tolga Ergen and Mert Pilanci",
      "orig_title": "Implicit convex regularizers of cnn architectures: Convex optimization of two- and three-layer networks in polynomial time",
      "paper_id": "2006.14798v3"
    },
    {
      "index": 45,
      "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M. Pauly, Morteza Mardani, and Mert Pilanci",
      "orig_title": "Hidden convexity of wasserstein GANs: Interpretable generative models with closed-form solutions",
      "paper_id": "2107.05680v2"
    },
    {
      "index": 46,
      "title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Tolga Ergen, Arda Sahiner, Batu Ozturkler, John M. Pauly, Morteza Mardani, and Mert Pilanci",
      "orig_title": "Demystifying batch normalization in reLU networks: Equivalent convex optimization models and implicit regularization",
      "paper_id": "2103.01499v3"
    },
    {
      "index": 47,
      "title": "Convex neural autoregressive models: Towards tractable, expressive, and theoretically-backed models for sequential forecasting and generation",
      "abstract": "",
      "year": "2021",
      "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci"
    },
    {
      "index": 48,
      "title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv",
      "authors": "Tolga Ergen, Behnam Neyshabur, and Harsh Mehta",
      "orig_title": "Convexifying transformers: Improving optimization and understanding of transformer networks",
      "paper_id": "2211.11052v1"
    },
    {
      "index": 49,
      "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "39th International Conference on Machine Learning",
      "authors": "Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci",
      "orig_title": "Unraveling attention via convex duality: Analysis and interpretations of vision transformers",
      "paper_id": "2205.08078v2"
    },
    {
      "index": 50,
      "title": "A neural network playground",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Daniel Smilkov Carter and Shan"
    },
    {
      "index": 51,
      "title": "CVX: Matlab software for disciplined convex programming, version 2.1",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Michael Grant and Stephen Boyd"
    },
    {
      "index": 52,
      "title": "CVXPY: A Python-embedded modeling language for convex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "Steven Diamond and Stephen Boyd"
    },
    {
      "index": 53,
      "title": "A Rewriting System for Convex Optimization Problems",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Control and Decision",
      "authors": "Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd",
      "orig_title": "A rewriting system for convex optimization problems",
      "paper_id": "1709.04494v2"
    },
    {
      "index": 54,
      "title": "UCI machine learning repository",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Dheeru Dua and Casey Graff"
    },
    {
      "index": 55,
      "title": "Do we need hundreds of classifiers to solve real world classification problems?",
      "abstract": "",
      "year": "2014",
      "venue": "Journal of Machine Learning Research",
      "authors": "Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, and Dinani Amorim"
    },
    {
      "index": 56,
      "title": "On general minimax theorems",
      "abstract": "",
      "year": "1958",
      "venue": "Pacific J. Math.",
      "authors": "Maurice Sion"
    },
    {
      "index": 57,
      "title": "Enumeration of linear threshold functions from the lattice of hyperplane intersections",
      "abstract": "",
      "year": "2000",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "Piyush C Ojha"
    },
    {
      "index": 58,
      "title": "An introduction to hyperplane arrangements",
      "abstract": "",
      "year": "2004",
      "venue": "Geometric combinatorics",
      "authors": "Richard P Stanley et al."
    },
    {
      "index": 59,
      "title": "Partitions of n-space by hyperplanes",
      "abstract": "",
      "year": "1966",
      "venue": "SIAM Journal on Applied Mathematics",
      "authors": "RO Winder"
    },
    {
      "index": 60,
      "title": "Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition",
      "abstract": "",
      "year": "1965",
      "venue": "IEEE transactions on electronic computers",
      "authors": "Thomas M Cover"
    },
    {
      "index": 61,
      "title": "Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Arda Sahiner, Tolga Ergen, John M. Pauly, and Mert Pilanci",
      "orig_title": "Vector-output relu neural network problems are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms",
      "paper_id": "2012.13329v2"
    }
  ]
}