{
  "paper_id": "2104.13844v3",
  "title": "A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning",
  "abstract": "Abstract\nMany reinforcement learning algorithms rely on value estimation, however, the most widely used algorithms‚Äînamely temporal difference algorithms‚Äîcan diverge under both off-policy sampling and nonlinear function approximation.\nMany algorithms have been developed for off-policy value estimation based on the linear mean squared projected Bellman error (PBE¬Ø¬ØPBE\\overline{\\text{PBE}}) and are sound under linear function approximation.\nExtending these methods to the nonlinear case has been largely unsuccessful.\nRecently, several methods have been introduced that approximate a different objective‚Äîthe mean-squared Bellman error (BE¬Ø¬ØBE\\overline{\\text{BE}})‚Äîwhich naturally facilitate nonlinear approximation.\nIn this work, we build on these insights and introduce a new generalized PBE¬Ø¬ØPBE\\overline{\\text{PBE}} that extends the linear PBE¬Ø¬ØPBE\\overline{\\text{PBE}} to the nonlinear setting.\nWe show how this generalized objective unifies previous work and obtain new bounds for the value error of the solutions of the generalized objective.\nWe derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path",
      "abstract": "",
      "year": "2008",
      "venue": "Machine Learning",
      "authors": "Andr√°s Antos, Csaba Szepesv√°ri, and R√©mi Munos"
    },
    {
      "index": 1,
      "title": "An Alternative Softmax Operator for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Kavosh Asadi and Michael¬†L. Littman",
      "orig_title": "An Alternative Softmax Operator for Reinforcement Learning",
      "paper_id": "1612.05628v5"
    },
    {
      "index": 2,
      "title": "Residual Algorithms: Reinforcement Learning with Function Approximation",
      "abstract": "",
      "year": "1995",
      "venue": "Machine Learning Proceedings",
      "authors": "Leemon Baird"
    },
    {
      "index": 3,
      "title": "Neuronlike adaptive elements that can solve difficult learning control problems",
      "abstract": "",
      "year": "1983",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics",
      "authors": "Andrew¬†G. Barto, Richard¬†S. Sutton, and Charles¬†W. Anderson"
    },
    {
      "index": 4,
      "title": "Neuro-Dynamic Programming",
      "abstract": "",
      "year": "1996",
      "venue": "Athena Scientific",
      "authors": "Dimitri¬†P Bertsekas and J.N. Tsitsiklis"
    },
    {
      "index": 5,
      "title": "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations",
      "abstract": "",
      "year": "2019",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Alberto Bietti and Julien Mairal",
      "orig_title": "Group invariance, stability to deformations, and complexity of deep convolutional representations",
      "paper_id": "1706.03078v4"
    },
    {
      "index": 6,
      "title": "Counterfactual reasoning and learning systems: The example of computational advertising",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Machine Learning Research",
      "authors": "L√©on Bottou, Jonas Peters, Joaquin Qui√±onero-Candela, Denis Charles, Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed¬†Snelson"
    },
    {
      "index": 7,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 8,
      "title": "Learning from Conditional Distributions via Dual Embeddings",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Bo¬†Dai, Niao He, Yunpeng Pan, Byron Boots, and Le¬†Song",
      "orig_title": "Learning from Conditional Distributions via Dual Embeddings",
      "paper_id": "1607.04579v2"
    },
    {
      "index": 9,
      "title": "SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Bo¬†Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le¬†Song"
    },
    {
      "index": 10,
      "title": "Policy Evaluation with Temporal Differences: A Survey and Comparison",
      "abstract": "",
      "year": "2014",
      "venue": "Journal of Machine Learning Research",
      "authors": "Christoph Dann, Gerhard Neumann, and Jan Peters"
    },
    {
      "index": 11,
      "title": "Stochastic Variance Reduction Methods for Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon¬†S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou",
      "orig_title": "Stochastic Variance Reduction Methods for Policy Evaluation",
      "paper_id": "1702.07944v2"
    },
    {
      "index": 12,
      "title": "Nonlinear orthogonal projection",
      "abstract": "",
      "year": "1994",
      "venue": "Annales Polonici Mathematici",
      "authors": "Ewa Dudek and Konstanty Holly"
    },
    {
      "index": 13,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu",
      "orig_title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 14,
      "title": "A Kernel Loss for Solving the Bellman Equation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32",
      "authors": "Yihao Feng, Lihong Li, and Qiang Liu",
      "orig_title": "A Kernel Loss for Solving the Bellman Equation",
      "paper_id": "1905.10506v3"
    },
    {
      "index": 15,
      "title": "Online Off-policy Prediction",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1811.02597",
      "authors": "Sina Ghiassian, Andrew Patterson, Martha White, Richard¬†S. Sutton, and Adam White",
      "orig_title": "Online Off-policy Prediction",
      "paper_id": "1811.02597v1"
    },
    {
      "index": 16,
      "title": "Gradient Temporal-Difference Learning with Regularized Corrections",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White",
      "orig_title": "Gradient Temporal-Difference Learning with Regularized Corrections",
      "paper_id": "2007.00611v4"
    },
    {
      "index": 17,
      "title": "Representations for Stable Off-Policy Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Dibya Ghosh and Marc¬†G. Bellemare",
      "orig_title": "Representations for Stable Off-Policy Reinforcement Learning",
      "paper_id": "2007.05520v2"
    },
    {
      "index": 18,
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Xavier Glorot and Yoshua Bengio"
    },
    {
      "index": 19,
      "title": "Faster Gradient-TD Algorithms",
      "abstract": "",
      "year": "2013",
      "venue": "PhD thesis, University of Alberta",
      "authors": "Leah Hackman"
    },
    {
      "index": 20,
      "title": "Consistent On-Line Off-Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Assaf Hallak and Shie Mannor",
      "orig_title": "Consistent on-line off-policy evaluation",
      "paper_id": "1702.07121v1"
    },
    {
      "index": 21,
      "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI",
      "authors": "Assaf Hallak, Aviv Tamar, Remi Munos, and Shie Mannor",
      "orig_title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis",
      "paper_id": "1509.05172v2"
    },
    {
      "index": 22,
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Max Jaderberg, Volodymyr Mnih, Wojciech¬†Marian Czarnecki, Tom Schaul, Joel¬†Z. Leibo, David Silver, and Koray Kavukcuoglu",
      "orig_title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "paper_id": "1611.05397v1"
    },
    {
      "index": 23,
      "title": "Approximately optimal approximate reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "International Conference on Machine Learning",
      "authors": "Sham Kakade and John Langford"
    },
    {
      "index": 24,
      "title": "Adam: A Method for Stochastic Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Diederik¬†P Kingma and Jimmy Ba"
    },
    {
      "index": 25,
      "title": "The Fixed Points of Off-Policy TD",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J¬†Z Kolter"
    },
    {
      "index": 26,
      "title": "Predictive Representations of State",
      "abstract": "",
      "year": "2002",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Michael¬†L. Littman and Richard¬†S Sutton"
    },
    {
      "index": 27,
      "title": "Finite-Sample Analysis of Proximal Gradient TD Algorithms",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Uncertainty in Artificial Intelligence",
      "authors": "Bo¬†Liu, Ji¬†Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik",
      "orig_title": "Finite-Sample Analysis of Proximal Gradient TD Algorithms",
      "paper_id": "2006.14364v2"
    },
    {
      "index": 28,
      "title": "Proximal Gradient Temporal Difference Learning Algorithms",
      "abstract": "",
      "year": "2016",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Bo¬†Liu, Ji¬†Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik"
    },
    {
      "index": 29,
      "title": "Off-Policy Policy Gradient with Stationary Distribution Correction",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill"
    },
    {
      "index": 30,
      "title": "Gradient Temporal-Difference Learning Algorithms",
      "abstract": "",
      "year": "2011",
      "venue": "PhD thesis, University of Alberta, Edmonton",
      "authors": "Hamid¬†Reza Maei"
    },
    {
      "index": 31,
      "title": "GQ (lambda): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "abstract": "",
      "year": "2010",
      "venue": "Conference on Artificial General Intelligence",
      "authors": "Hamid¬†Reza Maei and Richard¬†S. Sutton"
    },
    {
      "index": 32,
      "title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hamid¬†Reza Maei, Csaba Szepesv√°ri, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard¬†S Sutton"
    },
    {
      "index": 33,
      "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv:1405.6757",
      "authors": "Sridhar Mahadevan, Bo¬†Liu, Philip Thomas, Will Dabney, Steve Giguere, Nicholas Jacek, Ian Gemp, and Ji¬†Liu"
    },
    {
      "index": 34,
      "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv:1702.03006",
      "authors": "Ashique¬†Rupam Mahmood, Huizhen Yu, and Richard¬†S. Sutton",
      "orig_title": "Multi-step Off-policy Learning Without Importance Sampling Ratios",
      "paper_id": "1702.03006v1"
    },
    {
      "index": 35,
      "title": "Finite-Sample Analysis of Bellman Residual Minimization",
      "abstract": "",
      "year": "2010",
      "venue": "Asian Conference on Machine Learning",
      "authors": "Odalric-Ambrym Maillard, Remi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh"
    },
    {
      "index": 36,
      "title": "Efficient Memory-Based Learning for Robot Control",
      "abstract": "",
      "year": "1990",
      "venue": "PhD thesis, University of Cambridge",
      "authors": "Andrew¬†William Moore"
    },
    {
      "index": 37,
      "title": "Safe and efficient off-policy reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare",
      "orig_title": "Safe and Efficient Off-Policy Reinforcement Learning",
      "paper_id": "1606.02647v2"
    },
    {
      "index": 38,
      "title": "Eligibility Traces for Off-Policy Policy Evaluation",
      "abstract": "",
      "year": "2000",
      "venue": "International Conference on Machine Learning",
      "authors": "Doina Precup, Richard¬†S Sutton, and Satinder Singh"
    },
    {
      "index": 39,
      "title": "Off-Policy Temporal-Difference Learning with Function Approximation",
      "abstract": "",
      "year": "2001",
      "venue": "International Conference on Machine Learning",
      "authors": "Doina Precup, Richard¬†S Sutton, and Sanjoy Dasgupta"
    },
    {
      "index": 40,
      "title": "Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Machine Learning",
      "authors": "Bruno Scherrer"
    },
    {
      "index": 41,
      "title": "Optimality of Reinforcement Learning Algorithms with Linear Function Approximation",
      "abstract": "",
      "year": "2003",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ralf Schoknecht"
    },
    {
      "index": 42,
      "title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
      "abstract": "",
      "year": "1996",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Richard¬†S. Sutton"
    },
    {
      "index": 43,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Richard¬†S. Sutton and Andrew¬†G. Barto"
    },
    {
      "index": 44,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Richard¬†S. Sutton, David¬†A. McAllester, Satinder¬†P. Singh, and Yishay Mansour"
    },
    {
      "index": 45,
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial Intelligence",
      "authors": "Richard¬†S. Sutton, Doina Precup, and Satinder Singh"
    },
    {
      "index": 46,
      "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Machine Learning",
      "authors": "Richard¬†S. Sutton, Hamid¬†Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv√°ri, and Eric Wiewiora"
    },
    {
      "index": 47,
      "title": "Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Autonomous Agents and Multi-Agent Systems",
      "authors": "Richard¬†S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick¬†M Pilarski, Adam White, and Doina Precup"
    },
    {
      "index": 48,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "Richard¬†S Sutton, A¬†Rupam Mahmood, and Martha White",
      "orig_title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 49,
      "title": "The many proofs of an identity on the norm of oblique projections",
      "abstract": "",
      "year": "2006",
      "venue": "Numerical Algorithms",
      "authors": "Daniel¬†B. Szyld"
    },
    {
      "index": 50,
      "title": "TD(ŒªùúÜ\\lambda) networks: Temporal-difference networks with eligibility traces",
      "abstract": "",
      "year": "2005",
      "venue": "International Conference on Machine Learning",
      "authors": "Brian Tanner and Richard¬†S. Sutton"
    },
    {
      "index": 51,
      "title": "Convergent Tree Backup and Retrace with Function Approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent",
      "orig_title": "Convergent Tree Backup and Retrace with Function Approximation",
      "paper_id": "1705.09322v4"
    },
    {
      "index": 52,
      "title": "An analysis of temporal-difference learning with function approximation",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "J.N. Tsitsiklis and B.¬†Van¬†Roy"
    },
    {
      "index": 53,
      "title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas",
      "orig_title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "paper_id": "1511.06581v3"
    },
    {
      "index": 54,
      "title": "Learning from Delayed Rewards",
      "abstract": "",
      "year": "1989",
      "venue": "PhD thesis, King‚Äôs College, Cambridge",
      "authors": "Christopher John Cornish¬†Hellaby Watkins"
    },
    {
      "index": 55,
      "title": "Developing a Predictive Approach to Knowledge",
      "abstract": "",
      "year": "2015",
      "venue": "PhD thesis, University of Alberta",
      "authors": "Adam White"
    },
    {
      "index": 56,
      "title": "Investigating Practical Linear Temporal Difference Learning ‚Äã‚Äã‚Äã",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Autonomous Agents and Multi-Agent Systems",
      "authors": "Adam White and Martha White",
      "orig_title": "Investigating practical linear temporal difference learning",
      "paper_id": "1602.08771v2"
    },
    {
      "index": 57,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Martha White",
      "orig_title": "Unifying Task Specification in Reinforcement Learning",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 58,
      "title": "Tight performance bounds on greedy policies based on imperfect value functions",
      "abstract": "",
      "year": "1993",
      "venue": "Technical report, Citeseer",
      "authors": "Ronald¬†J. Williams and Leemon¬†C. Baird"
    },
    {
      "index": 59,
      "title": "On Convergence of Emphatic Temporal-Difference Learning",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on Learning Theory",
      "authors": "Huizhen Yu"
    },
    {
      "index": 60,
      "title": "Error bounds for approximations from projected linear equations",
      "abstract": "",
      "year": "2010",
      "venue": "Mathematics of Operations Research",
      "authors": "Huizhen Yu and Dimitri¬†P. Bertsekas"
    }
  ]
}