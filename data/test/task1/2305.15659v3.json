{
  "paper_id": "2305.15659v3",
  "title": "How to escape sharp minima with random perturbations",
  "abstract": "Abstract\nModern machine learning applications have witnessed the remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this design choice, we undertake a formal study that (i) formulates the notion of flat minima, and (ii) studies the complexity of finding them. Specifically, we adopt the trace of the Hessian of the cost function as a measure of flatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima efficiently.\nFor general cost functions, we discuss a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima.\nFor the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, supporting its success in practice.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Finding approximate local minima faster than gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "ACM SIGACT Symposium on Theory of Computing",
      "authors": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma"
    },
    {
      "index": 1,
      "title": "Learning threshold neurons via edge of stability",
      "abstract": "",
      "year": "2023",
      "venue": "Neural Information Processing Systems",
      "authors": "Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang"
    },
    {
      "index": 2,
      "title": "Natasha 2: Faster non-convex optimization than sgd",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Zeyuan Allen-Zhu"
    },
    {
      "index": 3,
      "title": "A Modern Look at the Relationship between Sharpness and Generalization",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.07011",
      "authors": "Maksym Andriushchenko, Francesco Croce, Maximilian MÃ¼ller, Matthias Hein, and Nicolas Flammarion",
      "orig_title": "A modern look at the relationship between sharpness and generalization",
      "paper_id": "2302.07011v2"
    },
    {
      "index": 4,
      "title": "Lower bounds for non-convex stochastic optimization",
      "abstract": "",
      "year": "2023",
      "venue": "Mathematical Programming",
      "authors": "Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth"
    },
    {
      "index": 5,
      "title": "Understanding gradient descent on the edge of stability in deep learning",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi"
    },
    {
      "index": 6,
      "title": "Sharpness-Aware Minimization Improves Language Model Generalization",
      "abstract": "",
      "year": "2022",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Dara Bahri, Hossein Mobahi, and Yi Tay",
      "orig_title": "Sharpness-aware minimization improves language model generalization",
      "paper_id": "2110.08529v2"
    },
    {
      "index": 7,
      "title": "The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.01513",
      "authors": "Peter L Bartlett, Philip M Long, and Olivier Bousquet",
      "orig_title": "The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima",
      "paper_id": "2210.01513v2"
    },
    {
      "index": 8,
      "title": "Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on learning theory",
      "authors": "Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant",
      "orig_title": "Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process",
      "paper_id": "1904.09080v2"
    },
    {
      "index": 9,
      "title": "Accelerated Methods for Non-Convex Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "SIAM Journal on Optimization",
      "authors": "Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford",
      "orig_title": "Accelerated methods for nonconvex optimization",
      "paper_id": "1611.00756v2"
    },
    {
      "index": 10,
      "title": "Lower bounds for finding stationary points i",
      "abstract": "",
      "year": "2020",
      "venue": "Mathematical Programming",
      "authors": "Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford"
    },
    {
      "index": 11,
      "title": "Lower bounds for finding stationary points ii: first-order methods",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematical Programming",
      "authors": "Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford"
    },
    {
      "index": 12,
      "title": "Entropy-SGD: Biasing gradient descent into wide valleys",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Statistical Mechanics: Theory and Experiment",
      "authors": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina"
    },
    {
      "index": 13,
      "title": "An SDE for Modeling SAM: Theory and Insights",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "authors": "Enea Monzio Compagnoni, Luca Biggio, Antonio Orvieto, Frank Norbert Proske, Hans Kersting, and Aurelien Lucchi",
      "orig_title": "An sde for modeling sam: Theory and insights",
      "paper_id": "2301.08203v3"
    },
    {
      "index": 14,
      "title": "Global minima of overparameterized neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Mathematics of Data Science",
      "authors": "Yaim Cooper"
    },
    {
      "index": 15,
      "title": "The Crucial Role of Normalization in Sharpness-Aware Minimization",
      "abstract": "",
      "year": "2023",
      "venue": "Neural Information Processing Systems",
      "authors": "Yan Dai, Kwangjun Ahn, and Suvrit Sra",
      "orig_title": "The crucial role of normalization in sharpness-aware minimization",
      "paper_id": "2305.15287v2"
    },
    {
      "index": 16,
      "title": "Label Noise SGD Provably Prefers Flat Global Minimizers",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Alex Damian, Tengyu Ma, and Jason D. Lee",
      "orig_title": "Label noise SGD provably prefers flat global minimizers",
      "paper_id": "2106.06530v2"
    },
    {
      "index": 17,
      "title": "Self-stabilization: The implicit bias of gradient descent at the edge of stability",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Learning Representations",
      "authors": "Alex Damian, Eshaan Nichani, and Jason D. Lee"
    },
    {
      "index": 18,
      "title": "Escaping Saddles with Stochastic Gradients",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann",
      "orig_title": "Escaping saddles with stochastic gradients",
      "paper_id": "1803.05999v2"
    },
    {
      "index": 19,
      "title": "Flat minima generalize for low-rank matrix recovery",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.03756",
      "authors": "Lijun Ding, Dmitriy Drusvyatskiy, and Maryam Fazel",
      "orig_title": "Flat minima generalize for low-rank matrix recovery",
      "paper_id": "2203.03756v2"
    },
    {
      "index": 20,
      "title": "Sharp Minima Can Generalize For Deep Nets",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio",
      "orig_title": "Sharp minima can generalize for deep nets",
      "paper_id": "1703.04933v2"
    },
    {
      "index": 21,
      "title": "Essentially No Barriers in Neural Network Energy Landscape",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht",
      "orig_title": "Essentially no barriers in neural network energy landscape",
      "paper_id": "1803.00885v5"
    },
    {
      "index": 22,
      "title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Yoel Drori and Ohad Shamir",
      "orig_title": "The complexity of finding stationary points with stochastic gradient descent",
      "paper_id": "1910.01845v3"
    },
    {
      "index": 23,
      "title": "Randomized smoothing for stochastic optimization",
      "abstract": "",
      "year": "2012",
      "venue": "SIAM Journal on Optimization",
      "authors": "John C Duchi, Peter L Bartlett, and Martin J Wainwright"
    },
    {
      "index": 24,
      "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.11008",
      "authors": "Gintare Karolina Dziugaite and Daniel M Roy",
      "orig_title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
      "paper_id": "1703.11008v2"
    },
    {
      "index": 25,
      "title": "Spider: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang",
      "orig_title": "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator",
      "paper_id": "1807.01695v2"
    },
    {
      "index": 26,
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur",
      "orig_title": "Sharpness-aware minimization for efficiently improving generalization",
      "paper_id": "2010.01412v3"
    },
    {
      "index": 27,
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.10026",
      "authors": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson",
      "orig_title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
      "paper_id": "1802.10026v4"
    },
    {
      "index": 28,
      "title": "What is the inductive bias of flatness regularization? a study of deep matrix factorization models",
      "abstract": "",
      "year": "2023",
      "venue": "Neural Information Processing Systems",
      "authors": "Khashayar Gatmiry, Zhiyuan Li, Tengyu Ma, Sashank J. Reddi, Stefanie Jegelka, and Ching-Yao Chuang"
    },
    {
      "index": 29,
      "title": "Escaping From Saddle Points â Online Stochastic Gradient for Tensor Decomposition",
      "abstract": "",
      "year": "2015",
      "venue": "Conference on learning theory",
      "authors": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan",
      "orig_title": "Escaping from saddle pointsâonline stochastic gradient for tensor decomposition",
      "paper_id": "1503.02101v1"
    },
    {
      "index": 30,
      "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
      "abstract": "",
      "year": "2013",
      "venue": "SIAM Journal on Optimization",
      "authors": "Saeed Ghadimi and Guanghui Lan"
    },
    {
      "index": 31,
      "title": "Why (and When) does Local SGD Generalize Better than SGD?",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Learning Representations",
      "authors": "Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora",
      "orig_title": "Why (and when) does local SGD generalize better than SGD?",
      "paper_id": "2303.01215v2"
    },
    {
      "index": 32,
      "title": "Asymmetric Valleys: Beyond Sharp and Flat Local Minima",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.00744",
      "authors": "Haowei He, Gao Huang, and Yang Yuan",
      "orig_title": "Asymmetric valleys: Beyond sharp and flat local minima",
      "paper_id": "1902.00744v2"
    },
    {
      "index": 33,
      "title": "Flat minima",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Sepp Hochreiter and JÃ¼rgen Schmidhuber"
    },
    {
      "index": 34,
      "title": "Averaging Weights Leads to Wider Optima and Better Generalization",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.05407",
      "authors": "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson",
      "orig_title": "Averaging weights leads to wider optima and better generalization",
      "paper_id": "1803.05407v3"
    },
    {
      "index": 35,
      "title": "How to Escape Saddle Points Efficiently",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan",
      "orig_title": "How to escape saddle points efficiently",
      "paper_id": "1703.00887v1"
    },
    {
      "index": 36,
      "title": "On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of the ACM (JACM)",
      "authors": "Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan",
      "orig_title": "On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points",
      "paper_id": "1902.04811v2"
    },
    {
      "index": 37,
      "title": "When Do Flat Minima Optimizers Work?",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner",
      "orig_title": "When do flat minima optimizers work?",
      "paper_id": "2202.00661v5"
    },
    {
      "index": 38,
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Åojasiewicz Condition",
      "abstract": "",
      "year": "2016",
      "venue": "Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD",
      "authors": "Hamed Karimi, Julie Nutini, and Mark Schmidt",
      "orig_title": "Linear convergence of gradient and proximal-gradient methods under the polyak-Åojasiewicz condition",
      "paper_id": "1608.04636v4"
    },
    {
      "index": 39,
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings on",
      "authors": "Simran Kaur, Jeremy Cohen, and Zachary Chase Lipton",
      "orig_title": "On the maximum hessian eigenvalue and generalization",
      "paper_id": "2206.10654v3"
    },
    {
      "index": 40,
      "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail Smelyanskiy"
    },
    {
      "index": 41,
      "title": "What Happens after SGD Reaches Zero Loss? âA Mathematical Framework",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhiyuan Li, Tianhao Wang, and Sanjeev Arora",
      "orig_title": "What happens after SGD reaches zero loss? âa mathematical framework",
      "paper_id": "2110.06914v4"
    },
    {
      "index": 42,
      "title": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "authors": "Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma",
      "orig_title": "Same pre-training loss, better downstream: Implicit bias matters for language models",
      "paper_id": "2210.14199v1"
    },
    {
      "index": 43,
      "title": "Bad Global Minima Exist and SGD Can Reach Them",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas",
      "orig_title": "Bad global minima exist and SGD can reach them",
      "paper_id": "1906.02613v2"
    },
    {
      "index": 44,
      "title": "On linear stability of SGD and input-smoothness of neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chao Ma and Lexing Ying"
    },
    {
      "index": 45,
      "title": "Unique properties of flat minima in deep networks",
      "abstract": "",
      "year": "2020",
      "venue": "International conference on machine learning",
      "authors": "Rotem Mulayoff and Tomer Michaeli"
    },
    {
      "index": 46,
      "title": "Exploring Generalization in Deep Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro",
      "orig_title": "Exploring generalization in deep learning",
      "paper_id": "1706.08947v2"
    },
    {
      "index": 47,
      "title": "Diametrical risk minimization: Theory and computations",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning",
      "authors": "Matthew D Norton and Johannes O Royset"
    },
    {
      "index": 48,
      "title": "Stochastic variance reduction for nonconvex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola"
    },
    {
      "index": 49,
      "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.04454",
      "authors": "Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou",
      "orig_title": "Empirical analysis of the hessian of over-parametrized neural networks",
      "paper_id": "1706.04454v3"
    },
    {
      "index": 50,
      "title": "Normalized Flat Minima: Exploring Scale-Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama",
      "orig_title": "Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis",
      "paper_id": "1901.04653v2"
    },
    {
      "index": 51,
      "title": "Large learning rate tames homogeneity: Convergence and balancing effect",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao"
    },
    {
      "index": 52,
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2211.05729",
      "authors": "Kaiyue Wen, Tengyu Ma, and Zhiyuan Li",
      "orig_title": "How does sharpness-aware minimization minimize sharpness?",
      "paper_id": "2211.05729v2"
    },
    {
      "index": 53,
      "title": "Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization",
      "abstract": "",
      "year": "2023",
      "venue": "Neural Information Processing Systems",
      "authors": "Kaiyue Wen, Zhiyuan Li, and Tengyu Ma",
      "orig_title": "Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization",
      "paper_id": "2307.11007v2"
    },
    {
      "index": 54,
      "title": "Adversarial Weight Perturbation Helps Robust Generalization",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dongxian Wu, Shu-Tao Xia, and Yisen Wang",
      "orig_title": "Adversarial weight perturbation helps robust generalization",
      "paper_id": "2004.05884v2"
    },
    {
      "index": 55,
      "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Zeke Xie, Issei Sato, and Masashi Sugiyama",
      "orig_title": "A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima",
      "paper_id": "2002.03495v14"
    },
    {
      "index": 56,
      "title": "Hessian-based Analysis of Large Batch Training and Robustness to Adversaries",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney",
      "orig_title": "Hessian-based analysis of large batch training and robustness to adversaries",
      "paper_id": "1802.08241v4"
    },
    {
      "index": 57,
      "title": "Understanding deep learning (still) requires rethinking generalization",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM",
      "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals"
    },
    {
      "index": 58,
      "title": "Regularizing Neural Networks via Adversarial Model Perturbation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Yaowei Zheng, Richong Zhang, and Yongyi Mao",
      "orig_title": "Regularizing neural networks via adversarial model perturbation",
      "paper_id": "2010.04925v4"
    },
    {
      "index": 59,
      "title": "Stochastic Recursive Variance-Reduced Cubic Regularization Methods",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Dongruo Zhou and Quanquan Gu",
      "orig_title": "Stochastic recursive variance-reduced cubic regularization methods",
      "paper_id": "1901.11518v2"
    }
  ]
}