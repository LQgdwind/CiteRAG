{
  "paper_id": "2303.06654v1",
  "title": "Twice Regularized Markov Decision Processes: The Equivalence between Robustness and Regularization",
  "abstract": "Abstract\nRobust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this significantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics. In this work, we aim to learn robust MDPs using regularization. We first show that regularized MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We then generalize regularized MDPs to twice regularized MDPs (R2 MDPs), i.e., MDPs with both value and policy regularization. The corresponding Bellman operators enable us to derive planning and learning schemes with convergence and generalization guarantees, thus reducing robustness to regularization. We numerically show this two-fold advantage on tabular and physical domains, highlighting the fact that R2 preserves its efficacy in continuous environments.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Kishan Panaganti Badrinath and Dileep Kalathil",
      "orig_title": "Robust reinforcement learning using least squares policy iteration with provable performance guarantees",
      "paper_id": "2006.11608v4"
    },
    {
      "index": 1,
      "title": "Solving uncertain Markov decision processes",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "J Andrew Bagnell, Andrew Y Ng, and Jeff G Schneider"
    },
    {
      "index": 2,
      "title": "Fast algorithms for l‚àûsubscriptùëôl_{\\infty}-constrained s-rectangular robust MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Bahram Behzadian, Marek Petrik, and Chin Pang Ho"
    },
    {
      "index": 3,
      "title": "Convex optimization theory",
      "abstract": "",
      "year": "2009",
      "venue": "Athena Scientific Belmont",
      "authors": "Dimitri P Bertsekas"
    },
    {
      "index": 4,
      "title": "Robust and Adaptive Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "Dynamic Ideas LLC",
      "authors": "Dimitris Bertsimas and Dick Den Hertog"
    },
    {
      "index": 5,
      "title": "Convex analysis and nonlinear optimization: theory and examples",
      "abstract": "",
      "year": "2010",
      "venue": "Springer Science & Business Media",
      "authors": "Jonathan Borwein and Adrian S Lewis"
    },
    {
      "index": 6,
      "title": "Your Policy Regularizer is Secretly an Adversary",
      "abstract": "",
      "year": "2022",
      "venue": "Transactions on Machine Learning Research (TMLR)",
      "authors": "Rob Brekelmans, Tim Genewein, Jordi Grau-Moya, Gr√©goire Del√©tang, Markus Kunesch, Shane Legg, and Pedro Ortega",
      "orig_title": "Your policy regularizer is secretly an adversary",
      "paper_id": "2203.12592v4"
    },
    {
      "index": 7,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 8,
      "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi",
      "orig_title": "Fast global convergence of natural policy gradient methods with entropy regularization",
      "paper_id": "2007.06558v5"
    },
    {
      "index": 9,
      "title": "Solving Linear Programs in the Current Matrix Multiplication Time",
      "abstract": "",
      "year": "2019",
      "venue": "ACM SIGACT Symposium on Theory of Computing",
      "authors": "Michael B Cohen, Yin Tat Lee, and Zhao Song",
      "orig_title": "Solving linear programs in the current matrix multiplication time",
      "paper_id": "1810.07896v3"
    },
    {
      "index": 10,
      "title": "Distributional Robustness and Regularization in Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning Workshop",
      "authors": "Esther Derman and Shie Mannor",
      "orig_title": "Distributional robustness and regularization in reinforcement learning",
      "paper_id": "2003.02894v2"
    },
    {
      "index": 11,
      "title": "Soft-robust actor-critic policy-gradient",
      "abstract": "",
      "year": "2018",
      "venue": "Association for Uncertainty in Artificial Intelligence",
      "authors": "Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor"
    },
    {
      "index": 12,
      "title": "Acting in delayed environments with non-stationary Markov policies",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Esther Derman, Gal Dalal, and Shie Mannor"
    },
    {
      "index": 13,
      "title": "Twice regularized MDPs and the equivalence between robustness and regularization",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Esther Derman, Matthieu Geist, and Shie Mannor",
      "orig_title": "Twice regularized MDPs and the equivalence between robustness and regularization",
      "paper_id": "2110.06267v1"
    },
    {
      "index": 14,
      "title": "Efficient projections onto the l1subscriptùëô1l_{1}-ball for learning in high dimensions",
      "abstract": "",
      "year": "2008",
      "venue": "International Conference on Machine Learning",
      "authors": "John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra"
    },
    {
      "index": 15,
      "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Benjamin Eysenbach and Sergey Levine",
      "orig_title": "Maximum entropy RL (provably) solves some robust RL problems",
      "paper_id": "2103.06257v2"
    },
    {
      "index": 16,
      "title": "A Theory of Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Matthieu Geist, Bruno Scherrer, and Olivier Pietquin",
      "orig_title": "A theory of regularized Markov decision processes",
      "paper_id": "1901.11275v2"
    },
    {
      "index": 17,
      "title": "Robust Markov decision processes: Beyond rectangularity",
      "abstract": "",
      "year": "2022",
      "venue": "Mathematics of Operations Research",
      "authors": "Vineet Goyal and Julien Grand-Clement"
    },
    {
      "index": 18,
      "title": "Scalable First-Order Methods for Robust MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Julien Grand-Cl√©ment and Christian Kroer",
      "orig_title": "Scalable first-order methods for robust MDPs",
      "paper_id": "2005.05434v5"
    },
    {
      "index": 19,
      "title": "Reinforcement Learning with Deep Energy-Based Policies",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Reinforcement learning with deep energy-based policies",
      "paper_id": "1702.08165v2"
    },
    {
      "index": 20,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 21,
      "title": "Deep Reinforcement Learning with Double Q-learning",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Hado van Hasselt, Arthur Guez, and David Silver",
      "orig_title": "Deep reinforcement learning with double q-learning",
      "paper_id": "1509.06461v3"
    },
    {
      "index": 22,
      "title": "The elements of statistical learning: data mining, inference, and prediction, volume 2",
      "abstract": "",
      "year": "2009",
      "venue": "Springer",
      "authors": "Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman"
    },
    {
      "index": 23,
      "title": "Fundamentals of convex analysis",
      "abstract": "",
      "year": "2004",
      "venue": "Springer Science & Business Media",
      "authors": "Jean-Baptiste Hiriart-Urruty and Claude Lemarechal"
    },
    {
      "index": 24,
      "title": "Fast Bellman updates for robust MDPs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann"
    },
    {
      "index": 25,
      "title": "Partial policy iteration for l1subscriptùëô1l_{1}-robust Markov decision processes",
      "abstract": "",
      "year": "2021",
      "venue": "J. Mach. Learn. Res.",
      "authors": "Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann"
    },
    {
      "index": 26,
      "title": "Robust œïitalic-œï\\phi-divergence MDPs",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.14202",
      "authors": "Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann"
    },
    {
      "index": 27,
      "title": "Regularized Policies are Reward Robust",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Hisham Husain, Kamil Ciosek, and Ryota Tomioka",
      "orig_title": "Regularized policies are reward robust",
      "paper_id": "2101.07012v1"
    },
    {
      "index": 28,
      "title": "Robust dynamic programming",
      "abstract": "",
      "year": "2005",
      "venue": "Mathematics of Operations Research",
      "authors": "Garud N Iyengar"
    },
    {
      "index": 29,
      "title": "Convergence of stochastic iterative dynamic programming algorithms",
      "abstract": "",
      "year": "1993",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tommi Jaakkola, Michael Jordan, and Satinder Singh"
    },
    {
      "index": 30,
      "title": "Approximately optimal approximate reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "International Conference on Machine Learning",
      "authors": "Sham Kakade and John Langford"
    },
    {
      "index": 31,
      "title": "Robust modified policy iteration",
      "abstract": "",
      "year": "2013",
      "venue": "INFORMS Journal on Computing",
      "authors": "David L Kaufman and Andrew J Schaefer"
    },
    {
      "index": 32,
      "title": "Wasserstein distributionally robust optimization: Theory and applications in machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "Operations Research & Management Science in the Age of Analytics",
      "authors": "Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh"
    },
    {
      "index": 33,
      "title": "Efficient Policy Iteration for Robust Markov Decision Processes via Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.14327",
      "authors": "Navdeep Kumar, Kfir Levy, Kaixin Wang, and Shie Mannor",
      "orig_title": "Efficient policy iteration for robust Markov decision processes via regularization",
      "paper_id": "2205.14327v2"
    },
    {
      "index": 34,
      "title": "Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy Regularization for Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Robotics and Automation Letters",
      "authors": "Kyungjae Lee, Sungjoon Choi, and Songhwai Oh",
      "orig_title": "Sparse Markov decision processes with causal sparse Tsallis entropy regularization for reinforcement learning",
      "paper_id": "1709.06293v3"
    },
    {
      "index": 35,
      "title": "Robust Value Iteration for Continuous Control Tasks",
      "abstract": "",
      "year": "2021",
      "venue": "Robotics: Science and Systems",
      "authors": "Michael Lutter, Shie Mannor, Jan Peters, Dieter Fox, and Animesh Garg",
      "orig_title": "Robust value iteration for continuous control tasks",
      "paper_id": "2105.12189v1"
    },
    {
      "index": 36,
      "title": "Learning Robust Options",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Daniel Mankowitz, Timothy Mann, Pierre-Luc Bacon, Doina Precup, and Shie Mannor",
      "orig_title": "Learning robust options",
      "paper_id": "1802.03236v1"
    },
    {
      "index": 37,
      "title": "Bias and variance approximation in value function estimates",
      "abstract": "",
      "year": "2007",
      "venue": "Management Science",
      "authors": "Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis"
    },
    {
      "index": 38,
      "title": "Lightning does not strike twice: Robust MDPs with coupled uncertainty",
      "abstract": "",
      "year": "2012",
      "venue": "International Conference on Machine Learning",
      "authors": "Shie Mannor, Ofir Mebel, and Huan Xu"
    },
    {
      "index": 39,
      "title": "Robust MDPs with k-rectangular uncertainty",
      "abstract": "",
      "year": "2016",
      "venue": "Mathematics of Operations Research",
      "authors": "Shie Mannor, Ofir Mebel, and Huan Xu"
    },
    {
      "index": 40,
      "title": "Differentiable dynamic programming for structured prediction and attention",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Arthur Mensch and Mathieu Blondel"
    },
    {
      "index": 41,
      "title": "Reinforcement learning via Fenchel-Rockafellar duality",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.01866",
      "authors": "Ofir Nachum and Bo Dai"
    },
    {
      "index": 42,
      "title": "Robust control of Markov decision processes with uncertain transition matrices",
      "abstract": "",
      "year": "2005",
      "venue": "Operations Research",
      "authors": "Arnab Nilim and Laurent El Ghaoui"
    },
    {
      "index": 43,
      "title": "The Rayleigh quotient iteration and some generalizations for nonnormal matrices",
      "abstract": "",
      "year": "1974",
      "venue": "Mathematics of Computation",
      "authors": "Beresford N Parlett"
    },
    {
      "index": 44,
      "title": "Robust Adversarial Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta",
      "orig_title": "Robust adversarial reinforcement learning",
      "paper_id": "1703.02702v1"
    },
    {
      "index": 45,
      "title": "Markov decision processes: discrete stochastic dynamic programming",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Martin L Puterman"
    },
    {
      "index": 46,
      "title": "An elementary proof that q-learning converges almost surely",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2108.02827",
      "authors": "Matthew T Regehr and Alex Ayoub"
    },
    {
      "index": 47,
      "title": "Convex analysis, volume 36",
      "abstract": "",
      "year": "1970",
      "venue": "Princeton university press",
      "authors": "R Tyrrell Rockafellar"
    },
    {
      "index": 48,
      "title": "Reinforcement Learning under Model Mismatch",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Aurko Roy, Huan Xu, and Sebastian Pokutta",
      "orig_title": "Reinforcement learning under model mismatch",
      "paper_id": "1706.04711v2"
    },
    {
      "index": 49,
      "title": "Approximate modified policy iteration and its application to the game of Tetris",
      "abstract": "",
      "year": "2015",
      "venue": "J. Mach. Learn. Res.",
      "authors": "Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist"
    },
    {
      "index": 50,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 51,
      "title": "Distributionally robust logistic regression",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn"
    },
    {
      "index": 52,
      "title": "Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Lior Shani, Yonathan Efroni, and Shie Mannor",
      "orig_title": "Adaptive trust region policy optimization: Global convergence and faster rates for regularized MDPs",
      "paper_id": "1909.02769v2"
    },
    {
      "index": 53,
      "title": "Deep Robust Kalman Filter",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1703.02310",
      "authors": "Shirli Di-Castro Shashua and Shie Mannor",
      "orig_title": "Deep robust Kalman filter",
      "paper_id": "1703.02310v1"
    },
    {
      "index": 54,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al."
    },
    {
      "index": 55,
      "title": "Scaling up robust MDPs using function approximation",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Aviv Tamar, Shie Mannor, and Huan Xu"
    },
    {
      "index": 56,
      "title": "Reward Constrained Policy Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Chen Tessler, Daniel J Mankowitz, and Shie Mannor",
      "orig_title": "Reward constrained policy optimization",
      "paper_id": "1805.11074v3"
    },
    {
      "index": 57,
      "title": "Action Robust Reinforcement Learning and Applications in Continuous Control",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Chen Tessler, Yonathan Efroni, and Shie Mannor",
      "orig_title": "Action robust reinforcement learning and applications in continuous control",
      "paper_id": "1901.09184v2"
    },
    {
      "index": 58,
      "title": "Deep conservative policy iteration",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Nino Vieillard, Olivier Pietquin, and Matthieu Geist"
    },
    {
      "index": 59,
      "title": "Policy Gradient Method For Robust Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Yue Wang and Shaofeng Zou",
      "orig_title": "Policy gradient method for robust reinforcement learning",
      "paper_id": "2205.07344v1"
    },
    {
      "index": 60,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Christopher JCH Watkins and Peter Dayan"
    },
    {
      "index": 61,
      "title": "Robust Markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "Mathematics of Operations Research",
      "authors": "Wolfram Wiesemann, Daniel Kuhn, and Ber√ß Rustem"
    },
    {
      "index": 62,
      "title": "Robustness and regularization of support vector machines",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of machine learning research",
      "authors": "Huan Xu, Constantine Caramanis, and Shie Mannor"
    },
    {
      "index": 63,
      "title": "Non-asymptotic performances of Robust Markov Decision Processes",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.03863",
      "authors": "Wenhao Yang and Zhihua Zhang"
    },
    {
      "index": 64,
      "title": "Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh",
      "orig_title": "Robust deep reinforcement learning against adversarial perturbations on state observations",
      "paper_id": "2003.08938v7"
    },
    {
      "index": 65,
      "title": "Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2207.02016",
      "authors": "Yuan Zhang, Jianhong Wang, and Joschka Boedecker",
      "orig_title": "Robust reinforcement learning in continuous control tasks with uncertainty set regularization",
      "paper_id": "2207.02016v4"
    }
  ]
}