{
  "paper_id": "2210.01007v1",
  "title": "Reward Learning with Trees: Methods and Evaluation",
  "abstract": "Abstract\nRecent efforts to learn reward functions from human feedback have tended to use deep neural networks, whose lack of transparency hampers our ability to explain agent behaviour or verify alignment. We\nexplore the merits of\nlearning intrinsically interpretable tree models instead. We develop a recently proposed method for learning reward trees from preference labels, and show it to be broadly competitive with neural networks on challenging high-dimensional tasks, with good robustness to limited or corrupted data. Having found that reward tree learning can be done effectively in complex settings, we then consider why it should be used, demonstrating that the interpretable reward structure gives significant scope for traceability, verification and explanation.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Highlights: Summarizing agent behavior to people",
      "abstract": "",
      "year": "2018",
      "venue": "17th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Dan Amir and Ofra Amir"
    },
    {
      "index": 1,
      "title": "Pitfalls of learning a reward function online",
      "abstract": "",
      "year": "2020",
      "venue": "Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20",
      "authors": "Stuart Armstrong, Jan Leike, Laurent Orseau, and Shane Legg"
    },
    {
      "index": 2,
      "title": "Learning robot objectives from physical human interaction",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Robot Learning",
      "authors": "Andrea Bajcsy, Dylan P Losey, Marcia K O’Malley, and Anca D Dragan"
    },
    {
      "index": 3,
      "title": "Verifiable Reinforcement Learning via Policy Extraction",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Osbert Bastani, Yewen Pu, and Armando Solar-Lezama",
      "orig_title": "Verifiable reinforcement learning via policy extraction",
      "paper_id": "1805.08328v2"
    },
    {
      "index": 4,
      "title": "Interpretable Preference-based Reinforcement Learning with Tree-Structured Reward Functions",
      "abstract": "",
      "year": "2022",
      "venue": "21st International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Tom Bewley and Freddy Lecue",
      "orig_title": "Interpretable preference-based reinforcement learning with tree-structured reward functions",
      "paper_id": "2112.11230v1"
    },
    {
      "index": 5,
      "title": "Summarising and Comparing Agent Dynamics with Contrastive Spatiotemporal Abstraction",
      "abstract": "",
      "year": "2022",
      "venue": "IJCAI/ECAI Workshop on Explainable Artificial Intelligence",
      "authors": "Tom Bewley, Jonathan Lawry, and Arthur Richards",
      "orig_title": "Summarising and comparing agent dynamics with contrastive spatiotemporal abstraction",
      "paper_id": "2201.07749v2"
    },
    {
      "index": 6,
      "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
      "abstract": "",
      "year": "1952",
      "venue": "Biometrika",
      "authors": "Ralph Allan Bradley and Milton E Terry"
    },
    {
      "index": 7,
      "title": "Classification and regression trees",
      "abstract": "",
      "year": "2017",
      "venue": "Routledge",
      "authors": "Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone"
    },
    {
      "index": 8,
      "title": "Weak Human Preference Supervision For Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Zehong Cao, KaiChiu Wong, and Chin-Teng Lin",
      "orig_title": "Weak human preference supervision for deep reinforcement learning",
      "paper_id": "2007.12904v2"
    },
    {
      "index": 9,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei",
      "orig_title": "Deep reinforcement learning from human preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 10,
      "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine",
      "orig_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
      "paper_id": "1805.12114v2"
    },
    {
      "index": 11,
      "title": "Automatic task decomposition and state abstraction from demonstration",
      "abstract": "",
      "year": "2012",
      "venue": "Georgia Institute of Technology",
      "authors": "Luis C Cobo, Charles L Isbell Jr, and Andrea L Thomaz"
    },
    {
      "index": 12,
      "title": "Distilling deep reinforcement learning policies in soft decision trees",
      "abstract": "",
      "year": "2019",
      "venue": "IJCAI/ECAI Workshop on Explainable Artificial Intelligence",
      "authors": "Youri Coppens, Kyriakos Efthymiadis, Tom Lenaerts, and Ann Nowé"
    },
    {
      "index": 13,
      "title": "Deep reinforcement learning monitor for snapshot recording",
      "abstract": "",
      "year": "2018",
      "venue": "17th IEEE International Conference on Machine Learning and Applications (ICMLA)",
      "authors": "Giang Dao, Indrajeet Mishra, and Minwoo Lee"
    },
    {
      "index": 14,
      "title": "Multidimensional scaling using majorization: Smacof in r",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of statistical software",
      "authors": "Jan De Leeuw and Patrick Mair"
    },
    {
      "index": 15,
      "title": "Explicable reward design for reinforcement learning agents",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Rati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla"
    },
    {
      "index": 16,
      "title": "Active Preference Learning with Discrete Choice Data",
      "abstract": "",
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Brochu Eric, Nando Freitas, and Abhijeet Ghosh"
    },
    {
      "index": 17,
      "title": "A Survey on Interpretable Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2112.13112",
      "authors": "Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu",
      "orig_title": "A survey on interpretable reinforcement learning",
      "paper_id": "2112.13112v2"
    },
    {
      "index": 18,
      "title": "Quantifying Differences in Reward Functions",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Adam Gleave, Michael D Dennis, Shane Legg, Stuart Russell, and Jan Leike",
      "orig_title": "Quantifying differences in reward functions",
      "paper_id": "2006.13900v3"
    },
    {
      "index": 19,
      "title": "Policy shaping: Integrating human feedback with reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in neural information processing systems",
      "authors": "Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz"
    },
    {
      "index": 20,
      "title": "A least squares solution for paired comparisons with incomplete data",
      "abstract": "",
      "year": "1956",
      "venue": "Psychometrika",
      "authors": "Harold Gulliksen"
    },
    {
      "index": 21,
      "title": "Experimental design under the bradley-terry model",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "Yuan Guo, Peng Tian, Jayashree Kalpathy-Cramer, Susan Ostmo, J Peter Campbell, Michael F Chiang, Deniz Erdogmus, Jennifer G Dy, and Stratis Ioannidis"
    },
    {
      "index": 22,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 23,
      "title": "Explainability in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Knowledge-Based Systems",
      "authors": "Alexandre Heuillet, Fabien Couthouis, and Natalia Díaz-Rodríguez",
      "orig_title": "Explainability in deep reinforcement learning",
      "paper_id": "2008.06693v4"
    },
    {
      "index": 24,
      "title": "Explainable ai planning (xaip): overview and the case of contrastive explanation",
      "abstract": "",
      "year": "2019",
      "venue": "Reasoning Web. Explainable Artificial Intelligence",
      "authors": "Jörg Hoffmann and Daniele Magazzeni"
    },
    {
      "index": 25,
      "title": "Establishing Appropriate Trust via Critical States",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "Sandy H Huang, Kush Bhatia, Pieter Abbeel, and Anca D Dragan",
      "orig_title": "Establishing appropriate trust via critical states",
      "paper_id": "1810.08174v1"
    },
    {
      "index": 26,
      "title": "Enhancing explainability of deep reinforcement learning through selective layer-wise relevance propagation",
      "abstract": "",
      "year": "2019",
      "venue": "Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz)",
      "authors": "Tobias Huber, Dominik Schiller, and Elisabeth André"
    },
    {
      "index": 27,
      "title": "Preprocessing reward functions for interpretability",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.13553",
      "authors": "Erik Jenner and Adam Gleave"
    },
    {
      "index": 28,
      "title": "An experience replay method based on tree structure for reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": "Wei-Cheng Jiang, Kao-Shing Hwang, and Jin-Ling Lin"
    },
    {
      "index": 29,
      "title": "Explainable reinforcement learning via reward decomposition",
      "abstract": "",
      "year": "2019",
      "venue": "IJCAI/ECAI Workshop on Explainable Artificial Intelligence",
      "authors": "Zoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez"
    },
    {
      "index": 30,
      "title": "A new measure of rank correlation",
      "abstract": "",
      "year": "1938",
      "venue": "Biometrika",
      "authors": "Maurice G Kendall"
    },
    {
      "index": 31,
      "title": "Rank Correlation Methods; Griffin, C., Ed",
      "abstract": "",
      "year": "1975",
      "venue": "",
      "authors": "MG Kendall"
    },
    {
      "index": 32,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1412.6980",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 33,
      "title": "Tamer: Training an agent manually via evaluative reinforcement",
      "abstract": "",
      "year": "2008",
      "venue": "7th IEEE international conference on development and learning",
      "authors": "W Bradley Knox and Peter Stone"
    },
    {
      "index": 34,
      "title": "Cognitive shadow: A policy capturing tool to support naturalistic decision making",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support (CogSIMA)",
      "authors": "Daniel Lafond, Sébastien Tremblay, and Simon Banbury"
    },
    {
      "index": 35,
      "title": "B-pref: Benchmarking preference-based reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)",
      "authors": "Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel"
    },
    {
      "index": 36,
      "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Kimin Lee, Laura M Smith, and Pieter Abbeel",
      "orig_title": "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
      "paper_id": "2106.05091v1"
    },
    {
      "index": 37,
      "title": "Scalable agent alignment via reward modeling: a research direction",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.07871",
      "authors": "Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg",
      "orig_title": "Scalable agent alignment via reward modeling: a research direction",
      "paper_id": "1811.07871v1"
    },
    {
      "index": 38,
      "title": "Information directed reward learning for reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, and Andreas Krause"
    },
    {
      "index": 39,
      "title": "Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees",
      "abstract": "",
      "year": "2018",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Guiliang Liu, Oliver Schulte, Wang Zhu, and Qingcan Li",
      "orig_title": "Toward interpretable deep reinforcement learning with linear model u-trees",
      "paper_id": "1807.05887v1"
    },
    {
      "index": 40,
      "title": "Understanding Learned Reward Functions",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.05862",
      "authors": "Eric J Michaud, Adam Gleave, and Stuart Russell",
      "orig_title": "Understanding learned reward functions",
      "paper_id": "2012.05862v1"
    },
    {
      "index": 41,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "Icml",
      "authors": "Andrew Y Ng, Stuart Russell, et al."
    },
    {
      "index": 42,
      "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Alexander Pan, Kush Bhatia, and Jacob Steinhardt",
      "orig_title": "The effects of reward misspecification: Mapping and mitigating misaligned models",
      "paper_id": "2201.03544v2"
    },
    {
      "index": 43,
      "title": "Explainable Reinforcement Learning: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "International cross-domain conference for machine learning and knowledge extraction",
      "authors": "Erika Puiutta and Eric Veith",
      "orig_title": "Explainable reinforcement learning: A survey",
      "paper_id": "2005.06247v1"
    },
    {
      "index": 44,
      "title": "Safe Deep RL in 3D Environments using Human Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.08102",
      "authors": "Matthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, and Jan Leike",
      "orig_title": "Safe deep rl in 3d environments using human feedback",
      "paper_id": "2201.08102v2"
    },
    {
      "index": 45,
      "title": "Learning Human Objectives by Evaluating Hypothetical Behavior",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike",
      "orig_title": "Learning human objectives by evaluating hypothetical behavior",
      "paper_id": "1912.05652v2"
    },
    {
      "index": 46,
      "title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.01180",
      "authors": "Aaron M Roth, Nicholay Topin, Pooyan Jamshidi, and Manuela Veloso",
      "orig_title": "Conservative q-improvement: Reinforcement learning for an interpretable decision-tree policy",
      "paper_id": "1907.01180v1"
    },
    {
      "index": 47,
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "abstract": "",
      "year": "2019",
      "venue": "Nature Machine Intelligence",
      "authors": "Cynthia Rudin"
    },
    {
      "index": 48,
      "title": "Explaining reward functions in Markov decision processes",
      "abstract": "",
      "year": "2019",
      "venue": "Thirty-Second International Flairs Conference",
      "authors": "Jacob Russell and Eugene Santos"
    },
    {
      "index": 49,
      "title": "Human compatible: Artificial intelligence and the problem of control",
      "abstract": "",
      "year": "2019",
      "venue": "Penguin",
      "authors": "Stuart Russell"
    },
    {
      "index": 50,
      "title": "Active preference-based learning of reward functions",
      "abstract": "",
      "year": "2017",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia"
    },
    {
      "index": 51,
      "title": "An empirical study of reward explanations with human-robot interaction applications",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Robotics and Automation Letters",
      "authors": "Lindsay Sanneman and Julie A Shah"
    },
    {
      "index": 52,
      "title": "Optimization methods for interpretable differentiable decision trees applied to reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Twenty Third International Conference on Artificial Intelligence and Statistics",
      "authors": "Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son"
    },
    {
      "index": 53,
      "title": "Tacit knowledge in professional practice: Researcher and practitioner perspectives",
      "abstract": "",
      "year": "1999",
      "venue": "Psychology Press",
      "authors": "Robert J Sternberg and Joseph A Horvath"
    },
    {
      "index": 54,
      "title": "Specifying and interpreting reinforcement learning policies through simulatable machine learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.07140",
      "authors": "Pradyumna Tambwekar, Andrew Silva, Nakul Gopalan, and Matthew Gombolay"
    },
    {
      "index": 55,
      "title": "A study of causal confusion in preference-based reward learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.06601",
      "authors": "Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D Dragan, and Daniel Brown"
    },
    {
      "index": 56,
      "title": "Contrastive explanations for reinforcement learning in terms of expected consequences",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI/ECAI Workshop on Explainable Artificial Intelligence",
      "authors": "Jasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, and Mark Neerincx",
      "orig_title": "Contrastive explanations for reinforcement learning in terms of expected consequences",
      "paper_id": "1807.08706v1"
    },
    {
      "index": 57,
      "title": "Programmatically Interpretable Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri",
      "orig_title": "Programmatically interpretable reinforcement learning",
      "paper_id": "1804.02477v3"
    },
    {
      "index": 58,
      "title": "Improving user specifications for robot behavior through active preference learning: Framework and evaluation",
      "abstract": "",
      "year": "2020",
      "venue": "International Journal of Robotics Research",
      "authors": "Nils Wilde, Alexandru Blidaru, Stephen L Smith, and Dana Kulić"
    },
    {
      "index": 59,
      "title": "Model-free preference-based reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "Thirtieth AAAI Conference on Artificial Intelligence",
      "authors": "Christian Wirth, Johannes Fürnkranz, and Gerhard Neumann"
    },
    {
      "index": 60,
      "title": "Graying the black box: Understanding DQNs",
      "abstract": "",
      "year": "2016",
      "venue": "International conference on machine learning",
      "authors": "Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor",
      "orig_title": "Graying the black box: Understanding dqns",
      "paper_id": "1602.02658v4"
    },
    {
      "index": 61,
      "title": "Object-Oriented Dynamics Predictor",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Guangxiang Zhu, Zhiao Huang, and Chongjie Zhang",
      "orig_title": "Object-oriented dynamics predictor",
      "paper_id": "1806.07371v3"
    }
  ]
}