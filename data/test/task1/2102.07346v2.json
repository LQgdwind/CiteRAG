{
  "paper_id": "2102.07346v2",
  "title": "On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers",
  "abstract": "Abstract\nA deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Convergence properties of the spline fit",
      "abstract": "",
      "year": "1963",
      "venue": "Journal of the Society for Industrial and Applied Mathematics",
      "authors": "J Harold Ahlberg and Edwin N Nilson"
    },
    {
      "index": 1,
      "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang",
      "orig_title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
      "paper_id": "1811.04918v6"
    },
    {
      "index": 2,
      "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Sanjeev Arora, Nadav Cohen, and Elad Hazan",
      "orig_title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
      "paper_id": "1802.06509v2"
    },
    {
      "index": 3,
      "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu",
      "orig_title": "A convergence analysis of gradient descent for deep linear neural networks",
      "paper_id": "1810.02281v3"
    },
    {
      "index": 4,
      "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.08584",
      "authors": "Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang",
      "orig_title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
      "paper_id": "1901.08584v2"
    },
    {
      "index": 5,
      "title": "Deep Equilibrium Models",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Shaojie Bai, J Zico Kolter, and Vladlen Koltun",
      "orig_title": "Deep equilibrium models",
      "paper_id": "1909.01377v2"
    },
    {
      "index": 6,
      "title": "Trellis Networks for Sequence Modeling",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Shaojie Bai, J Zico Kolter, and Vladlen Koltun",
      "orig_title": "Trellis networks for sequence modeling",
      "paper_id": "1810.06682v2"
    },
    {
      "index": 7,
      "title": "Matrix differentiation",
      "abstract": "",
      "year": "2006",
      "venue": "Springs Journal",
      "authors": "Randal J Barnes"
    },
    {
      "index": 8,
      "title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks",
      "abstract": "",
      "year": "2019",
      "venue": "Neural computation",
      "authors": "Peter L Bartlett, David P Helmbold, and Philip M Long",
      "orig_title": "Gradient descent with identity initialization efficiently learns positive-definite linear transformations by deep residual networks",
      "paper_id": "1802.06093v4"
    },
    {
      "index": 9,
      "title": "Algorithmic differentiation of implicit functions and optimal values",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Automatic Differentiation",
      "authors": "Bradley M Bell and James V Burke"
    },
    {
      "index": 10,
      "title": "On Lazy Training in Differentiable Programming",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Lenaic Chizat, Edouard Oyallon, and Francis Bach",
      "orig_title": "On lazy training in differentiable programming",
      "paper_id": "1812.07956v5"
    },
    {
      "index": 11,
      "title": "Reverse accumulation and attractive fixed points",
      "abstract": "",
      "year": "1994",
      "venue": "Optimization Methods and Software",
      "authors": "Bruce Christianson"
    },
    {
      "index": 12,
      "title": "Deep Learning for Classical Japanese Literature",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS Creativity Workshop 2019",
      "authors": "Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha",
      "orig_title": "Deep learning for classical japanese literature",
      "paper_id": "1812.01718v1"
    },
    {
      "index": 13,
      "title": "Recurrent Stacking of Layers for Compact Neural Machine Translation Models",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Raj Dabre and Atsushi Fujita",
      "orig_title": "Recurrent stacking of layers for compact neural machine translation models",
      "paper_id": "1807.05353v2"
    },
    {
      "index": 14,
      "title": "Universal Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser",
      "orig_title": "Universal transformers",
      "paper_id": "1807.03819v3"
    },
    {
      "index": 15,
      "title": "Width Provably Matters in Optimization for Deep Linear Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon Du and Wei Hu",
      "orig_title": "Width provably matters in optimization for deep linear neural networks",
      "paper_id": "1901.08572v3"
    },
    {
      "index": 16,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 17,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.03804",
      "authors": "Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 18,
      "title": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.01452",
      "authors": "Cong Fang, Jason D Lee, Pengkun Yang, and Tong Zhang",
      "orig_title": "Modeling from features: a mean-field framework for over-parameterized deep neural networks",
      "paper_id": "2007.01452v1"
    },
    {
      "index": 19,
      "title": "Über matrizen aus nicht negativen elementen",
      "abstract": "",
      "year": "1912",
      "venue": "Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften",
      "authors": "Georg Frobenius"
    },
    {
      "index": 20,
      "title": "Evaluating derivatives: principles and techniques of algorithmic differentiation",
      "abstract": "",
      "year": "2008",
      "venue": "SIAM",
      "authors": "Andreas Griewank and Andrea Walther"
    },
    {
      "index": 21,
      "title": "Implicit Regularization in Matrix Factorization",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro",
      "orig_title": "Implicit regularization in matrix factorization",
      "paper_id": "1705.09280v1"
    },
    {
      "index": 22,
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro",
      "orig_title": "Implicit bias of gradient descent on linear convolutional networks",
      "paper_id": "1806.00468v2"
    },
    {
      "index": 23,
      "title": "Identity Matters in Deep Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Moritz Hardt and Tengyu Ma",
      "orig_title": "Identity matters in deep learning",
      "paper_id": "1611.04231v3"
    },
    {
      "index": 24,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 25,
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Arthur Jacot, Franck Gabriel, and Clément Hongler",
      "orig_title": "Neural tangent kernel: Convergence and generalization in neural networks",
      "paper_id": "1806.07572v4"
    },
    {
      "index": 26,
      "title": "Locally adaptive activation functions with slope recovery for deep and physics-informed neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the Royal Society A",
      "authors": "Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis"
    },
    {
      "index": 27,
      "title": "Adaptive activation functions accelerate convergence in deep and physics-informed neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Computational Physics",
      "authors": "Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis"
    },
    {
      "index": 28,
      "title": "Directional convergence and alignment in deep learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.06657",
      "authors": "Ziwei Ji and Matus Telgarsky",
      "orig_title": "Directional convergence and alignment in deep learning",
      "paper_id": "2006.06657v2"
    },
    {
      "index": 29,
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition",
      "abstract": "",
      "year": "2016",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Hamed Karimi, Julie Nutini, and Mark Schmidt",
      "orig_title": "Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition",
      "paper_id": "1608.04636v4"
    },
    {
      "index": 30,
      "title": "Deep Learning without Poor Local Minima",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kenji Kawaguchi",
      "orig_title": "Deep learning without poor local minima",
      "paper_id": "1605.07110v3"
    },
    {
      "index": 31,
      "title": "Depth with nonlinearity creates no bad local minima in resnets",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "Kenji Kawaguchi and Yoshua Bengio"
    },
    {
      "index": 32,
      "title": "Gradient descent finds global minima for generalizable deep neural networks of practical sizes",
      "abstract": "",
      "year": "2019",
      "venue": "57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",
      "authors": "Kenji Kawaguchi and Jiaoyang Huang"
    },
    {
      "index": 33,
      "title": "Elimination of all bad local minima in deep learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Kenji Kawaguchi and Leslie Kaelbling"
    },
    {
      "index": 34,
      "title": "Generalization in Deep Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1710.05468",
      "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio",
      "orig_title": "Generalization in deep learning",
      "paper_id": "1710.05468v9"
    },
    {
      "index": 35,
      "title": "Effect of depth and width on local minima in deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "Neural computation",
      "authors": "Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling"
    },
    {
      "index": 36,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "Citeseer",
      "authors": "Alex Krizhevsky and Geoffrey Hinton"
    },
    {
      "index": 37,
      "title": "Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Thomas Laurent and James Brecht",
      "orig_title": "Deep linear networks with arbitrary loss: All local minima are global",
      "paper_id": "1712.01473v2"
    },
    {
      "index": 38,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner"
    },
    {
      "index": 39,
      "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington"
    },
    {
      "index": 40,
      "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yuanzhi Li and Yingyu Liang",
      "orig_title": "Learning overparameterized neural networks via stochastic gradient descent on structured data",
      "paper_id": "1808.01204v3"
    },
    {
      "index": 41,
      "title": "Adding One Neuron Can Eliminate All Bad Local Minima",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant",
      "orig_title": "Adding one neuron can eliminate all bad local minima",
      "paper_id": "1805.08671v1"
    },
    {
      "index": 42,
      "title": "The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.12826",
      "authors": "Andrea Montanari and Yiqiao Zhong",
      "orig_title": "The interpolation phase transition in neural networks: memorization and generalization under lazy training",
      "paper_id": "2007.12826v3"
    },
    {
      "index": 43,
      "title": "Bounds for norms of the matrix inverse and the smallest singular value",
      "abstract": "",
      "year": "2008",
      "venue": "Linear algebra and its applications",
      "authors": "Nenad Morača"
    },
    {
      "index": 44,
      "title": "Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.06738",
      "authors": "Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D Lee, Nathan Srebro, and Daniel Soudry",
      "orig_title": "Implicit bias in deep linear classification: Initialization scale vs training accuracy",
      "paper_id": "2007.06738v1"
    },
    {
      "index": 45,
      "title": "Reading digits in natural images with unsupervised feature learning",
      "abstract": "",
      "year": "2011",
      "venue": "NIPS workshop on deep learning and unsupervised feature learning",
      "authors": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng"
    },
    {
      "index": 46,
      "title": "On Connected Sublevel Sets in Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Quynh Nguyen",
      "orig_title": "On connected sublevel sets in deep learning",
      "paper_id": "1901.07417v2"
    },
    {
      "index": 47,
      "title": "A note on connectivity of sublevel sets in deep learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.08576",
      "authors": "Quynh Nguyen"
    },
    {
      "index": 48,
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.",
      "orig_title": "Pytorch: An imperative style, high-performance deep learning library",
      "paper_id": "1912.01703v1"
    },
    {
      "index": 49,
      "title": "Zur theorie der matrices",
      "abstract": "",
      "year": "1907",
      "venue": "Mathematische Annalen",
      "authors": "Oskar Perron"
    },
    {
      "index": 50,
      "title": "Theory of deep learning iii: explaining the non-overfitting puzzle",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1801.00173",
      "authors": "Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar"
    },
    {
      "index": 51,
      "title": "Gradient methods for minimizing functionals",
      "abstract": "",
      "year": "1963",
      "venue": "Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki",
      "authors": "Boris Teodorovich Polyak"
    },
    {
      "index": 52,
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "Andrew M Saxe, James L McClelland, and Surya Ganguli"
    },
    {
      "index": 53,
      "title": "Are ResNets Provably Better than Linear Predictors?",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ohad Shamir",
      "orig_title": "Are ResNets provably better than linear predictors?",
      "paper_id": "1804.06739v4"
    },
    {
      "index": 54,
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "abstract": "",
      "year": "2018",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro",
      "orig_title": "The implicit bias of gradient descent on separable data",
      "paper_id": "1710.10345v7"
    },
    {
      "index": 55,
      "title": "Semeion handwritten digit data set",
      "abstract": "",
      "year": "1994",
      "venue": "Semeion Research Center of Sciences of Communication, Rome, Italy",
      "authors": "B Tactile Srl and Italy Brescia"
    },
    {
      "index": 56,
      "title": "A lower bound for the smallest singular value of a matrix",
      "abstract": "",
      "year": "1975",
      "venue": "Linear Algebra and its Applications",
      "authors": "James M Varah"
    },
    {
      "index": 57,
      "title": "Graphmix: Regularized training of graph neural networks for semi-supervised learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.11715",
      "authors": "Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang"
    },
    {
      "index": 58,
      "title": "Kernel and Rich Regimes in Overparametrized Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2002.09277",
      "authors": "Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro",
      "orig_title": "Kernel and rich regimes in overparametrized models",
      "paper_id": "2002.09277v3"
    },
    {
      "index": 59,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.07747",
      "authors": "Han Xiao, Kashif Rasul, and Roland Vollgraf",
      "orig_title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 60,
      "title": "On the power and limitations of random features for understanding neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Gilad Yehudai and Ohad Shamir"
    },
    {
      "index": 61,
      "title": "Gradient descent optimizes over-parameterized deep ReLU networks",
      "abstract": "",
      "year": "2020",
      "venue": "Machine Learning",
      "authors": "Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu"
    },
    {
      "index": 62,
      "title": "On the Global Convergence of Training Deep Linear ResNets",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Difan Zou, Philip M Long, and Quanquan Gu",
      "orig_title": "On the global convergence of training deep linear resnets",
      "paper_id": "2003.01094v1"
    }
  ]
}