{
  "paper_id": "2010.11151v2",
  "title": "1 INTRODUCTION",
  "abstract": "Abstract\nWe propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of\noptimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm\nof Peters et al., (2010), with the key difference that our method introduces a Q-function that enables efficient\nexact model-free implementation. The main feature of our algorithm (called Q-REPS) is a convex loss\nfunction for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman\nerror. We provide a practical saddle-point optimization method for minimizing this loss function and provide an\nerror-propagation analysis that relates the quality of the individual updates to the performance of the output policy.\nFinally, we demonstrate the effectiveness of our method on a range of benchmark problems.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Politex: Regret bounds for policy iteration using expert prediction",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvári, Cs., and Weisz, G."
    },
    {
      "index": 1,
      "title": "Maximum a Posteriori Policy Optimisation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M.",
      "orig_title": "Maximum a posteriori policy optimisation",
      "paper_id": "1806.06920v1"
    },
    {
      "index": 2,
      "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.",
      "orig_title": "FLAMBE: Structural complexity and representation learning of low rank MDPs",
      "paper_id": "2006.10814v2"
    },
    {
      "index": 3,
      "title": "Optimality and approximation with policy gradient methods in Markov decision processes",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G."
    },
    {
      "index": 4,
      "title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path",
      "abstract": "",
      "year": "2006",
      "venue": "Conference on Learning Theory",
      "authors": "Antos, A., Szepesvári, Cs., and Munos, R."
    },
    {
      "index": 5,
      "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Ayoub, A., Jia, Z., Szepesvári, Cs., Wang, M., and Yang, L. F.",
      "orig_title": "Model-based reinforcement learning with value-targeted regression",
      "paper_id": "2006.01107v1"
    },
    {
      "index": 6,
      "title": "Covariant policy search",
      "abstract": "",
      "year": "2003",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Bagnell, J. A. and Schneider, J."
    },
    {
      "index": 7,
      "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "abstract": "",
      "year": "2003",
      "venue": "Operations Research Letters",
      "authors": "Beck, A. and Teboulle, M."
    },
    {
      "index": 8,
      "title": "Concentration inequalities: A Nonasymptotic Theory of Independence",
      "abstract": "",
      "year": "2013",
      "venue": "Oxford University Press",
      "authors": "Boucheron, S., Lugosi, G., and Massart, P."
    },
    {
      "index": 9,
      "title": "OpenAI gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 10,
      "title": "Provably Efficient Exploration in Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Cai, Q., Yang, Z., Jin, C., and Wang, Z.",
      "orig_title": "Provably efficient exploration in policy optimization",
      "paper_id": "1912.05830v4"
    },
    {
      "index": 11,
      "title": "Prediction, Learning, and Games",
      "abstract": "",
      "year": "2006",
      "venue": "Cambridge University Press",
      "authors": "Cesa-Bianchi, N. and Lugosi, G."
    },
    {
      "index": 12,
      "title": "RL-Lib - a PyTorch-based library for reinforcement learning research",
      "abstract": "",
      "year": "2020",
      "venue": "Github",
      "authors": "Curi, S."
    },
    {
      "index": 13,
      "title": "SBEED: Convergent reinforcement learning with nonlinear function approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L."
    },
    {
      "index": 14,
      "title": "The linear programming approach to approximate dynamic programming",
      "abstract": "",
      "year": "2003",
      "venue": "Operations Research",
      "authors": "de Farias, D. P. and Van Roy, B."
    },
    {
      "index": 15,
      "title": "A survey on policy search for robotics",
      "abstract": "",
      "year": "2013",
      "venue": "Foundations and Trends in Robotics",
      "authors": "Deisenroth, M., Neumann, G., and Peters, J."
    },
    {
      "index": 16,
      "title": "Approximate dynamic programming via a smoothed linear program",
      "abstract": "",
      "year": "2012",
      "venue": "Operations Research",
      "authors": "Desai, V. V., Farias, V. F., and Moallemi, C. C."
    },
    {
      "index": 17,
      "title": "A Theoretical Analysis of Deep Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Learning for Dynamics and Control",
      "authors": "Fan, J., Wang, Z., Xie, Y., and Yang, Z.",
      "orig_title": "A theoretical analysis of deep Q-learning",
      "paper_id": "1901.00137v3"
    },
    {
      "index": 18,
      "title": "A Kernel Loss for Solving the Bellman Equation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Feng, Y., Li, L., and Liu, Q.",
      "orig_title": "A kernel loss for solving the Bellman equation",
      "paper_id": "1905.10506v3"
    },
    {
      "index": 19,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Fujimoto, S., van Hoof, H., and Meger, D.",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 20,
      "title": "Variational methods for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "Furmston, T. and Barber, D."
    },
    {
      "index": 21,
      "title": "Is the Bellman residual a bad proxy?",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Geist, M., Piot, B., and Pietquin, O.",
      "orig_title": "Is the Bellman residual a bad proxy?",
      "paper_id": "1606.07636v3"
    },
    {
      "index": 22,
      "title": "A Theory of Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Geist, M., Scherrer, B., and Pietquin, O.",
      "orig_title": "A theory of regularized Markov decision processes",
      "paper_id": "1901.11275v2"
    },
    {
      "index": 23,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 24,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Jin, C., Yang, Z., Wang, Z., and Jordan, M. I.",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 25,
      "title": "A natural policy gradient",
      "abstract": "",
      "year": "2001",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kakade, S."
    },
    {
      "index": 26,
      "title": "Approximately optimal approximate reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "International Conference on Machine Learning",
      "authors": "Kakade, S. and Langford, J."
    },
    {
      "index": 27,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "Kingma, D. P. and Ba, J."
    },
    {
      "index": 28,
      "title": "A Linearly Relaxed Approximate Linear Program for Markov Decision Processes",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Lakshminarayanan, C., Bhatnagar, S., and Szepesvári, Cs.",
      "orig_title": "A linearly relaxed approximate linear program for Markov decision processes",
      "paper_id": "1704.02544v1"
    },
    {
      "index": 29,
      "title": "Stochastic primal-dual Q-learning algorithm for discounted MDPs",
      "abstract": "",
      "year": "2019",
      "venue": "American Control Conference",
      "authors": "Lee, D. and He, N."
    },
    {
      "index": 30,
      "title": "Linear programming and sequential decisions",
      "abstract": "",
      "year": "1960",
      "venue": "Management Science",
      "authors": "Manne, A. S."
    },
    {
      "index": 31,
      "title": "Régularisation d’inéquations variationnelles par approximations successives",
      "abstract": "",
      "year": "1970",
      "venue": "ESAIM: Mathematical Modelling and Numerical Analysis - Modélisation Mathématique et Analyse Numérique",
      "authors": "Martinet, B."
    },
    {
      "index": 32,
      "title": "Q-learning and Pontryagin’s minimum principle",
      "abstract": "",
      "year": "2009",
      "venue": "Conference on Decision and Control",
      "authors": "Mehta, P. and Meyn, S."
    },
    {
      "index": 33,
      "title": "Convex Q-Learning Part 1: Deterministic Optimal Control",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.03559",
      "authors": "Mehta, P. G. and Meyn, S. P.",
      "orig_title": "Convex Q-learning, part 1: Deterministic optimal control",
      "paper_id": "2008.03559v1"
    },
    {
      "index": 34,
      "title": "Q-learning with linear function approximation",
      "abstract": "",
      "year": "2007",
      "venue": "Conference on Learning Theory",
      "authors": "Melo, F. S. and Ribeiro, M. I."
    },
    {
      "index": 35,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K."
    },
    {
      "index": 36,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., and Ostrovski, G."
    },
    {
      "index": 37,
      "title": "A Unified View of Entropy-Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.07798",
      "authors": "Neu, G., Jonsson, A., and Gómez, V.",
      "orig_title": "A unified view of entropy-regularized Markov decision processes",
      "paper_id": "1705.07798v1"
    },
    {
      "index": 38,
      "title": "A Unifying View of Optimism in Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Neu, G. and Pike-Burke, C.",
      "orig_title": "A unifying view of optimism in episodic reinforcement learning",
      "paper_id": "2007.01891v1"
    },
    {
      "index": 39,
      "title": "Is the policy gradient a gradient?",
      "abstract": "",
      "year": "2020",
      "venue": "Autonomous Agents and Multiagent Systems",
      "authors": "Nota, C. and Thomas, P. S."
    },
    {
      "index": 40,
      "title": "Automatic differentiation in PyTorch",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A."
    },
    {
      "index": 41,
      "title": "Relative entropy policy search",
      "abstract": "",
      "year": "2010",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Peters, J., Mülling, K., and Altun, Y."
    },
    {
      "index": 42,
      "title": "Constraint relaxation in approximate linear programs",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Machine Learning",
      "authors": "Petrik, M. and Zilberstein, S."
    },
    {
      "index": 43,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "Wiley-Interscience",
      "authors": "Puterman, M. L."
    },
    {
      "index": 44,
      "title": "Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method",
      "abstract": "",
      "year": "2005",
      "venue": "European Conference on Machine Learning",
      "authors": "Riedmiller, M."
    },
    {
      "index": 45,
      "title": "A stochastic approximation method",
      "abstract": "",
      "year": "1951",
      "venue": "Annals of Mathematical Statistics",
      "authors": "Robbins, H. and Monro, S."
    },
    {
      "index": 46,
      "title": "Monotone Operators and the Proximal Point Algorithm",
      "abstract": "",
      "year": "1976",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Rockafellar, R. T."
    },
    {
      "index": 47,
      "title": "Prioritized experience replay",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Schaul, T., Quan, J., Antonoglou, I., and Silver, D."
    },
    {
      "index": 48,
      "title": "Approximate modified policy iteration and its application to the game of tetris",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Machine Learning Research",
      "authors": "Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M."
    },
    {
      "index": 49,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 50,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O."
    },
    {
      "index": 51,
      "title": "Generalized polynomial approximations in Markovian decision processes",
      "abstract": "",
      "year": "1985",
      "venue": "J. of Math. Anal. and Appl.",
      "authors": "Schweitzer, P. and Seidmann, A."
    },
    {
      "index": 52,
      "title": "V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Song, H. F., Abdolmaleki, A., Springenberg, J. T., Clark, A., Soyer, H., Rae, J. W., Noury, S., Ahuja, A., Liu, S., Tirumala, D., Heess, N., Belov, D., Riedmiller, M. A., and Botvinick, M. M."
    },
    {
      "index": 53,
      "title": "An analysis of model-based interval estimation for Markov decision processes",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Computer and System Sciences",
      "authors": "Strehl, A. L. and Littman, M. L."
    },
    {
      "index": 54,
      "title": "Reinforcement Learning: An Introduction (second edition)",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Sutton, R. and Barto, A."
    },
    {
      "index": 55,
      "title": "Bias in natural actor-critic algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Thomas, P."
    },
    {
      "index": 56,
      "title": "Leverage the average: an analysis of KL regularization in reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M."
    },
    {
      "index": 57,
      "title": "𝑞-Munchausen Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vieillard, N., Pietquin, O., and Geist, M.",
      "orig_title": "Munchausen reinforcement learning",
      "paper_id": "2205.07467v1"
    },
    {
      "index": 58,
      "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wang, R., Du, S. S., Yang, L., and Salakhutdinov, R. R.",
      "orig_title": "On reward-free reinforcement learning with linear function approximation",
      "paper_id": "2006.11274v1"
    },
    {
      "index": 59,
      "title": "Q∗superscript𝑄Q^{*} approximation schemes for batch reinforcement learning: A theoretical comparison",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Xie, T. and Jiang, N."
    },
    {
      "index": 60,
      "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Yang, L. F. and Wang, M.",
      "orig_title": "Sample-optimal parametric Q-learning using linearly additive features",
      "paper_id": "1902.04779v2"
    },
    {
      "index": 61,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Ziebart, B., Maas, A. L., Bagnell, J. A., and Dey, A. K."
    },
    {
      "index": 62,
      "title": "Online learning in episodic Markovian decision processes by relative entropy policy search",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zimin, A. and Neu, G."
    }
  ]
}