{
  "paper_id": "2010.11151v2",
  "title": "1 INTRODUCTION",
  "abstract": "Abstract\nWe propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of\noptimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm\nof Peters et¬†al., (2010), with the key difference that our method introduces a Q-function that enables efficient\nexact model-free implementation. The main feature of our algorithm (called Q-REPS) is a convex loss\nfunction for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman\nerror. We provide a practical saddle-point optimization method for minimizing this loss function and provide an\nerror-propagation analysis that relates the quality of the individual updates to the performance of the output policy.\nFinally, we demonstrate the effectiveness of our method on a range of benchmark problems.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Politex: Regret bounds for policy iteration using expert prediction",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesv√°ri, Cs., and Weisz, G."
    },
    {
      "index": 1,
      "title": "Maximum a Posteriori Policy Optimisation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M.",
      "orig_title": "Maximum a posteriori policy optimisation",
      "paper_id": "1806.06920v1"
    },
    {
      "index": 2,
      "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.",
      "orig_title": "FLAMBE: Structural complexity and representation learning of low rank MDPs",
      "paper_id": "2006.10814v2"
    },
    {
      "index": 3,
      "title": "Optimality and approximation with policy gradient methods in Markov decision processes",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G."
    },
    {
      "index": 4,
      "title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path",
      "abstract": "",
      "year": "2006",
      "venue": "Conference on Learning Theory",
      "authors": "Antos, A., Szepesv√°ri, Cs., and Munos, R."
    },
    {
      "index": 5,
      "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Ayoub, A., Jia, Z., Szepesv√°ri, Cs., Wang, M., and Yang, L. F.",
      "orig_title": "Model-based reinforcement learning with value-targeted regression",
      "paper_id": "2006.01107v1"
    },
    {
      "index": 6,
      "title": "Covariant policy search",
      "abstract": "",
      "year": "2003",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Bagnell, J. A. and Schneider, J."
    },
    {
      "index": 7,
      "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "abstract": "",
      "year": "2003",
      "venue": "Operations Research Letters",
      "authors": "Beck, A. and Teboulle, M."
    },
    {
      "index": 8,
      "title": "Concentration inequalities: A Nonasymptotic Theory of Independence",
      "abstract": "",
      "year": "2013",
      "venue": "Oxford University Press",
      "authors": "Boucheron, S., Lugosi, G., and Massart, P."
    },
    {
      "index": 9,
      "title": "OpenAI gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01540",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 10,
      "title": "Provably Efficient Exploration in Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Cai, Q., Yang, Z., Jin, C., and Wang, Z.",
      "orig_title": "Provably efficient exploration in policy optimization",
      "paper_id": "1912.05830v4"
    },
    {
      "index": 11,
      "title": "Prediction, Learning, and Games",
      "abstract": "",
      "year": "2006",
      "venue": "Cambridge University Press",
      "authors": "Cesa-Bianchi, N. and Lugosi, G."
    },
    {
      "index": 12,
      "title": "RL-Lib - a PyTorch-based library for reinforcement learning research",
      "abstract": "",
      "year": "2020",
      "venue": "Github",
      "authors": "Curi, S."
    },
    {
      "index": 13,
      "title": "SBEED: Convergent reinforcement learning with nonlinear function approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L."
    },
    {
      "index": 14,
      "title": "The linear programming approach to approximate dynamic programming",
      "abstract": "",
      "year": "2003",
      "venue": "Operations Research",
      "authors": "de Farias, D. P. and Van Roy, B."
    },
    {
      "index": 15,
      "title": "A survey on policy search for robotics",
      "abstract": "",
      "year": "2013",
      "venue": "Foundations and Trends in Robotics",
      "authors": "Deisenroth, M., Neumann, G., and Peters, J."
    },
    {
      "index": 16,
      "title": "Approximate dynamic programming via a smoothed linear program",
      "abstract": "",
      "year": "2012",
      "venue": "Operations Research",
      "authors": "Desai, V. V., Farias, V. F., and Moallemi, C. C."
    },
    {
      "index": 17,
      "title": "A Theoretical Analysis of Deep Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Learning for Dynamics and Control",
      "authors": "Fan, J., Wang, Z., Xie, Y., and Yang, Z.",
      "orig_title": "A theoretical analysis of deep Q-learning",
      "paper_id": "1901.00137v3"
    },
    {
      "index": 18,
      "title": "A Kernel Loss for Solving the Bellman Equation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Feng, Y., Li, L., and Liu, Q.",
      "orig_title": "A kernel loss for solving the Bellman equation",
      "paper_id": "1905.10506v3"
    },
    {
      "index": 19,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Fujimoto, S., van Hoof, H., and Meger, D.",
      "orig_title": "Addressing function approximation error in actor-critic methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 20,
      "title": "Variational methods for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "Furmston, T. and Barber, D."
    },
    {
      "index": 21,
      "title": "Is the Bellman residual a bad proxy?",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Geist, M., Piot, B., and Pietquin, O.",
      "orig_title": "Is the Bellman residual a bad proxy?",
      "paper_id": "1606.07636v3"
    },
    {
      "index": 22,
      "title": "A Theory of Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Geist, M., Scherrer, B., and Pietquin, O.",
      "orig_title": "A theory of regularized Markov decision processes",
      "paper_id": "1901.11275v2"
    },
    {
      "index": 23,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 24,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Jin, C., Yang, Z., Wang, Z., and Jordan, M. I.",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 25,
      "title": "A natural policy gradient",
      "abstract": "",
      "year": "2001",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kakade, S."
    },
    {
      "index": 26,
      "title": "Approximately optimal approximate reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "International Conference on Machine Learning",
      "authors": "Kakade, S. and Langford, J."
    },
    {
      "index": 27,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations",
      "authors": "Kingma, D. P. and Ba, J."
    },
    {
      "index": 28,
      "title": "A Linearly Relaxed Approximate Linear Program for Markov Decision Processes",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Lakshminarayanan, C., Bhatnagar, S., and Szepesv√°ri, Cs.",
      "orig_title": "A linearly relaxed approximate linear program for Markov decision processes",
      "paper_id": "1704.02544v1"
    },
    {
      "index": 29,
      "title": "Stochastic primal-dual Q-learning algorithm for discounted MDPs",
      "abstract": "",
      "year": "2019",
      "venue": "American Control Conference",
      "authors": "Lee, D. and He, N."
    },
    {
      "index": 30,
      "title": "Linear programming and sequential decisions",
      "abstract": "",
      "year": "1960",
      "venue": "Management Science",
      "authors": "Manne, A. S."
    },
    {
      "index": 31,
      "title": "R√©gularisation d‚Äôin√©quations variationnelles par approximations successives",
      "abstract": "",
      "year": "1970",
      "venue": "ESAIM: Mathematical Modelling and Numerical Analysis - Mod√©lisation Math√©matique et Analyse Num√©rique",
      "authors": "Martinet, B."
    },
    {
      "index": 32,
      "title": "Q-learning and Pontryagin‚Äôs minimum principle",
      "abstract": "",
      "year": "2009",
      "venue": "Conference on Decision and Control",
      "authors": "Mehta, P. and Meyn, S."
    },
    {
      "index": 33,
      "title": "Convex Q-Learning Part 1: Deterministic Optimal Control",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2008.03559",
      "authors": "Mehta, P. G. and Meyn, S. P.",
      "orig_title": "Convex Q-learning, part 1: Deterministic optimal control",
      "paper_id": "2008.03559v1"
    },
    {
      "index": 34,
      "title": "Q-learning with linear function approximation",
      "abstract": "",
      "year": "2007",
      "venue": "Conference on Learning Theory",
      "authors": "Melo, F. S. and Ribeiro, M. I."
    },
    {
      "index": 35,
      "title": "Asynchronous methods for deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K."
    },
    {
      "index": 36,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., and Ostrovski, G."
    },
    {
      "index": 37,
      "title": "A Unified View of Entropy-Regularized Markov Decision Processes",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1705.07798",
      "authors": "Neu, G., Jonsson, A., and G√≥mez, V.",
      "orig_title": "A unified view of entropy-regularized Markov decision processes",
      "paper_id": "1705.07798v1"
    },
    {
      "index": 38,
      "title": "A Unifying View of Optimism in Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Neu, G. and Pike-Burke, C.",
      "orig_title": "A unifying view of optimism in episodic reinforcement learning",
      "paper_id": "2007.01891v1"
    },
    {
      "index": 39,
      "title": "Is the policy gradient a gradient?",
      "abstract": "",
      "year": "2020",
      "venue": "Autonomous Agents and Multiagent Systems",
      "authors": "Nota, C. and Thomas, P. S."
    },
    {
      "index": 40,
      "title": "Automatic differentiation in PyTorch",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A."
    },
    {
      "index": 41,
      "title": "Relative entropy policy search",
      "abstract": "",
      "year": "2010",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Peters, J., M√ºlling, K., and Altun, Y."
    },
    {
      "index": 42,
      "title": "Constraint relaxation in approximate linear programs",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Machine Learning",
      "authors": "Petrik, M. and Zilberstein, S."
    },
    {
      "index": 43,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "Wiley-Interscience",
      "authors": "Puterman, M. L."
    },
    {
      "index": 44,
      "title": "Neural fitted Q iteration‚Äìfirst experiences with a data efficient neural reinforcement learning method",
      "abstract": "",
      "year": "2005",
      "venue": "European Conference on Machine Learning",
      "authors": "Riedmiller, M."
    },
    {
      "index": 45,
      "title": "A stochastic approximation method",
      "abstract": "",
      "year": "1951",
      "venue": "Annals of Mathematical Statistics",
      "authors": "Robbins, H. and Monro, S."
    },
    {
      "index": 46,
      "title": "Monotone Operators and the Proximal Point Algorithm",
      "abstract": "",
      "year": "1976",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Rockafellar, R. T."
    },
    {
      "index": 47,
      "title": "Prioritized experience replay",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Schaul, T., Quan, J., Antonoglou, I., and Silver, D."
    },
    {
      "index": 48,
      "title": "Approximate modified policy iteration and its application to the game of tetris",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Machine Learning Research",
      "authors": "Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M."
    },
    {
      "index": 49,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 50,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O."
    },
    {
      "index": 51,
      "title": "Generalized polynomial approximations in Markovian decision processes",
      "abstract": "",
      "year": "1985",
      "venue": "J. of Math. Anal. and Appl.",
      "authors": "Schweitzer, P. and Seidmann, A."
    },
    {
      "index": 52,
      "title": "V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Song, H. F., Abdolmaleki, A., Springenberg, J. T., Clark, A., Soyer, H., Rae, J. W., Noury, S., Ahuja, A., Liu, S., Tirumala, D., Heess, N., Belov, D., Riedmiller, M. A., and Botvinick, M. M."
    },
    {
      "index": 53,
      "title": "An analysis of model-based interval estimation for Markov decision processes",
      "abstract": "",
      "year": "2008",
      "venue": "Journal of Computer and System Sciences",
      "authors": "Strehl, A. L. and Littman, M. L."
    },
    {
      "index": 54,
      "title": "Reinforcement Learning: An Introduction (second edition)",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Sutton, R. and Barto, A."
    },
    {
      "index": 55,
      "title": "Bias in natural actor-critic algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Thomas, P."
    },
    {
      "index": 56,
      "title": "Leverage the average: an analysis of KL regularization in reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M."
    },
    {
      "index": 57,
      "title": "ùëû-Munchausen Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vieillard, N., Pietquin, O., and Geist, M.",
      "orig_title": "Munchausen reinforcement learning",
      "paper_id": "2205.07467v1"
    },
    {
      "index": 58,
      "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wang, R., Du, S. S., Yang, L., and Salakhutdinov, R. R.",
      "orig_title": "On reward-free reinforcement learning with linear function approximation",
      "paper_id": "2006.11274v1"
    },
    {
      "index": 59,
      "title": "Q‚àósuperscriptùëÑQ^{*} approximation schemes for batch reinforcement learning: A theoretical comparison",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Xie, T. and Jiang, N."
    },
    {
      "index": 60,
      "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Yang, L. F. and Wang, M.",
      "orig_title": "Sample-optimal parametric Q-learning using linearly additive features",
      "paper_id": "1902.04779v2"
    },
    {
      "index": 61,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Ziebart, B., Maas, A. L., Bagnell, J. A., and Dey, A. K."
    },
    {
      "index": 62,
      "title": "Online learning in episodic Markovian decision processes by relative entropy policy search",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zimin, A. and Neu, G."
    }
  ]
}