{
  "paper_id": "2406.01698v3",
  "title": "Demystifying Platform Requirements for Diverse LLM Inference Use Cases",
  "abstract": "Abstract\nLarge language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question.\nIn this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters.\nOur analysis provides insights into configuring platforms for different LLM workloads and use cases. We quantify the platform requirements to support SOTA LLMs under diverse serving settings. Furthermore, we project the hardware capabilities needed to enable\nfuture LLMs\npotentially exceeding hundreds of trillions of parameters. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms.\nUltimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications.\nThe source code is available at GitHub.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Accelerating self-attentions for llm serving with flashinfer.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Amazon web services.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 2,
      "title": "Amd instinct mi300x plat- form.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 3,
      "title": "Cognitivecomputations dolphin model, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 4,
      "title": "Dbrx, a large language model by databricks.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 5,
      "title": "Dissecting batching effects in gpt inference.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 6,
      "title": "Generative llm inference with neuron.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 7,
      "title": "Google tpu pods.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 8,
      "title": "Gpt-4 architecture details.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 9,
      "title": "Gpt-4 architecture, infrastructure, training dataset, costs, vision, moe.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 10,
      "title": "Grok-1.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 11,
      "title": "Groqchip.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 12,
      "title": "A guide to llm inference and performance.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 13,
      "title": "Hugging face datasets.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 14,
      "title": "Hugging face pipelines.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 15,
      "title": "Huggingface generate infereface.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 16,
      "title": "Intel hls-gaudi2.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 17,
      "title": "Llm inference performance engineering: Best practices.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 18,
      "title": "Mastering llm techniques: Inference optimization.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 19,
      "title": "Meta llama 3.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 20,
      "title": "Microsoft azure.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 21,
      "title": "Mixtral-8x22b.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 22,
      "title": "Multi-turn conversational prompts from chatgpt-4 (10k+ tokens).",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 23,
      "title": "Nccl tests.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 24,
      "title": "Nvidia faster transformer.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 25,
      "title": "Nvidia hgx.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 26,
      "title": "Openai’s plans according to sam altman.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 27,
      "title": "Rayllm - llms on ray.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 28,
      "title": "The rise and rise of a.i. large language models (llms).",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 29,
      "title": "Sora.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 30,
      "title": "Text generation inference.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 31,
      "title": "Thudm longbench govreport dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 32,
      "title": "Thudm longbench lcc dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 33,
      "title": "Thudm longbench multinews dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 34,
      "title": "Thudm longbench qmsum dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 35,
      "title": "Thudm longbench repobench-p dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 36,
      "title": "Thudm longbench vcsum dataset, hugging face.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 37,
      "title": "Transformer inference arithmetic.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "Triton tensorrt-llm backend.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 39,
      "title": "Sambanova whitepaper,",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 40,
      "title": "S. Anadkat et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "“Llm inference performance engineering: Best practices",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Sarathi: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills",
      "paper_id": "2308.16369v1"
    },
    {
      "index": 43,
      "title": "“Loggp: Incorporating long messages into the logp model for parallel computation",
      "abstract": "",
      "year": "1997",
      "venue": "",
      "authors": ""
    },
    {
      "index": 44,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 45,
      "title": "“Longbench: A bilingual",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 46,
      "title": "Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Progressive gradient flow for robust n:m sparsity training in transformers",
      "paper_id": "2402.04744v1"
    },
    {
      "index": 47,
      "title": "Improving language models by retrieving from trillions of tokens",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Improving language models by retrieving from trillions of tokens",
      "paper_id": "2112.04426v3"
    },
    {
      "index": 48,
      "title": "“Proteinbert: a universal deep-learning model of protein sequence and function",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 49,
      "title": "A. Askell et al",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 50,
      "title": "“How many words do we read per minute? a review and meta-analysis of reading rate",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "Broken Neural Scaling Laws",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Broken neural scaling laws",
      "paper_id": "2210.14891v17"
    },
    {
      "index": 52,
      "title": "Factual Error Correction for Abstractive Summarization Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Factual error correction for abstractive summarization models",
      "paper_id": "2010.08712v2"
    },
    {
      "index": 53,
      "title": "“Introducing chatgpt.” [Online]. Available: https://openai.com/blog/chatgpt",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "“Bge m3-embedding: Multi-lingual",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Benchmarking large language models in retrieval-augmented generation",
      "paper_id": "2309.01431v2"
    },
    {
      "index": 56,
      "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism",
      "paper_id": "2312.04916v3"
    },
    {
      "index": 57,
      "title": "A Survey of Techniques for Optimizing Transformer Inference",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey of techniques for optimizing transformer inference",
      "paper_id": "2307.07982v1"
    },
    {
      "index": 58,
      "title": "S. Gehrmann et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "“Flash-decoding for long-context inference.” [Online]. Available: https://pytorch.org/blog/flash-decoding/",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "“Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "“Dmazerunner: Executing perfectly nested loops on dataflow accelerators",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "“dmazerunner: Optimizing convolutions on dataflow accelerators",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "Depth-Adaptive Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Depth-adaptive transformer",
      "paper_id": "1910.10073v4"
    },
    {
      "index": 64,
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "paper_id": "2101.03961v3"
    },
    {
      "index": 65,
      "title": "“Sparsegpt: Massive language models can be accurately pruned in one-shot",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Break the sequential dependency of llm inference using lookahead decoding",
      "paper_id": "2402.02057v1"
    },
    {
      "index": 67,
      "title": "“A survey of quantization methods for efficient neural network inference",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "“Introducing gemini: Google’s most capable ai model yet.” [Online]. Available: https://blog.google/technology/ai/google-gemini-ai/",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Knowledge distillation: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Hallucinations in Large Multilingual Translation Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Hallucinations in large multilingual translation models",
      "paper_id": "2303.16104v1"
    },
    {
      "index": 71,
      "title": "“Retrieval augmented language model pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Rethinking with retrieval: Faithful large language model inference",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 74,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 75,
      "title": "L. Kurilenko et al",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "“The curious case of neural text degeneration",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "“Chem-bert: Molecular representation learning.” [Online]. Available: https://github.com/HyunSeobKim/CHEM-BERT",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "“Atlas: Few-shot learning with retrieval augmented language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models",
      "paper_id": "2309.14509v2"
    },
    {
      "index": 81,
      "title": "“Product quantization for nearest neighbor search",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Abstracting sparse dnn acceleration via structured sparse tensor decomposition",
      "paper_id": "2403.07953v3"
    },
    {
      "index": 83,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Survey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 84,
      "title": "F. Bressand et al",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm",
      "paper_id": "2403.05527v4"
    },
    {
      "index": 86,
      "title": "“An optimized dataflow for mitigating attention performance bottlenecks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 88,
      "title": "“Guiding text generation with constrained beam search in transformers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "“Full stack optimization of transformer inference: a survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "Speculative Decoding with Big Little Decoder",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Speculative decoding with big little decoder",
      "paper_id": "2302.07863v4"
    },
    {
      "index": 91,
      "title": "“Literature survey on low rank approximation of matrices",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Videopoet: A large language model for zero-shot video generation",
      "paper_id": "2312.14125v4"
    },
    {
      "index": 93,
      "title": "“Maestro: A data-centric approach to understand reuse",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient memory management for large language model serving with pagedattention",
      "paper_id": "2309.06180v1"
    },
    {
      "index": 95,
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Efficient memory management for large language model serving with pagedattention",
      "paper_id": "2309.06180v1"
    },
    {
      "index": 96,
      "title": "T. Rocktäschel et al",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "A. Sukumaran-Rajam",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Sequence parallelism: Long sequence training from system perspective",
      "paper_id": "2105.13120v3"
    },
    {
      "index": 99,
      "title": "“Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "“Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "“Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "Best-First Beam Search",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Best-first beam search",
      "paper_id": "2007.03909v5"
    },
    {
      "index": 103,
      "title": "“Github copilot · your ai pair programmer.” [Online]. Available: https://github.com/features/copilot",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 105,
      "title": "CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Cagra: Highly parallel graph construction and approximate nearest neighbor search for gpus",
      "paper_id": "2308.15136v2"
    },
    {
      "index": 106,
      "title": "“Timeloop: A systematic approach to dnn accelerator evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Splitwise: Efficient generative llm inference using phase splitting",
      "paper_id": "2311.18677v2"
    },
    {
      "index": 108,
      "title": "Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Chiplet cloud: Building ai supercomputers for serving large generative language models",
      "paper_id": "2307.02666v4"
    },
    {
      "index": 109,
      "title": "“ASTRA-SIM: Enabling sw/hw co-design exploration for distributed dl training platforms",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "“Astra-sim: Enabling sw/hw co-design exploration for distributed dl training platforms",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "The Curious Case of Hallucinations in Neural Machine Translation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“The curious case of hallucinations in neural machine translation",
      "paper_id": "2104.06683v1"
    },
    {
      "index": 112,
      "title": "“A case for efficient accelerator design space exploration via bayesian optimization",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "D. Van Essendelft",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "here’s what i discovered — scientific american",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "“A systematic methodology for characterizing scalability of dnn accelerators using scale-sim",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "“A systematic methodology for characterizing scalability of dnn accelerators using scale-sim",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“In chatgpt we trust? measuring and characterizing the reliability of chatgpt",
      "paper_id": "2304.08979v2"
    },
    {
      "index": 118,
      "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Flexgen: High-throughput generative inference of large language models with a single gpu",
      "paper_id": "2303.06865v2"
    },
    {
      "index": 119,
      "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "paper_id": "1909.08053v4"
    },
    {
      "index": 120,
      "title": "Allies: Prompting Large Language Model with Beam Search",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Allies: Prompting large language model with beam search",
      "paper_id": "2305.14766v3"
    },
    {
      "index": 121,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 122,
      "title": "S. Bhosale et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 124,
      "title": "Emergent Abilities of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Emergent abilities of large language models",
      "paper_id": "2206.07682v2"
    },
    {
      "index": 125,
      "title": "ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“Astra-sim2. 0: Modeling hierarchical networks and disaggregated systems for large-model training at scale",
      "paper_id": "2303.14006v1"
    },
    {
      "index": 126,
      "title": "Exploring Multi-dimensional Hierarchical Network Topologies for Efficient Distributed Training of Trillion Parameter DL Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring multi-dimensional hierarchical network topologies for efficient distributed training of trillion parameter dl models",
      "paper_id": "2109.11762v2"
    },
    {
      "index": 127,
      "title": "“Efficient llm inference solution on intel gpu",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "“Smoothquant: Accurate and efficient post-training quantization for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "“Orca: A distributed serving system for {{\\{Transformer-Based}}\\} generative models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "“Llm inference unveiled: Survey and roofline model insights",
      "paper_id": "2402.16363v6"
    },
    {
      "index": 131,
      "title": "A Hardware Evaluation Framework for Large Language Model Inference",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“A hardware evaluation framework for large language model inference",
      "paper_id": "2312.03134v1"
    },
    {
      "index": 132,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey of large language models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 133,
      "title": "“Song: Approximate nearest neighbor search on gpu",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    }
  ]
}