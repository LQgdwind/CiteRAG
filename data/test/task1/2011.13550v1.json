{
  "paper_id": "2011.13550v1",
  "title": "Tight Hardness Results for Training Depth-2 ReLU Networks11footnote 1This work subsumes our earlier manuscript [MR18].",
  "abstract": "Abstract\nWe prove several hardness results for training depth-2 neural networks with the ReLU activation function; these networks are simply weighted sums (that may include negative coefficients) of ReLUs.\nOur goal is to output a depth-2 neural network that minimizes the square loss with respect to a given training set.\nWe prove that this problem is NP-hard already for a network with a single ReLU. We also prove NP-hardness for outputting a weighted sum of kk ReLUs minimizing the squared error (for k>11k>1) even in the realizable setting (i.e., when the labels are consistent with an unknown depth-2 ReLU network).\nWe are also able to obtain lower bounds on the running time\nin terms of the desired additive error 系italic-系\\epsilon. To obtain our lower bounds, we use the Gap Exponential Time Hypothesis (Gap-ETH) as well as a new hypothesis regarding the hardness of approximating the well known Densest 魏\\kappa-Subgraph problem in subexponential time (these hypotheses are used separately in proving different lower bounds).\nFor example, we prove that under reasonable hardness assumptions, any proper learning algorithm for finding the best fitting ReLU must run in time exponential in 1/系21superscriptitalic-系21/\\epsilon^{2}. Together with a previous work regarding improperly learning a ReLU[GKKT17], this implies the first separation between proper and improper algorithms for learning a ReLU. We also study the problem of properly learning a depth-2 network of ReLUs with bounded weights giving new (worst-case) upper bounds on the running time needed to learn such networks both in the realizable and agnostic settings. Our upper bounds on the running time essentially matches our lower bounds in terms of the dependency on 系italic-系\\epsilon.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Inapproximabilty of densest kk-subgraph from average case hardness",
      "abstract": "",
      "year": "2011",
      "venue": "Unpublished Manuscript",
      "authors": "Noga Alon, Sanjeev Arora, Rajsekar Manokaran, Dana Moshkovitz, and Omri Weinstein"
    },
    {
      "index": 1,
      "title": "Understanding Deep Neural Networks with Rectified Linear Units",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee",
      "orig_title": "Understanding deep neural networks with rectified linear units",
      "paper_id": "1611.01491v6"
    },
    {
      "index": 2,
      "title": "Minimum propositional proof length is NP-hard to linearly approximate",
      "abstract": "",
      "year": "2001",
      "venue": "J. Symb. Log.",
      "authors": "Michael Alekhnovich, SamuelR. Buss, Shlomo Moran, and Toniann Pitassi"
    },
    {
      "index": 3,
      "title": "On basing lower-bounds for learning on worst-case assumptions",
      "abstract": "",
      "year": "2008",
      "venue": "FOCS",
      "authors": "Benny Applebaum, Boaz Barak, and David Xiao"
    },
    {
      "index": 4,
      "title": "How to refute a random CSP",
      "abstract": "",
      "year": "2015",
      "venue": "FOCS",
      "authors": "SarahR. Allen, Ryan ODonnell, and David Witmer"
    },
    {
      "index": 5,
      "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang",
      "orig_title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
      "paper_id": "1811.04918v6"
    },
    {
      "index": 6,
      "title": "Convex Optimization: Algorithms and Complexity",
      "abstract": "",
      "year": "2015",
      "venue": "Foundations and Trends庐 in Machine Learning",
      "authors": "S茅bastien Bubeck etal.",
      "orig_title": "Convex optimization: Algorithms and complexity",
      "paper_id": "1405.4980v2"
    },
    {
      "index": 7,
      "title": "Breaking the curse of dimensionality with convex neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Francis Bach"
    },
    {
      "index": 8,
      "title": "Polynomial integrality gaps for strong SDP relaxations of densest kk-subgraph",
      "abstract": "",
      "year": "2012",
      "venue": "SODA",
      "authors": "Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan, Venkatesan Guruswami, and Yuan Zhou"
    },
    {
      "index": 9,
      "title": "Complexity of Training ReLU Neural Network",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1809.10787",
      "authors": "Digvijay Boob, SantanuS Dey, and Guanghui Lan",
      "orig_title": "Complexity of training relu neural network",
      "paper_id": "1809.10787v2"
    },
    {
      "index": 10,
      "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1702.07966",
      "authors": "Alon Brutzkus and Amir Globerson",
      "orig_title": "Globally optimal gradient descent for a convnet with gaussian inputs",
      "paper_id": "1702.07966v1"
    },
    {
      "index": 11,
      "title": "NP-hardness of coloring 2-colorable hypergraph with poly-logarithmically many colors",
      "abstract": "",
      "year": "2018",
      "venue": "45th International Colloquium on Automata, Languages, and Programming (ICALP 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik",
      "authors": "Amey Bhangale"
    },
    {
      "index": 12,
      "title": "Learning Two Layer Rectified Neural Networks in Polynomial Time",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory",
      "authors": "Ainesh Bakshi, Rajesh Jayaram, and DavidP Woodruff",
      "orig_title": "Learning two layer rectified neural networks in polynomial time",
      "paper_id": "1811.01885v1"
    },
    {
      "index": 13,
      "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "PeterL. Bartlett and Shahar Mendelson"
    },
    {
      "index": 14,
      "title": "Training a 3-node neural network is NP-complete",
      "abstract": "",
      "year": "1989",
      "venue": "Advances in neural information processing systems",
      "authors": "Avrim Blum and RonaldL Rivest"
    },
    {
      "index": 15,
      "title": "From Gap-ETH to FPT-Inapproximability: Clique, Dominating Set, and More",
      "abstract": "",
      "year": "2017",
      "venue": "FOCS",
      "authors": "Parinya Chalermsook, Marek Cygan, Guy Kortsarz, Bundit Laekhanukit, Pasin Manurangsi, Danupon Nanongkai, and Luca Trevisan",
      "orig_title": "From Gap-ETH to FPT-inapproximability: Clique, dominating set, and more",
      "paper_id": "1708.04218v1"
    },
    {
      "index": 16,
      "title": "Approximation algorithms for label cover and the log-density threshold",
      "abstract": "",
      "year": "2017",
      "venue": "SODA",
      "authors": "Eden Chlamt谩c, Pasin Manurangsi, Dana Moshkovitz, and Aravindan Vijayaraghavan"
    },
    {
      "index": 17,
      "title": "Approximation by superpositions of a sigmoidal function",
      "abstract": "",
      "year": "1989",
      "venue": "Mathematics of control, signals and systems",
      "authors": "George Cybenko"
    },
    {
      "index": 18,
      "title": "Polynomially low error PCPs with polyloglog n queries via modular composition",
      "abstract": "",
      "year": "2015",
      "venue": "STOC",
      "authors": "Irit Dinur, Prahladh Harsha, and Guy Kindler"
    },
    {
      "index": 19,
      "title": "Mildly exponential reduction from gap 3SAT to polynomial-gap label-cover",
      "abstract": "",
      "year": "2016",
      "venue": "Electronic Colloquium on Computational Complexity (ECCC)",
      "authors": "Irit Dinur"
    },
    {
      "index": 20,
      "title": "Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ilias Diakonikolas, Daniel Kane, and Pasin Manurangsi",
      "orig_title": "Nearly tight bounds for robust proper learning of halfspaces with a margin",
      "paper_id": "1908.11335v1"
    },
    {
      "index": 21,
      "title": "Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin",
      "abstract": "",
      "year": "2019",
      "venue": "CoRR",
      "authors": "Ilias Diakonikolas, DanielM. Kane, and Pasin Manurangsi",
      "orig_title": "Nearly tight bounds for robust proper learning of halfspaces with a margin",
      "paper_id": "1908.11335v1"
    },
    {
      "index": 22,
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.03804",
      "authors": "SimonS Du, JasonD Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai",
      "orig_title": "Gradient descent finds global minima of deep neural networks",
      "paper_id": "1811.03804v4"
    },
    {
      "index": 23,
      "title": "The hardness of 3-uniform hypergraph coloring",
      "abstract": "",
      "year": "2005",
      "venue": "Combinatorica",
      "authors": "Irit Dinur, Oded Regev, and CliffordD. Smyth"
    },
    {
      "index": 24,
      "title": "On the hardness of approximating label-cover",
      "abstract": "",
      "year": "2004",
      "venue": "Inf. Process. Lett.",
      "authors": "Irit Dinur and Shmuel Safra"
    },
    {
      "index": 25,
      "title": "An approximation algorithm for training one-node relu neural network",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.03592",
      "authors": "SantanuS Dey, Guanyi Wang, and Yao Xie"
    },
    {
      "index": 26,
      "title": "A threshold of ln n for approximating set cover",
      "abstract": "",
      "year": "1998",
      "venue": "J. ACM",
      "authors": "Uriel Feige"
    },
    {
      "index": 27,
      "title": "Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Surbhi Goel, Sushrut Karmalkar, and Adam Klivans"
    },
    {
      "index": 28,
      "title": "Reliably learning the relu in polynomial time",
      "abstract": "",
      "year": "2017",
      "venue": "COLT",
      "authors": "Surbhi Goel, Varun Kanade, AdamR. Klivans, and Justin Thaler"
    },
    {
      "index": 29,
      "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "abstract": "",
      "year": "1992",
      "venue": "Inf. Comput.",
      "authors": "David Haussler"
    },
    {
      "index": 30,
      "title": "Multilayer feedforward networks are universal approximators",
      "abstract": "",
      "year": "1989",
      "venue": "Neural networks",
      "authors": "Kurt Hornik, Maxwell Stinchcombe, and Halbert White"
    },
    {
      "index": 31,
      "title": "On the complexity of SAT",
      "abstract": "",
      "year": "2001",
      "venue": "J. Comput. Syst. Sci.",
      "authors": "Russell Impagliazzo and Ramamohan Paturi",
      "orig_title": "On the complexity of k-SAT",
      "paper_id": "2111.06967v3"
    },
    {
      "index": 32,
      "title": "Which problems have strongly exponential complexity?",
      "abstract": "",
      "year": "2001",
      "venue": "J. Comput. Syst. Sci.",
      "authors": "Russell Impagliazzo, Ramamohan Paturi, and Francis Zane"
    },
    {
      "index": 33,
      "title": "On the complexity of loading shallow neural networks",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Complexity",
      "authors": "Stephen Judd"
    },
    {
      "index": 34,
      "title": "Reducibility among combinatorial problems",
      "abstract": "",
      "year": "1972",
      "venue": "symposium on the Complexity of Computer Computations",
      "authors": "RichardM. Karp"
    },
    {
      "index": 35,
      "title": "Toward efficient agnostic learning",
      "abstract": "",
      "year": "1994",
      "venue": "Machine Learning",
      "authors": "MichaelJ. Kearns, RobertE. Schapire, and Linda Sellie"
    },
    {
      "index": 36,
      "title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "ShamM. Kakade, Karthik Sridharan, and Ambuj Tewari"
    },
    {
      "index": 37,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "nature",
      "authors": "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 38,
      "title": "Coverings and colorings of hypergraphs",
      "abstract": "",
      "year": "1973",
      "venue": "Proc. 4th Southeastern Conf. on Comb.",
      "authors": "Laszlo Lovasz"
    },
    {
      "index": 39,
      "title": "On the Computational Efficiency of Training Neural Networks",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir",
      "orig_title": "On the computational efficiency of training neural networks",
      "paper_id": "1410.1141v2"
    },
    {
      "index": 40,
      "title": "Probability in Banach Spaces: isoperimetry and processes",
      "abstract": "",
      "year": "1991",
      "venue": "Springer, Berlin",
      "authors": "Michel Ledoux and Michel Talagrand"
    },
    {
      "index": 41,
      "title": "On the hardness of approximating minimization problems",
      "abstract": "",
      "year": "1994",
      "venue": "J. ACM",
      "authors": "Carsten Lund and Mihalis Yannakakis"
    },
    {
      "index": 42,
      "title": "On approximating projection games",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Pasin Manurangsi"
    },
    {
      "index": 43,
      "title": "Almost-polynomial ratio ETH-hardness of approximating densest kk-subgraph",
      "abstract": "",
      "year": "2017",
      "venue": "STOC",
      "authors": "Pasin Manurangsi"
    },
    {
      "index": 44,
      "title": "On the complexity of polyhedral separability",
      "abstract": "",
      "year": "1988",
      "venue": "Discrete & Computational Geometry",
      "authors": "Nimrod Megiddo"
    },
    {
      "index": 45,
      "title": "A birthday repetition theorem and complexity of approximating dense csps",
      "abstract": "",
      "year": "2017",
      "venue": "44th International Colloquium on Automata, Languages, and Programming, ICALP 2017",
      "authors": "Pasin Manurangsi and Prasad Raghavendra"
    },
    {
      "index": 46,
      "title": "The Computational Complexity of Training ReLU(s)",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Pasin Manurangsi and Daniel Reichman",
      "orig_title": "The computational complexity of training relu(s)",
      "paper_id": "1810.04207v2"
    },
    {
      "index": 47,
      "title": "The hardness of approximation: Gap location",
      "abstract": "",
      "year": "1994",
      "venue": "Computational Complexity",
      "authors": "Erez Petrank"
    },
    {
      "index": 48,
      "title": "Expressiveness of rectifier networks",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Xingyuan Pan and Vivek Srikumar"
    },
    {
      "index": 49,
      "title": "Beyond the worst-case analysis of algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Tim Roughgarden"
    },
    {
      "index": 50,
      "title": "Linear level lasserre lower bounds for certain k-CSPs",
      "abstract": "",
      "year": "2008",
      "venue": "FOCS",
      "authors": "Grant Schoenebeck"
    },
    {
      "index": 51,
      "title": "Refined complexity of PCA with outliers",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Kirill Simonov, Fedor Fomin, Petr Golovach, and Fahad Panolan"
    },
    {
      "index": 52,
      "title": "Smoothness, low noise and fast rates",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nathan Srebro, Karthik Sridharan, and Ambuj Tewari"
    },
    {
      "index": 53,
      "title": "What circuit classes can be learned with non-trivial savings?",
      "abstract": "",
      "year": "2017",
      "venue": "8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik",
      "authors": "RoccoA Servedio and Li-Yang Tan"
    },
    {
      "index": 54,
      "title": "Planar 3-colorability is polynomial complete",
      "abstract": "",
      "year": "1973",
      "venue": "SIGACT News",
      "authors": "Larry Stockmeyer"
    },
    {
      "index": 55,
      "title": "On the Complexity of Learning Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "LeSong, Santosh Vempala, John Wilmes, and BoXie",
      "orig_title": "On the complexity of learning neural networks",
      "paper_id": "1707.04615v1"
    },
    {
      "index": 56,
      "title": "CSP gaps and reductions in the lasserre hierarchy",
      "abstract": "",
      "year": "2009",
      "venue": "STOC",
      "authors": "Madhur Tulsiani"
    },
    {
      "index": 57,
      "title": "On the infeasibility of training neural networks with small mean-squared error",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "VanH Vu"
    },
    {
      "index": 58,
      "title": "Polynomial convergence of gradient descent for training one-hidden-layer neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory",
      "authors": "Santosh Vempala and John Wilmes"
    },
    {
      "index": 59,
      "title": "Understanding deep learning requires rethinking generalization",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.03530",
      "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals"
    }
  ]
}