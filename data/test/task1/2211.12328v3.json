{
  "paper_id": "2211.12328v3",
  "title": "A survey on knowledge-enhanced multimodal learning",
  "abstract": "Abstract\nMultimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. In the same time, knowledge graphs enhance explainability, fairness and validity of decision making, issues of outermost importance for such complex implementations. The current survey aims to unify the fields of VL representation learning and knowledge graphs, and provides a taxonomy and analysis of knowledge-enhanced VL models.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Imdb",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Meet the flintstones dataset",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 2,
      "title": "Movie genre from its poster",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 3,
      "title": "Integrating rule-based entity masking into image captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Dietrich Klakow Aditya Mogadala, Xiaoyu Shen"
    },
    {
      "index": 4,
      "title": "Vqa: Visual question answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh"
    },
    {
      "index": 5,
      "title": "Fusion of Detected Objects in Text for Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter",
      "orig_title": "Fusion of detected objects in text for visual question answering",
      "paper_id": "1908.05054v2"
    },
    {
      "index": 6,
      "title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould",
      "orig_title": "Spice: Semantic propositional image caption evaluation",
      "paper_id": "1607.08822v1"
    },
    {
      "index": 7,
      "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel",
      "orig_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
      "paper_id": "1711.07280v3"
    },
    {
      "index": 8,
      "title": "Dbpedia: A nucleus for a web of open data",
      "abstract": "",
      "year": "2007",
      "venue": "ISWC/ASWC",
      "authors": "Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives"
    },
    {
      "index": 9,
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency",
      "orig_title": "Multimodal machine learning: A survey and taxonomy",
      "paper_id": "1705.09406v2"
    },
    {
      "index": 10,
      "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "abstract": "",
      "year": "2005",
      "venue": "ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
      "authors": "Satanjeev Banerjee and Alon Lavie"
    },
    {
      "index": 11,
      "title": "Ernie-nli: Analyzing the impact of domain-specific external knowledge on enhanced representations for nli",
      "abstract": "",
      "year": "2021",
      "venue": "DEELIO",
      "authors": "Lisa Bauer, Lingjia Deng, and Mohit Bansal"
    },
    {
      "index": 12,
      "title": "Do Dogs have Whiskers? A New Knowledge Base of hasPart Relations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Sumithra Bhakthavatsalam, Kyle Richardson, Niket Tandon, and Peter Clark",
      "orig_title": "Do dogs have whiskers? a new knowledge base of haspart relations",
      "paper_id": "2006.07510v1"
    },
    {
      "index": 13,
      "title": "Enriching Word Vectors with Subword Information",
      "abstract": "",
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov",
      "orig_title": "Enriching word vectors with subword information",
      "paper_id": "1607.04606v2"
    },
    {
      "index": 14,
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi"
    },
    {
      "index": 15,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 16,
      "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu",
      "orig_title": "Behind the scene: Revealing the secrets of pre-trained vision-and-language models",
      "paper_id": "2005.07310v2"
    },
    {
      "index": 17,
      "title": "Explainable high-order visual question reasoning: A new benchmark and knowledge-routed network",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Qingxing Cao, Bailin Li, Xiaodan Liang, and Liang Lin"
    },
    {
      "index": 18,
      "title": "Universal Sentence Encoder",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil",
      "orig_title": "Universal sentence encoder",
      "paper_id": "1803.11175v2"
    },
    {
      "index": 19,
      "title": "KG-GAN: Knowledge-Guided Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Che-Han Chang, Chun-Hsien Yu, Szu-Ying Chen, and Edward Y. Chang",
      "orig_title": "Kg-gan: Knowledge-guided generative adversarial networks",
      "paper_id": "1905.12261v2"
    },
    {
      "index": 20,
      "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Hila Chefer, Shir Gur, and Lior Wolf",
      "orig_title": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers",
      "paper_id": "2103.15679v1"
    },
    {
      "index": 21,
      "title": "Commonsense Knowledge Aware Concept Selection For Diverse and Informative Visual Storytelling",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Hong Chen, Yifei Huang, Hiroya Takamura, and Hideki Nakayama",
      "orig_title": "Commonsense knowledge aware concept selection for diverse and informative visual storytelling",
      "paper_id": "2102.02963v1"
    },
    {
      "index": 22,
      "title": "Kb-vlp: Knowledge based vision and language pretraining",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Kezhen Chen, Qiuyuan Huang, Yonatan Bisk, Daniel J. McDuff, and Jianfeng Gao"
    },
    {
      "index": 23,
      "title": "Neural Natural Language Inference Models Enhanced with External Knowledge",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Qian Chen, Xiao-Dan Zhu, Zhenhua Ling, Diana Inkpen, and Si Wei",
      "orig_title": "Neural natural language inference models enhanced with external knowledge",
      "paper_id": "1711.04289v3"
    },
    {
      "index": 24,
      "title": "Neural Natural Language Inference Models Enhanced with External Knowledge",
      "abstract": "",
      "year": "2018",
      "venue": "Association for Computational Linguistics",
      "authors": "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and Si Wei",
      "orig_title": "Neural natural language inference models enhanced with external knowledge",
      "paper_id": "1711.04289v3"
    },
    {
      "index": 25,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu",
      "orig_title": "Uniter: Universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 26,
      "title": "Zero-shot Visual Question Answering using Knowledge Graph",
      "abstract": "",
      "year": "2021",
      "venue": "SEMWEB",
      "authors": "Zhuo Chen, Jiaoyan Chen, Yuxia Geng, Jeff Z. Pan, Zonggang Yuan, and Huajun Chen",
      "orig_title": "Zero-shot visual question answering using knowledge graph",
      "paper_id": "2107.05348v4"
    },
    {
      "index": 27,
      "title": "Unifying vision-and-language tasks via text generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal"
    },
    {
      "index": 28,
      "title": "X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP",
      "authors": "Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha Kembhavi",
      "orig_title": "X-lxmert: Paint, caption and answer questions with multi-modal transformers",
      "paper_id": "2009.11278v1"
    },
    {
      "index": 29,
      "title": "On the properties of neural machine translation: Encoder-decoder approaches",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio"
    },
    {
      "index": 30,
      "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Leyang Cui, Yu Wu, Shujie Liu, and Yue Zhang",
      "orig_title": "Knowledge enhanced fine-tuning for better handling unseen entities in dialogue generation",
      "paper_id": "2109.05487v1"
    },
    {
      "index": 31,
      "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration",
      "abstract": "",
      "year": "2021",
      "venue": "ACM International Conference on Multimedia",
      "authors": "Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and Jun Yu",
      "orig_title": "Rosita: Enhancing vision-and-language semantic alignments via cross- and intra-modal knowledge integration",
      "paper_id": "2108.07073v1"
    },
    {
      "index": 32,
      "title": "Visual Dialog",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra",
      "orig_title": "Visual dialog",
      "paper_id": "1611.08669v5"
    },
    {
      "index": 33,
      "title": "GuessWhat?! Visual object discovery through multi-modal dialogue",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville",
      "orig_title": "Guesswhat?! visual object discovery through multi-modal dialogue",
      "paper_id": "1611.08481v2"
    },
    {
      "index": 34,
      "title": "Imagenet: A large-scale hierarchical image database",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei"
    },
    {
      "index": 35,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 36,
      "title": "External knowledge enabled text visual question answering",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Arka Ujjal Dey, Ernest Valveny, and Gaurav Harit"
    },
    {
      "index": 37,
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Prafulla Dhariwal and Alex Nichol",
      "orig_title": "Diffusion models beat gans on image synthesis",
      "paper_id": "2105.05233v4"
    },
    {
      "index": 38,
      "title": "e-snli-ve: Corrected visual-textual entailment with natural language explanations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Virginie Do, Oana-Maria Camburu, Zeynep Akata, and Thomas Lukasiewicz"
    },
    {
      "index": 39,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 40,
      "title": "A survey of vision-language pre-trained models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yifan Du, Zikang Liu, Junyi Li, and Wayne Zhao"
    },
    {
      "index": 41,
      "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "authors": "Shiv Ram Dubey",
      "orig_title": "A decade survey of content based image retrieval using deep learning",
      "paper_id": "2012.00641v2"
    },
    {
      "index": 42,
      "title": "Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W. Taylor"
    },
    {
      "index": 43,
      "title": "Multi30K: Multilingual English-German Image Descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "Workshop on Vision and Language",
      "authors": "D. Elliott, S. Frank, K. Sima’an, and L. Specia",
      "orig_title": "Multi30k: Multilingual english-german image descriptions",
      "paper_id": "1605.00459v1"
    },
    {
      "index": 44,
      "title": "An attention-based regression model for grounding textual phrases in images",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Ko Endo, Masaki Aono, Eric Nichols, and Kotaro Funakoshi"
    },
    {
      "index": 45,
      "title": "Taming Transformers for High-Resolution Image Synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Patrick Esser, Robin Rombach, and Björn Ommer",
      "orig_title": "Taming transformers for high-resolution image synthesis",
      "paper_id": "2012.09841v3"
    },
    {
      "index": 46,
      "title": "Wordnet: An electronic lexical database",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": "Christiane Fellbaum"
    },
    {
      "index": 47,
      "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Difei Gao, Ruiping Wang, S. Shan, and Xilin Chen",
      "orig_title": "Cric: A vqa dataset for compositional reasoning on vision and commonsense",
      "paper_id": "1908.02962v3"
    },
    {
      "index": 48,
      "title": "How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Noa Garcia and George Vogiatzis",
      "orig_title": "How to read paintings: Semantic art understanding with multi-modal retrieval",
      "paper_id": "1810.09617v1"
    },
    {
      "index": 49,
      "title": "A Dataset and Baselines for Visual Question Answering on Art",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu Otani, Chenhui Chu, Yuta Nakashima, and Teruko Mitamura",
      "orig_title": "A dataset and baselines for visual question answering on art",
      "paper_id": "2008.12520v1"
    },
    {
      "index": 50,
      "title": "Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Diego Garcia-Olano, Yasumasa Onoe, and Joydeep Ghosh",
      "orig_title": "Improving and diagnosing knowledge-based visual question answering via entity enhanced knowledge injection",
      "paper_id": "2112.06888v1"
    },
    {
      "index": 51,
      "title": "Conceptbert: Concept-aware representation for visual question answering",
      "abstract": "",
      "year": "2020",
      "venue": "FINDINGS",
      "authors": "François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and Freddy\nLécué. 2020."
    },
    {
      "index": 52,
      "title": "OntoZSL: Ontology-enhanced Zero-shot Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the Web Conference",
      "authors": "Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Jeff Z. Pan, Zhiquan Ye, Zonggang Yuan,\nYantao Jia, and Huajun Chen. 2021.",
      "orig_title": "Ontozsl: Ontology-enhanced zero-shot learning",
      "paper_id": "2102.07339v1"
    },
    {
      "index": 53,
      "title": "Generative adversarial zero-shot learning via knowledge graphs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Zhiquan Ye, Zonggang Yuan, Yantao Jia, and\nHuajun Chen. 2020."
    },
    {
      "index": 54,
      "title": "Fast R-CNN",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Ross Girshick. 2015.",
      "orig_title": "Fast r-cnn",
      "paper_id": "1504.08083v2"
    },
    {
      "index": 55,
      "title": "Injecting Prior Knowledge into Image Caption Generation",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV Workshops",
      "authors": "Arushi Goel, Basura Fernando, Thanh-Son Nguyen, and Hakan Bilen. 2020.",
      "orig_title": "Injecting prior knowledge into image caption generation",
      "paper_id": "1911.10082v2"
    },
    {
      "index": 56,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014."
    },
    {
      "index": 57,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\n2016.",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 58,
      "title": "Design of the muc-6 evaluation",
      "abstract": "",
      "year": "1996",
      "venue": "Proceedings of a Workshop on Held at Vienna, Virginia: May\n6-8, 1996, TIPSTER ’96, page 413–422, USA. Association for Computational\nLinguistics",
      "authors": "Ralph Grishman and Beth Sundheim. 1996."
    },
    {
      "index": 59,
      "title": "node2vec: Scalable Feature Learning for Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining",
      "authors": "Aditya Grover and Jure Leskovec. 2016.",
      "orig_title": "node2vec: Scalable feature learning for networks",
      "paper_id": "1607.00653v1"
    },
    {
      "index": 60,
      "title": "Deep multimodal representation learning: A survey",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access, 7:",
      "authors": "Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019."
    },
    {
      "index": 61,
      "title": "Graph representation learning",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "William L. Hamilton."
    },
    {
      "index": 62,
      "title": "Representation Learning on Graphs: Methods and Applications",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "William L. Hamilton, Rex Ying, and Jure Leskovec. 2018.",
      "orig_title": "Representation learning on graphs: Methods and applications",
      "paper_id": "1709.05584v3"
    },
    {
      "index": 63,
      "title": "Interpretable visual reasoning: A survey",
      "abstract": "",
      "year": "",
      "venue": "Image and Vision Computing, 112:",
      "authors": "Feijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. 2021a."
    },
    {
      "index": 64,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 65,
      "title": "Context-Aware Layout to Image Generation with Enhanced Object Appearance",
      "abstract": "",
      "year": "",
      "venue": "CVPR",
      "authors": "Sen He, Wentong Liao, Michael Yang, Yongxin Yang, Yi-Zhe Song, Bodo Rosenhahn,\nand Tao Xiang. 2021b.",
      "orig_title": "Context-aware layout to image generation with enhanced object appearance",
      "paper_id": "2103.11897v1"
    },
    {
      "index": 66,
      "title": "Fine-grained Visual-textual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video\nTechnology, 30(2):520–531",
      "authors": "Xiangteng He and Yuxin Peng. 2020.",
      "orig_title": "Fine-grained visual-textual representation learning",
      "paper_id": "1709.00340v4"
    },
    {
      "index": 67,
      "title": "Generating Visual Explanations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt\nSchiele, and Trevor Darrell. 2016.",
      "orig_title": "Generating visual explanations",
      "paper_id": "1603.08507v1"
    },
    {
      "index": 68,
      "title": "Localizing Moments in Video with Natural Language",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,\nand Bryan Russell. 2017.",
      "orig_title": "Localizing moments in video with natural language",
      "paper_id": "1708.01641v1"
    },
    {
      "index": 69,
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp\nHochreiter. 2017.",
      "orig_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "paper_id": "1706.08500v6"
    },
    {
      "index": 70,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation, 9:",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber. 1997."
    },
    {
      "index": 71,
      "title": "Relational Reasoning using Prior Knowledge for Visual Captioning",
      "abstract": "",
      "year": "1906",
      "venue": "ArXiv, abs/",
      "authors": "Jingyi Hou, Xinxiao Wu, Yayun Qi, Wentian Zhao, Jiebo Luo, and Yunde Jia. 2019.",
      "orig_title": "Relational reasoning using prior knowledge for visual captioning",
      "paper_id": "1906.01290v1"
    },
    {
      "index": 72,
      "title": "Joint commonsense and relation reasoning for image and video captioning",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Jingyi Hou, Xinxiao Wu, Xiaoxun Zhang, Yayun Qi, Yunde Jia, and Jiebo Luo.\n2020."
    },
    {
      "index": 73,
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Jeremy Howard and Sebastian Ruder. 2018.",
      "orig_title": "Universal language model fine-tuning for text classification",
      "paper_id": "1801.06146v5"
    },
    {
      "index": 74,
      "title": "Knowledge-Enriched Visual Storytelling",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin,\nTing-Hao ’Kenneth’ Huang, and Lun-Wei Ku. 2019.",
      "orig_title": "Knowledge-enriched visual storytelling",
      "paper_id": "1912.01496v1"
    },
    {
      "index": 75,
      "title": "Heterogeneous Graph Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of The Web Conference",
      "authors": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020.",
      "orig_title": "Heterogeneous graph transformer",
      "paper_id": "2003.01332v1"
    },
    {
      "index": 76,
      "title": "Image captioning with internal and external knowledge",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management",
      "authors": "Feicheng Huang, Zhixin Li, Shengjia Chen, Canlong Zhang, and Huifang Ma.\n2020a."
    },
    {
      "index": 77,
      "title": "Visual Storytelling",
      "abstract": "",
      "year": "2016",
      "venue": "15th Annual Conference of the North American Chapter of the\nAssociation for Computational Linguistics (NAACL",
      "authors": "Ting-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Jacob\nDevlin, Aishwarya Agrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv\nBatra, et al. 2016a.",
      "orig_title": "Visual storytelling",
      "paper_id": "1604.03968v1"
    },
    {
      "index": 78,
      "title": "Visual Storytelling",
      "abstract": "",
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 1233–1239, San Diego, California. Association for\nComputational Linguistics",
      "authors": "Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra,\nAishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli,\nDhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel\nGalley, and Margaret Mitchell. 2016b.",
      "orig_title": "Visual storytelling",
      "paper_id": "1604.03968v1"
    },
    {
      "index": 79,
      "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong\nFu. 2021.",
      "orig_title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
      "paper_id": "2104.03135v2"
    },
    {
      "index": 80,
      "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu.\n2020b.",
      "orig_title": "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers",
      "paper_id": "2004.00849v2"
    },
    {
      "index": 81,
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Drew A. Hudson and Christopher D. Manning. 2019."
    },
    {
      "index": 82,
      "title": "Wenlan: Bridging vision and language by large-scale multi-modal pre-training",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke\nSakaguchi, Antoine Bosselut, and Yejin Choi. 2021."
    },
    {
      "index": 84,
      "title": "Dimensions of Commonsense Knowledge",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L.\nMcGuinness, and Pedro Szekely. 2021.",
      "orig_title": "Dimensions of commonsense knowledge",
      "paper_id": "2101.04640v2"
    },
    {
      "index": 85,
      "title": "Judging a Book by its Cover",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, and\nSeiichi Uchida. 2017.",
      "orig_title": "Judging a book by its cover",
      "paper_id": "1610.09204v3"
    },
    {
      "index": 86,
      "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval",
      "authors": "Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh\nRamakrishnan, and Soumen Chakrabarti. 2021.",
      "orig_title": "Select, substitute, search: A new benchmark for knowledge-augmented visual question answering",
      "paper_id": "2103.05568v3"
    },
    {
      "index": 87,
      "title": "A Survey on Knowledge Graphs: Representation, Acquisition and Applications",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE transactions on neural networks and learning systems, PP",
      "authors": "Shaoxiong Ji, Shirui Pan, E. Cambria, P. Marttinen, and Philip S. Yu. 2021.",
      "orig_title": "A survey on knowledge graphs: Representation, acquisition and applications",
      "paper_id": "2002.00388v4"
    },
    {
      "index": 88,
      "title": "Leveraging concept-enhanced pre-training model and masked-entity language model for named entity disambiguation",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access, 8:",
      "authors": "Zizheng Ji, Lin Dai, Jin Pang, and Tingting Shen. 2020."
    },
    {
      "index": 89,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.\nLe, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021."
    },
    {
      "index": 90,
      "title": "Image Generation from Scene Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018.",
      "orig_title": "Image generation from scene graphs",
      "paper_id": "1804.01622v1"
    },
    {
      "index": 91,
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick. 2017.",
      "orig_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "paper_id": "1612.06890v1"
    },
    {
      "index": 92,
      "title": "Challenges and Prospects in Vision and Language Research",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Kushal Kafle, Robik Shrestha, and Christopher Kanan. 2019.",
      "orig_title": "Challenges and prospects in vision and language research",
      "paper_id": "1904.09317v2"
    },
    {
      "index": 93,
      "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Tero Karras, Samuli Laine, and Timo Aila. 2018.",
      "orig_title": "A style-based generator architecture for generative adversarial networks",
      "paper_id": "1812.04948v3"
    },
    {
      "index": 94,
      "title": "Deepstory: Video story qa by deep embedded memory networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017."
    },
    {
      "index": 95,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021.",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 96,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Thomas N Kipf and Max Welling. 2016.",
      "orig_title": "Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 97,
      "title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus\nRohrbach. 2019.",
      "orig_title": "Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog",
      "paper_id": "1903.03166v2"
    },
    {
      "index": 98,
      "title": "Referring Relationships",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Ranjay Krishna, Ines Chami, Michael Bernstein, and Li Fei-Fei. 2018.",
      "orig_title": "Referring relationships",
      "paper_id": "1803.10362v2"
    },
    {
      "index": 99,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua\nKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Fei-Fei Li. 2016."
    },
    {
      "index": 100,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Alex Krizhevsky. 2009."
    },
    {
      "index": 101,
      "title": "Distributed Representations of Sentences and Documents",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Quoc V. Le and Tomas Mikolov. 2014.",
      "orig_title": "Distributed representations of sentences and documents",
      "paper_id": "1405.4053v2"
    },
    {
      "index": 102,
      "title": "Vision–language–knowledge co-embedding for visual commonsense reasoning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "JaeYun Lee and Incheol Kim. 2020."
    },
    {
      "index": 103,
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.",
      "orig_title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "paper_id": "1910.13461v1"
    },
    {
      "index": 104,
      "title": "Improved-storygan for sequential images visualization",
      "abstract": "",
      "year": "",
      "venue": "Journal of Visual Communication and Image Representation,\n73:",
      "authors": "Chunye Li, Liya Kong, and Zhiping Zhou. 2020a."
    },
    {
      "index": 105,
      "title": "Boosting visual question answering with context-aware knowledge aggregation",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 28th ACM International Conference on\nMultimedia",
      "authors": "Guohao Li, Xin Wang, and Wenwu Zhu. 2020b."
    },
    {
      "index": 106,
      "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015.",
      "orig_title": "A diversity-promoting objective function for neural conversation models",
      "paper_id": "1510.03055v3"
    },
    {
      "index": 107,
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,\nCaiming Xiong, and Steven Hoi. 2021.",
      "orig_title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "paper_id": "2107.07651v2"
    },
    {
      "index": 108,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.\n2019a.",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 109,
      "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. 2018.",
      "orig_title": "Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions",
      "paper_id": "1803.07464v2"
    },
    {
      "index": 110,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan\nWang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.\n2020c.",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 111,
      "title": "PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang.\n2019b.",
      "orig_title": "Pastegan: A semi-parametric method to generate image from scene graph",
      "paper_id": "1905.01608v2"
    },
    {
      "index": 112,
      "title": "StoryGAN: A Sequential Conditional GAN for Story Visualization",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence\nCarin, David Carlson, and Jianfeng Gao. 2019c.",
      "orig_title": "Storygan: A sequential conditional gan for story visualization",
      "paper_id": "1812.02784v2"
    },
    {
      "index": 113,
      "title": "ROUGE: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "Chin-Yew Lin. 2004."
    },
    {
      "index": 114,
      "title": "InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang.\n2021.",
      "orig_title": "Interbert: Vision-and-language interaction for multi-modal pretraining",
      "paper_id": "2003.13198v4"
    },
    {
      "index": 115,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "Computer Vision – ECCV 2014, pages 740–755, Cham.\nSpringer International Publishing",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 116,
      "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Zachary C. Lipton, John Berkowitz, and Charles Elkan. 2015.",
      "orig_title": "A critical review of recurrent neural networks for sequence learning",
      "paper_id": "1506.00019v4"
    },
    {
      "index": 117,
      "title": "Knowledge based multilingual language model",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq R. Joty, and Luo Si.\n2021a."
    },
    {
      "index": 118,
      "title": "CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Runtao Liu, Chenxi Liu, Yutong Bai, and Alan Yuille. 2019a.",
      "orig_title": "Clevr-ref+: Diagnosing visual reasoning with referring expressions",
      "paper_id": "1901.00850v2"
    },
    {
      "index": 119,
      "title": "Fusedream: Training-free text-to-image generation with improved clip+gan space optimization",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu.\n2021b."
    },
    {
      "index": 120,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 121,
      "title": "Swin transformer v2: Scaling up capacity and resolution",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Computer Vision and Pattern\nRecognition (CVPR)",
      "authors": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue\nCao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. 2022."
    },
    {
      "index": 122,
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV)",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. 2021c."
    },
    {
      "index": 123,
      "title": "Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 5962–5971, Florence, Italy.\nAssociation for Computational Linguistics",
      "authors": "Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh.\n2019."
    },
    {
      "index": 124,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019."
    },
    {
      "index": 125,
      "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2020.",
      "orig_title": "12-in-1: Multi-task vision and language representation learning",
      "paper_id": "1912.02315v2"
    },
    {
      "index": 126,
      "title": "Kelm: Knowledge enhanced pre-trained language representations with message passing on hierarchical relational graphs",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Yinquan Lu, H. Lu, Guirong Fu, and Qun Liu. 2021."
    },
    {
      "index": 127,
      "title": "Weakly-supervised visual-retriever-reader for knowledge-based question answering",
      "abstract": "",
      "year": "2021",
      "venue": "EMNLP",
      "authors": "Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. 2021."
    },
    {
      "index": 128,
      "title": "Integrating visuospatial, linguistic, and commonsense structure into story visualization",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Adyasha Maharana and Mohit Bansal. 2021."
    },
    {
      "index": 129,
      "title": "Improving generation and evaluation of visual stories via semantic consistency",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2021."
    },
    {
      "index": 130,
      "title": "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022.",
      "orig_title": "Storydall-e: Adapting pretrained text-to-image transformers for story continuation",
      "paper_id": "2209.06192v1"
    },
    {
      "index": 131,
      "title": "Commonsense knowledge base completion with structural and semantic context",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Chaitanya Malaviya, Chandra Bhagavatula, Antoine Bosselut, and Yejin Choi.\n2019."
    },
    {
      "index": 132,
      "title": "Towards a Visual Turing Challenge",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Mateusz Malinowski and Mario Fritz. 2014.",
      "orig_title": "Towards a visual turing challenge",
      "paper_id": "1410.8027v3"
    },
    {
      "index": 133,
      "title": "Introduction to Information Retrieval",
      "abstract": "",
      "year": "2008",
      "venue": "Cambridge University Press, USA",
      "authors": "Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008."
    },
    {
      "index": 134,
      "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs",
      "abstract": "",
      "year": "2020",
      "venue": "FINDINGS",
      "authors": "Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A.\nSmith, and Yejin Choi. 2020.",
      "orig_title": "Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs",
      "paper_id": "2010.07526v1"
    },
    {
      "index": 135,
      "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA",
      "abstract": "",
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR)",
      "authors": "Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Kumar Gupta, and Marcus\nRohrbach. 2021.",
      "orig_title": "Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa",
      "paper_id": "2012.11014v1"
    },
    {
      "index": 136,
      "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR)",
      "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.",
      "orig_title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
      "paper_id": "1906.00067v2"
    },
    {
      "index": 137,
      "title": "Learned in Translation: Contextualized Word Vectors",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2018.",
      "orig_title": "Learned in translation: Contextualized word vectors",
      "paper_id": "1708.00107v2"
    },
    {
      "index": 138,
      "title": "Distributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013."
    },
    {
      "index": 139,
      "title": "Conditional Generative Adversarial Nets",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Mehdi Mirza and Simon Osindero. 2014.",
      "orig_title": "Conditional generative adversarial nets",
      "paper_id": "1411.1784v1"
    },
    {
      "index": 140,
      "title": "Trends in integration of vision and language research: A survey of tasks, datasets, and methods",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research, 71:",
      "authors": "Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. 2021."
    },
    {
      "index": 141,
      "title": "Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Medhini Narasimhan, Svetlana Lazebnik, and Alexander G. Schwing. 2018.",
      "orig_title": "Out of the box: Reasoning with graph convolution nets for factual visual question answering",
      "paper_id": "1811.00538v1"
    },
    {
      "index": 142,
      "title": "Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv, abs/",
      "authors": "Medhini Narasimhan and Alexander G. Schwing. 2018.",
      "orig_title": "Straight to the facts: Learning knowledge base retrieval for factual visual question answering",
      "paper_id": "1809.01124v1"
    },
    {
      "index": 143,
      "title": "graph2vec: Learning Distributed Representations of Graphs",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen,\nYang Liu, and Shantanu Jaiswal. 2017.",
      "orig_title": "graph2vec: Learning distributed representations of graphs",
      "paper_id": "1707.05005v1"
    },
    {
      "index": 144,
      "title": "Zero-Shot Learning with Common Sense Knowledge Graphs",
      "abstract": "",
      "year": "2006",
      "venue": "arXiv:",
      "authors": "Nihal V. Nayak and Stephen H. Bach. 2020.",
      "orig_title": "Zero-shot learning with common sense knowledge graphs",
      "paper_id": "2006.10713v4"
    },
    {
      "index": 145,
      "title": "Automated flower classification over a large number of classes",
      "abstract": "",
      "year": "2008",
      "venue": "2008 Sixth Indian Conference on Computer Vision, Graphics &\nImage Processing",
      "authors": "Maria-Elena Nilsback and Andrew Zisserman. 2008."
    },
    {
      "index": 146,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011."
    },
    {
      "index": 147,
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Association\nfor Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania,\nUSA. Association for Computational Linguistics",
      "authors": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002."
    },
    {
      "index": 148,
      "title": "Expressing an Image Stream with a Sequence of Natural Sentences",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "Cesc Chunseong Park and Gunhee Kim. 2015."
    },
    {
      "index": 149,
      "title": "Visualcomet: Reasoning about the dynamic context of a still image",
      "abstract": "",
      "year": "2020",
      "venue": "In Proceedings of the European Conference on Computer Vision\n(ECCV)",
      "authors": "Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin\nChoi. 2020."
    },
    {
      "index": 150,
      "title": "GloVe: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014."
    },
    {
      "index": 151,
      "title": "DeepWalk: Online Learning of Social Representations",
      "abstract": "",
      "year": "2014",
      "venue": "Proceedings of the 20th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, KDD ’14, pages 701–710, New York,\nNY, USA. ACM",
      "authors": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014.",
      "orig_title": "Deepwalk: Online learning of social representations",
      "paper_id": "1403.6652v2"
    },
    {
      "index": 152,
      "title": "Knowledge enhanced contextual word representations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Matthew E. Peters, Mark Neumann, Robert L. Logan IV au2, Roy Schwartz, Vidur\nJoshi, Sameer Singh, and Noah A. Smith. 2019."
    },
    {
      "index": 153,
      "title": "Deep contextualized word representations",
      "abstract": "",
      "year": "2018",
      "venue": "Proc. of NAACL",
      "authors": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018.",
      "orig_title": "Deep contextualized word representations",
      "paper_id": "1802.05365v2"
    },
    {
      "index": 154,
      "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT",
      "abstract": "",
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 803–818, Online. Association for Computational\nLinguistics",
      "authors": "Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.",
      "orig_title": "E-BERT: Efficient-yet-effective entity embeddings for BERT",
      "paper_id": "1911.03681v2"
    },
    {
      "index": 155,
      "title": "Grounded Situation Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi.\n2020.",
      "orig_title": "Grounded situation recognition",
      "paper_id": "2003.12058v1"
    },
    {
      "index": 156,
      "title": "Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning",
      "abstract": "",
      "year": "2021",
      "venue": "ACL/IJCNLP",
      "authors": "Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie\nHuang, Maosong Sun, and Jie Zhou. 2021."
    },
    {
      "index": 157,
      "title": "Passage Retrieval for Outside-Knowledge Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval",
      "authors": "Chen Qu, Hamed Zamani, Liu Yang, William Bruce Croft, and Erik G.\nLearned-Miller. 2021.",
      "orig_title": "Passage retrieval for outside-knowledge visual question answering",
      "paper_id": "2105.03938v1"
    },
    {
      "index": 158,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever.\n2019."
    },
    {
      "index": 159,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. 2021.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 160,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research, 21(140):1–67",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 161,
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.",
      "orig_title": "Hierarchical text-conditional image generation with clip latents",
      "paper_id": "2204.06125v1"
    },
    {
      "index": 162,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever. 2021.",
      "orig_title": "Zero-shot text-to-image generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 163,
      "title": "Learning What and Where to Draw",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and\nHonglak Lee. 2016a.",
      "orig_title": "Learning what and where to draw",
      "paper_id": "1610.02454v1"
    },
    {
      "index": 164,
      "title": "Learning Deep Representations of Fine-Grained Visual Descriptions",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Scott Reed, Zeynep Akata, Bernt Schiele, and Honglak Lee. 2016b.",
      "orig_title": "Learning deep representations of fine-grained visual descriptions",
      "paper_id": "1605.05395v1"
    },
    {
      "index": 165,
      "title": "Generative Adversarial Text to Image Synthesis",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of The 33rd International Conference on Machine\nLearning, volume 48 of Proceedings of Machine Learning Research",
      "authors": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and\nHonglak Lee. 2016c.",
      "orig_title": "Generative adversarial text to image synthesis",
      "paper_id": "1605.05396v2"
    },
    {
      "index": 166,
      "title": "Generative Adversarial Text to Image Synthesis",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and\nHonglak Lee. 2016d.",
      "orig_title": "Generative adversarial text to image synthesis",
      "paper_id": "1605.05396v2"
    },
    {
      "index": 167,
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Nils Reimers and Iryna Gurevych. 2019.",
      "orig_title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "paper_id": "1908.10084v1"
    },
    {
      "index": 168,
      "title": "Exploring models and data for image question answering",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Mengye Ren, Ryan Kiros, and Richard Zemel. 2015."
    },
    {
      "index": 169,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,\n39(06):",
      "authors": "S. Ren, K. He, R. Girshick, and J. Sun. 2017.",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 170,
      "title": "The probabilistic relevance framework: Bm25 and beyond",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Stephen Robertson and Hugo Zaragoza. 2009."
    },
    {
      "index": 171,
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\nOmmer. 2021.",
      "orig_title": "High-resolution image synthesis with latent diffusion models",
      "paper_id": "2112.10752v2"
    },
    {
      "index": 172,
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2022.",
      "orig_title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
      "paper_id": "2208.12242v2"
    },
    {
      "index": 173,
      "title": "Relational World Knowledge Representation in Contextual Language Models: A Review",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Tara Safavi and Danai Koutra. 2021.",
      "orig_title": "Relational world knowledge representation in contextual language models: A review",
      "paper_id": "2104.05837v2"
    },
    {
      "index": 174,
      "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily\nDenton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. 2022.",
      "orig_title": "Photorealistic text-to-image diffusion models with deep language understanding",
      "paper_id": "2205.11487v1"
    },
    {
      "index": 175,
      "title": "Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa Etxabe, and\nEneko Agirre. 2021.",
      "orig_title": "Image captioning for effective use of language models in knowledge-based visual question answering",
      "paper_id": "2109.08029v3"
    },
    {
      "index": 176,
      "title": "Improved Techniques for Training GANs",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and\nXi Chen. 2016.",
      "orig_title": "Improved techniques for training gans",
      "paper_id": "1606.03498v1"
    },
    {
      "index": 177,
      "title": "Modeling relational data with graph convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan\nTitov, and Max Welling. 2017."
    },
    {
      "index": 178,
      "title": "Bidirectional recurrent neural networks",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Trans. Signal Process., 45:",
      "authors": "Mike Schuster and Kuldip K. Paliwal. 1997."
    },
    {
      "index": 179,
      "title": "BLEURT: Learning Robust Metrics for Text Generation",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7881–7892, Online. Association for\nComputational Linguistics",
      "authors": "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.",
      "orig_title": "BLEURT: Learning robust metrics for text generation",
      "paper_id": "2004.04696v5"
    },
    {
      "index": 180,
      "title": "Kvqa: Knowledge-aware visual question answering",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019."
    },
    {
      "index": 181,
      "title": "The Cost of Training NLP Models A Concise Overview",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Or Sharir, Barak Peleg, and Yoav Shoham. 2020.",
      "orig_title": "The cost of training nlp models: A concise overview",
      "paper_id": "2004.08900v1"
    },
    {
      "index": 182,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018."
    },
    {
      "index": 183,
      "title": "Reasoning over Vision and Language: Exploring the Benefits of Supplemental Knowledge",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Violetta Shevchenko, Damien Teney, Anthony R. Dick, and Anton van den Hengel.\n2021.",
      "orig_title": "Reasoning over vision and language: Exploring the benefits of supplemental knowledge",
      "paper_id": "2101.06013v1"
    },
    {
      "index": 184,
      "title": "Explainable and Explicit Visual Reasoning over Scene Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Jiaxin Shi, Hanwang Zhang, and Juanzi Li. 2018.",
      "orig_title": "Explainable and explicit visual reasoning over scene graphs",
      "paper_id": "1812.01855v2"
    },
    {
      "index": 185,
      "title": "ALFRED A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)",
      "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han,\nRoozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020.",
      "orig_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
      "paper_id": "1912.01734v2"
    },
    {
      "index": 186,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Karen Simonyan and Andrew Zisserman. 2015.",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 187,
      "title": "From strings to things: Knowledge-enabled vqa model that can read and reason",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision\n(ICCV)",
      "authors": "Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, and Anirban Chakraborty.\n2019."
    },
    {
      "index": 188,
      "title": "Are we pretraining it right? Digging deeper into visio-linguistic pretraining",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Amanpreet Singh, Vedanuj Goswami, and Devi Parikh. 2020.",
      "orig_title": "Are we pretraining it right? digging deeper into visio-linguistic pretraining",
      "paper_id": "2004.08744v1"
    },
    {
      "index": 189,
      "title": "FLAVA: A Foundational Language And Vision Alignment Model",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech\nGaluba, Marcus Rohrbach, and Douwe Kiela. 2021.",
      "orig_title": "Flava: A foundational language and vision alignment model",
      "paper_id": "2112.04482v3"
    },
    {
      "index": 190,
      "title": "KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning",
      "abstract": "",
      "year": "2021",
      "venue": "Knowl. Based Syst., 230:",
      "authors": "Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, and Lejian Liao. 2021.",
      "orig_title": "Kvl-bert: Knowledge enhanced visual-and-linguistic bert for visual commonsense reasoning",
      "paper_id": "2012.07000v1"
    },
    {
      "index": 191,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 192,
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "abstract": "",
      "year": "1958",
      "venue": "Journal of Machine Learning Research, 15(56):",
      "authors": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014."
    },
    {
      "index": 193,
      "title": "From Show to Tell: A Survey on Deep Learning-based Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli,\nGiuseppe Fiameni, and Rita Cucchiara. 2021.",
      "orig_title": "From show to tell: A survey on deep learning-based image captioning",
      "paper_id": "2107.06912v3"
    },
    {
      "index": 194,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.\n2020."
    },
    {
      "index": 195,
      "title": "A corpus of natural language for visual reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), pages 217–223,\nVancouver, Canada. Association for Computational Linguistics",
      "authors": "Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017."
    },
    {
      "index": 196,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 6418–6428, Florence, Italy.\nAssociation for Computational Linguistics",
      "authors": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.\n2019.",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 197,
      "title": "CoLAKE: Contextualized Language and Knowledge Embedding",
      "abstract": "",
      "year": "2020",
      "venue": "COLING",
      "authors": "Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang,\nand Zheng Zhang. 2020.",
      "orig_title": "Colake: Contextualized language and knowledge embedding",
      "paper_id": "2010.00309v1"
    },
    {
      "index": 198,
      "title": "Rethinking the inception architecture for computer vision",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and\nZbigniew Wojna. 2015."
    },
    {
      "index": 199,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Hao Tan and Mohit Bansal. 2019.",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 200,
      "title": "Semantics-enhanced adversarial nets for text-to-image synthesis",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision\n(ICCV)",
      "authors": "Hongchen Tan, Xiuping Liu, Xin Li, Yi Zhang, and Baocai Yin. 2019."
    },
    {
      "index": 201,
      "title": "Acquiring comparative commonsense knowledge from the web",
      "abstract": "",
      "year": "2014",
      "venue": "Proceedings of the National Conference on Artificial\nIntelligence, 1:166–172",
      "authors": "N. Tandon, Gerard de Melo, and G. Weikum. 2014."
    },
    {
      "index": 202,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of ACL",
      "authors": "Niket Tandon, Gerard de Melo, and Gerhard Weikum. 2017.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 203,
      "title": "LINE: Large-scale Information Network Embedding",
      "abstract": "",
      "year": "2015",
      "venue": "Proceedings of the 24th International Conference on World Wide\nWeb",
      "authors": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015.",
      "orig_title": "Line: Large-scale information network embedding",
      "paper_id": "1503.03578v1"
    },
    {
      "index": 204,
      "title": "Yago 4: A reason-able knowledge base",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Thomas Tanon, Gerhard Weikum, and Fabian Suchanek. 2020."
    },
    {
      "index": 205,
      "title": "Vision-and-Dialog Navigation",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Robot Learning (CoRL)",
      "authors": "Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. 2019.",
      "orig_title": "Vision-and-dialog navigation",
      "paper_id": "1907.04957v3"
    },
    {
      "index": 206,
      "title": "AN IMPARTIAL TRANSFORMER FOR STORY VISUALIZATION",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Nikolaos Tsakas, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou.\n2023.",
      "orig_title": "An impartial transformer for story visualization",
      "paper_id": "2301.03563v1"
    },
    {
      "index": 207,
      "title": "Multimodal Research in Vision and Language: A Review of Current and Emerging Trends",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya\nPoria, Roger Zimmermann, and Amir Zadeh. 2020.",
      "orig_title": "Multimodal research in vision and language: A review of current and emerging trends",
      "paper_id": "2010.09522v2"
    },
    {
      "index": 208,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 209,
      "title": "CIDEr: Consensus-based Image Description Evaluation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2014.",
      "orig_title": "Cider: Consensus-based image description evaluation",
      "paper_id": "1411.5726v2"
    },
    {
      "index": 210,
      "title": "Graph Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLiò, and Yoshua Bengio. 2018.",
      "orig_title": "Graph attention networks",
      "paper_id": "1710.10903v3"
    },
    {
      "index": 211,
      "title": "Wikidata: a free collaborative knowledgebase",
      "abstract": "",
      "year": "2014",
      "venue": "Commun. ACM, 57:78–85",
      "authors": "Denny Vrandec̆ic̀ and Markus Krötzsch. 2014."
    },
    {
      "index": 212,
      "title": "Explicit Knowledge-based Reasoning for Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.\n2017.",
      "orig_title": "Explicit knowledge-based reasoning for visual question answering",
      "paper_id": "1511.02570v2"
    },
    {
      "index": 213,
      "title": "FVQA: Fact-based Visual Question Answering",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 40:",
      "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.\n2018a.",
      "orig_title": "Fvqa: Fact-based visual question answering",
      "paper_id": "1606.05433v4"
    },
    {
      "index": 214,
      "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. 2021a.",
      "orig_title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
      "paper_id": "2111.02358v2"
    },
    {
      "index": 215,
      "title": "Heterogeneous Graph Attention Network",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, and Yanfang Ye.\n2021b.",
      "orig_title": "Heterogeneous graph attention network",
      "paper_id": "1903.07293v2"
    },
    {
      "index": 216,
      "title": "Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Xiaolong Wang, Yufei Ye, and Abhinav Gupta. 2018b.",
      "orig_title": "Zero-shot recognition via semantic embeddings and knowledge graphs",
      "paper_id": "1803.08035v2"
    },
    {
      "index": 217,
      "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.\n2021c.",
      "orig_title": "Simvlm: Simple visual language model pretraining with weak supervision",
      "paper_id": "2108.10904v3"
    },
    {
      "index": 218,
      "title": "Multi-level knowledge injecting for visual commonsense reasoning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video\nTechnology, 31:",
      "authors": "Zhang Wen and Yuxin Peng. 2021."
    },
    {
      "index": 219,
      "title": "Multi-Modal Answer Validation for Knowledge-Based VQA",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi.\n2021a.",
      "orig_title": "Multi-modal answer validation for knowledge-based vqa",
      "paper_id": "2103.12248v3"
    },
    {
      "index": 220,
      "title": "Image captioning and visual question answering based on attributes and their related external knowledge",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine\nIntelligence, PP",
      "authors": "Qi Wu, Chunhua Shen, Anton Hengel, Peng Wang, and Anthony Dick.\n2016a."
    },
    {
      "index": 221,
      "title": "Ask me anything: Free-form visual question answering based on knowledge from external sources",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR)",
      "authors": "Qi Wu, Peng Wang, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.\n2016b."
    },
    {
      "index": 222,
      "title": "Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "paper_id": "1609.08144v2"
    },
    {
      "index": 223,
      "title": "Verbs semantics and lexical selection",
      "abstract": "",
      "year": "1994",
      "venue": "Proceedings of the 32nd Annual Meeting on Association for\nComputational Linguistics, ACL ’94, page 133–138, USA. Association for\nComputational Linguistics",
      "authors": "Zhibiao Wu and Martha Palmer. 1994."
    },
    {
      "index": 224,
      "title": "A Comprehensive Survey on Graph Neural Networks",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems,\n32(1):4–24",
      "authors": "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and\nPhilip S. Yu. 2021b.",
      "orig_title": "A comprehensive survey on graph neural networks",
      "paper_id": "1901.00596v4"
    },
    {
      "index": 225,
      "title": "Xgpt: Cross-modal generative pre-training for image captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui,\nEdward Cui, Taroon Bharti, Xin Liu, and Ming Zhou. 2020."
    },
    {
      "index": 226,
      "title": "Sun database: Large-scale scene recognition from abbey to zoo",
      "abstract": "",
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition",
      "authors": "Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio\nTorralba. 2010."
    },
    {
      "index": 227,
      "title": "Visual Entailment Task for Visually-Grounded Language Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2018.",
      "orig_title": "Visual entailment task for visually-grounded language learning",
      "paper_id": "1811.10582v2"
    },
    {
      "index": 228,
      "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding",
      "abstract": "",
      "year": "1901",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019.",
      "orig_title": "Visual entailment: A novel task for fine-grained image understanding",
      "paper_id": "1901.06706v1"
    },
    {
      "index": 229,
      "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation",
      "abstract": "",
      "year": "2021",
      "venue": "ACL/IJCNLP",
      "authors": "Yiran Xing, Z. Shi, Zhao Meng, Yunpu Ma, and Roger Wattenhofer. 2021.",
      "orig_title": "Km-bart: Knowledge enhanced multimodal bart for visual commonsense generation",
      "paper_id": "2101.00419v2"
    },
    {
      "index": 230,
      "title": "Imagine, reason and write: Visual storytelling with graph knowledge and relational reasoning",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Chunpu Xu, Min Yang, Chengming Li, Ying Shen, Xiang Ao, and Ruifeng Xu. 2021."
    },
    {
      "index": 231,
      "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and\nXiaodong He. 2018.",
      "orig_title": "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
      "paper_id": "1711.10485v1"
    },
    {
      "index": 232,
      "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and\nJiebo Luo. 2021.",
      "orig_title": "Probing inter-modality: Visual parsing with self-attention for vision-language pre-training",
      "paper_id": "2106.13488v4"
    },
    {
      "index": 233,
      "title": "Luke: Deep contextualized entity representations with entity-aware self-attention",
      "abstract": "",
      "year": "2020",
      "venue": "EMNLP",
      "authors": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.\n2020."
    },
    {
      "index": 234,
      "title": "Knowledgeable storyteller: A commonsense-driven generative model for visual storytelling",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the Twenty-Eighth International Joint\nConference on Artificial Intelligence, IJCAI-19, pages 5356–5362.\nInternational Joint Conferences on Artificial Intelligence Organization",
      "authors": "Pengcheng Yang, Fuli Luo, Peng Chen, Lei Li, Zhiyi Yin, Xiaodong He, and\nXu Sun. 2019a."
    },
    {
      "index": 235,
      "title": "Auto-Encoding Scene Graphs for Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2018.",
      "orig_title": "Auto-encoding scene graphs for image captioning",
      "paper_id": "1812.02378v3"
    },
    {
      "index": 236,
      "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and\nLijuan Wang. 2021.",
      "orig_title": "An empirical study of gpt-3 for few-shot knowledge-based vqa",
      "paper_id": "2109.05014v2"
    },
    {
      "index": 237,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019b.",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 238,
      "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "abstract": "",
      "year": "2014",
      "venue": "Transactions of the Association for Computational Linguistics,\n2:67–78",
      "authors": "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014."
    },
    {
      "index": 239,
      "title": "Ernie-vil: Knowledge enhanced vision-language representations through scene graph",
      "abstract": "",
      "year": "",
      "venue": "AAAI",
      "authors": "Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.\n2021a."
    },
    {
      "index": 240,
      "title": "Cross-Modal Knowledge Reasoning for Knowledge-based Visual Question Answering",
      "abstract": "",
      "year": "2009",
      "venue": "ArXiv, abs/",
      "authors": "J. Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, and Jianlong Tan. 2020.",
      "orig_title": "Cross-modal knowledge reasoning for knowledge-based visual question answering",
      "paper_id": "2009.00145v1"
    },
    {
      "index": 241,
      "title": "Modeling context in referring expressions",
      "abstract": "",
      "year": "2016",
      "venue": "Computer Vision – ECCV 2016, pages 69–85, Cham. Springer\nInternational Publishing",
      "authors": "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg.\n2016."
    },
    {
      "index": 242,
      "title": "A survey of knowledge-enhanced text generation",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and\nMeng Jiang. 2021b."
    },
    {
      "index": 243,
      "title": "Florence: A New Foundation Model for Computer Vision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,\nHoudong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu,\nZicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen\nXiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. 2021.",
      "orig_title": "Florence: A new foundation model for computer vision",
      "paper_id": "2111.11432v1"
    },
    {
      "index": 244,
      "title": "Graph Transformer Networks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim.\n2020.",
      "orig_title": "Graph transformer networks",
      "paper_id": "1911.06455v2"
    },
    {
      "index": 245,
      "title": "From Recognition to Cognition: Visual Commonsense Reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",
      "orig_title": "From recognition to cognition: Visual commonsense reasoning",
      "paper_id": "1811.10830v2"
    },
    {
      "index": 246,
      "title": "Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018.",
      "orig_title": "Swag: A large-scale adversarial dataset for grounded commonsense inference",
      "paper_id": "1808.05326v1"
    },
    {
      "index": 247,
      "title": "Pororogan: An improved story visualization model on pororo-sv dataset",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 2019 3rd International Conference on\nComputer Science and Artificial Intelligence, CSAI2019, page 155–159, New\nYork, NY, USA. Association for Computing Machinery",
      "authors": "Gangyan Zeng, Zhaohui Li, and Yuan Zhang. 2019."
    },
    {
      "index": 248,
      "title": "Multimodal Intelligence: Representation Learning, Information Fusion, and Applications",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Journal of Selected Topics in Signal Processing,\n14(3):478–493",
      "authors": "Chao Zhang, Zichao Yang, Xiaodong He, and Li Deng. 2020.",
      "orig_title": "Multimodal intelligence: Representation learning, information fusion, and applications",
      "paper_id": "1911.03977v3"
    },
    {
      "index": 249,
      "title": "RAVEN: A Dataset for Relational and Analogical Visual rEasoNing",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.\n2019a.",
      "orig_title": "Raven: A dataset for relational and analogical visual reasoning",
      "paper_id": "1903.02741v1"
    },
    {
      "index": 250,
      "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Han Zhang, Tao Xu, and Hongsheng Li. 2017.",
      "orig_title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
      "paper_id": "1612.03242v2"
    },
    {
      "index": 251,
      "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,\nand Dimitris Metaxas. 2018a.",
      "orig_title": "Stackgan++: Realistic image synthesis with stacked generative adversarial networks",
      "paper_id": "1710.10916v3"
    },
    {
      "index": 252,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR)",
      "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,\nYejin Choi, and Jianfeng Gao. 2021a.",
      "orig_title": "Vinvl: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    },
    {
      "index": 253,
      "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
      "abstract": "",
      "year": "",
      "venue": "CVPR",
      "authors": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\n2018b.",
      "orig_title": "The unreasonable effectiveness of deep features as a perceptual metric",
      "paper_id": "1801.03924v2"
    },
    {
      "index": 254,
      "title": "Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Shunyu Zhang, Xiaoze Jiang, Zequn Yang, Tao Wan, and Zengchang Qin. 2022.",
      "orig_title": "Reasoning with multi-structure commonsense knowledge in visual dialog",
      "paper_id": "2204.04680v1"
    },
    {
      "index": 255,
      "title": "Image captioning with transformer and knowledge graph",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yu Zhang, Xinyu Shi, Siya Mi, and Xu Yang. 2021b."
    },
    {
      "index": 256,
      "title": "ERNIE: Enhanced Language Representation with Informative Entities",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019b.",
      "orig_title": "Ernie: Enhanced language representation with informative entities",
      "paper_id": "1905.07129v3"
    },
    {
      "index": 257,
      "title": "Boosting Entity-aware Image Captioning with Multi-modal Knowledge Graph",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Wentian Zhao, Yao Hu, Heda Wang, Xinxiao Wu, and Jiebo Luo. 2021.",
      "orig_title": "Boosting entity-aware image captioning with multi-modal knowledge graph",
      "paper_id": "2107.11970v1"
    },
    {
      "index": 258,
      "title": "Knowledge is power: Hierarchical-knowledge embedded meta-learning for visual reasoning in artistic domains",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining",
      "authors": "Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. 2021."
    },
    {
      "index": 259,
      "title": "Places: A 10 million image database for scene recognition",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 40(6):",
      "authors": "Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2018."
    },
    {
      "index": 260,
      "title": "Learning deep features for scene recognition using places database",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.\n2014."
    },
    {
      "index": 261,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng\nGao. 2019a.",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 262,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng\nGao. 2019b.",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 263,
      "title": "Improving Image Captioning by Leveraging Knowledge Graphs",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision\n(WACV)",
      "authors": "Yimin Zhou, Yiwei Sun, and Vasant G Honavar. 2019c.",
      "orig_title": "Improving image captioning by leveraging knowledge graphs",
      "paper_id": "1901.08942v1"
    },
    {
      "index": 264,
      "title": "DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis",
      "abstract": "",
      "year": "1904",
      "venue": "CoRR, abs/",
      "authors": "Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019.",
      "orig_title": "DM-GAN: dynamic memory generative adversarial networks for text-to-image synthesis",
      "paper_id": "1904.01310v1"
    },
    {
      "index": 265,
      "title": "Visual7w: Grounded question answering in images",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR)",
      "authors": "Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016."
    },
    {
      "index": 266,
      "title": "Building a large-scale multimodal knowledge base system for answering visual queries",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Yuke Zhu, Ce Zhang, Christopher Ré, and Li Fei-Fei. 2015."
    },
    {
      "index": 267,
      "title": "Mucko: Multi-layer cross-modal knowledge reasoning for fact-based visual question answering",
      "abstract": "",
      "year": "2020",
      "venue": "IJCAI",
      "authors": "Zihao Zhu, J. Yu, Yujing Wang, Yajing Sun, Yue Hu, and Qi Wu. 2020."
    },
    {
      "index": 268,
      "title": "Towards knowledge-augmented visual question answering",
      "abstract": "",
      "year": "2020",
      "venue": "COLING",
      "authors": "Maryam Ziaeefard and Freddy Lécué. 2020."
    }
  ]
}