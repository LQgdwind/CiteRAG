{
  "paper_id": "2112.13112v2",
  "title": "A Survey on Interpretable Reinforcement Learning",
  "abstract": "Abstract\nAlthough deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications.\nIn such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons).\nThis survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL).\nTo that aim,\nwe distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion.\nIn particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making.\nBased on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years.\nWe also discuss briefly some related research areas and point to some potential promising research directions.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 1,
      "title": "Mastering the game of Go without human knowledge",
      "abstract": "",
      "year": "2017",
      "venue": "Nature",
      "authors": "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis"
    },
    {
      "index": 2,
      "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Nature",
      "authors": "O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard et al."
    },
    {
      "index": 3,
      "title": "Solving Rubik’s Cube with a Robot Hand",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1910.07113",
      "authors": "OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang",
      "orig_title": "Solving Rubik’s Cube with a Robot Hand",
      "paper_id": "1910.07113v1"
    },
    {
      "index": 4,
      "title": "Graying the black box: Understanding DQNs",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "T. Zahavy, N. Ben-Zrihem, and S. Mannor",
      "orig_title": "Graying the black box: Understanding DQNs",
      "paper_id": "1602.02658v4"
    },
    {
      "index": 5,
      "title": "Deep Reinforcement Learning that Matters",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger",
      "orig_title": "Deep reinforcement learning that matters",
      "paper_id": "1709.06560v3"
    },
    {
      "index": 6,
      "title": "A Study on Overfitting in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1804.06893",
      "authors": "C. Zhang, O. Vinyals, R. Munos, and S. Bengio"
    },
    {
      "index": 7,
      "title": "Adversarial Attacks on Neural Network Policies",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR Workshop",
      "authors": "S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel"
    },
    {
      "index": 8,
      "title": "AI Now Report",
      "abstract": "",
      "year": "2016",
      "venue": "AI Now Institute, Tech. Rep.",
      "authors": "K. Crawford, R. Dobbe, T. Dryer, G. Fried, B. Green, E. Kaziunas, A. Kak, V. Mathur, E. McElroy, A. N. Sánchez, D. Raji, J. L. Rankin, R. Richardson, J. Schultz, S. M. West, and M. Whittaker"
    },
    {
      "index": 9,
      "title": "Building Ethics into Artificial Intelligence",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser, and Q. Yang",
      "orig_title": "Building ethics into artificial intelligence",
      "paper_id": "1812.02953v1"
    },
    {
      "index": 10,
      "title": "Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector",
      "abstract": "",
      "year": "2020",
      "venue": "SSRN Electronic Journal",
      "authors": "D. Leslie"
    },
    {
      "index": 11,
      "title": "From what to how: An initial review of publicly available AI ethics tools, methods and research to translate principles into practices",
      "abstract": "",
      "year": "2020",
      "venue": "Science and Engineering Ethics",
      "authors": "J. Morley, L. Floridi, L. Kinsey, and A. Elhalal"
    },
    {
      "index": 12,
      "title": "Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward",
      "abstract": "",
      "year": "2020",
      "venue": "Humanities and Social Sciences Communications",
      "authors": "S. Lo Piano"
    },
    {
      "index": 13,
      "title": "Fairness through awareness",
      "abstract": "",
      "year": "2012",
      "venue": "ICTS",
      "authors": "C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel"
    },
    {
      "index": 14,
      "title": "The (Im)possibility of fairness: Different value systems require different mechanisms for fair decision making",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM",
      "authors": "S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian"
    },
    {
      "index": 15,
      "title": "A Survey on Bias and Fairness in Machine Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1908.09635",
      "authors": "N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan",
      "orig_title": "A Survey on Bias and Fairness in Machine Learning",
      "paper_id": "1908.09635v3"
    },
    {
      "index": 16,
      "title": "Concrete Problems in AI Safety",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv:1606.06565",
      "authors": "D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané",
      "orig_title": "Concrete Problems in AI Safety",
      "paper_id": "1606.06565v2"
    },
    {
      "index": 17,
      "title": "Accountability of AI Under the Law: The Role of Explanation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1711.01134",
      "authors": "F. Doshi-Velez, M. Kortz, R. Budish, C. Bavitz, S. Gershman, D. O’Brien, K. Scott, S. Schieber, J. Waldo, D. Weinberger, A. Weller, and A. Wood"
    },
    {
      "index": 18,
      "title": "Ethics guidelines for trustworthy AI",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "E. Commission"
    },
    {
      "index": 19,
      "title": "Data, privacy, and the greater good",
      "abstract": "",
      "year": "2015",
      "venue": "Science",
      "authors": "E. Horvitz and D. Mulligan"
    },
    {
      "index": 20,
      "title": "The Trolley, The Bull Bar, and Why Engineers Should Care About The Ethics of Autonomous Cars [point of view]",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE",
      "authors": "J. Bonnefon, A. Shariff, and I. Rahwan"
    },
    {
      "index": 21,
      "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:1811.11839",
      "authors": "S. Mohseni, N. Zarei, and E. D. Ragan",
      "orig_title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
      "paper_id": "1811.11839v5"
    },
    {
      "index": 22,
      "title": "The Societal Implications of Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "JAIR",
      "authors": "J. Whittlestone, K. Arulkumaran, and M. Crosby"
    },
    {
      "index": 23,
      "title": "Explainable Reinforcement Learning: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "LNCS",
      "authors": "E. Puiutta and E. M. Veith",
      "orig_title": "Explainable reinforcement learning: A survey",
      "paper_id": "2005.06247v1"
    },
    {
      "index": 24,
      "title": "Reinforcement Learning Interpretation Methods: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "A. Alharin, T.-N. Doan, and M. Sartipi"
    },
    {
      "index": 25,
      "title": "Explainability in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Knowledge-Based Systems",
      "authors": "A. Heuillet, F. Couthouis, and N. Díaz-Rodríguez",
      "orig_title": "Explainability in deep reinforcement learning",
      "paper_id": "2008.06693v4"
    },
    {
      "index": 26,
      "title": "Markov decision processes: discrete stochastic dynamic programming",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": ""
    },
    {
      "index": 27,
      "title": "“Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 28,
      "title": "“Proximal Policy Optimization Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 29,
      "title": "Addressing Function Approximation Error in Actor-Critic Methods",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Addressing Function Approximation Error in Actor-Critic Methods",
      "paper_id": "1802.09477v3"
    },
    {
      "index": 30,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 31,
      "title": "Combined Reinforcement Learning via Abstract Representations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Combined reinforcement learning via abstract representations",
      "paper_id": "1809.04506v2"
    },
    {
      "index": 32,
      "title": "“Entity abstraction in visual model-based reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 33,
      "title": "“A Physics-Based Model Prior for Object-Oriented MDPs",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 34,
      "title": "“Recent Advances in Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 35,
      "title": "“Model-Agnostic Interpretability of Machine Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 36,
      "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "paper_id": "1706.07269v3"
    },
    {
      "index": 37,
      "title": "“Interpretable Machine Learning: A Guide for Making Black Box Models Explainable",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "The Mythos of Model Interpretability",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“The Mythos of Model Interpretability",
      "paper_id": "1606.03490v3"
    },
    {
      "index": 39,
      "title": "N. Díaz-Rodríguez",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 40,
      "title": "“Directions for Explainable Knowledge-Enabled Systems",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Explaining Explanations: An Overview of Interpretability of Machine Learning",
      "paper_id": "1806.00069v3"
    },
    {
      "index": 42,
      "title": "“The Pragmatic Turn in Explainable Artificial Intelligence (XAI)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 43,
      "title": "“Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 44,
      "title": "Learning Programmatically Structured Representations with Perceptor Gradients",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning Programmatically Structured Representations with Perceptor Gradients",
      "paper_id": "1905.00956v1"
    },
    {
      "index": 45,
      "title": "“Learning the structure of factored Markov decision processes in reinforcement learning problems",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 46,
      "title": "“Relational reinforcement learning",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": ""
    },
    {
      "index": 47,
      "title": "“An introduction to first-order logic",
      "abstract": "",
      "year": "1977",
      "venue": "",
      "authors": ""
    },
    {
      "index": 48,
      "title": "“Knowledge Representation",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 49,
      "title": "“Blocks World Revisited",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 50,
      "title": "“Generalizing plans to new environments in relational MDPs",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "“Relational Reinforcement Learning",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 52,
      "title": "“Learning Digger using Hierarchical Reinforcement Learning for Concurrent Goals.” in EWRL",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "“Symbolic Learning for Adaptive Agents",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "“Relational Reinforcement Learning via Sampling the Space of First-Order Conjunctive Features",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "“Simultaneous Learning of Structure and Value in Relational Reinforcement Learning",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "“Graph Kernels and Gaussian Processes for Relational Reinforcement Learning",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "“Symbolic Network: Generalized Neural Policies for Relational MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "“Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "“Stochastic dynamic programming with factored representations",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "“Probabilistic relational models",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "“A model for reasoning about persistence and causation",
      "abstract": "",
      "year": "1990",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "“Relational Dynamic Influence Diagram Language (RDDL): Language Description",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "“PPDDL1.0: The language for the probabilistic part of IPC-4",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "“An object-oriented representation for efficient reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "“Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "“A survey of reinforcement learning in relational domains",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "J. Schrittwieser",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "The Logic of Adaptive Behavior Knowledge Representation and Algorithms for Adaptive Sequential Decision Making under Uncertainty in First Order and Relational Domains",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Solving relational and first order logical markov decision processes: A survey",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "“Relational reinforcement learning with guided demonstrations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 71,
      "title": "“The symbol grounding problem",
      "abstract": "",
      "year": "1990",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Constructing Symbolic Representations for High-Level Planning",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "“Symbol Acquisition for Probabilistic High-Level Planning",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "Active Exploration for Learning Symbolic Representations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Active Exploration for Learning Symbolic Representations",
      "paper_id": "1709.01490v2"
    },
    {
      "index": 75,
      "title": "“From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "“Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Active Learning for Teaching a Robot Grounded Relational Symbols.” in IJCAI",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "“Program Guided Agent",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "Object-sensitive Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Object-sensitive Deep Reinforcement Learning",
      "paper_id": "1809.06064v1"
    },
    {
      "index": 80,
      "title": "Towards Deep Symbolic Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Deep Symbolic Reinforcement Learning",
      "paper_id": "1609.05518v2"
    },
    {
      "index": 81,
      "title": "“Unsupervised video object segmentation for deep reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "“Symbolic Relation Networks for Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "“Deep reinforcement learning with relational inductive biases",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "Template Matching Techniques in Computer Vision: Theory and Practice. Wiley Publishing",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "“Robust real-time object detection",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "Transparency and Explanation in Deep Reinforcement Learning Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Transparency and Explanation in Deep Reinforcement Learning Neural Networks",
      "paper_id": "1809.06061v1"
    },
    {
      "index": 87,
      "title": "“Dimensions of neural-symbolic integration — a structured survey",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "“A Comparison between deep Q-networks and deep symbolic reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "Towards Symbolic Reinforcement Learning with Common Sense",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Symbolic Reinforcement Learning with Common Sense",
      "paper_id": "1804.08597v1"
    },
    {
      "index": 90,
      "title": "“Relational inductive biases",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "“The Graph Neural Network Model",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "Neural Message Passing for Quantum Chemistry",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural message passing for quantum chemistry",
      "paper_id": "1704.01212v2"
    },
    {
      "index": 93,
      "title": "A. Sanchez Gonzalez",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "Graph Networks as Learnable Physics Engines for Inference and Control",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Graph networks as learnable physics engines for inference and control",
      "paper_id": "1806.01242v1"
    },
    {
      "index": 95,
      "title": "Gated Graph Sequence Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Gated Graph Sequence Neural Networks",
      "paper_id": "1511.05493v4"
    },
    {
      "index": 96,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 97,
      "title": "“NerveNet: Learning Structured Policy with Graph Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "“A simple neural network module for relational reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "“A Compositional Object-Based Approach to Learning Physical Dynamics",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "“Object Detection and Tracking Algorithms for Vehicle Counting: A Comparative Analysis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "Learning Physical Graph Representations from Visual Scenes",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning physical graph representations from visual scenes",
      "paper_id": "2006.12373v2"
    },
    {
      "index": 102,
      "title": "“Logic tensor networks for semantic image interpretation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "“Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "Towards a Definition of Disentangled Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards a Definition of Disentangled Representations",
      "paper_id": "1812.02230v1"
    },
    {
      "index": 105,
      "title": "“Hierarchical reinforcement learning",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "“Dot-to-dot: Explainable hierarchical reinforcement learning for robotic manipulation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "“REBA: A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Modular multitask reinforcement learning with policy sketches",
      "paper_id": "1611.01796v2"
    },
    {
      "index": 109,
      "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
      "paper_id": "1712.07294v1"
    },
    {
      "index": 110,
      "title": "“Model primitive hierarchical lifelong reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "“A synthesis of automated planning and reinforcement learning for efficient",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making",
      "paper_id": "1804.07779v3"
    },
    {
      "index": 113,
      "title": "Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots",
      "paper_id": "1811.08955v1"
    },
    {
      "index": 114,
      "title": "SDRL: Interpretable and Data-efficient Deep Reinforcement Learning Leveraging Symbolic Planning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“SDRL: Interpretable and Data-Efficient Deep Reinforcement Learning Leveraging Symbolic Planning",
      "paper_id": "1811.00090v4"
    },
    {
      "index": 115,
      "title": "“Induction and Exploitation of Subgoal Automata for Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "“A formal methods approach to interpretable reinforcement learning for robotic planning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning",
      "paper_id": "1910.09986v1"
    },
    {
      "index": 118,
      "title": "Robot Representation and Reasoning with Knowledge from Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Robot Representation and Reasoning with Knowledge from Reinforcement Learning",
      "paper_id": "1809.11074v3"
    },
    {
      "index": 119,
      "title": "“LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "“Plan-based reward shaping for reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "“Automated planning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "“Symbolic Plans as High-Level Instructions for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "A Survey of Knowledge-based Sequential Decision Making under Uncertainty",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A Survey of Knowledge-based Sequential Decision Making under Uncertainty",
      "paper_id": "2008.08548v3"
    },
    {
      "index": 124,
      "title": "“Algorithms for Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "“Learning symbolic models of stochastic domains",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "“Building relational world models for reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "“Efficient Learning of Relational Models for Sequential Decision Making",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "“Learning relational dynamics of stochastic domains for planning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "“Learning Graph-Based Representations for Continuous Reinforcement Learning Domains",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "M. Lázaro-Gredilla",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 131,
      "title": "Interpretable Dynamics Models for Data-Efficient Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable dynamics models for data-efficient reinforcement learning",
      "paper_id": "1907.04902v1"
    },
    {
      "index": 132,
      "title": "Composable Planning with Attributes",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Composable Planning with Attributes",
      "paper_id": "1803.00512v2"
    },
    {
      "index": 133,
      "title": "“Search on the replay buffer: Bridging planning and reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "“Interaction networks for learning about objects",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "Unsupervised Learning for Physical Interaction through Video Prediction",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Unsupervised learning for physical interaction through video prediction",
      "paper_id": "1605.07157v4"
    },
    {
      "index": 136,
      "title": "Deep Visual Foresight for Planning Robot Motion",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep visual foresight for planning robot motion",
      "paper_id": "1610.00696v2"
    },
    {
      "index": 137,
      "title": "Object-Oriented Dynamics Predictor",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Object-oriented dynamics predictor",
      "paper_id": "1806.07371v3"
    },
    {
      "index": 138,
      "title": "Object-Oriented Dynamics Learning through Multi-Level Abstraction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Object-Oriented Dynamics Learning through Multi-Level Abstraction",
      "paper_id": "1904.07482v4"
    },
    {
      "index": 139,
      "title": "“Unsupervised Object-Level Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "“Learning Agents for Uncertain Environments",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“Learning to drive a bicycle using reinforcement learning and shaping",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Value Alignment or Misalignment – What Will Keep Systems Accountable?” in AAAI Workshop",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Alvinn: An autonomous land vehicle in a neural network",
      "abstract": "",
      "year": "1989",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "“Algorithmic Perspective on Imitation Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "“A survey of inverse reinforcement learning: Challenges",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "“Creating advice-taking reinforcement learners",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "“Guiding Autonomous Agents to Better Behaviors through Human Advice",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "“Preference Elicitation and Inverse Reinforcement Learning",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“Interactive Q-Learning with Ordinal Rewards and Unreliable Tutor",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“Inverse reinforcement learning in relational domains",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "“Relational reinforcement learning for planning with exogenous effects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "“Interpretable batch IRL to extract clinician goals in ICU hypotension management",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "“Q-Learning for robust satisfaction of signal temporal logic specifications",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "Environment-Independent Task Specifications via GLTL",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Environment-Independent Task Specifications via GLTL",
      "paper_id": "1704.04341v1"
    },
    {
      "index": 155,
      "title": "Reinforcement Learning With Temporal Logic Rewards",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Reinforcement learning with temporal logic rewards",
      "paper_id": "1612.03471v2"
    },
    {
      "index": 156,
      "title": "“Teaching Multiple Tasks to an RL Agent using LTL",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "“Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 158,
      "title": "“Deep Reinforcement Learning with Temporal Logics",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Learning Reward Machines for Partially Observable Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "“Joint Inference of Reward Machines and Policies for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "Reinforcement Learning with Non-Markovian Rewards",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Reinforcement Learning with Non-Markovian Rewards",
      "paper_id": "1912.02552v1"
    },
    {
      "index": 162,
      "title": "A Boolean Task Algebra For Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A Boolean Task Algebra for Reinforcement Learning",
      "paper_id": "2001.01394v2"
    },
    {
      "index": 163,
      "title": "“Deep Neural Decision Trees",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "Interpretable Apprenticeship Learning with Temporal Logic Specifications",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable Apprenticeship Learning with Temporal Logic Specifications",
      "paper_id": "1710.10532v1"
    },
    {
      "index": 165,
      "title": "“Compositionality of optimal control laws",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "“Tree-based batch mode reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "“Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 168,
      "title": "“Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "“Neural-encoding Human Experts’ Domain Knowledge to Warm Start Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "“Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 171,
      "title": "“Policy tree: Adaptive representation for policy gradient",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "“Policy Search in a Space of Simple Closed-form Formulas: Towards Interpretability of Reinforcement Learning",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 173,
      "title": "“Automatic Discovery of Ranking Formulas for Playing with Multi-armed Bandits",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable policies for reinforcement learning by genetic programming",
      "paper_id": "1712.04170v2"
    },
    {
      "index": 175,
      "title": "“Generating interpretable reinforcement learning policies using genetic programming",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "“Learning an Interpretable Traffic Signal Control Policy",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 177,
      "title": "“Towards reinforcement learning of human readable policies",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 178,
      "title": "“Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "Learning Explanatory Rules from Noisy Data",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning explanatory rules from noisy data",
      "paper_id": "1711.04574v2"
    },
    {
      "index": 180,
      "title": "Neural Logic Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural Logic Reinforcement Learning",
      "paper_id": "1904.10729v2"
    },
    {
      "index": 181,
      "title": "“Inductive Logic Programming via Differentiable Deep Neural Logic Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 182,
      "title": "Learning Algorithms via Neural Logic Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning Algorithms via Neural Logic Networks",
      "paper_id": "1904.01554v1"
    },
    {
      "index": 183,
      "title": "Incorporating Relational Background Knowledge into Reinforcement Learning via Differentiable Inductive Logic Programming",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Incorporating Relational Background Knowledge into Reinforcement Learning via Differentiable Inductive Logic Programming",
      "paper_id": "2003.10386v1"
    },
    {
      "index": 184,
      "title": "Differentiable Logic Machines",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Differentiable logic machines",
      "paper_id": "2102.11529v5"
    },
    {
      "index": 185,
      "title": "Learn to Explain Efficiently via Neural Logic Inductive Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learn to Explain Efficiently via Neural Logic Inductive Learning",
      "paper_id": "1910.02481v3"
    },
    {
      "index": 186,
      "title": "“Interpretable Reinforcement Learning With Neural Symbolic Logic",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "Imitation-Projected Programmatic Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Imitation-Projected Programmatic Reinforcement Learning",
      "paper_id": "1907.05431v4"
    },
    {
      "index": 188,
      "title": "Neurosymbolic Reinforcement Learning with Formally Verified Exploration",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Neurosymbolic Reinforcement Learning with Formally Verified Exploration",
      "paper_id": "2009.12612v2"
    },
    {
      "index": 189,
      "title": "Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning",
      "paper_id": "2001.08726v3"
    },
    {
      "index": 190,
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "paper_id": "1805.00909v3"
    },
    {
      "index": 191,
      "title": "“Neural Logic Machines",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "“Relational retrieval using a combination of path-constrained random walks",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 193,
      "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Differentiable Learning of Logical Rules for Knowledge Base Reasoning",
      "paper_id": "1702.08367v3"
    },
    {
      "index": 194,
      "title": "“Imitation Learning: A Survey of Learning Methods",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 195,
      "title": "Policy Distillation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Policy distillation",
      "paper_id": "1511.06295v2"
    },
    {
      "index": 196,
      "title": "Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees",
      "paper_id": "1807.05887v1"
    },
    {
      "index": 197,
      "title": "“Learning to use selective attention and short-term memory in sequential tasks",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "“Why Should I Trust You?” Explaining the Predictions of Any Classifier",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“”Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
      "paper_id": "1602.04938v3"
    },
    {
      "index": 199,
      "title": "Verifiable Reinforcement Learning via Policy Extraction",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Verifiable Reinforcement Learning via Policy Extraction",
      "paper_id": "1805.08328v2"
    },
    {
      "index": 200,
      "title": "“A Reduction of Imitation Learning and Structured Prediction to No-regret Online Learning",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy",
      "paper_id": "1907.01180v1"
    },
    {
      "index": 202,
      "title": "“MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "“Imitation learning in relational domains: A functional-gradient boosting approach",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "“Imitation learning of car driving skills with decision trees and random forests",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 205,
      "title": "“Interpretable approximation of a deep reinforcement learning agent as a set of if-then rules",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "Programmatically Interpretable Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Programmatically interpretable reinforcement learning",
      "paper_id": "1804.02477v3"
    },
    {
      "index": 207,
      "title": "An Inductive Synthesis Framework for Verifiable Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“An inductive synthesis framework for verifiable reinforcement learning",
      "paper_id": "1907.07273v1"
    },
    {
      "index": 208,
      "title": "Safe Reinforcement Learning via Shielding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Safe reinforcement learning via shielding",
      "paper_id": "1708.08611v2"
    },
    {
      "index": 209,
      "title": "“From explanation to synthesis: Compositional program induction for learning from demonstration",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "Learning Finite State Representations of Recurrent Policy Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning Finite State Representations of Recurrent Policy Networks",
      "paper_id": "1811.12530v1"
    },
    {
      "index": 211,
      "title": "“Teaching on a Budget: Agents Advising Agents in Reinforcement Learning",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "“Teacher-Student Framework: A Reinforcement Learning Approach",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "Neuroevolution of Self-Interpretable Agents",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Neuroevolution of Self-Interpretable Agents",
      "paper_id": "2003.08165v2"
    },
    {
      "index": 214,
      "title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Interpretable Reinforcement Learning Using Attention Augmented Agents",
      "paper_id": "1906.02500v1"
    },
    {
      "index": 215,
      "title": "Towards Better Interpretability in Deep Q-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Better Interpretability in Deep Q-Networks",
      "paper_id": "1809.05630v2"
    },
    {
      "index": 216,
      "title": "“Attention is not not Explanation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "“Attention is not Explanation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "“On Identifiability in Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "“Fast relational learning using bottom clause propositionalization with artificial neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "“Advanced building control via deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 221,
      "title": "“Model compression",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "ℓ₁-regularized Neural Networks are Improperly Learnable in Polynomial Time",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“L1-regularized Neural Networks are Improperly Learnable in Polynomial Time",
      "paper_id": "1510.03528v1"
    },
    {
      "index": 223,
      "title": "“Legibility and predictability of robot motion",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "Optimizing for Interpretability in Deep Neural Networks with Tree Regularization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Optimizing for Interpretability in Deep Neural Networks with Tree Regularization",
      "paper_id": "1908.05254v1"
    },
    {
      "index": 225,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 226,
      "title": "“Semantic-based regularization for learning and inference",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 227,
      "title": "Integrating Deep Learning with Logic Fusion for Information Extraction",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Integrating Deep Learning with Logic Fusion for Information Extraction",
      "paper_id": "1912.03041v1"
    },
    {
      "index": 228,
      "title": "“Injecting Logical Background Knowledge into Embeddings for Relation Extraction",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 229,
      "title": "“Lifted Rule Injection for Relation Embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 230,
      "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“A semantic loss function for deep learning with symbolic knowledge",
      "paper_id": "1711.11157v2"
    },
    {
      "index": 231,
      "title": "Adversarial Sets for Regularising Neural Link Predictors",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Adversarial Sets for Regularising Neural Link Predictors",
      "paper_id": "1707.07596v1"
    },
    {
      "index": 232,
      "title": "“Regularizing Black-box Models for Improved Interpretability",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 233,
      "title": "Visualizing and Understanding Atari Agents",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Visualizing and understanding atari agents",
      "paper_id": "1711.00138v5"
    },
    {
      "index": 234,
      "title": "B. Krishnamurthy",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 235,
      "title": "“Attribution-based Salience Method towards Interpretable Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 236,
      "title": "Self-Supervised Discovering of Interpretable Features for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-Supervised Discovering of Interpretable Features for Reinforcement Learning",
      "paper_id": "2003.07069v4"
    },
    {
      "index": 237,
      "title": "Attentional Bottleneck: Towards an Interpretable Deep Driving Network",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Attentional Bottleneck: Towards an Interpretable Deep Driving Network",
      "paper_id": "2005.04298v1"
    },
    {
      "index": 238,
      "title": "“Interestingness Elements for Explainable Reinforcement Learning: Understanding Agents’ Capabilities and Limitations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 239,
      "title": "“Improving Robot Controller Transparency Through Autonomous Policy Explanation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 240,
      "title": "K. van den Bosch",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 241,
      "title": "Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents",
      "paper_id": "1810.08811v1"
    },
    {
      "index": 242,
      "title": "“Explainable Reinforcement Learning Through a Causal Lens",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 243,
      "title": "“Distal Explanations for Model-free Explainable Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 244,
      "title": "“Distilling deep reinforcement learning policies in soft decision trees",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 245,
      "title": "TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments",
      "paper_id": "2009.04743v2"
    },
    {
      "index": 246,
      "title": "“Explainable reinforcement learning via reward decomposition",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 247,
      "title": "Generation of Policy-Level Explanations for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Generation of Policy-Level Explanations for Reinforcement Learning",
      "paper_id": "1905.12044v1"
    },
    {
      "index": 248,
      "title": "“Memory-Based Explainable Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 249,
      "title": "Visualizing Data using GTSNE",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "",
      "orig_title": "“Visualizing Data using t-SNE",
      "paper_id": "2108.01301v1"
    },
    {
      "index": 250,
      "title": "A Unified Approach to Interpreting Model Predictions",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“A unified approach to interpreting model predictions",
      "paper_id": "1705.07874v2"
    },
    {
      "index": 251,
      "title": "“Why these Explanations? Selecting Intelligibility Types for Explanation Goals",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 252,
      "title": "“Explaining Explanations in AI",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 253,
      "title": "“Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 254,
      "title": "“The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to Be More Effective at Data Analysis",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 255,
      "title": "“Simple random search of static linear policies is competitive for reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 256,
      "title": "“Program Synthesis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 257,
      "title": "The Human Use of Human Beings",
      "abstract": "",
      "year": "1954",
      "venue": "",
      "authors": ""
    },
    {
      "index": 258,
      "title": "“Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 259,
      "title": "Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing",
      "paper_id": "2001.00973v1"
    },
    {
      "index": 260,
      "title": "“Artificial Intelligence",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    }
  ]
}