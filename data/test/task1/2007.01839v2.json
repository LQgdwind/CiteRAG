{
  "paper_id": "2007.01839v2",
  "title": "Expected Eligibility Traces",
  "abstract": "Abstract\nThe question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence.\nEligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion.\nWe discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(ŒªùúÜ\\lambda). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Natural gradient works efficiently in learning",
      "abstract": "",
      "year": "1998",
      "venue": "Neural computation",
      "authors": "Amari, S. I."
    },
    {
      "index": 1,
      "title": "Successor features for transfer in reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Barreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.; van Hasselt, H. P.; and Silver, D."
    },
    {
      "index": 2,
      "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
      "abstract": "",
      "year": "2013",
      "venue": "J. Artif. Intell. Res. (JAIR)",
      "authors": "Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M."
    },
    {
      "index": 3,
      "title": "Dynamic Programming",
      "abstract": "",
      "year": "1957",
      "venue": "Princeton University Press",
      "authors": "Bellman, R."
    },
    {
      "index": 4,
      "title": "Neuro-dynamic Programming",
      "abstract": "",
      "year": "1996",
      "venue": "Athena Scientific",
      "authors": "Bertsekas, D. P.; and Tsitsiklis, J. N."
    },
    {
      "index": 5,
      "title": "JAX: composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Bradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary, C.; Maclaurin, D.; and Wanderman-Milne, S."
    },
    {
      "index": 6,
      "title": "JAX: composable transformations of Python+NumPy programs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Bradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary, C.; Maclaurin, D.; and Wanderman-Milne, S."
    },
    {
      "index": 7,
      "title": "Geometric Insights into the Convergence of Nonlinear TD Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Brandfonbrener, D.; and Bruna, J.",
      "orig_title": "Geometric Insights into the Convergence of Non-linear TD Learning",
      "paper_id": "1905.12185v4"
    },
    {
      "index": 8,
      "title": "The convergence of TD(Œª)ùúÜ(\\lambda) for general lambda",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "Dayan, P."
    },
    {
      "index": 9,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation",
      "authors": "Dayan, P."
    },
    {
      "index": 10,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Duan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel, P.",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 11,
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Neural Networks",
      "authors": "Elfwing, S.; Uchibe, E.; and Doya, K.",
      "orig_title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
      "paper_id": "1702.03118v3"
    },
    {
      "index": 12,
      "title": "Haiku: Sonnet for JAX",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Hennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I."
    },
    {
      "index": 13,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Hessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and Silver, D.",
      "orig_title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 14,
      "title": "Distributed Prioritized Experience Replay",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Horgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel, M.; van Hasselt, H. P.; and Silver, D.",
      "orig_title": "Distributed Prioritized Experience Replay",
      "paper_id": "1803.00933v1"
    },
    {
      "index": 15,
      "title": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Imani, E.; Graves, E.; and White, M.",
      "orig_title": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings",
      "paper_id": "1811.09013v2"
    },
    {
      "index": 16,
      "title": "Planning and Acting in Partially Observable Stochastic Domains",
      "abstract": "",
      "year": "1995",
      "venue": "",
      "authors": "Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R."
    },
    {
      "index": 17,
      "title": "Recurrent experience replay in distributed reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on learning representations",
      "authors": "Kapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and Dabney, W."
    },
    {
      "index": 18,
      "title": "A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representation",
      "authors": "Kingma, D. P.; and Adam, J. B."
    },
    {
      "index": 19,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning",
      "authors": "Lin, L."
    },
    {
      "index": 20,
      "title": "Second-order optimization for neural networks",
      "abstract": "",
      "year": "2016",
      "venue": "University of Toronto (Canada)",
      "authors": "Martens, J."
    },
    {
      "index": 21,
      "title": "Steps Toward Artificial Intelligence",
      "abstract": "",
      "year": "1963",
      "venue": "Computers and Thought",
      "authors": "Minsky, M."
    },
    {
      "index": 22,
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K."
    },
    {
      "index": 23,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D."
    },
    {
      "index": 24,
      "title": "Prioritized Sweeping: Reinforcement Learning with less Data and less Time",
      "abstract": "",
      "year": "1993",
      "venue": "Machine Learning",
      "authors": "Moore, A. W.; and Atkeson, C. G."
    },
    {
      "index": 25,
      "title": "Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Ollivier, Y.",
      "orig_title": "Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies",
      "paper_id": "1805.00869v1"
    },
    {
      "index": 26,
      "title": "Efficient dynamic programming-based learning for control",
      "abstract": "",
      "year": "1993",
      "venue": "",
      "authors": "Peng, J."
    },
    {
      "index": 27,
      "title": "Incremental Multi-step Q-learning",
      "abstract": "",
      "year": "1996",
      "venue": "Machine Learning",
      "authors": "Peng, J.; and Williams, R. J."
    },
    {
      "index": 28,
      "title": "Source Traces for Temporal Difference Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "Pitis, S."
    },
    {
      "index": 29,
      "title": "Observe and Look Further: Achieving Consistent Performance on Atari",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "Pohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.; Budden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.; Veƒçer√≠k, M.; Hessel, M.; Munos, R.; and Pietquin, O.",
      "orig_title": "Observe and look further: Achieving consistent performance on Atari",
      "paper_id": "1805.11593v1"
    },
    {
      "index": 30,
      "title": "Adaptive critic designs",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Neural Networks",
      "authors": "Prokhorov, D. V.; and Wunsch, D. C."
    },
    {
      "index": 31,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "John Wiley & Sons, Inc.",
      "authors": "Puterman, M. L."
    },
    {
      "index": 32,
      "title": "Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method",
      "abstract": "",
      "year": "2005",
      "venue": "16th European Conference on Machine Learning (ECML‚Äô05)",
      "authors": "Riedmiller, M."
    },
    {
      "index": 33,
      "title": "Prioritized Experience Replay",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D."
    },
    {
      "index": 34,
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al."
    },
    {
      "index": 35,
      "title": "Reinforcement Learning with replacing eligibility traces",
      "abstract": "",
      "year": "1996",
      "venue": "Machine Learning",
      "authors": "Singh, S. P.; and Sutton, R. S."
    },
    {
      "index": 36,
      "title": "Temporal Credit Assignment in Reinforcement Learning",
      "abstract": "",
      "year": "1984",
      "venue": "",
      "authors": "Sutton, R. S."
    },
    {
      "index": 37,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "Machine learning",
      "authors": "Sutton, R. S."
    },
    {
      "index": 38,
      "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
      "abstract": "",
      "year": "1990",
      "venue": "seventh international conference on machine learning",
      "authors": "Sutton, R. S."
    },
    {
      "index": 39,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Sutton, R. S.; and Barto, A. G."
    },
    {
      "index": 40,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "Journal of Machine Learning Research",
      "authors": "Sutton, R. S.; Mahmood, A. R.; and White, M.",
      "orig_title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 41,
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "abstract": "",
      "year": "2000",
      "venue": "Neural Information Processing Systems 13 (NIPS-00)",
      "authors": "Sutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y."
    },
    {
      "index": 42,
      "title": "Practical Issues in Temporal Difference Learning",
      "abstract": "",
      "year": "1992",
      "venue": "Advances in Neural Information Processing Systems 4",
      "authors": "Tesauro, G."
    },
    {
      "index": 43,
      "title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play",
      "abstract": "",
      "year": "1994",
      "venue": "Neural computation",
      "authors": "Tesauro, G. J."
    },
    {
      "index": 44,
      "title": "Asynchronous stochastic approximation and Q-learning",
      "abstract": "",
      "year": "1994",
      "venue": "Machine Learning",
      "authors": "Tsitsiklis, J. N."
    },
    {
      "index": 45,
      "title": "An analysis of temporal-difference learning with function approximation",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Tsitsiklis, J. N.; and Van Roy, B."
    },
    {
      "index": 46,
      "title": "Reinforcement Learning in Continuous State and Action Spaces",
      "abstract": "",
      "year": "2012",
      "venue": "Reinforcement Learning: State of the Art",
      "authors": "van Hasselt, H. P."
    },
    {
      "index": 47,
      "title": "Learning values across many orders of magnitude",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems 29",
      "authors": "van Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver, D.",
      "orig_title": "Learning values across many orders of magnitude",
      "paper_id": "1602.07714v2"
    },
    {
      "index": 48,
      "title": "Deep Reinforcement Learning with Double Q-learning",
      "abstract": "",
      "year": "2016",
      "venue": "Thirtieth AAAI Conference on Artificial Intelligence",
      "authors": "van Hasselt, H. P.; Guez, A.; and Silver, D.",
      "orig_title": "Deep reinforcement learning with double Q-Learning",
      "paper_id": "1509.06461v3"
    },
    {
      "index": 49,
      "title": "When to use parametric models in reinforcement learning?",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "van Hasselt, H. P.; Hessel, M.; and Aslanides, J."
    },
    {
      "index": 50,
      "title": "Off-policy TD(ŒªùúÜ\\lambda) with a true online equivalence",
      "abstract": "",
      "year": "2014",
      "venue": "30th Conference on Uncertainty in Artificial Intelligence",
      "authors": "van Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S."
    },
    {
      "index": 51,
      "title": "General non-linear Bellman equations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "van Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.; and Barreto, A."
    },
    {
      "index": 52,
      "title": "Learning to predict independent of span",
      "abstract": "",
      "year": "2015",
      "venue": "CoRR",
      "authors": "van Hasselt, H. P.; and Sutton, R. S."
    },
    {
      "index": 53,
      "title": "Planning by Prioritized Sweeping with Small Backups",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "van Seijen, H.; and Sutton, R. S."
    },
    {
      "index": 54,
      "title": "True online TD(ŒªùúÜ\\lambda)",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "van Seijen, H.; and Sutton, R. S."
    },
    {
      "index": 55,
      "title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Wang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt, H. P.; and Lanctot, M.",
      "orig_title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "paper_id": "1511.06581v3"
    },
    {
      "index": 56,
      "title": "A menu of designs for reinforcement learning over time",
      "abstract": "",
      "year": "1990",
      "venue": "Neural networks for control",
      "authors": "Werbos, P. J."
    },
    {
      "index": 57,
      "title": "Learning to perceive and act by trial and error",
      "abstract": "",
      "year": "1991",
      "venue": "Machine Learning",
      "authors": "Whitehead, S. D.; and Ballard, D. H."
    },
    {
      "index": 58,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "Williams, R. J."
    },
    {
      "index": 59,
      "title": "Generalized Off-Policy Actor-Critic",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhang, S.; Boehmer, W.; and Whiteson, S.",
      "orig_title": "Generalized off-policy actor-critic",
      "paper_id": "1903.11329v8"
    }
  ]
}