{
  "paper_id": "2203.00543v1",
  "title": "1 INTRODUCTION",
  "abstract": "Abstract\nIn reinforcement learning, state representations are used to tractably deal with large problem spaces. State representations serve both to approximate the value function with few parameters, but also to generalize to newly encountered states. Their features may be learned implicitly (as part of a neural network) or explicitly (for example, the successor representation of Dayan (1993)). While the approximation properties of representations are reasonably well-understood, a precise characterization of how and when these representations generalize is lacking. In this work, we address this gap and provide an informative bound on the generalization error arising from a specific state representation. This bound is based on the notion of effective dimension which measures the degree to which knowing the value at one state informs the value at other states.\nOur bound applies to any state representation and quantifies the natural tension between representations that generalize well and those that approximate well. We complement our theoretical results with an empirical survey of classic representation learning methods from the literature and results on the Arcade Learning Environment, and find that the generalization behaviour of learned representations is well-explained by their effective dimension.",
  "reference_labels": [
    {
      "index": 0,
      "title": "An optimistic perspective on offline reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi"
    },
    {
      "index": 1,
      "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare",
      "orig_title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning",
      "paper_id": "2101.05265v2"
    },
    {
      "index": 2,
      "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare",
      "orig_title": "Deep reinforcement learning at the edge of the statistical precipice",
      "paper_id": "2108.13264v4"
    },
    {
      "index": 3,
      "title": "Hindsight experience replay",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.01495",
      "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba"
    },
    {
      "index": 4,
      "title": "Residual algorithms: Reinforcement learning with function approximation",
      "abstract": "",
      "year": "1995",
      "venue": "Machine Learning Proceedings 1995",
      "authors": "Leemon Baird"
    },
    {
      "index": 5,
      "title": "Low-rank feature selection for reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "ISAIM",
      "authors": "Bahram Behzadian and Marek Petrik"
    },
    {
      "index": 6,
      "title": "A Geometric Perspective on Optimal Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle",
      "orig_title": "A geometric perspective on optimal representations for reinforcement learning",
      "paper_id": "1901.11530v2"
    },
    {
      "index": 7,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling"
    },
    {
      "index": 8,
      "title": "A distributional perspective on reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Marc G. Bellemare, Will Dabney, and Rémi Munos"
    },
    {
      "index": 9,
      "title": "Learning Successor States and Goal-Dependent Values: A Mathematical Viewpoint",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.07123",
      "authors": "Léonard Blier, Corentin Tallec, and Yann Ollivier",
      "orig_title": "Learning successor states and goal-dependent values: A mathematical viewpoint",
      "paper_id": "2101.07123v1"
    },
    {
      "index": 10,
      "title": "Jax: composable transformations of python+ numpy programs",
      "abstract": "",
      "year": "2018",
      "venue": "URL http://github. com/google/jax",
      "authors": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye Wanderman-Milne"
    },
    {
      "index": 11,
      "title": "Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31",
      "abstract": "",
      "year": "2013",
      "venue": "Springer Science & Business Media",
      "authors": "Pierre Brémaud"
    },
    {
      "index": 12,
      "title": "Exact matrix completion via convex optimization",
      "abstract": "",
      "year": "2009",
      "venue": "Foundations of Computational mathematics",
      "authors": "Emmanuel J Candès and Benjamin Recht"
    },
    {
      "index": 13,
      "title": "Dopamine: A Research Framework for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Pablo S. Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare",
      "orig_title": "Dopamine: A research framework for deep reinforcement learning",
      "paper_id": "1812.06110v1"
    },
    {
      "index": 14,
      "title": "Two-timescale networks for nonlinear value function approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on learning representations",
      "authors": "Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White"
    },
    {
      "index": 15,
      "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos",
      "orig_title": "Implicit quantile networks for distributional reinforcement learning",
      "paper_id": "1806.06923v1"
    },
    {
      "index": 16,
      "title": "The Value-Improvement Path: Towards Better Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.02243",
      "authors": "Will Dabney, André Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G Bellemare, and David Silver",
      "orig_title": "The value-improvement path: Towards better representations for reinforcement learning",
      "paper_id": "2006.02243v2"
    },
    {
      "index": 17,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation",
      "authors": "Peter Dayan"
    },
    {
      "index": 18,
      "title": "Representations for Stable Off-Policy Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Dibya Ghosh and Marc G Bellemare",
      "orig_title": "Representations for stable off-policy reinforcement learning",
      "paper_id": "2007.05520v2"
    },
    {
      "index": 19,
      "title": "Toeplitz and circulant matrices: A review",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "Robert M. Gray"
    },
    {
      "index": 20,
      "title": "Introduction to probability",
      "abstract": "",
      "year": "2012",
      "venue": "American Mathematical Soc.",
      "authors": "Charles Miller Grinstead and James Laurie Snell"
    },
    {
      "index": 21,
      "title": "Rl unplugged: A collection of benchmarks for offline reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gómez, Konrad Zolna, Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al."
    },
    {
      "index": 22,
      "title": "Array Programming with NumPy",
      "abstract": "",
      "year": "2020",
      "venue": "Nature",
      "authors": "Charles R Harris, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al.",
      "orig_title": "Array programming with numpy",
      "paper_id": "2006.10256v1"
    },
    {
      "index": 23,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver",
      "orig_title": "Rainbow: Combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 24,
      "title": "A tail inequality for quadratic forms of subgaussian random vectors",
      "abstract": "",
      "year": "2012",
      "venue": "Electronic Communications in Probability",
      "authors": "Daniel Hsu, Sham Kakade, and Tong Zhang"
    },
    {
      "index": 25,
      "title": "Random design analysis of ridge regression",
      "abstract": "",
      "year": "2012",
      "venue": "Conference on learning theory",
      "authors": "Daniel Hsu, Sham M Kakade, and Tong Zhang"
    },
    {
      "index": 26,
      "title": "Matplotlib: A 2d graphics environment",
      "abstract": "",
      "year": "2007",
      "venue": "Computing in science & engineering",
      "authors": "John D Hunter"
    },
    {
      "index": 27,
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Max Jaderberg, Volodymyr Mnih, Wojciech M. Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu",
      "orig_title": "Reinforcement learning with unsupervised auxiliary tasks",
      "paper_id": "1611.05397v1"
    },
    {
      "index": 28,
      "title": "Scipy: Open source scientific tools for python",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": "Eric Jones, Travis Oliphant, Pearu Peterson, et al."
    },
    {
      "index": 29,
      "title": "Finite continuous time markov chains",
      "abstract": "",
      "year": "1961",
      "venue": "Theory of Probability & Its Applications",
      "authors": "John G Kemeny and J Laurie Snell"
    },
    {
      "index": 30,
      "title": "Value function approximation in reinforcement learning using the fourier basis",
      "abstract": "",
      "year": "2011",
      "venue": "25th Conference on Artificial Intelligence",
      "authors": "George D. Konidaris, Sarah Osentoski, and Philip S. Thomas"
    },
    {
      "index": 31,
      "title": "Implicit under-parameterization inhibits data-efficient deep reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine"
    },
    {
      "index": 32,
      "title": "Dr3: Value-based deep reinforcement learning requires explicit regularization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine"
    },
    {
      "index": 33,
      "title": "Shallow Updates for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nir Levine, Tom Zahavy, Daniel Mankowitz, Aviv Tamar, and Shie Mannor",
      "orig_title": "Shallow updates for deep reinforcement learning",
      "paper_id": "1705.07461v2"
    },
    {
      "index": 34,
      "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.01643",
      "authors": "Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu",
      "orig_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "paper_id": "2005.01643v3"
    },
    {
      "index": 35,
      "title": "On the effect of auxiliary tasks on representation dynamics",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney"
    },
    {
      "index": 36,
      "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "M.C. Machado, M.G. Bellemare, and M. Bowling",
      "orig_title": "A Laplacian framework for option discovery in reinforcement learning",
      "paper_id": "1703.00956v2"
    },
    {
      "index": 37,
      "title": "Representation learning on graphs: A reinforcement learning application",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Sephora Madjiheurem and Laura Toni"
    },
    {
      "index": 38,
      "title": "Proto-value functions: A laplacian framework for learning representation and control in markov decision processes",
      "abstract": "",
      "year": "2007",
      "venue": "Journal of Machine Learning Research",
      "authors": "Sridhar Mahadevan and Mauro Maggioni"
    },
    {
      "index": 39,
      "title": "Compressed least-squares regression",
      "abstract": "",
      "year": "2009",
      "venue": "NIPS 2009",
      "authors": "Odalric-Ambrym Maillard and Rémi Munos"
    },
    {
      "index": 40,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 41,
      "title": "Foundations of machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar"
    },
    {
      "index": 42,
      "title": "A guide to NumPy, volume 1",
      "abstract": "",
      "year": "2006",
      "venue": "Trelgol Publishing USA",
      "authors": "Travis E Oliphant"
    },
    {
      "index": 43,
      "title": "Python for scientific computing",
      "abstract": "",
      "year": "2007",
      "venue": "Computing in Science & Engineering",
      "authors": "Travis E Oliphant"
    },
    {
      "index": 44,
      "title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "25th international conference on Machine learning",
      "authors": "Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L Littman"
    },
    {
      "index": 45,
      "title": "An analysis of laplacian methods for value function approximation in mdps",
      "abstract": "",
      "year": "2007",
      "venue": "IJCAI",
      "authors": "Marek Petrik"
    },
    {
      "index": 46,
      "title": "Markov decision processes: Discrete stochastic dynamic programming",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": "Martin L Puterman"
    },
    {
      "index": 47,
      "title": "Sparse distributed memories for on-line value-based reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "15th European Conference on Machine Learning",
      "authors": "Bohdana Ratitch and Doina Precup"
    },
    {
      "index": 48,
      "title": "Optimal behavioral hierarchy",
      "abstract": "",
      "year": "2014",
      "venue": "PLoS Computational Biology",
      "authors": "Alec Solway, Carlos Diuk, Natalia Córdova, Debbie Yee, Andrew G Barto, Yael Niv, and Matthew M Botvinick"
    },
    {
      "index": 49,
      "title": "Design principles of the hippocampal cognitive map",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kimberly L. Stachenfeld, Matthew Botvinick, and Samuel J. Gershman"
    },
    {
      "index": 50,
      "title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
      "abstract": "",
      "year": "1996",
      "venue": "neural information processing systems",
      "authors": "Richard S Sutton"
    },
    {
      "index": 51,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 52,
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial Intelligence",
      "authors": "R.S. Sutton, D. Precup, and S. Singh"
    },
    {
      "index": 53,
      "title": "Transfer learning for reinforcement learning domains: A survey",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research",
      "authors": "Matthew E. Taylor and Peter Stone"
    },
    {
      "index": 54,
      "title": "An introduction to matrix concentration inequalities",
      "abstract": "",
      "year": "2015",
      "venue": "Foundations and Trends in Machine Learning",
      "authors": "Joel A. Tropp"
    },
    {
      "index": 55,
      "title": "Python reference manual",
      "abstract": "",
      "year": "1995",
      "venue": "Centrum voor Wiskunde en Informatica Amsterdam",
      "authors": "Guido Van Rossum and Fred L Drake Jr"
    },
    {
      "index": 56,
      "title": "The nature of statistical learning",
      "abstract": "",
      "year": "1995",
      "venue": "Theory",
      "authors": "Vladimir N Vapnik"
    },
    {
      "index": 57,
      "title": "Introduction to the non-asymptotic analysis of random matrices",
      "abstract": "",
      "year": "2010",
      "venue": "arXiv preprint arXiv:1011.3027",
      "authors": "Roman Vershynin"
    },
    {
      "index": 58,
      "title": "𝑞-Munchausen Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2007.14430",
      "authors": "Nino Vieillard, Olivier Pietquin, and Matthieu Geist",
      "orig_title": "Munchausen reinforcement learning",
      "paper_id": "2205.07467v1"
    },
    {
      "index": 59,
      "title": "The numpy array: a structure for efficient numerical computation",
      "abstract": "",
      "year": "2011",
      "venue": "Computing in science & engineering",
      "authors": "Stéfan van der Walt, S Chris Colbert, and Gael Varoquaux"
    }
  ]
}