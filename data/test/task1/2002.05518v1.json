{
  "paper_id": "2002.05518v1",
  "title": "Learning State Abstractions for Transfer in Continuous Control",
  "abstract": "Abstract\nCan simple algorithms with a good representation solve challenging reinforcement learning problems?\nIn this work, we answer this question in the affirmative, where we take “simple learning algorithm” to be tabular Q-Learning, the “good representations” to be a learned state abstraction, and “challenging problems” to be continuous control tasks.\nOur main contribution is a learning algorithm that abstracts a continuous state-space into a discrete one. We transfer this learned representation to unseen problems to enable effective learning. We provide theory showing that learned abstractions maintain a bounded value loss, and we report experiments showing that the abstractions empower tabular Q-Learning to learn efficiently in unseen tasks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Near optimal behavior via approximate state abstraction",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Abel, D., Hershkowitz, D. E., and Littman, M. L."
    },
    {
      "index": 1,
      "title": "State abstractions for lifelong reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Abel, D., Arumugam, D., Lehnert, L., and Littman, M. L."
    },
    {
      "index": 2,
      "title": "State abstraction as compression in apprenticeship learning",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Abel, D., Arumugam, D., Asadi, K., Jinnai, Y., Littman, M. L., and Wong, L. L."
    },
    {
      "index": 3,
      "title": "Successor features for transfer in reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D."
    },
    {
      "index": 4,
      "title": "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "Bartlett, P. L. and Mendelson, S."
    },
    {
      "index": 5,
      "title": "Generalization in reinforcement learning: Safely approximating the value function",
      "abstract": "",
      "year": "1995",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Boyan, J. A. and Moore, A. W."
    },
    {
      "index": 6,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 7,
      "title": "Intelligence without representation",
      "abstract": "",
      "year": "1991",
      "venue": "Artificial Intelligence",
      "authors": "Brooks, R. A."
    },
    {
      "index": 8,
      "title": "PAC-inspired option discovery in lifelong reinforcement learning",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Brunskill, E. and Li, L."
    },
    {
      "index": 9,
      "title": "Input generalization in delayed reinforcement learning: An algorithm and performance comparisons",
      "abstract": "",
      "year": "1991",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Chapman, D. and Kaelbling, L. P."
    },
    {
      "index": 10,
      "title": "Automatic state abstraction from demonstration",
      "abstract": "",
      "year": "2011",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Cobo, L. C., Zang, P., Isbell Jr, C. L., and Thomaz, A. L."
    },
    {
      "index": 11,
      "title": "Automatic task decomposition and state abstraction from demonstration",
      "abstract": "",
      "year": "2012",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Cobo, L. C., Isbell Jr, C. L., and Thomaz, A. L."
    },
    {
      "index": 12,
      "title": "Learning parameterized skills",
      "abstract": "",
      "year": "2012",
      "venue": "International Conference on Machine Learning",
      "authors": "Da Silva, B., Konidaris, G., and Barto, A."
    },
    {
      "index": 13,
      "title": "Feudal reinforcement learning",
      "abstract": "",
      "year": "1993",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dayan, P. and Hinton, G. E."
    },
    {
      "index": 14,
      "title": "Hierarchical reinforcement learning with the MAXQ value function decomposition",
      "abstract": "",
      "year": "2000",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Dietterich, T. G."
    },
    {
      "index": 15,
      "title": "Dynamic programming for structured continuous markov decision problems",
      "abstract": "",
      "year": "2004",
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": "Feng, Z., Dearden, R., Meuleau, N., and Washington, R."
    },
    {
      "index": 16,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Finn, C., Abbeel, P., and Levine, S.",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 17,
      "title": "An algorithm for finding best matches in logarithmic time",
      "abstract": "",
      "year": "1976",
      "venue": "ACM Trans. Math. Software",
      "authors": "Friedman, J. H., Bentley, J. L., and Finkel, R. A."
    },
    {
      "index": 18,
      "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Higgins, I., Pal, A., Rusu, A. A., Matthey, L., Burgess, C. P., Pritzel, A., Botvinick, M., Blundell, C., and Lerchner, A.",
      "orig_title": "DARLA: Improving zero-shot transfer in reinforcement learning",
      "paper_id": "1707.08475v2"
    },
    {
      "index": 19,
      "title": "Learning state representations with robotic priors",
      "abstract": "",
      "year": "2015",
      "venue": "Autonomous Robots",
      "authors": "Jonschkowski, R. and Brock, O."
    },
    {
      "index": 20,
      "title": "On the Sample Complexity of Reinforcement Learning",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Kakade, S. M."
    },
    {
      "index": 21,
      "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Karl, M., Soelch, M., Bayer, J., and van der Smagt, P.",
      "orig_title": "Deep variational Bayes filters: Unsupervised learning of state space models from raw data",
      "paper_id": "1605.06432v3"
    },
    {
      "index": 22,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Kingma, D. P. and Ba, J."
    },
    {
      "index": 23,
      "title": "On the necessity of abstraction",
      "abstract": "",
      "year": "2019",
      "venue": "Current Opinion in Behavioral Sciences",
      "authors": "Konidaris, G."
    },
    {
      "index": 24,
      "title": "Autonomous shaping: Knowledge transfer in reinforcement learning",
      "abstract": "",
      "year": "2006",
      "venue": "International Conference on Machine Learning",
      "authors": "Konidaris, G. and Barto, A."
    },
    {
      "index": 25,
      "title": "Building portable options: Skill transfer in reinforcement learning",
      "abstract": "",
      "year": "2007",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Konidaris, G. and Barto, A. G."
    },
    {
      "index": 26,
      "title": "Transfer in reinforcement learning via shared features",
      "abstract": "",
      "year": "2012",
      "venue": "Journal of Machine Learning Research",
      "authors": "Konidaris, G., Scheidwasser, I., and Barto, A."
    },
    {
      "index": 27,
      "title": "Adaptive state space quantisation for reinforcement learning of collision-free navigation",
      "abstract": "",
      "year": "1992",
      "venue": "Intelligent Robots and Systems, IEEE/RSJ International Conference",
      "authors": "Krose, B. J. and Van Dam, J. W."
    },
    {
      "index": 28,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "LeCun, Y., Bengio, Y., and Hinton, G.",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 29,
      "title": "Adaptive state space partitioning for reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "Engineering applications of artificial intelligence",
      "authors": "Lee, I. S. and Lau, H. Y."
    },
    {
      "index": 30,
      "title": "State Representation Learning for Control: An Overview",
      "abstract": "",
      "year": "2018",
      "venue": "Neural Networks",
      "authors": "Lesort, T., Díaz-Rodríguez, N., Goudou, J.-F., and Filliat, D.",
      "orig_title": "State representation learning for control: An overview",
      "paper_id": "1802.04181v2"
    },
    {
      "index": 31,
      "title": "Towards a unified theory of state abstraction for MDPs",
      "abstract": "",
      "year": "2006",
      "venue": "ISAIM",
      "authors": "Li, L., Walsh, T. J., and Littman, M. L."
    },
    {
      "index": 32,
      "title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "2016 International Conference on Autonomous Agents & Multiagent Systems",
      "authors": "Liang, Y., Machado, M. C., Talvitie, E., and Bowling, M.",
      "orig_title": "State of the art control of Atari games using shallow reinforcement learning",
      "paper_id": "1512.01563v2"
    },
    {
      "index": 33,
      "title": "On Q-learning convergence for non-Markov decision processes",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "Majeed, S. J. and Hutter, M."
    },
    {
      "index": 34,
      "title": "A vector-contraction inequality for Rademacher complexities",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Algorithmic Learning Theory",
      "authors": "Maurer, A.",
      "orig_title": "A vector-contraction inequality for Rademacher complexities",
      "paper_id": "1605.00251v1"
    },
    {
      "index": 35,
      "title": "Learning to use selective attention and short-term memory in sequential tasks",
      "abstract": "",
      "year": "1996",
      "venue": "Fourth International Conference on Simulation of Adaptive Behavior",
      "authors": "McCallum, A. K."
    },
    {
      "index": 36,
      "title": "A proposal for the Dartmouth summer research project on artificial intelligence, August 31, 1955",
      "abstract": "",
      "year": "1955",
      "venue": "AI Magazine",
      "authors": "McCarthy, J., Minsky, M. L., Rochester, N., and Shannon, C. E."
    },
    {
      "index": 37,
      "title": "Transfer in variable-reward hierarchical reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Machine Learning",
      "authors": "Mehta, N., Natarajan, S., Tadepalli, P., and Fern, A."
    },
    {
      "index": 38,
      "title": "State abstraction synthesis for discrete models of continuous domains",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Spring Symposium Series",
      "authors": "Menashe, J. and Stone, P."
    },
    {
      "index": 39,
      "title": "Foundations of machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Mohri, M., Rostamizadeh, A., and Talwalkar, A."
    },
    {
      "index": 40,
      "title": "The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces",
      "abstract": "",
      "year": "1994",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Moore, A. W."
    },
    {
      "index": 41,
      "title": "Value Prediction Network",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Oh, J., Singh, S., and Lee, H.",
      "orig_title": "Value prediction network",
      "paper_id": "1707.03497v2"
    },
    {
      "index": 42,
      "title": "Actor-Mimic Deep Multitask and Transfer Reinforcement Learning",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1511.06342",
      "authors": "Parisotto, E., Ba, J. L., and Salakhutdinov, R.",
      "orig_title": "Actor-mimic: Deep multitask and transfer reinforcement learning",
      "paper_id": "1511.06342v4"
    },
    {
      "index": 43,
      "title": "Reinforcement learning with hierarchies of machines",
      "abstract": "",
      "year": "1998",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Parr, R. and Russell, S. J."
    },
    {
      "index": 44,
      "title": "PAC optimal exploration in continuous space Markov decision processes",
      "abstract": "",
      "year": "2013",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Pazis, J. and Parr, R."
    },
    {
      "index": 45,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Puterman, M. L."
    },
    {
      "index": 46,
      "title": "Meta Reinforcement Learning with Latent Variable Gaussian Processes",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.07551",
      "authors": "Sæmundsson, S., Hofmann, K., and Deisenroth, M. P.",
      "orig_title": "Meta reinforcement learning with latent variable gaussian processes",
      "paper_id": "1803.07551v2"
    },
    {
      "index": 47,
      "title": "Reinforcement learning with soft state aggregation",
      "abstract": "",
      "year": "1995",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Singh, S. P., Jaakkola, T., and Jordan, M. I."
    },
    {
      "index": 48,
      "title": "Learning to predict by the methods of temporal differences",
      "abstract": "",
      "year": "1988",
      "venue": "Machine Learning",
      "authors": "Sutton, R. S."
    },
    {
      "index": 49,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Sutton, R. S. and Barto, A. G."
    },
    {
      "index": 50,
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial Intelligence",
      "authors": "Sutton, R. S., Precup, D., and Singh, S."
    },
    {
      "index": 51,
      "title": "Behavior transfer for value-function-based reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "Fourth international joint conference on Autonomous agents and multiagent systems",
      "authors": "Taylor, M. E. and Stone, P."
    },
    {
      "index": 52,
      "title": "Transfer learning for reinforcement learning domains: A survey",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research",
      "authors": "Taylor, M. E. and Stone, P."
    },
    {
      "index": 53,
      "title": "Distral: Robust Multitask Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Teh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R.",
      "orig_title": "Distral: Robust multitask reinforcement learning",
      "paper_id": "1707.04175v1"
    },
    {
      "index": 54,
      "title": "Temporal difference learning and TD-gammon",
      "abstract": "",
      "year": "1995",
      "venue": "Communications of the ACM",
      "authors": "Tesauro, G."
    },
    {
      "index": 55,
      "title": "A deep hierarchical approach to lifelong learning in minecraft",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Tessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S."
    },
    {
      "index": 56,
      "title": "Learning to learn",
      "abstract": "",
      "year": "1998",
      "venue": "Springer Science & Business Media",
      "authors": "Thrun, S. and Pratt, L."
    },
    {
      "index": 57,
      "title": "Tree based discretization for continuous state space reinforcement learning",
      "abstract": "",
      "year": "1998",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Uther, W. T. and Veloso, M. M."
    },
    {
      "index": 58,
      "title": "Transferring state abstractions between MDPs",
      "abstract": "",
      "year": "2006",
      "venue": "ICML Workshop on Structural Knowledge Transfer for Machine Learning",
      "authors": "Walsh, T. J., Li, L., and Littman, M. L."
    },
    {
      "index": 59,
      "title": "Learning to reinforcement learn",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.05763",
      "authors": "Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M."
    },
    {
      "index": 60,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "Watkins, C. J. and Dayan, P."
    },
    {
      "index": 61,
      "title": "Adaptive tile coding for value function approximation, 2007.",
      "abstract": "",
      "year": "2007",
      "venue": "AI Technical Report AI-TR-07-339, University of Texas at Austin",
      "authors": "Whiteson, S., Taylor, M. E., and Stone, P."
    },
    {
      "index": 62,
      "title": "Pattern-recognizing control systems",
      "abstract": "",
      "year": "1964",
      "venue": "Computer and Information Sciences (COINS)",
      "authors": "Widrow, B. and Smith, F. W."
    },
    {
      "index": 63,
      "title": "Multi-task reinforcement learning: A hierarchical bayesian approach",
      "abstract": "",
      "year": "2007",
      "venue": "International Conference on Machine Learning",
      "authors": "Wilson, A., Fern, A., Ray, S., and Tadepalli, P."
    }
  ]
}