{
  "paper_id": "2012.13490v2",
  "title": "Towards Continual Reinforcement Learning: A Review and Perspectives",
  "abstract": "Abstract\nIn this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.111This work has been accepted by the Journal of Artificial Intelligence Research (JAIR).",
  "reference_labels": [
    {
      "index": 0,
      "title": "Interference and Generalization in Temporal Difference Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.06350",
      "authors": "Emmanuel Bengio, Joelle Pineau, and Doina Precup",
      "orig_title": "Interference and generalization in temporal difference learning",
      "paper_id": "2003.06350v1"
    },
    {
      "index": 1,
      "title": "Child: A first step towards continual learning",
      "abstract": "",
      "year": "1997",
      "venue": "Machine Learning",
      "authors": "Mark B Ring"
    },
    {
      "index": 2,
      "title": "Continual Lifelong Learning with Neural Networks: A Review",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter",
      "orig_title": "Continual lifelong learning with neural networks: A review",
      "paper_id": "1802.07569v4"
    },
    {
      "index": 3,
      "title": "Continual learning: A comparative study on how to defy forgetting in classification tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.08383",
      "authors": "Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars"
    },
    {
      "index": 4,
      "title": "A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2009.01797",
      "authors": "Martin Mundt, Yong Won Hong, Iuliia Pliushch, and Visvanathan Ramesh",
      "orig_title": "A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning",
      "paper_id": "2009.01797v3"
    },
    {
      "index": 5,
      "title": "Embracing change: Continual learning in deep neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Trends in Cognitive Sciences",
      "authors": "Raia Hadsell, Dushyant Rao, Andrei A. Rusu, and Razvan Pascanu"
    },
    {
      "index": 6,
      "title": "Markov decision processes. 1994.",
      "abstract": "",
      "year": "1994",
      "venue": "Jhon Wiley & Sons, New Jersey",
      "authors": "ML Puterman"
    },
    {
      "index": 7,
      "title": "Introduction to Reinforcement Learning",
      "abstract": "",
      "year": "1998",
      "venue": "MIT Press, Cambridge, MA, USA",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 8,
      "title": "Learning to achieve goals",
      "abstract": "",
      "year": "1993",
      "venue": "IJCAI",
      "authors": "Leslie Pack Kaelbling"
    },
    {
      "index": 9,
      "title": "Deep reinforcement learning amidst continual structured non-stationarity",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Annie Xie, James Harrison, and Chelsea Finn"
    },
    {
      "index": 10,
      "title": "Continual Learning In Environments With Polynomial Mixing Times",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Matthew Riemer, Sharath Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj, Maximilian Puelma Touzel, and Irina Rish",
      "orig_title": "Continual learning in environments with polynomial mixing times",
      "paper_id": "2112.07066v2"
    },
    {
      "index": 11,
      "title": "Beyond domain randomization",
      "abstract": "",
      "year": "2019",
      "venue": "Slides",
      "authors": "Josh Tobin"
    },
    {
      "index": 12,
      "title": "Transfer learning for reinforcement learning domains: A survey",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research",
      "authors": "Matthew E Taylor and Peter Stone"
    },
    {
      "index": 13,
      "title": "Multi-task reinforcement learning: a hierarchical bayesian approach",
      "abstract": "",
      "year": "2007",
      "venue": "24th international conference on Machine learning",
      "authors": "Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli"
    },
    {
      "index": 14,
      "title": "Online multi-task learning for policy gradient methods",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew Taylor"
    },
    {
      "index": 15,
      "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.01643",
      "authors": "Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu",
      "orig_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "paper_id": "2005.01643v3"
    },
    {
      "index": 16,
      "title": "Generalization and Regularization in DQN",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.00123",
      "authors": "Jesse Farebrother, Marlos C Machado, and Michael Bowling",
      "orig_title": "Generalization and regularization in dqn",
      "paper_id": "1810.00123v3"
    },
    {
      "index": 17,
      "title": "Gotta learn fast: A new benchmark for generalization in rl",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.03720",
      "authors": "Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman"
    },
    {
      "index": 18,
      "title": "Assessing Generalization in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.12282",
      "authors": "Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song",
      "orig_title": "Assessing generalization in deep reinforcement learning",
      "paper_id": "1810.12282v2"
    },
    {
      "index": 19,
      "title": "Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.10729",
      "authors": "Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi",
      "orig_title": "Illuminating generalization in deep reinforcement learning through procedural level generation",
      "paper_id": "1806.10729v5"
    },
    {
      "index": 20,
      "title": "A study on overfitting in deep reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.06893",
      "authors": "Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio"
    },
    {
      "index": 21,
      "title": "Quantifying Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.02341",
      "authors": "Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman",
      "orig_title": "Quantifying generalization in reinforcement learning",
      "paper_id": "1812.02341v3"
    },
    {
      "index": 22,
      "title": "Is a good representation sufficient for sample efficient reinforcement learning?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.03016",
      "authors": "Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang"
    },
    {
      "index": 23,
      "title": "A massively parallel architecture for a self-organizing neural pattern recognition machine",
      "abstract": "",
      "year": "1987",
      "venue": "Computer vision, graphics, and image processing",
      "authors": "Gail A Carpenter and Stephen Grossberg"
    },
    {
      "index": 24,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychology of learning and motivation",
      "authors": "Michael McCloskey and Neal J Cohen"
    },
    {
      "index": 25,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "national academy of sciences",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 26,
      "title": "Gradient episodic memory for continuum learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Lopez-Paz and Marc’Aurelio Ranzato"
    },
    {
      "index": 27,
      "title": "Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro",
      "orig_title": "Learning to learn without forgetting by maximizing transfer and minimizing interference",
      "paper_id": "1810.11910v3"
    },
    {
      "index": 28,
      "title": "Efficient lifelong learning with a-gem",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny"
    },
    {
      "index": 29,
      "title": "Reinforcement learning and its relationship to supervised learning",
      "abstract": "",
      "year": "2004",
      "venue": "Handbook of learning and approximate dynamic programming",
      "authors": "Andrew G Barto and Thomas G Dietterich"
    },
    {
      "index": 30,
      "title": "Near-optimal model-free reinforcement learning in non-stationary episodic mdps",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar"
    },
    {
      "index": 31,
      "title": "Markov games as a framework for multi-agent reinforcement learning",
      "abstract": "",
      "year": "1994",
      "venue": "ISBN 1-",
      "authors": "Michael L. Littman."
    },
    {
      "index": 32,
      "title": "Influencing Long-Term Behavior in Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dong Ki Kim, Matthew Riemer, Miao Liu, Jakob N Foerster, Michael Everett,\nChuangchuang Sun, Gerald Tesauro, and Jonathan P How.",
      "orig_title": "Influencing long-term behavior in multiagent reinforcement learning",
      "paper_id": "2203.03535v4"
    },
    {
      "index": 33,
      "title": "Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Finale Doshi-Velez and George Konidaris."
    },
    {
      "index": 34,
      "title": "Hidden-mode markov decision processes for nonstationary sequential decision making",
      "abstract": "",
      "year": "2000",
      "venue": "Sequence Learning",
      "authors": "Samuel PM Choi, Dit-Yan Yeung, and Nevin L Zhang."
    },
    {
      "index": 35,
      "title": "Planning under uncertainty for robotic tasks with mixed observability",
      "abstract": "",
      "year": "2010",
      "venue": "The International Journal of Robotics Research, 29(8):",
      "authors": "Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee."
    },
    {
      "index": 36,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Machine Learning Research, 11(4)",
      "authors": "Thomas Jaksch, Ronald Ortner, and Peter Auer."
    },
    {
      "index": 37,
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos.",
      "orig_title": "Minimax regret bounds for reinforcement learning",
      "paper_id": "1703.05449v2"
    },
    {
      "index": 38,
      "title": "Provably efficient RL with Rich Observations via Latent State Decoding",
      "abstract": "",
      "year": "1901",
      "venue": "arXiv preprint arXiv:",
      "authors": "Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav\nDudík, and John Langford.",
      "orig_title": "Provably efficient rl with rich observations via latent state decoding",
      "paper_id": "1901.09018v3"
    },
    {
      "index": 39,
      "title": "Sparsely Changing Latent States for Prediction and Planning in Partially Observable Domains",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Christian Gumbsch, Martin V Butz, and Georg Martius.",
      "orig_title": "Sparsely changing latent states for prediction and planning in partially observable domains",
      "paper_id": "2110.15949v2"
    },
    {
      "index": 40,
      "title": "Efficient reinforcement learning in factored mdps",
      "abstract": "",
      "year": "1999",
      "venue": "IJCAI, volume 16",
      "authors": "Michael Kearns and Daphne Koller."
    },
    {
      "index": 41,
      "title": "Stochastic dynamic programming with factored representations",
      "abstract": "",
      "year": "2000",
      "venue": "Artificial Intelligence, 121(1):49–107",
      "authors": "Craig Boutilier, Richard Dearden, and Moisés Goldszmidt."
    },
    {
      "index": 42,
      "title": "Sample-efficient reinforcement learning for pomdps with linear function approximations",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Qi Cai, Zhuoran Yang, and Zhaoran Wang."
    },
    {
      "index": 43,
      "title": "FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems,\n33:",
      "authors": "Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.",
      "orig_title": "Flambe: Structural complexity and representation learning of low rank mdps",
      "paper_id": "2006.10814v2"
    },
    {
      "index": 44,
      "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Masatoshi Uehara, Xuezhou Zhang, and Wen Sun.",
      "orig_title": "Representation learning for online and offline rl in low-rank mdps",
      "paper_id": "2110.04652v3"
    },
    {
      "index": 45,
      "title": "Provably efficient representation learning in low-rank markov decision processes",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu."
    },
    {
      "index": 46,
      "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in neural information processing systems,\n34:",
      "authors": "Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.",
      "orig_title": "Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms",
      "paper_id": "2102.00815v4"
    },
    {
      "index": 47,
      "title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang.",
      "orig_title": "Adarl: What, where, and how to adapt in transfer reinforcement learning",
      "paper_id": "2107.02729v4"
    },
    {
      "index": 48,
      "title": "Provable rich observation reinforcement learning with combinatorial latent states",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford."
    },
    {
      "index": 49,
      "title": "Bayesian reinforcement learning in factored pomdps",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 18th International Conference on\nAutonomous Agents and MultiAgent Systems",
      "authors": "Sammie Katt, Frans A Oliehoek, and Christopher Amato."
    },
    {
      "index": 50,
      "title": "Block Contextual MDPs for Continual Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Learning for Dynamics and Control Conference",
      "authors": "Shagun Sodhani, Franziska Meier, Joelle Pineau, and Amy Zhang.",
      "orig_title": "Block contextual mdps for continual learning",
      "paper_id": "2110.06972v1"
    },
    {
      "index": 51,
      "title": "Learning Domain Invariant Representations in Goal-conditioned Block MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:764–776",
      "authors": "Beining Han, Chongyi Zheng, Harris Chan, Keiran Paster, Michael Zhang, and\nJimmy Ba.",
      "orig_title": "Learning domain invariant representations in goal-conditioned block mdps",
      "paper_id": "2110.14248v2"
    },
    {
      "index": 52,
      "title": "Generalizing plans to new environments in relational mdps",
      "abstract": "",
      "year": "2003",
      "venue": "Proceedings of the 18th international joint conference on\nArtificial intelligence",
      "authors": "Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia."
    },
    {
      "index": 53,
      "title": "Symbolic dynamic programming for first-order mdps",
      "abstract": "",
      "year": "2001",
      "venue": "IJCAI, volume 1",
      "authors": "Craig Boutilier, Raymond Reiter, and Bob Price."
    },
    {
      "index": 54,
      "title": "An object-oriented representation for efficient reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine\nlearning",
      "authors": "Carlos Diuk, Andre Cohen, and Michael L Littman."
    },
    {
      "index": 55,
      "title": "LTL2Action: Generalizing LTL Instructions for Multi-Task RL",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Pashootan Vaezipoor, Andrew C Li, Rodrigo A Toro Icarte, and Sheila A\nMcilraith.",
      "orig_title": "Ltl2action: Generalizing ltl instructions for multi-task rl",
      "paper_id": "2102.06858v3"
    },
    {
      "index": 56,
      "title": "Temporal-logic-based reward shaping for continuing reinforcement learning tasks",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35",
      "authors": "Yuqian Jiang, Suda Bharadwaj, Bo Wu, Rishi Shah, Ufuk Topcu, and Peter Stone."
    },
    {
      "index": 57,
      "title": "Learning Action Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip S\nThomas.",
      "orig_title": "Learning action representations for reinforcement learning",
      "paper_id": "1902.00183v2"
    },
    {
      "index": 58,
      "title": "How RL Agents Behave When Their Actions Are Modified",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35",
      "authors": "Eric D Langlois and Tom Everitt.",
      "orig_title": "How rl agents behave when their actions are modified",
      "paper_id": "2102.07716v2"
    },
    {
      "index": 59,
      "title": "Know your action set: Learning action relations for reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Ayush Jain, Norio Kosaka, Kyung-Min Kim, and Joseph J Lim."
    },
    {
      "index": 60,
      "title": "Anymorph: Learning transferable polices by inferring agent morphology",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Brandon Trabucco, Mariano Phielipp, and Glen Berseth."
    },
    {
      "index": 61,
      "title": "Invariant Causal Prediction for Block MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle\nPineau, Yarin Gal, and Doina Precup.",
      "orig_title": "Invariant causal prediction for block mdps",
      "paper_id": "2003.06016v2"
    },
    {
      "index": 62,
      "title": "Multitask learning",
      "abstract": "",
      "year": "1997",
      "venue": "Machine Learning, 28(1):41–75",
      "authors": "Rich Caruana."
    },
    {
      "index": 63,
      "title": "Multi-criteria reinforcement learning",
      "abstract": "",
      "year": "1998",
      "venue": "ICML, volume 98",
      "authors": "Zoltán Gábor, Zsolt Kalmár, and Csaba Szepesvári."
    },
    {
      "index": 64,
      "title": "Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks",
      "abstract": "",
      "year": "1991",
      "venue": "Proceedings of the 13th annual cognitive science society\nconference, volume 1",
      "authors": "Robert M French."
    },
    {
      "index": 65,
      "title": "Gradient Surgery for Multi-Task Learning",
      "abstract": "",
      "year": "2001",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and\nChelsea Finn.",
      "orig_title": "Gradient surgery for multi-task learning",
      "paper_id": "2001.06782v4"
    },
    {
      "index": 66,
      "title": "Multi-task reinforcement learning in partially observable stochastic environments",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research, 10(May):",
      "authors": "Hui Li, Xuejun Liao, and Lawrence Carin."
    },
    {
      "index": 67,
      "title": "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-… hook",
      "abstract": "",
      "year": "1987",
      "venue": "PhD thesis, Technische Universität München",
      "authors": "Jürgen Schmidhuber."
    },
    {
      "index": 68,
      "title": "On the optimization of a synaptic learning rule",
      "abstract": "",
      "year": "1992",
      "venue": "Preprints Conf. Optimality in Artificial and Biological\nNeural Networks, volume 2. Univ. of Texas",
      "authors": "Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei."
    },
    {
      "index": 69,
      "title": "Learning to reinforcement learn",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo,\nRemi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick."
    },
    {
      "index": 70,
      "title": "Rl2: Fast reinforcement learning via slow reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter\nAbbeel."
    },
    {
      "index": 71,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine.",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 72,
      "title": "A simple neural attentive meta-learner",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel."
    },
    {
      "index": 73,
      "title": "Reptile: a scalable metalearning algorithm",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alex Nichol and John Schulman."
    },
    {
      "index": 74,
      "title": "A perspective view and survey of meta-learning",
      "abstract": "",
      "year": "2002",
      "venue": "Artificial intelligence review, 18(2):77–95",
      "authors": "Ricardo Vilalta and Youssef Drissi."
    },
    {
      "index": 75,
      "title": "Meta-Learning in Neural Networks: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.",
      "orig_title": "Meta-learning in neural networks: A survey",
      "paper_id": "2004.05439v2"
    },
    {
      "index": 76,
      "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "Artificial intelligence, 112(1-2):181–211",
      "authors": "Richard S Sutton, Doina Precup, and Satinder Singh."
    },
    {
      "index": 77,
      "title": "Solving hidden-semi-markov-mode markov decision problems",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Scalable Uncertainty\nManagement",
      "authors": "Emmanuel Hadoux, Aurélie Beynier, and Paul Weng."
    },
    {
      "index": 78,
      "title": "Non-stationary markov decision processes, a worst-case approach using model-based reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Erwan Lecarpentier and Emmanuel Rachelson."
    },
    {
      "index": 79,
      "title": "Dealing with Non-Stationarity in MARL via Trust-Region Decomposition",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, and Hongyuan Zha.",
      "orig_title": "Dealing with non-stationarity in marl via trust-region decomposition",
      "paper_id": "2102.10616v2"
    },
    {
      "index": 80,
      "title": "Reinforcement learning in non-stationary environments",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sindhu Padakandla, Shalabh Bhatnagar, et al."
    },
    {
      "index": 81,
      "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu.",
      "orig_title": "Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism",
      "paper_id": "2006.14389v1"
    },
    {
      "index": 82,
      "title": "A kernel-based approach to non-stationary reinforcement learning in metric spaces",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Omar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann,\nand Michal Valko."
    },
    {
      "index": 83,
      "title": "Efficient learning in non-stationary linear markov decision processes, 2020",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Ahmed Touati and Pascal Vincent."
    },
    {
      "index": 84,
      "title": "Curious model-building control systems",
      "abstract": "",
      "year": "1991",
      "venue": "Proc. international joint conference on neural networks",
      "authors": "Jürgen Schmidhuber."
    },
    {
      "index": 85,
      "title": "Intrinsically motivated reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "Advances in neural information processing systems",
      "authors": "Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh."
    },
    {
      "index": 86,
      "title": "Where do rewards come from",
      "abstract": "",
      "year": "2009",
      "venue": "Proceedings of the annual conference of the cognitive\nscience society",
      "authors": "Satinder Singh, Richard L Lewis, and Andrew G Barto."
    },
    {
      "index": 87,
      "title": "Intrinsic motivation and reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "Intrinsically motivated learning in natural and artificial\nsystems",
      "authors": "Andrew G Barto."
    },
    {
      "index": 88,
      "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Joshua Achiam and Shankar Sastry.",
      "orig_title": "Surprise-based intrinsic motivation for deep reinforcement learning",
      "paper_id": "1703.01732v1"
    },
    {
      "index": 89,
      "title": "Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem",
      "abstract": "",
      "year": "2013",
      "venue": "Frontiers in psychology, 4:313",
      "authors": "Jürgen Schmidhuber."
    },
    {
      "index": 90,
      "title": "Automated Curriculum Learning by Rewarding Temporally Rare Events",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE Conference on Computational Intelligence and Games\n(CIG)",
      "authors": "Niels Justesen and Sebastian Risi.",
      "orig_title": "Automated curriculum learning by rewarding temporally rare events",
      "paper_id": "1803.07131v2"
    },
    {
      "index": 91,
      "title": "Automated curriculum generation for Policy Gradients from Demonstrations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anirudh Srinivasan, Dzmitry Bahdanau, Maxime Chevalier-Boisvert, and Yoshua\nBengio.",
      "orig_title": "Automated curriculum generation for policy gradients from demonstrations",
      "paper_id": "1912.00444v1"
    },
    {
      "index": 92,
      "title": "Automatic curriculum learning for deep rl: A short survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, and\nPierre-Yves Oudeyer."
    },
    {
      "index": 93,
      "title": "Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley."
    },
    {
      "index": 94,
      "title": "Automatic data augmentation for generalization in reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob\nFergus."
    },
    {
      "index": 95,
      "title": "Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang, Jiaming Song,\nYu Wang, and Yi Wu.",
      "orig_title": "Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems",
      "paper_id": "2111.04613v2"
    },
    {
      "index": 96,
      "title": "Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Suyoung Lee and Sae-Young Chung.",
      "orig_title": "Improving generalization in meta-rl with imaginary tasks from latent dynamics mixture",
      "paper_id": "2105.13524v2"
    },
    {
      "index": 97,
      "title": "Automatic Goal Generation for Reinforcement Learning Agents",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.",
      "orig_title": "Automatic goal generation for reinforcement learning agents",
      "paper_id": "1705.06366v5"
    },
    {
      "index": 98,
      "title": "Automated curricula through setter-solver interactions",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sébastien Racaniere, Andrew K Lampinen, Adam Santoro, David P Reichert,\nVlad Firoiu, and Timothy P Lillicrap.",
      "orig_title": "Automated curricula through setter-solver interactions",
      "paper_id": "1909.12892v2"
    },
    {
      "index": 99,
      "title": "Autonomous Reinforcement Learning via Subgoal Curricula",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.",
      "orig_title": "Autonomous reinforcement learning via subgoal curricula",
      "paper_id": "2107.12931v2"
    },
    {
      "index": 100,
      "title": "Boosted curriculum reinforcement learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Pascal Klink, Carlo D’Eramo, Jan Peters, and Joni Pajarinen."
    },
    {
      "index": 101,
      "title": "Replay-Guided Adversarial Environment Design",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward\nGrefenstette, and Tim Rocktäschel.",
      "orig_title": "Replay-guided adversarial environment design",
      "paper_id": "2110.02439v2"
    },
    {
      "index": 102,
      "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur\nSzlam, and Rob Fergus.",
      "orig_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
      "paper_id": "1703.05407v5"
    },
    {
      "index": 103,
      "title": "Dream architecture: a developmental approach to open-ended learning in robotics",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Stephane Doncieux, Nicolas Bredeche, Léni Le Goff, Benoît Girard,\nAlexandre Coninx, Olivier Sigaud, Mehdi Khamassi, Natalia\nDíaz-Rodríguez, David Filliat, Timothy Hospedales, et al."
    },
    {
      "index": 104,
      "title": "Learning with Opponent-Learning Awareness",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 17th International Conference on\nAutonomous Agents and MultiAgent Systems, pages 122–130. International\nFoundation for Autonomous Agents and Multiagent Systems",
      "authors": "Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter\nAbbeel, and Igor Mordatch.",
      "orig_title": "Learning with opponent-learning awareness",
      "paper_id": "1709.04326v4"
    },
    {
      "index": 105,
      "title": "DiCE: The Infinitely Differentiable Monte Carlo Estimator",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktäschel,\nEric P Xing, and Shimon Whiteson.",
      "orig_title": "Dice: The infinitely differentiable monte-carlo estimator",
      "paper_id": "1802.05098v3"
    },
    {
      "index": 106,
      "title": "A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Dong Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun, Marwa Abdulhai,\nGolnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, and Jonathan How.",
      "orig_title": "A policy gradient algorithm for learning to learn in multiagent reinforcement learning",
      "paper_id": "2011.00382v5"
    },
    {
      "index": 107,
      "title": "Learning shared representations in multi-task reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Diana Borsa, Thore Graepel, and John Shawe-Taylor."
    },
    {
      "index": 108,
      "title": "Sparse multi-task reinforcement learning",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli."
    },
    {
      "index": 109,
      "title": "Progressive Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James\nKirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.",
      "orig_title": "Progressive neural networks",
      "paper_id": "1606.04671v4"
    },
    {
      "index": 110,
      "title": "The Beneﬁt of Multitask Representation Learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes."
    },
    {
      "index": 111,
      "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan\nPeters.",
      "orig_title": "Sharing knowledge in multi-task deep reinforcement learning",
      "paper_id": "2401.09561v1"
    },
    {
      "index": 112,
      "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Guanya Shi, Kamyar Azizzadenesheli, Michael O’Connell, Soon-Jo Chung, and\nYisong Yue.",
      "orig_title": "Meta-adaptive nonlinear control: Theory and algorithms",
      "paper_id": "2106.06098v3"
    },
    {
      "index": 113,
      "title": "Gradient Surgery for Multi-Task Learning",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n33:",
      "authors": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and\nChelsea Finn.",
      "orig_title": "Gradient surgery for multi-task learning",
      "paper_id": "2001.06782v4"
    },
    {
      "index": 114,
      "title": "Conflict-Averse Gradient Descent for Multi-task Learning",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.",
      "orig_title": "Conflict-averse gradient descent for multi-task learning",
      "paper_id": "2110.14048v2"
    },
    {
      "index": 115,
      "title": "Superposition of many models into one",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno\nOlshausen.",
      "orig_title": "Superposition of many models into one",
      "paper_id": "1902.05522v2"
    },
    {
      "index": 116,
      "title": "Supermasks in Superposition",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad\nRastegari, Jason Yosinski, and Ali Farhadi.",
      "orig_title": "Supermasks in superposition",
      "paper_id": "2006.14769v3"
    },
    {
      "index": 117,
      "title": "Model compression",
      "abstract": "",
      "year": "2006",
      "venue": "Proceedings of the 12th ACM SIGKDD international conference\non Knowledge discovery and data mining",
      "authors": "Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil."
    },
    {
      "index": 118,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 119,
      "title": "Policy Distillation",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,\nJames Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and\nRaia Hadsell.",
      "orig_title": "Policy distillation",
      "paper_id": "1511.06295v2"
    },
    {
      "index": 120,
      "title": "Learning without Forgetting",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Computer Vision, pages 614–629.\nSpringer",
      "authors": "Zhizhong Li and Derek Hoiem.",
      "orig_title": "Learning without forgetting",
      "paper_id": "1606.09282v3"
    },
    {
      "index": 121,
      "title": "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Matthew Riemer, Elham Khabiri, and Richard Goodwin.",
      "orig_title": "Representation stability as a regularizer for improved text analytics transfer learning",
      "paper_id": "1704.03617v1"
    },
    {
      "index": 122,
      "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom\nWard, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al.",
      "orig_title": "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
      "paper_id": "1802.01561v3"
    },
    {
      "index": 123,
      "title": "Progress & Compress: A scalable framework for continual learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka\nGrabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell.",
      "orig_title": "Progress & compress: A scalable framework for continual learning",
      "paper_id": "1805.06370v2"
    },
    {
      "index": 124,
      "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Glen Berseth, Cheng Xie, Paul Cernek, and Michiel Van de Panne.",
      "orig_title": "Progressive reinforcement learning with distillation for multi-skilled motion control",
      "paper_id": "1802.04765v1"
    },
    {
      "index": 125,
      "title": "Policy Consolidation for Continual Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christos Kaplanis, Murray Shanahan, and Claudia Clopath.",
      "orig_title": "Policy consolidation for continual reinforcement learning",
      "paper_id": "1902.00255v2"
    },
    {
      "index": 126,
      "title": "DisCoRL: Continual Reinforcement Learning via Policy Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun,\nGuanghang Cai, Natalia Díaz-Rodríguez, and David Filliat.",
      "orig_title": "Discorl: Continual reinforcement learning via policy distillation",
      "paper_id": "1907.05855v1"
    },
    {
      "index": 127,
      "title": "Exploiting hierarchy for learning and transfer in kl-regularized rl",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dhruva Tirumala, Hyeonwoo Noh, Alexandre Galashov, Leonard Hasenclever, Arun\nAhuja, Greg Wayne, Razvan Pascanu, Yee Whye Teh, and Nicolas Heess."
    },
    {
      "index": 128,
      "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "9th International Conference on Learning Representations",
      "authors": "Maximilian Igl, Gregory Farquhar, Jelena Luketina, JW Böhmer, and Shimon\nWhiteson.",
      "orig_title": "Transient non-stationarity and generalisation in deep reinforcement learning",
      "paper_id": "2006.05826v4"
    },
    {
      "index": 129,
      "title": "Memory-efficient reinforcement learning with knowledge consolidation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Qingfeng Lan, Yangchen Pan, Jun Luo, and A Rupam Mahmood."
    },
    {
      "index": 130,
      "title": "Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Tiantian Zhang, Xueqian Wang, Bin Liang, and Bo Yuan.",
      "orig_title": "Catastrophic interference in reinforcement learning: A solution based on context division and knowledge distillation",
      "paper_id": "2109.00525v2"
    },
    {
      "index": 131,
      "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "abstract": "",
      "year": "1992",
      "venue": "Machine learning, 8(3-4):293–321",
      "authors": "Long-Ji Lin."
    },
    {
      "index": 132,
      "title": "Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White.",
      "orig_title": "Organizing experience: a deeper look at replay mechanisms for sample-based planning in continuous state domains",
      "paper_id": "1806.04624v1"
    },
    {
      "index": 133,
      "title": "Dyna, an integrated architecture for learning, planning, and reacting",
      "abstract": "",
      "year": "1991",
      "venue": "ACM SIGART Bulletin, 2(4):160–163",
      "authors": "Richard S Sutton."
    },
    {
      "index": 134,
      "title": "Selective Experience Replay for Lifelong Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-second AAAI conference on artificial intelligence",
      "authors": "David Isele and Akansel Cosgun.",
      "orig_title": "Selective experience replay for lifelong learning",
      "paper_id": "1802.10269v1"
    },
    {
      "index": 135,
      "title": "Experience Replay for Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory\nWayne.",
      "orig_title": "Experience replay for continual learning",
      "paper_id": "1811.11682v2"
    },
    {
      "index": 136,
      "title": "Model-augmented prioritized experience replay",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Youngmin Oh, Jinwoo Shin, Eunho Yang, and Sung Ju Hwang."
    },
    {
      "index": 137,
      "title": "Posterior meta-replay for continual learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Christian Henning, Maria Cervera, Francesco D’Angelo, Johannes Von Oswald,\nRegina Traber, Benjamin Ehret, Seijin Kobayashi, Benjamin F Grewe, and\nJoão Sacramento."
    },
    {
      "index": 138,
      "title": "Regret minimization experience replay in off-policy reinforcement learning",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Xu-Hui Liu, Zhenghai Xue, Jingcheng Pang, Shengyi Jiang, Feng Xu, and Yang Yu."
    },
    {
      "index": 139,
      "title": "Generalized Proximal Policy Optimization with Sample Reuse",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "James Queeney, Yannis Paschalidis, and Christos G Cassandras.",
      "orig_title": "Generalized proximal policy optimization with sample reuse",
      "paper_id": "2111.00072v1"
    },
    {
      "index": 140,
      "title": "Universal Off-Policy Evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Yash Chandak, Scott Niekum, Bruno da Silva, Erik Learned-Miller, Emma\nBrunskill, and Philip S Thomas.",
      "orig_title": "Universal off-policy evaluation",
      "paper_id": "2104.12820v2"
    },
    {
      "index": 141,
      "title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Andrew Lampinen, Stephanie Chan, Andrea Banino, and Felix Hill.",
      "orig_title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
      "paper_id": "2105.14039v3"
    },
    {
      "index": 142,
      "title": "Policy Gradients Incorporating the Future",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "David Venuto, Elaine Lau, Doina Precup, and Ofir Nachum.",
      "orig_title": "Policy gradients incorporating the future",
      "paper_id": "2108.02096v2"
    },
    {
      "index": 143,
      "title": "Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Pierre Liotet, Francesco Vidaich, Alberto Maria Metelli, and Marcello Restelli.",
      "orig_title": "Lifelong hyper-policy optimization with multiple importance sampling regularization",
      "paper_id": "2112.06625v1"
    },
    {
      "index": 144,
      "title": "Catastrophic forgetting, rehearsal and pseudorehearsal",
      "abstract": "",
      "year": "1995",
      "venue": "Connection Science, 7(2):123–146",
      "authors": "Anthony Robins."
    },
    {
      "index": 145,
      "title": "Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Craig Atkinson, Brendan McCane, Lech Szymanski, and Anthony Robins.",
      "orig_title": "Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting",
      "paper_id": "1812.02464v6"
    },
    {
      "index": 146,
      "title": "Model-Free Generative Replay for Lifelong Reinforcement Learning: Application to Starcraft-2",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zachary Daniels, Aswin Raghavan, Jesse Hostetler, Abrar Rahman, Indranil Sur,\nMichael Piacentino, and Ajay Divakaran.",
      "orig_title": "Model-free generative replay for lifelong reinforcement learning: Application to starcraft-2",
      "paper_id": "2208.05056v2"
    },
    {
      "index": 147,
      "title": "Scalable Recollections for Continual Lifelong Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Matthew Riemer, Tim Klinger, Djallel Bouneffouf, and Michele Franceschini.",
      "orig_title": "Scalable recollections for continual lifelong learning",
      "paper_id": "1711.06761v4"
    },
    {
      "index": 148,
      "title": "Online learned continual compression with stacked quantization module",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and Joelle Pineau."
    },
    {
      "index": 149,
      "title": "Continual Reinforcement Learning with Complex Synapses",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christos Kaplanis, Murray Shanahan, and Claudia Clopath.",
      "orig_title": "Continual reinforcement learning with complex synapses",
      "paper_id": "1802.07239v2"
    },
    {
      "index": 150,
      "title": "Discovering structure in multiple learning tasks: The tc algorithm",
      "abstract": "",
      "year": "1996",
      "venue": "ICML, volume 96",
      "authors": "Sebastian Thrun and Joseph O’Sullivan."
    },
    {
      "index": 151,
      "title": "Doing more with less: meta-reasoning and meta-learning in humans and machines",
      "abstract": "",
      "year": "2019",
      "venue": "Current Opinion in Behavioral Sciences, 29:24–30",
      "authors": "Thomas L Griffiths, Frederick Callaway, Michael B Chang, Erin Grant, Paul M\nKrueger, and Falk Lieder."
    },
    {
      "index": 152,
      "title": "Reinforcement learning with hierarchies of machines",
      "abstract": "",
      "year": "1998",
      "venue": "Advances in neural information processing systems",
      "authors": "Ronald Parr and Stuart J Russell."
    },
    {
      "index": 153,
      "title": "Finding structure in reinforcement learning",
      "abstract": "",
      "year": "1995",
      "venue": "Advances in neural information processing systems",
      "authors": "Sebastian Thrun and Anton Schwartz."
    },
    {
      "index": 154,
      "title": "Transfer of learning by composing solutions of elemental sequential tasks",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning, 8(3-4):323–339",
      "authors": "Satinder Pal Singh."
    },
    {
      "index": 155,
      "title": "Multiple model-based reinforcement learning",
      "abstract": "",
      "year": "2002",
      "venue": "Neural computation, 14(6):1347–1369",
      "authors": "Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato."
    },
    {
      "index": 156,
      "title": "Learning modular neural network policies for multi-task and multi-robot transfer",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE International Conference on Robotics and\nAutomation (ICRA)",
      "authors": "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine."
    },
    {
      "index": 157,
      "title": "Meta Learning Shared Hierarchies",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman.",
      "orig_title": "Meta learning shared hierarchies",
      "paper_id": "1710.09767v1"
    },
    {
      "index": 158,
      "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,\nAndrei A Rusu, Alexander Pritzel, and Daan Wierstra.",
      "orig_title": "Pathnet: Evolution channels gradient descent in super neural networks",
      "paper_id": "1701.08734v1"
    },
    {
      "index": 159,
      "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Clemens Rosenbaum, Tim Klinger, and Matthew Riemer.",
      "orig_title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning",
      "paper_id": "1711.01239v2"
    },
    {
      "index": 160,
      "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Elliot Meyerson and Risto Miikkulainen.",
      "orig_title": "Beyond shared hierarchies: Deep multitask learning through soft layer ordering",
      "paper_id": "1711.00108v2"
    },
    {
      "index": 161,
      "title": "Modular Networks: Learning to Decompose Neural Computation",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 32nd International Conference on Neural\nInformation Processing Systems",
      "authors": "Louis Kirsch, Julius Kunze, and David Barber.",
      "orig_title": "Modular networks: learning to decompose neural computation",
      "paper_id": "1811.05249v1"
    },
    {
      "index": 162,
      "title": "Diversity and depth in per-example routing models",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Prajit Ramachandran and Quoc V Le."
    },
    {
      "index": 163,
      "title": "Automatically Composing Representation Transformations as a Means for Generalization",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Michael B Chang, Abhishek Gupta, Sergey Levine, and Thomas L Griffiths.",
      "orig_title": "Automatically composing representation transformations as a means for generalization",
      "paper_id": "1807.04640v2"
    },
    {
      "index": 164,
      "title": "Evolutionary Architecture Search For Deep Multitask Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the Genetic and Evolutionary Computation\nConference",
      "authors": "Jason Liang, Elliot Meyerson, and Risto Miikkulainen.",
      "orig_title": "Evolutionary architecture search for deep multitask networks",
      "paper_id": "1803.03745v2"
    },
    {
      "index": 165,
      "title": "Modular meta-learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ferran Alet, Tomás Lozano-Pérez, and Leslie P Kaelbling.",
      "orig_title": "Modular meta-learning",
      "paper_id": "1806.10166v2"
    },
    {
      "index": 166,
      "title": "Recursive routing networks: Learning to compose modules for language understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers)",
      "authors": "Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus Geiger, Tim Klinger,\nAlex Tamkin, Olivia Li, Sandhini Agarwal, Joshua D Greene, Dan Jurafsky,\net al."
    },
    {
      "index": 167,
      "title": "Multi-Task Reinforcement Learning with Soft Modularization",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang.",
      "orig_title": "Multi-task reinforcement learning with soft modularization",
      "paper_id": "2003.13661v2"
    },
    {
      "index": 168,
      "title": "Sharing less is more: Lifelong learning in deep networks with selective layer transfer",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Seungwon Lee, Sima Behpour, and Eric Eaton."
    },
    {
      "index": 169,
      "title": "Toward robust long range policy transfer",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35",
      "authors": "Wei-Cheng Tseng, Jin-Siang Lin, Yao-Min Feng, and Min Sun."
    },
    {
      "index": 170,
      "title": "How to reuse and compose knowledge for a lifetime of tasks: A survey on continual learning and functional composition",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jorge A Mendez and Eric Eaton."
    },
    {
      "index": 171,
      "title": "Modular lifelong reinforcement learning via neural composition",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jorge A Mendez, Harm van Seijen, and Eric Eaton."
    },
    {
      "index": 172,
      "title": "Building a subspace of policies for scalable continual learning",
      "abstract": "",
      "year": "2022",
      "venue": "Decision Awareness in Reinforcement Learning Workshop at\nICML",
      "authors": "Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer,\nand Roberta Raileanu."
    },
    {
      "index": 173,
      "title": "Neural Architecture Search with Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Barret Zoph and Quoc V Le.",
      "orig_title": "Neural architecture search with reinforcement learning",
      "paper_id": "1611.01578v2"
    },
    {
      "index": 174,
      "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.",
      "orig_title": "Smash: one-shot model architecture search through hypernetworks",
      "paper_id": "1708.05344v1"
    },
    {
      "index": 175,
      "title": "Continual and Multi-Task Architecture Search",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ramakanth Pasunuru and Mohit Bansal.",
      "orig_title": "Continual and multi-task architecture search",
      "paper_id": "1906.05226v1"
    },
    {
      "index": 176,
      "title": "Reinforced Continual Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ju Xu and Zhanxing Zhu.",
      "orig_title": "Reinforced continual learning",
      "paper_id": "1805.12369v1"
    },
    {
      "index": 177,
      "title": "Routing networks and the challenges of modular and compositional computation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger."
    },
    {
      "index": 178,
      "title": "Towards a unified theory of state abstraction for mdps",
      "abstract": "",
      "year": "2006",
      "venue": "ISAIM",
      "authors": "Lihong Li, Thomas J Walsh, and Michael L Littman."
    },
    {
      "index": 179,
      "title": "State abstractions for lifelong reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the International Conference on Machine\nLearning",
      "authors": "David Abel, Dilip Arumugam, Lucas Lehnert, and Michael L. Littman."
    },
    {
      "index": 180,
      "title": "Decoupling Dynamics and Reward for Transfer Learning",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Amy Zhang, Harsh Satija, and Joelle Pineau.",
      "orig_title": "Decoupling dynamics and reward for transfer learning",
      "paper_id": "1804.10689v2"
    },
    {
      "index": 181,
      "title": "Combined Reinforcement Learning via Abstract Representations",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33",
      "authors": "Vincent François-Lavet, Yoshua Bengio, Doina Precup, and Joelle Pineau.",
      "orig_title": "Combined reinforcement learning via abstract representations",
      "paper_id": "1809.04506v2"
    },
    {
      "index": 182,
      "title": "Learning Causal State Representations of Partially Observable Environments",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Amy Zhang, Zachary C Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima\nAnandkumar, Laurent Itti, Joelle Pineau, and Tommaso Furlanello.",
      "orig_title": "Learning causal state representations of partially observable environments",
      "paper_id": "1906.10437v2"
    },
    {
      "index": 183,
      "title": "Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick Yin, and Sergey\nLevine.",
      "orig_title": "Bisimulation makes analogies in goal-conditioned reinforcement learning",
      "paper_id": "2204.13060v3"
    },
    {
      "index": 184,
      "title": "Structural similarity for improved transfer in reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "C Chace Ashcraft, Benjamin Stoler, Chigozie Ewulum, and Susama Agarwala."
    },
    {
      "index": 185,
      "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Cameron Allen, Neev Parikh, Omer Gottesman, and George Konidaris.",
      "orig_title": "Learning markov state abstractions for deep reinforcement learning",
      "paper_id": "2106.04379v4"
    },
    {
      "index": 186,
      "title": "The Value Equivalence Principle for Model-Based Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems,\n33:",
      "authors": "Christopher Grimm, André Barreto, Satinder Singh, and David Silver.",
      "orig_title": "The value equivalence principle for model-based reinforcement learning",
      "paper_id": "2011.03506v1"
    },
    {
      "index": 187,
      "title": "Monte carlo tree search with iteratively refining state abstractions",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Samuel Sokota, Caleb Y Ho, Zaheen Ahmad, and J Zico Kolter."
    },
    {
      "index": 188,
      "title": "Control-Aware Representations for Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Brandon Cui, Yinlam Chow, and Mohammad Ghavamzadeh.",
      "orig_title": "Control-aware representations for model-based reinforcement learning",
      "paper_id": "2006.13408v1"
    },
    {
      "index": 189,
      "title": "Discovering state and action abstractions for generalized task and motion planning",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Aidan Curtis, Tom Silver, Joshua B Tenenbaum, Tomás Lozano-Pérez, and\nLeslie Kaelbling."
    },
    {
      "index": 190,
      "title": "Context-Specific Representation Abstraction for Deep Option Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Marwa Abdulhai, Dong Ki Kim, Matthew Riemer, Miao Liu, Gerald Tesauro, and\nJonathan P How.",
      "orig_title": "Context-specific representation abstraction for deep option learning",
      "paper_id": "2109.09876v2"
    },
    {
      "index": 191,
      "title": "Hierarchical solution of markov decision processes using macro-actions",
      "abstract": "",
      "year": "1998",
      "venue": "Proceedings of the Fourteenth conference on Uncertainty in\nartificial intelligence. Morgan Kaufmann Publishers Inc",
      "authors": "Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas Dean, and\nCraig Boutilier."
    },
    {
      "index": 192,
      "title": "The Option-Critic Architecture",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Pierre-Luc Bacon, Jean Harb, and Doina Precup.",
      "orig_title": "The option-critic architecture",
      "paper_id": "1609.05140v2"
    },
    {
      "index": 193,
      "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 34th International Conference on Machine\nLearning-Volume 70",
      "authors": "Marios C Machado, Marc G Bellemare, and Michael Bowling.",
      "orig_title": "A laplacian framework for option discovery in reinforcement learning",
      "paper_id": "1703.00956v2"
    },
    {
      "index": 194,
      "title": "Eigenoption Discovery through the Deep Successor Representation",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro,\nand Murray Campbell.",
      "orig_title": "Eigenoption discovery through the deep successor representation",
      "paper_id": "1710.11089v3"
    },
    {
      "index": 195,
      "title": "When waiting is not an option: Learning options with a deliberation cost",
      "abstract": "",
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
      "authors": "Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup."
    },
    {
      "index": 196,
      "title": "Options of Interest: Temporal Abstraction with Interest Functions",
      "abstract": "",
      "year": "2001",
      "venue": "arXiv preprint arXiv:",
      "authors": "Khimya Khetarpal, Martin Klissarov, Maxime Chevalier-Boisvert, Pierre-Luc\nBacon, and Doina Precup.",
      "orig_title": "Options of interest: Temporal abstraction with interest functions",
      "paper_id": "2001.00271v1"
    },
    {
      "index": 197,
      "title": "On the role of weight sharing during deep option learning",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34",
      "authors": "Matthew Riemer, Ignacio Cases, Clemens Rosenbaum, Miao Liu, and Gerald Tesauro."
    },
    {
      "index": 198,
      "title": "Flexible Option Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Martin Klissarov and Doina Precup.",
      "orig_title": "Flexible option learning",
      "paper_id": "2112.03097v1"
    },
    {
      "index": 199,
      "title": "Pac-inspired option discovery in lifelong reinforcement learning",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Emma Brunskill and Lihong Li."
    },
    {
      "index": 200,
      "title": "Variational Option Discovery Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel.",
      "orig_title": "Variational option discovery algorithms",
      "paper_id": "1807.10299v1"
    },
    {
      "index": 201,
      "title": "Learning Abstract Options",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Matthew Riemer, Miao Liu, and Gerald Tesauro.",
      "orig_title": "Learning abstract options",
      "paper_id": "1810.11583v4"
    },
    {
      "index": 202,
      "title": "Multitask soft option learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Maximilian Igl, Andrew Gambardella, Jinke He, Nantas Nardelli, N Siddharth,\nWendelin Böhmer, and Shimon Whiteson."
    },
    {
      "index": 203,
      "title": "Adaptive skills adaptive partitions (asap)",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Daniel J Mankowitz, Timothy A Mann, and Shie Mannor."
    },
    {
      "index": 204,
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.",
      "orig_title": "Diversity is all you need: Learning skills without a reward function",
      "paper_id": "1802.06070v6"
    },
    {
      "index": 205,
      "title": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Víctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier\nGiro-i Nieto, and Jordi Torres.",
      "orig_title": "Explore, discover and learn: Unsupervised discovery of state-covering skills",
      "paper_id": "2002.03647v4"
    },
    {
      "index": 206,
      "title": "A deep hierarchical approach to lifelong learning in minecraft",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor."
    },
    {
      "index": 207,
      "title": "Learning to Compose Skills",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles Isbell.",
      "orig_title": "Learning to compose skills",
      "paper_id": "1711.11289v1"
    },
    {
      "index": 208,
      "title": "The option keyboard: Combining skills in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "André Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygün,\nPhilippe Hamel, Daniel Toyama, Shibl Mourad, David Silver, Doina Precup,\net al."
    },
    {
      "index": 209,
      "title": "Reset-Free Lifelong Learning with Skill-Space Planning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.",
      "orig_title": "Reset-free lifelong learning with skill-space planning",
      "paper_id": "2012.03548v3"
    },
    {
      "index": 210,
      "title": "Reset-Free Lifelong Learning with Skill-Space Planning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS Workshop on Deep Reinforcement Learning",
      "authors": "Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.",
      "orig_title": "Reset-free lifelong learning with skill-space planning",
      "paper_id": "2012.03548v3"
    },
    {
      "index": 211,
      "title": "Toward good abstractions for lifelong learning",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS Workshop on Hierarchical Reinforcement Learning",
      "authors": "David Abel, Dilip Arumugam, Lucas Lehnert, and Michael L Littman."
    },
    {
      "index": 212,
      "title": "Value preserving state-action abstractions",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and\nStatistics",
      "authors": "David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam, Doina Precup,\nand Michael Littman."
    },
    {
      "index": 213,
      "title": "Reward-Free Exploration for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.",
      "orig_title": "Reward-free exploration for reinforcement learning",
      "paper_id": "2002.02794v1"
    },
    {
      "index": 214,
      "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "",
      "venue": "Advances in neural information processing systems,\n33:",
      "authors": "Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov.",
      "orig_title": "On reward-free reinforcement learning with linear function approximation",
      "paper_id": "2006.11274v1"
    },
    {
      "index": 215,
      "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Weitong Zhang, Dongruo Zhou, and Quanquan Gu.",
      "orig_title": "Reward-free model-based reinforcement learning with linear function approximation",
      "paper_id": "2110.06394v2"
    },
    {
      "index": 216,
      "title": "Variational Intrinsic Control",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra.",
      "orig_title": "Variational intrinsic control",
      "paper_id": "1611.07507v1"
    },
    {
      "index": 217,
      "title": "Learning an embedding space for transferable robot skills",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin\nRiedmiller."
    },
    {
      "index": 218,
      "title": "Learning One Representation to Optimize All Rewards",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:13–23",
      "authors": "Ahmed Touati and Yann Ollivier.",
      "orig_title": "Learning one representation to optimize all rewards",
      "paper_id": "2103.07945v3"
    },
    {
      "index": 219,
      "title": "Many-Goals Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Vivek Veeriah, Junhyuk Oh, and Satinder Singh.",
      "orig_title": "Many-goals reinforcement learning",
      "paper_id": "1806.09605v1"
    },
    {
      "index": 220,
      "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine\nLearning-Volume 70",
      "authors": "Jacob Andreas, Dan Klein, and Sergey Levine.",
      "orig_title": "Modular multitask reinforcement learning with policy sketches",
      "paper_id": "1611.01796v2"
    },
    {
      "index": 221,
      "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine\nLearning-Volume 70",
      "authors": "Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.",
      "orig_title": "Zero-shot task generalization with multi-task deep reinforcement learning",
      "paper_id": "1706.05064v2"
    },
    {
      "index": 222,
      "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianmin Shu, Caiming Xiong, and Richard Socher.",
      "orig_title": "Hierarchical and interpretable skill acquisition in multi-task reinforcement learning",
      "paper_id": "1712.07294v1"
    },
    {
      "index": 223,
      "title": "Dynamic dialogue policy transformer for continual reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christian Geishauser, Carel van Niekerk, Nurul Lubis, Michael Heck, Hsien-Chin\nLin, Shutong Feng, and Milica Gašić."
    },
    {
      "index": 224,
      "title": "Automatic Goal Generation for Reinforcement Learning Agents",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.",
      "orig_title": "Automatic goal generation for reinforcement learning agents",
      "paper_id": "1705.06366v5"
    },
    {
      "index": 225,
      "title": "Learning effective subgoals with multi-task hierarchical reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Dagui Chen, Qi Yan, Shangqi Guo, Zhile Yang, Xin Su, and Feng Chen."
    },
    {
      "index": 226,
      "title": "Universal value function approximators",
      "abstract": "",
      "year": "",
      "venue": "International conference on machine learning",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver."
    },
    {
      "index": 227,
      "title": "Unicorn: Continual learning with a universal, off-policy agent",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Daniel J Mankowitz, Augustin Žídek, André Barreto, Dan Horgan,\nMatteo Hessel, John Quan, Junhyuk Oh, Hado van Hasselt, David Silver, and Tom\nSchaul.",
      "orig_title": "Unicorn: Continual learning with a universal, off-policy agent",
      "paper_id": "1802.08294v2"
    },
    {
      "index": 228,
      "title": "Continual Reinforcement Learning with Diversity Exploration and Adversarial Self-Correction",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Fengda Zhu, Xiaojun Chang, Runhao Zeng, and Mingkui Tan.",
      "orig_title": "Continual reinforcement learning with diversity exploration and adversarial self-correction",
      "paper_id": "1906.09205v1"
    },
    {
      "index": 229,
      "title": "Hindsight experience replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,\nPeter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech\nZaremba."
    },
    {
      "index": 230,
      "title": "Hindsight policy gradients",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and Juergen Schmidhuber.",
      "orig_title": "Hindsight policy gradients",
      "paper_id": "1711.06006v3"
    },
    {
      "index": 231,
      "title": "Generalized Hindsight for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander C Li, Lerrel Pinto, and Pieter Abbeel.",
      "orig_title": "Generalized hindsight for reinforcement learning",
      "paper_id": "2002.11708v1"
    },
    {
      "index": 232,
      "title": "Goal-Aware Cross-Entropy for Multi-Target Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Kibeom Kim, Min Whoo Lee, Yoonsung Kim, JeHwan Ryu, Minsu Lee, and Byoung-Tak\nZhang.",
      "orig_title": "Goal-aware cross-entropy for multi-target reinforcement learning",
      "paper_id": "2110.12985v2"
    },
    {
      "index": 233,
      "title": "Goal-directed planning via hindsight experience replay",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Lorenzo Moro, Amarildo Likmeta, Enrico Prati, Marcello Restelli, et al."
    },
    {
      "index": 234,
      "title": "Learning Multi-Level Hierarchies with Hindsight",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko.",
      "orig_title": "Learning multi-level hierarchies with hindsight",
      "paper_id": "1712.00948v5"
    },
    {
      "index": 235,
      "title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Cédric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and\nPierre-Yves Oudeyer.",
      "orig_title": "Curious: intrinsically motivated modular multi-goal reinforcement learning",
      "paper_id": "1810.06284v4"
    },
    {
      "index": 236,
      "title": "Recurrent Reinforcement Learning: A Hybrid Approach",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and\nJi He.",
      "orig_title": "Recurrent reinforcement learning: a hybrid approach",
      "paper_id": "1509.03044v2"
    },
    {
      "index": 237,
      "title": "Playing FPS Games with Deep Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
      "authors": "Guillaume Lample and Devendra Singh Chaplot.",
      "orig_title": "Playing fps games with deep reinforcement learning",
      "paper_id": "1609.05521v2"
    },
    {
      "index": 238,
      "title": "Learning to Navigate in Complex Environments",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard,\nAndrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu,\net al.",
      "orig_title": "Learning to navigate in complex environments",
      "paper_id": "1611.03673v3"
    },
    {
      "index": 239,
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z\nLeibo, David Silver, and Koray Kavukcuoglu.",
      "orig_title": "Reinforcement learning with unsupervised auxiliary tasks",
      "paper_id": "1611.05397v1"
    },
    {
      "index": 240,
      "title": "Multi-task Deep Reinforcement Learning with PopArt",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33",
      "authors": "Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,\nand Hado van Hasselt.",
      "orig_title": "Multi-task deep reinforcement learning with popart",
      "paper_id": "1809.04474v1"
    },
    {
      "index": 241,
      "title": "Loss is its own reward: Self-supervision for reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell."
    },
    {
      "index": 242,
      "title": "Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-bastien Grill, Florent\nAltché, Rémi Munos, and Mohammad Gheshlaghi Azar.",
      "orig_title": "Bootstrap latent-predictive representations for multitask reinforcement learning",
      "paper_id": "2004.14646v1"
    },
    {
      "index": 243,
      "title": "Discovery of Useful Questions as Auxiliary Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L\nLewis, Junhyuk Oh, Hado P van Hasselt, David Silver, and Satinder Singh.",
      "orig_title": "Discovery of useful questions as auxiliary tasks",
      "paper_id": "1909.04607v1"
    },
    {
      "index": 244,
      "title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Richard S Sutton, Joseph Modayil, Michael Delp Thomas Degris, Patrick M\nPilarski, and Adam White."
    },
    {
      "index": 245,
      "title": "A Geometric Perspective on Optimal Representations for Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel\nCastro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle.",
      "orig_title": "A geometric perspective on optimal representations for reinforcement learning",
      "paper_id": "1901.11530v2"
    },
    {
      "index": 246,
      "title": "Fast Context Adaptation via Meta-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon\nWhiteson.",
      "orig_title": "Fast context adaptation via meta-learning",
      "paper_id": "1810.03642v4"
    },
    {
      "index": 247,
      "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine.",
      "orig_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
      "paper_id": "1903.08254v1"
    },
    {
      "index": 248,
      "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin\nGal, Katja Hofmann, and Shimon Whiteson.",
      "orig_title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning",
      "paper_id": "1910.08348v2"
    },
    {
      "index": 249,
      "title": "Meta reinforcement learning as task inference",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye\nTeh, and Nicolas Heess."
    },
    {
      "index": 250,
      "title": "Pratik Chaudhari",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola."
    },
    {
      "index": 251,
      "title": "Meta-learning of sequential strategies",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Pedro A Ortega, Jane X Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson,\nRazvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann,\net al."
    },
    {
      "index": 252,
      "title": "Generalized Hidden Parameter MDPs: Transferable Model-based RL in a Handful of Trials",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christian F Perez, Felipe Petroski Such, and Theofanis Karaletsos.",
      "orig_title": "Generalized hidden parameter mdps transferable model-based rl in a handful of trials",
      "paper_id": "2002.03072v1"
    },
    {
      "index": 253,
      "title": "Offline meta reinforcement learning–identifiability challenges and effective data collection strategies",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Ron Dorfman, Idan Shenfeld, and Aviv Tamar."
    },
    {
      "index": 254,
      "title": "Risk-Averse Bayes-Adaptive Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems,\n34:",
      "authors": "Marc Rigter, Bruno Lacerda, and Nick Hawes.",
      "orig_title": "Risk-averse bayes-adaptive reinforcement learning",
      "paper_id": "2102.05762v2"
    },
    {
      "index": 255,
      "title": "Multi-task reinforcement learning with context-based representations",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Shagun Sodhani, Amy Zhang, and Joelle Pineau."
    },
    {
      "index": 256,
      "title": "Towards effective context for meta-reinforcement learning: an approach based on contrastive learning",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35",
      "authors": "Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and\nWulong Liu."
    },
    {
      "index": 257,
      "title": "Dealing with non-stationary environments using context detection",
      "abstract": "",
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine\nlearning",
      "authors": "Bruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and Paulo M Engel."
    },
    {
      "index": 258,
      "title": "Scalable bayesian reinforcement learning for multiagent pomdps",
      "abstract": "",
      "year": "2013",
      "venue": "Citeseer",
      "authors": "Christopher Amato, Frans A Oliehoek, and Eric Shyu."
    },
    {
      "index": 259,
      "title": "Prediction-based multi-agent reinforcement learning in inherently non-stationary environments",
      "abstract": "",
      "year": "2017",
      "venue": "ACM Transactions on Autonomous and Adaptive Systems (TAAS),\n12(2):1–23",
      "authors": "Andrei Marinescu, Ivana Dusparic, and Siobhán Clarke."
    },
    {
      "index": 260,
      "title": "Options as responses: Grounding behavioural hierarchies in multi-agent rl",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander Sasha Vezhnevets, Yuhuai Wu, Remi Leblond, and Joel Leibo."
    },
    {
      "index": 261,
      "title": "Policy Evaluation Networks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon.",
      "orig_title": "Policy evaluation networks",
      "paper_id": "2002.11833v1"
    },
    {
      "index": 262,
      "title": "Fast adaptation via policy-dynamics value functions",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus."
    },
    {
      "index": 263,
      "title": "A multitask representation using reusable local policy templates",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Benjamin Saul Rosman and Subramanian Ramamoorthy."
    },
    {
      "index": 264,
      "title": "Sequential decision-making under non-stationary environments via sequential change-point detection",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Emmanuel Hadoux, Aurélie Beynier, and Paul Weng."
    },
    {
      "index": 265,
      "title": "Context-Aware Policy Reuse",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 18th International Conference on\nAutonomous Agents and MultiAgent Systems, pages 989–997. International\nFoundation for Autonomous Agents and Multiagent Systems",
      "authors": "Siyuan Li, Fangda Gu, Guangxiang Zhu, and Chongjie Zhang.",
      "orig_title": "Context-aware policy reuse",
      "paper_id": "1806.03793v4"
    },
    {
      "index": 266,
      "title": "Same State, Different Task: Continual Reinforcement Learning without Interference",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Samuel Kessler, Jack Parker-Holder, Philip Ball, Stefan Zohren, and Stephen J\nRoberts.",
      "orig_title": "Same state, different task: Continual reinforcement learning without interference",
      "paper_id": "2106.02940v2"
    },
    {
      "index": 267,
      "title": "Adapt to environment sudden changes by learning a context sensitive policy",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, Virtual Event",
      "authors": "Fan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, and Yi-Feng Zhang."
    },
    {
      "index": 268,
      "title": "An exploration strategy for non-stationary opponents",
      "abstract": "",
      "year": "2017",
      "venue": "Autonomous Agents and Multi-Agent Systems, 31(5):971–",
      "authors": "Pablo Hernandez-Leal, Yusen Zhan, Matthew E Taylor, L Enrique Sucar, and\nEnrique Munoz de Cote."
    },
    {
      "index": 269,
      "title": "On q-learning convergence for non-markov decision processes",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "Sultan Javed Majeed and Marcus Hutter."
    },
    {
      "index": 270,
      "title": "Bayesian decision problems and Markov chains",
      "abstract": "",
      "year": "1967",
      "venue": "Wiley",
      "authors": "James John Martin."
    },
    {
      "index": 271,
      "title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
      "abstract": "",
      "year": "2002",
      "venue": "PhD thesis, University of Massachusetts at Amherst",
      "authors": "Michael O’Gordon Duff."
    },
    {
      "index": 272,
      "title": "Reinforcement learning with self-modifying policies",
      "abstract": "",
      "year": "1998",
      "venue": "Learning to learn",
      "authors": "Jürgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph."
    },
    {
      "index": 273,
      "title": "Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement",
      "abstract": "",
      "year": "1997",
      "venue": "Machine Learning, 28(1):105–130",
      "authors": "Jürgen Schmidhuber, Jieyu Zhao, and Marco Wiering."
    },
    {
      "index": 274,
      "title": "A general method for incremental self-improvement and multi-agent learning",
      "abstract": "",
      "year": "1999",
      "venue": "Evolutionary Computation: Theory and Applications",
      "authors": "Juergen Schmidhuber."
    },
    {
      "index": 275,
      "title": "Learning a synaptic learning rule",
      "abstract": "",
      "year": "1990",
      "venue": "Citeseer",
      "authors": "Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier."
    },
    {
      "index": 276,
      "title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks",
      "abstract": "",
      "year": "1992",
      "venue": "Neural Computation, 4(1):131–139",
      "authors": "Jürgen Schmidhuber."
    },
    {
      "index": 277,
      "title": "Meta-Learning Representations for Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "URL\nhttp://papers.nips.cc/paper/",
      "authors": "Khurram Javed and Martha White.",
      "orig_title": "Meta-learning representations for continual learning",
      "paper_id": "1905.12588v2"
    },
    {
      "index": 278,
      "title": "Meta-learnt priors slow down catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Giacomo Spigler."
    },
    {
      "index": 279,
      "title": "Learning to Continually Learn",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff\nClune, and Nick Cheney.",
      "orig_title": "Learning to continually learn",
      "paper_id": "2002.09571v2"
    },
    {
      "index": 280,
      "title": "Online fast adaptation and knowledge accumulation: a new approach to continual learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin,\nLucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez,\net al."
    },
    {
      "index": 281,
      "title": "Accelerating online reinforcement learning via model-based meta-learning",
      "abstract": "",
      "year": "2021",
      "venue": "Learning to Learn-Workshop at ICLR 2021",
      "authors": "John D Co-Reyes, Sarah Feng, Glen Berseth, Jie Qui, and Sergey Levine."
    },
    {
      "index": 282,
      "title": "Bayesian Model-Agnostic Meta-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin\nAhn.",
      "orig_title": "Bayesian model-agnostic meta-learning",
      "paper_id": "1806.03836v4"
    },
    {
      "index": 283,
      "title": "ProMP: Proximal Meta-Policy Search",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel.",
      "orig_title": "Promp: Proximal meta-policy search",
      "paper_id": "1810.06784v4"
    },
    {
      "index": 284,
      "title": "Transferring Knowledge across Learning Processes",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sebastian Flennerhag, Pablo G Moreno, Neil D Lawrence, and Andreas Damianou.",
      "orig_title": "Transferring knowledge across learning processes",
      "paper_id": "1812.01054v3"
    },
    {
      "index": 285,
      "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anusha Nagabandi, Chelsea Finn, and Sergey Levine.",
      "orig_title": "Deep online learning via meta-learning: Continual adaptation for model-based rl",
      "paper_id": "1812.07671v2"
    },
    {
      "index": 286,
      "title": "Guided Meta-Policy Search",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine,\nand Chelsea Finn.",
      "orig_title": "Guided meta-policy search",
      "paper_id": "1904.00956v2"
    },
    {
      "index": 287,
      "title": "Online meta-learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine."
    },
    {
      "index": 288,
      "title": "Model-based adversarial meta-reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma."
    },
    {
      "index": 289,
      "title": "Comps: Continual meta policy search",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn, and Sergey Levine."
    },
    {
      "index": 290,
      "title": "Evolving Reinforcement Learning Algorithms",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "John D Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V\nLe, Honglak Lee, and Aleksandra Faust.",
      "orig_title": "Evolving reinforcement learning algorithms",
      "paper_id": "2101.03958v6"
    },
    {
      "index": 291,
      "title": "Introducing Symmetries to Black Box Meta Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36",
      "authors": "Louis Kirsch, Sebastian Flennerhag, Hado van Hasselt, Abram Friesen, Junhyuk\nOh, and Yutian Chen.",
      "orig_title": "Introducing symmetries to black box meta reinforcement learning",
      "paper_id": "2109.10781v2"
    },
    {
      "index": 292,
      "title": "Hindsight Foresight Relabeling for Meta-Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Michael Wan, Jian Peng, and Tanmay Gangwani.",
      "orig_title": "Hindsight foresight relabeling for meta-reinforcement learning",
      "paper_id": "2109.09031v2"
    },
    {
      "index": 293,
      "title": "Transformers are Meta-Reinforcement Learners",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Luckeciano C Melo.",
      "orig_title": "Transformers are meta-reinforcement learners",
      "paper_id": "2206.06614v1"
    },
    {
      "index": 294,
      "title": "Skill-based Meta-Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Taewook Nam, Shao-Hua Sun, Karl Pertsch, Sung Ju Hwang, and Joseph J Lim.",
      "orig_title": "Skill-based meta-reinforcement learning",
      "paper_id": "2204.11828v1"
    },
    {
      "index": 295,
      "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch,\nand Pieter Abbeel.",
      "orig_title": "Continuous adaptation via meta-learning in nonstationary and competitive environments",
      "paper_id": "1710.03641v2"
    },
    {
      "index": 296,
      "title": "Learning to Learn: Meta-Critic Networks for Sample Efficient Learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang.",
      "orig_title": "Learning to learn: Meta-critic networks for sample efficient learning",
      "paper_id": "1706.09529v1"
    },
    {
      "index": 297,
      "title": "Learning to learn: Hierarchical meta-critic networks",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access, 7:",
      "authors": "Zhixiong Xu, Lei Cao, and Xiliang Chen."
    },
    {
      "index": 298,
      "title": "Online Meta-Critic Learning for Off-Policy Actor-Critic Methods",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, and Timothy M Hospedales.",
      "orig_title": "Online meta-critic learning for off-policy actor-critic methods",
      "paper_id": "2003.05334v2"
    },
    {
      "index": 299,
      "title": "Bootstrapped meta-learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David\nSilver, and Satinder Singh."
    },
    {
      "index": 300,
      "title": "Optimizing for the Future in Non-Stationary MDPs",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yash Chandak, Georgios Theocharous, Shiv Shankar, Sridhar Mahadevan, Martha\nWhite, and Philip S Thomas.",
      "orig_title": "Optimizing for the future in non-stationary mdps",
      "paper_id": "2005.08158v4"
    },
    {
      "index": 301,
      "title": "Adapting bias by gradient descent: an incremental version of the delta-bar-delta",
      "abstract": "",
      "year": "1992",
      "venue": "Tenth National Conference on Artificial Intelligence. MIT\nPress",
      "authors": "Rich Sutton."
    },
    {
      "index": 302,
      "title": "Meta-Gradient Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhongwen Xu, Hado P van Hasselt, and David Silver.",
      "orig_title": "Meta-gradient reinforcement learning",
      "paper_id": "1805.09801v1"
    },
    {
      "index": 303,
      "title": "Self-tuning deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado van\nHasselt, David Silver, and Satinder Singh."
    },
    {
      "index": 304,
      "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, 33",
      "authors": "Zhongwen Xu, Hado P van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and\nDavid Silver.",
      "orig_title": "Meta-gradient reinforcement learning with an objective discovered online",
      "paper_id": "2007.08433v1"
    },
    {
      "index": 305,
      "title": "R-iac: Robust intrinsically motivated exploration and active learning",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE Transactions on Autonomous Mental Development, 1(3):155–169",
      "authors": "Adrien Baranes and Pierre-Yves Oudeyer."
    },
    {
      "index": 306,
      "title": "On Learning Intrinsic Rewards for Policy Gradient Methods",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zeyu Zheng, Junhyuk Oh, and Satinder Singh.",
      "orig_title": "On learning intrinsic rewards for policy gradient methods",
      "paper_id": "1804.06459v2"
    },
    {
      "index": 307,
      "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter\nAbbeel, and Ilya Sutskever.",
      "orig_title": "Some considerations on learning to explore via meta-reinforcement learning",
      "paper_id": "1803.01118v2"
    },
    {
      "index": 308,
      "title": "Learning to Explore with Meta-Policy Gradient",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng.",
      "orig_title": "Learning to explore with meta-policy gradient",
      "paper_id": "1803.05044v2"
    },
    {
      "index": 309,
      "title": "Evolved policy gradients",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski,\nOpenAI Jonathan Ho, and Pieter Abbeel."
    },
    {
      "index": 310,
      "title": "NoRML: No-Reward Meta Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 18th International Conference on\nAutonomous Agents and MultiAgent Systems, pages 323–331. International\nFoundation for Autonomous Agents and Multiagent Systems",
      "authors": "Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Jie Tan, and Chelsea Finn.",
      "orig_title": "Norml: No-reward meta learning",
      "paper_id": "1903.01063v1"
    },
    {
      "index": 311,
      "title": "Reward Shaping via Meta-Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu.",
      "orig_title": "Reward shaping via meta-learning",
      "paper_id": "1901.09330v1"
    },
    {
      "index": 312,
      "title": "What Can Learned Intrinsic Rewards Capture?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van\nHasselt, David Silver, and Satinder Singh.",
      "orig_title": "What can learned intrinsic rewards capture?",
      "paper_id": "1912.05500v3"
    },
    {
      "index": 313,
      "title": "Making the world differentiable: On using self-supervised fully recurrent n eu al networks for dynamic reinforcement learning and planning in non-stationary environm nts",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Jiirgen Schmidhuber."
    },
    {
      "index": 314,
      "title": "Empowerment: A universal agent-centric measure of control",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv."
    },
    {
      "index": 315,
      "title": "Curiosity-driven development",
      "abstract": "",
      "year": "2006",
      "venue": "Proceedings of the International Workshop on Synergistic\nIntelligence Dynamics. Citeseer",
      "authors": "Frederic Kaplan and Pierre-Yves Oudeyer."
    },
    {
      "index": 316,
      "title": "Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes",
      "abstract": "",
      "year": "2008",
      "venue": "Workshop on anticipatory behavior in adaptive learning\nsystems",
      "authors": "Jürgen Schmidhuber."
    },
    {
      "index": 317,
      "title": "Curiosity driven reinforcement learning for motion planning on humanoids",
      "abstract": "",
      "year": "2014",
      "venue": "Frontiers in neurorobotics, 7:25",
      "authors": "Mikhail Frank, Jürgen Leitner, Marijn Stollenga, Alexander Förster, and\nJürgen Schmidhuber."
    },
    {
      "index": 318,
      "title": "Variational information maximisation for intrinsically motivated reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Shakir Mohamed and Danilo Jimenez Rezende."
    },
    {
      "index": 319,
      "title": "Unifying count-based exploration and intrinsic motivation",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in neural information processing systems",
      "authors": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\nand Remi Munos."
    },
    {
      "index": 320,
      "title": "Vime: Variational information maximizing exploration",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter\nAbbeel."
    },
    {
      "index": 321,
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops",
      "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell.",
      "orig_title": "Curiosity-driven exploration by self-supervised prediction",
      "paper_id": "1705.05363v1"
    },
    {
      "index": 322,
      "title": "Unsupervised real-time control through variational empowerment",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Maximilian Karl, Maximilian Soelch, Philip Becker-Ehmck, Djalel Benbouzid,\nPatrick van der Smagt, and Justin Bayer."
    },
    {
      "index": 323,
      "title": "Large-Scale Study of Curiosity-Driven Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and\nAlexei A Efros.",
      "orig_title": "Large-scale study of curiosity-driven learning",
      "paper_id": "1808.04355v1"
    },
    {
      "index": 324,
      "title": "Model-Based Active Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Pranav Shyam, Wojciech Jaśkowski, and Faustino Gomez.",
      "orig_title": "Model-based active exploration",
      "paper_id": "1810.12162v5"
    },
    {
      "index": 325,
      "title": "Competitive experience replay",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hao Liu, Alexander Trott, Richard Socher, and Caiming Xiong.",
      "orig_title": "Competitive experience replay",
      "paper_id": "1902.00528v4"
    },
    {
      "index": 326,
      "title": "Go-explore: a new approach for hard-exploration problems",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune."
    },
    {
      "index": 327,
      "title": "Planning to explore via self-supervised world models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner,\nand Deepak Pathak."
    },
    {
      "index": 328,
      "title": "Reactive Exploration to Cope with Non-Stationarity in \\Lifelong Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christian Steinparz, Thomas Schmied, Fabian Paischer, Marius-Constantin Dinu,\nVihang Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter.",
      "orig_title": "Reactive exploration to cope with non-stationarity in lifelong reinforcement learning",
      "paper_id": "2207.05742v2"
    },
    {
      "index": 329,
      "title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Glen Berseth, Daniel Geng, Coline Manon Devin, Nicholas Rhinehart, Chelsea\nFinn, Dinesh Jayaraman, and Sergey Levine.",
      "orig_title": "Smirl: Surprise minimizing reinforcement learning in unstable environments",
      "paper_id": "1912.05510v4"
    },
    {
      "index": 330,
      "title": "Learning to Multi-Task by Active Sampling",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sahil Sharma, Ashutosh Jha, Parikshit Hegde, and Balaraman Ravindran.",
      "orig_title": "Learning to multi-task by active sampling",
      "paper_id": "1702.06053v4"
    },
    {
      "index": 331,
      "title": "Interactive Agent Modeling by Learning to Probe",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianmin Shu, Caiming Xiong, Ying Nian Wu, and Song-Chun Zhu.",
      "orig_title": "Interactive agent modeling by learning to probe",
      "paper_id": "1810.00510v1"
    },
    {
      "index": 332,
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research, 47:253–279",
      "authors": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling."
    },
    {
      "index": 333,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,\nJie Tang, and Wojciech Zaremba."
    },
    {
      "index": 334,
      "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play",
      "abstract": "",
      "year": "2018",
      "venue": "Science, 362(",
      "authors": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, et al."
    },
    {
      "index": 335,
      "title": "The barbados 2018 list of open issues in continual learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tom Schaul, Hado van Hasselt, Joseph Modayil, Martha White, Adam White,\nPierre-Luc Bacon, Jean Harb, Shibl Mourad, Marc Bellemare, and Doina Precup."
    },
    {
      "index": 336,
      "title": "Hierarchical reinforcement learning with the maxq value function decomposition",
      "abstract": "",
      "year": "2000",
      "venue": "Journal of artificial intelligence research, 13:227–303",
      "authors": "Thomas G Dietterich."
    },
    {
      "index": 337,
      "title": "DeepMind Lab",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:",
      "authors": "Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright,\nHeinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés,\nAmir Sadik, et al.",
      "orig_title": "Deepmind lab",
      "paper_id": "1612.03801v2"
    },
    {
      "index": 338,
      "title": "Minecraft, beyond construction and survival",
      "abstract": "",
      "year": "2011",
      "venue": "Well Played: a journal on video games, value and meaning,\n1(1):1–22",
      "authors": "Sean C Duncan."
    },
    {
      "index": 339,
      "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning",
      "abstract": "",
      "year": "2016",
      "venue": "Computational Intelligence and Games (CIG), 2016 IEEE\nConference on",
      "authors": "Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech\nJaśkowski.",
      "orig_title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning",
      "paper_id": "1605.02097v2"
    },
    {
      "index": 340,
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research, 61:523–562",
      "authors": "Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew\nHausknecht, and Michael Bowling.",
      "orig_title": "Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents",
      "paper_id": "1709.06009v2"
    },
    {
      "index": 341,
      "title": "Benchmark environments for multitask learning in continuous domains",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen, David Meger,\nand Gregory Dudek."
    },
    {
      "index": 342,
      "title": "Quantifying Generalization in Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman.",
      "orig_title": "Quantifying generalization in reinforcement learning",
      "paper_id": "1812.02341v3"
    },
    {
      "index": 343,
      "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
      "abstract": "",
      "year": "1912",
      "venue": "arXiv preprint arXiv:",
      "authors": "Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.",
      "orig_title": "Leveraging procedural generation to benchmark reinforcement learning",
      "paper_id": "1912.01588v2"
    },
    {
      "index": 344,
      "title": "Behaviour suite for reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre\nSaraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh,\net al."
    },
    {
      "index": 345,
      "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "Conference on Robot Learning, pages 1094–1100",
      "authors": "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea\nFinn, and Sergey Levine.",
      "orig_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
      "paper_id": "1910.10897v2"
    },
    {
      "index": 346,
      "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ossama Ahmed, Frederik Träuble, Anirudh Goyal, Alexander Neitz, Manuel\nWüthrich, Yoshua Bengio, Bernhard Schölkopf, and Stefan Bauer.",
      "orig_title": "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning",
      "paper_id": "2010.04296v2"
    },
    {
      "index": 347,
      "title": "Embodiment theory and education: The foundations of cognition in perception and action",
      "abstract": "",
      "year": "2012",
      "venue": "Trends in Neuroscience and Education, 1(1):15–20",
      "authors": "Markus Kiefer and Natalie M Trumpp."
    },
    {
      "index": 348,
      "title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR, abs/",
      "authors": "Douwe Kiela, Luana Bulat, Anita L. Vero, and Stephen Clark."
    },
    {
      "index": 349,
      "title": "Ingredients of intelligence: From classic debates to an engineering roadmap",
      "abstract": "",
      "year": "2017",
      "venue": "Behavioral and Brain Sciences, 40",
      "authors": "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman."
    },
    {
      "index": 350,
      "title": "Experience grounds language",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce\nChai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich,\net al."
    },
    {
      "index": 351,
      "title": "Environments for Lifelong Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Khimya Khetarpal, Shagun Sodhani, Sarath Chandar, and Doina Precup.",
      "orig_title": "Environments for lifelong reinforcement learning",
      "paper_id": "1811.10732v2"
    },
    {
      "index": 352,
      "title": "Jelly bean world: A testbed for never-ending learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Emmanouil Antonios Platanios, Abulhair Saparov, and Tom Mitchell."
    },
    {
      "index": 353,
      "title": "Continuous Coordination As a Realistic Scenario for Lifelong Learning",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Hadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville, and Sarath Chandar.",
      "orig_title": "Continuous coordination as a realistic scenario for lifelong learning",
      "paper_id": "2103.03216v2"
    },
    {
      "index": 354,
      "title": "CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta.",
      "orig_title": "Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents",
      "paper_id": "2110.10067v2"
    },
    {
      "index": 355,
      "title": "L2Explorer: A Lifelong Reinforcement Learning Assessment Environment",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Erik C Johnson, Eric Q Nguyen, Blake Schreurs, Chigozie S Ewulum, Chace\nAshcraft, Neil M Fendley, Megan M Baker, Alexander New, and Gautam K\nVallabha.",
      "orig_title": "L2explorer: A lifelong reinforcement learning assessment environment",
      "paper_id": "2203.07454v1"
    },
    {
      "index": 356,
      "title": "Novelgridworlds: A benchmark environment for detecting and adapting to novelties in open worlds",
      "abstract": "",
      "year": "2021",
      "venue": "International Foundation for Autonomous Agents and Multiagent\nSystems, AAMAS",
      "authors": "Shivam Goel, Gyan Tatiya, Matthias Scheutz, and Jivko Sinapov."
    },
    {
      "index": 357,
      "title": "Towards safe policy improvement for non-stationary mdps, 2020b",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Yash Chandak, Scott M. Jordan, Georgios Theocharous, Martha White, and\nPhilip S. Thomas."
    },
    {
      "index": 358,
      "title": "Deep Reinforcement Learning that Matters",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,\nand David Meger.",
      "orig_title": "Deep reinforcement learning that matters",
      "paper_id": "1709.06560v3"
    },
    {
      "index": 359,
      "title": "Re-evaluate: Reproducibility in evaluating reinforcement learning algorithms",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Khimya Khetarpal, Zafarali Ahmed, Andre Cianflone, Riashat Islam, and Joelle\nPineau."
    },
    {
      "index": 360,
      "title": "A note on measurement of utility",
      "abstract": "",
      "year": "1937",
      "venue": "The review of economic studies, 4(2):155–161",
      "authors": "Paul A Samuelson."
    },
    {
      "index": 361,
      "title": "Neuroscience-inspired artificial intelligence",
      "abstract": "",
      "year": "2017",
      "venue": "Neuron, 95(2):245 – 258",
      "authors": "Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew\nBotvinick."
    },
    {
      "index": 362,
      "title": "Reinforcement learning in the brain",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Mathematical Psychology, 53(3):139–154",
      "authors": "Yael Niv."
    },
    {
      "index": 363,
      "title": "Optimizing Agent Behavior over Long Time Scales by Transporting Value",
      "abstract": "",
      "year": "2019",
      "venue": "Nature communications, 10(1):1–12",
      "authors": "Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico\nCarnevale, Arun Ahuja, and Greg Wayne.",
      "orig_title": "Optimizing agent behavior over long time scales by transporting value",
      "paper_id": "1810.06721v2"
    },
    {
      "index": 364,
      "title": "Moderate levels of activation lead to forgetting in the think/no-think paradigm",
      "abstract": "",
      "year": "2013",
      "venue": "Neuropsychologia, 51(12):",
      "authors": "Greg J Detre, Annamalai Natarajan, Samuel J Gershman, and Kenneth A Norman."
    },
    {
      "index": 365,
      "title": "Sebastian Musslick",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yotam Sagiv, Sebastian Musslick, Yael Niv, and Jonathan D Cohen."
    },
    {
      "index": 366,
      "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
      "abstract": "",
      "year": "1995",
      "venue": "Psychological review, 102(3):419",
      "authors": "James L McClelland, Bruce L McNaughton, and Randall C O’Reilly."
    },
    {
      "index": 367,
      "title": "A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits",
      "abstract": "",
      "year": "2013",
      "venue": "Proceedings of the National Academy of Sciences, 110(52):E",
      "authors": "Robert Ajemian, Alessandro D’Ausilio, Helene Moorman, and Emilio Bizzi."
    },
    {
      "index": 368,
      "title": "Continual Backprop: Stochastic Gradient Descent with Persistent Randomness",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shibhansh Dohare, A Rupam Mahmood, and Richard S Sutton.",
      "orig_title": "Continual backprop: Stochastic gradient descent with persistent randomness",
      "paper_id": "2108.06325v3"
    },
    {
      "index": 369,
      "title": "Behavioral considerations suggest an average reward td model of the dopamine system",
      "abstract": "",
      "year": "2000",
      "venue": "Neurocomputing, 32:679–684",
      "authors": "Nathaniel D Daw and David S Touretzky."
    },
    {
      "index": 370,
      "title": "On the value of information and other rewards",
      "abstract": "",
      "year": "2011",
      "venue": "Nature neuroscience, 14(9):1095–1097",
      "authors": "Yael Niv and Stephanie Chan."
    },
    {
      "index": 371,
      "title": "Computational models of reinforcement learning: the role of dopamine as a reward signal",
      "abstract": "",
      "year": "2010",
      "venue": "Cognitive neurodynamics, 4(2):91–105",
      "authors": "RD Samson, MJ Frank, and Jean-Marc Fellous."
    },
    {
      "index": 372,
      "title": "Dopamine reward prediction errors reflect hidden-state inference across time",
      "abstract": "",
      "year": "2017",
      "venue": "Nature neuroscience, 20(4):581–589",
      "authors": "Clara Kwon Starkweather, Benedicte M Babayan, Naoshige Uchida, and Samuel J\nGershman."
    },
    {
      "index": 373,
      "title": "Prefrontal cortex as a meta-reinforcement learning system",
      "abstract": "",
      "year": "2018",
      "venue": "Nature neuroscience, 21(6):860–868",
      "authors": "Jane X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer,\nJoel Z Leibo, Demis Hassabis, and Matthew Botvinick."
    },
    {
      "index": 374,
      "title": "Acute stress selectively reduces reward sensitivity",
      "abstract": "",
      "year": "2013",
      "venue": "Frontiers in human neuroscience, 7:133",
      "authors": "Lisa H Berghorst, Ryan Bogdan, Michael J Frank, and Diego A Pizzagalli."
    },
    {
      "index": 375,
      "title": "The role of executive function in shaping reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Milena Rmus, Samuel McDougle, and Anne Collins."
    },
    {
      "index": 376,
      "title": "How much of reinforcement learning is working memory, not reinforcement learning? a behavioral, computational, and neurogenetic analysis",
      "abstract": "",
      "year": "2012",
      "venue": "European Journal of Neuroscience, 35(7):",
      "authors": "Anne GE Collins and Michael J Frank."
    },
    {
      "index": 377,
      "title": "Prioritized memory access explains planning and hippocampal replay",
      "abstract": "",
      "year": "2018",
      "venue": "Nature neuroscience, 21(11):1609–1617",
      "authors": "Marcelo G Mattar and Nathaniel D Daw."
    },
    {
      "index": 378,
      "title": "Offline replay supports planning in human reinforcement learning",
      "abstract": "",
      "year": "2018",
      "venue": "Elife, 7:e",
      "authors": "Ida Momennejad, A Ross Otto, Nathaniel D Daw, and Kenneth A Norman."
    },
    {
      "index": 379,
      "title": "Hippocampal contributions to model-based planning and spatial memory",
      "abstract": "",
      "year": "2019",
      "venue": "Neuron, 102(3):683–693",
      "authors": "Oliver M Vikbladh, Michael R Meager, John King, Karen Blackmon, Orrin Devinsky,\nDaphna Shohamy, Neil Burgess, and Nathaniel D Daw."
    },
    {
      "index": 380,
      "title": "Learning structures: Predictive representations, replay, and generalization",
      "abstract": "",
      "year": "2020",
      "venue": "Current Opinion in Behavioral Sciences, 32:155–166",
      "authors": "Ida Momennejad."
    },
    {
      "index": 381,
      "title": "Search on the replay buffer: Bridging planning and reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine."
    },
    {
      "index": 382,
      "title": "Consolidation in neural networks and in the sleeping brain",
      "abstract": "",
      "year": "1996",
      "venue": "Connection Science, 8(2):259–276",
      "authors": "Anthony Robins."
    },
    {
      "index": 383,
      "title": "Catastrophic forgetting in simple networks: an analysis of the pseudorehearsal solution",
      "abstract": "",
      "year": "1999",
      "venue": "Network: Computation in Neural Systems, 10(3):227–236",
      "authors": "Marcus Frean and Anthony Robins."
    },
    {
      "index": 384,
      "title": "Sequential replay of nonspatial task states in the human hippocampus",
      "abstract": "",
      "year": "2019",
      "venue": "Science, 364(",
      "authors": "Nicolas W Schuck and Yael Niv."
    },
    {
      "index": 385,
      "title": "Generalization of value in reinforcement learning by humans",
      "abstract": "",
      "year": "2012",
      "venue": "European Journal of Neuroscience, 35(7):",
      "authors": "G Elliott Wimmer, Nathaniel D Daw, and Daphna Shohamy."
    },
    {
      "index": 386,
      "title": "Reinforcement learning and causal models",
      "abstract": "",
      "year": "2017",
      "venue": "The Oxford handbook of causal reasoning, page 295. Oxford\nUniversity Press",
      "authors": "Samuel J Gershman."
    },
    {
      "index": 387,
      "title": "Positive reward prediction errors during decision-making strengthen memory encoding",
      "abstract": "",
      "year": "2019",
      "venue": "Nature human behaviour, 3(7):719–732",
      "authors": "Anthony I Jang, Matthew R Nassar, Daniel G Dillon, and Michael J Frank."
    },
    {
      "index": 388,
      "title": "Prioritized experience replay",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver."
    },
    {
      "index": 389,
      "title": "States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Neuron, 66(4):585–595",
      "authors": "Jan Gläscher, Nathaniel Daw, Peter Dayan, and John P O’Doherty."
    },
    {
      "index": 390,
      "title": "Model-based influences on humans’ choices and striatal prediction errors",
      "abstract": "",
      "year": "2011",
      "venue": "Neuron, 69(6):",
      "authors": "Nathaniel D Daw, Samuel J Gershman, Ben Seymour, Peter Dayan, and Raymond J\nDolan."
    },
    {
      "index": 391,
      "title": "The ubiquity of model-based reinforcement learning",
      "abstract": "",
      "year": "2012",
      "venue": "Current opinion in neurobiology, 22(6):",
      "authors": "Bradley B Doll, Dylan A Simon, and Nathaniel D Daw."
    },
    {
      "index": 392,
      "title": "Model-based predictions for dopamine",
      "abstract": "",
      "year": "2018",
      "venue": "Current Opinion in Neurobiology, 49:1–7",
      "authors": "Angela J Langdon, Melissa J Sharpe, Geoffrey Schoenbaum, and Yael Niv."
    },
    {
      "index": 393,
      "title": "Environmental statistics and the trade-off between model-based and td learning in humans",
      "abstract": "",
      "year": "2011",
      "venue": "Advances in neural information processing systems",
      "authors": "Dylan A Simon and Nathaniel D Daw."
    },
    {
      "index": 394,
      "title": "Model-based learning protects against forming habits",
      "abstract": "",
      "year": "2015",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience, 15(3):523–536",
      "authors": "Claire M Gillan, A Ross Otto, Elizabeth A Phelps, and Nathaniel D Daw."
    },
    {
      "index": 395,
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "abstract": "",
      "year": "1993",
      "venue": "Neural Computation, 5(4):613–624",
      "authors": "Peter Dayan."
    },
    {
      "index": 396,
      "title": "The successor representation in human reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Nature Human Behaviour, 1(9):680–692",
      "authors": "Ida Momennejad, Evan M Russek, Jin H Cheong, Matthew M Botvinick,\nNathaniel Douglass Daw, and Samuel J Gershman."
    },
    {
      "index": 397,
      "title": "Internal models in the cerebellum",
      "abstract": "",
      "year": "1998",
      "venue": "Trends in cognitive sciences, 2(9):338–347",
      "authors": "Daniel M Wolpert, R Chris Miall, and Mitsuo Kawato."
    },
    {
      "index": 398,
      "title": "Separated modules for visuomotor control and learning in the cerebellum: a functional mri study",
      "abstract": "",
      "year": "1997",
      "venue": "NeuroImage: Third International Conference on Functional\nMapping of the Human Brain, volume 5",
      "authors": "Hiroshi Imamizu."
    },
    {
      "index": 399,
      "title": "Human cerebellar activity reflecting an acquired internal model of a new tool",
      "abstract": "",
      "year": "2000",
      "venue": "Nature, 403(",
      "authors": "Hiroshi Imamizu, Satoru Miyauchi, Tomoe Tamada, Yuka Sasaki, Ryousuke Takino,\nBenno PuÈtz, Toshinori Yoshioka, and Mitsuo Kawato."
    },
    {
      "index": 400,
      "title": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
      "abstract": "",
      "year": "2005",
      "venue": "Nature neuroscience, 8(12):1704–1711",
      "authors": "Nathaniel D Daw, Yael Niv, and Peter Dayan."
    },
    {
      "index": 401,
      "title": "Dopamine, uncertainty and td learning",
      "abstract": "",
      "year": "2005",
      "venue": "Behavioral and brain Functions, 1(1):6",
      "authors": "Yael Niv, Michael O Duff, and Peter Dayan."
    },
    {
      "index": 402,
      "title": "Self-evaluation of decision-making: A general bayesian framework for metacognitive computation",
      "abstract": "",
      "year": "2017",
      "venue": "Psychological review, 124(1):91",
      "authors": "Stephen M Fleming and Nathaniel D Daw."
    },
    {
      "index": 403,
      "title": "Belief state representation in the dopamine system",
      "abstract": "",
      "year": "2018",
      "venue": "Nature communications, 9(1):1–10",
      "authors": "Benedicte M Babayan, Naoshige Uchida, and Samuel J Gershman."
    },
    {
      "index": 404,
      "title": "Distributional reinforcement learning in the brain",
      "abstract": "",
      "year": "2020",
      "venue": "Trends in Neurosciences",
      "authors": "Adam S Lowet, Qiao Zheng, Sara Matias, Jan Drugowitsch, and Naoshige Uchida."
    },
    {
      "index": 405,
      "title": "Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective",
      "abstract": "",
      "year": "2009",
      "venue": "Cognition, 113(3):262–280",
      "authors": "Matthew M Botvinick, Yael Niv, and Andew G Barto."
    },
    {
      "index": 406,
      "title": "Human reinforcement learning subdivides structured action spaces by learning effector-specific values",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Neuroscience, 29(43):",
      "authors": "Samuel J Gershman, Bijan Pesaran, and Nathaniel D Daw."
    },
    {
      "index": 407,
      "title": "Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis",
      "abstract": "",
      "year": "2012",
      "venue": "Cerebral cortex, 22(3):509–526",
      "authors": "Michael J Frank and David Badre."
    },
    {
      "index": 408,
      "title": "Mechanisms of hierarchical reinforcement learning in cortico–striatal circuits 2: Evidence from fmri",
      "abstract": "",
      "year": "2012",
      "venue": "Cerebral cortex, 22(3):527–536",
      "authors": "David Badre and Michael J Frank."
    },
    {
      "index": 409,
      "title": "Divide and conquer: hierarchical reinforcement learning and task decomposition in humans",
      "abstract": "",
      "year": "2013",
      "venue": "Computational and robotic models of the hierarchical\norganization of behavior",
      "authors": "Carlos Diuk, Anna Schapiro, Natalia Córdova, José Ribas-Fernandes, Yael\nNiv, and Matthew Botvinick."
    },
    {
      "index": 410,
      "title": "Model-based hierarchical reinforcement learning and human action control",
      "abstract": "",
      "year": "2014",
      "venue": "Philosophical Transactions of the Royal Society B: Biological\nSciences, 369(",
      "authors": "Matthew Botvinick and Ari Weinstein."
    },
    {
      "index": 411,
      "title": "Optimal behavioral hierarchy",
      "abstract": "",
      "year": "2014",
      "venue": "PLOS Comput Biol, 10(8):e",
      "authors": "Alec Solway, Carlos Diuk, Natalia Córdova, Debbie Yee, Andrew G Barto, Yael\nNiv, and Matthew M Botvinick."
    },
    {
      "index": 412,
      "title": "Subgoal-and goal-related reward prediction errors in medial prefrontal cortex",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of cognitive neuroscience, 31(1):8–23",
      "authors": "José JF Ribas-Fernandes, Danesh Shahnazian, Clay B Holroyd, and Matthew M\nBotvinick."
    },
    {
      "index": 413,
      "title": "Reinforcement learning in multidimensional environments relies on attention mechanisms",
      "abstract": "",
      "year": "2015",
      "venue": "Journal of Neuroscience, 35(21):",
      "authors": "Yael Niv, Reka Daniel, Andra Geana, Samuel J Gershman, Yuan Chang Leong, Angela\nRadulescu, and Robert C Wilson."
    },
    {
      "index": 414,
      "title": "Learning task-state representations",
      "abstract": "",
      "year": "2019",
      "venue": "Nature neuroscience, 22(10):1544–1553",
      "authors": "Yael Niv."
    },
    {
      "index": 415,
      "title": "Discovering latent causes in reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Current Opinion in Behavioral Sciences, 5:43–50",
      "authors": "Samuel J Gershman, Kenneth A Norman, and Yael Niv."
    },
    {
      "index": 416,
      "title": "The bitter lesson",
      "abstract": "",
      "year": "2019",
      "venue": "Incomplete Ideas (blog), March, 13:12",
      "authors": "Richard Sutton."
    },
    {
      "index": 417,
      "title": "Improving Generalization in Meta Reinforcement Learning using Learned Objectives",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Louis Kirsch, Sjoerd van Steenkiste, and Juergen Schmidhuber.",
      "orig_title": "Improving generalization in meta reinforcement learning using learned objectives",
      "paper_id": "1910.04098v2"
    },
    {
      "index": 418,
      "title": "Discovering Reinforcement Learning Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, 33",
      "authors": "Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado P van\nHasselt, Satinder Singh, and David Silver.",
      "orig_title": "Discovering reinforcement learning algorithms",
      "paper_id": "2007.08794v3"
    },
    {
      "index": 419,
      "title": "Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, and\nKenneth O Stanley.",
      "orig_title": "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions",
      "paper_id": "2003.08536v2"
    },
    {
      "index": 420,
      "title": "On Value Functions and the Agent-Environment Boundary",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nan Jiang.",
      "orig_title": "On value functions and the agent-environment boundary",
      "paper_id": "1905.13341v3"
    },
    {
      "index": 421,
      "title": "What is an agent?",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Anna Harutyunyan."
    },
    {
      "index": 422,
      "title": "What can I do here? A Theory of Affordances in Reinforcement Learning",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina\nPrecup.",
      "orig_title": "What can i do here? a theory of affordances in reinforcement learning",
      "paper_id": "2006.15085v1"
    },
    {
      "index": 423,
      "title": "Navigating the affordance landscape: feedback control as a process model of behavior and cognition",
      "abstract": "",
      "year": "2016",
      "venue": "Trends in cognitive sciences, 20(6):414–424",
      "authors": "Giovanni Pezzulo and Paul Cisek."
    },
    {
      "index": 424,
      "title": "The theory of affordances",
      "abstract": "",
      "year": "1977",
      "venue": "Hilldale, USA, 1(2)",
      "authors": "James J Gibson."
    },
    {
      "index": 425,
      "title": "Affordances and the body: An intentional analysis of gibson’s ecological approach to visual perception",
      "abstract": "",
      "year": "1989",
      "venue": "Journal for the theory of social behaviour, 19(1):1–30",
      "authors": "Harry Heft."
    },
    {
      "index": 426,
      "title": "An outline of a theory of affordances",
      "abstract": "",
      "year": "2003",
      "venue": "Ecological psychology, 15(2):181–195",
      "authors": "Anthony Chemero."
    },
    {
      "index": 427,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "Advances in neural information processing systems",
      "authors": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton."
    },
    {
      "index": 428,
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "abstract": "",
      "year": "2015",
      "venue": "International journal of computer vision, 115(3):211–252",
      "authors": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
      "orig_title": "Imagenet large scale visual recognition challenge",
      "paper_id": "1409.0575v3"
    },
    {
      "index": 429,
      "title": "Visualizing and understanding convolutional networks",
      "abstract": "",
      "year": "2014",
      "venue": "European conference on computer vision, pages 818–833.\nSpringer",
      "authors": "Matthew D Zeiler and Rob Fergus."
    },
    {
      "index": 430,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus."
    },
    {
      "index": 431,
      "title": "The hardware lottery",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sara Hooker."
    },
    {
      "index": 432,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 433,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 434,
      "title": "A Neural Scaling Law from the Dimension of the Data Manifold",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Utkarsh Sharma and Jared Kaplan.",
      "orig_title": "A neural scaling law from the dimension of the data manifold",
      "paper_id": "2004.10802v1"
    },
    {
      "index": 435,
      "title": "Scaling Laws for Autoregressive Generative Modeling",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob\nJackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.",
      "orig_title": "Scaling laws for autoregressive generative modeling",
      "paper_id": "2010.14701v2"
    },
    {
      "index": 436,
      "title": "A survey and critique of multiagent deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Autonomous Agents and Multi-Agent Systems, 33(6):750–797",
      "authors": "Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor."
    },
    {
      "index": 437,
      "title": "Friend-or-foe q-learning in general-sum games",
      "abstract": "",
      "year": "2001",
      "venue": "page 322–328, San Francisco, CA, USA, 2001. Morgan Kaufmann\nPublishers Inc",
      "authors": "Michael L. Littman."
    },
    {
      "index": 438,
      "title": "Reinforcement learning to play an optimal nash equilibrium in team markov games",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": "Xiaofeng Wang and Tuomas Sandholm."
    },
    {
      "index": 439,
      "title": "Convergence and no-regret in multiagent learning",
      "abstract": "",
      "year": "2004",
      "venue": "Advances in neural information processing systems, 17",
      "authors": "Michael Bowling."
    },
    {
      "index": 440,
      "title": "Correlated-Q learning",
      "abstract": "",
      "year": "2003",
      "venue": "ISBN",
      "authors": "Amy Greenwald and Keith Hall."
    },
    {
      "index": 441,
      "title": "Cyclic equilibria in markov games",
      "abstract": "",
      "year": "2006",
      "venue": "volume 18. MIT Press",
      "authors": "Martin Zinkevich, Amy Greenwald, and Michael Littman."
    },
    {
      "index": 442,
      "title": "Stable Opponent Shaping in Differentiable Games",
      "abstract": "",
      "year": "2019",
      "venue": "URL https://openreview.net/forum?id=SyGjjsC5tQ",
      "authors": "Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktäschel, and Shimon\nWhiteson.",
      "orig_title": "Stable opponent shaping in differentiable games",
      "paper_id": "1811.08469v3"
    },
    {
      "index": 443,
      "title": "Teaching on a budget: Agents advising agents in reinforcement learning",
      "abstract": "",
      "year": "2013",
      "venue": "Proceedings of the 2013 international conference on\nAutonomous agents and multi-agent systems",
      "authors": "Lisa Torrey and Matthew Taylor."
    },
    {
      "index": 444,
      "title": "Learning to Teach in Cooperative Multiagent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33",
      "authors": "Shayegan Omidshafiei, Dong Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer,\nChristopher Amato, Murray Campbell, and Jonathan P How.",
      "orig_title": "Learning to teach in cooperative multiagent reinforcement learning",
      "paper_id": "1805.07830v4"
    },
    {
      "index": 445,
      "title": "Learning Hierarchical Teaching Policies for Cooperative Agents",
      "abstract": "",
      "year": "2020",
      "venue": "AAMAS",
      "authors": "Dong Ki Kim, Miao Liu, Shayegan Omidshafiei, Sebastian Lopez-Cot, Matthew\nRiemer, Golnaz Habibi, Gerald Tesauro, Sami Mourad, Murray Campbell, and\nJonathan P How.",
      "orig_title": "Learning hierarchical teaching policies for cooperative agents",
      "paper_id": "1903.03216v6"
    },
    {
      "index": 446,
      "title": "Multi-agent learning with policy prediction",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Chongjie Zhang and Victor R. Lesser."
    },
    {
      "index": 447,
      "title": "Toward an architecture for never-ending language learning",
      "abstract": "",
      "year": "2010",
      "venue": "Twenty-Fourth AAAI Conference on Artificial Intelligence",
      "authors": "Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R\nHruschka, and Tom M Mitchell."
    },
    {
      "index": 448,
      "title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen\nRoller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al."
    }
  ]
}