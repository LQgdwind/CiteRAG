{
  "paper_id": "2105.03793v2",
  "title": "1 Introduction",
  "abstract": "Abstract\nMany machine learning problems can be formulated as minimax problems such as Generative Adversarial Networks (GANs), AUC maximization and robust estimation, to mention but a few. A substantial amount of studies are devoted to studying the convergence behavior of their stochastic gradient-type algorithms. In contrast, there is relatively little work on understanding their generalization, i.e., how the learning models built from training examples would behave on test examples. In this paper, we provide a comprehensive generalization analysis of stochastic gradient methods for minimax problems under both convex-concave and nonconvex-nonconcave cases through the lens of algorithmic stability. We establish a quantitative connection between stability and several generalization measures both in expectation and with high probability. For the convex-concave setting, our stability analysis shows that stochastic gradient descent ascent attains optimal generalization bounds for both smooth and nonsmooth minimax problems. We also establish generalization bounds for both weakly-convex-weakly-concave and gradient-dominated problems. We report preliminary experimental results to verify our theory.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Wasserstein generative adversarial networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Arjovsky, M., Chintala, S., and Bottou, L."
    },
    {
      "index": 1,
      "title": "Robust linear least squares regression",
      "abstract": "",
      "year": "2011",
      "venue": "Annals of Statistics",
      "authors": "Audibert, J.-Y. and Catoni, O."
    },
    {
      "index": 2,
      "title": "Stochastic Variance Reduction Methods for Saddle-Point Problems",
      "abstract": "",
      "year": "2016",
      "venue": "Advance In Neural Information Processing Systems",
      "authors": "Balamurugan, P. and Bach, F.",
      "orig_title": "Stochastic variance reduction methods for saddle-point problems",
      "paper_id": "1605.06398v2"
    },
    {
      "index": 3,
      "title": "Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Bassily, R., Feldman, V., Guzmán, C., and Talwar, K.",
      "orig_title": "Stability of stochastic gradient descent on nonsmooth convex losses",
      "paper_id": "2006.06914v1"
    },
    {
      "index": 4,
      "title": "The tradeoffs of large scale learning",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Bousquet, O. and Bottou, L."
    },
    {
      "index": 5,
      "title": "Stability and generalization",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "Bousquet, O. and Elisseeff, A."
    },
    {
      "index": 6,
      "title": "Sharper Bounds for Uniformly Stable Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "Bousquet, O., Klochkov, Y., and Zhivotovskiy, N.",
      "orig_title": "Sharper bounds for uniformly stable algorithms",
      "paper_id": "1910.07833v2"
    },
    {
      "index": 7,
      "title": "Empirical risk minimization for heavy-tailed losses",
      "abstract": "",
      "year": "2015",
      "venue": "The Annals of Statistics",
      "authors": "Brownlees, C., Joly, E., Lugosi, G., et al."
    },
    {
      "index": 8,
      "title": "Libsvm: a library for support vector machines",
      "abstract": "",
      "year": "2011",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "authors": "Chang, C.-C. and Lin, C.-J."
    },
    {
      "index": 9,
      "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Charles, Z. and Papailiopoulos, D.",
      "orig_title": "Stability and generalization of learning algorithms that converge to global optima",
      "paper_id": "1710.08402v1"
    },
    {
      "index": 10,
      "title": "Robust Optimization for Non-Convex Objectives",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chen, R. S., Lucier, B., Singer, Y., and Syrgkanis, V.",
      "orig_title": "Robust optimization for non-convex objectives",
      "paper_id": "1707.01047v1"
    },
    {
      "index": 11,
      "title": "Stability and Convergence Trade-off of Iterative Optimization Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.01619",
      "authors": "Chen, Y., Jin, C., and Yu, B.",
      "orig_title": "Stability and convergence trade-off of iterative optimization algorithms",
      "paper_id": "1804.01619v1"
    },
    {
      "index": 12,
      "title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations",
      "abstract": "",
      "year": "1952",
      "venue": "Annals of Mathematical Statistics",
      "authors": "Chernoff, H."
    },
    {
      "index": 13,
      "title": "Sbeed: Convergent reinforcement learning with nonlinear function approximation",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L."
    },
    {
      "index": 14,
      "title": "Toward better generalization bounds with locally elastic stability",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.13988",
      "authors": "Deng, Z., He, H., and Su, W. J."
    },
    {
      "index": 15,
      "title": "Stochastic Variance Reduction Methods for Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Du, S. S., Chen, J., Li, L., Xiao, L., and Zhou, D.",
      "orig_title": "Stochastic variance reduction methods for policy evaluation",
      "paper_id": "1702.07944v2"
    },
    {
      "index": 16,
      "title": "Stability of randomized learning algorithms",
      "abstract": "",
      "year": "2005",
      "venue": "Journal of Machine Learning Research",
      "authors": "Elisseeff, A., Evgeniou, T., and Pontil, M."
    },
    {
      "index": 17,
      "title": "Train simultaneously, generalize better: Stability of gradient-based minimax learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.12561",
      "authors": "Farnia, F. and Ozdaglar, A.",
      "orig_title": "Train simultaneously, generalize better: Stability of gradient-based minimax learners",
      "paper_id": "2010.12561v1"
    },
    {
      "index": 18,
      "title": "High probability generalization bounds for uniformly stable algorithms with nearly optimal rate",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory",
      "authors": "Feldman, V. and Vondrak, J."
    },
    {
      "index": 19,
      "title": "Hypothesis Set Stability and Generalization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Foster, D. J., Greenberg, S., Kale, S., Luo, H., Mohri, M., and Sridharan, K.",
      "orig_title": "Hypothesis set stability and generalization",
      "paper_id": "1904.04755v3"
    },
    {
      "index": 20,
      "title": "One-pass auc optimization",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "Gao, W., Jin, R., Zhu, S., and Zhou, Z.-H."
    },
    {
      "index": 21,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y."
    },
    {
      "index": 22,
      "title": "Train faster, generalize better: Stability of stochastic gradient descent",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Hardt, M., Recht, B., and Singer, Y.",
      "orig_title": "Train faster, generalize better: Stability of stochastic gradient descent",
      "paper_id": "1509.01240v2"
    },
    {
      "index": 23,
      "title": "Probability inequalities for sums of bounded random variables",
      "abstract": "",
      "year": "1963",
      "venue": "Journal of the American Statistical Association",
      "authors": "Hoeffding, W."
    },
    {
      "index": 24,
      "title": "On the Convergence of Single-Call Stochastic Extra-Gradient Methods",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P.",
      "orig_title": "On the convergence of single-call stochastic extra-gradient methods",
      "paper_id": "1908.08465v2"
    },
    {
      "index": 25,
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Machine Learning",
      "authors": "Karimi, H., Nutini, J., and Schmidt, M.",
      "orig_title": "Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition",
      "paper_id": "1608.04636v4"
    },
    {
      "index": 26,
      "title": "Data-Dependent Stability of Stochastic Gradient Descent",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Kuzborskij, I. and Lampert, C.",
      "orig_title": "Data-dependent stability of stochastic gradient descent",
      "paper_id": "1703.01678v4"
    },
    {
      "index": 27,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P."
    },
    {
      "index": 28,
      "title": "Fine-grained analysis of stability and generalization for stochastic gradient descent",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Lei, Y. and Ying, Y."
    },
    {
      "index": 29,
      "title": "Sharper generalization bounds for learning with gradient-dominated objective functions",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Lei, Y. and Ying, Y."
    },
    {
      "index": 30,
      "title": "Stochastic Proximal AUC Maximization",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Lei, Y. and Ying, Y.",
      "orig_title": "Stochastic proximal auc maximization",
      "paper_id": "1906.06053v1"
    },
    {
      "index": 31,
      "title": "On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Li, J., Luo, X., and Qiao, M.",
      "orig_title": "On generalization error bounds of noisy gradient methods for non-convex learning",
      "paper_id": "1902.00621v4"
    },
    {
      "index": 32,
      "title": "Generalization properties and implicit regularization for multiple passes SGM",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Lin, J., Camoriano, R., and Rosasco, L."
    },
    {
      "index": 33,
      "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Lin, T., Jin, C., and Jordan, M.",
      "orig_title": "On gradient descent ascent for nonconvex-concave minimax problems",
      "paper_id": "1906.00331v10"
    },
    {
      "index": 34,
      "title": "Fast stochastic AUC maximization with O(1/n)-convergence rate",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Liu, M., Zhang, X., Chen, Z., Wang, X., and Yang, T."
    },
    {
      "index": 35,
      "title": "First-order convergence theory for weakly-convex-weakly-concave min-max problems",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Liu, M., Rafique, H., Lin, Q., and Yang, T."
    },
    {
      "index": 36,
      "title": "Algorithmic Stability and Hypothesis Complexity",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Liu, T., Lugosi, G., Neu, G., and Tao, D.",
      "orig_title": "Algorithmic stability and hypothesis complexity",
      "paper_id": "1702.08712v2"
    },
    {
      "index": 37,
      "title": "Stochastic Hamiltonian Gradient Methods for Smooth Games",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Loizou, N., Berard, H., Jolicoeur-Martineau, A., Vincent, P., Lacoste-Julien, S., and Mitliagkas, I.",
      "orig_title": "Stochastic hamiltonian gradient methods for smooth games",
      "paper_id": "2007.04202v1"
    },
    {
      "index": 38,
      "title": "A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "London, B.",
      "orig_title": "A PAC-bayesian analysis of randomized learning with application to stochastic gradient descent",
      "paper_id": "1709.06617v5"
    },
    {
      "index": 39,
      "title": "Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Luo, L., Ye, H., Huang, Z., and Zhang, T.",
      "orig_title": "Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems",
      "paper_id": "2001.03724v2"
    },
    {
      "index": 40,
      "title": "High probability convergence and uniform stability bounds for nonconvex stochastic gradient descent",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.05610",
      "authors": "Madden, L., Dall’Anese, E., and Becker, S."
    },
    {
      "index": 41,
      "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Learning Theory",
      "authors": "Mou, W., Wang, L., Zhai, X., and Zheng, K.",
      "orig_title": "Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints",
      "paper_id": "1707.05947v1"
    },
    {
      "index": 42,
      "title": "Variance-based regularization with convex objectives",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Namkoong, H. and Duchi, J. C."
    },
    {
      "index": 43,
      "title": "Subgradient methods for saddle-point problems",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Optimization Theory and Applications",
      "authors": "Nedić, A. and Ozdaglar, A."
    },
    {
      "index": 44,
      "title": "Robust stochastic approximation approach to stochastic programming",
      "abstract": "",
      "year": "2009",
      "venue": "SIAM Journal on Optimization",
      "authors": "Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A."
    },
    {
      "index": 45,
      "title": "Non-convex min-max optimization: Provable algorithms and applications in machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.02060",
      "authors": "Rafique, H., Liu, M., Lin, Q., and Yang, T."
    },
    {
      "index": 46,
      "title": "Probabilistic construction of deterministic algorithms: approximating packing integer programs",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of Computer and System Sciences",
      "authors": "Raghavan, P."
    },
    {
      "index": 47,
      "title": "Stability results in learning theory",
      "abstract": "",
      "year": "2005",
      "venue": "Analysis and Applications",
      "authors": "Rakhlin, A., Mukherjee, S., and Poggio, T."
    },
    {
      "index": 48,
      "title": "Learning with Gradient Descent and Weakly Convex Losses",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.04968",
      "authors": "Richards, D. and Rabbat, M.",
      "orig_title": "Learning with gradient descent and weakly convex losses",
      "paper_id": "2101.04968v2"
    },
    {
      "index": 49,
      "title": "Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "Richards, D. et al.",
      "orig_title": "Graph-dependent implicit regularisation for distributed stochastic subgradient descent",
      "paper_id": "1809.06958v1"
    },
    {
      "index": 50,
      "title": "Monotone operators and the proximal point algorithm",
      "abstract": "",
      "year": "1976",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Rockafellar, R. T."
    },
    {
      "index": 51,
      "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.04454",
      "authors": "Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., and Bottou, L.",
      "orig_title": "Empirical analysis of the hessian of over-parametrized neural networks",
      "paper_id": "1706.04454v3"
    },
    {
      "index": 52,
      "title": "Learnability, stability and uniform convergence",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Machine Learning Research",
      "authors": "Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K."
    },
    {
      "index": 53,
      "title": "Online learning as stochastic approximation of regularization paths: optimality and almost-sure convergence",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Tarres, P. and Yao, Y."
    },
    {
      "index": 54,
      "title": "Efficient Algorithms for Smooth Minimax Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Thekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S.",
      "orig_title": "Efficient algorithms for smooth minimax optimization",
      "paper_id": "1907.01543v1"
    },
    {
      "index": 55,
      "title": "High-dimensional probability: An introduction with applications in data science",
      "abstract": "",
      "year": "2018",
      "venue": "Cambridge university press",
      "authors": "Vershynin, R."
    },
    {
      "index": 56,
      "title": "Learning with Non-Convex Truncated Losses by SGD",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Xu, Y., Zhu, S., Yang, S., Zhang, C., Jin, R., and Yang, T.",
      "orig_title": "Learning with non-convex truncated losses by sgd",
      "paper_id": "1805.07880v1"
    },
    {
      "index": 57,
      "title": "Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yan, Y., Xu, Y., Lin, Q., Liu, W., and Yang, T.",
      "orig_title": "Optimal epoch stochastic gradient descent ascent methods for min-max optimization",
      "paper_id": "2002.05309v2"
    },
    {
      "index": 58,
      "title": "Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yang, J., Kiyavash, N., and He, N."
    },
    {
      "index": 59,
      "title": "Stochastic online AUC maximization",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ying, Y., Wen, L., and Lyu, S."
    },
    {
      "index": 60,
      "title": "Stagewise Training Accelerates Convergence of Testing Error Over SGD",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yuan, Z., Yan, Y., Jin, R., and Yang, T.",
      "orig_title": "Stagewise training accelerates convergence of testing error over sgd",
      "paper_id": "1812.03934v3"
    },
    {
      "index": 61,
      "title": "Generalization bounds for stochastic saddle point problems",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.02067",
      "authors": "Zhang, J., Hong, M., Wang, M., and Zhang, S."
    },
    {
      "index": 62,
      "title": "Online AUC maximization",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Machine Learning",
      "authors": "Zhao, P., Hoi, S. C., Jin, R., and Yang, T."
    }
  ]
}