{
  "paper_id": "2403.07883v1",
  "title": "Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection",
  "abstract": "Abstract\nVision Transformers (ViTs) have become increasingly popular in large-scale Vision and Language Pre-training (VLP) models. Although previous VLP research has demonstrated the efficacy of ViTs, these efforts still struggle with computational inefficiencies caused by lengthy visual sequences. To address this challenge, we introduce an efficient VLP approach called TRIPS, which stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the visual sequence using a text-guided patch-selection layer in the visual backbone, thereby accelerating both training and inference processes. This patch-selection layer dynamically computes text-dependent visual attention, enabling it to identify attentive image tokens with text guidance and fuse inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any extra parameters and generalizes to most ViT-based VLP models. We incorporate TRIPS into three representative VLP models covering single-stream, dual-stream, and generative paradigms, and conduct extensive experiments on five widely-used multi-modal benchmark datasets. Our experimental results reveal that TRIPS delivers a 40% speedup, while maintaining competitive or superior performance on downstream tasks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "Vqa: Visual question answering",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "Agrawal A, Lu J, Antol S, et al"
    },
    {
      "index": 2,
      "title": "nocaps: novel object captioning at scale",
      "abstract": "",
      "year": "2018",
      "venue": "CoRR",
      "authors": "Agrawal H, Desai K, Wang Y, et al"
    },
    {
      "index": 3,
      "title": "Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Bi B, Li C, Wu C, et al"
    },
    {
      "index": 4,
      "title": "Emerging properties in self-supervised vision transformers",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Caron M, Touvron H, Misra I, et al"
    },
    {
      "index": 5,
      "title": "Uniter: Learning universal image-text representations",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Chen YC, Li L, Yu L, et al"
    },
    {
      "index": 6,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Chen YC, Li L, Yu L, et al",
      "orig_title": "Uniter: Universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 7,
      "title": "Rethinking attention with performers",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Choromanski K, Likhosherstov V, Dohan D, et al"
    },
    {
      "index": 8,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
      "authors": "Cubuk ED, Zoph B, Shlens J, et al"
    },
    {
      "index": 9,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Devlin J, Chang MW, Lee K, et al",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 10,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Dosovitskiy A, Beyer L, Kolesnikov A, et al",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 11,
      "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Dou ZY, Xu Y, Gan Z, et al",
      "orig_title": "An empirical study of training end-to-end vision-and-language transformers",
      "paper_id": "2111.02387v3"
    },
    {
      "index": 12,
      "title": "Compressing Visual-linguistic Model via Knowledge Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Fang Z, Wang J, Hu X, et al",
      "orig_title": "Compressing visual-linguistic model via knowledge distillation",
      "paper_id": "2104.02096v1"
    },
    {
      "index": 13,
      "title": "Playing Lottery Tickets with Vision and Language",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Gan Z, Chen YC, Li L, et al",
      "orig_title": "Playing lottery tickets with vision and language",
      "paper_id": "2104.11832v2"
    },
    {
      "index": 14,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Goyal Y, Khot T, Summers-Stay D, et al",
      "orig_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 15,
      "title": "Mask R-CNN",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "authors": "He K, Gkioxari G, Dollár P, et al",
      "orig_title": "Mask r-cnn",
      "paper_id": "1703.06870v3"
    },
    {
      "index": 16,
      "title": "Rethinking Spatial Dimensions of Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Heo B, Yun S, Han D, et al",
      "orig_title": "Rethinking spatial dimensions of vision transformers",
      "paper_id": "2103.16302v2"
    },
    {
      "index": 17,
      "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Huang Z, Zeng Z, Liu B, et al",
      "orig_title": "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers",
      "paper_id": "2004.00849v2"
    },
    {
      "index": 18,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Jia C, Yang Y, Xia Y, et al"
    },
    {
      "index": 19,
      "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "Karpathy A, Fei-Fei L",
      "orig_title": "Deep visual-semantic alignments for generating image descriptions",
      "paper_id": "1412.2306v2"
    },
    {
      "index": 20,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Kim W, Son B, Kim I",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 21,
      "title": "Reformer: The Efficient Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Kitaev N, Kaiser L, Levskaya A",
      "orig_title": "Reformer: The efficient transformer",
      "paper_id": "2001.04451v2"
    },
    {
      "index": 22,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2016",
      "venue": "International Journal of Computer Vision",
      "authors": "Krishna R, Zhu Y, Groth O, et al"
    },
    {
      "index": 23,
      "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Li C, Xu H, Tian J, et al",
      "orig_title": "mplug: Effective and efficient vision-language learning by cross-modal skip-connections",
      "paper_id": "2205.12005v2"
    },
    {
      "index": 24,
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Li J, Selvaraju RR, Gotmare AD, et al",
      "orig_title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "paper_id": "2107.07651v2"
    },
    {
      "index": 25,
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Li J, Li D, Xiong C, et al"
    },
    {
      "index": 26,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Li LH, Yatskar M, Yin D, et al",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 27,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Li X, Yin X, Li C, et al",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 28,
      "title": "Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "Liang Y, Ge C, Tong Z, et al",
      "orig_title": "Not all patches are what you need: Expediting vision transformers via token reorganizations",
      "paper_id": "2202.07800v2"
    },
    {
      "index": 29,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Lin TY, Maire M, Belongie SJ, et al",
      "orig_title": "Microsoft coco: Common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 30,
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Liu Z, Lin Y, Cao Y, et al"
    },
    {
      "index": 31,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Loshchilov I, Hutter F",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 32,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Lu J, Batra D, Parikh D, et al"
    },
    {
      "index": 33,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "NIPS",
      "authors": "Ordonez V, Kulkarni G, Berg TL"
    },
    {
      "index": 34,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "authors": "Plummer BA, Wang L, Cervantes CM, et al",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 35,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Radford A, Kim JW, Hallacy C, et al",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 36,
      "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
      "abstract": "",
      "year": "2021",
      "venue": "Neural Information Processing Systems",
      "authors": "Rao Y, Zhao W, Liu B, et al"
    },
    {
      "index": 37,
      "title": "You Only Look Once: Unified, Real-Time Object Detection",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Redmon J, Divvala SK, Girshick RB, et al",
      "orig_title": "You only look once: Unified, real-time object detection",
      "paper_id": "1506.02640v5"
    },
    {
      "index": 38,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Ren S, He K, Girshick RB, et al",
      "orig_title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 39,
      "title": "Self-critical Sequence Training for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Rennie SJ, Marcheret E, Mroueh Y, et al",
      "orig_title": "Self-critical sequence training for image captioning",
      "paper_id": "1612.00563v2"
    },
    {
      "index": 40,
      "title": "Tokenlearner: Adaptive space-time tokenization for videos",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Ryoo MS, Piergiovanni AJ, Arnab A, et al"
    },
    {
      "index": 41,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Sharma P, Ding N, Goodman S, et al"
    },
    {
      "index": 42,
      "title": "FLAVA: A Foundational Language And Vision Alignment Model",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Singh A, Hu R, Goswami V, et al",
      "orig_title": "Flava: A foundational language and vision alignment model",
      "paper_id": "2112.04482v3"
    },
    {
      "index": 43,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Su W, Zhu X, Cao Y, et al"
    },
    {
      "index": 44,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Suhr A, Zhou S, Zhang I, et al",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 45,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Tan HH, Bansal M",
      "orig_title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 46,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Touvron H, Cord M, Douze M, et al",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 47,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv",
      "authors": "Vaswani A, Shazeer NM, Parmar N, et al",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 48,
      "title": "MiniVLM: A Smaller and Faster Vision-Language Model",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Wang J, Hu X, Zhang P, et al",
      "orig_title": "Minivlm: A smaller and faster vision-language model",
      "paper_id": "2012.06946v2"
    },
    {
      "index": 49,
      "title": "Linformer: Self-Attention with Linear Complexity",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Wang S, Li BZ, Khabsa M, et al",
      "orig_title": "Linformer: Self-attention with linear complexity",
      "paper_id": "2006.04768v3"
    },
    {
      "index": 50,
      "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Wang W, Bao H, Dong L, et al",
      "orig_title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
      "paper_id": "2111.02358v2"
    },
    {
      "index": 51,
      "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Wang W, Xie E, Li X, et al",
      "orig_title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "paper_id": "2102.12122v2"
    },
    {
      "index": 52,
      "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Wang Z, Yu J, Yu AW, et al",
      "orig_title": "Simvlm: Simple visual language model pretraining with weak supervision",
      "paper_id": "2108.10904v3"
    },
    {
      "index": 53,
      "title": "CvT: Introducing Convolutions to Vision Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "authors": "Wu H, Xiao B, Codella NCF, et al",
      "orig_title": "Cvt: Introducing convolutions to vision transformers",
      "paper_id": "2103.15808v1"
    },
    {
      "index": 54,
      "title": "E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Xu H, Yan M, Li C, et al"
    },
    {
      "index": 55,
      "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Xu Y, Zhang Z, Zhang M, et al",
      "orig_title": "Evo-vit: Slow-fast token evolution for dynamic vision transformer",
      "paper_id": "2108.01390v5"
    },
    {
      "index": 56,
      "title": "Crossing the format boundary of text and boxes: Towards unified vision-language modeling",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Yang Z, Gan Z, Wang J, et al"
    },
    {
      "index": 57,
      "title": "Ernie-vil: Knowledge enhanced vision-language representations through scene graph",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Yu F, Tang J, Yin W, et al"
    },
    {
      "index": 58,
      "title": "Modeling context in referring expressions",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Computer Vision",
      "authors": "Yu L, Poirson P, Yang S, et al"
    },
    {
      "index": 59,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Zhang P, Li X, Hu X, et al",
      "orig_title": "Vinvl: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    },
    {
      "index": 60,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Zhou L, Palangi H, Zhang L, et al",
      "orig_title": "Unified vision-language pre-training for image captioning and vqa",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 61,
      "title": "Self-slimmed Vision Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Zong Z, Li K, Song G, et al",
      "orig_title": "Self-slimmed vision transformer",
      "paper_id": "2111.12624v3"
    }
  ]
}