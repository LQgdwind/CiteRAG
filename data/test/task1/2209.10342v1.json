{
  "paper_id": "2209.10342v1",
  "title": "Partially Observable Markov Decision Processes in Robotics: A Survey",
  "abstract": "Abstract\nNoisy sensing, imperfect control, and environment changes are defining characteristics of many real-world robot tasks.\nThe partially observable Markov decision process (POMDP) provides a principled mathematical framework for modeling and solving robot decision and control tasks under uncertainty.\nOver the last decade, it has seen many successful applications, spanning localization and navigation, search and tracking, autonomous driving, multi-robot systems, manipulation, and human-robot interaction.\nThis survey aims to bridge the gap between the development of POMDP models and algorithms at one end and application to diverse robot decision tasks at the other.\nIt analyzes the characteristics of these tasks and connects them with the mathematical and algorithmic properties of the POMDP framework for effective modeling and solution.\nFor practitioners, the survey provides some of the key task characteristics in deciding when and how to apply POMDPs to robot tasks successfully.\nFor POMDP algorithm designers, the survey provides new insights into the unique challenges of applying POMDPs to robot systems and points to promising new directions for further research.",
  "reference_labels": [
    {
      "index": 0,
      "title": "FIRM: Sampling-based feedback motion-planning under motion uncertainty and imperfect measurements",
      "abstract": "",
      "year": "2014",
      "venue": "International Journal of Robotics Research",
      "authors": "A.-A. Agha-Mohammadi, S. Chakravorty, and N. M. Amato"
    },
    {
      "index": 1,
      "title": "Health aware stochastic planning for persistent package delivery missions using quadrotors",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "A.-a. Agha-mohammadi, N. K. Ure, J. P. How, and J. Vian"
    },
    {
      "index": 2,
      "title": "Safe Policy Synthesis in Multi-Agent POMDPs via Discrete-Time Barrier Functions",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Decision and Control (CDC)",
      "authors": "M. Ahmadi, A. Singletary, J. W. Burdick, and A. D. Ames",
      "orig_title": "Safe policy synthesis in multi-agent POMDPs via discrete-time barrier functions",
      "paper_id": "1903.07823v2"
    },
    {
      "index": 3,
      "title": "Policy search for multi-robot coordination under uncertainty",
      "abstract": "",
      "year": "2016",
      "venue": "International Journal of Robotics Research",
      "authors": "C. Amato, G. Konidaris, A. Anders, G. Cruz, J. P. How, and L. P. Kaelbling"
    },
    {
      "index": 4,
      "title": "Oracular partially observable Markov decision processes: A very special case",
      "abstract": "",
      "year": "2007",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "N. Armstrong-Crews and M. Veloso"
    },
    {
      "index": 5,
      "title": "Optimal control of Markov processes with incomplete state information",
      "abstract": "",
      "year": "1965",
      "venue": "Journal of Mathematical Analysis and Applications",
      "authors": "K. J. Åstrom"
    },
    {
      "index": 6,
      "title": "Nonmyopic view planning for active object classification and pose estimation",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Transactions on Robotics",
      "authors": "N. Atanasov, B. Sankaran, J. Le Ny, G. J. Pappas, and K. Daniilidis"
    },
    {
      "index": 7,
      "title": "A Bayesian reinforcement learning approach for customizing human-robot interfaces",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Intelligent User Interfaces (IUI)",
      "authors": "A. Atrash and J. Pineau"
    },
    {
      "index": 8,
      "title": "A bayesian method for learning POMDP observation parameters for robot interaction management systems",
      "abstract": "",
      "year": "2010",
      "venue": "The POMDP practitioners workshop",
      "authors": "A. Atrash and J. Pineau"
    },
    {
      "index": 9,
      "title": "Intention-aware online POMDP planning for autonomous driving in a crowd",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee"
    },
    {
      "index": 10,
      "title": "Unmanned aircraft collision avoidance using continuous-state POMDPs",
      "abstract": "",
      "year": "2011",
      "venue": "Robotics: Science and Systems",
      "authors": "H. Bai, D. Hsu, M. J. Kochenderfer, and W. S. Lee"
    },
    {
      "index": 11,
      "title": "Integrated perception and planning in the continuous space: A POMDP approach",
      "abstract": "",
      "year": "2014",
      "venue": "International Journal of Robotics Research",
      "authors": "H. Bai, D. Hsu, and W. S. Lee"
    },
    {
      "index": 12,
      "title": "Monte Carlo value iteration for continuous-state POMDPs",
      "abstract": "",
      "year": "2010",
      "venue": "9th International Workshop on the Algorithmic Foundations of Robotics (WAFR)",
      "authors": "H. Bai, D. Hsu, W. S. Lee, and V. A. Ngo"
    },
    {
      "index": 13,
      "title": "Planning how to learn",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "H. Bai, D. Hsu, and W. Lee"
    },
    {
      "index": 14,
      "title": "Unifying system health management and automated decision making",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "E. Balaban, S. B. Johnson, and M. J. Kochenderfer"
    },
    {
      "index": 15,
      "title": "Dynamic programming",
      "abstract": "",
      "year": "1966",
      "venue": "Science",
      "authors": "R. Bellman"
    },
    {
      "index": 16,
      "title": "The complexity of decentralized control of Markov decision processes",
      "abstract": "",
      "year": "2002",
      "venue": "Mathematics of operations research",
      "authors": "D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein"
    },
    {
      "index": 17,
      "title": "Dynamic programming and suboptimal control: A survey from ADP to MPC",
      "abstract": "",
      "year": "2005",
      "venue": "European Journal of Control",
      "authors": "D. P. Bertsekas"
    },
    {
      "index": 18,
      "title": "Dec-MCTS: Decentralized planning for multi-robot active perception",
      "abstract": "",
      "year": "2019",
      "venue": "International Journal of Robotics Research",
      "authors": "G. Best, O. M. Cliff, T. Patten, R. R. Mettu, and R. Fitch"
    },
    {
      "index": 19,
      "title": "Multiagent Rollout and Policy Iteration for POMDP with Application to Multi-Robot Repair Problems",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Robot Learning",
      "authors": "S. Bhattacharya, S. Kailas, S. Badyal, S. Gil, and D. Bertsekas",
      "orig_title": "Multiagent rollout and policy iteration for POMDP with application to multi-robot repair problems",
      "paper_id": "2011.04222v1"
    },
    {
      "index": 20,
      "title": "Data-driven grasp synthesis—a survey",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Transactions on Robotics",
      "authors": "J. Bohg, A. Morales, T. Asfour, and D. Kragic"
    },
    {
      "index": 21,
      "title": "Estimation of soft tissue mechanical parameters from robotic manipulation data",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE/ASME Transactions on Mechatronics",
      "authors": "P. Boonvisut and M. C. Çavuşoğlu"
    },
    {
      "index": 22,
      "title": "Solving continuous POMDPs: Value iteration with incremental learning of an efficient space representation",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "S. Brechtel, T. Gindele, and R. Dillmann"
    },
    {
      "index": 23,
      "title": "Probabilistic decision-making under uncertainty for autonomous driving using continuous POMDPs",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE International Conference on Intelligent Transportation Systems (ITSC)",
      "authors": "S. Brechtel, T. Gindele, and R. Dillmann"
    },
    {
      "index": 24,
      "title": "A survey of Monte Carlo tree search methods",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
      "authors": "C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton"
    },
    {
      "index": 25,
      "title": "Designing POMDP models of socially situated tasks",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)",
      "authors": "F. Broz, I. Nourbakhsh, and R. Simmons"
    },
    {
      "index": 26,
      "title": "Continuous-state POMDPs with hybrid dynamics",
      "abstract": "",
      "year": "2008",
      "venue": "International Symposium on Artificial Intelligence and Mathematics (ISAIM)",
      "authors": "E. Brunskill, L. P. Kaelbling, T. Lozano-Perez, and N. Roy"
    },
    {
      "index": 27,
      "title": "The factored policy-gradient planner",
      "abstract": "",
      "year": "2009",
      "venue": "Artificial Intelligence",
      "authors": "O. Buffet and D. Aberdeen"
    },
    {
      "index": 28,
      "title": "Optimal continuous state POMDP planning with semantic observations: A variational approach",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Robotics",
      "authors": "L. Burks, I. Loefgren, and N. R. Ahmed"
    },
    {
      "index": 29,
      "title": "HyP-DESPOT: A Hybrid Parallel Algorithm for Online Planning under Uncertainty",
      "abstract": "",
      "year": "2021",
      "venue": "International Journal of Robotics Research",
      "authors": "P. Cai, Y. Luo, D. Hsu, and W. S. Lee",
      "orig_title": "HyP-DESPOT: A hybrid parallel algorithm for online planning under uncertainty",
      "paper_id": "1802.06215v1"
    },
    {
      "index": 30,
      "title": "TARE: A hierarchical framework for efficiently exploring complex 3D environments",
      "abstract": "",
      "year": "2021",
      "venue": "Robotics: Science and Systems",
      "authors": "C. Cao, H. Zhu, H. Choset, and J. Zhang"
    },
    {
      "index": 31,
      "title": "Decentralized multi-robot cooperation with auctioned POMDPs",
      "abstract": "",
      "year": "2013",
      "venue": "International Journal of Robotics Research",
      "authors": "J. Capitan, M. T. Spaan, L. Merino, and A. Ollero"
    },
    {
      "index": 32,
      "title": "Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes",
      "abstract": "",
      "year": "1997",
      "venue": "13th Conference in Uncertainty in Artificial Intelligence (UAI)",
      "authors": "A. Cassandra, M. L. Littman, and N. L. Zhang"
    },
    {
      "index": 33,
      "title": "Acting optimally in partially observable stochastic domains",
      "abstract": "",
      "year": "1994",
      "venue": "12th National Conference on Artificial Intelligence (AAAI)",
      "authors": "A. R. Cassandra, L. P. Kaelbling, and M. L. Littman"
    },
    {
      "index": 34,
      "title": "Deterministic equivalents for optimizing and satisficing under chance constraints",
      "abstract": "",
      "year": "1963",
      "venue": "Operations Research",
      "authors": "A. Charnes and W. W. Cooper"
    },
    {
      "index": 35,
      "title": "Sampling-based algorithms for continuous-time POMDPs",
      "abstract": "",
      "year": "2013",
      "venue": "American Control Conference (ACC)",
      "authors": "P. Chaudhari, S. Karaman, D. Hsu, and E. Frazzoli"
    },
    {
      "index": 36,
      "title": "POMDP-lite for robust robot planning under uncertainty",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "M. Chen, E. Frazzoli, D. Hsu, and W. S. Lee"
    },
    {
      "index": 37,
      "title": "Planning with trust for human-robot collaboration",
      "abstract": "",
      "year": "2018",
      "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": "M. Chen, S. Nikolaidis, H. Soh, D. Hsu, and S. Srinivasa"
    },
    {
      "index": 38,
      "title": "Bayesian filtering: From Kalman filters to particle filters, and beyond",
      "abstract": "",
      "year": "2003",
      "venue": "Statistics",
      "authors": "Z. Chen"
    },
    {
      "index": 39,
      "title": "Distributed matroid-constrained submodular maximization for multi-robot exploration: theory and practice",
      "abstract": "",
      "year": "2019",
      "venue": "Autonomous Robots",
      "authors": "M. Corah and N. Michael"
    },
    {
      "index": 40,
      "title": "MPDM: Multipolicy decision-making in dynamic, uncertain environments for autonomous driving",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "A. G. Cunningham, E. Galceran, R. M. Eustice, and E. Olson"
    },
    {
      "index": 41,
      "title": "Bayesian reinforcement learning in continuous POMDPs with Gaussian processes",
      "abstract": "",
      "year": "2009",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "P. Dallaire, C. Besse, S. Ross, and B. Chaib-draa"
    },
    {
      "index": 42,
      "title": "RAO*: An algorithm for chance-constrained POMDP’s",
      "abstract": "",
      "year": "2016",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "P. H. de Rodrigues Quemel e Assis Santana, S. Thiébaux, and B. C. Williams"
    },
    {
      "index": 43,
      "title": "Maximum likelihood from incomplete data via the EM algorithm",
      "abstract": "",
      "year": "1977",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "A. P. Dempster, N. M. Laird, and D. B. Rubin"
    },
    {
      "index": 44,
      "title": "Simultaneous localization and mapping: part I",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Robotics and Automation Magazine",
      "authors": "H. F. Durrant-Whyte and T. Bailey"
    },
    {
      "index": 45,
      "title": "Active perception and scene modeling by planning with probabilistic 6D object poses",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "R. Eidenberger and J. Scharinger"
    },
    {
      "index": 46,
      "title": "Map-based navigation in mobile robots:: I. a review of localization strategies",
      "abstract": "",
      "year": "2003",
      "venue": "Cognitive Systems Research",
      "authors": "D. Filliat and J.-A. Meyer"
    },
    {
      "index": 47,
      "title": "Predictive control of robot velocity to avoid obstacles in dynamic environments",
      "abstract": "",
      "year": "2003",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "authors": "A. F. Foka and P. E. Trahanias"
    },
    {
      "index": 48,
      "title": "Learning to grasp under uncertainty using POMDPs",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "N. P. Garg, D. Hsu, and W. S. Lee"
    },
    {
      "index": 49,
      "title": "Online Replanning in Belief Space for Partially Observable Task and Motion Problems",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)",
      "authors": "C. R. Garrett, C. Paxton, T. Lozano-Pérez, L. P. Kaelbling, and D. Fox",
      "orig_title": "Online replanning in belief space for partially observable task and motion problems",
      "paper_id": "1911.04577v2"
    },
    {
      "index": 50,
      "title": "Bayesian Reinforcement Learning: A Survey",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Bayesian reinforcement learning: A survey",
      "paper_id": "1609.04436v1"
    },
    {
      "index": 51,
      "title": "“Searching and tracking people with cooperative mobile robots",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 52,
      "title": "“Automated model approximation for robotic navigation with POMDPs",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "“Extending the applicability of POMDP solutions to robotic tasks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "A Survey of Deep Learning Techniques for Autonomous Driving",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A survey of deep learning techniques for autonomous driving",
      "paper_id": "1910.07738v2"
    },
    {
      "index": 55,
      "title": "A Decision-theoretic Approach to Detection-based Target Search with a UAV",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“A decision-theoretic approach to detection-based target search with a UAV",
      "paper_id": "1801.01228v1"
    },
    {
      "index": 56,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 57,
      "title": "“Value-function approximations for partially observable Markov decision processes",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "“Utilizing human feedback in POMDP execution and specification",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "“Automated handwashing assistance for persons with dementia using video and a partially observable Markov decision process",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "“Efficient multi-robot search for a moving target",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "“Interactive non-prehensile manipulation for grasping via POMDPs",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "“Grasping POMDPs",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "“Robust belief-based execution of manipulation programs",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "“A POMDP treatment of vehicle-pedestrian interaction: Implicit coordination via uncertainty-aware planning",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "“Decision making for autonomous driving considering interaction and uncertain prediction of surrounding vehicles",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "“Planning in the continuous domain: A generalized belief space approach for autonomous navigation in unknown environments",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "“A formal framework for robot learning and control under model uncertainty",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "“Planning and acting in partially observable stochastic domains",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Pre-image backchaining in belief space for mobile manipulation",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "“Integrated task and motion planning in belief space",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 71,
      "title": "Differentiable Algorithm Networks for Composable Robot Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Differentiable algorithm networks for composable robot learning",
      "paper_id": "1905.11602v1"
    },
    {
      "index": 72,
      "title": "“Probabilistic roadmaps for path planning in high-dimensional configuration spaces",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "“Review of machine learning methods in soft robotics",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "“PLGRIM: Hierarchical value learning for large-scale exploration in unknown environments",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "“TAPIR: A software toolkit for approximating and adapting POMDP solutions online",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "“Reinforcement learning in robotics: A survey",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "Xavier: A robot navigation architecture based on partially observable markov decision process models. MIT Press",
      "abstract": "",
      "year": "1998",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "“No belief propagation required: Belief space planning in high-dimensional state spaces via factor graphs",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "“A comprehensive taxonomy for multi-robot task allocation",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "“Pre-and post-contact policy decomposition for planar contact manipulation under uncertainty",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "“SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "“An online POMDP solver for uncertainty planning in dynamic environment",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "“Least-squares policy iteration",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "“A survey of methods for safe human-robot interaction",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "Multi-Robot Active Information Gathering with Periodic Communication",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-robot active information gathering with periodic communication",
      "paper_id": "1703.02610v1"
    },
    {
      "index": 86,
      "title": "Multi-agent active perception with prediction rewards",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-agent active perception with prediction rewards",
      "paper_id": "2010.11835v1"
    },
    {
      "index": 87,
      "title": "“Multi-agent active information gathering in discrete and continuous-state decentralized POMDPs by policy graph improvement",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "Planning for robotic exploration based on forward simulation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Planning for robotic exploration based on forward simulation",
      "paper_id": "1502.02474v2"
    },
    {
      "index": 89,
      "title": "“Monte-Carlo Tree Search for Constrained POMDPs",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "MAGIC: Learning Macro-Actions for Online POMDP Planning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“MAGIC: learning macro-actions for online POMDP planning",
      "paper_id": "2011.03813v4"
    },
    {
      "index": 91,
      "title": "“Act to see and see to act: POMDP planning for objects search in clutter",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "“Learning policies for partially observable environments: Scaling up",
      "abstract": "",
      "year": "1995",
      "venue": "",
      "authors": ""
    },
    {
      "index": 93,
      "title": "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning",
      "paper_id": "1709.10082v3"
    },
    {
      "index": 94,
      "title": "“A survey of algorithmic methods for partially observed Markov decision processes",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“PORCA: Modeling and planning for autonomous driving among many pedestrians",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "“A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "“Coordinated multi-robot exploration under communication constraints using decentralized Markov decision processes",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "“A POMDP framework for coordinated guidance of autonomous UAVs for multitarget tracking",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "“State of the art—a survey of partially observable Markov decision processes: theory",
      "abstract": "",
      "year": "1982",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "“POMDP approach to robotized clothes separation",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "“The expectation-maximization algorithm",
      "abstract": "",
      "year": "1996",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "“From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Belief-grounded networks for accelerated robot learning under partial observability",
      "paper_id": "2010.09170v5"
    },
    {
      "index": 104,
      "title": "A concise introduction to decentralized POMDPs. Springer",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "A. Agha-mohammadi",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "A.-A. Agha-Mohammadi",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "“Planning under uncertainty for robotic tasks with mixed observability",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "“Robotic manipulation in object composition space",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "“Robotic manipulation of multiple objects as a POMDP",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "“POMDP planning under object composition uncertainty: Application to robotic manipulation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "“FIG-OP: exploring large-scale unknown environments on a fixed time budget",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "“Hierarchical POMDP framework for a robot-assisted ASD diagnostic protocol",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "“Combined task and motion planning under partial observability: An optimization-based approach",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "“Point-based value iteration: An anytime algorithm for POMDPs",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "“Anytime point-based approximations for large POMDPs",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "“Towards robotic assistants in nursing homes: Challenges and results",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "“ALVINN: An autonomous land vehicle in a neural network",
      "abstract": "",
      "year": "1988",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "“Exploiting structure to efficiently solve large scale partially observable Markov decision processes",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "“Hybrid online POMDP planning and deep reinforcement learning for safer self-driving cars",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "“Decentralized guidance control of UAVs with explicit optimization of communication",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "“Personalized robot tutoring using the assistive tutor POMDP (AT-POMDP)",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "Gaussian Processes for Machine Learning. MIT Press",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "“Mixed reality as a bidirectional communication interface for human-robot interaction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "“Modeling humans as observation providers using POMDPs",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "“Bayesian reinforcement learning in continuous POMDPs with application to robot navigation",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "“Online planning algorithms for POMDPs",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "Exploiting Submodular Value Functions for Scaling Up Active Perception",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploiting submodular value functions for scaling up active perception",
      "paper_id": "2009.09696v1"
    },
    {
      "index": 129,
      "title": "“Learning flexible",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "T. Graepel et al",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 131,
      "title": "“A survey of point-based POMDP solvers",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "“Monte-carlo planning in large POMDPs",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "Simultaneous active parameter estimation and control using sampling-based Bayesian reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Simultaneous active parameter estimation and control using sampling-based Bayesian reinforcement learning",
      "paper_id": "1707.09055v1"
    },
    {
      "index": 134,
      "title": "“The optimal control of partially observable Markov processes over a finite horizon",
      "abstract": "",
      "year": "1973",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "“Heuristic search value iteration for POMDPs",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“Point-based POMDP algorithms: Improved analysis and implementation",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "DESPOT: Online POMDP Planning with Regularization",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": "",
      "orig_title": "“DESPOT: Online POMDP planning with regularization",
      "paper_id": "1609.03250v3"
    },
    {
      "index": 138,
      "title": "“The optimal control of partially observable Markov processes",
      "abstract": "",
      "year": "1971",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "“Intention-aware autonomous driving decision-making in an uncontrolled intersection",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "“Cooperative active perception using POMDPs",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“Active cooperative perception in network robot systems using POMDPs",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Perseus: Randomized point-based value iteration for POMDPs",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Planning to see: A hierarchical approach to planning visual actions on a robot using POMDPs",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "“Multi-robot region-of-interest reconstruction with Dec-MCTS",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "“The value of inferring the internal state of traffic participants for autonomous freeway driving",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "“Online algorithms for POMDPs with continuous state",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "“Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "“MAA*: A heuristic search algorithm for solving decentralized POMDPs",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“A POMDP framework for modelling human interaction with assistive robots",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“Autonomous vehicle speed control for safe navigation of occluded pedestrian crosswalk",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "“Towards tactical lane change behavior planning for automated vehicles",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "“An online algorithm for constrained POMDPs",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "“Semi-supervised learning of decision-making models for human-robot collaboration",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "“Decision-making for bidirectional communication in sequential human-robot collaborative tasks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "“Optimizing gaze direction in a visual navigation task",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "“Efficient approximate value iteration for continuous Gaussian POMDPs",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "“UAV based target finding and tracking in GPS-denied and cluttered environments",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 158,
      "title": "“DeepIG: Multi-robot information gathering with deep reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Multi-object search using object-oriented POMDPs",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "Model-based Reinforcement Learning for Decentralized Multiagent Rendezvous",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Model-based reinforcement learning for decentralized multiagent rendezvous",
      "paper_id": "2003.06906v2"
    },
    {
      "index": 161,
      "title": "POMP: Pomcp-based Online Motion Planning for active visual search in indoor environments",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“POMP: POMCP-based Online Motion Planning for active visual search in indoor environments",
      "paper_id": "2009.08140v1"
    },
    {
      "index": 162,
      "title": "Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives",
      "paper_id": "1801.09780v2"
    },
    {
      "index": 163,
      "title": "“Anticipatory action selection for human–robot table tennis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "“A point-based MDP for robust single-lane autonomous driving behavior under uncertainties",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "“A survey of solution techniques for the partially observed Markov decision process",
      "abstract": "",
      "year": "1991",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "S. Hagaribommanahalli",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "“Online decision-making for scalable autonomous systems",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 168,
      "title": "Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning multi-robot decentralized macro-action-based policies via a centralized Q-net",
      "paper_id": "1909.08776v2"
    },
    {
      "index": 169,
      "title": "“Online planning for target object search in clutter under partial observability",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "“A review on mechanical and hydraulic system modeling of excavator manipulator system",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 171,
      "title": "“Indoor pursuit-evasion with hybrid hierarchical partially observable Markov decision processes for multi-robot systems",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "“A role-based POMDPs approach for decentralized implicit cooperation of multiple agents",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 173,
      "title": "“Active visual planning for mobile robot teams using hierarchical POMDPs",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "Multi-Resolution POMDP Planning for Multi-Object Search in 3D",
      "abstract": "",
      "year": "2029",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-resolution POMDP planning for multi-object search in 3D",
      "paper_id": "2005.02878v5"
    },
    {
      "index": 175,
      "title": "POMDP Model Learning for Human Robot Collaboration",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“POMDP model learning for human robot collaboration",
      "paper_id": "1803.11300v1"
    },
    {
      "index": 176,
      "title": "“A probabilistic planning framework for planar grasping under uncertainty",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    }
  ]
}