{
  "paper_id": "2006.10701v1",
  "title": "Deep Reinforcement Learning amidst Lifelong Non-Stationarity",
  "abstract": "Abstract\nAs humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.111Videos of our results are available at https://sites.google.com/stanford.edu/lilac/",
  "reference_labels": [
    {
      "index": 0,
      "title": "Bayesian online changepoint detection",
      "abstract": "",
      "year": "2007",
      "venue": "arXiv",
      "authors": "Ryan Prescott Adams and David JC MacKay"
    },
    {
      "index": 1,
      "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel",
      "orig_title": "Continuous adaptation via meta-learning in nonstationary and competitive environments",
      "paper_id": "1710.03641v2"
    },
    {
      "index": 2,
      "title": "Task-Free Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars",
      "orig_title": "Task-free continual learning",
      "paper_id": "1812.03596v3"
    },
    {
      "index": 3,
      "title": "Quickest Change Detection Approach to Optimal Control in Markov Decision Processes with Model Changes",
      "abstract": "",
      "year": "2017",
      "venue": "American Control Conference (ACC)",
      "authors": "Taposh Banerjee, Miao Liu, and Jonathan P How",
      "orig_title": "Quickest change detection approach to optimal control in markov decision processes with model changes",
      "paper_id": "1609.06757v2"
    },
    {
      "index": 4,
      "title": "Dual effect, certainty equivalence, and separation in stochastic control",
      "abstract": "",
      "year": "1974",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Yaakov Bar-Shalom and Edison Tse"
    },
    {
      "index": 5,
      "title": "Openai gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba"
    },
    {
      "index": 6,
      "title": "Optimizing for the Future in Non-Stationary MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Yash Chandak, Georgios Theocharous, Shiv Shankar, Sridhar Mahadevan, Martha White, and Philip S Thomas",
      "orig_title": "Optimizing for the future in non-stationary mdps",
      "paper_id": "2005.08158v4"
    },
    {
      "index": 7,
      "title": "Hidden-mode markov decision processes for nonstationary sequential decision making",
      "abstract": "",
      "year": "2000",
      "venue": "Sequence Learning",
      "authors": "Samuel PM Choi, Dit-Yan Yeung, and Nevin L Zhang"
    },
    {
      "index": 8,
      "title": "Dealing with non-stationary environments using context detection",
      "abstract": "",
      "year": "2006",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Bruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and Paulo M Engel"
    },
    {
      "index": 9,
      "title": "Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations",
      "abstract": "",
      "year": "2016",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
      "authors": "Finale Doshi-Velez and George Konidaris"
    },
    {
      "index": 10,
      "title": "RL2: Fast reinforcement learning via slow reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel"
    },
    {
      "index": 11,
      "title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
      "abstract": "",
      "year": "2002",
      "venue": "University of Massachusetts at Amherst",
      "authors": "Michael O’Gordon Duff"
    },
    {
      "index": 12,
      "title": "On-line inference for multiple changepoint problems",
      "abstract": "",
      "year": "2007",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
      "authors": "Paul Fearnhead and Zhen Liu"
    },
    {
      "index": 13,
      "title": "Dual control theory. I.",
      "abstract": "",
      "year": "1960",
      "venue": "Avtomatika i Telemekhanika",
      "authors": "AA Feldbaum"
    },
    {
      "index": 14,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 15,
      "title": "Online meta-learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine"
    },
    {
      "index": 16,
      "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "Roy Fox, Ari Pakman, and Naftali Tishby",
      "orig_title": "Taming the noise in reinforcement learning via soft updates",
      "paper_id": "1512.08562v4"
    },
    {
      "index": 17,
      "title": "A survey on concept drift adaptation",
      "abstract": "",
      "year": "2014",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "João Gama, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia"
    },
    {
      "index": 18,
      "title": "Reinforcement Learning with Deep Energy-Based Policies",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Reinforcement learning with deep energy-based policies",
      "paper_id": "1702.08165v2"
    },
    {
      "index": 19,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 20,
      "title": "Sequential decision-making under non-stationary environments via sequential change-point detection",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Emmanuel Hadoux, Aurélie Beynier, and Paul Weng"
    },
    {
      "index": 21,
      "title": "Learning Latent Dynamics for Planning from Pixels",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson",
      "orig_title": "Learning latent dynamics for planning from pixels",
      "paper_id": "1811.04551v5"
    },
    {
      "index": 22,
      "title": "Variational recurrent models for solving partially observable control tasks",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Dongqi Han, Kenji Doya, and Jun Tani"
    },
    {
      "index": 23,
      "title": "Continuous Meta-Learning without Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone",
      "orig_title": "Continuous meta-learning without tasks",
      "paper_id": "1912.08866v2"
    },
    {
      "index": 24,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural computation",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber"
    },
    {
      "index": 25,
      "title": "Deep Variational Reinforcement Learning for POMDPs",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson",
      "orig_title": "Deep variational reinforcement learning for pomdps",
      "paper_id": "1806.02426v1"
    },
    {
      "index": 26,
      "title": "Planning and acting in partially observable stochastic domains",
      "abstract": "",
      "year": "1998",
      "venue": "Artificial intelligence",
      "authors": "Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra"
    },
    {
      "index": 27,
      "title": "Recurrent experience replay in distributed reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos"
    },
    {
      "index": 28,
      "title": "Auto-encoding variational Bayes",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Diederik P Kingma and Max Welling"
    },
    {
      "index": 29,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "National Academy of Sciences",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 30,
      "title": "Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces",
      "abstract": "",
      "year": "2008",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Hanna Kurniawati, David Hsu, and Wee Sun Lee"
    },
    {
      "index": 31,
      "title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine"
    },
    {
      "index": 32,
      "title": "Bayesian Policy Optimization for Model Uncertainty",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Gilwoo Lee, Brian Hou, Aditya Mandalika, Jeongseok Lee, Sanjiban Choudhury, and Siddhartha S Srinivasa",
      "orig_title": "Bayesian policy optimization for model uncertainty",
      "paper_id": "1810.01014v2"
    },
    {
      "index": 33,
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Sergey Levine",
      "orig_title": "Reinforcement learning and control as probabilistic inference: Tutorial and review",
      "paper_id": "1805.00909v3"
    },
    {
      "index": 34,
      "title": "Gradient Episodic Memory for Continual Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "David Lopez-Paz et al.",
      "orig_title": "Gradient episodic memory for continual learning",
      "paper_id": "1706.08840v6"
    },
    {
      "index": 35,
      "title": "A simple neural attentive meta-learner",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel"
    },
    {
      "index": 36,
      "title": "Continual Lifelong Learning with Neural Networks: A Review",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Networks",
      "authors": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter",
      "orig_title": "Continual lifelong learning with neural networks: A review",
      "paper_id": "1802.07569v4"
    },
    {
      "index": 37,
      "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine",
      "orig_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
      "paper_id": "1903.08254v1"
    },
    {
      "index": 38,
      "title": "On stochastic optimal control and reinforcement learning by approximate inference",
      "abstract": "",
      "year": "2013",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
      "authors": "Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar"
    },
    {
      "index": 39,
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, and Christoph H Lampert",
      "orig_title": "icarl: Incremental classifier and representation learning",
      "paper_id": "1611.07725v2"
    },
    {
      "index": 40,
      "title": "Experience Replay for Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne",
      "orig_title": "Experience replay for continual learning",
      "paper_id": "1811.11682v2"
    },
    {
      "index": 41,
      "title": "Bayes-adaptive pomdps",
      "abstract": "",
      "year": "2008",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "Stephane Ross, Brahim Chaib-draa, and Joelle Pineau"
    },
    {
      "index": 42,
      "title": "ProMP: Proximal Meta-Policy Search",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel",
      "orig_title": "Promp: Proximal meta-policy search",
      "paper_id": "1810.06784v4"
    },
    {
      "index": 43,
      "title": "Finding approximate pomdp solutions through belief compression",
      "abstract": "",
      "year": "2005",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Nicholas Roy, Geoffrey Gordon, and Sebastian Thrun"
    },
    {
      "index": 44,
      "title": "Progressive Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell",
      "orig_title": "Progressive neural networks",
      "paper_id": "1606.04671v4"
    },
    {
      "index": 45,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov"
    },
    {
      "index": 46,
      "title": "Online learning and online convex optimization",
      "abstract": "",
      "year": "2012",
      "venue": "Foundations and Trends in Machine Learning",
      "authors": "Shai Shalev-Shwartz"
    },
    {
      "index": 47,
      "title": "Continual Learning with Deep Generative Replay",
      "abstract": "",
      "year": "2017",
      "venue": "Neural Information Processing Systems (NeurIPS)",
      "authors": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim",
      "orig_title": "Continual learning with deep generative replay",
      "paper_id": "1705.08690v3"
    },
    {
      "index": 48,
      "title": "Incremental Learning of Object Detectors without Catastrophic Forgetting",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari",
      "orig_title": "Incremental learning of object detectors without catastrophic forgetting",
      "paper_id": "1708.06977v1"
    },
    {
      "index": 49,
      "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever",
      "orig_title": "Some considerations on learning to explore via meta-reinforcement learning",
      "paper_id": "1803.01118v2"
    },
    {
      "index": 50,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Richard S Sutton and Andrew G Barto"
    },
    {
      "index": 51,
      "title": "On the role of tracking in stationary environments",
      "abstract": "",
      "year": "2007",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Richard S Sutton, Anna Koop, and David Silver"
    },
    {
      "index": 52,
      "title": "Sim-to-Real: Learning Agile Locomotion For Quadruped Robots",
      "abstract": "",
      "year": "2018",
      "venue": "Robotics: Science and Systems (RSS)",
      "authors": "Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke",
      "orig_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
      "paper_id": "1804.10332v2"
    },
    {
      "index": 53,
      "title": "Lifelong learning algorithms",
      "abstract": "",
      "year": "1998",
      "venue": "Learning to learn",
      "authors": "Sebastian Thrun"
    },
    {
      "index": 54,
      "title": "Robot trajectory optimization using approximate inference",
      "abstract": "",
      "year": "2009",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Marc Toussaint"
    },
    {
      "index": 55,
      "title": "Learning to reinforcement learn",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick"
    },
    {
      "index": 56,
      "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Robot Learning (CoRL)",
      "authors": "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine",
      "orig_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
      "paper_id": "1910.10897v2"
    },
    {
      "index": 57,
      "title": "Continual learning through synaptic intelligence",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Friedemann Zenke, Ben Poole, and Surya Ganguli"
    },
    {
      "index": 58,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey"
    },
    {
      "index": 59,
      "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)",
      "authors": "Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson",
      "orig_title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning",
      "paper_id": "1910.08348v2"
    },
    {
      "index": 60,
      "title": "Fast Context Adaptation via Meta-Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson",
      "orig_title": "Fast context adaptation via meta-learning",
      "paper_id": "1810.03642v4"
    }
  ]
}