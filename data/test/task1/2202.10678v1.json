{
  "paper_id": "2202.10678v1",
  "title": "Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning",
  "abstract": "Abstract\nIn today‚Äôs economy, it becomes important for Internet platforms to consider the sequential information design problem to align its long term interest with incentives of the gig service providers (e.g., drivers, hosts). This paper proposes a novel model of sequential information design, namely the Markov persuasion processes (MPPs).\nSpecifically, in an MPP, a sender, with informational advantage, seeks to persuade a stream of myopic receivers to take actions that maximizes the sender‚Äôs cumulative utilities in a finite horizon Markovian environment with varying prior and utility functions. Planning in MPPs thus faces the unique challenge in finding a signaling policy that is simultaneously persuasive to the myopic receivers and inducing the optimal long-term cumulative utilities of the sender. Nevertheless, in the population level where the model is known, it turns out that we can efficiently determine the optimal (resp. œµitalic-œµ\\epsilon-optimal) policy with finite (resp. infinite) states and outcomes, through a modified formulation of the Bellman equation that additionally takes persuasiveness into consideration.\nOur main technical contribution is to study the MPP under the online reinforcement learning (RL) setting, where the goal is to learn the optimal signaling policy by interacting with with the underlying MPP, without the knowledge of the sender‚Äôs utility functions, prior distributions, and the Markov transition kernels.\nFor such a problem, we design a provably efficient no-regret learning algorithm, the Optimism-Pessimism Principle for Persuasion Process (OP4), which features a novel combination of both optimism and pessimism principles. In particular, we obtain optimistic estimates of the value functions to encourage exploration under the unknown environment. Meanwhile, we additionally robustify the signaling policy with respect to the uncertainty of prior estimation to prevent receiver‚Äôs detrimental equilibrium behavior. Our algorithm enjoys sample efficiency by achieving a sublinear Tùëá\\sqrt{T}-regret upper bound. Furthermore, both our algorithm and theory can be applied to MPPs with large space of outcomes and states via function approximation, and we showcase such a success under the linear setting.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Reinforcement learning: Theory and algorithms",
      "abstract": "",
      "year": "2019",
      "venue": "CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep.",
      "authors": "Agarwal, A., Jiang, N., Kakade, S. M. and Sun, W."
    },
    {
      "index": 1,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in neural information processing systems",
      "authors": "Auer, P., Jaksch, T. and Ortner, R."
    },
    {
      "index": 2,
      "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Ayoub, A., Jia, Z., Szepesvari, C., Wang, M. and Yang, L.",
      "orig_title": "Model-based reinforcement learning with value-targeted regression",
      "paper_id": "2006.01107v1"
    },
    {
      "index": 3,
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Azar, M. G., Osband, I. and Munos, R.",
      "orig_title": "Minimax regret bounds for reinforcement learning",
      "paper_id": "1703.05449v2"
    },
    {
      "index": 4,
      "title": "Weighted sums of certain dependent random variables",
      "abstract": "",
      "year": "1967",
      "venue": "Tohoku Mathematical Journal, Second Series",
      "authors": "Azuma, K."
    },
    {
      "index": 5,
      "title": "Targeting and Signaling in Ad Auctions",
      "abstract": "",
      "year": "2018",
      "venue": "Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM",
      "authors": "Badanidiyuru, A., Bhawalkar, K. and Xu, H.",
      "orig_title": "Targeting and signaling in ad auctions",
      "paper_id": "1708.00611v3"
    },
    {
      "index": 6,
      "title": "A markovian decision process",
      "abstract": "",
      "year": "1957",
      "venue": "Indiana Univ. Math. J.",
      "authors": "Bellman, R."
    },
    {
      "index": 7,
      "title": "Information design: A unified perspective",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Economic Literature",
      "authors": "Bergemann, D. and Morris, S."
    },
    {
      "index": 8,
      "title": "Provably Efficient Exploration in Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Cai, Q., Yang, Z., Jin, C. and Wang, Z.",
      "orig_title": "Provably efficient exploration in policy optimization",
      "paper_id": "1912.05830v4"
    },
    {
      "index": 9,
      "title": "Computing the optimal strategy to commit to",
      "abstract": "",
      "year": "2006",
      "venue": "7th ACM conference on Electronic commerce",
      "authors": "Conitzer, V. and Sandholm, T."
    },
    {
      "index": 10,
      "title": "On Oracle-Efficient PAC RL with Rich Observations",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Dann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E.",
      "orig_title": "On oracle-efficient pac rl with rich observations",
      "paper_id": "1803.00606v4"
    },
    {
      "index": 11,
      "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Dann, C., Lattimore, T. and Brunskill, E.",
      "orig_title": "Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning",
      "paper_id": "1703.07710v3"
    },
    {
      "index": 12,
      "title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W. and Wang, R.",
      "orig_title": "Bilinear classes: A structural framework for provable generalization in rl",
      "paper_id": "2103.10897v3"
    },
    {
      "index": 13,
      "title": "Is a good representation sufficient for sample efficient reinforcement learning?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.03016",
      "authors": "Du, S. S., Kakade, S. M., Wang, R. and Yang, L. F."
    },
    {
      "index": 14,
      "title": "Algorithmic Bayesian Persuasion",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Computing",
      "authors": "Dughmi, S. and Xu, H.",
      "orig_title": "Algorithmic bayesian persuasion",
      "paper_id": "1503.05988v3"
    },
    {
      "index": 15,
      "title": "Beeps",
      "abstract": "",
      "year": "2017",
      "venue": "American Economic Review",
      "authors": "Ely, J. C."
    },
    {
      "index": 16,
      "title": "Dynamic information design: a simple problem on optimal sequential information disclosure",
      "abstract": "",
      "year": "2021",
      "venue": "Dynamic Games and Applications",
      "authors": "Farhadi, F. and Teneketzis, D."
    },
    {
      "index": 17,
      "title": "Parametric bandits: The generalized linear case",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Filippi, S., Cappe, O., Garivier, A. and Szepesv√°ri, C."
    },
    {
      "index": 18,
      "title": "Bayesian persuasion in sequential decision-making",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.05137",
      "authors": "Gan, J., Majumdar, R., Radanovic, G. and Singla, A."
    },
    {
      "index": 19,
      "title": "Inventory management in supply chains: a reinforcement learning approach",
      "abstract": "",
      "year": "2002",
      "venue": "International Journal of Production Economics",
      "authors": "Giannoccaro, I. and Pontrandolfo, P."
    },
    {
      "index": 20,
      "title": "Stress tests and information disclosure",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Economic Theory",
      "authors": "Goldstein, I. and Leitner, Y."
    },
    {
      "index": 21,
      "title": "Logarithmic Regret for Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "He, J., Zhou, D. and Gu, Q.",
      "orig_title": "Logarithmic regret for reinforcement learning with linear function approximation",
      "paper_id": "2011.11566v2"
    },
    {
      "index": 22,
      "title": "Contextual decision processes with low bellman rank are pac-learnable",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E."
    },
    {
      "index": 23,
      "title": "Is q-learning provably efficient?",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in neural information processing systems",
      "authors": "Jin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M. I."
    },
    {
      "index": 24,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory. PMLR",
      "authors": "Jin, C., Yang, Z., Wang, Z. and Jordan, M. I.",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation",
      "paper_id": "1907.05388v2"
    },
    {
      "index": 25,
      "title": "Bayesian persuasion",
      "abstract": "",
      "year": "2011",
      "venue": "American Economic Review",
      "authors": "Kamenica, E. and Gentzkow, M."
    },
    {
      "index": 26,
      "title": "Reinforcement learning in robotics: A survey",
      "abstract": "",
      "year": "2013",
      "venue": "The International Journal of Robotics Research",
      "authors": "Kober, J., Bagnell, J. A. and Peters, J."
    },
    {
      "index": 27,
      "title": "Bandit algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Cambridge University Press",
      "authors": "Lattimore, T. and Szepesv√°ri, C."
    },
    {
      "index": 28,
      "title": "Markovian persuasion",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.14365",
      "authors": "Lehrer, E. and Shaiderman, D."
    },
    {
      "index": 29,
      "title": "Deep reinforcement learning for dialogue generation",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.01541",
      "authors": "Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J. and Jurafsky, D."
    },
    {
      "index": 30,
      "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits",
      "abstract": "",
      "year": "2017",
      "venue": "CoRR",
      "authors": "Li, L., Lu, Y. and Zhou, D.",
      "orig_title": "Provable optimal algorithms for generalized linear contextual bandits",
      "paper_id": "1703.00048v2"
    },
    {
      "index": 31,
      "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Li, L., Lu, Y. and Zhou, D.",
      "orig_title": "Provably optimal algorithms for generalized linear contextual bandits",
      "paper_id": "1703.00048v2"
    },
    {
      "index": 32,
      "title": "Efficient Ridesharing Order Dispatching with Mean Field Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "The world wide web conference",
      "authors": "Li, M., Qin, Z., Jiao, Y., Yang, Y., Wang, J., Wang, C., Wu, G. and Ye, J.",
      "orig_title": "Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning",
      "paper_id": "1901.11454v1"
    },
    {
      "index": 33,
      "title": "An integrated reinforcement learning and centralized programming approach for online taxi dispatching",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Liang, E., Wen, K., Lam, W. H., Sumalee, A. and Zhong, R."
    },
    {
      "index": 34,
      "title": "Bayesian exploration: Incentivizing exploration in bayesian games",
      "abstract": "",
      "year": "2021",
      "venue": "Operations Research",
      "authors": "Mansour, Y., Slivkins, A., Syrgkanis, V. and Wu, Z. S."
    },
    {
      "index": 35,
      "title": "Using reinforcement learning for a large variable-dimensional inventory management problem",
      "abstract": "",
      "year": "2020",
      "venue": "Adaptive Learning Agents Workshop at AAMAS",
      "authors": "Meisheri, H., Baniwal, V., Sultana, N. N., Khadilkar, H. and Ravindran, B."
    },
    {
      "index": 36,
      "title": "Recommender systems and their ethical challenges",
      "abstract": "",
      "year": "2020",
      "venue": "Ai & Society",
      "authors": "Milano, S., Taddeo, M. and Floridi, L."
    },
    {
      "index": 37,
      "title": "A Unifying View of Optimism in Episodic Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Neu, G. and Pike-Burke, C.",
      "orig_title": "A unifying view of optimism in episodic reinforcement learning",
      "paper_id": "2007.01891v1"
    },
    {
      "index": 38,
      "title": "Generalization and exploration via randomized value functions",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Osband, I., Van Roy, B. and Wen, Z."
    },
    {
      "index": 39,
      "title": "Learning optimal strategies to commit to",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Peng, B., Shen, W., Tang, P. and Zuo, S."
    },
    {
      "index": 40,
      "title": "Markov decision processes: discrete stochastic dynamic programming",
      "abstract": "",
      "year": "2014",
      "venue": "John Wiley & Sons",
      "authors": "Puterman, M. L."
    },
    {
      "index": 41,
      "title": "Ride-hailing order dispatching at didi via reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "INFORMS Journal on Applied Analytics",
      "authors": "Qin, Z., Tang, X., Jiao, Y., Zhang, F., Xu, Z., Zhu, H. and Ye, J."
    },
    {
      "index": 42,
      "title": "Reinforcement learning for ridesharing: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE",
      "authors": "Qin, Z. T., Zhu, H. and Ye, J."
    },
    {
      "index": 43,
      "title": "Information disclosure as a means to security",
      "abstract": "",
      "year": "2015",
      "venue": "2015 International Conference on Autonomous Agents and Multiagent Systems. Citeseer",
      "authors": "Rabinovich, Z., Jiang, A. X., Jain, M. and Xu, H."
    },
    {
      "index": 44,
      "title": "Optimal dynamic information provision",
      "abstract": "",
      "year": "2017",
      "venue": "Games and Economic Behavior",
      "authors": "Renault, J., Solan, E. and Vieille, N."
    },
    {
      "index": 45,
      "title": "Worst-Case Regret Bounds for Exploration via Randomized Value Functions",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Russo, D.",
      "orig_title": "Worst-case regret bounds for exploration via randomized value functions",
      "paper_id": "1906.02870v3"
    },
    {
      "index": 46,
      "title": "Pac model-free reinforcement learning",
      "abstract": "",
      "year": "2006",
      "venue": "23rd international conference on Machine learning",
      "authors": "Strehl, A. L., Li, L., Wiewiora, E., Langford, J. and Littman, M. L."
    },
    {
      "index": 47,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Sutton, R. S. and Barto, A. G."
    },
    {
      "index": 48,
      "title": "I should not recommend it to you even if you will like it: the ethics of recommender systems",
      "abstract": "",
      "year": "2016",
      "venue": "New Review of Hypermedia and Multimedia",
      "authors": "Tang, T. Y. and Winoto, P."
    },
    {
      "index": 49,
      "title": "Provably efficient reinforcement learning with general value function approximation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Wang, R., Salakhutdinov, R. and Yang, L. F."
    },
    {
      "index": 50,
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wang, T., Zhou, D. and Gu, Q.",
      "orig_title": "Provably efficient reinforcement learning with linear function approximation under adaptivity constraints",
      "paper_id": "2101.02195v2"
    },
    {
      "index": 51,
      "title": "Optimism in reinforcement learning with generalized linear function approximation",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.04136",
      "authors": "Wang, Y., Wang, R., Du, S. S. and Krishnamurthy, A."
    },
    {
      "index": 52,
      "title": "Beyond Personalization: Social Content Recommendation for Creator Equality and Consumer Satisfaction",
      "abstract": "",
      "year": "2019",
      "venue": "25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "authors": "Xiao, W., Zhao, H., Pan, H., Song, Y., Zheng, V. W. and Yang, Q.",
      "orig_title": "Beyond personalization: Social content recommendation for creator equality and consumer satisfaction",
      "paper_id": "1905.11900v3"
    },
    {
      "index": 53,
      "title": "Exploring information asymmetry in two-stage security games",
      "abstract": "",
      "year": "2015",
      "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "authors": "Xu, H., Rabinovich, Z., Dughmi, S. and Tambe, M."
    },
    {
      "index": 54,
      "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Yang, L. and Wang, M.",
      "orig_title": "Sample-optimal parametric q-learning using linearly additive features",
      "paper_id": "1902.04779v2"
    },
    {
      "index": 55,
      "title": "Bridging exploration and general function approximation in reinforcement learning: Provably efficient kernel and neural value iterations",
      "abstract": "",
      "year": "2020",
      "venue": "CoRR",
      "authors": "Yang, Z., Jin, C., Wang, Z., Wang, M. and Jordan, M. I."
    },
    {
      "index": 56,
      "title": "Learning Near Optimal Policies with Low Inherent Bellman Error",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Zanette, A., Lazaric, A., Kochenderfer, M. and Brunskill, E.",
      "orig_title": "Learning near optimal policies with low inherent bellman error",
      "paper_id": "2003.00153v3"
    },
    {
      "index": 57,
      "title": "The ai economist: Improving equality and productivity with ai-driven tax policies",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.13332",
      "authors": "Zheng, S., Trott, A., Srinivasa, S., Naik, N., Gruesbeck, M., Parkes, D. C. and Socher, R."
    },
    {
      "index": 58,
      "title": "Nearly minimax optimal reinforcement learning for linear mixture markov decision processes",
      "abstract": "",
      "year": "2021",
      "venue": "Conference on Learning Theory. PMLR",
      "authors": "Zhou, D., Gu, Q. and Szepesvari, C."
    },
    {
      "index": 59,
      "title": "Provably efficient reinforcement learning for discounted mdps with feature mapping",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning. PMLR",
      "authors": "Zhou, D., He, J. and Gu, Q."
    },
    {
      "index": 60,
      "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
      "abstract": "",
      "year": "2021",
      "venue": "EC ‚Äô21, Association for Computing Machinery, New York, NY, USA",
      "authors": "Zu, Y., Iyer, K. and Xu, H.",
      "orig_title": "Learning to persuade on the fly: Robustness against ignorance",
      "paper_id": "2102.10156v2"
    }
  ]
}