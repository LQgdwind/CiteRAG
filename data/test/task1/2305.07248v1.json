{
  "paper_id": "2305.07248v1",
  "title": "1 INTRODUCTION",
  "abstract": "",
  "reference_labels": [
    {
      "index": 0,
      "title": "Artificial intelligence: Can seemingly collusive outcomes be avoided?",
      "abstract": "",
      "year": "2020",
      "venue": "Management Science",
      "authors": "Abada I, Lambin X"
    },
    {
      "index": 1,
      "title": "Market making and incentives design in the presence of a dark pool: A stackelberg actor–critic approach",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Baldacci B, Manziuk I, Mastrolia T, Rosenbaum M"
    },
    {
      "index": 2,
      "title": "Nonlinear programming",
      "abstract": "",
      "year": "1997",
      "venue": "Journal of the Operational Research Society",
      "authors": "Bertsekas DP"
    },
    {
      "index": 3,
      "title": "Quantile regression—opportunities and challenges from a user’s perspective",
      "abstract": "",
      "year": "2014",
      "venue": "American Journal of Epidemiology",
      "authors": "Beyerlein A"
    },
    {
      "index": 4,
      "title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation",
      "abstract": "",
      "year": "2021",
      "venue": "Operations Research",
      "authors": "Bhandari J, Russo D, Singal R",
      "orig_title": "A finite time analysis of temporal difference learning with linear function approximation",
      "paper_id": "1806.02450v2"
    },
    {
      "index": 5,
      "title": "The pricing of options and corporate liabilities",
      "abstract": "",
      "year": "1973",
      "venue": "Journal of political economy",
      "authors": "Black F, Scholes M"
    },
    {
      "index": 6,
      "title": "Risk-constrained markov decision processes",
      "abstract": "",
      "year": "2014",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Borkar V, Jain R"
    },
    {
      "index": 7,
      "title": "Stochastic approximation with two time scales",
      "abstract": "",
      "year": "1997",
      "venue": "Systems & Control Letters",
      "authors": "Borkar VS"
    },
    {
      "index": 8,
      "title": "A sensitivity formula for risk-sensitive cost and the actor–critic algorithm",
      "abstract": "",
      "year": "2001",
      "venue": "Systems & Control Letters",
      "authors": "Borkar VS"
    },
    {
      "index": 9,
      "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Cen S, Cheng C, Chen Y, Wei Y, Chi Y",
      "orig_title": "Fast global convergence of natural policy gradient methods with entropy regularization",
      "paper_id": "2007.06558v5"
    },
    {
      "index": 10,
      "title": "Algorithms for CVaR Optimization in MDPs",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chow Y, Ghavamzadeh M",
      "orig_title": "Algorithms for cvar optimization in mdps",
      "paper_id": "1406.3339v3"
    },
    {
      "index": 11,
      "title": "Risk-constrained reinforcement learning with percentile risk criteria",
      "abstract": "",
      "year": "2017",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Chow Y, Ghavamzadeh M, Janson L, Pavone M"
    },
    {
      "index": 12,
      "title": "Stochastic optimal control with dynamic, time-consistent risk constraints",
      "abstract": "",
      "year": "2013",
      "venue": "American Control Conference",
      "authors": "Chow YL, Pavone M"
    },
    {
      "index": 13,
      "title": "Dynamo: Amazon’s highly available key-value store",
      "abstract": "",
      "year": "2007",
      "venue": "ACM SIGOPS Operating Systems Review",
      "authors": "DeCandia G, Hastorun D, Jampani M, Kakulapati G, Lakshman A, Pilchin A, Sivasubramanian S, Vosshall P, Vogels W"
    },
    {
      "index": 14,
      "title": "Probability: Theory and Examples, volume 49",
      "abstract": "",
      "year": "2019",
      "venue": "Cambridge university press",
      "authors": "Durrett R"
    },
    {
      "index": 15,
      "title": "Portfolio selection",
      "abstract": "",
      "year": "2008",
      "venue": "Handbook of finance",
      "authors": "Fabozzi FJ, Markowitz HM, Gupta F"
    },
    {
      "index": 16,
      "title": "Gradient estimation",
      "abstract": "",
      "year": "2006",
      "venue": "Handbooks in Operations Research and Management Science",
      "authors": "Fu MC"
    },
    {
      "index": 17,
      "title": "Conditional monte carlo estimation of quantile sensitivities",
      "abstract": "",
      "year": "2009",
      "venue": "Management Science",
      "authors": "Fu MC, Hong LJ, Hu JQ"
    },
    {
      "index": 18,
      "title": "Can deep reinforcement learning improve inventory management? performance on lost sales, dual-sourcing, and multi-echelon problems",
      "abstract": "",
      "year": "2022",
      "venue": "Manufacturing & Service Operations Management",
      "authors": "Gijsbrechts J, Boute RN, Van Mieghem JA, Zhang DJ"
    },
    {
      "index": 19,
      "title": "Monte Carlo methods in financial engineering, volume 53",
      "abstract": "",
      "year": "2004",
      "venue": "Springer",
      "authors": "Glasserman P"
    },
    {
      "index": 20,
      "title": "Importance sampling for stochastic simulations",
      "abstract": "",
      "year": "1989",
      "venue": "Management Science",
      "authors": "Glynn PW, Iglehart DL"
    },
    {
      "index": 21,
      "title": "Computing sensitivities for distortion risk measures",
      "abstract": "",
      "year": "2021",
      "venue": "INFORMS Journal on Computing",
      "authors": "Glynn PW, Peng Y, Fu MC, Hu JQ"
    },
    {
      "index": 22,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Haarnoja T, Zhou A, Abbeel P, Levine S",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 23,
      "title": "A measure-valued differentiation approach to sensitivities of quantiles",
      "abstract": "",
      "year": "2016",
      "venue": "Mathematics of Operations Research",
      "authors": "Heidergott B, Volk-Makarewicz W"
    },
    {
      "index": 24,
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Hessel M, Modayil J, Van Hasselt H, Schaul T, Ostrovski G, Dabney W, Horgan D, Piot B, Azar M, Silver D",
      "orig_title": "Rainbow: Combining improvements in deep reinforcement learning",
      "paper_id": "1710.02298v1"
    },
    {
      "index": 25,
      "title": "Estimating quantile sensitivities",
      "abstract": "",
      "year": "2009",
      "venue": "Operations Research",
      "authors": "Hong LJ"
    },
    {
      "index": 26,
      "title": "Simulating sensitivities of conditional value at risk",
      "abstract": "",
      "year": "2009",
      "venue": "Management Science",
      "authors": "Hong LJ, Liu G"
    },
    {
      "index": 27,
      "title": "A stochastic approximation method for simulation-based quantile optimization",
      "abstract": "",
      "year": "2022",
      "venue": "INFORMS Journal on Computing",
      "authors": "Hu J, Peng Y, Zhang G, Zhang Q"
    },
    {
      "index": 28,
      "title": "Risk-Averse Approximate Dynamic Programming with Quantile-Based Risk Measures",
      "abstract": "",
      "year": "2018",
      "venue": "Mathematics of Operations Research",
      "authors": "Jiang DR, Powell WB",
      "orig_title": "Risk-averse approximate dynamic programming with quantile-based risk measures",
      "paper_id": "1509.01920v4"
    },
    {
      "index": 29,
      "title": "On estimating quantile sensitivities via infinitesimal perturbation analysis",
      "abstract": "",
      "year": "2015",
      "venue": "Operations Research",
      "authors": "Jiang G, Fu MC"
    },
    {
      "index": 30,
      "title": "Quantile-Based Policy Optimization for Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Winter Simulation Conference",
      "authors": "Jiang J, Hu J, Peng Y",
      "orig_title": "Quantile-based policy optimization for reinforcement learning",
      "paper_id": "2201.11463v2"
    },
    {
      "index": 31,
      "title": "Stochastic differential equations",
      "abstract": "",
      "year": "1992",
      "venue": "Springer",
      "authors": "Kloeden PE, Platen E, Kloeden PE, Platen E"
    },
    {
      "index": 32,
      "title": "Stochastic Approximation and Recursive Algorithms and Applications, volume 35",
      "abstract": "",
      "year": "2003",
      "venue": "Springer Science & Business Media",
      "authors": "Kushner H, Yin GG"
    },
    {
      "index": 33,
      "title": "End-to-End Training of Deep Visuomotor Policies",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Levine S, Finn C, Darrell T, Abbeel P",
      "orig_title": "End-to-end training of deep visuomotor policies",
      "paper_id": "1504.00702v5"
    },
    {
      "index": 34,
      "title": "Quantile markov decision processes",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Li X, Zhong H, Brandeau ML"
    },
    {
      "index": 35,
      "title": "Probleme General de la Stabilite du Mouvement.(AM-17), Volume 17",
      "abstract": "",
      "year": "2016",
      "venue": "Princeton University Press",
      "authors": "Liapounoff AM"
    },
    {
      "index": 36,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Silver D, Wierstra D"
    },
    {
      "index": 37,
      "title": "Kernel estimation of quantile sensitivities",
      "abstract": "",
      "year": "2009",
      "venue": "Naval Research Logistics (NRL)",
      "authors": "Liu G, Hong LJ"
    },
    {
      "index": 38,
      "title": "Experimental results for gradient estimation and optimization of a markov chain in steady-state",
      "abstract": "",
      "year": "1992",
      "venue": "Simulation and Optimization",
      "authors": "L’Ecuyer P, Giroux N, Glynn PW"
    },
    {
      "index": 39,
      "title": "On the sub-gaussianity of the beta and dirichlet distributions",
      "abstract": "",
      "year": "2017",
      "venue": "Electronic Communications in Probability",
      "authors": "Marchal O, Arbel J"
    },
    {
      "index": 40,
      "title": "Option pricing when underlying stock returns are discontinuous",
      "abstract": "",
      "year": "1976",
      "venue": "Journal of financial economics",
      "authors": "Merton RC"
    },
    {
      "index": 41,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al."
    },
    {
      "index": 42,
      "title": "Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms",
      "abstract": "",
      "year": "2006",
      "venue": "The Annals of Applied Probability",
      "authors": "Mokkadem A, Pelletier M"
    },
    {
      "index": 43,
      "title": "Manufacturing productivity with worker turnover",
      "abstract": "",
      "year": "2022",
      "venue": "Management Science",
      "authors": "Moon K, Bergemann P, Brown D, Chen A, Chu J, Eisen EA, Fischer GM, Loyalka P, Rho S, Cohen J"
    },
    {
      "index": 44,
      "title": "An approximate solution method for large risk-averse markov decision processes",
      "abstract": "",
      "year": "2012",
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": "Petrik M, Subramanian D"
    },
    {
      "index": 45,
      "title": "Risk-sensitive reinforcement learning via policy gradient search",
      "abstract": "",
      "year": "2022",
      "venue": "Foundations and Trends® in Machine Learning",
      "authors": "Prashanth L, Fu MC, et al."
    },
    {
      "index": 46,
      "title": "Actor-critic algorithms for risk-sensitive mdps",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Prashanth L, Ghavamzadeh M"
    },
    {
      "index": 47,
      "title": "Cumulative prospect theory meets reinforcement learning: Prediction and control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Prashanth L, Jie C, Fu M, Marcus S, Szepesvári C"
    },
    {
      "index": 48,
      "title": "Scalable Reinforcement Learning for Multi-Agent Networked Systems",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Qu G, Wierman A, Li N",
      "orig_title": "Scalable reinforcement learning for multiagent networked systems",
      "paper_id": "1912.02906v3"
    },
    {
      "index": 49,
      "title": "Risk and utility in the duality framework of convex analysis",
      "abstract": "",
      "year": "2020",
      "venue": "Jonathan M. Borwein Commemorative Conference",
      "authors": "Rockafellar RT"
    },
    {
      "index": 50,
      "title": "Linear System Theory",
      "abstract": "",
      "year": "1996",
      "venue": "Prentice-Hall, Inc.",
      "authors": "Rugh WJ"
    },
    {
      "index": 51,
      "title": "Risk-averse dynamic programming for markov decision processes",
      "abstract": "",
      "year": "2010",
      "venue": "Mathematical Programming",
      "authors": "Ruszczyński A"
    },
    {
      "index": 52,
      "title": "Trust Region Policy Optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Schulman J, Levine S, Abbeel P, Jordan M, Moritz P",
      "orig_title": "Trust region policy optimization",
      "paper_id": "1502.05477v5"
    },
    {
      "index": 53,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1707.06347",
      "authors": "Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O"
    },
    {
      "index": 54,
      "title": "Mastering the game of go with deep neural networks and tree search",
      "abstract": "",
      "year": "2016",
      "venue": "Nature",
      "authors": "Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al."
    },
    {
      "index": 55,
      "title": "Adaptive Discretization in Online Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Sinclair SR, Banerjee S, Yu CL",
      "orig_title": "Adaptive discretization in online reinforcement learning",
      "paper_id": "2110.15843v3"
    },
    {
      "index": 56,
      "title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation",
      "abstract": "",
      "year": "1992",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Spall JC"
    },
    {
      "index": 57,
      "title": "Policy gradients beyond expectations: Conditional value-at-risk",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1404.3862",
      "authors": "Tamar A, Glassner Y, Mannor S"
    },
    {
      "index": 58,
      "title": "High-Dimensional Probability: An Introduction with Applications in Data Science, volume 47",
      "abstract": "",
      "year": "2018",
      "venue": "Cambridge university press",
      "authors": "Vershynin R"
    },
    {
      "index": 59,
      "title": "Reliable off-policy evaluation for reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "Operations Research",
      "authors": "Wang J, Gao R, Zha H"
    },
    {
      "index": 60,
      "title": "Deep reinforcement learning for sequential targeting",
      "abstract": "",
      "year": "2022",
      "venue": "Management Science",
      "authors": "Wang W, Li B, Luo X, Wang X"
    },
    {
      "index": 61,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Machine Learning",
      "authors": "Williams RJ"
    },
    {
      "index": 62,
      "title": "Drn: A deep reinforcement learning framework for news recommendation",
      "abstract": "",
      "year": "2018",
      "venue": "World Wide Web Conference",
      "authors": "Zheng G, Zhang F, Zheng Z, Xiang Y, Yuan NJ, Xie X, Li Z"
    }
  ]
}