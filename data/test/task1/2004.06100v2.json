{
  "paper_id": "2004.06100v2",
  "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
  "abstract": "Abstract\nAlthough pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "abstract": "",
      "year": "2007",
      "venue": "ACL",
      "authors": "John Blitzer, Mark Dredze, and Fernando Pereira"
    },
    {
      "index": 1,
      "title": "A large annotated corpus for learning natural language inference",
      "abstract": "",
      "year": "2015",
      "venue": "EMNLP",
      "authors": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning",
      "orig_title": "A large annotated corpus for learning natural language inference",
      "paper_id": "1508.05326v1"
    },
    {
      "index": 2,
      "title": "Pay attention to the ending: Strong neural baselines for the roc story cloze task",
      "abstract": "",
      "year": "2017",
      "venue": "ACL",
      "authors": "Zheng Cai, Lifu Tu, and Kevin Gimpel"
    },
    {
      "index": 3,
      "title": "Deep Weighted Averaging Classifiers",
      "abstract": "",
      "year": "2018",
      "venue": "FAT",
      "authors": "Dallas Card, Michael Zhang, and Noah A. Smith",
      "orig_title": "Deep weighted averaging classifiers",
      "paper_id": "1811.02579v2"
    },
    {
      "index": 4,
      "title": "SemEval-2017 Task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "SemEval",
      "authors": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia"
    },
    {
      "index": 5,
      "title": "Simple and Effective Multi-Paragraph Reading Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Christopher Clark and Matt Gardner",
      "orig_title": "Simple and effective multi-paragraph reading comprehension",
      "paper_id": "1710.10723v2"
    },
    {
      "index": 6,
      "title": "The PASCAL recognising textual entailment challenge",
      "abstract": "",
      "year": "2005",
      "venue": "Machine Learning Challenges Workshop",
      "authors": "Ido Dagan, Oren Glickman, and Bernardo Magnini"
    },
    {
      "index": 7,
      "title": "Frustratingly easy domain adaptation",
      "abstract": "",
      "year": "2007",
      "venue": "ACL",
      "authors": "Hal Daumé III"
    },
    {
      "index": 8,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL-HLT",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 9,
      "title": "Multi30K: Multilingual English-German Image Descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "ACL",
      "authors": "Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia",
      "orig_title": "Multi30k: Multilingual english-german image descriptions",
      "paper_id": "1605.00459v1"
    },
    {
      "index": 10,
      "title": "A Meta-Analysis of the Anomaly Detection Problem",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Andrew Emmott, Shubhomoy Das, Thomas G. Dietterich, Alan Fern, and Weng-Keen Wong",
      "orig_title": "A meta-analysis of the anomaly detection problem",
      "paper_id": "1503.01158v2"
    },
    {
      "index": 11,
      "title": "Misleading failures of partial-input baselines",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Shi Feng, Eric Wallace, and Jordan Boyd-Graber"
    },
    {
      "index": 12,
      "title": "Pathologies of Neural Models Make Interpretations Difficult",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "Shi Feng, Eric Wallace, II Grissom, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber",
      "orig_title": "Pathologies of neural models make interpretations difficult",
      "paper_id": "1804.07781v3"
    },
    {
      "index": 13,
      "title": "Proceedings of the 2nd workshop on machine reading for question answering",
      "abstract": "",
      "year": "2019",
      "venue": "MRQA Workshop",
      "authors": "Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen"
    },
    {
      "index": 14,
      "title": "Cross-Domain Generalization of Neural Constituency Parsers",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Daniel Fried, Nikita Kitaev, and Dan Klein",
      "orig_title": "Cross-domain generalization of neural constituency parsers",
      "paper_id": "1907.04347v1"
    },
    {
      "index": 15,
      "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov",
      "orig_title": "Compressing large-scale transformer-based models: A case study on BERT",
      "paper_id": "2002.11985v2"
    },
    {
      "index": 16,
      "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
      "abstract": "",
      "year": "2018",
      "venue": "Workshop for NLP Open Source Software",
      "authors": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer",
      "orig_title": "AllenNLP: a deep semantic natural language processing platform",
      "paper_id": "1803.07640v2"
    },
    {
      "index": 17,
      "title": "Motivating the Rules of the Game for Adversarial Example Research",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David Andersen, and George E. Dahl",
      "orig_title": "Motivating the rules of the game for adversarial example research",
      "paper_id": "1807.06732v2"
    },
    {
      "index": 18,
      "title": "Annotation Artifacts in Natural Language Inference Data",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith",
      "orig_title": "Annotation artifacts in natural language inference data",
      "paper_id": "1803.02324v2"
    },
    {
      "index": 19,
      "title": "Distributional structure",
      "abstract": "",
      "year": "1954",
      "venue": "Word",
      "authors": "Zellig S Harris"
    },
    {
      "index": 20,
      "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "abstract": "",
      "year": "2016",
      "venue": "WWW",
      "authors": "Ruining He and Julian J. McAuley"
    },
    {
      "index": 21,
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Dan Hendrycks and Thomas Dietterich",
      "orig_title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "paper_id": "1903.12261v1"
    },
    {
      "index": 22,
      "title": "Gaussian Error Linear Units (GELUs)",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint",
      "authors": "Dan Hendrycks and Kevin Gimpel",
      "orig_title": "Gaussian error linear units (GELUs)",
      "paper_id": "1606.08415v5"
    },
    {
      "index": 23,
      "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Dan Hendrycks and Kevin Gimpel",
      "orig_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "paper_id": "1610.02136v3"
    },
    {
      "index": 24,
      "title": "Using pre-training can improve model robustness and uncertainty",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Dan Hendrycks, Kimin Lee, and Mantas Mazeika"
    },
    {
      "index": 25,
      "title": "Deep Anomaly Detection with Outlier Exposure",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich",
      "orig_title": "Deep anomaly detection with outlier exposure",
      "paper_id": "1812.04606v3"
    },
    {
      "index": 26,
      "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song",
      "orig_title": "Using self-supervised learning can improve model robustness and uncertainty",
      "paper_id": "1906.12340v2"
    },
    {
      "index": 27,
      "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan",
      "orig_title": "AugMix: A simple data processing method to improve robustness and uncertainty",
      "paper_id": "1912.02781v2"
    },
    {
      "index": 28,
      "title": "Natural Adversarial Examples",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song",
      "orig_title": "Natural adversarial examples",
      "paper_id": "1907.07174v4"
    },
    {
      "index": 29,
      "title": "Long short-term memory",
      "abstract": "",
      "year": "1997",
      "venue": "Neural Computation",
      "authors": "Sepp Hochreiter and Jürgen Schmidhuber"
    },
    {
      "index": 30,
      "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
      "abstract": "",
      "year": "2017",
      "venue": "EMNLP",
      "authors": "Robin Jia and Percy Liang",
      "orig_title": "Adversarial examples for evaluating reading comprehension systems",
      "paper_id": "1707.07328v1"
    },
    {
      "index": 31,
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",
      "orig_title": "ALBERT: a lite BERT for self-supervised learning of language representations",
      "paper_id": "1909.11942v6"
    },
    {
      "index": 32,
      "title": "NewsWeeder: Learning to filter Netnews",
      "abstract": "",
      "year": "1995",
      "venue": "ICML",
      "authors": "Ken Lang"
    },
    {
      "index": 33,
      "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin",
      "orig_title": "Training confidence-calibrated classifiers for detecting out-of-distribution samples",
      "paper_id": "1711.09325v3"
    },
    {
      "index": 34,
      "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E Gonzalez",
      "orig_title": "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
      "paper_id": "2002.11794v2"
    },
    {
      "index": 35,
      "title": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Nelson F Liu, Roy Schwartz, and Noah A Smith",
      "orig_title": "Inoculation by fine-tuning: A method for analyzing challenge datasets",
      "paper_id": "1904.02668v4"
    },
    {
      "index": 36,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov",
      "orig_title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 37,
      "title": "Learning word vectors for sentiment analysis",
      "abstract": "",
      "year": "2011",
      "venue": "ACL",
      "authors": "Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts"
    },
    {
      "index": 38,
      "title": "Image-based Recommendations on Styles and Substitutes",
      "abstract": "",
      "year": "2015",
      "venue": "SIGIR",
      "authors": "Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel",
      "orig_title": "Image-based recommendations on styles and substitutes",
      "paper_id": "1506.04757v1"
    },
    {
      "index": 39,
      "title": "Distributed representations of words and phrases and their compositionality",
      "abstract": "",
      "year": "2013",
      "venue": "NIPS",
      "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean"
    },
    {
      "index": 40,
      "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer",
      "orig_title": "Compositional questions do not necessitate multi-hop reasoning",
      "paper_id": "1906.02900v1"
    },
    {
      "index": 41,
      "title": "Stress Test Evaluation for Natural Language Inference",
      "abstract": "",
      "year": "2018",
      "venue": "COLING",
      "authors": "Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig",
      "orig_title": "Stress test evaluation for natural language inference",
      "paper_id": "1806.00692v3"
    },
    {
      "index": 42,
      "title": "Robustness properties of Facebook’s ResNeXt WSL models",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "A. Emin Orhan",
      "orig_title": "Robustness properties of facebook’s ResNeXt WSL models",
      "paper_id": "1907.07640v5"
    },
    {
      "index": 43,
      "title": "GloVe: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning"
    },
    {
      "index": 44,
      "title": "Dataset shift in machine learning",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence"
    },
    {
      "index": 45,
      "title": "Semantically equivalent adversarial rules for debugging NLP models",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin"
    },
    {
      "index": 46,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS EMC2 Workshop",
      "authors": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf",
      "orig_title": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 47,
      "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer",
      "orig_title": "Q-BERT: Hessian based ultra low precision quantization of BERT",
      "paper_id": "1909.05840v2"
    },
    {
      "index": 48,
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "abstract": "",
      "year": "2013",
      "venue": "EMNLP",
      "authors": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts"
    },
    {
      "index": 49,
      "title": "Unbiased look at dataset bias",
      "abstract": "",
      "year": "2011",
      "venue": "CVPR",
      "authors": "Antonio Torralba and Alexei A. Efros"
    },
    {
      "index": 50,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 51,
      "title": "Universal adversarial triggers for attacking and analyzing NLP",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matthew Gardner, and Sameer Singh"
    },
    {
      "index": 52,
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman",
      "orig_title": "GLUE: A multitask benchmark and analysis platform for natural language understanding",
      "paper_id": "1804.07461v3"
    },
    {
      "index": 53,
      "title": "Towards universal paraphrastic sentence embeddings",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu"
    },
    {
      "index": 54,
      "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
      "abstract": "",
      "year": "2018",
      "venue": "NAACL-HLT",
      "authors": "Adina Williams, Nikita Nangia, and Samuel R Bowman",
      "orig_title": "A broad-coverage challenge corpus for sentence understanding through inference",
      "paper_id": "1704.05426v4"
    },
    {
      "index": 55,
      "title": "HuggingFace’s Transformers: State-of-the-art natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew"
    },
    {
      "index": 56,
      "title": "Intriguing properties of adversarial training at scale",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Cihang Xie and Alan L. Yuille"
    },
    {
      "index": 57,
      "title": "Self-training with noisy student improves imagenet classification",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le"
    },
    {
      "index": 58,
      "title": "Learning and Evaluating General Linguistic Intelligence",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomás Kociský, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom",
      "orig_title": "Learning and evaluating general linguistic intelligence",
      "paper_id": "1901.11373v1"
    },
    {
      "index": 59,
      "title": "ReCoRD: bridging the gap between human and machine commonsense reading comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme"
    }
  ]
}