{
  "paper_id": "2107.06912v3",
  "title": "From Show to Tell: A Survey on Deep Learning-based Image Captioning",
  "abstract": "Abstract\nConnecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.",
  "reference_labels": [
    {
      "index": 0,
      "title": "“Language and visual perception associations: meta-analytic connectivity modeling of Brodmann Area 37",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Language and visual perception associations: meta-analytic connectivity modeling of Brodmann Area 37",
      "abstract": "",
      "year": "2015",
      "venue": "Behavioural Neurology",
      "authors": "A. Ardila, B. Bernal, and M. Rosselli"
    },
    {
      "index": 2,
      "title": "Automatic image captioning",
      "abstract": "",
      "year": "2004",
      "venue": "ICME",
      "authors": "J.-Y. Pan, H.-J. Yang, P. Duygulu, and C. Faloutsos"
    },
    {
      "index": 3,
      "title": "Every picture tells a story: Generating sentences from images",
      "abstract": "",
      "year": "2010",
      "venue": "ECCV",
      "authors": "A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth"
    },
    {
      "index": 4,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "NeurIPS",
      "authors": "V. Ordonez, G. Kulkarni, and T. Berg"
    },
    {
      "index": 5,
      "title": "DeViSE: a deep visual-semantic embedding model",
      "abstract": "",
      "year": "2013",
      "venue": "NeurIPS",
      "authors": "A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov"
    },
    {
      "index": 6,
      "title": "Unifying visual-semantic embeddings with multimodal neural language models",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS Workshops",
      "authors": "R. Kiros, R. Salakhutdinov, and R. S. Zemel"
    },
    {
      "index": 7,
      "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "A. Karpathy, A. Joulin, and L. Fei-Fei",
      "orig_title": "Deep fragment embeddings for bidirectional image sentence mapping",
      "paper_id": "1406.5679v1"
    },
    {
      "index": 8,
      "title": "I2T: Image parsing to text description",
      "abstract": "",
      "year": "2010",
      "venue": "IEEE",
      "authors": "B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu"
    },
    {
      "index": 9,
      "title": "Generating image descriptions using dependency relational patterns",
      "abstract": "",
      "year": "2010",
      "venue": "ACL",
      "authors": "A. Aker and R. Gaizauskas"
    },
    {
      "index": 10,
      "title": "Corpus-guided sentence generation of natural images",
      "abstract": "",
      "year": "2011",
      "venue": "EMNLP",
      "authors": "Y. Yang, C. Teo, H. Daumé III, and Y. Aloimonos"
    },
    {
      "index": 11,
      "title": "Composing simple image descriptions using web-scale n-grams",
      "abstract": "",
      "year": "2011",
      "venue": "CoNLL",
      "authors": "S. Li, G. Kulkarni, T. Berg, A. Berg, and Y. Choi"
    },
    {
      "index": 12,
      "title": "Choosing linguistics over vision to describe images",
      "abstract": "",
      "year": "2012",
      "venue": "AAAI",
      "authors": "A. Gupta, Y. Verma, and C. Jawahar"
    },
    {
      "index": 13,
      "title": "Midge: Generating image descriptions from computer vision detections",
      "abstract": "",
      "year": "2012",
      "venue": "ACL",
      "authors": "M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, X. Han, A. Mensch, A. Berg, T. Berg, and H. Daumé III"
    },
    {
      "index": 14,
      "title": "BabyTalk: Understanding and generating simple image descriptions",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE Trans. PAMI",
      "authors": "G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg"
    },
    {
      "index": 15,
      "title": "Treetalk: Composition and compression of trees for image descriptions",
      "abstract": "",
      "year": "2014",
      "venue": "TACL",
      "authors": "P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi"
    },
    {
      "index": 16,
      "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures",
      "abstract": "",
      "year": "2016",
      "venue": "JAIR",
      "authors": "R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis, F. Keller, A. Muscat, and B. Plank",
      "orig_title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures",
      "paper_id": "1601.03896v2"
    },
    {
      "index": 17,
      "title": "A survey on automatic image caption generation",
      "abstract": "",
      "year": "2018",
      "venue": "Neurocomputing",
      "authors": "S. Bai and S. An"
    },
    {
      "index": 18,
      "title": "A Comprehensive Survey of Deep Learning for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "ACM Computing Surveys",
      "authors": "M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga",
      "orig_title": "A Comprehensive Survey of Deep Learning for Image Captioning",
      "paper_id": "1810.04020v2"
    },
    {
      "index": 19,
      "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
      "abstract": "",
      "year": "2013",
      "venue": "JAIR",
      "authors": "M. Hodosh, P. Young, and J. Hockenmaier"
    },
    {
      "index": 20,
      "title": "Vision to Language: Methods, Metrics and Datasets",
      "abstract": "",
      "year": "2020",
      "venue": "Machine Learning Paradigms",
      "authors": "N. Sharif, U. Nadeem, S. A. A. Shah, M. Bennamoun, and W. Liu"
    },
    {
      "index": 21,
      "title": "A survey on deep neural network-based image captioning",
      "abstract": "",
      "year": "2019",
      "venue": "The Visual Computer",
      "authors": "X. Liu, Q. Xu, and N. Wang"
    },
    {
      "index": 22,
      "title": "Image captioning: a comprehensive survey",
      "abstract": "",
      "year": "2020",
      "venue": "PARC",
      "authors": "H. Sharma, M. Agrahari, S. K. Singh, M. Firoj, and R. K. Mishra"
    },
    {
      "index": 23,
      "title": "Show and Tell: A Neural Image Caption Generator",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "O. Vinyals, A. Toshev, S. Bengio, and D. Erhan",
      "orig_title": "Show and tell: A neural image caption generator",
      "paper_id": "1411.4555v2"
    },
    {
      "index": 24,
      "title": "Going deeper with convolutions",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich",
      "orig_title": "Going deeper with convolutions",
      "paper_id": "1409.4842v1"
    },
    {
      "index": 25,
      "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "A. Karpathy and L. Fei-Fei",
      "orig_title": "Deep visual-semantic alignments for generating image descriptions",
      "paper_id": "1412.2306v2"
    },
    {
      "index": 26,
      "title": "Imagenet classification with deep convolutional neural networks",
      "abstract": "",
      "year": "2012",
      "venue": "NeurIPS",
      "authors": "A. Krizhevsky, I. Sutskever, and G. E. Hinton"
    },
    {
      "index": 27,
      "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille",
      "orig_title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
      "paper_id": "1412.6632v5"
    },
    {
      "index": 28,
      "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell",
      "orig_title": "Long-term recurrent convolutional networks for visual recognition and description",
      "paper_id": "1411.4389v4"
    },
    {
      "index": 29,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2015",
      "venue": "ICLR",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 30,
      "title": "Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "X. Chen and C. Lawrence Zitnick"
    },
    {
      "index": 31,
      "title": "From captions to visual concepts and back",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell, J. C. Platt et al."
    },
    {
      "index": 32,
      "title": "Guiding the Long-Short Term Memory model for Image Caption Generation",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars"
    },
    {
      "index": 33,
      "title": "Image Captioning with Semantic Attention",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo",
      "orig_title": "Image captioning with semantic attention",
      "paper_id": "1603.03925v1"
    },
    {
      "index": 34,
      "title": "What Value Do Explicit High Level Concepts Have in Vision to Language Problems?",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Q. Wu, C. Shen, L. Liu, A. Dick, and A. Van Den Hengel"
    },
    {
      "index": 35,
      "title": "An Empirical Study of Language CNN for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "J. Gu, G. Wang, J. Cai, and T. Chen",
      "orig_title": "An Empirical Study of Language CNN for Image Captioning",
      "paper_id": "1612.07086v3"
    },
    {
      "index": 36,
      "title": "StructCap: Structured Semantic Embedding for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "ACM Multimedia",
      "authors": "F. Chen, R. Ji, J. Su, Y. Wu, and Y. Wu"
    },
    {
      "index": 37,
      "title": "GroupCap: Group-based Image Captioning with Structured Relevance and Diversity Constraints",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "F. Chen, R. Ji, X. Sun, Y. Wu, and J. Su"
    },
    {
      "index": 38,
      "title": "Self-critical Sequence Training for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel",
      "orig_title": "Self-critical sequence training for image captioning",
      "paper_id": "1612.00563v2"
    },
    {
      "index": 39,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 40,
      "title": "Boosting Image Captioning with Attributes",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei",
      "orig_title": "Boosting image captioning with attributes",
      "paper_id": "1611.01646v1"
    },
    {
      "index": 41,
      "title": "Semantic Compositional Networks for Visual Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Z. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao, L. Carin, and L. Deng",
      "orig_title": "Semantic Compositional Networks for Visual Captioning",
      "paper_id": "1611.08002v2"
    },
    {
      "index": 42,
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio"
    },
    {
      "index": 43,
      "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "J. Lu, C. Xiong, D. Parikh, and R. Socher"
    },
    {
      "index": 44,
      "title": "Rethinking the Form of Latent States in Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "ECCV",
      "authors": "B. Dai, D. Ye, and D. Lin",
      "orig_title": "Rethinking the form of latent states in image captioning",
      "paper_id": "1807.09958v1"
    },
    {
      "index": 45,
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "",
      "year": "2014",
      "venue": "ICLR",
      "authors": "D. Bahdanau, K. Cho, and Y. Bengio",
      "orig_title": "Neural machine translation by jointly learning to align and translate",
      "paper_id": "1409.0473v7"
    },
    {
      "index": 46,
      "title": "Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "X. Chen, L. Ma, W. Jiang, J. Yao, and W. Liu",
      "orig_title": "Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present",
      "paper_id": "1803.11439v2"
    },
    {
      "index": 47,
      "title": "Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Y. Wang, Z. Lin, X. Shen, S. Cohen, and G. W. Cottrell"
    },
    {
      "index": 48,
      "title": "Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "H. Ge, Z. Yan, K. Zhang, M. Zhao, and L. Sun",
      "orig_title": "Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style",
      "paper_id": "1910.06475v1"
    },
    {
      "index": 49,
      "title": "Stack-Captioning: Coarse-to-Fine Learning for Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "J. Gu, J. Cai, G. Wang, and T. Chen"
    },
    {
      "index": 50,
      "title": "Review Networks for Caption Generation",
      "abstract": "",
      "year": "2016",
      "venue": "NeurIPS",
      "authors": "Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. R. Salakhutdinov",
      "orig_title": "Review Networks for Caption Generation",
      "paper_id": "1605.07912v4"
    },
    {
      "index": 51,
      "title": "Recurrent Fusion Network for Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Recurrent Fusion Network for Image Captioning",
      "paper_id": "1807.09986v3"
    },
    {
      "index": 52,
      "title": "“Seeing with Humans: Gaze-Assisted Neural Image Captioning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "“Paying attention to descriptions generated by image captioning models",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "Top-down Visual Saliency Guided by Captions",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Top-down visual saliency guided by captions",
      "paper_id": "1612.07360v2"
    },
    {
      "index": 55,
      "title": "Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention",
      "paper_id": "1706.08474v4"
    },
    {
      "index": 56,
      "title": "“Boosted attention: Leveraging human attention for image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Bottom-up and top-down attention for image captioning and visual question answering",
      "paper_id": "1707.07998v3"
    },
    {
      "index": 58,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Faster R-CNN: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 59,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Faster R-CNN: towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 60,
      "title": "“Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "Reflective Decoding Network for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Reflective Decoding Network for Image Captioning",
      "paper_id": "1908.11824v1"
    },
    {
      "index": 62,
      "title": "“Look Back and Predict Forward in Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "Adaptively Aligned Image Captioning via Adaptive Attention Time",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Adaptively Aligned Image Captioning via Adaptive Attention Time",
      "paper_id": "1909.09060v3"
    },
    {
      "index": 64,
      "title": "and Tell: Image Captioning with Recall Mechanism",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "Context-Aware Visual Policy Network for Fine-Grained Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Context-aware visual policy network for fine-grained image captioning",
      "paper_id": "1906.02365v1"
    },
    {
      "index": 66,
      "title": "“Areas of Attention for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "Exploring Visual Relationship for Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring Visual Relationship for Image Captioning",
      "paper_id": "1809.07041v1"
    },
    {
      "index": 68,
      "title": "Aligning Linguistic Words and Visual Semantic Units for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Aligning linguistic words and visual semantic units for image captioning",
      "paper_id": "1908.02127v1"
    },
    {
      "index": 69,
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Semi-supervised classification with graph convolutional networks",
      "paper_id": "1609.02907v4"
    },
    {
      "index": 70,
      "title": "Auto-Encoding Scene Graphs for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Auto-Encoding Scene Graphs for Image Captioning",
      "paper_id": "1812.02378v3"
    },
    {
      "index": 71,
      "title": "“Improving Image Captioning with Better Use of Captions",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Hierarchy Parsing for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 74,
      "title": "Learning to Collocate Neural Modules for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to Collocate Neural Modules for Image Captioning",
      "paper_id": "1904.08608v1"
    },
    {
      "index": 75,
      "title": "“Entangled Transformer for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "Image Captioning: Transforming Objects into Words",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Image Captioning: Transforming Objects into Words",
      "paper_id": "1906.05963v2"
    },
    {
      "index": 77,
      "title": "Normalized and Geometry-Aware Self-Attention Network for Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Normalized and Geometry-Aware Self-Attention Network for Image Captioning",
      "paper_id": "2003.08897v1"
    },
    {
      "index": 78,
      "title": "“Attention on Attention for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "“X-Linear Attention Networks for Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "Meshed-Memory Transformer for Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Meshed-Memory Transformer for Image Captioning",
      "paper_id": "1912.08226v2"
    },
    {
      "index": 81,
      "title": "Image Captioning through Image Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Image captioning through image transformer",
      "paper_id": "2004.14231v2"
    },
    {
      "index": 82,
      "title": "“Prophet Attention: Predicting Attention with Future Attention",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability",
      "paper_id": "1910.02974v3"
    },
    {
      "index": 84,
      "title": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network",
      "paper_id": "2012.07061v1"
    },
    {
      "index": 85,
      "title": "Dual-Level Collaborative Transformer for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Dual-Level Collaborative Transformer for Image Captioning",
      "paper_id": "2101.06462v2"
    },
    {
      "index": 86,
      "title": "Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Aligning visual regions and textual concepts for semantic-grounded image representations",
      "paper_id": "1905.06139v3"
    },
    {
      "index": 87,
      "title": "E. Learned-Miller",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "“RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 90,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 91,
      "title": "CPTR: Full Transformer Network for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“CPTR: Full Transformer Network for Image Captioning",
      "paper_id": "2101.10804v3"
    },
    {
      "index": 92,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning Transferable Visual Models From Natural Language Supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 93,
      "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“SimVLM: Simple visual language model pretraining with weak supervision",
      "paper_id": "2108.10904v3"
    },
    {
      "index": 94,
      "title": "“How Much Can CLIP Benefit Vision-and-Language Tasks?” arXiv preprint arXiv:2107.06383",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“ClipCap: CLIP Prefix for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "“Universal Captioner: Long-Tail Vision-and-Language Model Training through Content-Style Separation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Lxmert: Learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 98,
      "title": "“Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 100,
      "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Unified Vision-Language Pre-Training for Image Captioning and VQA",
      "paper_id": "1909.11059v3"
    },
    {
      "index": 101,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 102,
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“VinVL: Revisiting visual representations in vision-language models",
      "paper_id": "2101.00529v2"
    },
    {
      "index": 103,
      "title": "Scaling Up Vision-Language Pre-training for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Scaling Up Vision-Language Pre-training for Image Captioning",
      "paper_id": "2111.12233v2"
    },
    {
      "index": 104,
      "title": "“Long Short-Term Memory",
      "abstract": "",
      "year": "1997",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "Neural Baby Talk",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural Baby Talk",
      "paper_id": "1803.09845v1"
    },
    {
      "index": 106,
      "title": "AutoCaption: Image Captioning with Neural Architecture Search",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“AutoCaption: Image Captioning with Neural Architecture Search",
      "paper_id": "2012.09742v3"
    },
    {
      "index": 107,
      "title": "Convolutional Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Convolutional image captioning",
      "paper_id": "1711.09151v1"
    },
    {
      "index": 108,
      "title": "“Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "Fast Image Caption Generation with Position Alignment",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Fast Image Caption Generation with Position Alignment",
      "paper_id": "1912.06365v1"
    },
    {
      "index": 110,
      "title": "Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Non-autoregressive image captioning with counterfactuals-critical multi-agent learning",
      "paper_id": "2005.04690v1"
    },
    {
      "index": 111,
      "title": "“Iterative Back Modification for Faster Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "Fast Sequence Generation with Multi-Agent Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Fast Sequence Generation with Multi-Agent Reinforcement Learning",
      "paper_id": "2101.09698v1"
    },
    {
      "index": 113,
      "title": "Statistical Machine Translation. Cambridge University Press",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Sequence level training with recurrent neural networks",
      "paper_id": "1511.06732v7"
    },
    {
      "index": 115,
      "title": "“Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "“BLEU: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "“Rouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep reinforcement learning-based image captioning with embedding reward",
      "paper_id": "1704.03899v1"
    },
    {
      "index": 119,
      "title": "“Improved Image Captioning via Policy Gradient Optimization of SPIDEr",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“SPICE: Semantic Propositional Image Caption Evaluation",
      "paper_id": "1607.08822v1"
    },
    {
      "index": 121,
      "title": "C. Lawrence Zitnick",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Actor-Critic Sequence Training for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Actor-Critic Sequence Training for Image Captioning",
      "paper_id": "1706.09601v2"
    },
    {
      "index": 123,
      "title": "“Self-critical n-step Training for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "“XGPT: Cross-modal Generative Pre-Training for Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "VirTex: Learning Visual Representations from Textual Annotations",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“VirTex: Learning Visual Representations From Textual Annotations",
      "paper_id": "2006.06666v3"
    },
    {
      "index": 126,
      "title": "“Neural Machine Translation of Rare Words with Subword Units",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "“Microsoft COCO: Common Objects in Context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 128,
      "title": "“From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "“Conceptual captions: A cleaned",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
      "paper_id": "2102.08981v2"
    },
    {
      "index": 131,
      "title": "Captioning Images Taken by People Who Are Blind",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Captioning Images Taken by People Who Are Blind",
      "paper_id": "2002.08565v2"
    },
    {
      "index": 132,
      "title": "Learning Deep Representations of Fine-Grained Visual Descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning deep representations of fine-grained visual descriptions",
      "paper_id": "1605.05395v1"
    },
    {
      "index": 133,
      "title": "Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards",
      "paper_id": "2008.02693v2"
    },
    {
      "index": 134,
      "title": "F. Moreno-Noguer",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "everyone! context driven entity-aware captioning for news images",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“TextCaps: a Dataset for Image Captioning with Reading Comprehension",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "“Connecting vision and language with localized narratives",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 138,
      "title": "Zero-Shot Text-to-Image Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Zero-Shot Text-to-Image Generation",
      "paper_id": "2102.12092v2"
    },
    {
      "index": 139,
      "title": "“YFCC100M: The new data in multimedia research",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
      "paper_id": "2103.01913v2"
    },
    {
      "index": 141,
      "title": "“Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Caltech-UCSD Birds 200",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "“Automated flower classification over a large number of classes",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "adapt and tell: Adversarial training of cross-domain image captioner",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "Generating Visual Explanations",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Generating visual explanations",
      "paper_id": "1603.08507v1"
    },
    {
      "index": 146,
      "title": "Grounding Visual Explanations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Grounding visual explanations",
      "paper_id": "1807.09685v2"
    },
    {
      "index": 147,
      "title": "Generative Adversarial Text to Image Synthesis",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Generative adversarial text to image synthesis",
      "paper_id": "1605.05396v2"
    },
    {
      "index": 148,
      "title": "Transparent Human Evaluation for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Transparent human evaluation for image captioning",
      "paper_id": "2111.08940v2"
    },
    {
      "index": 149,
      "title": "“METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "“NNEval: Neural network based evaluation metric for image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "“Learning to evaluate image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale",
      "paper_id": "2010.13588v1"
    },
    {
      "index": 153,
      "title": "L. Anne Hendricks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": "“Measuring the diversity of automatic image descriptions",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "Describing like Humans: on Diversity in Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Describing like humans: on diversity in image captioning",
      "paper_id": "1903.12020v3"
    },
    {
      "index": 156,
      "title": "“On Diversity in Image Captioning: Metrics and Methods",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "Object Hallucination in Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Object Hallucination in Image Captioning",
      "paper_id": "1809.02156v2"
    },
    {
      "index": 158,
      "title": "Omission: A Fine-grained Evaluation for Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Towards unique and informative captioning of images",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "“From word embeddings to document distances",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "N. Ikizler-Cinbis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Control and Tell: A Framework for Generating Controllable and Grounded Captions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "“Explore and Explain: Self-supervised Navigation and Recounting",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "“ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "“Improving image captioning evaluation by considering inter references variance",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "“FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 167,
      "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning",
      "paper_id": "2106.14019v1"
    },
    {
      "index": 168,
      "title": "A Neural Compositional Paradigm for Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“A neural compositional paradigm for image captioning",
      "paper_id": "1810.09630v1"
    },
    {
      "index": 169,
      "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Diverse and Natural Image Descriptions via a Conditional GAN",
      "paper_id": "1703.06029v3"
    },
    {
      "index": 170,
      "title": "BERTScore: Evaluating Text Generation with BERT",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“BERTScore: Evaluating Text Generation with BERT",
      "paper_id": "1904.09675v3"
    },
    {
      "index": 171,
      "title": "BERTTune: Fine-Tuning Neural Machine Translation with BERTScore",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“BERTTune: Fine-Tuning Neural Machine Translation with BERTScore",
      "paper_id": "2106.02208v1"
    },
    {
      "index": 172,
      "title": "TIGEr: Text-to-Image Grounding for Image Caption Evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“TIGEr: text-to-image grounding for image caption evaluation",
      "paper_id": "1909.02050v1"
    },
    {
      "index": 173,
      "title": "Stacked Cross Attention for Image-Text Matching",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Stacked Cross Attention for Image-Text Matching",
      "paper_id": "1803.08024v2"
    },
    {
      "index": 174,
      "title": "“CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 175,
      "title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
      "paper_id": "1511.05284v2"
    },
    {
      "index": 176,
      "title": "Captioning Images with Diverse Objects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Captioning Images with Diverse Objects",
      "paper_id": "1606.07770v3"
    },
    {
      "index": 177,
      "title": "“nocaps: novel object captioning at scale",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 178,
      "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects",
      "paper_id": "1708.05271v1"
    },
    {
      "index": 179,
      "title": "“Pointing Novel Objects in Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 180,
      "title": "Decoupled Novel Object Captioner",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Decoupled Novel Object Captioner",
      "paper_id": "1804.03803v2"
    },
    {
      "index": 181,
      "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Guided Open Vocabulary Image Captioning with Constrained Beam Search",
      "paper_id": "1612.00576v2"
    },
    {
      "index": 182,
      "title": "VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning",
      "paper_id": "2009.13682v2"
    },
    {
      "index": 183,
      "title": "“Unpaired image captioning by language pivoting",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "Unsupervised Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Unsupervised image captioning",
      "paper_id": "1811.10787v2"
    },
    {
      "index": 185,
      "title": "Towards Unsupervised Image Captioning with Shared Multimodal Embeddings",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards unsupervised image captioning with shared multimodal embeddings",
      "paper_id": "1908.09317v1"
    },
    {
      "index": 186,
      "title": "Unpaired Image Captioning via Scene Graph Alignments",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Unpaired image captioning via scene graph alignments",
      "paper_id": "1903.10658v4"
    },
    {
      "index": 187,
      "title": "Recurrent Relational Memory Network for Unsupervised Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Recurrent relational memory network for unsupervised image captioning",
      "paper_id": "2006.13611v1"
    },
    {
      "index": 188,
      "title": "Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach",
      "paper_id": "1909.02201v2"
    },
    {
      "index": 189,
      "title": "“Unpaired Image Captioning with Semantic-Constrained Self-Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning",
      "paper_id": "2007.06271v2"
    },
    {
      "index": 191,
      "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“DenseCap: Fully convolutional Localization Networks for Dense Captioning",
      "paper_id": "1511.07571v1"
    },
    {
      "index": 192,
      "title": "Dense Captioning with Joint Inference and Visual Context",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Dense captioning with joint inference and visual context",
      "paper_id": "1611.06949v2"
    },
    {
      "index": 193,
      "title": "“Learning object context for dense captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 194,
      "title": "Context and Attribute Grounded Dense Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Context and attribute grounded dense captioning",
      "paper_id": "1904.01410v1"
    },
    {
      "index": 195,
      "title": "“Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“A hierarchical approach for generating descriptive image paragraphs",
      "paper_id": "1611.06607v2"
    },
    {
      "index": 197,
      "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Recurrent topic-transition GAN for visual paragraph generation",
      "paper_id": "1703.07022v2"
    },
    {
      "index": 198,
      "title": "“Show and Tell More: Topic-Oriented Multi-Sentence Image Captioning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "Diverse and Coherent Paragraph Generation from Images",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Diverse and coherent paragraph generation from images",
      "paper_id": "1809.00681v1"
    },
    {
      "index": 200,
      "title": "Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Curiosity-driven reinforcement learning for diverse visual paragraph generation",
      "paper_id": "1908.00169v2"
    },
    {
      "index": 201,
      "title": "“TAP: Text-Aware Pre-training for Text-VQA and Text-Caption",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 202,
      "title": "“Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "“Improving OCR-based Image Captioning by Incorporating Geometrical Relationship",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps",
      "paper_id": "2012.05153v1"
    },
    {
      "index": 205,
      "title": "Towards Accurate Text-based Image Captioning with Content Diversity Exploration",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards Accurate Text-based Image Captioning with Content Diversity Exploration",
      "paper_id": "2105.03236v1"
    },
    {
      "index": 206,
      "title": "Learning to Describe Differences Between Pairs of Similar Images",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to describe differences between pairs of similar images",
      "paper_id": "1808.10584v1"
    },
    {
      "index": 207,
      "title": "Robust Image Captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Robust Change Captioning",
      "paper_id": "2012.09732v1"
    },
    {
      "index": 208,
      "title": "Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning",
      "paper_id": "2009.14352v1"
    },
    {
      "index": 209,
      "title": "“Image Difference Captioning with Instance-Level Fine-Grained Feature Representation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "“Viewpoint-Agnostic Change Captioning With Cycle Consistency",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "“Image Change Captioning by Learning from an Auxiliary Task",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "“Diverse Beam Search for Improved Description of Complex Scenes",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "Contrastive Learning for Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Contrastive learning for image captioning",
      "paper_id": "1710.02534v1"
    },
    {
      "index": 214,
      "title": "“Generating diverse and descriptive image captions using visual paraphrases",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space",
      "paper_id": "1711.07068v1"
    },
    {
      "index": 216,
      "title": "“Sequential latent spaces for modeling the intention during diverse image captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "“Variational structured semantic inference for diverse image captioning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "Diverse Image Captioning with Context-Object Split Latent Spaces",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Diverse image captioning with context-object split latent spaces",
      "paper_id": "2011.00966v1"
    },
    {
      "index": 219,
      "title": "diverse and accurate image captioning guided by part-of-speech",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "Multilingual Image Description with Neural Sequence Models",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "“Multilingual image description with neural sequence models",
      "paper_id": "1510.04709v2"
    },
    {
      "index": 221,
      "title": "“COCO-CN for cross-lingual image tagging",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "“Cross-lingual image caption generation",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "Multi30K: Multilingual English-German Image Descriptions",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi30K: Multilingual English-German Image Descriptions",
      "paper_id": "1605.00459v1"
    },
    {
      "index": 224,
      "title": "Fluency-Guided Cross-Lingual Image Captioning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Fluency-guided cross-lingual image captioning",
      "paper_id": "1708.04390v1"
    },
    {
      "index": 225,
      "title": "Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Unpaired cross-lingual image caption generation with self-supervised rewards",
      "paper_id": "1908.05407v1"
    },
    {
      "index": 226,
      "title": "On the Automatic Generation of Medical Imaging Reports",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“On the Automatic Generation of Medical Imaging Reports",
      "paper_id": "1711.08195v3"
    },
    {
      "index": 227,
      "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation",
      "paper_id": "2106.06963v2"
    },
    {
      "index": 228,
      "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation",
      "paper_id": "2106.06471v1"
    },
    {
      "index": 229,
      "title": "Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation",
      "paper_id": "2109.05743v1"
    },
    {
      "index": 230,
      "title": "“Automatic Caption Generation for News Images",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 231,
      "title": "Transform and Tell: Entity-Aware News Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Transform and Tell: Entity-Aware News Image Captioning",
      "paper_id": "2004.08070v2"
    },
    {
      "index": 232,
      "title": "Visual News: Benchmark and Challenges in News Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual News: Benchmark and Challenges in News Image Captioning",
      "paper_id": "2010.03743v3"
    },
    {
      "index": 233,
      "title": "Journalistic Guidelines Aware News Image Captioning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Journalistic Guidelines Aware News Image Captioning",
      "paper_id": "2109.02865v2"
    },
    {
      "index": 234,
      "title": "“Automatic alt-text: Computer-generated image descriptions for blind users on a social network service",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 235,
      "title": "“Attend to you: Personalized image captioning with context sequence memory networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 236,
      "title": "“Towards personalized image captioning via multimodal memory networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 237,
      "title": "“Learning Long-and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 238,
      "title": "“StyleNet: Generating Attractive Visual Captions with Styles",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 239,
      "title": "SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Semstyle: Learning to generate stylised image captions using unaligned text",
      "paper_id": "1805.07030v1"
    },
    {
      "index": 240,
      "title": "“Mscap: Multi-style image captioning with unpaired stylized text",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 241,
      "title": "“MemCap: Memorizing style knowledge for image captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 242,
      "title": "Engaging Image Captioning via Personality",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Engaging image captioning via personality",
      "paper_id": "1810.10665v2"
    },
    {
      "index": 243,
      "title": "Intention Oriented Image Captions with Guiding Objects",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Intention oriented image captions with guiding objects",
      "paper_id": "1811.07662v2"
    },
    {
      "index": 244,
      "title": "Connecting What to Say With Where to Look by Modeling Human Attention Traces",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Connecting What to Say With Where to Look by Modeling Human Attention Traces",
      "paper_id": "2105.05964v1"
    },
    {
      "index": 245,
      "title": "Human-like Controllable Image Captioning with Verb-specific Semantic Roles",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Human-like Controllable Image Captioning with Verb-specific Semantic Roles",
      "paper_id": "2103.12204v1"
    },
    {
      "index": 246,
      "title": "Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Say as you wish: Fine-grained control of image caption generation with abstract scene graphs",
      "paper_id": "2003.00387v1"
    },
    {
      "index": 247,
      "title": "Comprehensive Image Captioning via Scene Graph Decomposition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Comprehensive image captioning via scene graph decomposition",
      "paper_id": "2007.11731v1"
    },
    {
      "index": 248,
      "title": "Length-Controllable Image Captioning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Length-controllable image captioning",
      "paper_id": "2007.09580v1"
    },
    {
      "index": 249,
      "title": "edit and tell: A framework for editing image captions",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 250,
      "title": "“Focused evaluation for image description with binary forced-choice tasks",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 251,
      "title": "“Going beneath the surface: Evaluating image captioning for grammaticality",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 252,
      "title": "“Cross-modal Coherence Modeling for Caption Generation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 253,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“UNITER: UNiversal Image-TExt Representation Learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 254,
      "title": "“Axiomatic attribution for deep networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    }
  ]
}