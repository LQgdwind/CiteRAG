{
  "paper_id": "2402.01763v4",
  "title": "When Large Language Models Meet Vector Databases: A Survey",
  "abstract": "Abstract\nThis survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration’s impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.",
  "reference_labels": [
    {
      "index": 0,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Josh Achiam, Steven Adler, et al.",
      "orig_title": "Gpt-4 technical report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 1,
      "title": "CM3: A Causal Masked Multimodal Model of the Internet",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Armen Aghajanyan, Bernie Huang, et al.",
      "orig_title": "Cm3: A causal masked multimodal model of the internet",
      "paper_id": "2201.07520v1"
    },
    {
      "index": 2,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Jean Alayrac, Jeff Donahue, et al."
    },
    {
      "index": 3,
      "title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions",
      "abstract": "",
      "year": "2008",
      "venue": "Comm. ACM",
      "authors": "Alexandr Andoni and Piotr Indyk"
    },
    {
      "index": 4,
      "title": "Retrieval-based language models and applications",
      "abstract": "",
      "year": "2023",
      "venue": "ACL",
      "authors": "Akari Asai, Sewon Min, et al."
    },
    {
      "index": 5,
      "title": "Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings",
      "abstract": "",
      "year": "2023",
      "venue": "NLP-OSS Workshop",
      "authors": "Fu Bang"
    },
    {
      "index": 6,
      "title": "On the dangers of stochastic parrots: Can language models be too big?",
      "abstract": "",
      "year": "2021",
      "venue": "FAccT",
      "authors": "Emily M Bender, Timnit Gebru, et al."
    },
    {
      "index": 7,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tom Brown, Benjamin Mann, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 8,
      "title": "SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS 2021",
      "authors": "Qi Chen, Bing Zhao, et al.",
      "orig_title": "Spann: Highly-efficient billion-scale approximate nearest neighbor search",
      "paper_id": "2111.08566v1"
    },
    {
      "index": 9,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2023",
      "venue": "JMLR",
      "authors": "Aakanksha Chowdhery, Sharan Narang, et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 10,
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Junyoung Chung, Caglar Gulcehre, et al.",
      "orig_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "paper_id": "1412.3555v1"
    },
    {
      "index": 11,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "Jacob Devlin, Ming-Wei Chang, et al.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 12,
      "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Deep Ganguli, Liane Lovitt, et al.",
      "orig_title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
      "paper_id": "2209.07858v2"
    },
    {
      "index": 13,
      "title": "A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Yikun Han, Chunjiang Liu, et al.",
      "orig_title": "A comprehensive survey on vector database: Storage and retrieval technique, challenge",
      "paper_id": "2310.11703v2"
    },
    {
      "index": 14,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "Kaiming He, Xiangyu Zhang, et al.",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 15,
      "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Lei Huang, Weijiang Yu, et al.",
      "orig_title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
      "paper_id": "2311.05232v2"
    },
    {
      "index": 16,
      "title": "Atlas: Few-shot learning with retrieval augmented language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Gautier Izacard, Lewis Patrick, et al."
    },
    {
      "index": 17,
      "title": "Product Quantization for Nearest Neighbor Search",
      "abstract": "",
      "year": "2011",
      "venue": "TPAMI",
      "authors": "Hervé Jégou, Matthijs Douze, et al."
    },
    {
      "index": 18,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "authors": "Ziwei Ji, Nayeon Lee, et al.",
      "orig_title": "Survey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 19,
      "title": "Active Retrieval Augmented Generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Zhengbao Jiang, Frank F Xu, et al.",
      "orig_title": "Active retrieval augmented generation",
      "paper_id": "2305.06983v2"
    },
    {
      "index": 20,
      "title": "Aligning artificial intelligence with climate change mitigation",
      "abstract": "",
      "year": "2022",
      "venue": "Nature Climate Change",
      "authors": "Lynn H Kaack, Priya L Donti, et al."
    },
    {
      "index": 21,
      "title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Minki Kang and Jin M Kwak",
      "orig_title": "Knowledge graph-augmented language models for knowledge-grounded dialogue generation",
      "paper_id": "2305.18846v1"
    },
    {
      "index": 22,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Jared Kaplan, Sam McCandlish, et al.",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 23,
      "title": "Measuring Catastrophic Forgetting in Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Ronald Kemker, Marc McClure, et al.",
      "orig_title": "Measuring catastrophic forgetting in neural networks",
      "paper_id": "1708.02072v4"
    },
    {
      "index": 24,
      "title": "Generalization through Memorization: Nearest Neighbor Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Urvashi Khandelwal, Levy Omer, et al.",
      "orig_title": "Generalization through memorization: Nearest neighbor language models",
      "paper_id": "1911.00172v2"
    },
    {
      "index": 25,
      "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
      "abstract": "",
      "year": "2023",
      "venue": "ICML",
      "authors": "Jing Yu Koh, Ruslan Salakhutdinov, et al.",
      "orig_title": "Grounding language models to images for multimodal inputs and outputs",
      "paper_id": "2301.13823v4"
    },
    {
      "index": 26,
      "title": "Deduplicating Training Data Makes Language Models Better",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Katherine Lee, Daphne Ippolito, et al.",
      "orig_title": "Deduplicating training data makes language models better",
      "paper_id": "2107.06499v2"
    },
    {
      "index": 27,
      "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Haoran Li, Yulin Chen, et al.",
      "orig_title": "Privacy in large language models: Attacks, defenses and future directions",
      "paper_id": "2310.10383v2"
    },
    {
      "index": 28,
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Junnan Li Li, Dongxu Li, et al.",
      "orig_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "paper_id": "2301.12597v3"
    },
    {
      "index": 29,
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "abstract": "",
      "year": "2022",
      "venue": "ACL",
      "authors": "Stephanie Lin, Jacob Hilton, and Owain Evans",
      "orig_title": "TruthfulQA: Measuring how models mimic human falsehoods",
      "paper_id": "2109.07958v2"
    },
    {
      "index": 30,
      "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Yun Luo, Zhen Yang, et al.",
      "orig_title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
      "paper_id": "2308.08747v5"
    },
    {
      "index": 31,
      "title": "Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs",
      "abstract": "",
      "year": "2018",
      "venue": "PAMI",
      "authors": "Yury A Malkov and Dmitry A Yashunin"
    },
    {
      "index": 32,
      "title": "Ret-LLM: Towards a General Read-Write Memory for Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Ali Modarressi, Ayyoob Imani, et al.",
      "orig_title": "Ret-llm: Towards a general read-write memory for large language models",
      "paper_id": "2305.14322v2"
    },
    {
      "index": 33,
      "title": "Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration",
      "abstract": "",
      "year": "2009",
      "venue": "VISAPP",
      "authors": "Marius Muja and David G Lowe"
    },
    {
      "index": 34,
      "title": "A Cost Analysis of Generative Language Models and Influence Operations",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Micah Musser",
      "orig_title": "A cost analysis of generative language models and influence operations",
      "paper_id": "2308.03740v1"
    },
    {
      "index": 35,
      "title": "Nationality Bias in Text Generation",
      "abstract": "",
      "year": "2023",
      "venue": "EACL",
      "authors": "Pranav Narayanan Venkit, Sanjana Gautam, et al.",
      "orig_title": "Nationality bias in text generation",
      "paper_id": "2302.02463v3"
    },
    {
      "index": 36,
      "title": "EASE: Entity-Aware Contrastive Learning of Sentence Embedding",
      "abstract": "",
      "year": "2022",
      "venue": "NAACL",
      "authors": "Sosuke Nishikawa, Ryokan Ri, et al.",
      "orig_title": "EASE: Entity-aware contrastive learning of sentence embedding",
      "paper_id": "2205.04260v1"
    },
    {
      "index": 37,
      "title": "Entity Cloze By Date: What LMs Know About Unseen Entities",
      "abstract": "",
      "year": "2022",
      "venue": "NAACL Findings",
      "authors": "Yasumasa Onoe, Michael Zhang, et al.",
      "orig_title": "Entity cloze by date: What LMs know about unseen entities",
      "paper_id": "2205.02832v1"
    },
    {
      "index": 38,
      "title": "Survey of Vector Database Management Systems",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "James Jie Pan, Jianguo Wang, et al.",
      "orig_title": "Survey of vector database management systems",
      "paper_id": "2310.14021v1"
    },
    {
      "index": 39,
      "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Guilherme Penedo, Quentin Malartic, et al.",
      "orig_title": "The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only",
      "paper_id": "2306.01116v1"
    },
    {
      "index": 40,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI Blog",
      "authors": "Alec Radford, Jeffrey Wu, et al."
    },
    {
      "index": 41,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "JMLR",
      "authors": "Colin Raffel, Noam Shazeer, et al.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 42,
      "title": "In-Context Retrieval-Augmented Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "TACL",
      "authors": "Ori Ram, Yoav Levine, et al.",
      "orig_title": "In-context retrieval-augmented language models",
      "paper_id": "2302.00083v3"
    },
    {
      "index": 43,
      "title": "E-scan: Consuming contextual data with model plugins",
      "abstract": "",
      "year": "2023",
      "venue": "VLDB Workshop",
      "authors": "Viktor Sanca and Anastasia Ailamaki"
    },
    {
      "index": 44,
      "title": "Green ai",
      "abstract": "",
      "year": "2020",
      "venue": "Commun. ACM",
      "authors": "Roy Schwartz, Jesse Dodge, et al."
    },
    {
      "index": 45,
      "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "Xingjian Shi, Zhourong Chen, et al.",
      "orig_title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "paper_id": "1506.04214v2"
    },
    {
      "index": 46,
      "title": "LLaSM: Large Language and Speech Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Yu Shu, Siwei Dong, et al.",
      "orig_title": "Llasm: Large language and speech model",
      "paper_id": "2308.15930v3"
    },
    {
      "index": 47,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Emma Strubell, Ananya Ganesh, et al.",
      "orig_title": "Energy and policy considerations for deep learning in nlp",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 48,
      "title": "Quality and efficiency in high dimensional nearest neighbor search",
      "abstract": "",
      "year": "2009",
      "venue": "SIGMOD",
      "authors": "Yufei Tao, Ke Yi, et al."
    },
    {
      "index": 49,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Hugo Touvron, Thibaut Lavril, et al.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 50,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NeurIPS",
      "authors": "Ashish Vaswani, Noam Shazeer, et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 51,
      "title": "MUST: An Effective and Scalable Framework for Multimodal Search of Target Modality",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Mengzhao Wang and Xiangyu Ke et al.",
      "orig_title": "Must: An effective and scalable framework for multimodal search of target modality",
      "paper_id": "2312.06397v1"
    },
    {
      "index": 52,
      "title": "Milvus: A purpose-built vector data management system",
      "abstract": "",
      "year": "2021",
      "venue": "SIGMOD",
      "authors": "Jianguo Wang, Xiaomeng Yi, et al."
    },
    {
      "index": 53,
      "title": "Emergent Abilities of Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Jason Wei, Yi Tay, et al.",
      "orig_title": "Emergent abilities of large language models",
      "paper_id": "2206.07682v2"
    },
    {
      "index": 54,
      "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities",
      "abstract": "",
      "year": "2022",
      "venue": "MLSys",
      "authors": "Carole J. Wu and Ramya others Raghavendra",
      "orig_title": "Sustainable ai: Environmental implications, challenges and opportunities",
      "paper_id": "2111.00364v2"
    },
    {
      "index": 55,
      "title": "Wav2Seq: Pre-training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Felix Wu, Kwangyoun Kim, et al.",
      "orig_title": "Wav2seq: Pre-training speech-to-text encoder-decoder models using pseudo languages",
      "paper_id": "2205.01086v1"
    },
    {
      "index": 56,
      "title": "Sustainable ai: Ai for sustainability and the sustainability of ai",
      "abstract": "",
      "year": "2021",
      "venue": "AI Ethics",
      "authors": "Almee V Wynsberghe"
    },
    {
      "index": 57,
      "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Cheng Xin, Luo Di, et al.",
      "orig_title": "Lift yourself up: Retrieval-augmented text generation with self memory",
      "paper_id": "2305.02437v3"
    },
    {
      "index": 58,
      "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Antoine Yang, Arsha Nagrani, et al.",
      "orig_title": "Vid2seq: Large-scale pretraining of a visual language model for dense video captioning",
      "paper_id": "2302.14115v2"
    },
    {
      "index": 59,
      "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Yunzhi Yao, Peng Wang, et al.",
      "orig_title": "Editing large language models: Problems, methods, and opportunities",
      "paper_id": "2305.13172v3"
    },
    {
      "index": 60,
      "title": "How transferable are features in deep neural networks?",
      "abstract": "",
      "year": "2014",
      "venue": "NeurIPS",
      "authors": "Jason Yosinski, Jeff Clune, et al.",
      "orig_title": "How transferable are features in deep neural networks?",
      "paper_id": "1411.1792v1"
    },
    {
      "index": 61,
      "title": "Generate rather than Retrieve: Large Langu-age Models are Strong Context Generators",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Wenhao Yu, Dan Iter, et al.",
      "orig_title": "Generate rather than retrieve: Large language models are strong context generators",
      "paper_id": "2209.10063v3"
    },
    {
      "index": 62,
      "title": "Recurrent Neural Network Regularization",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint",
      "authors": "Wojciech Zaremba, Ilya Sutskever, et al.",
      "orig_title": "Recurrent neural network regularization",
      "paper_id": "1409.2329v5"
    },
    {
      "index": 63,
      "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint",
      "authors": "Chenshuang Zhang, Chaoning Zhang, et al.",
      "orig_title": "Text-to-image diffusion models in generative ai: A survey",
      "paper_id": "2303.07909v3"
    },
    {
      "index": 64,
      "title": "Long-term memory for large language models through topic-based vector database",
      "abstract": "",
      "year": "2023",
      "venue": "IALP",
      "authors": "Yi Zhang, Zhongyang Yu, et al."
    }
  ]
}