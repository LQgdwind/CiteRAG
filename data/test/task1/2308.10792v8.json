{
  "paper_id": "2308.10792v8",
  "title": "Instruction Tuning for Large Language Models: A Survey",
  "abstract": "Abstract\nThis paper surveys research works in the quickly advancing field of instruction tuning (IT),\na crucial technique to enhance the capabilities and controllability of large language models (LLMs).\nInstruction tuning\nrefers to the process of further training LLMs on a dataset consisting\nof (instruction, output) pairs\nin a supervised fashion,\nwhich bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions.\nIn this work, we make a systematic review of the literature, including the general methodology of IT,\nthe construction of IT datasets, the training of IT models,\nand applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also\nreview the potential pitfalls of IT along with criticism against it, along with efforts\npointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.\n††footnotetext: ♠Zhejiang University, ♣Shannon.AI, ▲Nanyang Technological University, ◆Amazon\n††footnotetext: Email: sy_zhang@zju.edu.cn\n††footnotetext: Project link: https://github.com/xiaoya-li/Instruction-Tuning-Survey",
  "reference_labels": [
    {
      "index": 0,
      "title": "Evaluating correctness and faithfulness of instruction-following models for question answering",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv",
      "authors": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy"
    },
    {
      "index": 1,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "NeurIPS",
      "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan"
    },
    {
      "index": 2,
      "title": "Falcon-40B: an open large language model with state-of-the-art performance",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo"
    },
    {
      "index": 3,
      "title": "Falcon-40b: an open large language model with state-of-the-art performance",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al."
    },
    {
      "index": 4,
      "title": "Openflamingo",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, et al."
    },
    {
      "index": 5,
      "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush",
      "orig_title": "Promptsource: An integrated development environment and repository for natural language prompts",
      "paper_id": "2202.01279v3"
    },
    {
      "index": 6,
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.",
      "orig_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "paper_id": "2204.05862v1"
    },
    {
      "index": 7,
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint",
      "authors": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.",
      "orig_title": "Constitutional ai: Harmlessness from ai feedback",
      "paper_id": "2212.08073v1"
    },
    {
      "index": 8,
      "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman"
    },
    {
      "index": 9,
      "title": "Text2LIVE: Text-Driven Layered Image and Video Editing",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel",
      "orig_title": "Text2live: Text-driven layered image and video editing",
      "paper_id": "2204.02491v2"
    },
    {
      "index": 10,
      "title": "The Pushshift Reddit Dataset",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Web and Social Media",
      "authors": "Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn",
      "orig_title": "The pushshift reddit dataset",
      "paper_id": "2001.08435v1"
    },
    {
      "index": 11,
      "title": "Open llm leaderboard",
      "abstract": "",
      "year": "2023",
      "venue": "Hugging Face",
      "authors": "Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf"
    },
    {
      "index": 12,
      "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv",
      "authors": "Stella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal",
      "orig_title": "Pythia: A suite for analyzing large language models across training and scaling",
      "paper_id": "2304.01373v2"
    },
    {
      "index": 13,
      "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "Sid Black, Stella Rose Biderman, Eric Hallahan, Quentin G. Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Martin Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Benqi Wang, and Samuel Weinbach",
      "orig_title": "Gpt-neox-20b: An open-source autoregressive language model",
      "paper_id": "2204.06745v1"
    },
    {
      "index": 14,
      "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "Tim Brooks, Aleksander Holynski, and Alexei A. Efros",
      "orig_title": "Instructpix2pix: Learning to follow image editing instructions",
      "paper_id": "2211.09800v2"
    },
    {
      "index": 15,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 16,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 17,
      "title": "Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv",
      "authors": "Tuhin Chakrabarty, Vishakh Padmakumar, and Hengxing He",
      "orig_title": "Help me write a poem - instruction tuning as a vehicle for collaborative poetry writing",
      "paper_id": "2210.13669v1"
    },
    {
      "index": 18,
      "title": "Code alpaca: An instruction-following llama model for code generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Sahil Chaudhary"
    },
    {
      "index": 19,
      "title": "Evaluating Large Language Models Trained on Code",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba",
      "orig_title": "Evaluating large language models trained on code",
      "paper_id": "2107.03374v2"
    },
    {
      "index": 20,
      "title": "Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering",
      "abstract": "",
      "year": "2023",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": "Qianglong Chen, Guohai Xu, Mingshi Yan, Ji Zhang, Fei Huang, Luo Si, and Yin Zhang",
      "orig_title": "Distinguish before answer: Generating contrastive explanation as knowledge for commonsense question answering",
      "paper_id": "2305.08135v2"
    },
    {
      "index": 21,
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "abstract": "",
      "year": "2023",
      "venue": "See https://vicuna. lmsys. org (accessed 14 April",
      "authors": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023."
    },
    {
      "index": 22,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 23,
      "title": "Scaling Instruction-Finetuned Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "",
      "orig_title": "Scaling instruction-finetuned language models",
      "paper_id": "2210.11416v5"
    },
    {
      "index": 24,
      "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
      "abstract": "",
      "year": "1905",
      "venue": "ArXiv, abs/",
      "authors": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael\nCollins, and Kristina Toutanova. 2019.",
      "orig_title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "paper_id": "1905.10044v1"
    },
    {
      "index": 25,
      "title": "Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages",
      "abstract": "",
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics,\n8:454–470",
      "authors": "J. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly\nNikolaev, and Jennimaria Palomaki. 2020."
    },
    {
      "index": 26,
      "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv, abs/",
      "authors": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. 2018.",
      "orig_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "paper_id": "1803.05457v1"
    },
    {
      "index": 27,
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv, abs/",
      "authors": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman. 2021.",
      "orig_title": "Training verifiers to solve math word problems",
      "paper_id": "2110.14168v2"
    },
    {
      "index": 28,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAccess AI Collective. 2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 29,
      "title": "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan,\nSam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, et al. 2023."
    },
    {
      "index": 30,
      "title": "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier,\nDavid Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco\nCaltagirone, Thibaut Lavril, et al. 2018.",
      "orig_title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
      "paper_id": "1805.10190v3"
    },
    {
      "index": 31,
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.",
      "orig_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "paper_id": "2305.06500v2"
    },
    {
      "index": 32,
      "title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022."
    },
    {
      "index": 33,
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.",
      "orig_title": "Qlora: Efficient finetuning of quantized llms",
      "paper_id": "2305.14314v1"
    },
    {
      "index": 34,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 35,
      "title": "Enhancing chat language models by scaling high-quality instructional conversations",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan\nLiu, Maosong Sun, and Bowen Zhou. 2023a."
    },
    {
      "index": 36,
      "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding\nHu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang,\nZhiyuan Liu, Haitao Zheng, Jianfei Chen, Y. Liu, Jie Tang, Juanzi Li, and\nMaosong Sun. 2023b."
    },
    {
      "index": 37,
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang. 2022."
    },
    {
      "index": 38,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jon Durbin. 2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 39,
      "title": "EditEval: An Instruction-Based Benchmark for Text Improvements",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:",
      "authors": "Jane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria Lomeli, Patrick Lewis,\nGautier Izacard, Edouard Grave, Sebastian Riedel, and Fabio Petroni. 2022.",
      "orig_title": "Editeval: An instruction-based benchmark for text improvements",
      "paper_id": "2209.13331v1"
    },
    {
      "index": 40,
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "William Fedus, Barret Zoph, and Noam M. Shazeer. 2021.",
      "orig_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "paper_id": "2101.03961v3"
    },
    {
      "index": 41,
      "title": "Exploring the Feasibility of ChatGPT for Event Extraction",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023a.",
      "orig_title": "Exploring the feasibility of chatgpt for event extraction",
      "paper_id": "2303.03836v2"
    },
    {
      "index": 42,
      "title": "A framework for few-shot language model evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\net al. 2021."
    },
    {
      "index": 43,
      "title": "Enabling large language models to generate text with citations",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b."
    },
    {
      "index": 44,
      "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "abstract": "",
      "year": "2009",
      "venue": "ArXiv, abs/",
      "authors": "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.\n2020."
    },
    {
      "index": 45,
      "title": "Imagebind: One embedding space to bind them all",
      "abstract": "",
      "year": "2023",
      "venue": "CVPR",
      "authors": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev\nAlwala, Armand Joulin, and Ishan Misra. 2023."
    },
    {
      "index": 46,
      "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qianmengke Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.",
      "orig_title": "Multimodal-gpt: A vision and language model for dialogue with humans",
      "paper_id": "2305.04790v3"
    },
    {
      "index": 47,
      "title": "The False Promise of Imitating Proprietary LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter\nAbbeel, Sergey Levine, and Dawn Song. 2023.",
      "orig_title": "The false promise of imitating proprietary llms",
      "paper_id": "2305.15717v1"
    },
    {
      "index": 48,
      "title": "Instruction tuned models are quick learners",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Himanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam\nMitra, Santosh Mashetty, and Chitta Baral. 2023."
    },
    {
      "index": 49,
      "title": "InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Empirical Methods in Natural Language\nProcessing",
      "authors": "Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskénazi, and\nJeffrey P. Bigham. 2022.",
      "orig_title": "Instructdial: Improving zero and few-shot generalization in dialogue through instruction tuning",
      "paper_id": "2205.12673v2"
    },
    {
      "index": 50,
      "title": "Sendong Zhao Bing Qin Ting Liu Haochun Wang",
      "abstract": "",
      "year": "2023",
      "venue": "https://github.com/SCIR-HI/Med-ChatGLM",
      "authors": "Sendong Zhao Bing Qin Ting Liu Haochun Wang, Chi Liu. 2023."
    },
    {
      "index": 51,
      "title": "Measuring Massive Multitask Language Understanding",
      "abstract": "",
      "year": "2009",
      "venue": "ArXiv, abs/",
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,\nDawn Xiaodong Song, and Jacob Steinhardt. 2020.",
      "orig_title": "Measuring massive multitask language understanding",
      "paper_id": "2009.03300v3"
    },
    {
      "index": 52,
      "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.",
      "orig_title": "Unnatural instructions: Tuning language models with (almost) no human labor",
      "paper_id": "2212.09689v1"
    },
    {
      "index": 53,
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine\nLearning, volume 97",
      "authors": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.",
      "orig_title": "Parameter-efficient transfer learning for NLP",
      "paper_id": "1902.00751v2"
    },
    {
      "index": 54,
      "title": "Lora: Low-rank adaptation of large language models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021."
    },
    {
      "index": 55,
      "title": "Towards Reasoning in Large Language Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jie Huang and Kevin Chen-Chuan Chang. 2022.",
      "orig_title": "Towards reasoning in large language models: A survey",
      "paper_id": "2212.10403v2"
    },
    {
      "index": 56,
      "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,\nJunteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and\nJunxian He. 2023.",
      "orig_title": "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models",
      "paper_id": "2305.08322v3"
    },
    {
      "index": 57,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "[Accessed 09-Jun-",
      "authors": "Anel Islamovic.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 58,
      "title": "Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and\nMatthew E. Peters. 2022."
    },
    {
      "index": 59,
      "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Srinivas Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig,\nPing Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli\nCelikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. 2022.",
      "orig_title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "paper_id": "2212.12017v3"
    },
    {
      "index": 60,
      "title": "Guanaco: Generative universal assistant for natural-language adaptive context-aware omnilingual outputs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "JosephusCheung. 2021."
    },
    {
      "index": 61,
      "title": "Unifiedqa: Crossing format boundaries with a single qa system",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,\nPeter Clark, and Hannaneh Hajishirzi. 2020."
    },
    {
      "index": 62,
      "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,\nZhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, et al. 2023.",
      "orig_title": "Openassistant conversations–democratizing large language model alignment",
      "paper_id": "2304.07327v2"
    },
    {
      "index": 63,
      "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Po-Nien Kung and Nanyun Peng. 2023.",
      "orig_title": "Do models really learn to follow instructions? an empirical study of instruction tuning",
      "paper_id": "2305.11383v2"
    },
    {
      "index": 64,
      "title": "Oig: the open instruction generalist dataset",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "LAION.ai. 2023."
    },
    {
      "index": 65,
      "title": "A dataset of clinically generated visual questions and answers about radiology images",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. 2018."
    },
    {
      "index": 66,
      "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems",
      "authors": "Mina Lee, Percy Liang, and Qian Yang. 2022.",
      "orig_title": "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
      "paper_id": "2201.06796v2"
    },
    {
      "index": 67,
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "abstract": "",
      "year": "2021",
      "venue": "Conference on Empirical Methods in Natural Language\nProcessing",
      "authors": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",
      "orig_title": "The power of scale for parameter-efficient prompt tuning",
      "paper_id": "2104.08691v2"
    },
    {
      "index": 68,
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019.",
      "orig_title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "paper_id": "1910.13461v1"
    },
    {
      "index": 69,
      "title": "Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun\nZhang. 2023a.",
      "orig_title": "Evaluating chatgpt’s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
      "paper_id": "2304.11633v1"
    },
    {
      "index": 70,
      "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.\n2023b.",
      "orig_title": "Otter: A multi-modal model with in-context instruction tuning",
      "paper_id": "2305.03726v2"
    },
    {
      "index": 71,
      "title": "Camel: Communicative agents for \"mind\" exploration of large scale language model society",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and\nBernard Ghanem. 2023c."
    },
    {
      "index": 72,
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "ICML",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023d.",
      "orig_title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
      "paper_id": "2301.12597v3"
    },
    {
      "index": 73,
      "title": "VideoChat : Chat-Centric Video Understanding",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,\nLimin Wang, and Yu Qiao. 2023e.",
      "orig_title": "Videochat: Chat-centric video understanding",
      "paper_id": "2305.06355v2"
    },
    {
      "index": 74,
      "title": "StarCoder: may the source be with you!",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023f.",
      "orig_title": "Starcoder: may the source be with you!",
      "paper_id": "2305.06161v2"
    },
    {
      "index": 75,
      "title": "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang.\n2023g."
    },
    {
      "index": 76,
      "title": "Holistic Evaluation of Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "Holistic evaluation of language models",
      "paper_id": "2211.09110v2"
    },
    {
      "index": 77,
      "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. 2022.",
      "orig_title": "Unsupervised cross-task generalization via retrieval augmentation",
      "paper_id": "2204.07937v2"
    },
    {
      "index": 78,
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Stephanie C. Lin, Jacob Hilton, and Owain Evans. 2021.",
      "orig_title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "paper_id": "2109.07958v2"
    },
    {
      "index": 79,
      "title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023.",
      "orig_title": "Pmc-clip: Contrastive language-image pre-training using biomedical documents",
      "paper_id": "2303.07240v1"
    },
    {
      "index": 80,
      "title": "Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering",
      "abstract": "",
      "year": "2021",
      "venue": "2021 IEEE 18th International Symposium on Biomedical Imaging\n(ISBI)",
      "authors": "Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\n2021a."
    },
    {
      "index": 81,
      "title": "Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang.\n2023a."
    },
    {
      "index": 82,
      "title": "Visual Instruction Tuning",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.",
      "orig_title": "Visual instruction tuning",
      "paper_id": "2304.08485v2"
    },
    {
      "index": 83,
      "title": "What Makes Good In-Context Examples for GPT-3?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu\nChen. 2021b.",
      "orig_title": "What makes good in-context examples for gpt-333?",
      "paper_id": "2101.06804v1"
    },
    {
      "index": 84,
      "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tiedong Liu and Bryan Kian Hsiang Low. 2023.",
      "orig_title": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
      "paper_id": "2305.14201v1"
    },
    {
      "index": 85,
      "title": "Radiology-GPT: A Large Language Model for Radiology",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Zheng Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma,\nPeng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Dajiang Zhu, Jun\nLiu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, and Tianming Liu.\n2023c.",
      "orig_title": "Radiology-gpt: A large language model for radiology",
      "paper_id": "2306.08666v2"
    },
    {
      "index": 86,
      "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny\nZhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023.",
      "orig_title": "The flan collection: Designing data and methods for effective instruction tuning",
      "paper_id": "2301.13688v2"
    },
    {
      "index": 87,
      "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang\nTao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.",
      "orig_title": "Wizardcoder: Empowering code large language models with evol-instruct",
      "paper_id": "2306.08568v2"
    },
    {
      "index": 88,
      "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Kai Lv, Yuqing Yang, Tengxiao Liu, Qi jie Gao, Qipeng Guo, and Xipeng Qiu.\n2023.",
      "orig_title": "Full parameter fine-tuning for large language models with limited resources",
      "paper_id": "2306.09782v2"
    },
    {
      "index": 89,
      "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Learning Representations",
      "authors": "Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and\nStefano Ermon. 2022.",
      "orig_title": "SDEdit: Guided image synthesis and editing with stochastic differential equations",
      "paper_id": "2108.01073v2"
    },
    {
      "index": 90,
      "title": "Cross-task generalization via natural language crowdsourcing instructions",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021."
    },
    {
      "index": 91,
      "title": "Crosslingual Generalization through Multitask Finetuning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella\nBiderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022.",
      "orig_title": "Crosslingual generalization through multitask finetuning",
      "paper_id": "2211.01786v2"
    },
    {
      "index": 92,
      "title": "Album Storytelling with Iterative Story-aware Captioning and Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Munan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu Yuan, Yonghong Tian,\nQixiang Ye, and Liuliang Yuan. 2023.",
      "orig_title": "Album storytelling with iterative story-aware captioning and large language models",
      "paper_id": "2305.12943v2"
    },
    {
      "index": 93,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "NousResearch. 2023.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 94,
      "title": "Introducing chatgpt",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "OpenAI. 2022."
    },
    {
      "index": 95,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "OpenAI. 2023.",
      "orig_title": "Gpt-4 technical report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 96,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022."
    },
    {
      "index": 97,
      "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023.",
      "orig_title": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "paper_id": "2306.01116v1"
    },
    {
      "index": 98,
      "title": "Instruction Tuning with GPT-4",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.",
      "orig_title": "Instruction tuning with gpt-4",
      "paper_id": "2304.03277v1"
    },
    {
      "index": 99,
      "title": "Controllable Natural Language Generation with Contrastive Prefixes",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu Chen. 2022.",
      "orig_title": "Controllable natural language generation with contrastive prefixes",
      "paper_id": "2202.13257v1"
    },
    {
      "index": 100,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 101,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019."
    },
    {
      "index": 102,
      "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,\nFrancis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,\net al. 2021.",
      "orig_title": "Scaling language models: Methods, analysis & insights from training gopher",
      "paper_id": "2112.11446v2"
    },
    {
      "index": 103,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "1910",
      "venue": "ArXiv, abs/",
      "authors": "Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 104,
      "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. 2023.",
      "orig_title": "Coedit: Text editing by task-specific instruction tuning",
      "paper_id": "2305.09857v2"
    },
    {
      "index": 105,
      "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining",
      "authors": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020."
    },
    {
      "index": 106,
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR)",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\nOmmer. 2022.",
      "orig_title": "High-resolution image synthesis with latent diffusion models",
      "paper_id": "2112.10752v2"
    },
    {
      "index": 107,
      "title": "Linguist: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Computational Linguistics",
      "authors": "Andrew Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese.\n2022.",
      "orig_title": "Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging",
      "paper_id": "2209.09900v1"
    },
    {
      "index": 108,
      "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,\nZaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2021.",
      "orig_title": "Multitask prompted training enables zero-shot task generalization",
      "paper_id": "2110.08207v3"
    },
    {
      "index": 109,
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "",
      "orig_title": "Bloom: A 176b-parameter open-access multilingual language model",
      "paper_id": "2211.05100v4"
    },
    {
      "index": 110,
      "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume",
      "authors": "Timo Schick and Hinrich Schütze. 2021."
    },
    {
      "index": 111,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "ArXiv, abs/",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017."
    },
    {
      "index": 112,
      "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush\nVosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,\nand Jason Wei. 2022.",
      "orig_title": "Language models are multilingual chain-of-thought reasoners",
      "paper_id": "2210.03057v1"
    },
    {
      "index": 113,
      "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2seq Model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael\nHamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna\nRumshisky, et al. 2022.",
      "orig_title": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "paper_id": "2208.01448v2"
    },
    {
      "index": 114,
      "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar\nAbid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022.",
      "orig_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "paper_id": "2206.04615v3"
    },
    {
      "index": 115,
      "title": "Answering ambiguous questions via iterative prompting",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Weiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren, Zhumin Chen, Maarten\nde Rijke, and Zhaochun Ren. 2023a."
    },
    {
      "index": 116,
      "title": "Pushing the Limits of ChatGPT on NLP Tasks",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang,\nJiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, et al. 2023b.",
      "orig_title": "Pushing the limits of chatgpt on nlp tasks",
      "paper_id": "2306.09719v2"
    },
    {
      "index": 117,
      "title": "Text Classification via Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and\nGuoyin Wang. 2023c.",
      "orig_title": "Text classification via large language models",
      "paper_id": "2305.08377v3"
    },
    {
      "index": 118,
      "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay,\nHyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny\nZhou, and Jason Wei. 2022."
    },
    {
      "index": 119,
      "title": "Alpaca: A strong, replicable instruction-following model",
      "abstract": "",
      "year": "2023",
      "venue": "Stanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/",
      "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023."
    },
    {
      "index": 120,
      "title": "UL2: Unifying Language Learning Paradigms",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,\nHyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2022.",
      "orig_title": "Ul2: Unifying language learning paradigms",
      "paper_id": "2205.05131v3"
    },
    {
      "index": 121,
      "title": "LaMDA: Language Models for Dialog Applications",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv\nKulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022.",
      "orig_title": "Lamda: Language models for dialog applications",
      "paper_id": "2201.08239v3"
    },
    {
      "index": 122,
      "title": "Sun Tianxiang and Qiu Xipeng",
      "abstract": "",
      "year": "2023",
      "venue": "Blog post txsun",
      "authors": "Sun Tianxiang and Qiu Xipeng. 2023."
    },
    {
      "index": 123,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and\nGuillaume Lample. 2023a.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 124,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023b.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 125,
      "title": "Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel\nBallesteros, Yassine Benajiba, Neha Ann John, Rishita Anubhai, Smaranda\nMuresan, and Dan Roth. 2022.",
      "orig_title": "Instruction tuning for few-shot aspect-based sentiment analysis",
      "paper_id": "2210.06629v2"
    },
    {
      "index": 126,
      "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and\nSadao Kurohashi. 2023.",
      "orig_title": "Gpt-re: In-context learning for relation extraction using large language models",
      "paper_id": "2305.02105v3"
    },
    {
      "index": 127,
      "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,\nChang Zhou, Jingren Zhou, and Hongxia Yang. 2022a.",
      "orig_title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
      "paper_id": "2202.03052v2"
    },
    {
      "index": 128,
      "title": "GPT-NER: Named Entity Recognition via Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang,\nJiwei Li, and Guoyin Wang. 2023a.",
      "orig_title": "Gpt-ner: Named entity recognition via large language models",
      "paper_id": "2304.10428v4"
    },
    {
      "index": 129,
      "title": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng,\nJunjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai Du.\n2023b.",
      "orig_title": "Instructuie: Multi-task instruction tuning for unified information extraction",
      "paper_id": "2304.08085v1"
    },
    {
      "index": 130,
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny\nZhou. 2022b.",
      "orig_title": "Self-consistency improves chain of thought reasoning in language models",
      "paper_id": "2203.11171v4"
    },
    {
      "index": 131,
      "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,\nKhyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hanna Hajishirzi. 2023c.",
      "orig_title": "How far can camels go? exploring the state of instruction tuning on open resources",
      "paper_id": "2306.04751v2"
    },
    {
      "index": 132,
      "title": "Self-instruct: Aligning language model with self generated instructions",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2022c."
    },
    {
      "index": 133,
      "title": "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva\nNaik, David Stap, et al. 2022d."
    },
    {
      "index": 134,
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi,\nF. Xia, Quoc Le, and Denny Zhou. 2022."
    },
    {
      "index": 135,
      "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun\nXie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023.",
      "orig_title": "Zero-shot information extraction via chatting with chatgpt",
      "paper_id": "2302.10205v2"
    },
    {
      "index": 136,
      "title": "Reframing human-ai collaboration for generating free-text explanations",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi.\n2021."
    },
    {
      "index": 137,
      "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Empirical Methods in Natural Language\nProcessing",
      "authors": "",
      "orig_title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "paper_id": "2201.05966v3"
    },
    {
      "index": 138,
      "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang\nTao, and Daxin Jiang. 2023a.",
      "orig_title": "Wizardlm: Empowering large language models to follow complex instructions",
      "paper_id": "2304.12244v3"
    },
    {
      "index": 139,
      "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b.",
      "orig_title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
      "paper_id": "2304.01196v4"
    },
    {
      "index": 140,
      "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023c.",
      "orig_title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
      "paper_id": "2304.01196v4"
    },
    {
      "index": 141,
      "title": "End-to-End Slot Alignment and Recognition for Cross-Lingual NLU",
      "abstract": "",
      "year": "2004",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weijia Xu, Batool Haider, and Saab Mansour. 2020.",
      "orig_title": "End-to-end slot alignment and recognition for cross-lingual nlu",
      "paper_id": "2004.14353v2"
    },
    {
      "index": 142,
      "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
      "abstract": "",
      "year": "2022",
      "venue": "ArXiv, abs/",
      "authors": "Zhiyang Xu, Ying Shen, and Lifu Huang. 2022.",
      "orig_title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "paper_id": "2212.10773v3"
    },
    {
      "index": 143,
      "title": "Instruction in the wild: A user-based instruction dataset",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Fuzhao Xue, Kabir Jain, Mahir Hitesh Shah, Zangwei Zheng, and Yang You. 2023."
    },
    {
      "index": 144,
      "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming\nJiang, Bing Yin, and Xia Hu. 2023.",
      "orig_title": "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "paper_id": "2304.13712v2"
    },
    {
      "index": 145,
      "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022a.",
      "orig_title": "Re3: Generating longer stories with recursive reprompting and revision",
      "paper_id": "2210.06774v3"
    },
    {
      "index": 146,
      "title": "Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing\nChen, and Jun Xie. 2022b.",
      "orig_title": "Tailor: A prompt-based approach to attribute-based controlled text generation",
      "paper_id": "2204.13362v1"
    },
    {
      "index": 147,
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan\nCao, and Karthik Narasimhan. 2023.",
      "orig_title": "Tree of thoughts: Deliberate problem solving with large language models",
      "paper_id": "2305.10601v2"
    },
    {
      "index": 148,
      "title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li,\nLu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Wanli Ouyang, and Jing Shao.\n2023.",
      "orig_title": "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark",
      "paper_id": "2306.06687v3"
    },
    {
      "index": 149,
      "title": "Yulan-chat: An open-source bilingual chatbot",
      "abstract": "",
      "year": "2023",
      "venue": "https://github.com/RUC-GSAI/YuLan-Chat",
      "authors": "YuLan-Chat-Team. 2023."
    },
    {
      "index": 150,
      "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 1–9. Association for Computational\nLinguistics",
      "authors": "Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.",
      "orig_title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "paper_id": "2106.10199v5"
    },
    {
      "index": 151,
      "title": "Chinese open instruction generalist: A preliminary release",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu,\nZhaoqun Li, Zekun Wang, Chenghua Lin, Wen-Fen Huang, and Jie Fu.\n2023a."
    },
    {
      "index": 152,
      "title": "Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hang Zhang, Xin Li, and Lidong Bing. 2023b.",
      "orig_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "paper_id": "2306.02858v4"
    },
    {
      "index": 153,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor\nMihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh\nKoura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a.",
      "orig_title": "Opt: Open pre-trained transformer language models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 154,
      "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023c.",
      "orig_title": "Pmc-vqa: Visual instruction tuning for medical visual question answering",
      "paper_id": "2305.10415v6"
    },
    {
      "index": 155,
      "title": "Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang,\nLu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu. 2022b.",
      "orig_title": "Bamboo: Building mega-scale vision dataset continually with human-machine synergy",
      "paper_id": "2203.07845v2"
    },
    {
      "index": 156,
      "title": "Multi-Task Instruction Tuning of LLaMA for Specific Scenarios: A Preliminary Study on Writing Assistance",
      "abstract": "",
      "year": "",
      "venue": "ArXiv, abs/",
      "authors": "Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi.\n2023d.",
      "orig_title": "Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance",
      "paper_id": "2305.13225v2"
    },
    {
      "index": 157,
      "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.",
      "orig_title": "Calibrate before use: Improving few-shot performance of language models",
      "paper_id": "2102.09690v2"
    },
    {
      "index": 158,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.",
      "orig_title": "A survey of large language models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 159,
      "title": "LIMA: Less Is More for Alignment",
      "abstract": "",
      "year": "2023",
      "venue": "ArXiv, abs/",
      "authors": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\nMa, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\nZettlemoyer, and Omer Levy. 2023.",
      "orig_title": "Lima: Less is more for alignment",
      "paper_id": "2305.11206v1"
    },
    {
      "index": 160,
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",
      "orig_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "paper_id": "2304.10592v2"
    }
  ]
}