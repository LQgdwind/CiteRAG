{
  "paper_id": "2309.01219v2",
  "title": "Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
  "abstract": "Abstract\nWhile large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge.\nThis phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios.\nIn this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs.\nWe present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Evaluating correctness and faithfulness of instruction-following models for question answering",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.16877",
      "authors": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy"
    },
    {
      "index": 1,
      "title": "Do Language Models Know When They’re Hallucinating References?",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.18248",
      "authors": "Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai",
      "orig_title": "Do language models know when they’re hallucinating references?",
      "paper_id": "2305.18248v3"
    },
    {
      "index": 2,
      "title": "MEGA: Multilingual Evaluation of Generative AI",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.12528",
      "authors": "Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al.",
      "orig_title": "Mega: Multilingual evaluation of generative ai",
      "paper_id": "2303.12528v4"
    },
    {
      "index": 3,
      "title": "Tracing knowledge in language models back to the training data",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2205.11482",
      "authors": "Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu"
    },
    {
      "index": 4,
      "title": "Self-consuming generative models go mad",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.01850",
      "authors": "Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Baraniuk"
    },
    {
      "index": 5,
      "title": "The Internal State of an LLM Knows When It’s Lying",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2304.13734",
      "authors": "Amos Azaria and Tom Mitchell",
      "orig_title": "The internal state of an llm knows when its lying",
      "paper_id": "2304.13734v2"
    },
    {
      "index": 6,
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.05862",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.",
      "orig_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "paper_id": "2204.05862v1"
    },
    {
      "index": 7,
      "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.04023",
      "authors": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.",
      "orig_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "paper_id": "2302.04023v4"
    },
    {
      "index": 8,
      "title": "Predicting sentences using n-gram language models",
      "abstract": "",
      "year": "2005",
      "venue": "human language technology conference and conference on empirical methods in natural language processing",
      "authors": "Steffen Bickel, Peter Haider, and Tobias Scheffer"
    },
    {
      "index": 9,
      "title": "Improving language models by retrieving from trillions of tokens",
      "abstract": "",
      "year": "2022",
      "venue": "International conference on machine learning",
      "authors": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.",
      "orig_title": "Improving language models by retrieving from trillions of tokens",
      "paper_id": "2112.04426v3"
    },
    {
      "index": 10,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 11,
      "title": "Neural machine translation with monolingual translation memory",
      "abstract": "",
      "year": "2021",
      "venue": "59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "authors": "Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu"
    },
    {
      "index": 12,
      "title": "Factual Error Correction for Abstractive Summarization Models",
      "abstract": "",
      "year": "2020",
      "venue": "2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "authors": "Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung",
      "orig_title": "Factual error correction for abstractive summarization models",
      "paper_id": "2010.08712v2"
    },
    {
      "index": 13,
      "title": "Instruction mining: High-quality instruction data selection for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.06290",
      "authors": "Yihan Cao, Yanbin Kang, and Lichao Sun"
    },
    {
      "index": 14,
      "title": "Bias and fairness in natural language processing",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts",
      "authors": "Kai-Wei Chang, Vinodkumar Prabhakaran, and Vicente Ordonez"
    },
    {
      "index": 15,
      "title": "A Survey on Evaluation of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.03109",
      "authors": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al.",
      "orig_title": "A survey on evaluation of large language models",
      "paper_id": "2307.03109v9"
    },
    {
      "index": 16,
      "title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.14908",
      "authors": "Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu",
      "orig_title": "Purr: Efficiently editing language model hallucinations by denoising language model corruptions",
      "paper_id": "2305.14908v1"
    },
    {
      "index": 17,
      "title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers",
      "abstract": "",
      "year": "2017",
      "venue": "Acm Sigkdd Explorations Newsletter",
      "authors": "Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang",
      "orig_title": "A survey on dialogue systems: Recent advances and new frontiers",
      "paper_id": "1711.01731v3"
    },
    {
      "index": 18,
      "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.08701",
      "authors": "Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al.",
      "orig_title": "Alpagasus: Training a better alpaca with fewer data",
      "paper_id": "2307.08701v5"
    },
    {
      "index": 19,
      "title": "FacTool: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.13528",
      "authors": "I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu",
      "orig_title": "Factool: Factuality detection in generative ai – a tool augmented framework for multi-task and multi-domain scenarios",
      "paper_id": "2307.13528v2"
    },
    {
      "index": 20,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.02311",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 21,
      "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2309.03883",
      "authors": "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He",
      "orig_title": "Dola: Decoding by contrasting layers improves factuality in large language models",
      "paper_id": "2309.03883v2"
    },
    {
      "index": 22,
      "title": "Scaling Instruction-Finetuned Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.11416",
      "authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.",
      "orig_title": "Scaling instruction-finetuned language models",
      "paper_id": "2210.11416v5"
    },
    {
      "index": 23,
      "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.13281",
      "authors": "Roi Cohen, May Hamri, Mor Geva, and Amir Globerson",
      "orig_title": "Lm vs lm: Detecting factual errors via cross examination",
      "paper_id": "2305.13281v1"
    },
    {
      "index": 24,
      "title": "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin"
    },
    {
      "index": 25,
      "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation",
      "abstract": "",
      "year": "2021",
      "venue": "EMNLP",
      "authors": "Leyang Cui, Yu Wu, Shujie Liu, and Yue Zhang",
      "orig_title": "Knowledge enhanced fine-tuning for better handling unseen entities in dialogue generation",
      "paper_id": "2109.05487v1"
    },
    {
      "index": 26,
      "title": "Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better",
      "abstract": "",
      "year": "2023",
      "venue": "61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023",
      "authors": "David Dale, Elena Voita, Loïc Barrault, and Marta R. Costa-jussà"
    },
    {
      "index": 27,
      "title": "Editing Factual Knowledge in Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "2021 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Nicola De Cao, Wilker Aziz, and Ivan Titov",
      "orig_title": "Editing factual knowledge in language models",
      "paper_id": "2104.08164v2"
    },
    {
      "index": 28,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 29,
      "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2309.11495",
      "authors": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston",
      "orig_title": "Chain-of-verification reduces hallucination in large language models",
      "paper_id": "2309.11495v2"
    },
    {
      "index": 30,
      "title": "A survey for in-context learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2301.00234",
      "authors": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui"
    },
    {
      "index": 31,
      "title": "Understanding Iterative Revision from Human-Written Text",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.03802",
      "authors": "Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa Lopez, and Dongyeop Kang",
      "orig_title": "Understanding iterative revision from human-written text",
      "paper_id": "2203.03802v2"
    },
    {
      "index": 32,
      "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.14325",
      "authors": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch",
      "orig_title": "Improving factuality and reasoning in language models through multiagent debate",
      "paper_id": "2305.14325v1"
    },
    {
      "index": 33,
      "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.01379",
      "authors": "Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu",
      "orig_title": "Shifting attention to relevance: Towards the uncertainty estimation of large language models",
      "paper_id": "2307.01379v3"
    },
    {
      "index": 34,
      "title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
      "abstract": "",
      "year": "2020",
      "venue": "58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020",
      "authors": "Esin Durmus, He He, and Mona T. Diab",
      "orig_title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "paper_id": "2005.03754v1"
    },
    {
      "index": 35,
      "title": "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?",
      "abstract": "",
      "year": "2022",
      "venue": "2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "authors": "Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy",
      "orig_title": "On the origin of hallucinations in conversational models: Is it the datasets or the models?",
      "paper_id": "2204.07931v1"
    },
    {
      "index": 36,
      "title": "Evaluating groundedness in dialogue systems: The BEGIN benchmark",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR, abs/2105.00071",
      "authors": "Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter"
    },
    {
      "index": 37,
      "title": "Improving factual consistency in summarization with compression-based post-editing",
      "abstract": "",
      "year": "2022",
      "venue": "2022 Conference on Empirical Methods in Natural Language Processing",
      "authors": "Alex Fabbri, Prafulla Kumar Choubey, Jesse Vig, Chien-Sheng Wu, and Caiming Xiong"
    },
    {
      "index": 38,
      "title": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2309.03118",
      "authors": "Chao Feng, Xinyu Zhang, and Zichu Fei",
      "orig_title": "Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs",
      "paper_id": "2309.03118v1"
    },
    {
      "index": 39,
      "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.00955",
      "authors": "Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, et al.",
      "orig_title": "Bridging the gap: A survey on integrating (human) feedback for natural language generation",
      "paper_id": "2305.00955v2"
    },
    {
      "index": 40,
      "title": "Scaling Laws for Reward Model Overoptimization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Leo Gao, John Schulman, and Jacob Hilton",
      "orig_title": "Scaling laws for reward model overoptimization",
      "paper_id": "2210.10760v1"
    },
    {
      "index": 41,
      "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty,\nYicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al.\n2023a.",
      "orig_title": "Rarr: Researching and revising what language models say, using language models",
      "paper_id": "2210.08726v3"
    },
    {
      "index": 42,
      "title": "Enabling large language models to generate text with citations",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b."
    },
    {
      "index": 43,
      "title": "Creating training corpora for NLG micro-planners",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura\nPerez-Beltrachini. 2017."
    },
    {
      "index": 44,
      "title": "A survey on bias in deep nlp",
      "abstract": "",
      "year": "2021",
      "venue": "Applied Sciences, 11(7):",
      "authors": "Ismael Garrido-Muñoz, Arturo Montejo-Ráez, Fernando\nMartínez-Santiago, and L Alfonso Ureña-López. 2021."
    },
    {
      "index": 45,
      "title": "Reinforcement learning for language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Yoav Goldberg. 2023."
    },
    {
      "index": 46,
      "title": "Critic: Large language models can self-correct with tool-interactive critiquing",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and\nWeizhu Chen. 2023."
    },
    {
      "index": 47,
      "title": "Hallucinations in Large Multilingual Translation Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra\nBirch, Pierre Colombo, and André FT Martins. 2023a.",
      "orig_title": "Hallucinations in large multilingual translation models",
      "paper_id": "2303.16104v1"
    },
    {
      "index": 48,
      "title": "Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2023, Dubrovnik,\nCroatia, May 2-6, 2023, pages 1059–1075. Association for Computational\nLinguistics",
      "authors": "Nuno Miguel Guerreiro, Elena Voita, and André F. T. Martins.\n2023b.",
      "orig_title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
      "paper_id": "2208.05309v2"
    },
    {
      "index": 49,
      "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anisha Gunjal, Jihan Yin, and Erhan Bas. 2023.",
      "orig_title": "Detecting and preventing hallucinations in large vision language models",
      "paper_id": "2308.06394v3"
    },
    {
      "index": 50,
      "title": "On Calibration of Modern Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International conference on machine learning",
      "authors": "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017.",
      "orig_title": "On calibration of modern neural networks",
      "paper_id": "1706.04599v2"
    },
    {
      "index": 51,
      "title": "The curious case of neural text degeneration",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019."
    },
    {
      "index": 52,
      "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, and Lei Ma.\n2023a.",
      "orig_title": "Look before you leap: An exploratory study of uncertainty measurement for large language models",
      "paper_id": "2307.10236v4"
    },
    {
      "index": 53,
      "title": "Transformer-Patcher: One Mistake worth One Neuron",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.\n2023b.",
      "orig_title": "Transformer-patcher: One mistake worth one neuron",
      "paper_id": "2301.09785v1"
    },
    {
      "index": 54,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Computing Surveys, 55(12):1–38",
      "authors": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023.",
      "orig_title": "Survey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 55,
      "title": "How can we know when language models know? on the calibration of language models for question answering",
      "abstract": "",
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics,\n9:962–977",
      "authors": "Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021."
    },
    {
      "index": 56,
      "title": "Language Models (Mostly) Know What They Know",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan\nPerez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022.",
      "orig_title": "Language models (mostly) know what they know",
      "paper_id": "2207.05221v4"
    },
    {
      "index": 57,
      "title": "Challenges and Applications of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta\nRaileanu, and Robert McHardy. 2023.",
      "orig_title": "Challenges and applications of large language models",
      "paper_id": "2307.10169v1"
    },
    {
      "index": 58,
      "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,\nZhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, et al. 2023.",
      "orig_title": "Openassistant conversations–democratizing large language model alignment",
      "paper_id": "2304.07327v2"
    },
    {
      "index": 59,
      "title": "Evaluating the factual consistency of abstractive text summarization",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP",
      "authors": "Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020."
    },
    {
      "index": 60,
      "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck\nDernoncourt, Trung Bui, and Thien Huu Nguyen. 2023.",
      "orig_title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "paper_id": "2304.05613v1"
    },
    {
      "index": 61,
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. 2019.",
      "orig_title": "Albert: A lite bert for self-supervised learning of language representations",
      "paper_id": "1909.11942v6"
    },
    {
      "index": 62,
      "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison,\nDanny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion,\net al. 2023.",
      "orig_title": "Measuring faithfulness in chain-of-thought reasoning",
      "paper_id": "2307.13702v1"
    },
    {
      "index": 63,
      "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai\nGrigorev. 2022.",
      "orig_title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
      "paper_id": "2203.05115v2"
    },
    {
      "index": 64,
      "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ariel N Lee, Cole J Hunter, and Nataniel Ruiz. 2023.",
      "orig_title": "Platypus: Quick, cheap, and powerful refinement of llms",
      "paper_id": "2308.07317v2"
    },
    {
      "index": 65,
      "title": "Hallucinations in neural machine translation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David\nSussillo. 2019."
    },
    {
      "index": 66,
      "title": "Factuality Enhanced Language Models for Open-Ended Text Generation",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad\nShoeybi, and Bryan Catanzaro. 2022.",
      "orig_title": "Factuality enhanced language models for open-ended text generation",
      "paper_id": "2206.04624v3"
    },
    {
      "index": 67,
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics",
      "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a.",
      "orig_title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "paper_id": "1910.13461v1"
    },
    {
      "index": 68,
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n33:",
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020b.",
      "orig_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "paper_id": "2005.11401v4"
    },
    {
      "index": 69,
      "title": "A Survey on Retrieval-Augmented Text Generation",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022a.",
      "orig_title": "A survey on retrieval-augmented text generation",
      "paper_id": "2202.01110v2"
    },
    {
      "index": 70,
      "title": "Halueval: A large-scale hallucination evaluation benchmark for large language models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.\n2023a."
    },
    {
      "index": 71,
      "title": "Pretrained Language Models for Text Generation: A Survey",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.\n2022b.",
      "orig_title": "Pretrained language models for text generation: A survey",
      "paper_id": "2105.10311v2"
    },
    {
      "index": 72,
      "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin\nWattenberg. 2023b.",
      "orig_title": "Inference-time intervention: Eliciting truthful answers from a language model",
      "paper_id": "2306.03341v6"
    },
    {
      "index": 73,
      "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Miaoran Li, Baolin Peng, and Zhu Zhang. 2023c.",
      "orig_title": "Self-checker: Plug-and-play modules for fact-checking with large language models",
      "paper_id": "2305.14623v2"
    },
    {
      "index": 74,
      "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis",
      "abstract": "",
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics:\nACL",
      "authors": "Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan\nLiu, Zhenzhou Ji, Xin Jiang, and Qun Liu. 2022c.",
      "orig_title": "How pre-trained language models capture factual knowledge? a causal-inspired analysis",
      "paper_id": "2203.16747v1"
    },
    {
      "index": 75,
      "title": "Chain of knowledge: A framework for grounding large language models with structured knowledge bases",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq\nJoty, and Soujanya Poria. 2023d."
    },
    {
      "index": 76,
      "title": "Evaluating Object Hallucination in Large Vision-Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.\n2023e.",
      "orig_title": "Evaluating object hallucination in large vision-language models",
      "paper_id": "2305.10355v3"
    },
    {
      "index": 77,
      "title": "Sébastien Bubeck",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya\nGunasekar, and Yin Tat Lee. 2023f."
    },
    {
      "index": 78,
      "title": "Batgpt: A bidirectional autoregessive talker from generative pre-trained transformer",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang.\n2023g."
    },
    {
      "index": 79,
      "title": "Let’s Verify Step by Step",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy\nLee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.",
      "orig_title": "Let’s verify step by step",
      "paper_id": "2305.20050v1"
    },
    {
      "index": 80,
      "title": "Rouge: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "Chin-Yew Lin. 2004."
    },
    {
      "index": 81,
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.",
      "orig_title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "paper_id": "2109.07958v2"
    },
    {
      "index": 82,
      "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023.",
      "orig_title": "Generating with confidence: Uncertainty quantification for black-box large language models",
      "paper_id": "2305.19187v3"
    },
    {
      "index": 83,
      "title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener,\nDevang Agrawal, D’Autume Cyprien De Masson, Tim Scholtes, Manzil Zaheer,\nSusannah Young, et al. 2022.",
      "orig_title": "Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models",
      "paper_id": "2205.11388v1"
    },
    {
      "index": 84,
      "title": "Aligning large multi-modal model with robust instruction tuning",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\n2023a."
    },
    {
      "index": 85,
      "title": "Visual Instruction Tuning",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.",
      "orig_title": "Visual instruction tuning",
      "paper_id": "2304.08485v2"
    },
    {
      "index": 86,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Jerry Liu. 2022.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 87,
      "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong\nWen. 2023c.",
      "orig_title": "Reta-llm: A retrieval-augmented large language model toolkit",
      "paper_id": "2306.05212v1"
    },
    {
      "index": 88,
      "title": "Lost in the Middle: How Language Models Use Long Contexts",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023d.",
      "orig_title": "Lost in the middle: How language models use long contexts",
      "paper_id": "2307.03172v3"
    },
    {
      "index": 89,
      "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and\nBill Dolan. 2022.",
      "orig_title": "A token-level reference-free hallucination detection benchmark for free-form text generation",
      "paper_id": "2104.08704v2"
    },
    {
      "index": 90,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "1907",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 91,
      "title": "Zero-Resource Hallucination Prevention for Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Junyu Luo, Cao Xiao, and Fenglong Ma. 2023a.",
      "orig_title": "Zero-resource hallucination prevention for large language models",
      "paper_id": "2309.02654v3"
    },
    {
      "index": 92,
      "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023b."
    },
    {
      "index": 93,
      "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin,\nand Daxin Jiang. 2023c.",
      "orig_title": "Augmented large language models with parametric knowledge guiding",
      "paper_id": "2305.04757v2"
    },
    {
      "index": 94,
      "title": "Time waits for no one! analysis and challenges of temporal misalignment",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies",
      "authors": "Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A\nSmith. 2022."
    },
    {
      "index": 95,
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023."
    },
    {
      "index": 96,
      "title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and\nHannaneh Hajishirzi. 2023."
    },
    {
      "index": 97,
      "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.",
      "orig_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "paper_id": "2303.08896v3"
    },
    {
      "index": 98,
      "title": "On Faithfulness and Factuality in Abstractive Summarization",
      "abstract": "",
      "year": "1919",
      "venue": "Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL",
      "authors": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020.",
      "orig_title": "On faithfulness and factuality in abstractive summarization",
      "paper_id": "2005.00661v1"
    },
    {
      "index": 99,
      "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson,\nand Mark Steedman. 2023.",
      "orig_title": "Sources of hallucination by large language models on inference tasks",
      "paper_id": "2305.14552v2"
    },
    {
      "index": 100,
      "title": "Locating and Editing Factual Associations in GPT",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a.",
      "orig_title": "Locating and editing factual associations in gpt",
      "paper_id": "2202.05262v5"
    },
    {
      "index": 101,
      "title": "Mass-Editing Memory in a Transformer",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.\n2022b.",
      "orig_title": "Mass-editing memory in a transformer",
      "paper_id": "2210.07229v2"
    },
    {
      "index": 102,
      "title": "Augmented Language Models: a Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,\nRam Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, et al. 2023.",
      "orig_title": "Augmented language models: a survey",
      "paper_id": "2302.07842v1"
    },
    {
      "index": 103,
      "title": "Recurrent neural network based language model",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev\nKhudanpur. 2010."
    },
    {
      "index": 104,
      "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Computing Surveys",
      "authors": "Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen,\nOscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021.",
      "orig_title": "Recent advances in natural language processing via large pre-trained language models: A survey",
      "paper_id": "2111.01243v1"
    },
    {
      "index": 105,
      "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh,\nMohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.",
      "orig_title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
      "paper_id": "2305.14251v2"
    },
    {
      "index": 106,
      "title": "Memory-Based Model Editing at Scale",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and\nChelsea Finn. 2022.",
      "orig_title": "Memory-based model editing at scale",
      "paper_id": "2206.06520v1"
    },
    {
      "index": 107,
      "title": "HaLo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Elaraby Mohamed, Lu Mengyin, Dunn Jacob, Zhang Xueying, Wang Yu, and Liu\nShizhu. 2023.",
      "orig_title": "Halo: Estimation and reduction of hallucinations in open-source weak large language models",
      "paper_id": "2308.11764v4"
    },
    {
      "index": 108,
      "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov,\nOmri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023.",
      "orig_title": "Generating benchmarks for factuality evaluation of language models",
      "paper_id": "2307.06908v2"
    },
    {
      "index": 109,
      "title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023."
    },
    {
      "index": 110,
      "title": "Fixing Model Bugs with Natural Language Patches",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing",
      "authors": "Shikhar Murty, Christopher Manning, Scott Lundberg, and Marco Tulio Ribeiro.\n2022.",
      "orig_title": "Fixing model bugs with natural language patches",
      "paper_id": "2211.03318v2"
    },
    {
      "index": 111,
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.",
      "orig_title": "Webgpt: Browser-assisted question-answering with human feedback",
      "paper_id": "2112.09332v3"
    },
    {
      "index": 112,
      "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial\nintelligence",
      "authors": "Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.",
      "orig_title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "paper_id": "1611.04230v1"
    },
    {
      "index": 113,
      "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 2, Short Papers",
      "authors": "Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2017.",
      "orig_title": "Jfleg: A fluency corpus and benchmark for grammatical error correction",
      "paper_id": "1702.04066v1"
    },
    {
      "index": 114,
      "title": "Biases in large language models: Origins, inventory and discussion",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Journal of Data and Information Quality",
      "authors": "Roberto Navigli, Simone Conia, and Björn Ross. 2023."
    },
    {
      "index": 115,
      "title": "Large Dual Encoders Are Generalizable Retrievers",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, pages 9844–9855. Association for Computational\nLinguistics",
      "authors": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego,\nJi Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2022.",
      "orig_title": "Large dual encoders are generalizable retrievers",
      "paper_id": "2112.07899v1"
    },
    {
      "index": 116,
      "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. 2023.",
      "orig_title": "Skeleton-of-thought: Large language models can do parallel decoding",
      "paper_id": "2307.15337v3"
    },
    {
      "index": 117,
      "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yasumasa Onoe, Michael JQ Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol\nChoi. 2023.",
      "orig_title": "Can lms learn new entities from descriptions? challenges in propagating injected knowledge",
      "paper_id": "2305.01651v1"
    },
    {
      "index": 118,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "OpenAI. 2023a.",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 119,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "OpenAI. 2023b.",
      "orig_title": "Gpt-4 technical report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 120,
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022."
    },
    {
      "index": 121,
      "title": "ToTTo: A Controlled Table-To-Text Generation Dataset",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP)",
      "authors": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra,\nDiyi Yang, and Dipanjan Das. 2020.",
      "orig_title": "ToTTo: A controlled table-to-text generation dataset",
      "paper_id": "2004.14373v3"
    },
    {
      "index": 122,
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S Bernstein. 2023.",
      "orig_title": "Generative agents: Interactive simulacra of human behavior",
      "paper_id": "2304.03442v2"
    },
    {
      "index": 123,
      "title": "Faster and smaller n-gram language models",
      "abstract": "",
      "year": "2011",
      "venue": "Proceedings of the 49th annual meeting of the Association\nfor Computational Linguistics: Human Language Technologies",
      "authors": "Adam Pauls and Dan Klein. 2011."
    },
    {
      "index": 124,
      "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023.",
      "orig_title": "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
      "paper_id": "2306.01116v1"
    },
    {
      "index": 125,
      "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan\nHuang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023a."
    },
    {
      "index": 126,
      "title": "Instruction Tuning with GPT-4",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.\n2023b.",
      "orig_title": "Instruction tuning with gpt-4",
      "paper_id": "2304.03277v1"
    },
    {
      "index": 127,
      "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen,\nEdwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu,\nSaurav Kadavath, et al. 2022.",
      "orig_title": "Discovering language model behaviors with model-written evaluations",
      "paper_id": "2212.09251v1"
    },
    {
      "index": 128,
      "title": "Summarization is (Almost) Dead",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023.",
      "orig_title": "Summarization is (almost) dead",
      "paper_id": "2309.09558v1"
    },
    {
      "index": 129,
      "title": "“Merge Conflicts!” Exploring the Impacts of External Distractors to Parametric Knowledge Graphs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Cheng Qian, Xinran Zhao, and Sherry Tongshuang Wu. 2023.",
      "orig_title": "merge conflicts!\" exploring the impacts of external distractors to parametric knowledge graphs",
      "paper_id": "2309.08594v1"
    },
    {
      "index": 130,
      "title": "Making Language Models Better Tool Learners with Execution Feedback",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. 2023.",
      "orig_title": "Making language models better tool learners with execution feedback",
      "paper_id": "2305.13068v3"
    },
    {
      "index": 131,
      "title": "Tool Learning with Foundation Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni\nZeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023.",
      "orig_title": "Tool learning with foundation models",
      "paper_id": "2304.08354v3"
    },
    {
      "index": 132,
      "title": "Pre-trained Models for Natural Language Processing: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "Science China Technological Sciences, 63(10):",
      "authors": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\n2020.",
      "orig_title": "Pre-trained models for natural language processing: A survey",
      "paper_id": "2003.08271v4"
    },
    {
      "index": 133,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019."
    },
    {
      "index": 134,
      "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny\nHernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė\nLukošiūtė, et al. 2023.",
      "orig_title": "Question decomposition improves the faithfulness of model-generated reasoning",
      "paper_id": "2307.11768v2"
    },
    {
      "index": 135,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "The Journal of Machine Learning Research, 21(1):",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 136,
      "title": "In-Context Retrieval-Augmented Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin\nLeyton-Brown, and Yoav Shoham. 2023.",
      "orig_title": "In-context retrieval-augmented language models",
      "paper_id": "2302.00083v3"
    },
    {
      "index": 137,
      "title": "Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Vipula Rawte, Prachi Priya, SM Tonmoy, SM Zaman, Amit Sheth, and Amitava Das.\n2023.",
      "orig_title": "Exploring the relationship between llm hallucinations and prompt linguistic nuances: Readability, formality, and concreteness",
      "paper_id": "2309.11064v1"
    },
    {
      "index": 138,
      "title": "Controlling hallucinations at word level in data-to-text generation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Clément Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten,\nRossella Cancelliere, and Patrick Gallinari. 2022."
    },
    {
      "index": 139,
      "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu,\nJi-Rong Wen, and Wang Haifeng. 2023.",
      "orig_title": "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
      "paper_id": "2307.11019v3"
    },
    {
      "index": 140,
      "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP)",
      "authors": "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.",
      "orig_title": "How much knowledge can you pack into the parameters of a language model?",
      "paper_id": "2002.08910v4"
    },
    {
      "index": 141,
      "title": "The probabilistic relevance framework: Bm25 and beyond",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": "Stephen Robertson, Hugo Zaragoza, et al. 2009."
    },
    {
      "index": 142,
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,\nDaniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al. 2022.",
      "orig_title": "Bloom: A 176b-parameter open-access multilingual language model",
      "paper_id": "2211.05100v4"
    },
    {
      "index": 143,
      "title": "Reinforcement learning from human feedback: Progress and challenges",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "John Schulman. 2023."
    },
    {
      "index": 144,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017."
    },
    {
      "index": 145,
      "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 40th International Conference on Machine\nLearning, volume 202",
      "authors": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi,\nNathanael Schärli, and Denny Zhou. 2023a.",
      "orig_title": "Large language models can be easily distracted by irrelevant context",
      "paper_id": "2302.00093v3"
    },
    {
      "index": 146,
      "title": "Natural Language to Code Translation with Execution",
      "abstract": "",
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing",
      "authors": "Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I.\nWang. 2022.",
      "orig_title": "Natural language to code translation with execution",
      "paper_id": "2204.11454v2"
    },
    {
      "index": 147,
      "title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and\nScott Wen-tau Yih. 2023b.",
      "orig_title": "Trusting your evidence: Hallucinate less with context-aware decoding",
      "paper_id": "2305.14739v1"
    },
    {
      "index": 148,
      "title": "RePlug: Retrieval-Augmented Black-Box Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2023c.",
      "orig_title": "Replug: Retrieval-augmented black-box language models",
      "paper_id": "2301.12652v4"
    },
    {
      "index": 149,
      "title": "Prompting GPT-3 To Be Reliable",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan\nBoyd-Graber, and Lijuan Wang. 2022.",
      "orig_title": "Prompting gpt-3 to be reliable",
      "paper_id": "2210.09150v2"
    },
    {
      "index": 150,
      "title": "Editable Neural Networks",
      "abstract": "",
      "year": "2004",
      "venue": "arXiv preprint arXiv:",
      "authors": "Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem\nBabenko. 2020.",
      "orig_title": "Editable neural networks",
      "paper_id": "2004.00345v2"
    },
    {
      "index": 151,
      "title": "PandaGPT: One Model To Instruction-Follow Them All",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023.",
      "orig_title": "Pandagpt: One model to instruction-follow them all",
      "paper_id": "2305.16355v1"
    },
    {
      "index": 152,
      "title": "Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong.\n2023a.",
      "orig_title": "Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs?",
      "paper_id": "2308.10168v2"
    },
    {
      "index": 153,
      "title": "Black-Box Tuning for Language-Model-as-a-Service",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022.",
      "orig_title": "Black-box tuning for language-model-as-a-service",
      "paper_id": "2201.03514v4"
    },
    {
      "index": 154,
      "title": "Moss: Training conversational language models from synthetic data",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan,\nXiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng,\nZhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang,\nLingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu.\n2023b."
    },
    {
      "index": 155,
      "title": "Task Ambiguity in Humans and Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. 2022.",
      "orig_title": "Task ambiguity in humans and language models",
      "paper_id": "2212.10711v1"
    },
    {
      "index": 156,
      "title": "Stanford alpaca: An instruction-following llama model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023."
    },
    {
      "index": 157,
      "title": "Behavioral cloning from observation",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 27th International Joint Conference on\nArtificial Intelligence",
      "authors": "Faraz Torabi, Garrett Warnell, and Peter Stone. 2018."
    },
    {
      "index": 158,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023a.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 159,
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\net al. 2023b.",
      "orig_title": "Llama 2: Open foundation and fine-tuned chat models",
      "paper_id": "2307.09288v2"
    },
    {
      "index": 160,
      "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023.",
      "orig_title": "Med-halt: Medical domain hallucination test for large language models",
      "paper_id": "2307.15343v2"
    },
    {
      "index": 161,
      "title": "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023."
    },
    {
      "index": 162,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 163,
      "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation",
      "abstract": "",
      "year": "2005",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chaojun Wang and Rico Sennrich. 2020.",
      "orig_title": "On exposure bias, hallucination and domain shift in neural machine translation",
      "paper_id": "2005.03642v1"
    },
    {
      "index": 164,
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023a.",
      "orig_title": "Voyager: An open-ended embodied agent with large language models",
      "paper_id": "2305.16291v2"
    },
    {
      "index": 165,
      "title": "Revisiting Challenges in Data-to-Text Generation with Fact Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 12th International Conference on Natural\nLanguage Generation",
      "authors": "Hongmin Wang. 2019.",
      "orig_title": "Revisiting challenges in data-to-text generation with fact grounding",
      "paper_id": "2001.03830v1"
    },
    {
      "index": 166,
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning\nRepresentations",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022.",
      "orig_title": "Self-consistency improves chain of thought reasoning in language models",
      "paper_id": "2203.11171v4"
    },
    {
      "index": 167,
      "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,\nKhyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith,\nIz Beltagy, et al. 2023b.",
      "orig_title": "How far can camels go? exploring the state of instruction tuning on open resources",
      "paper_id": "2306.04751v2"
    },
    {
      "index": 168,
      "title": "Self-instruct: Aligning language models with self-generated instructions",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2023c."
    },
    {
      "index": 169,
      "title": "Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji.\n2023d."
    },
    {
      "index": 170,
      "title": "Jailbroken: How Does LLM Safety Training Fail?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023a.",
      "orig_title": "Jailbroken: How does llm safety training fail?",
      "paper_id": "2307.02483v1"
    },
    {
      "index": 171,
      "title": "Finetuned Language Models Are Zero-Shot Learners",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M Dai, and Quoc V Le. 2021.",
      "orig_title": "Finetuned language models are zero-shot learners",
      "paper_id": "2109.01652v5"
    },
    {
      "index": 172,
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems,\n35:",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\nLe, Denny Zhou, et al. 2022."
    },
    {
      "index": 173,
      "title": "Simple synthetic data reduces sycophancy in large language models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. 2023b.",
      "orig_title": "Simple synthetic data reduces sycophancy in large language models",
      "paper_id": "2308.03958v2"
    },
    {
      "index": 174,
      "title": "Qafacteval: Improved qa-based factual consistency evaluation for summarization",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Alexander R Fabbri Chien-Sheng Wu and Wenhao Liu Caiming Xiong. 2023."
    },
    {
      "index": 175,
      "title": "On decoder-only architecture for speech-to-text and large language model integration",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu\nLi, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023a."
    },
    {
      "index": 176,
      "title": "Do PLMs Know and Understand Ontological Knowledge?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie, and Kewei Tu.\n2023b.",
      "orig_title": "Do plms know and understand ontological knowledge?",
      "paper_id": "2309.05936v1"
    },
    {
      "index": 177,
      "title": "On Hallucination and Predictive Uncertainty in Conditional Language Generation",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume",
      "authors": "Yijun Xiao and William Yang Wang. 2021.",
      "orig_title": "On hallucination and predictive uncertainty in conditional language generation",
      "paper_id": "2103.15025v1"
    },
    {
      "index": 178,
      "title": "Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023."
    },
    {
      "index": 179,
      "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan\nHooi. 2023.",
      "orig_title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
      "paper_id": "2306.13063v2"
    },
    {
      "index": 180,
      "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023.",
      "orig_title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
      "paper_id": "2304.01196v4"
    },
    {
      "index": 181,
      "title": "React: Synergizing reasoning and acting in language models",
      "abstract": "",
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning\nRepresentations",
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan,\nand Yuan Cao. 2022."
    },
    {
      "index": 182,
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\nWang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023."
    },
    {
      "index": 183,
      "title": "Do Large Language Models Know What They Don’t Know?",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing\nHuang. 2023.",
      "orig_title": "Do large language models know what they don’t know?",
      "paper_id": "2305.18153v2"
    },
    {
      "index": 184,
      "title": "Kola: Carefully benchmarking world knowledge of large language models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao\nPeng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. 2023a."
    },
    {
      "index": 185,
      "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal.\n2023b.",
      "orig_title": "Improving language models via plug-and-play retrieval feedback",
      "paper_id": "2305.14002v1"
    },
    {
      "index": 186,
      "title": "Automatic Evaluation of Attribution by Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023.",
      "orig_title": "Automatic evaluation of attribution by large language models",
      "paper_id": "2305.06311v2"
    },
    {
      "index": 187,
      "title": "Decoding methods in neural language generation: a survey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Sina Zarrieß, Henrik Voigt, and Simeon Schüz. 2021."
    },
    {
      "index": 188,
      "title": "Glm-130b: An open bilingual pre-trained model",
      "abstract": "",
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning\nRepresentations",
      "authors": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi\nYang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022."
    },
    {
      "index": 189,
      "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
      "abstract": "",
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.",
      "orig_title": "AlignScore: Evaluating factual consistency with a unified alignment function",
      "paper_id": "2305.16739v1"
    },
    {
      "index": 190,
      "title": "How Language Model Hallucinations Can Snowball",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith.\n2023a.",
      "orig_title": "How language model hallucinations can snowball",
      "paper_id": "2305.13534v1"
    },
    {
      "index": 191,
      "title": "Instruction Tuning for Large Language Models: A Survey",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,\nJiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023b.",
      "orig_title": "Instruction tuning for large language models: A survey",
      "paper_id": "2308.10792v8"
    },
    {
      "index": 192,
      "title": "Mitigating language model hallucination with interactive question-knowledge alignment",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang.\n2023c."
    },
    {
      "index": 193,
      "title": "BERTScore: Evaluating Text Generation with BERT",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\n2019.",
      "orig_title": "Bertscore: Evaluating text generation with bert",
      "paper_id": "1904.09675v3"
    },
    {
      "index": 194,
      "title": "Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan,\nand Victor Ruhle. 2023d.",
      "orig_title": "Hybrid retrieval-augmented generation for real-time composition assistance",
      "paper_id": "2308.04215v3"
    },
    {
      "index": 195,
      "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing.\n2023a.",
      "orig_title": "Verify-and-edit: A knowledge-enhanced chain-of-thought framework",
      "paper_id": "2305.03268v1"
    },
    {
      "index": 196,
      "title": "Automatic calibration and error correction for large language models via pareto optimal self-supervision",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Theodore Zhao, Mu Wei, J Samuel Preston, and Hoifung Poon. 2023b."
    },
    {
      "index": 197,
      "title": "Dense Text Retrieval based on Pretrained Language Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2022.",
      "orig_title": "Dense text retrieval based on pretrained language models: A survey",
      "paper_id": "2211.14876v1"
    },
    {
      "index": 198,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.\n2023c.",
      "orig_title": "A survey of large language models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 199,
      "title": "Can We Edit Factual Knowledge by In-Context Learning?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao\nChang. 2023a.",
      "orig_title": "Can we edit factual knowledge by in-context learning?",
      "paper_id": "2305.12740v1"
    },
    {
      "index": 200,
      "title": "Secrets of RLHF in Large Language Models Part I: PPO",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie\nJin, Qin Liu, Limao Xiong, Lu Chen, et al. 2023b.",
      "orig_title": "Secrets of rlhf in large language models part i: Ppo",
      "paper_id": "2307.04964v2"
    },
    {
      "index": 201,
      "title": "Why Does ChatGPT Fall Short in Providing Truthful Answers?",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023c.",
      "orig_title": "Why does chatgpt fall short in providing truthful answers",
      "paper_id": "2304.10513v3"
    },
    {
      "index": 202,
      "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 5905–5921.\nAssociation for Computational Linguistics",
      "authors": "Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha,\nAhmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and\nDragomir R. Radev. 2021.",
      "orig_title": "Qmsum: A new benchmark for query-based multi-domain meeting summarization",
      "paper_id": "2104.05938v1"
    },
    {
      "index": 203,
      "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi\nChen. 2023.",
      "orig_title": "Mquake: Assessing knowledge editing in language models via multi-hop questions",
      "paper_id": "2305.14795v3"
    },
    {
      "index": 204,
      "title": "LIMA: Less Is More for Alignment",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\nMa, Avia Efrat, Ping Yu, Lili Yu, et al. 2023a.",
      "orig_title": "Lima: Less is more for alignment",
      "paper_id": "2305.11206v1"
    },
    {
      "index": 205,
      "title": "Context-faithful Prompting for Large Language Models",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023b.",
      "orig_title": "Context-faithful prompting for large language models",
      "paper_id": "2303.11315v2"
    },
    {
      "index": 206,
      "title": "Enhancing Factual Consistency of Abstractive Summarization",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies",
      "authors": "Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng,\nXuedong Huang, and Meng Jiang. 2021.",
      "orig_title": "Enhancing factual consistency of abstractive summarization",
      "paper_id": "2003.08612v8"
    },
    {
      "index": 207,
      "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang,\nLinyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.",
      "orig_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
      "paper_id": "2306.04528v5"
    },
    {
      "index": 208,
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.",
      "orig_title": "Universal and transferable adversarial attacks on aligned language models",
      "paper_id": "2307.15043v2"
    }
  ]
}