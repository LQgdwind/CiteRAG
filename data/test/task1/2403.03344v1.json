{
  "paper_id": "2403.03344v1",
  "title": "Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation",
  "abstract": "Abstract\nThe increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains.\nHere, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code’s “green capacity”, based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Tracking Clean Energy Progress 2023",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "IEA"
    },
    {
      "index": 1,
      "title": "Recalibrating global data center energy-use estimates",
      "abstract": "",
      "year": "2020",
      "venue": "Science",
      "authors": "Eric Masanet, Arman Shehabi, Nuoa Lei and Sarah Smith"
    },
    {
      "index": 2,
      "title": "Energy consumption of data centers worldwide How will the Internet become green?",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Ralph Hintemann and Simon Hinterholzer"
    },
    {
      "index": 3,
      "title": "Data Centres Metered Electricity Consumption 2022",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ireland Central Statistics Office"
    },
    {
      "index": 4,
      "title": "AI and Compute",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Amodei and  Hernandez"
    },
    {
      "index": 5,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "Association for Computational Linguistics",
      "authors": "Emma Strubell, Ananya Ganesh and Andrew McCallum",
      "orig_title": "Energy and Policy Considerations for Deep Learning in NLP",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 6,
      "title": "Carbon Emissions and Large Neural Network Training",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Patterson et al."
    },
    {
      "index": 7,
      "title": "Green AI",
      "abstract": "",
      "year": "2020",
      "venue": "Communications of the ACM",
      "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith and Oren Etzioni"
    },
    {
      "index": 8,
      "title": "Trends in AI inference energy consumption: beyond the performance-vs-parameter laws of deep learning",
      "abstract": "",
      "year": "2023",
      "venue": "Sustainable Computing: Informatics and Systems",
      "authors": "Radosvet Desislavov, Fernando Martinez-Plumed and Jose Hernander-Orallo"
    },
    {
      "index": 9,
      "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities",
      "abstract": "",
      "year": "2022",
      "venue": "Machine Learning and Systems",
      "authors": "Carole-Jean Wu et al.",
      "orig_title": "Sustainable ai: Environmental implications, challenges and opportunities",
      "paper_id": "2111.00364v2"
    },
    {
      "index": 10,
      "title": "World Energy Outlook 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "IEA"
    },
    {
      "index": 11,
      "title": "Carbon Dependencies in Datacenter Design and Management",
      "abstract": "",
      "year": "2023",
      "venue": "ACM SIGEnergy Energy Informatics Review",
      "authors": "Bilge Acun et al."
    },
    {
      "index": 12,
      "title": "Program Synthesis",
      "abstract": "",
      "year": "2017",
      "venue": "Foundations and Trends® in Programming Languages",
      "authors": "Sumit Gulwani, Oleksandr Polozov and Rishabh Singh"
    },
    {
      "index": 13,
      "title": "Reconciling enumerative and deductive program synthesis",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI",
      "authors": "Kangjing Huang, Xiaokang Qiu, Peiyuan Shen and Yanjun Wang"
    },
    {
      "index": 14,
      "title": "Write, Execute, Assess: Program Synthesis with a REPL",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Kevin Ellis et al.",
      "orig_title": "Write, execute, assess: Program synthesis with a repl",
      "paper_id": "1906.04604v1"
    },
    {
      "index": 15,
      "title": "Dimensions in Program Synthesis",
      "abstract": "",
      "year": "2010",
      "venue": "ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming, PPDP",
      "authors": "Sumit Gulwani"
    },
    {
      "index": 16,
      "title": "The World’s Leading Online Programming Learning Platform — leetcode.com",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "LeetCode"
    },
    {
      "index": 17,
      "title": "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Peter Henderson et al."
    },
    {
      "index": 18,
      "title": "Data Center Cooling using Model-predictive Control",
      "abstract": "",
      "year": "2018",
      "venue": "Neural Information Processing Systems",
      "authors": "Nevena Lazic et al."
    },
    {
      "index": 19,
      "title": "Quantifying the Carbon Emissions of Machine Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt and Thomas Dandres"
    },
    {
      "index": 20,
      "title": "The Evolved Transformer",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "David Richard So, Quoc V. Le and Chen Liang",
      "orig_title": "The Evolved Transformer",
      "paper_id": "1901.11117v4"
    },
    {
      "index": 21,
      "title": "Constructing Fast Network through Deconstruction of Convolution",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yunho Jeon and Junmo Kim",
      "orig_title": "Constructing Fast Network through Deconstruction of Convolution",
      "paper_id": "1806.07370v5"
    },
    {
      "index": 22,
      "title": "Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions",
      "abstract": "",
      "year": "2018",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "authors": "Zheng Qin et al.",
      "orig_title": "Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions",
      "paper_id": "1803.09926v1"
    },
    {
      "index": 23,
      "title": "RAPL: memory power estimation and capping",
      "abstract": "",
      "year": "2010",
      "venue": "ACM/IEEE international symposium on Low power electronics and design, ISLPED",
      "authors": "Howard David et al."
    },
    {
      "index": 24,
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Mark Sandler et al.",
      "orig_title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "paper_id": "1801.04381v4"
    },
    {
      "index": 25,
      "title": "Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Mahmoud Assran et al.",
      "orig_title": "Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning",
      "paper_id": "1906.04585v2"
    },
    {
      "index": 26,
      "title": "Accelerating Reinforcement Learning through GPU Atari Emulation",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Neural Information Processing Systems, NIPS",
      "authors": "Steven Dalton and Iuri Frosio",
      "orig_title": "Accelerating Reinforcement Learning through GPU Atari Emulation",
      "paper_id": "1907.08467v2"
    },
    {
      "index": 27,
      "title": "An Analysis of Deep Neural Network Models for Practical Applications",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv",
      "authors": "Alfredo Canziani, Adam Paszke and Eugenio Culurciello",
      "orig_title": "An Analysis of Deep Neural Network Models for Practical Applications",
      "paper_id": "1605.07678v4"
    },
    {
      "index": 28,
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Mingxing Tan and Quoc V. Le",
      "orig_title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "paper_id": "1905.11946v5"
    },
    {
      "index": 29,
      "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
      "abstract": "",
      "year": "2018",
      "venue": "Lecture Notes in Computer Science",
      "authors": "Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng and Jian Sun",
      "orig_title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
      "paper_id": "1807.11164v1"
    },
    {
      "index": 30,
      "title": "Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning",
      "abstract": "",
      "year": "2023",
      "venue": "Sustainable Computing: Informatics and Systems",
      "authors": "Radosvet Desislavov, Fernando Martínez-Plumed and José Hernández-Orallo"
    },
    {
      "index": 31,
      "title": "The New Linux ’ perf ’ Tools",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "Arnaldo Carvalho Melo and Red Hat"
    },
    {
      "index": 32,
      "title": "A process for analysing the energy efficiency of software",
      "abstract": "",
      "year": "2021",
      "venue": "Information and Software Technology",
      "authors": "Javier Mancebo, Félix García and Coral Calero"
    },
    {
      "index": 33,
      "title": "The Information Factories: Data centers are chewing up vast amounts of energy — So researchers are trying to make them more efficient",
      "abstract": "",
      "year": "2019",
      "venue": "Nature",
      "authors": "Nicola Jones"
    },
    {
      "index": 34,
      "title": "Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE International Conferences on Big Data and Cloud Computing (BDCloud)",
      "authors": "Dongsheng Li, Xiaoming Chen, Matteo Becci and Zhi Zong"
    },
    {
      "index": 35,
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "Emma Strubell, Ananya Ganesh and Andrew McCallum",
      "orig_title": "Energy and Policy Considerations for Deep Learning in NLP",
      "paper_id": "1906.02243v1"
    },
    {
      "index": 36,
      "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
      "abstract": "",
      "year": "2020",
      "venue": "Neural Information Processing Systems",
      "authors": "Dai,  Lai,  Yang and  Li",
      "orig_title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
      "paper_id": "2006.03236v1"
    },
    {
      "index": 37,
      "title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Fernandez et al."
    },
    {
      "index": 38,
      "title": "A method to estimate the energy consumption of deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Tianji Yang, Yu-Hsin Chen, Joel Emer and Vivienne Sze"
    },
    {
      "index": 39,
      "title": "Energy efficiency across programming languages: how do energy, time, and memory relate?",
      "abstract": "",
      "year": "2017",
      "venue": "ACM SIGPLAN international conference on software language engineering",
      "authors": "Rui Pereira et al."
    },
    {
      "index": 40,
      "title": "How is the speed of code review affected by activity, usage and code quality?",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv",
      "authors": "William Brown",
      "orig_title": "How is the speed of code review affected by activity, usage and code quality?",
      "paper_id": "2305.05770v1"
    },
    {
      "index": 41,
      "title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Burak Yetiştiren, Işık Özsoy, Miray Ayerdem and Eray Tüzün",
      "orig_title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
      "paper_id": "2304.10778v2"
    },
    {
      "index": 42,
      "title": "A Dataset for Analysis of Quality Code and Toxic Comments",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jaime Sayago Heredia, Gustavo Chango, Ricardo Pérez-Castillo and Mario Piattini"
    },
    {
      "index": 43,
      "title": "Assessing the Quality of GitHub Copilot’s Code Generation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Burak Yetiştiren, Eray Tüzün and Işık Özsoy"
    },
    {
      "index": 44,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Sanh,  Debut,  Chaumond and  Wolf",
      "orig_title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 45,
      "title": "Learning both Weights and Connections for Efficient Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "NIPS",
      "authors": "Song Han, Jeff Pool, John Tran and William J. Dally",
      "orig_title": "Learning both Weights and Connections for Efficient Neural Networks",
      "paper_id": "1506.02626v3"
    },
    {
      "index": 46,
      "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "Dally Han"
    },
    {
      "index": 47,
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint",
      "authors": "Shazeer Fedus",
      "orig_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "paper_id": "2101.03961v3"
    },
    {
      "index": 48,
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint",
      "authors": "Noam Shazeer et al.",
      "orig_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "paper_id": "1701.06538v1"
    },
    {
      "index": 49,
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Dmitry Lepikhin et al."
    },
    {
      "index": 50,
      "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "Jesse Dodge et al.",
      "orig_title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
      "paper_id": "2002.06305v1"
    },
    {
      "index": 51,
      "title": "Measuring the Carbon Intensity of AI in Cloud Instances",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Conference on Fairness, Accountability, and Transparency",
      "authors": "Dodge et al."
    },
    {
      "index": 52,
      "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "Journal of Machine Learning Research",
      "authors": "Luccioni,  Viguier and  Ligozat",
      "orig_title": "ESTIMATING THE CARBON FOOTPRINT OF BLOOM, A 176B PARAMETER LANGUAGE MODEL",
      "paper_id": "2211.02001v1"
    },
    {
      "index": 53,
      "title": "Investigating the Use of Natural Language Processing for Automated Code Generation",
      "abstract": "",
      "year": "2023",
      "venue": "SSRN Electronic Journal",
      "authors": "Tapomoy Adhikari"
    },
    {
      "index": 54,
      "title": "Automated Code generation, status, quality and validity in 2022",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Esko Malinen and Jesse Nygrén"
    },
    {
      "index": 55,
      "title": "Green AI: Do Deep Learning Frameworks Have Different Costs?",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Stefanos Georgiou et al."
    },
    {
      "index": 56,
      "title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Burak Yetistiren, Isık Özsoy, Miray Ayerdem and Eray Tüzün",
      "orig_title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
      "paper_id": "2304.10778v2"
    },
    {
      "index": 57,
      "title": "PROW: A Step Toward Automatic Program Writing",
      "abstract": "",
      "year": "1969",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Richard J. Waldinger and Richard C.. Lee"
    },
    {
      "index": 58,
      "title": "On the Synthesis of a Reactive Module",
      "abstract": "",
      "year": "1989",
      "venue": "ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL",
      "authors": "A. Pnueli and R. Rosner"
    },
    {
      "index": 59,
      "title": "Stochastic superoptimization",
      "abstract": "",
      "year": "2013",
      "venue": "Architectural Support for Programming Languages and Operating Systems, ASPLOS",
      "authors": "Eric Schkufza, Rahul Sharma and Alex Aiken"
    },
    {
      "index": 60,
      "title": "Superoptimizer - A Look at the Smallest Program",
      "abstract": "",
      "year": "1987",
      "venue": "Architectural Support for Programming Languages and Operating Systems, ASPLOS",
      "authors": "Henry Massalin"
    },
    {
      "index": 61,
      "title": "A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "Xiaotao Song, Hailong Sun, Xu Wang and Jiafei Yan"
    },
    {
      "index": 62,
      "title": "An Empirical Evaluation of GitHub Copilot’s Code Suggestions",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Mining Software Repositories, MSR",
      "authors": "Nhan Nguyen and Sarah Nadi"
    },
    {
      "index": 63,
      "title": "Jigsaw: Large Language Models meet Program Synthesis",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/ACM International Conference on Software Engineering, ICSE",
      "authors": "Naman Jain et al.",
      "orig_title": "Jigsaw: Large Language Models meet Program Synthesis",
      "paper_id": "2112.02969v1"
    },
    {
      "index": 64,
      "title": "Angelix: scalable multiline program patch synthesis via symbolic analysis",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Software Engineering, ICSE",
      "authors": "Sergey Mechtaev, Jooyong Yi and Abhik Roychoudhury"
    },
    {
      "index": 65,
      "title": "LooPy: interactive program synthesis with control structures",
      "abstract": "",
      "year": "2021",
      "venue": "Proc. ACM Program. Lang.",
      "authors": "Kasra Ferdowsifard et al."
    },
    {
      "index": 66,
      "title": "A Systematic Evaluation of Large Language Models of Code",
      "abstract": "",
      "year": "2022",
      "venue": "ACM SIGPLAN International Symposium on Machine Programming, MAPS@PLDI",
      "authors": "Frank F. Xu, Uri Alon, Graham Neubig and Vincent Josua Hellendoorn",
      "orig_title": "A systematic evaluation of large language models of code",
      "paper_id": "2202.13169v3"
    },
    {
      "index": 67,
      "title": "A systematic literature review of green software metrics",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Patricia Lago, Qing Gu and Paolo Bozzelli"
    },
    {
      "index": 68,
      "title": "Green AI: Do Deep Learning Frameworks Have Different Costs?",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Software Engineering, ICSE",
      "authors": "Stefanos Georgiou et al."
    }
  ]
}