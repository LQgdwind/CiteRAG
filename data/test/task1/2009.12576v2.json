{
  "paper_id": "2009.12576v2",
  "title": "Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics",
  "abstract": "Abstract\nA fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent’s actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal.\nWe first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning.\nCrucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent. Our method successfully recovers the true model of rational agents. This approach provides a foundation for interpreting the behavioral and neural dynamics of animal brains during complex tasks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Massively parallel architectures for al: Netl, thistle, and boltzmann machines",
      "abstract": "",
      "year": "1983",
      "venue": "National Conference on Artificial Intelligence, AAAI",
      "authors": "Scott E Fahlman, Geoffrey E Hinton, and Terrence J Sejnowski"
    },
    {
      "index": 1,
      "title": "Learning agents for uncertain environments",
      "abstract": "",
      "year": "1998",
      "venue": "Computational learning theory",
      "authors": "Stuart Russell"
    },
    {
      "index": 2,
      "title": "Inverse reinforcement learning in partially observable environments",
      "abstract": "",
      "year": "2011",
      "venue": "Journal of Machine Learning Research",
      "authors": "Jaedeug Choi and Kee-Eung Kim"
    },
    {
      "index": 3,
      "title": "Apprenticeship learning about multiple intentions",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Machine Learning (ICML-11)",
      "authors": "Monica Babes, Vukosi Marivate, Kaushik Subramanian, and Michael L Littman"
    },
    {
      "index": 4,
      "title": "Inverse optimal control with linearly-solvable mdps",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Machine Learning (ICML-10)",
      "authors": "Krishnamurthy Dvijotham and Emanuel Todorov"
    },
    {
      "index": 5,
      "title": "I see what you see: Inferring sensor and policy models of human real-world motor behavior",
      "abstract": "",
      "year": "2017",
      "venue": "AAAI",
      "authors": "Felix Schmitt, Hans-Joachim Bieg, Michael Herman, and Constantin A Rothkopf"
    },
    {
      "index": 6,
      "title": "Rational thoughts in neural codes",
      "abstract": "",
      "year": "2020",
      "venue": "National Academy of Sciences of the United States of America",
      "authors": "Z. Wu, M. Kwon, S. Daptardar, P. Schrater, and X. Pitkow"
    },
    {
      "index": 7,
      "title": "Reinforcement learning: An introduction",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "Richard S Sutton, Andrew G Barto, et al."
    },
    {
      "index": 8,
      "title": "Optimal control of markov processes with incomplete state information",
      "abstract": "",
      "year": "1965",
      "venue": "Journal of Mathematical Analysis and Applications",
      "authors": "Karl Johan Åström"
    },
    {
      "index": 9,
      "title": "Planning and acting in partially observable stochastic domains",
      "abstract": "",
      "year": "1998",
      "venue": "Artificial intelligence",
      "authors": "Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra"
    },
    {
      "index": 10,
      "title": "Decision making under uncertainty: a neural model based on partially observable markov decision processes",
      "abstract": "",
      "year": "2010",
      "venue": "Frontiers in computational neuroscience",
      "authors": "Rajesh PN Rao"
    },
    {
      "index": 11,
      "title": "Theories of bounded rationality",
      "abstract": "",
      "year": "1972",
      "venue": "Decision and organization",
      "authors": "Herbert A Simon"
    },
    {
      "index": 12,
      "title": "Algorithms for inverse reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "Icml",
      "authors": "Andrew Y Ng, Stuart J Russell, et al."
    },
    {
      "index": 13,
      "title": "Apprenticeship learning via inverse reinforcement learning",
      "abstract": "",
      "year": "2004",
      "venue": "Machine learning",
      "authors": "Pieter Abbeel and Andrew Y Ng"
    },
    {
      "index": 14,
      "title": "Maximum margin planning",
      "abstract": "",
      "year": "2006",
      "venue": "International Conference on Machine Learning",
      "authors": "Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich"
    },
    {
      "index": 15,
      "title": "Inferring the function performed by a recurrent neural network",
      "abstract": "",
      "year": "2019",
      "venue": "bioRxiv",
      "authors": "Matthew Chalk, Gašper Tkačik, and Olivier Marre"
    },
    {
      "index": 16,
      "title": "Maximum entropy inverse reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "AAAI",
      "authors": "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey"
    },
    {
      "index": 17,
      "title": "Information theory and statistical mechanics",
      "abstract": "",
      "year": "1957",
      "venue": "Physical review",
      "authors": "Edwin T Jaynes"
    },
    {
      "index": 18,
      "title": "Learning Task Specifications from Demonstrations",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K Ho, and Sanjit Seshia",
      "orig_title": "Learning task specifications from demonstrations",
      "paper_id": "1710.03875v5"
    },
    {
      "index": 19,
      "title": "Maximum likelihood constraint inference for inverse reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Dexter RR Scobee and S Shankar Sastry"
    },
    {
      "index": 20,
      "title": "Generative adversarial imitation learning",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jonathan Ho and Stefano Ermon"
    },
    {
      "index": 21,
      "title": "A bayesian approach to generative adversarial imitation learning",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wonseok Jeon, Seokin Seo, and Kee-Eung Kim"
    },
    {
      "index": 22,
      "title": "ADAIL: Adaptive Adversarial Imitation Learning",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS Workshop on Learning Transferable Skills",
      "authors": "Balaraman Ravindran and Sergey Levine",
      "orig_title": "Adail: Adaptive adversarial imitation learning",
      "paper_id": "2008.12647v1"
    },
    {
      "index": 23,
      "title": "State-only Imitation with Transition Dynamics Mismatch",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Tanmay Gangwani and Jian Peng",
      "orig_title": "State-only imitation with transition dynamics mismatch",
      "paper_id": "2002.11879v1"
    },
    {
      "index": 24,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 25,
      "title": "Fast Context Adaptation via Meta-Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson",
      "orig_title": "Fast context adaptation via meta-learning",
      "paper_id": "1810.03642v4"
    },
    {
      "index": 26,
      "title": "Meta-learning curiosity algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Ferran Alet, Martin F Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling",
      "orig_title": "Meta-learning curiosity algorithms",
      "paper_id": "2003.05325v1"
    },
    {
      "index": 27,
      "title": "Meta-q-learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola"
    },
    {
      "index": 28,
      "title": "Meta reinforcement learning as task inference",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and Nicolas Heess"
    },
    {
      "index": 29,
      "title": "Bayesian Model-Agnostic Meta-Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn",
      "orig_title": "Bayesian model-agnostic meta-learning",
      "paper_id": "1806.03836v4"
    },
    {
      "index": 30,
      "title": "Bayesian brain: Probabilistic approaches to neural coding",
      "abstract": "",
      "year": "2007",
      "venue": "MIT press",
      "authors": "Kenji Doya, Shin Ishii, Alexandre Pouget, and Rajesh PN Rao"
    },
    {
      "index": 31,
      "title": "Decision theory, reinforcement learning, and the brain",
      "abstract": "",
      "year": "2008",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience",
      "authors": "Peter Dayan and Nathaniel D Daw"
    },
    {
      "index": 32,
      "title": "Reward optimization in the primate brain: A probabilistic model of decision making under uncertainty",
      "abstract": "",
      "year": "2013",
      "venue": "PloS one",
      "authors": "Yanping Huang and Rajesh PN Rao"
    },
    {
      "index": 33,
      "title": "Cognitive tomography reveals complex, task-independent mental representations",
      "abstract": "",
      "year": "2013",
      "venue": "Current Biology",
      "authors": "Neil MT Houlsby, Ferenc Huszár, Mohammad M Ghassemi, Gergő Orbán, Daniel M Wolpert, and Máté Lengyel"
    },
    {
      "index": 34,
      "title": "Observing the observer (i): meta-bayesian models of learning and decision-making",
      "abstract": "",
      "year": "2010",
      "venue": "PloS one",
      "authors": "Jean Daunizeau, Hanneke EM Den Ouden, Matthias Pessiglione, Stefan J Kiebel, Klaas E Stephan, and Karl J Friston"
    },
    {
      "index": 35,
      "title": "Observing the observer (ii): deciding when to decide",
      "abstract": "",
      "year": "2010",
      "venue": "PLoS one",
      "authors": "Jean Daunizeau, Hanneke EM Den Ouden, Matthias Pessiglione, Stefan J Kiebel, Karl J Friston, and Klaas E Stephan"
    },
    {
      "index": 36,
      "title": "Cognitive Model Priors for Predicting Human Decisions",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "David D Bourgin, Joshua C Peterson, Daniel Reichman, Stuart J Russell, and Thomas L Griffiths",
      "orig_title": "Cognitive model priors for predicting human decisions",
      "paper_id": "1905.09397v1"
    },
    {
      "index": 37,
      "title": "Rational adaptation under task and processing constraints: Implications for testing theories of cognition and action",
      "abstract": "",
      "year": "2009",
      "venue": "Psychological review",
      "authors": "Andrew Howes, Richard L Lewis, and Alonso Vera"
    },
    {
      "index": 38,
      "title": "Computational rationality: Linking mechanism and behavior through bounded utility maximization",
      "abstract": "",
      "year": "2014",
      "venue": "Topics in cognitive science",
      "authors": "Richard L Lewis, Andrew Howes, and Satinder Singh"
    },
    {
      "index": 39,
      "title": "Resource-rational analysis: understanding human cognition as the optimal use of limited computational resources",
      "abstract": "",
      "year": "2020",
      "venue": "Behavioral and Brain Sciences",
      "authors": "Falk Lieder and Thomas L Griffiths"
    },
    {
      "index": 40,
      "title": "Not noisy, just wrong: the role of suboptimal inference in behavioral variability",
      "abstract": "",
      "year": "2012",
      "venue": "Neuron",
      "authors": "Jeffrey M Beck, Wei Ji Ma, Xaq Pitkow, Peter E Latham, and Alexandre Pouget"
    },
    {
      "index": 41,
      "title": "A dynamic bayesian observer model reveals origins of bias in visual path integration",
      "abstract": "",
      "year": "2018",
      "venue": "Neuron",
      "authors": "Kaushik J. Lakshminarasimhan, Marina Petsalis, Hyeshin Park, Gregory C. DeAngelis, Xaq Pitkow, and Dora E. Angelaki"
    },
    {
      "index": 42,
      "title": "Tracking the mind’s eye: Primate gaze behavior during virtual visuomotor navigation reflects belief dynamics",
      "abstract": "",
      "year": "2020",
      "venue": "Neuron",
      "authors": "Kaushik J Lakshminarasimhan, Eric Avila, Erin Neyhart, Gregory C DeAngelis, Xaq Pitkow, and Dora E Angelaki"
    },
    {
      "index": 43,
      "title": "The animal-ai olympics",
      "abstract": "",
      "year": "2019",
      "venue": "Nature Machine Intelligence",
      "authors": "Matthew Crosby, Benjamin Beyret, and Marta Halina"
    },
    {
      "index": 44,
      "title": "The animal-AI testbed",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 45,
      "title": "A markovian decision process",
      "abstract": "",
      "year": "1957",
      "venue": "Journal of mathematics and mechanics",
      "authors": "Richard Bellman"
    },
    {
      "index": 46,
      "title": "Dynamic programming and markov processes",
      "abstract": "",
      "year": "1960",
      "venue": "",
      "authors": "Ronald A Howard"
    },
    {
      "index": 47,
      "title": "A set of successive approximation methods for discounted markovian decision problems",
      "abstract": "",
      "year": "1976",
      "venue": "Zeitschrift fuer operations research",
      "authors": "JAEE Van Nunen"
    },
    {
      "index": 48,
      "title": "Modified policy iteration algorithms for discounted markov decision problems",
      "abstract": "",
      "year": "1978",
      "venue": "Management Science",
      "authors": "Martin L Puterman and Moon Chirl Shin"
    },
    {
      "index": 49,
      "title": "Unscented filtering and nonlinear estimation",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE",
      "authors": "Simon J Julier and Jeffrey K Uhlmann"
    },
    {
      "index": 50,
      "title": "Flexible and accurate inference and learning for deep generative models",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Eszter Vértes and Maneesh Sahani",
      "orig_title": "Flexible and accurate inference and learning for deep generative models",
      "paper_id": "1805.11051v1"
    },
    {
      "index": 51,
      "title": "Representation learning: A review and new perspectives",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Yoshua Bengio, Aaron Courville, and Pascal Vincent"
    },
    {
      "index": 52,
      "title": "Continuous control with deep reinforcement learning",
      "abstract": "",
      "year": "2016",
      "venue": "ICLR",
      "authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra"
    },
    {
      "index": 53,
      "title": "Reinforcement learning in continuous action spaces through sequential monte carlo methods",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in neural information processing systems",
      "authors": "Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini"
    },
    {
      "index": 54,
      "title": "Actor-critic algorithms",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "Vijay R Konda and John N Tsitsiklis"
    },
    {
      "index": 55,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 56,
      "title": "Q-learning for continuous actions with cross-entropy guided policies",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Riley Simmons-Edler, Ben Eisner, Eric Mitchell, Sebastian Seung, and Daniel Lee"
    },
    {
      "index": 57,
      "title": "Pattern recognition and machine learning",
      "abstract": "",
      "year": "2006",
      "venue": "springer",
      "authors": "Christopher M Bishop"
    },
    {
      "index": 58,
      "title": "A dynamic bayesian observer model reveals origins of bias in visual path integration",
      "abstract": "",
      "year": "2018",
      "venue": "Neuron",
      "authors": "Kaushik J Lakshminarasimhan, Marina Petsalis, Hyeshin Park, Gregory C DeAngelis, Xaq Pitkow, and Dora E Angelaki"
    },
    {
      "index": 59,
      "title": "Rational quantitative attribution of beliefs, desires and percepts in human mentalizing",
      "abstract": "",
      "year": "2017",
      "venue": "Nature Human Behaviour",
      "authors": "Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum"
    },
    {
      "index": 60,
      "title": "Inferring learners’ knowledge from their actions",
      "abstract": "",
      "year": "2015",
      "venue": "Cognitive Science",
      "authors": "Anna N Rafferty, Michelle M LaMar, and Thomas L Griffiths"
    },
    {
      "index": 61,
      "title": "Bayesian theory of mind: Modeling joint belief-desire attribution",
      "abstract": "",
      "year": "2011",
      "venue": "annual meeting of the cognitive science society",
      "authors": "Chris Baker, Rebecca Saxe, and Joshua Tenenbaum"
    },
    {
      "index": 62,
      "title": "Maximum likelihood from incomplete data via the em algorithm",
      "abstract": "",
      "year": "1977",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "Arthur P Dempster, Nan M Laird, and Donald B Rubin"
    }
  ]
}