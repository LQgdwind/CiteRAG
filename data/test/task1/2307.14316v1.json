{
  "paper_id": "2307.14316v1",
  "title": "Reinforcement Learning by Guided Safe Exploration",
  "abstract": "Abstract\nSafety is critical to broadening the application of reinforcement learning (RL).\nOften, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world.\nHowever, the real-world target task might be unknown prior to deployment.\nReward-free RL trains an agent without the reward to adapt quickly once the reward is revealed.\nWe consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal.\nThis agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal.\nAfter the target task is revealed, safety violations are not allowed anymore.\nThus, the guide is leveraged to compose a safe behaviour policy.\nDrawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable\nand gradually eliminate the influence of the guide as training progresses.\nThe empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Near optimal behavior via approximate state abstraction",
      "abstract": "",
      "year": "2016",
      "venue": "ICML",
      "authors": "David Abel, D. Ellis Hershkowitz, and Michael L. Littman"
    },
    {
      "index": 1,
      "title": "Constrained Policy Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel",
      "orig_title": "Constrained Policy Optimization",
      "paper_id": "1705.10528v1"
    },
    {
      "index": 2,
      "title": "Safe Reinforcement Learning via Shielding",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI",
      "authors": "Mohammed Alshiekh, Roderick Bloem, Rüdiger Ehlers, Bettina Könighofer, Scott Niekum, and Ufuk Topcu",
      "orig_title": "Safe Reinforcement Learning via Shielding",
      "paper_id": "1708.08611v2"
    },
    {
      "index": 3,
      "title": "Constrained Markov decision processes",
      "abstract": "",
      "year": "1999",
      "venue": "CRC Press",
      "authors": "Eitan Altman"
    },
    {
      "index": 4,
      "title": "Curriculum learning",
      "abstract": "",
      "year": "2009",
      "venue": "ICML",
      "authors": "Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston"
    },
    {
      "index": 5,
      "title": "Constrained Optimization and Lagrange Multiplier Methods",
      "abstract": "",
      "year": "1982",
      "venue": "Academic press",
      "authors": "Dimitri P Bertsekas"
    },
    {
      "index": 6,
      "title": "An actor-critic algorithm for constrained Markov decision processes",
      "abstract": "",
      "year": "2005",
      "venue": "Systems & control letters",
      "authors": "Vivek S Borkar"
    },
    {
      "index": 7,
      "title": "Safe reinforcement learning via shielding under partial observability",
      "abstract": "",
      "year": "2023",
      "venue": "AAAI",
      "authors": "Steven Carr, Nils Jansen, Sebastian Junges, and Ufuk Topcu"
    },
    {
      "index": 8,
      "title": "Risk-constrained reinforcement learning with percentile risk criteria",
      "abstract": "",
      "year": "2017",
      "venue": "JMLR",
      "authors": "Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone"
    },
    {
      "index": 9,
      "title": "Recommender systems under european ai regulations",
      "abstract": "",
      "year": "2022",
      "venue": "Communications of the ACM",
      "authors": "Tommaso Di Noia, Nava Tintarev, Panagiota Fatourou, and Markus Schedl"
    },
    {
      "index": 10,
      "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
      "abstract": "",
      "year": "2021",
      "venue": "Mach. Learn.",
      "authors": "Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester"
    },
    {
      "index": 11,
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine",
      "orig_title": "Diversity is all you need: Learning skills without a reward function",
      "paper_id": "1802.06070v6"
    },
    {
      "index": 12,
      "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
      "abstract": "",
      "year": "2022",
      "venue": "ICLR",
      "authors": "Benjamin Eysenbach and Sergey Levine",
      "orig_title": "Maximum entropy RL (provably) solves some robust RL problems",
      "paper_id": "2103.06257v2"
    },
    {
      "index": 13,
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "paper_id": "1703.03400v3"
    },
    {
      "index": 14,
      "title": "A Comprehensive Survey on Safe Reinforcement Learning",
      "abstract": "",
      "year": "2015",
      "venue": "JMLR",
      "authors": "Javier García and Fernando Fernández"
    },
    {
      "index": 15,
      "title": "Multiple Plans are Better than One: Diverse Stochastic Planning",
      "abstract": "",
      "year": "2021",
      "venue": "ICAPS",
      "authors": "Mahsa Ghasemi, Evan Scope Crafts, Bo Zhao, and Ufuk Topcu"
    },
    {
      "index": 16,
      "title": "Risk-Aware Transfer in Reinforcement Learning using Successor Features",
      "abstract": "",
      "year": "2021",
      "venue": "NeurIPS",
      "authors": "Michael Gimelfarb, Andre Barreto, Scott Sanner, and Chi-Guhn Lee",
      "orig_title": "Risk-aware transfer in reinforcement learning using successor features",
      "paper_id": "2105.14127v1"
    },
    {
      "index": 17,
      "title": "Safe Reinforcement Learning through Meta-learned Instincts",
      "abstract": "",
      "year": "2020",
      "venue": "ALIFE",
      "authors": "Djordje Grbic and Sebastian Risi",
      "orig_title": "Safe Reinforcement Learning through Meta-learned Instincts",
      "paper_id": "2005.03233v1"
    },
    {
      "index": 18,
      "title": "Learning to Walk in the Real World with Minimal Human Effort",
      "abstract": "",
      "year": "2020",
      "venue": "CoRL",
      "authors": "Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, and Jie Tan",
      "orig_title": "Learning to Walk in the Real World with Minimal Human Effort",
      "paper_id": "2002.08550v3"
    },
    {
      "index": 19,
      "title": "Reinforcement Learning with Deep Energy-Based Policies",
      "abstract": "",
      "year": "2017",
      "venue": "ICML",
      "authors": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Reinforcement Learning with Deep Energy-Based Policies",
      "paper_id": "1702.08165v2"
    },
    {
      "index": 20,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 21,
      "title": "Soft Actor-Critic Algorithms and Applications",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine",
      "orig_title": "Soft Actor-Critic Algorithms and Applications",
      "paper_id": "1812.05905v2"
    },
    {
      "index": 22,
      "title": "Provably Efficient Maximum Entropy Exploration",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest",
      "orig_title": "Provably Efficient Maximum Entropy Exploration",
      "paper_id": "1812.02690v2"
    },
    {
      "index": 23,
      "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson",
      "orig_title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning",
      "paper_id": "2006.05826v4"
    },
    {
      "index": 24,
      "title": "Marginalized State Distribution Entropy Regularization in Policy Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Riashat Islam, Zafarali Ahmed, and Doina Precup",
      "orig_title": "Marginalized State Distribution Entropy Regularization in Policy Optimization",
      "paper_id": "1912.05128v1"
    },
    {
      "index": 25,
      "title": "Safe Reinforcement Learning Using Probabilistic Shields (Invited Paper)",
      "abstract": "",
      "year": "2020",
      "venue": "CONCUR",
      "authors": "Nils Jansen, Bettina Könighofer, Sebastian Junges, Alex Serban, and Roderick Bloem"
    },
    {
      "index": 26,
      "title": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IJCNN",
      "authors": "Thommen George Karimpanal, Santu Rana, Sunil Gupta, Truyen Tran, and Svetha Venkatesh",
      "orig_title": "Learning transferable domain priors for safe exploration in reinforcement learning",
      "paper_id": "1909.04307v5"
    },
    {
      "index": 27,
      "title": "One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn"
    },
    {
      "index": 28,
      "title": "Efficient Exploration via State Marginal Matching",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov",
      "orig_title": "Efficient Exploration via State Marginal Matching",
      "paper_id": "1906.05274v3"
    },
    {
      "index": 29,
      "title": "Safe Active Dynamics Learning and Control: A Sequential Exploration-Exploitation Framework",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Robotics",
      "authors": "Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, and Marco Pavone",
      "orig_title": "Safe Active Dynamics Learning and Control: A Sequential Exploration–Exploitation Framework",
      "paper_id": "2008.11700v4"
    },
    {
      "index": 30,
      "title": "Towards a Unified Theory of State Abstraction for MDPs",
      "abstract": "",
      "year": "2006",
      "venue": "AI&M",
      "authors": "Lihong Li, Thomas J Walsh, and Michael L Littman"
    },
    {
      "index": 31,
      "title": "MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Michael Luo, Ashwin Balakrishna, Brijen Thananjeyan, Suraj Nair, Julian Ibarz, Jie Tan, Chelsea Finn, Ion Stoica, and Ken Goldberg",
      "orig_title": "MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance",
      "paper_id": "2112.03575v1"
    },
    {
      "index": 32,
      "title": "Curriculum learning for safe mapless navigation",
      "abstract": "",
      "year": "2022",
      "venue": "SAC",
      "authors": "Luca Marzari, Davide Corsi, Enrico Marchesini, and Alessandro Farinelli"
    },
    {
      "index": 33,
      "title": "A Simple Reward-free Approach to Constrained Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "ICML",
      "authors": "Sobhan Miryoosefi and Chi Jin",
      "orig_title": "A simple reward-free approach to constrained reinforcement learning",
      "paper_id": "2107.05216v1"
    },
    {
      "index": 34,
      "title": "Human-level control through deep reinforcement learning",
      "abstract": "",
      "year": "2015",
      "venue": "Nature",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis"
    },
    {
      "index": 35,
      "title": "Safe Driving via Expert Guided Policy Optimization",
      "abstract": "",
      "year": "2022",
      "venue": "CoRL",
      "authors": "Zhenghao Peng, Quanyi Li, Chunxiao Liu, and Bolei Zhou",
      "orig_title": "Safe driving via expert guided policy optimization",
      "paper_id": "2110.06831v2"
    },
    {
      "index": 36,
      "title": "Density Constrained Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Zengyi Qin, Yuxiao Chen, and Chuchu Fan"
    },
    {
      "index": 37,
      "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Alex Ray, Joshua Achiam, and Dario Amodei"
    },
    {
      "index": 38,
      "title": "Entropy Maximization for Constrained Markov Decision Processes",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Allerton Conference on Communication, Control, and Computing",
      "authors": "Yagiz Savas, Melkior Ornik, Murat Cubuktepe, and Ufuk Topcu"
    },
    {
      "index": 39,
      "title": "The design of LEO: A 2D bipedal walking robot for online autonomous Reinforcement Learning",
      "abstract": "",
      "year": "2010",
      "venue": "IROS",
      "authors": "Erik Schuitema, Martijn Wisse, Thijs Ramakers, and Pieter Jonker"
    },
    {
      "index": 40,
      "title": "State Entropy Maximization with Random Encoders for Efficient Exploration",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee"
    },
    {
      "index": 41,
      "title": "AlwaysSafe: Reinforcement Learning Without Safety Constraint Violations During Training",
      "abstract": "",
      "year": "2021",
      "venue": "AAMAS",
      "authors": "Thiago D. Simão, Nils Jansen, and Matthijs T. J. Spaan"
    },
    {
      "index": 42,
      "title": "Learning to be Safe: Deep RL with a Safety Critic",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv",
      "authors": "Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn"
    },
    {
      "index": 43,
      "title": "Safe Exploration for Optimization with Gaussian Processes",
      "abstract": "",
      "year": "2015",
      "venue": "ICML",
      "authors": "Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause"
    },
    {
      "index": 44,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 45,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "JMLR",
      "authors": "Richard S. Sutton, A. Rupam Mahmood, and Martha White",
      "orig_title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 46,
      "title": "Maximum Entropy Model-based Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Oleg Svidchenko and Aleksei Shpilman"
    },
    {
      "index": 47,
      "title": "Transfer Learning for Reinforcement Learning Domains: A Survey",
      "abstract": "",
      "year": "2009",
      "venue": "JMLR",
      "authors": "Matthew E. Taylor and Peter Stone"
    },
    {
      "index": 48,
      "title": "Reward Constrained Policy Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "ICLR",
      "authors": "Chen Tessler, Daniel J. Mankowitz, and Shie Mannor",
      "orig_title": "Reward Constrained Policy Optimization",
      "paper_id": "1805.11074v3"
    },
    {
      "index": 49,
      "title": "Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters",
      "authors": "Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg",
      "orig_title": "Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones",
      "paper_id": "2010.15920v2"
    },
    {
      "index": 50,
      "title": "Safe Reinforcement Learning via Curriculum Induction",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal",
      "orig_title": "Safe Reinforcement Learning via Curriculum Induction",
      "paper_id": "2006.12136v2"
    },
    {
      "index": 51,
      "title": "Learning latent state representation for speeding up exploration",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Giulia Vezzani, Abhishek Gupta, Lorenzo Natale, and Pieter Abbeel"
    },
    {
      "index": 52,
      "title": "Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real",
      "abstract": "",
      "year": "2019",
      "venue": "CoRL",
      "authors": "Zhaoming Xie, Patrick Clary, Jeremy Dao, Pedro Morais, Jonathan W. Hurst, and Michiel van de Panne"
    },
    {
      "index": 53,
      "title": "WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "AAAI",
      "authors": "Qisong Yang, Thiago D. Simão, Simon H. Tindemans, and Matthijs T. J. Spaan"
    },
    {
      "index": 54,
      "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration",
      "abstract": "",
      "year": "2023",
      "venue": "AAAI",
      "authors": "Qisong Yang and Matthijs T. J. Spaan"
    },
    {
      "index": 55,
      "title": "Projection-Based Constrained Policy Optimization",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge",
      "orig_title": "Projection-Based Constrained Policy Optimization",
      "paper_id": "2010.03152v1"
    },
    {
      "index": 56,
      "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies",
      "abstract": "",
      "year": "2021",
      "venue": "ICML",
      "authors": "Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge",
      "orig_title": "Accelerating safe reinforcement learning with constraint-mismatched baseline policies",
      "paper_id": "2006.11645v3"
    },
    {
      "index": 57,
      "title": "Discovering Diverse Nearly Optimal Policies with Successor Features",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Tom Zahavy, Brendan O’Donoghue, André Barreto, Sebastian Flennerhag, Volodymyr Mnih, and Satinder Singh",
      "orig_title": "Discovering Diverse Nearly Optimal Policies with Successor Features",
      "paper_id": "2106.00669v2"
    },
    {
      "index": 58,
      "title": "Safe Continuous Control with Constrained Model-Based Policy Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "IROS",
      "authors": "Moritz A. Zanger, Karam Daaboul, and J. Marius Zöllner",
      "orig_title": "Safe Continuous Control with Constrained Model-Based Policy Optimization",
      "paper_id": "2104.06922v1"
    },
    {
      "index": 59,
      "title": "Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman",
      "orig_title": "Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings",
      "paper_id": "2008.06622v1"
    },
    {
      "index": 60,
      "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, and Jiayu Zhou",
      "orig_title": "Transfer learning in deep reinforcement learning: A survey",
      "paper_id": "2009.07888v7"
    },
    {
      "index": 61,
      "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy",
      "abstract": "",
      "year": "2010",
      "venue": "Carnegie Mellon University",
      "authors": "Brian D Ziebart"
    }
  ]
}