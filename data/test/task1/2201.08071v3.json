{
  "paper_id": "2201.08071v3",
  "title": "Temporal Sentence Grounding in Videos: A Survey and Future Directions",
  "abstract": "Abstract\nTemporal sentence grounding in videos (TSGV), a.k.a., natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. Lastly, we discuss issues with the current TSGV research and share our insights about promising research directions.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri"
    },
    {
      "index": 1,
      "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "C. Feichtenhofer, A. Pinz, and A. Zisserman",
      "orig_title": "Convolutional two-stream network fusion for video action recognition",
      "paper_id": "1604.06573v2"
    },
    {
      "index": 2,
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "J. Carreira and A. Zisserman",
      "orig_title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "paper_id": "1705.07750v3"
    },
    {
      "index": 3,
      "title": "G-tad: Sub-graph localization for temporal action detection",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem"
    },
    {
      "index": 4,
      "title": "Gated self-matching networks for reading comprehension and question answering",
      "abstract": "",
      "year": "2017",
      "venue": "ACL",
      "authors": "W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou"
    },
    {
      "index": 5,
      "title": "Bi-Directional Attention Flow for Machine Comprehension",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi",
      "orig_title": "Bidirectional attention flow for machine comprehension",
      "paper_id": "1611.01603v6"
    },
    {
      "index": 6,
      "title": "Fast and accurate reading comprehension by combining self-attention and convolution",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "A. W. Yu, D. Dohan, Q. Le, T. Luong, R. Zhao, and K. Chen"
    },
    {
      "index": 7,
      "title": "FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "H. Huang, C. Zhu, Y. Shen, and W. Chen",
      "orig_title": "Fusionnet: Fusing via fully-aware attention with application to machine comprehension",
      "paper_id": "1711.07341v2"
    },
    {
      "index": 8,
      "title": "TALL: Temporal Activity Localization via Language Query",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "J. Gao, C. Sun, Z. Yang, and R. Nevatia",
      "orig_title": "Tall: Temporal activity localization via language query",
      "paper_id": "1705.02101v2"
    },
    {
      "index": 9,
      "title": "Localizing Moments in Video with Natural Language",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "L. A. Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell",
      "orig_title": "Localizing moments in video with natural language",
      "paper_id": "1708.01641v1"
    },
    {
      "index": 10,
      "title": "Cross-modal moment localization in videos",
      "abstract": "",
      "year": "2018",
      "venue": "ACM MM",
      "authors": "M. Liu, X. Wang, L. Nie, Q. Tian, B. Chen, and T.-S. Chua"
    },
    {
      "index": 11,
      "title": "Attentive moment retrieval in videos",
      "abstract": "",
      "year": "2018",
      "venue": "SIGIR",
      "authors": "M. Liu, X. Wang, L. Nie, X. He, B. Chen, and T.-S. Chua"
    },
    {
      "index": 12,
      "title": "MAC: Mining Activity Concepts for Language-based Temporal Localization",
      "abstract": "",
      "year": "2019",
      "venue": "WACV",
      "authors": "R. Ge, J. Gao, K. Chen, and R. Nevatia",
      "orig_title": "Mac: Mining activity concepts for language-based temporal localization",
      "paper_id": "1811.08925v1"
    },
    {
      "index": 13,
      "title": "Text-to-clip video retrieval with early fusion and re-captioning",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "H. Xu, K. He, L. Sigal, S. Sclaroff, and K. Saenko"
    },
    {
      "index": 14,
      "title": "Temporally grounding natural sentence in video",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "J. Chen, X. Chen, L. Ma, Z. Jie, and T.-S. Chua"
    },
    {
      "index": 15,
      "title": "MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "D. Zhang, X. Dai, X. Wang, Y.-F. Wang, and L. S. Davis",
      "orig_title": "Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment",
      "paper_id": "1812.00087v2"
    },
    {
      "index": 16,
      "title": "Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Y. Yuan, L. Ma, J. Wang, W. Liu, and W. Zhu",
      "orig_title": "Semantic conditioned dynamic modulation for temporal sentence grounding in videos",
      "paper_id": "1910.14303v1"
    },
    {
      "index": 17,
      "title": "Learning 2d temporal adjacent networks formoment localization with natural language",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "S. Zhang, H. Peng, J. Fu, and J. Luo"
    },
    {
      "index": 18,
      "title": "To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "Y. Yuan, T. Mei, and W. Zhu",
      "orig_title": "To find where you talk: Temporal sentence localization in video with attention based location regression",
      "paper_id": "1804.07014v4"
    },
    {
      "index": 19,
      "title": "ExCL: Extractive Clip Localization Using Natural Language Descriptions",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "S. Ghosh, A. Agarwal, Z. Parekh, and A. Hauptmann",
      "orig_title": "ExCL: Extractive Clip Localization Using Natural Language Descriptions",
      "paper_id": "1904.02755v1"
    },
    {
      "index": 20,
      "title": "Localizing natural language in videos",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "J. Chen, L. Ma, X. Chen, Z. Jie, and J. Luo"
    },
    {
      "index": 21,
      "title": "DEBUG: A dense bottom-up grounding approach for natural language video localization",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "C. Lu, L. Chen, C. Tan, X. Li, and J. Xiao"
    },
    {
      "index": 22,
      "title": "Span-based Localizing Network for Natural Language Video Localization",
      "abstract": "",
      "year": "2020",
      "venue": "ACL",
      "authors": "H. Zhang, A. Sun, W. Jing, and J. T. Zhou",
      "orig_title": "Span-based localizing network for natural language video localization",
      "paper_id": "2004.13931v2"
    },
    {
      "index": 23,
      "title": "Read, watch, and move: Reinforcement learning for temporally grounding natural language descriptions in videos",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI",
      "authors": "D. He, X. Zhao, J. Huang, F. Li, X. Liu, and S. Wen"
    },
    {
      "index": 24,
      "title": "Language-driven temporal activity localization: A semantic matching reinforcement learning model",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "W. Wang, Y. Huang, and L. Wang"
    },
    {
      "index": 25,
      "title": "Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "J. Wu, G. Li, S. Liu, and L. Lin",
      "orig_title": "Tree-structured policy based progressive reinforcement learning for temporally language grounding in video",
      "paper_id": "2001.06680v1"
    },
    {
      "index": 26,
      "title": "Weakly Supervised Video Moment Retrieval From Text Queries",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "N. C. Mithun, S. Paul, and A. K. Roy-Chowdhury",
      "orig_title": "Weakly supervised video moment retrieval from text queries",
      "paper_id": "1904.03282v2"
    },
    {
      "index": 27,
      "title": "WSLLN: Weakly Supervised Natural Language Localization Networks",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "M. Gao, L. Davis, R. Socher, and C. Xiong",
      "orig_title": "WSLLN:weakly supervised natural language localization networks",
      "paper_id": "1909.00239v1"
    },
    {
      "index": 28,
      "title": "Weakly Supervised Dense Event Captioning in Videos",
      "abstract": "",
      "year": "2018",
      "venue": "NeurIPS",
      "authors": "X. Duan, W. Huang, C. Gan, J. Wang, W. Zhu, and J. Huang",
      "orig_title": "Weakly supervised dense event captioning in videos",
      "paper_id": "1812.03849v1"
    },
    {
      "index": 29,
      "title": "Weakly-Supervised Video Moment Retrieval via Semantic Completion Network",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Z. Lin, Z. Zhao, Z. Zhang, Q. Wang, and H. Liu",
      "orig_title": "Weakly-supervised video moment retrieval via semantic completion network",
      "paper_id": "1911.08199v3"
    },
    {
      "index": 30,
      "title": "Towards bridging event captioner and sentence localizer for weakly supervised dense event captioning",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "S. Chen and Y.-G. Jiang"
    },
    {
      "index": 31,
      "title": "A survey of temporal activity localization via language in untrimmed videos",
      "abstract": "",
      "year": "2020",
      "venue": "ICCST",
      "authors": "Y. Yang, Z. Li, and G. Zeng"
    },
    {
      "index": 32,
      "title": "A Survey on Natural Language Video Localization",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "X. Liu, X. Nie, Z. Tan, J. Guo, and Y. Yin",
      "orig_title": "A survey on natural language video localization",
      "paper_id": "2104.00234v1"
    },
    {
      "index": 33,
      "title": "A Survey on Temporal Sentence Grounding in Videos",
      "abstract": "",
      "year": "2021",
      "venue": "ArXiv",
      "authors": "X. Lan, Y. Yuan, X. Wang, Z. Wang, and W. Zhu",
      "orig_title": "A survey on temporal sentence grounding in videos",
      "paper_id": "2109.08039v2"
    },
    {
      "index": 34,
      "title": "A Survey on Video Moment Localization",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Comput. Surv.",
      "authors": "M. Liu, L. Nie, Y. Wang, M. Wang, and Y. Rui",
      "orig_title": "A survey on video moment localization",
      "paper_id": "2306.07515v1"
    },
    {
      "index": 35,
      "title": "Efficient estimation of word representations in vector space",
      "abstract": "",
      "year": "2013",
      "venue": "ArXiv",
      "authors": "T. Mikolov, K. Chen, G. Corrado, and J. Dean"
    },
    {
      "index": 36,
      "title": "GloVe: Global vectors for word representation",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "J. Pennington, R. Socher, and C. Manning"
    },
    {
      "index": 37,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 38,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 39,
      "title": "Skip-Thought Vectors",
      "abstract": "",
      "year": "2015",
      "venue": "NeurIPS",
      "authors": "R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler",
      "orig_title": "Skip-thought vectors",
      "paper_id": "1506.06726v1"
    },
    {
      "index": 40,
      "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
      "abstract": "",
      "year": "2017",
      "venue": "EMNLP",
      "authors": "A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes",
      "orig_title": "Supervised learning of universal sentence representations from natural language inference data",
      "paper_id": "1705.02364v5"
    },
    {
      "index": 41,
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "N. Reimers and I. Gurevych",
      "orig_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
      "paper_id": "1908.10084v1"
    },
    {
      "index": 42,
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "abstract": "",
      "year": "2014",
      "venue": "ArXiv",
      "authors": "K. Simonyan and A. Zisserman",
      "orig_title": "Very deep convolutional networks for large-scale image recognition",
      "paper_id": "1409.1556v6"
    },
    {
      "index": 43,
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "K. He, X. Zhang, S. Ren, and J. Sun",
      "orig_title": "Deep residual learning for image recognition",
      "paper_id": "1512.03385v1"
    },
    {
      "index": 44,
      "title": "Localizing Moments in Video with Temporal Language",
      "abstract": "",
      "year": "2018",
      "venue": "EMNLP",
      "authors": "L. A. Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell",
      "orig_title": "Localizing moments in video with temporal language",
      "paper_id": "1809.01337v1"
    },
    {
      "index": 45,
      "title": "Multi-modal circulant fusion for video-to-language and backward",
      "abstract": "",
      "year": "2018",
      "venue": "IJCAI",
      "authors": "A. Wu and Y. Han"
    },
    {
      "index": 46,
      "title": "Exploiting Temporal Relationships in Video Moment Localization with Natural Language",
      "abstract": "",
      "year": "2019",
      "venue": "ACM MM",
      "authors": "S. Zhang, J. Su, and J. Luo",
      "orig_title": "Exploiting temporal relationships in video moment localization with natural language",
      "paper_id": "1908.03846v1"
    },
    {
      "index": 47,
      "title": "Cross-modal video moment retrieval with spatial and language-temporal attention",
      "abstract": "",
      "year": "2019",
      "venue": "ACM ICMR",
      "authors": "B. Jiang, X. Huang, C. Yang, and J. Yuan"
    },
    {
      "index": 48,
      "title": "An attentive sequence to sequence translator for localizing video clips by natural language",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE TMM",
      "authors": "K. Ning, M. Cai, D. Xie, and F. Wu"
    },
    {
      "index": 49,
      "title": "Multi-modal relational graph for cross-modal video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Y. Zeng, D. Cao, X. Wei, M. Liu, Z. Zhao, and Z. Qin"
    },
    {
      "index": 50,
      "title": "“Interaction-integrated network for natural language moment localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 51,
      "title": "R-C3D: Region Convolutional 3D Network for Temporal Activity Detection",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“R-c3d: Region convolutional 3d network for temporal activity detection",
      "paper_id": "1703.07814v2"
    },
    {
      "index": 52,
      "title": "Multilevel Language and Vision Integration for Text-to-Clip Retrieval",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Multilevel language and vision integration for text-to-clip retrieval",
      "paper_id": "1804.05113v3"
    },
    {
      "index": 53,
      "title": "“Semantic proposal for activity localization in videos via sentence query",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 54,
      "title": "Boundary Proposal Network for Two-Stage Natural Language Video Localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Boundary proposal network for two-stage natural language video localization",
      "paper_id": "2103.08109v2"
    },
    {
      "index": 55,
      "title": "Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Adaptive proposal generation network for temporal sentence localization in videos",
      "paper_id": "2109.06398v1"
    },
    {
      "index": 56,
      "title": "Natural Language Video Localization with Learnable Moment Proposals",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Natural language video localization with learnable moment proposals",
      "paper_id": "2109.10678v1"
    },
    {
      "index": 57,
      "title": "“Video moment localization via deep cross-modal hashing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Cross-modal interaction networks for query-based moment retrieval in videos",
      "paper_id": "1906.02497v2"
    },
    {
      "index": 59,
      "title": "Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Semantic conditioned dynamic modulation for temporal sentence grounding in videos",
      "paper_id": "1910.14303v1"
    },
    {
      "index": 60,
      "title": "“Moment retrieval via cross-modal interaction networks with query reconstruction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Temporally grounding language queries in videos by contextual boundary-aware prediction",
      "paper_id": "1909.05010v2"
    },
    {
      "index": 62,
      "title": "Fine-grained Iterative Attention Network for Temporal Language Localization in Videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Fine-grained iterative attention network for temporal language localization in videos",
      "paper_id": "2008.02448v1"
    },
    {
      "index": 63,
      "title": "“Jointly cross- and self-modal graph attention network for query-based moment localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "“Reasoning step-by-step: Temporal sentence localization in videos via deep rectification-modulation network",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "Hierarchical Deep Residual Reasoning for Temporal Moment Localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Hierarchical deep residual reasoning for temporal moment localization",
      "paper_id": "2111.00417v1"
    },
    {
      "index": 66,
      "title": "Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-modal interaction graph convolutional network for temporal language localization in videos",
      "paper_id": "2110.06058v1"
    },
    {
      "index": 67,
      "title": "Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Progressively guide to attend: An iterative alignment framework for temporal sentence grounding",
      "paper_id": "2109.06400v1"
    },
    {
      "index": 68,
      "title": "“Dct-net: A deep co-interactive transformer network for video temporal grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Temporal modular networks for retrieving complex compositional activities in videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Multi-Scale 2D Temporal Adjacency Networks for Moment Localization with Natural Language",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-scale 2d temporal adjacency networks for moment localization with natural language",
      "paper_id": "2012.02646v2"
    },
    {
      "index": 71,
      "title": "Progressive Localization Networks for Language based Moment Localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Progressive localization networks for language-based moment localization",
      "paper_id": "2102.01282v2"
    },
    {
      "index": 72,
      "title": "“Structured multi-level interaction network for video moment localization via language query",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "“Coarse-to-fine semantic alignment for cross-modal moment localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Vlg-net: Video-language graph matching network for video grounding",
      "paper_id": "2011.10132v2"
    },
    {
      "index": 75,
      "title": "Relation-aware Video Reading Comprehension for Temporal Language Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Relation-aware video reading comprehension for temporal language grounding",
      "paper_id": "2110.05717v3"
    },
    {
      "index": 76,
      "title": "“Multi-stage aggregated transformer network for temporal language localization in videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Aligned dual channel graph convolutional network for visual question answering",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "“Fast video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "“Diving into the relations: Leveraging semantic and visual structures for video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Negative sample matters: A renaissance of metric learning for temporal grounding",
      "paper_id": "2109.04872v2"
    },
    {
      "index": 81,
      "title": "“Stcm-net: A symmetrical one-stage network for temporal language localization in videos",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "“Find and focus: Retrieve and localize video events with natural language queries",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "Context-aware Biaffine Localizing Network for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Context-aware biaffine localizing network for temporal sentence grounding",
      "paper_id": "2103.11555v1"
    },
    {
      "index": 84,
      "title": "“Dual path interaction network for video moment localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "“Dense events grounding in video",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "Support-Set Based Cross-Supervision for Video Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Support-set based cross-supervision for video grounding",
      "paper_id": "2108.10576v1"
    },
    {
      "index": 87,
      "title": "A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A simple yet effective method for video temporal grounding with cross-modality attention",
      "paper_id": "2009.11232v1"
    },
    {
      "index": 88,
      "title": "“Dense regression network for video grounding",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "Local-Global Video-Text Interactions for Temporal Grounding",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Local-global video-text interactions for temporal grounding",
      "paper_id": "2004.07514v1"
    },
    {
      "index": 90,
      "title": "“Proposal-free video grounding with contextual pyramid network",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Embracing uncertainty: Decoupling and de-bias for robust temporal grounding",
      "paper_id": "2103.16848v2"
    },
    {
      "index": 92,
      "title": "End-to-end Multi-modal Video Temporal Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end multi-modal video temporal grounding",
      "paper_id": "2107.05624v2"
    },
    {
      "index": 93,
      "title": "On Pursuit of Designing Multi-modal Transformer for Video Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“On pursuit of designing multi-modal transformer for video grounding",
      "paper_id": "2109.06085v2"
    },
    {
      "index": 94,
      "title": "“Rethinking the bottom-up framework for query-based video localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "“Single-shot semantic matching network for moment localization in videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "Boundary-sensitive Pre-training for Temporal Localization in Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Boundary-sensitive pre-training for temporal localization in videos",
      "paper_id": "2011.10830v3"
    },
    {
      "index": 97,
      "title": "“Hierarchical visual-textual graph for temporal activity localization via language",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "“Learning modality interaction for temporal sentence localization and event captioning in videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "Natural Language Video Localization: A Revisit in Span-based Question Answering Framework",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Natural language video localization: A revisit in span-based question answering framework",
      "paper_id": "2102.13558v3"
    },
    {
      "index": 100,
      "title": "Parallel Attention Network with Sequence Matching for Video Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Parallel attention network with sequence matching for video grounding",
      "paper_id": "2105.08481v1"
    },
    {
      "index": 101,
      "title": "Interventional Video Grounding with Dual Contrastive Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Interventional video grounding with dual contrastive learning",
      "paper_id": "2106.11013v2"
    },
    {
      "index": 102,
      "title": "“Cross interaction network for natural language guided video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "“Multi-level query interaction for temporal language grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 104,
      "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Frame-wise cross-modal matching for video moment retrieval",
      "paper_id": "2009.10434v2"
    },
    {
      "index": 105,
      "title": "“Temporal textual localization in video via adversarial bi-directional interaction networks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "“Collaborative spatial-temporal interaction for language-based moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "“Natural language video moment localization through query-controlled temporal convolution",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "SNEAK: Synonymous Sentences-Aware Adversarial Attack on Natural Language Video Localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Sneak: Synonymous sentences-aware adversarial attack on natural language video localization",
      "paper_id": "2112.04154v1"
    },
    {
      "index": 109,
      "title": "E. Marrese-Taylor",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "“Cascaded prediction network via segment tree for temporal video grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "“Local-enhanced interaction for temporal moment localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "E. Marrese-Taylor",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "“Yfcc100m: The new data in multimedia research",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Hollywood in homes: Crowdsourcing data collection for activity understanding",
      "paper_id": "1604.01753v3"
    },
    {
      "index": 115,
      "title": "“The Stanford CoreNLP natural language processing toolkit",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Dense-Captioning Events in Videos",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Dense-captioning events in videos",
      "paper_id": "1705.00754v1"
    },
    {
      "index": 117,
      "title": "“Activitynet: A large-scale video benchmark for human activity understanding",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "“Grounding action descriptions in videos",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "“Script data for attribute-based recognition of composite activities",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Mad: A scalable dataset for language grounding in videos from movie audio descriptions",
      "paper_id": "2112.00431v2"
    },
    {
      "index": 121,
      "title": "“Rich feature hierarchies for accurate object detection and semantic segmentation",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Faster r-cnn: Towards real-time object detection with region proposal networks",
      "paper_id": "1506.01497v3"
    },
    {
      "index": 123,
      "title": "Focal Loss for Dense Object Detection",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Focal loss for dense object detection",
      "paper_id": "1708.02002v2"
    },
    {
      "index": 124,
      "title": "“Natural language object retrieval",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“A closer look at temporal sentence grounding in videos: Dataset and metric",
      "paper_id": "2101.09028v3"
    },
    {
      "index": 126,
      "title": "“Camg: Context-aware moment graph network for multimodal temporal activity localization via language",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "“Efficient video grounding with which-where reading comprehension",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "then perusing: A human-like framework for natural language video localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "“Eccl: Explicit correlation-based convolution boundary locator for moment localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring optical-flow-guided motion and detection-based appearance for temporal sentence grounding",
      "paper_id": "2203.02966v1"
    },
    {
      "index": 131,
      "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Simple bert models for relation extraction and semantic role labeling",
      "paper_id": "1904.05255v1"
    },
    {
      "index": 132,
      "title": "Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning commonsense-aware moment-text alignment for fast video temporal grounding",
      "paper_id": "2204.01450v2"
    },
    {
      "index": 133,
      "title": "“A hybird alignment loss for temporal moment localization with natural language",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "“Fast and robust earth mover’s distances",
      "abstract": "",
      "year": "2009",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "“Dual-channel localization networks for moment retrieval with natural language",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“Learning to combine the modalities of language and video for temporal moment localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "Contrastive Language-Action Pre-training for Temporal Localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Contrastive language-action pre-training for temporal localization",
      "paper_id": "2204.12293v1"
    },
    {
      "index": 138,
      "title": "Learning Sample Importance for Cross-Scenario Video Temporal Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning sample importance for cross-scenario video temporal grounding",
      "paper_id": "2201.02848v1"
    },
    {
      "index": 139,
      "title": "Team PKU-WICT-MIPL PIC Makeup Temporal Video Grounding Challenge 2022 Technical Report",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Team pku-wict-mipl pic makeup temporal video grounding challenge 2022 technical report",
      "paper_id": "2207.02687v1"
    },
    {
      "index": 140,
      "title": "“Exploring language hierarchy for video grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "“Cross-modal dynamic networks for video moment retrieval with text query",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "“Language-enhanced object reasoning networks for video moment retrieval with text query",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“You need to read again: Multi-granularity perception network for moment retrieval in videos",
      "paper_id": "2205.12886v2"
    },
    {
      "index": 144,
      "title": "Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Compositional temporal grounding with structured variational cross-graph correspondence learning",
      "paper_id": "2203.13049v2"
    },
    {
      "index": 145,
      "title": "Hierarchical Local-Global Transformer for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Hierarchical local-global transformer for temporal sentence grounding",
      "paper_id": "2208.14882v1"
    },
    {
      "index": 146,
      "title": "“Hisa: Hierarchically semantic associating for video temporal grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Memory-guided semantic learning network for temporal sentence grounding",
      "paper_id": "2201.00454v1"
    },
    {
      "index": 148,
      "title": "“Phrase-level prediction for video temporal localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "“Taohighlight: Commodity-aware multi-modal video highlight detection in e-commerce",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 151,
      "title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploring motion and appearance information for temporal sentence grounding",
      "paper_id": "2201.00457v1"
    },
    {
      "index": 152,
      "title": "“Soundnet: Learning sound representations from unlabeled video",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "Simple and Effective Multi-Paragraph Reading Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Simple and effective multi-paragraph reading comprehension",
      "paper_id": "1710.10723v2"
    },
    {
      "index": 154,
      "title": "“Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "“Coarse-to-fine spatial-temporal relationship inference for temporal sentence grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "Entity-aware and Motion-aware Transformers for Language-driven Action Localization in Videos",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Entity-aware and motion-aware transformers for language-driven action localization in videos",
      "paper_id": "2205.05854v1"
    },
    {
      "index": 157,
      "title": "“Joint modality synergy and spatio-temporal cue purification for moment localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 158,
      "title": "E. Marrese-Taylor",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Multiple cross-attention for video-subtitle moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "“Natural language video moment localization through query-controlled temporal convolution",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "“Point prompt tuning for temporally language grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "“Query-aware video encoder for video moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "Reducing the Vision and Language Bias for Temporal Sentence Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Reducing the vision and language bias for temporal sentence grounding",
      "paper_id": "2207.13457v1"
    },
    {
      "index": 164,
      "title": "“Stdnet: Spatio-temporal decomposed network for video grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards visual-prompt temporal answering grounding in medical instructional video",
      "paper_id": "2203.06667v6"
    },
    {
      "index": 166,
      "title": "Video Activity Localisation with Uncertainties in Temporal Boundary",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Video activity localisation with uncertainties in temporal boundary",
      "paper_id": "2206.12923v2"
    },
    {
      "index": 167,
      "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "paper_id": "1603.01354v5"
    },
    {
      "index": 168,
      "title": "“Dual adversarial neural transfer for low-resource named entity recognition",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "Named Entity Recognition as Dependency Parsing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Named entity recognition as dependency parsing",
      "paper_id": "2005.07150v3"
    },
    {
      "index": 170,
      "title": "Causal inference in statistics: A primer. John Wiley & Sons",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 171,
      "title": "Reinforcement learning: An introduction. MIT press",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "“Monte carlo sampling methods",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 173,
      "title": "A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“A2-rl: Aesthetics aware reinforcement learning for image cropping",
      "paper_id": "1709.04595v3"
    },
    {
      "index": 174,
      "title": "“Adversarial video moment retrieval by jointly modeling ranking and localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 175,
      "title": "“Moment is important: Language-based video moment retrieval via adversarial learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "“Strong: Spatio-temporal reinforcement learning for cross-modal video moment localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 177,
      "title": "“Tripping through time: Efficient localization of activities in videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 178,
      "title": "“Maban: Multi-agent boundary-aware network for natural language moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "“Temporal action detection with structured segment networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 180,
      "title": "“Joint video summarization and moment localization by cross-task sample transfer",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 181,
      "title": "“Support-set bottlenecks for video-text representation learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 182,
      "title": "“End-to-end dense video grounding via parallel regression",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "“Gtlr: Graph-based transformer with language reconstruction for video paragraph grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 184,
      "title": "“Semi-supervised video paragraph grounding with contrastive encoder",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "“Video moment retrieval with cross-modal neural architecture search",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "LocVTP: Video-Text Pre-training for Temporal Localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Locvtp: Video-text pre-training for temporal localization",
      "paper_id": "2207.10362v1"
    },
    {
      "index": 187,
      "title": "“Temporal moment localization via natural language by utilizing video question answers as a special variant and bypassing nlp for corpora",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "“A multi-level alignment training scheme for video-and-language grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 189,
      "title": "“End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 190,
      "title": "Explore-And-Match: Bridging Proposal-Based and Proposal-Free With Transformer for Sentence Grounding in Videos",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Explore-and-match: Bridging proposal-based and proposal-free with transformer for sentence grounding in videos",
      "paper_id": "2201.10168v4"
    },
    {
      "index": 191,
      "title": "UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection",
      "paper_id": "2203.12745v2"
    },
    {
      "index": 192,
      "title": "Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Look closer to ground better: Weakly-supervised temporal grounding of sentence in video",
      "paper_id": "2001.09308v1"
    },
    {
      "index": 193,
      "title": "VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Vlanet: Video-language alignment network for weakly-supervised video moment retrieval",
      "paper_id": "2008.10238v1"
    },
    {
      "index": 194,
      "title": "Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Reinforcement learning for weakly supervised temporal grounding of natural language in untrimmed videos",
      "paper_id": "2009.08614v1"
    },
    {
      "index": 195,
      "title": "“Counterfactual contrastive learning for weakly-supervised vision-language grounding",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "“Asynce: Disentangling false-positives for weakly-supervised video grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 197,
      "title": "“Visual co-occurrence alignment learning for weakly-supervised video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "“Fine-grained semantic alignment network for weakly supervised temporal language grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Cross-sentence temporal and semantic relations in video activity localisation",
      "paper_id": "2107.11443v2"
    },
    {
      "index": 200,
      "title": "“Local correspondence network for weakly supervised temporal sentence grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "“Regularized two granularity loss function for weakly supervised video moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 202,
      "title": "Weakly Supervised Temporal Adjacent Network for Language Grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Weakly supervised temporal adjacent network for language grounding",
      "paper_id": "2106.16136v1"
    },
    {
      "index": 203,
      "title": "LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Logan: Latent graph co-attention network for weakly-supervised video moment retrieval",
      "paper_id": "1909.13784v2"
    },
    {
      "index": 204,
      "title": "“Explore inter-contrast between videos via composition for weakly supervised temporal sentence grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 205,
      "title": "Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-scale self-contrastive learning with hard negative mining for weakly-supervised query-based video grounding",
      "paper_id": "2203.03838v1"
    },
    {
      "index": 206,
      "title": "“Siamese alignment network for weakly supervised video moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 207,
      "title": "“Weakly supervised moment localization with natural language based on semantic reconstruction",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos",
      "paper_id": "2003.07048v1"
    },
    {
      "index": 209,
      "title": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Bmn: Boundary-matching network for temporal action proposal generation",
      "paper_id": "1907.09702v1"
    },
    {
      "index": 210,
      "title": "“Towards bridging video and language by caption generation and sentence localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "“Weakly supervised temporal sentence grounding with gaussian-based contrastive proposal learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "“Weakly supervised video moment localization with contrastive negative sample mining",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Regularized two-branch proposal networks for weakly-supervised moment retrieval in videos",
      "paper_id": "2008.08257v1"
    },
    {
      "index": 214,
      "title": "“Self-supervised learning for semi-supervised temporal language grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "Zero-shot Natural Language Video Localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Zero-shot natural language video localization",
      "paper_id": "2110.00428v1"
    },
    {
      "index": 216,
      "title": "“Learning video moment retrieval without a single annotated video",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "“Unsupervised temporal video grounding with deep semantic clustering",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "“Text-based temporal localization of novel events",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "“Point-supervised video temporal grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 220,
      "title": "“Video moment retrieval from text queries via single frame annotation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 221,
      "title": "“Multi-scale 2d representation learning for weakly-supervised moment retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 222,
      "title": "inception-resnet and the impact of residual connections on learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 223,
      "title": "“Uncovering hidden challenges in query-based video moment retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "“Thinking inside uncertainty: Interest moment perception for diverse temporal grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 225,
      "title": "“Deconfounded video moment retrieval with causal intervention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 226,
      "title": "Towards Debiasing Temporal Sentence Grounding in Video",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Towards debiasing temporal sentence grounding in video",
      "paper_id": "2111.04321v1"
    },
    {
      "index": 227,
      "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Qvhighlights: Detecting moments and highlights in videos via natural language queries",
      "paper_id": "2107.09609v2"
    },
    {
      "index": 228,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 229,
      "title": "“Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 230,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 231,
      "title": "ActBERT: Learning Global-Local Video-Text Representations",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Actbert: Learning global-local video-text representations",
      "paper_id": "2011.07231v1"
    },
    {
      "index": 232,
      "title": "“Bevt: Bert pretraining of video transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 233,
      "title": "“Less is more: Clipbert for video-and-language learningvia sparse sampling",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 234,
      "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Videoclip: Contrastive pre-training for zero-shot video-text understanding",
      "paper_id": "2109.14084v2"
    },
    {
      "index": 235,
      "title": "C. Feichtenhofer",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 236,
      "title": "MERLOT: Multimodal Neural Script Knowledge Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Merlot: Multimodal neural script knowledge models",
      "paper_id": "2106.02636v3"
    },
    {
      "index": 237,
      "title": "“Finding ”it”: Weakly-supervised reference-aware visual grounding in instructional videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 238,
      "title": "“Not all frames are equal: Weakly-supervised video grounding with contextual similarity and visual clustering losses",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 239,
      "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Weakly-supervised spatio-temporally grounding natural sentence in video",
      "paper_id": "1906.02549v1"
    },
    {
      "index": 240,
      "title": "“Activity-driven weakly-supervised spatio-temporal grounding from untrimmed videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 241,
      "title": "“Video object grounding using semantic roles in language description",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 242,
      "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Where does it exist: Spatio-temporal video grounding for multi-form sentences",
      "paper_id": "2001.06891v3"
    },
    {
      "index": 243,
      "title": "Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Object-aware multi-branch relation networks for spatio-temporal video grounding",
      "paper_id": "2008.06941v2"
    },
    {
      "index": 244,
      "title": "“Hierarchical attention based spatial-temporal graph-to-sequence learning for grounded video description",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 245,
      "title": "“Decoupled spatial temporal graphs for generic visual grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 246,
      "title": "“Human-centric spatio-temporal video grounding with visual transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 247,
      "title": "Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Look at what i’m doing: Self-supervised spatial grounding of narrations in instructional videos",
      "paper_id": "2110.10596v2"
    },
    {
      "index": 248,
      "title": "“Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 249,
      "title": "Correspondence Matters for Video Referring Expression Comprehension",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Correspondence matters for video referring expression comprehension",
      "paper_id": "2207.10400v2"
    },
    {
      "index": 250,
      "title": "“Cross-modal target retrieval for tracking by natural language",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 251,
      "title": "End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end modeling via information tree for one-shot natural language spatial video grounding",
      "paper_id": "2203.08013v2"
    },
    {
      "index": 252,
      "title": "Gaussian Kernel-based Cross Modal Network for Spatio-Temporal Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Gaussian kernel-based cross modal network for spatio-temporal video grounding",
      "paper_id": "2207.00744v1"
    },
    {
      "index": 253,
      "title": "STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Stvgformer: Spatio-temporal video grounding with static-dynamic cross-modal understanding",
      "paper_id": "2207.02756v1"
    },
    {
      "index": 254,
      "title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Tubedetr: Spatio-temporal video grounding with transformers",
      "paper_id": "2203.16434v2"
    },
    {
      "index": 255,
      "title": "Audio-Visual Event Localization in Unconstrained Videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual event localization in unconstrained videos",
      "paper_id": "1803.08842v1"
    },
    {
      "index": 256,
      "title": "“Dual attention matching for audio-visual event localization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 257,
      "title": "“Cross-modal relation-aware networks for audio-visual event localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 258,
      "title": "“Cross-modal attention network for temporal inconsistent audio-visual event localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 259,
      "title": "Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual event localization via recursive fusion by joint co-attention",
      "paper_id": "2008.06581v1"
    },
    {
      "index": 260,
      "title": "“Discriminative cross-modality attention network for temporal inconsistent audio-visual event localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 261,
      "title": "“Audio-visual event localization by learning spatial and semantic co-attention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 262,
      "title": "Video-Guided Curriculum Learning for Spoken Video Grounding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Video-guided curriculum learning for spoken video grounding",
      "paper_id": "2209.00277v1"
    },
    {
      "index": 263,
      "title": "“Asymmetric spatio-temporal embeddings for large-scale image-to-video retrieval",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 264,
      "title": "“Localizing unseen activities in video via image query",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 265,
      "title": "A Proposal-based Approach for Activity Image-to-Video Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A proposal-based approach for activity image-to-video retrieval",
      "paper_id": "1911.10531v1"
    },
    {
      "index": 266,
      "title": "“Activity image-to-video retrieval by disentangling appearance and motion",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 267,
      "title": "“Video re-localization",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 268,
      "title": "Spatio-temporal Video Re-localization by Warp LSTM",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Spatio-temporal video re-localization by warp lstm",
      "paper_id": "1905.03922v1"
    },
    {
      "index": 269,
      "title": "“Weakly-supervised video re-localization with multiscale attention model",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 270,
      "title": "Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning segment similarity and alignment in large-scale content based video retrieval",
      "paper_id": "2309.11091v1"
    },
    {
      "index": 271,
      "title": "“Audio set: An ontology and human-labeled dataset for audio events",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 272,
      "title": "Deep Audio-Visual Speech Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep audio-visual speech recognition",
      "paper_id": "1809.02108v2"
    },
    {
      "index": 273,
      "title": "“Racial disparities in automated speech recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 274,
      "title": "“Deep graph random process for relational-thinking-based speech recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 275,
      "title": "“Temporal localization of moments in video collections with natural language",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 276,
      "title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Tvr: A large-scale dataset for video-subtitle moment retrieval",
      "paper_id": "2001.09099v2"
    },
    {
      "index": 277,
      "title": "mTVR: Multilingual Moment Retrieval in Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“mTVR: Multilingual moment retrieval in videos",
      "paper_id": "2108.00061v1"
    },
    {
      "index": 278,
      "title": "Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“HERO: Hierarchical encoder for Video+Language omni-representation pre-training",
      "paper_id": "2005.00200v2"
    },
    {
      "index": 279,
      "title": "A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A hierarchical multi-modal encoder for moment localization in video corpus",
      "paper_id": "2011.09046v2"
    },
    {
      "index": 280,
      "title": "“Video corpus moment retrieval with contrastive learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 281,
      "title": "Video Moment Retrieval with Text Query Considering Many-to-Many Correspondence Using Potentially Relevant Pair",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Video moment retrieval with text query considering many-to-many correspondence using potentially relevant pair",
      "paper_id": "2106.13566v1"
    },
    {
      "index": 282,
      "title": "Text-based Localization of Moments in a Video Corpus",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Text-based localization of moments in a video corpus",
      "paper_id": "2008.08716v2"
    },
    {
      "index": 283,
      "title": "CONQUER: Contextual Query-aware Ranking for Video Corpus Moment Retrieval",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Conquer: Contextual query-aware ranking for video corpus moment retrieval",
      "paper_id": "2109.10016v1"
    },
    {
      "index": 284,
      "title": "“Coarse to fine: Video retrieval before moment localization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 285,
      "title": "“Cascaded mpn: Cascaded moment proposal network for video corpus moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 286,
      "title": "“Cross-lingual cross-modal consolidation for effective multilingual video corpus moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 287,
      "title": "“Semantic association network for video corpus moment retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 288,
      "title": "“Vsrnet: End-to-end video segment retrieval with text query",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 289,
      "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "",
      "orig_title": "“Caffe: Convolutional architecture for fast feature embedding",
      "paper_id": "1408.5093v1"
    },
    {
      "index": 290,
      "title": "“Visual grounding via accumulated attention",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 291,
      "title": "Dynamic Graph Attention for Referring Expression Comprehension",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Dynamic graph attention for referring expression comprehension",
      "paper_id": "1909.08164v1"
    },
    {
      "index": 292,
      "title": "TransVG: End-to-End Visual Grounding with Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Transvg: End-to-end visual grounding with transformers",
      "paper_id": "2104.08541v4"
    },
    {
      "index": 293,
      "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Jointly modeling embedding and translation to bridge video and language",
      "paper_id": "1505.01861v3"
    },
    {
      "index": 294,
      "title": "SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Sea: Sentence encoder assembly for video retrieval by textual queries",
      "paper_id": "2011.12091v1"
    },
    {
      "index": 295,
      "title": "Multi-Query Video Retrieval",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-query video retrieval",
      "paper_id": "2201.03639v2"
    },
    {
      "index": 296,
      "title": "“Align and tell: Boosting text-video retrieval with local alignment and fine-grained supervision",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 297,
      "title": "“TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 298,
      "title": "“TVQA: Localized",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 299,
      "title": "Motion-Appearance Co-Memory Networks for Video Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Motion-appearance co-memory networks for video question answering",
      "paper_id": "1803.10906v1"
    },
    {
      "index": 300,
      "title": "“Focal visual-text attention for memex question answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 301,
      "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Activitynet-qa: A dataset for understanding complex web videos via question answering",
      "paper_id": "1906.02467v1"
    },
    {
      "index": 302,
      "title": "“Multimodal transformer networks for end-to-end video-grounded dialogue systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 303,
      "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“TVQA+: Spatio-temporal grounding for video question answering",
      "paper_id": "1904.11574v2"
    },
    {
      "index": 304,
      "title": "Modality Shifting Attention Network for Multi-modal Video Question Answering",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Modality shifting attention network for multi-modal video question answering",
      "paper_id": "2007.02036v1"
    },
    {
      "index": 305,
      "title": "Self-supervised Pre-training and Contrastive Representation Learning for Multiple-choice Video QA",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised pre-training and contrastive representation learning for multiple-choice video qa",
      "paper_id": "2009.08043v2"
    },
    {
      "index": 306,
      "title": "Game-Based Video-Context Dialogue",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Game-based video-context dialogue",
      "paper_id": "1809.04560v2"
    },
    {
      "index": 307,
      "title": "“Dstc7-avsd: Scene-aware video-dialogue systems with dual attention",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 308,
      "title": "VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Vgnmn: Video-grounded neural module networks for video-grounded dialogue systems",
      "paper_id": "2104.07921v2"
    }
  ]
}