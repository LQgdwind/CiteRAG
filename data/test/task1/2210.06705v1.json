{
  "paper_id": "2210.06705v1",
  "title": "Proof.",
  "abstract": "Abstract\nStochastic Gradient Descent (SGD) has been the method of choice for learning large-scale non-convex models. While a general analysis of when SGD works has been elusive, there has been a lot of recent progress in understanding the convergence of Gradient Flow (GF) on the population loss, partly due to the simplicity that a continuous-time analysis buys us. An overarching theme of our paper is providing general conditions under which SGD converges, assuming that GF on the population loss converges. Our main tool to establish this connection is a general converse Lyapunov like theorem, which implies the existence of a Lyapunov potential under mild assumptions on the rates of convergence of GF. In fact, using these potentials, we show a one-to-one correspondence between rates of convergence of GF and geometrical properties of the underlying objective. When these potentials further satisfy certain self-bounding properties, we show that they can be used to provide a convergence guarantee for Gradient Descent (GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It turns out that these self-bounding assumptions are in a sense also necessary for GD/SGD to work. Using our framework, we provide a unified analysis for GD/SGD not only for classical settings like convex losses, or objectives that satisfy PŁ / KŁ  properties, but also for more complex problems including Phase Retrieval and Matrix sq-root, and extending the results in the recent work of Chatterjee (2022).",
  "reference_labels": [
    {
      "index": 0,
      "title": "Information-theoretic lower bounds on the oracle complexity of convex optimization",
      "abstract": "",
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar"
    },
    {
      "index": 1,
      "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift",
      "abstract": "",
      "year": "2021",
      "venue": "J. Mach. Learn. Res.",
      "authors": "Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan",
      "orig_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift",
      "paper_id": "1908.00261v5"
    },
    {
      "index": 2,
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song",
      "orig_title": "A convergence theory for deep learning via over-parameterization",
      "paper_id": "1811.03962v5"
    },
    {
      "index": 3,
      "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding",
      "abstract": "",
      "year": "2015",
      "venue": "The 28th Conference on Learning Theory, COLT 2015",
      "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra",
      "orig_title": "Simple, efficient, and neural algorithms for sparse coding",
      "paper_id": "1503.00778v1"
    },
    {
      "index": 4,
      "title": "Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-Łojasiewicz inequality",
      "abstract": "",
      "year": "2010",
      "venue": "Mathematics of Operations Research",
      "authors": "Hédy Attouch, Jérôme Bolte, Patrick Redont, and Antoine Soubeyran"
    },
    {
      "index": 5,
      "title": "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent",
      "abstract": "",
      "year": "2021",
      "venue": "38th International Conference on Machine Learning, ICML 2021",
      "authors": "Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E. Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry",
      "orig_title": "On the implicit bias of initialization shape: Beyond infinitesimal mirror descent",
      "paper_id": "2102.09769v1"
    },
    {
      "index": 6,
      "title": "Potential-function proofs for first-order methods",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.04581",
      "authors": "Nikhil Bansal and Anupam Gupta"
    },
    {
      "index": 7,
      "title": "Local and global linear convergence of general low-rank matrix recovery problems",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Yingjie Bi, Haixiang Zhang, and Javad Lavaei"
    },
    {
      "index": 8,
      "title": "LQR through the Lens of First Order Methods: Discrete-time Case",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.08921",
      "authors": "Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi",
      "orig_title": "Lqr through the lens of first order methods: Discrete-time case",
      "paper_id": "1907.08921v2"
    },
    {
      "index": 9,
      "title": "Convex Optimization: Algorithms and Complexity",
      "abstract": "",
      "year": "2015",
      "venue": "Foundations and Trends® in Machine Learning",
      "authors": "Sébastien Bubeck et al.",
      "orig_title": "Convex optimization: Algorithms and complexity",
      "paper_id": "1405.4980v2"
    },
    {
      "index": 10,
      "title": "Phase Retrieval via Wirtinger Flow: Theory and Algorithms",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi",
      "orig_title": "Phase retrieval via wirtinger flow: Theory and algorithms",
      "paper_id": "1407.1065v3"
    },
    {
      "index": 11,
      "title": "Lyapunov analysis: from dynamical systems theory to applications",
      "abstract": "",
      "year": "2013",
      "venue": "Journal of Physics A: Mathematical and Theoretical",
      "authors": "Massimo Cencini and Francesco Ginelli"
    },
    {
      "index": 12,
      "title": "Convergence of gradient descent for deep neural networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.16462",
      "authors": "Sourav Chatterjee",
      "orig_title": "Convergence of gradient descent for deep neural networks",
      "paper_id": "2203.16462v4"
    },
    {
      "index": 13,
      "title": "The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks",
      "abstract": "",
      "year": "2021",
      "venue": "CoRR",
      "authors": "Niladri S. Chatterji, Philip M. Long, and Peter L. Bartlett",
      "orig_title": "The interplay between implicit bias and benign overfitting in two-layer linear networks",
      "paper_id": "2108.11489v3"
    },
    {
      "index": 14,
      "title": "Nonlinear dynamical systems and control: A Lyapunov-based approach",
      "abstract": "",
      "year": "2008",
      "venue": "Princeton University Press",
      "authors": "VijaySekhar Chellaboina and Wassim M Haddad"
    },
    {
      "index": 15,
      "title": "Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval",
      "abstract": "",
      "year": "2019",
      "venue": "Mathematical Programming",
      "authors": "Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma"
    },
    {
      "index": 16,
      "title": "A note on lazy training in supervised differentiable programming",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Lenaic Chizat and Francis Bach"
    },
    {
      "index": 17,
      "title": "Lyapunov functions and feedback in nonlinear control",
      "abstract": "",
      "year": "2004",
      "venue": "Optimal control, stabilization and nonsmooth analysis",
      "authors": "Francis Clarke"
    },
    {
      "index": 18,
      "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems",
      "abstract": "",
      "year": "2015",
      "venue": "International conference on machine learning",
      "authors": "Christopher De Sa, Christopher Re, and Kunle Olukotun",
      "orig_title": "Global convergence of stochastic gradient descent for some non-convex matrix problems",
      "paper_id": "1411.1134v3"
    },
    {
      "index": 19,
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.02054",
      "authors": "Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh",
      "orig_title": "Gradient descent provably optimizes over-parameterized neural networks",
      "paper_id": "1810.02054v2"
    },
    {
      "index": 20,
      "title": "Continuous vs. discrete optimization of deep neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Omer Elkabetz and Nadav Cohen"
    },
    {
      "index": 21,
      "title": "Inequalities for the trace of matrix product",
      "abstract": "",
      "year": "1994",
      "venue": "IEEE Transactions on Automatic Control",
      "authors": "Yuguang Fang, Kenneth A Loparo, and Xiangbo Feng"
    },
    {
      "index": 22,
      "title": "Optimizing static linear feedback: Gradient method",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Ilyas Fatkhullin and Boris Polyak"
    },
    {
      "index": 23,
      "title": "Characterizing Implicit Bias in Terms of Optimization Geometry",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro",
      "orig_title": "Characterizing implicit bias in terms of optimization geometry",
      "paper_id": "1802.08246v3"
    },
    {
      "index": 24,
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.00468",
      "authors": "Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro",
      "orig_title": "Implicit bias of gradient descent on linear convolutional networks",
      "paper_id": "1806.00468v2"
    },
    {
      "index": 25,
      "title": "Implicit Regularization in Matrix Factorization",
      "abstract": "",
      "year": "2018",
      "venue": "2018 Information Theory and Applications Workshop (ITA)",
      "authors": "Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro",
      "orig_title": "Implicit regularization in matrix factorization",
      "paper_id": "1705.09280v1"
    },
    {
      "index": 26,
      "title": "Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Suriya Gunasekar, Blake Woodworth, and Nathan Srebro",
      "orig_title": "Mirrorless mirror descent: A natural derivation of mirror descent",
      "paper_id": "2004.01025v3"
    },
    {
      "index": 27,
      "title": "Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot",
      "abstract": "",
      "year": "2017",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli",
      "orig_title": "Global convergence of non-convex gradient descent for computing matrix squareroot",
      "paper_id": "1507.05854v2"
    },
    {
      "index": 28,
      "title": "Risk and parameter convergence of logistic regression",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.07300",
      "authors": "Ziwei Ji and Matus Telgarsky",
      "orig_title": "Risk and parameter convergence of logistic regression",
      "paper_id": "1803.07300v3"
    },
    {
      "index": 29,
      "title": "Provable efficient online matrix completion via non-convex stochastic gradient descent",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Chi Jin, Sham M Kakade, and Praneeth Netrapalli"
    },
    {
      "index": 30,
      "title": "Provable efficient online matrix completion via non-convex stochastic gradient descent",
      "abstract": "",
      "year": "2016",
      "venue": "NIPS",
      "authors": "Chi Jin, Sham M. Kakade, and Praneeth Netrapalli"
    },
    {
      "index": 31,
      "title": "SGD: the role of implicit regularization, batch-size and multiple-epochs",
      "abstract": "",
      "year": "2021",
      "venue": "Neural Information Processing Systems 2021, NeurIPS 2021",
      "authors": "Satyen Kale, Ayush Sekhari, and Karthik Sridharan"
    },
    {
      "index": 32,
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition",
      "abstract": "",
      "year": "2016",
      "venue": "European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Hamed Karimi, Julie Nutini, and Mark Schmidt",
      "orig_title": "Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition",
      "paper_id": "1608.04636v4"
    },
    {
      "index": 33,
      "title": "Classical converse theorems in lyapunov’s second method",
      "abstract": "",
      "year": "2015",
      "venue": "Discrete and Continuous Dynamical Systems - B",
      "authors": "Christopher M. Kellett"
    },
    {
      "index": 34,
      "title": "An Alternative View: When Does SGD Escape Local Minima?",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Robert Kleinberg, Yuanzhi Li, and Yang Yuan",
      "orig_title": "An alternative view: When does sgd escape local minima?",
      "paper_id": "1802.06175v2"
    },
    {
      "index": 35,
      "title": "Continuous time analysis of momentum methods",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Nikola B Kovachki and Andrew M Stuart"
    },
    {
      "index": 36,
      "title": "Continuous and discrete dynamics for online learning and convex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Walid Krichene"
    },
    {
      "index": 37,
      "title": "A Lyapunov Approach to Accelerated First-Order Optimization In Continuous and Discrete Time",
      "abstract": "",
      "year": "2016",
      "venue": "University of California, Berkeley",
      "authors": "Walid Krichene"
    },
    {
      "index": 38,
      "title": "Accelerated mirror descent in continuous and discrete time",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Walid Krichene, Alexandre Bayen, and Peter L Bartlett"
    },
    {
      "index": 39,
      "title": "On gradients of functions definable in o-minimal structures",
      "abstract": "",
      "year": "1998",
      "venue": "Annales de l’institut Fourier",
      "authors": "Krzysztof Kurdyka"
    },
    {
      "index": 40,
      "title": "A topological property of real analytic subsets",
      "abstract": "",
      "year": "1963",
      "venue": "Coll. du CNRS, Les équations aux dérivées partielles",
      "authors": "Stanislaw Lojasiewicz"
    },
    {
      "index": 41,
      "title": "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen"
    },
    {
      "index": 42,
      "title": "On the Global Convergence Rates of Softmax Policy Gradient Methods",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans",
      "orig_title": "On the global convergence rates of softmax policy gradient methods",
      "paper_id": "2005.06392v3"
    },
    {
      "index": 43,
      "title": "Leveraging non-uniformity in first-order non-convex optimization",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans"
    },
    {
      "index": 44,
      "title": "Global exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic regulator",
      "abstract": "",
      "year": "2019",
      "venue": "2019 IEEE 58th Conference on Decision and Control (CDC)",
      "authors": "Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanović"
    },
    {
      "index": 45,
      "title": "Stochastic approximation approach to stochastic programming",
      "abstract": "",
      "year": "",
      "venue": "SIAM J. Optim.",
      "authors": "A Nemirovski, A Juditsky, G Lan, and A Shapiro"
    },
    {
      "index": 46,
      "title": "Problem complexity and method efficiency in optimization",
      "abstract": "",
      "year": "1983",
      "venue": "",
      "authors": "Arkadij Semenovič Nemirovskij and David Borisovich Yudin"
    },
    {
      "index": 47,
      "title": "Continuous-time Models for Stochastic Optimization Algorithms",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Antonio Orvieto and Aurelien Lucchi",
      "orig_title": "Continuous-time models for stochastic optimization algorithms",
      "paper_id": "1810.02565v3"
    },
    {
      "index": 48,
      "title": "Gradient methods for the minimisation of functionals",
      "abstract": "",
      "year": "1963",
      "venue": "USSR Computational Mathematics and Mathematical Physics",
      "authors": "B.T. Polyak"
    },
    {
      "index": 49,
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "abstract": "",
      "year": "2018",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro",
      "orig_title": "The implicit bias of gradient descent on separable data",
      "paper_id": "1710.10345v7"
    },
    {
      "index": 50,
      "title": "Smoothness, low noise and fast rates",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nathan Srebro, Karthik Sridharan, and Ambuj Tewari"
    },
    {
      "index": 51,
      "title": "A differential equation for modeling nesterov’s accelerated gradient method: theory and insights",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "Weijie Su, Stephen Boyd, and Emmanuel Candes"
    },
    {
      "index": 52,
      "title": "Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.12837",
      "authors": "Yan Shuo Tan and Roman Vershynin"
    },
    {
      "index": 53,
      "title": "Implicit Regularization in ReLU Networks with the Square Loss",
      "abstract": "",
      "year": "2021",
      "venue": "Conference on Learning Theory",
      "authors": "Gal Vardi and Ohad Shamir",
      "orig_title": "Implicit regularization in relu networks with the square loss",
      "paper_id": "2012.05156v3"
    },
    {
      "index": 54,
      "title": "Lyapunov arguments in optimization",
      "abstract": "",
      "year": "2018",
      "venue": "University of California, Berkeley",
      "authors": "Ashia Wilson"
    },
    {
      "index": 55,
      "title": "A lyapunov analysis of accelerated methods in optimization",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Ashia C. Wilson, Ben Recht, and Michael I. Jordan"
    },
    {
      "index": 56,
      "title": "A lyapunov analysis of accelerated methods in optimization",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Ashia C Wilson, Ben Recht, and Michael I Jordan"
    },
    {
      "index": 57,
      "title": "Stochastic gradient descent with noise of machine learning type Part I: Discrete time analysis",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2105.01650",
      "authors": "Stephan Wojtowytsch",
      "orig_title": "Stochastic gradient descent with noise of machine learning type. part i: Discrete time analysis",
      "paper_id": "2105.01650v2"
    },
    {
      "index": 58,
      "title": "Stochastic gradient descent with noise of machine learning type Part II: Continuous time analysis",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.02588",
      "authors": "Stephan Wojtowytsch",
      "orig_title": "Stochastic gradient descent with noise of machine learning type. part ii: Continuous time analysis",
      "paper_id": "2106.02588v2"
    },
    {
      "index": 59,
      "title": "A general sample complexity analysis of vanilla policy gradient",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Rui Yuan, Robert M Gower, and Alessandro Lazaric"
    },
    {
      "index": 60,
      "title": "Global convergence in deep learning with variable splitting via the kurdyka-łojasiewicz property",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1803.00225",
      "authors": "Jinshan Zeng, Shikang Ouyang, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao"
    },
    {
      "index": 61,
      "title": "Revisiting the role of euler numerical integration on acceleration and stability in convex optimization",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Peiyuan Zhang, Antonio Orvieto, Hadi Daneshmand, Thomas Hofmann, and Roy S Smith"
    }
  ]
}