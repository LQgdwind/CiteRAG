{
  "paper_id": "2305.18864v2",
  "title": "Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution",
  "abstract": "Abstract\nStochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise.\nSince the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation.\nIn this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective.\nThe proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size.\nNumerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Furthermore, we provide a simple PyTorch implementation of the proposed algorithm.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Privacy of noisy stochastic gradient descent: More iterations without more privacy loss",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jason Altschuler and Kunal Talwar"
    },
    {
      "index": 1,
      "title": "Sigma-delta quantization and finite frames",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": "J.J. Benedetto, O. Yilmaz, and A.M. Powell"
    },
    {
      "index": 2,
      "title": "A Note on the Generation of Random Normal Deviates",
      "abstract": "",
      "year": "1958",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "G. E. P. Box and Mervin E. Muller"
    },
    {
      "index": 3,
      "title": "The promises and pitfalls of stochastic gradient langevin dynamics",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nicolas Brosse, Alain Durmus, and Eric Moulines"
    },
    {
      "index": 4,
      "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
      "abstract": "",
      "year": "2018",
      "venue": "Information Theory and Applications Workshop",
      "authors": "Pratik Chaudhari and Stefano Soatto",
      "orig_title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
      "paper_id": "1710.11029v2"
    },
    {
      "index": 5,
      "title": "Stochastic Gradient and Langevin Processes",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan",
      "orig_title": "Stochastic gradient and langevin processes",
      "paper_id": "1907.03215v7"
    },
    {
      "index": 6,
      "title": "Stochastic Gradient and Langevin Processes",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan",
      "orig_title": "Stochastic gradient and Langevin processes",
      "paper_id": "1907.03215v7"
    },
    {
      "index": 7,
      "title": "User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient",
      "abstract": "",
      "year": "2019",
      "venue": "Stochastic Processes and their Applications",
      "authors": "Arnak S. Dalalyan and Avetik Karagulyan",
      "orig_title": "User-friendly guarantees for the langevin monte carlo with inaccurate gradient",
      "paper_id": "1710.00095v4"
    },
    {
      "index": 8,
      "title": "Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Christopher M De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré, and Christopher Ré",
      "orig_title": "Taming the wild: A unified analysis of hogwild-style algorithms",
      "paper_id": "1506.06438v2"
    },
    {
      "index": 9,
      "title": "Incorporating Nesterov Momentum into Adam",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations: Workshop Track",
      "authors": "Timothy Dozat"
    },
    {
      "index": 10,
      "title": "Statistical Learning and Inverse Problems: A Stochastic Gradient Approach",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yuri Fonseca and Yuri Saporito",
      "orig_title": "Statistical learning and inverse problems: A stochastic gradient approach",
      "paper_id": "2209.14967v3"
    },
    {
      "index": 11,
      "title": "Diffusions for global optimization",
      "abstract": "",
      "year": "1986",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Stuart Geman and Chii-Ruey Hwang"
    },
    {
      "index": 12,
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He",
      "orig_title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "paper_id": "1706.02677v2"
    },
    {
      "index": 13,
      "title": "Learning rates as a function of batch size: A random matrix theory approach to neural network training",
      "abstract": "",
      "year": "2022",
      "venue": "Journal of Machine Learning Research",
      "authors": "Diego Granziol, Stefan Zohren, and Stephen Roberts"
    },
    {
      "index": 14,
      "title": "Quantization",
      "abstract": "",
      "year": "2006",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "Robert. M. Gray and David L. Neuhoff"
    },
    {
      "index": 15,
      "title": "Learning both Weights and Connections for Efficient Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Song Han, Jeff Pool, John Tran, and William Dally",
      "orig_title": "Learning both weights and connections for efficient neural network",
      "paper_id": "1506.02626v3"
    },
    {
      "index": 16,
      "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Neural Information Processing Systems",
      "authors": "Elad Hoffer, Itay Hubara, and Daniel Soudry",
      "orig_title": "Train longer, generalize better: Closing the generalization gap in large batch training of neural networks",
      "paper_id": "1705.08741v2"
    },
    {
      "index": 17,
      "title": "White noise hypothesis for uniform quantization errors",
      "abstract": "",
      "year": "2007",
      "venue": "SIAM J. Math. Analysis",
      "authors": "David Jiménez, Long Wang, and Yang Wang"
    },
    {
      "index": 18,
      "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Sangil Jung, Changyong Son, Seohyung Lee, JinWoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi",
      "orig_title": "Learning to quantize deep networks by optimizing quantization intervals with task loss",
      "paper_id": "1808.05779v3"
    },
    {
      "index": 19,
      "title": "Approaching quartic convergence rates for quasi-stochastic approximation with application to gradient-free optimization",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Caio Kalil Lauand and Sean Meyn"
    },
    {
      "index": 20,
      "title": "On the insufficiency of existing momentum schemes for stochastic optimization",
      "abstract": "",
      "year": "2018",
      "venue": "ITA",
      "authors": "Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade"
    },
    {
      "index": 21,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Diederik P. Kingma and Jimmy Ba"
    },
    {
      "index": 22,
      "title": "Introduction To Stochastic Calculus With Applications",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": "Fima C. Klebaner"
    },
    {
      "index": 23,
      "title": "One weird trick for parallelizing convolutional neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": "Alex Krizhevsky",
      "orig_title": "One weird trick for parallelizing convolutional neural networks",
      "paper_id": "1404.5997v2"
    },
    {
      "index": 24,
      "title": "On the Weak Convergence of Interpolated Markov Chains to a Diffusion",
      "abstract": "",
      "year": "1974",
      "venue": "The Annals of Probability",
      "authors": "Harold J. Kushner"
    },
    {
      "index": 25,
      "title": "Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations",
      "abstract": "",
      "year": "2019",
      "venue": "Journal of Machine Learning Research",
      "authors": "Qianxiao Li, Cheng Tai, and Weinan E",
      "orig_title": "Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations",
      "paper_id": "1811.01558v1"
    },
    {
      "index": 26,
      "title": "Generalization error analysis of quantized compressive learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Xiaoyun Li and Ping Li"
    },
    {
      "index": 27,
      "title": "On the validity of modeling sgd with stochastic differential equations (sdes)",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora"
    },
    {
      "index": 28,
      "title": "Fast mixing of stochastic gradient descent with normalization and weight decay",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zhiyuan Li, Tianhao Wang, and Dingli Yu"
    },
    {
      "index": 29,
      "title": "Don’t Use Large Mini-Batches, Use Local SGD",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi",
      "orig_title": "Don’t use large mini-batches, use local sgd",
      "paper_id": "1808.07217v6"
    },
    {
      "index": 30,
      "title": "Accelerating SGD with momentum for over-parameterized learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Chaoyue Liu and Mikhail Belkin",
      "orig_title": "Accelerating sgd with momentum for over-parameterized learning",
      "paper_id": "1810.13395v5"
    },
    {
      "index": 31,
      "title": "On the Variance of the Adaptive Learning Rate and Beyond",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han",
      "orig_title": "On the variance of the adaptive learning rate and beyond",
      "paper_id": "1908.03265v4"
    },
    {
      "index": 32,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Ilya Loshchilov and Frank Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 33,
      "title": "The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Siyuan Ma, Raef Bassily, and Mikhail Belkin",
      "orig_title": "The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning",
      "paper_id": "1712.06559v3"
    },
    {
      "index": 34,
      "title": "On the sdes and scaling rules for adaptive gradient algorithms",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora"
    },
    {
      "index": 35,
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "abstract": "",
      "year": "2017",
      "venue": "Journal of Machine Learning Research",
      "authors": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei",
      "orig_title": "Stochastic gradient descent as approximate bayesian inference",
      "paper_id": "1704.04289v2"
    },
    {
      "index": 36,
      "title": "The validity of the additive noise model for uniform scalar quantizers",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "D. Marco and D.L. Neuhoff"
    },
    {
      "index": 37,
      "title": "Generating a variable from the tail of the normal distribution",
      "abstract": "",
      "year": "1963",
      "venue": "Mathematical Note No. 322",
      "authors": "G. Marsaglia"
    },
    {
      "index": 38,
      "title": "The ziggurat method for generating random variables",
      "abstract": "",
      "year": "2000",
      "venue": "Journal of Statistical Software",
      "authors": "George Marsaglia and Wai Wan Tsang"
    },
    {
      "index": 39,
      "title": "Stochastic Second-Order Methods Improve Best-Known Sample Complexity of SGD for Gradient-Dominated Functions",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Saeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, and Patrick Thiran",
      "orig_title": "Stochastic second-order methods improve best-known sample complexity of sgd for gradient-dominated functions",
      "paper_id": "2205.12856v2"
    },
    {
      "index": 40,
      "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints",
      "abstract": "",
      "year": "2018",
      "venue": "Conference On Learning Theory",
      "authors": "Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng",
      "orig_title": "Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints",
      "paper_id": "1707.05947v1"
    },
    {
      "index": 41,
      "title": "First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gaël RICHARD",
      "orig_title": "First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise",
      "paper_id": "1906.09069v1"
    },
    {
      "index": 42,
      "title": "Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Analysis",
      "abstract": "",
      "year": "2017",
      "venue": "Conference on Learning Theory",
      "authors": "Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky",
      "orig_title": "Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis",
      "paper_id": "1702.03849v3"
    },
    {
      "index": 43,
      "title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns",
      "abstract": "",
      "year": "2014",
      "venue": "Interspeech",
      "authors": "Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu"
    },
    {
      "index": 44,
      "title": "Measuring the Effects of Data Parallelism on Neural Network Training",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Machine Learning Research",
      "authors": "Chris Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-dickstein, Roy Frostig, and George Dahl",
      "orig_title": "Measuring the effects of data parallelism on neural network training",
      "paper_id": "1811.03600v3"
    },
    {
      "index": 45,
      "title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
      "abstract": "",
      "year": "2013",
      "venue": "International Conference on Machine Learning",
      "authors": "Ohad Shamir and Tong Zhang"
    },
    {
      "index": 46,
      "title": "A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban",
      "orig_title": "A tail-index analysis of stochastic gradient noise in deep neural networks",
      "paper_id": "1901.06053v1"
    },
    {
      "index": 47,
      "title": "Momentum enables large batch training",
      "abstract": "",
      "year": "2019",
      "venue": "ICML workshop on Theoretical Physics in Deep Learning",
      "authors": "Samuel L. Smith, Erich Elsen, and Soham De"
    },
    {
      "index": 48,
      "title": "On the Generalization Benefit of Noise in Stochastic Gradient Descent",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Samuel L. Smith, Erich Elsen, and Soham De",
      "orig_title": "On the generalization benefit of noise in stochastic gradient descent",
      "paper_id": "2006.15081v1"
    },
    {
      "index": 49,
      "title": "Gaussian random number generators",
      "abstract": "",
      "year": "2007",
      "venue": "ACM Comput. Surv.",
      "authors": "David B. Thomas, Wayne Luk, Philip H.W. Leong, and John D. Villasenor"
    },
    {
      "index": 50,
      "title": "Analyzing the generalization capability of sgld using properties of gaussian channels",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Hao Wang, Yizhe Huang, Rui Gao, and Flavio Calmon"
    },
    {
      "index": 51,
      "title": "Bayesian learning via stochastic gradient langevin dynamics",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Machine Learning",
      "authors": "Max Welling and Yee Whye Teh"
    },
    {
      "index": 52,
      "title": "Learning Structured Sparsity in Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li",
      "orig_title": "Learning structured sparsity in deep neural networks",
      "paper_id": "1608.03665v4"
    },
    {
      "index": 53,
      "title": "On the Noisy Gradient Descent that Generalizes as SGD",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu",
      "orig_title": "On the noisy gradient descent that generalizes as sgd",
      "paper_id": "1906.07405v3"
    },
    {
      "index": 54,
      "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Han Xiao, Kashif Rasul, and Roland Vollgraf",
      "orig_title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "paper_id": "1708.07747v2"
    },
    {
      "index": 55,
      "title": "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Zeke Xie, Issei Sato, and Masashi Sugiyama",
      "orig_title": "A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima",
      "paper_id": "2002.03495v14"
    },
    {
      "index": 56,
      "title": "Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu",
      "orig_title": "Global convergence of langevin dynamics based algorithms for nonconvex optimization",
      "paper_id": "1707.06618v3"
    },
    {
      "index": 57,
      "title": "On lattice quantization noise",
      "abstract": "",
      "year": "1996",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "R. Zamir and M. Feder"
    },
    {
      "index": 58,
      "title": "Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue, and Roger B Grosse",
      "orig_title": "Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model",
      "paper_id": "1907.04164v2"
    },
    {
      "index": 59,
      "title": "Low-Precision Stochastic Gradient Langevin Dynamics",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Ruqi Zhang, Andrew Gordon Wilson, and Christopher De Sa",
      "orig_title": "Low-precision stochastic gradient langevin dynamics",
      "paper_id": "2206.09909v1"
    },
    {
      "index": 60,
      "title": "Stochastic Differential Equations:An Introduction with Applications",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": "Bernt Øksendal"
    }
  ]
}