{
  "paper_id": "2209.00347v2",
  "title": "Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization",
  "abstract": "Abstract\nA key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the RL agentâ€™s behavior as the environment changes over its lifetime, while minimizing the catastrophic forgetting of the learned information.\nTo address this challenge, in this article, we propose DaCoRL, i.e., dynamics-adaptive continual RL.\nDaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy.\nSpecifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts.\nUnder the assumption of a Chinese restaurant process prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance.\nFurthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context, and a knowledge distillation regularization term for retaining the performance on learned tasks.\nAs a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of the stability, overall performance and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "MIT press",
      "authors": "R. S. Sutton and A. G. Barto"
    },
    {
      "index": 1,
      "title": "Q-learning",
      "abstract": "",
      "year": "1992",
      "venue": "Mach. Learn.",
      "authors": "C. J. C. H. Watkins and P. Dayan"
    },
    {
      "index": 2,
      "title": "On-line q-learning using connectionist systems",
      "abstract": "",
      "year": "1994",
      "venue": "Technical Report",
      "authors": "G. A. Rummery and M. Niranjan"
    },
    {
      "index": 3,
      "title": "A Survey on Transformers in Reinforcement Learning",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2301.03044",
      "authors": "W. Li, H. Luo et al.",
      "orig_title": "A survey on transformers in reinforcement learning",
      "paper_id": "2301.03044v3"
    },
    {
      "index": 4,
      "title": "Towards Playing Full MOBA Games with Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "D. Ye et al.",
      "orig_title": "Towards playing full moba games with deep reinforcement learning",
      "paper_id": "2011.12692v4"
    },
    {
      "index": 5,
      "title": "TiKick: Towards Playing Multi-agent Football Full Games from Single-agent Demonstrations",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2110.04507",
      "authors": "S. Huang et al.",
      "orig_title": "Tikick: towards playing multi-agent football full games from single-agent demonstrations",
      "paper_id": "2110.04507v5"
    },
    {
      "index": 6,
      "title": "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conf. Artif. Intell.",
      "authors": "D. Ye et al.",
      "orig_title": "Mastering complex control in moba games with deep reinforcement learning",
      "paper_id": "1912.09729v3"
    },
    {
      "index": 7,
      "title": "Supervised learning achieves human-level performance in moba games: A case study of honor of kings",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst.",
      "authors": "D. Ye, G. Chen et al."
    },
    {
      "index": 8,
      "title": "Which Heroes to Pick? Learning to Draft in MOBA Games with Neural Networks and Tree Search",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Trans. Games",
      "authors": "S. Chen et al.",
      "orig_title": "Which heroes to pick? learning to draft in moba games with neural networks and tree search",
      "paper_id": "2012.10171v4"
    },
    {
      "index": 9,
      "title": "Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning",
      "abstract": "",
      "year": "2022",
      "venue": "Int. Joint Conf. Artif. Intell.",
      "authors": "Z. Lin et al."
    },
    {
      "index": 10,
      "title": "Long-Range Indoor Navigation with PRM-RL",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Trans. Robot.",
      "authors": "A. Francis et al.",
      "orig_title": "Long-range indoor navigation with PRM-RL",
      "paper_id": "1902.09458v2"
    },
    {
      "index": 11,
      "title": "Autonomous navigation of stratospheric balloons using reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Nature",
      "authors": "M. G. Bellemare et al."
    },
    {
      "index": 12,
      "title": "Reinforcement based mobile robot navigation in dynamic environment",
      "abstract": "",
      "year": "2011",
      "venue": "Robot. Comput. Integr. Manuf.",
      "authors": "M. A. K. Jaradat, M. Al-Rousan, and L. Quadan"
    },
    {
      "index": 13,
      "title": "Towards Continual Reinforcement Learning: A Review and Perspectives",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2012.13490",
      "authors": "K. Khetarpal, M. Riemer, I. Rish, and D. Precup",
      "orig_title": "Towards continual reinforcement learning: A review and perspectives",
      "paper_id": "2012.13490v2"
    },
    {
      "index": 14,
      "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
      "abstract": "",
      "year": "1989",
      "venue": "Psychol. Learn. Motiv.",
      "authors": "M. McCloskey and N. J. Cohen"
    },
    {
      "index": 15,
      "title": "Catastrophic forgetting in connectionist networks",
      "abstract": "",
      "year": "1999",
      "venue": "Trends Cogn. Sci.",
      "authors": "R. M. French"
    },
    {
      "index": 16,
      "title": "Embracing change: Continual learning in deep neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Trends Cogn. Sci.",
      "authors": "R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu"
    },
    {
      "index": 17,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Nat. Acad. Sci.",
      "authors": "J. Kirkpatrick et al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 18,
      "title": "UNCLEAR: A straightforward method for continual reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "S. Kessler, J. Parker-Holder, P. Ball, S. Zohren, and S. J. Roberts"
    },
    {
      "index": 19,
      "title": "Reinforcement learning algorithm for non-stationary environments",
      "abstract": "",
      "year": "2020",
      "venue": "Appl. Intell.",
      "authors": "S. Padakandla, K. Prabuchandran, and S. Bhatnagar"
    },
    {
      "index": 20,
      "title": "Catastrophic Interference in Reinforcement Learning: A Solution Based on Context Division and Knowledge Distillation",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst.",
      "authors": "T. Zhang, X. Wang, B. Liang, and B. Yuan",
      "orig_title": "Catastrophic interference in reinforcement learning: A solution based on context division and knowledge distillation",
      "paper_id": "2109.00525v2"
    },
    {
      "index": 21,
      "title": "The infinite gaussian mixture model",
      "abstract": "",
      "year": "1999",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "C. Rasmussen"
    },
    {
      "index": 22,
      "title": "Combinatorial stochastic processes",
      "abstract": "",
      "year": "2002",
      "venue": "Dept. of Stat., UC Berkeley, USA, Technical Report",
      "authors": "J. Pitman"
    },
    {
      "index": 23,
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "abstract": "",
      "year": "2017",
      "venue": "Conf. Comput. Vis. Pattern Recognit.",
      "authors": "S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert",
      "orig_title": "iCaRL: Incremental classifier and representation learning",
      "paper_id": "1611.07725v2"
    },
    {
      "index": 24,
      "title": "Incremental Learning Through Deep Adaptation",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "A. Rosenfeld and J. K. Tsotsos",
      "orig_title": "Incremental learning through deep adaptation",
      "paper_id": "1705.04228v2"
    },
    {
      "index": 25,
      "title": "Constrained Few-shot Class-incremental Learning",
      "abstract": "",
      "year": "2022",
      "venue": "Conf. Comput. Vis. Pattern Recognit.",
      "authors": "M. Hersche, G. Karunaratne, G. Cherubini, L. Benini, A. Sebastian, and A. Rahimi",
      "orig_title": "Constrained few-shot class-incremental learning",
      "paper_id": "2203.16588v1"
    },
    {
      "index": 26,
      "title": "Incremental reinforcement learning in continuous spaces via policy relaxation and importance weighting",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst.",
      "authors": "Z. Wang, H.-X. Li, and C. Chen"
    },
    {
      "index": 27,
      "title": "Lifelong incremental reinforcement learning with online bayesian inference",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst.",
      "authors": "Z. Wang, C. Chen, and D. Dong"
    },
    {
      "index": 28,
      "title": "Minimum-delay adaptation in non-stationary reinforcement learning via online high-confidence change-point detection",
      "abstract": "",
      "year": "2021",
      "venue": "Int. Conf. Auton. Agent. Multi. Agent Syst.",
      "authors": "L. N. Alegre, A. L. Bazzan, and B. C. da Silva"
    },
    {
      "index": 29,
      "title": "Online learning and stochastic approximations",
      "abstract": "",
      "year": "1998",
      "venue": "Online Learn. Neural Netw.",
      "authors": "L. Bottou et al."
    },
    {
      "index": 30,
      "title": "Online learning and online convex optimization",
      "abstract": "",
      "year": "2012",
      "venue": "Found. Trends Mach. Learn.",
      "authors": "S. Shalev-Shwartz et al."
    },
    {
      "index": 31,
      "title": "Curriculum-based Asymmetric Multi-task Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
      "authors": "H. Huang, D. Ye, L. Shen, and W. Liu",
      "orig_title": "Curriculum-based asymmetric multi-task reinforcement learning",
      "paper_id": "2211.03352v1"
    },
    {
      "index": 32,
      "title": "Multi-Task Learning with Deep Neural Networks: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2009.09796",
      "authors": "M. Crawshaw",
      "orig_title": "Multi-task learning with deep neural networks: A survey",
      "paper_id": "2009.09796v1"
    },
    {
      "index": 33,
      "title": "A Survey on Multi-Task Learning",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Trans. Knowl. Data. Eng.",
      "authors": "Y. Zhang and Q. Yang",
      "orig_title": "A survey on multi-task learning",
      "paper_id": "1707.08114v3"
    },
    {
      "index": 34,
      "title": "Selective Experience Replay for Lifelong Learning",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conf. Artif. Intell.",
      "authors": "D. Isele and A. Cosgun",
      "orig_title": "Selective experience replay for lifelong learning",
      "paper_id": "1802.10269v1"
    },
    {
      "index": 35,
      "title": "Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference",
      "abstract": "",
      "year": "2019",
      "venue": "Int. Conf. Learn. Represent.",
      "authors": "M. Riemer et al.",
      "orig_title": "Learning to learn without forgetting by maximizing transfer and minimizing interference",
      "paper_id": "1810.11910v3"
    },
    {
      "index": 36,
      "title": "Experience Replay for Continual Learning",
      "abstract": "",
      "year": "2019",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne",
      "orig_title": "Experience replay for continual learning",
      "paper_id": "1811.11682v2"
    },
    {
      "index": 37,
      "title": "Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting",
      "abstract": "",
      "year": "2021",
      "venue": "Neurocomputing",
      "authors": "C. Atkinson, B. McCane, L. Szymanski, and A. Robins",
      "orig_title": "Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting",
      "paper_id": "1812.02464v6"
    },
    {
      "index": 38,
      "title": "Policy Distillation",
      "abstract": "",
      "year": "2016",
      "venue": "Int. Conf. Learn. Represent.",
      "authors": "A. A. Rusu et al.",
      "orig_title": "Policy distillation",
      "paper_id": "1511.06295v2"
    },
    {
      "index": 39,
      "title": "DisCoRL: Continual Reinforcement Learning via Policy Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "Conf. Neural Inf. Process. Syst. Workshop",
      "authors": "TraorÃ© et al.",
      "orig_title": "Discorl: Continual reinforcement learning via policy distillation",
      "paper_id": "1907.05855v1"
    },
    {
      "index": 40,
      "title": "Progressive Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv:1606.04671",
      "authors": "A. A. Rusu et al.",
      "orig_title": "Progressive neural networks",
      "paper_id": "1606.04671v4"
    },
    {
      "index": 41,
      "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
      "abstract": "",
      "year": "2018",
      "venue": "Conf. Comput. Vis. Pattern Recognit.",
      "authors": "A. Mallya and S. Lazebnik",
      "orig_title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
      "paper_id": "1711.05769v2"
    },
    {
      "index": 42,
      "title": "A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong Reinforcement Learning",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Cybern.",
      "authors": "Z. Wang, C. Chen, and D. Dong",
      "orig_title": "A dirichlet process mixture of robust task models for scalable lifelong reinforcement learning",
      "paper_id": "2205.10787v1"
    },
    {
      "index": 43,
      "title": "Near-optimal regret bounds for reinforcement learning",
      "abstract": "",
      "year": "2008",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "P. Auer, T. Jaksch, and R. Ortner"
    },
    {
      "index": 44,
      "title": "A Sliding-Window Algorithm for Markov Decision Processes with Arbitrarily Changing Rewards and Transitions",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1805.10066",
      "authors": "P. Gajane, R. Ortner, and P. Auer",
      "orig_title": "A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions",
      "paper_id": "1805.10066v1"
    },
    {
      "index": 45,
      "title": "Variational Regret Bounds for Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conf. Uncertain. Artif. Intell.",
      "authors": "R. Ortner, P. Gajane, and P. Auer",
      "orig_title": "Variational regret bounds for reinforcement learning",
      "paper_id": "1905.05857v3"
    },
    {
      "index": 46,
      "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism",
      "abstract": "",
      "year": "2020",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "W. C. Cheung, D. Simchi-Levi, and R. Zhu",
      "orig_title": "Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism",
      "paper_id": "2006.14389v1"
    },
    {
      "index": 47,
      "title": "A kernel-based approach to non-stationary reinforcement learning in metric spaces",
      "abstract": "",
      "year": "2021",
      "venue": "Int. Conf. Uncertain. Artif. Intell.",
      "authors": "O. D. Domingues, P. MÃ©nard, M. Pirotta, E. Kaufmann, and M. Valko"
    },
    {
      "index": 48,
      "title": "Dealing with non-stationary environments using context detection",
      "abstract": "",
      "year": "2006",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "B. C. Da Silva, E. W. Basso, A. L. Bazzan, and P. M. Engel"
    },
    {
      "index": 49,
      "title": "Sequential decision-making under non-stationary environments via sequential change-point detection",
      "abstract": "",
      "year": "2014",
      "venue": "Int. Workshop Learn. over Multiple Contexts",
      "authors": "E. Hadoux, A. Beynier, and P. Weng"
    },
    {
      "index": 50,
      "title": "Model-free non-stationarity detection and adaptation in reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "Eur. Conf. Artif. Intell.",
      "authors": "G. Canonaco, M. Restelli, and M. Roveri"
    },
    {
      "index": 51,
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "abstract": "",
      "year": "1992",
      "venue": "Mach. Learn.",
      "authors": "R. J. Williams"
    },
    {
      "index": 52,
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "abstract": "",
      "year": "2018",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine",
      "orig_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
      "paper_id": "1801.01290v2"
    },
    {
      "index": 53,
      "title": "Revisiting Discrete Soft Actor-Critic",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2209.10081",
      "authors": "H. Zhou, Z. Lin, J. Li, D. Ye, Q. Fu, and W. Yang",
      "orig_title": "Revisiting discrete soft actor-critic",
      "paper_id": "2209.10081v4"
    },
    {
      "index": 54,
      "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv:1812.07671",
      "authors": "A. Nagabandi, C. Finn, and S. Levine",
      "orig_title": "Deep online learning via meta-learning: Continual adaptation for model-based rl",
      "paper_id": "1812.07671v2"
    },
    {
      "index": 55,
      "title": "Continual Reinforcement Learning in 3D Non-stationary Environments",
      "abstract": "",
      "year": "2020",
      "venue": "Conf. Comput. Vis. Pattern Recognit. Workshops",
      "authors": "V. Lomonaco, K. Desai, E. Culurciello, and D. Maltoni",
      "orig_title": "Continual reinforcement learning in 3d non-stationary environments",
      "paper_id": "1905.10112v2"
    },
    {
      "index": 56,
      "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Int. Conf. Learn. Represent.",
      "authors": "C. Dâ€™Eramo et al.",
      "orig_title": "Sharing knowledge in multi-task deep reinforcement learning",
      "paper_id": "2401.09561v1"
    },
    {
      "index": 57,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 58,
      "title": "State entropy maximization with random encoders for efficient exploration",
      "abstract": "",
      "year": "2021",
      "venue": "Int. Conf. Mach. Learn.",
      "authors": "Y. Seo, L. Chen, J. Shin, H. Lee, P. Abbeel, and K. Lee"
    },
    {
      "index": 59,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "I. Goodfellow et al."
    },
    {
      "index": 60,
      "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conf. Robot Learn.",
      "authors": "T. Yu et al.",
      "orig_title": "Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning",
      "paper_id": "1910.10897v2"
    },
    {
      "index": 61,
      "title": "Proximal policy optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv:1707.06347",
      "authors": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov"
    },
    {
      "index": 62,
      "title": "Coordinated Proximal Policy Optimization",
      "abstract": "",
      "year": "2021",
      "venue": "Conf. Neural Inf. Process. Syst.",
      "authors": "Z. Wu et al.",
      "orig_title": "Coordinated proximal policy optimization",
      "paper_id": "2111.04051v1"
    }
  ]
}