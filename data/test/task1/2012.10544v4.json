{
  "paper_id": "2012.10544v4",
  "title": "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses",
  "abstract": "Abstract\nAs machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance.\nThe absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Deep Learning with Differential Privacy",
      "abstract": "",
      "year": "2016",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang",
      "orig_title": "Deep learning with differential privacy",
      "paper_id": "1607.00133v2"
    },
    {
      "index": 1,
      "title": "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring",
      "abstract": "",
      "year": "2018",
      "venue": "USENIX Security Symposium",
      "authors": "Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet",
      "orig_title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
      "paper_id": "1802.04633v3"
    },
    {
      "index": 2,
      "title": "Venomave: Clean-label poisoning against speech recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Hojjat Aghakhani, Thorsten Eisenhofer, Lea Sch√∂nherr, Dorothea Kolossa, Thorsten Holz, Christopher Kruegel, and Giovanni Vigna"
    },
    {
      "index": 3,
      "title": "Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00191",
      "authors": "Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna",
      "orig_title": "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability",
      "paper_id": "2005.00191v3"
    },
    {
      "index": 4,
      "title": "BaFFLe: Backdoor Detection via Feedback-based Federated Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.02167",
      "authors": "Sebastien Andreina, Giorgia Azzurra Marson, Helen M√∂llering, and Ghassan Karame",
      "orig_title": "Baffle: Backdoor detection via feedback-based federated learning",
      "paper_id": "2011.02167v2"
    },
    {
      "index": 5,
      "title": "Possible malware found hidden inside images from the ImageNet dataset",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Anonymous"
    },
    {
      "index": 6,
      "title": "Synthesizing Robust Adversarial Examples",
      "abstract": "",
      "year": "2018",
      "venue": "International conference on machine learning",
      "authors": "Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok",
      "orig_title": "Synthesizing robust adversarial examples",
      "paper_id": "1707.07397v3"
    },
    {
      "index": 7,
      "title": "On the impossibility of cryptography with tamperable randomness",
      "abstract": "",
      "year": "2014",
      "venue": "Annual Cryptology Conference",
      "authors": "Per Austrin, Kai-Min Chung, Mohammad Mahmoody, Rafael Pass, and Karn Seth"
    },
    {
      "index": 8,
      "title": "How To Backdoor Federated Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov",
      "orig_title": "How to backdoor federated learning",
      "paper_id": "1807.00459v3"
    },
    {
      "index": 9,
      "title": "A little is enough: Circumventing defenses for distributed learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Gilad Baruch, Moran Baruch, and Yoav Goldberg"
    },
    {
      "index": 10,
      "title": "Influence Functions in Deep Learning Are Fragile",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.14651",
      "authors": "Samyadeep Basu, Philip Pope, and Soheil Feizi",
      "orig_title": "Influence functions in deep learning are fragile",
      "paper_id": "2006.14651v2"
    },
    {
      "index": 11,
      "title": "Analyzing Federated Learning through an Adversarial Lens",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo",
      "orig_title": "Analyzing federated learning through an adversarial lens",
      "paper_id": "1811.12470v4"
    },
    {
      "index": 12,
      "title": "Support vector machines under adversarial label noise",
      "abstract": "",
      "year": "2011",
      "venue": "Asian conference on machine learning",
      "authors": "Battista Biggio, Blaine Nelson, and Pavel Laskov"
    },
    {
      "index": 13,
      "title": "Poisoning attacks against support vector machines",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:1206.6389",
      "authors": "Battista Biggio, Blaine Nelson, and Pavel Laskov"
    },
    {
      "index": 14,
      "title": "Evasion attacks against machine learning at test time",
      "abstract": "",
      "year": "2013",
      "venue": "Joint European conference on machine learning and knowledge discovery in databases",
      "authors": "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ≈†rndiƒá, Pavel Laskov, Giorgio Giacinto, and Fabio Roli",
      "orig_title": "Evasion attacks against machine learning at test time",
      "paper_id": "1708.06131v1"
    },
    {
      "index": 15,
      "title": "Machine learning with adversaries: Byzantine tolerant gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al."
    },
    {
      "index": 16,
      "title": "Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2011.09527",
      "authors": "Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, and Arjun Gupta",
      "orig_title": "Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff",
      "paper_id": "2011.09527v1"
    },
    {
      "index": 17,
      "title": "Dp-instahide: Provably defusing poisoning and backdoor attacks with differentially private data augmentations",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.02079",
      "authors": "Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, and Tom Goldstein"
    },
    {
      "index": 18,
      "title": "Analysis of causative attacks against svms learning from data streams",
      "abstract": "",
      "year": "2017",
      "venue": "ACM on International Workshop on Security And Privacy Analytics",
      "authors": "Cody Burkard and Brent Lagesse"
    },
    {
      "index": 19,
      "title": "LEAF: A Benchmark for Federated Settings",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1812.01097",
      "authors": "Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneƒçn·ª≥, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar",
      "orig_title": "Leaf: A benchmark for federated settings",
      "paper_id": "1812.01097v3"
    },
    {
      "index": 20,
      "title": "Understanding distributed poisoning attack in federated learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE International Conference on Parallel and Distributed Systems (ICPADS)",
      "authors": "Di Cao, Shan Chang, Zhijian Lin, Guohua Liu, and Donghong Sun"
    },
    {
      "index": 21,
      "title": "Data Poisoning Attacks to Local Differential Privacy Protocols",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong",
      "orig_title": "Data poisoning attacks to local differential privacy protocols",
      "paper_id": "1911.02046v2"
    },
    {
      "index": 22,
      "title": "Learning from untrusted data",
      "abstract": "",
      "year": "2017",
      "venue": "Symposium on Theory of Computing (STOC)",
      "authors": "Moses Charikar, Jacob Steinhardt, and Gregory Valiant"
    },
    {
      "index": 23,
      "title": "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1811.03728",
      "authors": "Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava",
      "orig_title": "Detecting backdoor attacks on deep neural networks by activation clustering",
      "paper_id": "1811.03728v1"
    },
    {
      "index": 24,
      "title": "Backdoor Attacks on Federated Meta-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.07026",
      "authors": "Chien-Lun Chen, Leana Golubchik, and Marco Paolieri",
      "orig_title": "Backdoor attacks on federated meta-learning",
      "paper_id": "2006.07026v2"
    },
    {
      "index": 25,
      "title": "Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence, IJCAI-19",
      "authors": "Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar"
    },
    {
      "index": 26,
      "title": "Robust covariance and scatter matrix estimation under huber‚Äôs contamination model",
      "abstract": "",
      "year": "2018",
      "venue": "The Annals of Statistics",
      "authors": "Mengjie Chen, Chao Gao, Zhao Ren, et al."
    },
    {
      "index": 27,
      "title": "BadNL: Backdoor attacks against nlp models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.01043",
      "authors": "Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang"
    },
    {
      "index": 28,
      "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.05526",
      "authors": "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song"
    },
    {
      "index": 29,
      "title": "REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.07205",
      "authors": "Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, and Dawn Song",
      "orig_title": "Refit: a unified watermark removal framework for deep learning systems with limited data",
      "paper_id": "1911.07205v3"
    },
    {
      "index": 30,
      "title": "Distributed statistical machine learning in adversarial settings: Byzantine gradient descent",
      "abstract": "",
      "year": "2017",
      "venue": "ACM on Measurement and Analysis of Computing Systems",
      "authors": "Yudong Chen, Lili Su, and Jiaming Xu"
    },
    {
      "index": 31,
      "title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "OpenReview",
      "authors": "Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John P Dickerson, Gavin Taylor, and Tom Goldstein",
      "orig_title": "Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition",
      "paper_id": "2101.07922v2"
    },
    {
      "index": 32,
      "title": "Sentinet: Detecting localized universal attack against deep learning systems",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE SPW 2020",
      "authors": "Edward Chou, Florian Tramer, and Giancarlo Pellegrino"
    },
    {
      "index": 33,
      "title": "Emnist: Extending mnist to handwritten letters",
      "abstract": "",
      "year": "2017",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "authors": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik"
    },
    {
      "index": 34,
      "title": "Certified Adversarial Robustness via Randomized Smoothing",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Jeremy Cohen, Elan Rosenfeld, and Zico Kolter",
      "orig_title": "Certified adversarial robustness via randomized smoothing",
      "paper_id": "1902.02918v2"
    },
    {
      "index": 35,
      "title": "A backdoor attack against lstm-based text classification systems",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access",
      "authors": "Jiazhu Dai, Chuanshuai Chen, and Yufeng Li"
    },
    {
      "index": 36,
      "title": "Recent advances in algorithmic high-dimensional robust statistics",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.05911",
      "authors": "Ilias Diakonikolas and Daniel M Kane"
    },
    {
      "index": 37,
      "title": "Being robust (in high dimensions) can be practical",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart"
    },
    {
      "index": 38,
      "title": "Robust estimators in high-dimensions without the computational intractability",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Computing",
      "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart"
    },
    {
      "index": 39,
      "title": "Sever: A Robust Meta-Algorithm for Stochastic Optimization",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart",
      "orig_title": "Sever: A robust meta-algorithm for stochastic optimization",
      "paper_id": "1803.02815v2"
    },
    {
      "index": 40,
      "title": "Trojan attack on deep generative models in autonomous driving",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Security and Privacy in Communication Systems",
      "authors": "Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong"
    },
    {
      "index": 41,
      "title": "The automatic robustness of minimum distance functionals",
      "abstract": "",
      "year": "1988",
      "venue": "The Annals of Statistics",
      "authors": "David L Donoho and Richard C Liu"
    },
    {
      "index": 42,
      "title": "Calibrating noise to sensitivity in private data analysis",
      "abstract": "",
      "year": "2006",
      "venue": "Theory of cryptography conference",
      "authors": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith"
    },
    {
      "index": 43,
      "title": "FoggySight: A Scheme for Facial Lookup Privacy",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.08588",
      "authors": "Ivan Evtimov, Pascal Sturmfels, and Tadayoshi Kohno",
      "orig_title": "Foggysight: A scheme for facial lookup privacy",
      "paper_id": "2012.08588v1"
    },
    {
      "index": 44,
      "title": "Robust physical-world attacks on deep learning visual classification",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "authors": "Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song"
    },
    {
      "index": 45,
      "title": "Poisoning Attacks to Graph-Based Recommender Systems",
      "abstract": "",
      "year": "2018",
      "venue": "Annual Computer Security Applications Conference",
      "authors": "Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu",
      "orig_title": "Poisoning attacks to graph-based recommender systems",
      "paper_id": "1809.04127v1"
    },
    {
      "index": 46,
      "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
      "abstract": "",
      "year": "2020",
      "venue": "USENIX Security Symposium",
      "authors": "Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong",
      "orig_title": "Local model poisoning attacks to byzantine-robust federated learning",
      "paper_id": "1911.11815v4"
    },
    {
      "index": 47,
      "title": "Influence Function based Data Poisoning Attacks to Top-ùëÅ Recommender Systems",
      "abstract": "",
      "year": "2020",
      "venue": "The Web Conference 2020",
      "authors": "Minghong Fang, Neil Zhenqiang Gong, and Jia Liu",
      "orig_title": "Influence function based data poisoning attacks to top-n recommender systems",
      "paper_id": "2002.08025v3"
    },
    {
      "index": 48,
      "title": "Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ji Feng, Qi-Zhi Cai, and Zhi-Hua Zhou",
      "orig_title": "Learning to confuse: Generating training time adversarial data with auto-encoder",
      "paper_id": "1905.09027v1"
    },
    {
      "index": 49,
      "title": "Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.02683",
      "authors": "Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek Czaja, and Tom Goldstein",
      "orig_title": "Preventing unauthorized use of proprietary data: Poisoning for secure dataset release",
      "paper_id": "2103.02683v2"
    },
    {
      "index": 50,
      "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
      "abstract": "",
      "year": "2015",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security",
      "authors": "Matt Fredrikson, Somesh Jha, and Thomas Ristenpart"
    },
    {
      "index": 51,
      "title": "Attack-Resistant Federated Learning with Residual-based Reweighting",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shuhao Fu, Chulin Xie, Bo¬†Li, and Qifeng Chen.",
      "orig_title": "Attack-resistant federated learning with residual-based reweighting",
      "paper_id": "1912.11464v3"
    },
    {
      "index": 52,
      "title": "Mitigating Sybils in Federated Learning Poisoning",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Clement Fung, Chris¬†JM Yoon, and Ivan Beschastnikh.",
      "orig_title": "Mitigating sybils in federated learning poisoning",
      "paper_id": "1808.04866v5"
    },
    {
      "index": 53,
      "title": "Robust estimation and generative adversarial nets",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chao Gao, Jiyi Liu, Yuan Yao, and Weizhi Zhu."
    },
    {
      "index": 54,
      "title": "Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective",
      "abstract": "",
      "year": "",
      "venue": "Journal of Machine Learning Research (JMLR)",
      "authors": "Chao Gao, Yuan Yao, and Weizhi Zhu.",
      "orig_title": "Generative adversarial nets for robust scatter estimation: A proper scoring rule perspective",
      "paper_id": "1903.01944v1"
    },
    {
      "index": 55,
      "title": "Face-off: Adversarial face obfuscation",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chuhan Gao, Varun Chandrasekaran, Kassem Fawaz, and Somesh Jha."
    },
    {
      "index": 56,
      "title": "Strip: A defence against trojan attacks on deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 35th Annual Computer Security\nApplications Conference",
      "authors": "Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith¬†C Ranasinghe, and\nSurya Nepal."
    },
    {
      "index": 57,
      "title": "Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review",
      "abstract": "",
      "year": "2007",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yansong Gao, Bao¬†Gia Doan, Zhi Zhang, Siqi Ma, Anmin Fu, Surya Nepal, and\nHyoungshick Kim.",
      "orig_title": "Backdoor attacks and countermeasures on deep learning: a comprehensive review",
      "paper_id": "2007.10760v3"
    },
    {
      "index": 58,
      "title": "Inverting Gradients - How easy is it to break privacy in federated learning?",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonas Geiping, Hartmut Bauermeister, Hannah Dr√∂ge, and Michael Moeller.",
      "orig_title": "Inverting gradients‚ÄìHow easy is it to break privacy in federated learning?",
      "paper_id": "2003.14053v2"
    },
    {
      "index": 59,
      "title": "Witches‚Äô Brew: Industrial Scale Data Poisoning via Gradient Matching",
      "abstract": "",
      "year": "2009",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonas Geiping, Liam Fowl, W¬†Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael\nMoeller, and Tom Goldstein.",
      "orig_title": "Witches‚Äô brew: Industrial scale data poisoning via gradient matching",
      "paper_id": "2009.02276v2"
    },
    {
      "index": 60,
      "title": "What doesn‚Äôt kill you makes you robust (er): Adversarial training against poisons and backdoors",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller,\nand Tom Goldstein."
    },
    {
      "index": 61,
      "title": "MaxUp: A Simple Way to Improve Generalization of Neural Network Training",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu.",
      "orig_title": "Maxup: A simple way to improve generalization of neural network training",
      "paper_id": "2002.09024v1"
    },
    {
      "index": 62,
      "title": "Generative adversarial nets",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio."
    },
    {
      "index": 63,
      "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg."
    },
    {
      "index": 64,
      "title": "Practical poisoning attacks on neural networks",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the European Conference on Computer Vision",
      "authors": "Junfeng Guo and Cong Liu."
    },
    {
      "index": 65,
      "title": "TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song.",
      "orig_title": "Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems",
      "paper_id": "1908.01763v2"
    },
    {
      "index": 66,
      "title": "Robust statistics: the approach based on influence functions, volume 196",
      "abstract": "",
      "year": "2011",
      "venue": "John Wiley & Sons",
      "authors": "Frank¬†R Hampel, Elvezio¬†M Ronchetti, Peter¬†J Rousseeuw, and Werner¬†A Stahel."
    },
    {
      "index": 67,
      "title": "On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Sanghyun Hong, Varun Chandrasekaran, Yiƒüitcan Kaya, Tudor Dumitra≈ü,\nand Nicolas Papernot.",
      "orig_title": "On the effectiveness of mitigating data poisoning attacks with gradient shaping",
      "paper_id": "2002.11497v2"
    },
    {
      "index": 68,
      "title": "Targeted poisoning attacks on social recommender systems",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Rui Hu, Yuanxiong Guo, Miao Pan, and Yanmin Gong."
    },
    {
      "index": 69,
      "title": "Unlearnable Examples: Making Personal Data Unexploitable",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "Hanxun Huang, Xingjun Ma, Sarah¬†Monazam Erfani, James Bailey, and Yisen Wang.",
      "orig_title": "Unlearnable examples: Making personal data unexploitable",
      "paper_id": "2101.04898v2"
    },
    {
      "index": 70,
      "title": "One-Pixel Signature: Characterizing CNN Models for Backdoor Detection",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the European Conference on Computer Vision\n(ECCV)",
      "authors": "Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and Zhuowen Tu.",
      "orig_title": "One-pixel signature: Characterizing cnn models for backdoor detection",
      "paper_id": "2008.07711v1"
    },
    {
      "index": 71,
      "title": "MetaPoison: Practical General-purpose Clean-label Data Poisoning",
      "abstract": "",
      "year": "2004",
      "venue": "arXiv preprint arXiv:",
      "authors": "W¬†Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.",
      "orig_title": "Metapoison: Practical general-purpose clean-label data poisoning",
      "paper_id": "2004.00225v2"
    },
    {
      "index": 72,
      "title": "Robust estimation of a location parameter",
      "abstract": "",
      "year": "1964",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "Peter¬†J Huber."
    },
    {
      "index": 73,
      "title": "Robust statistics, volume 523",
      "abstract": "",
      "year": "2004",
      "venue": "John Wiley & Sons",
      "authors": "Peter¬†J Huber."
    },
    {
      "index": 74,
      "title": "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina\nNita-Rotaru, and Bo¬†Li."
    },
    {
      "index": 75,
      "title": "Subpopulation Data Poisoning Attacks",
      "abstract": "",
      "year": "2006",
      "venue": "arXiv preprint arXiv:",
      "authors": "Matthew Jagielski, Giorgio Severi, Niklas¬†Pousette Harger, and Alina Oprea.",
      "orig_title": "Subpopulation data poisoning attacks",
      "paper_id": "2006.14026v3"
    },
    {
      "index": 76,
      "title": "Auditing differentially private machine learning: How private is private sgd?",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems, 33",
      "authors": "Matthew Jagielski, Jonathan Ullman, and Alina Oprea."
    },
    {
      "index": 77,
      "title": "Model-reuse attacks on deep learning systems",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM SIGSAC Conference on Computer\nand Communications Security",
      "authors": "Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang."
    },
    {
      "index": 78,
      "title": "Certified robustness of nearest neighbors against data poisoning attacks",
      "abstract": "",
      "year": "2012",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jinyuan Jia, Xiaoyu Cao, and Neil¬†Zhenqiang Gong."
    },
    {
      "index": 79,
      "title": "Intrinsic certified robustness of bagging against data poisoning attacks",
      "abstract": "",
      "year": "2008",
      "venue": "arXiv preprint arXiv:",
      "authors": "Jinyuan Jia, Xiaoyu Cao, and Neil¬†Zhenqiang Gong."
    },
    {
      "index": 80,
      "title": "Learning in the presence of malicious errors",
      "abstract": "",
      "year": "1993",
      "venue": "SIAM Journal on Computing, 22(4):807‚Äì837",
      "authors": "Michael Kearns and Ming Li."
    },
    {
      "index": 81,
      "title": "Auto-encoding variational bayes",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Diederik¬†P Kingma and Max Welling."
    },
    {
      "index": 82,
      "title": "Trojdrl: Evaluation of backdoor attacks on deep reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li."
    },
    {
      "index": 83,
      "title": "Overcoming catastrophic forgetting in neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences, 114(13):",
      "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei¬†A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, et¬†al.",
      "orig_title": "Overcoming catastrophic forgetting in neural networks",
      "paper_id": "1612.00796v2"
    },
    {
      "index": 84,
      "title": "Learning halfspaces with malicious noise",
      "abstract": "",
      "year": "2009",
      "venue": "Journal of Machine Learning Research, 10(12)",
      "authors": "Adam¬†R Klivans, Philip¬†M Long, and Rocco¬†A Servedio."
    },
    {
      "index": 85,
      "title": "Understanding Black-box Predictions via Influence Functions",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Pang¬†Wei Koh and Percy Liang.",
      "orig_title": "Understanding black-box predictions via influence functions",
      "paper_id": "1703.04730v3"
    },
    {
      "index": 86,
      "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Pang¬†Wei Koh, Jacob Steinhardt, and Percy Liang.",
      "orig_title": "Stronger data poisoning attacks break data sanitization defenses",
      "paper_id": "1811.00741v2"
    },
    {
      "index": 87,
      "title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition",
      "authors": "Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann.",
      "orig_title": "Universal litmus patterns: Revealing backdoor attacks in cnns",
      "paper_id": "1906.10842v2"
    },
    {
      "index": 88,
      "title": "Learning multiple layers of features from tiny images",
      "abstract": "",
      "year": "2009",
      "venue": "Technical report",
      "authors": "Alex Krizhevsky."
    },
    {
      "index": 89,
      "title": "Adversarial machine learning‚Äìindustry perspectives",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ram Shankar¬†Siva Kumar, Magnus Nystr√∂m, John Lambert, Andrew Marshall,\nMario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia."
    },
    {
      "index": 90,
      "title": "Agnostic Estimation of Mean and Covariance",
      "abstract": "",
      "year": "2016",
      "venue": "2016 IEEE 57th Annual Symposium on Foundations of Computer\nScience (FOCS)",
      "authors": "Kevin¬†A Lai, Anup¬†B Rao, and Santosh Vempala.",
      "orig_title": "Agnostic estimation of mean and covariance",
      "paper_id": "1604.06968v2"
    },
    {
      "index": 91,
      "title": "Tiny imagenet visual recognition challenge",
      "abstract": "",
      "year": "2015",
      "venue": "CS 231N, 7",
      "authors": "Ya¬†Le and Xuan Yang."
    },
    {
      "index": 92,
      "title": "Certified Robustness to Adversarial Examples with Differential Privacy",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman\nJana.",
      "orig_title": "Certified robustness to adversarial examples with differential privacy",
      "paper_id": "1802.03471v4"
    },
    {
      "index": 93,
      "title": "Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander Levine and Soheil Feizi.",
      "orig_title": "Deep partition aggregation: Provable defense against general poisoning attacks",
      "paper_id": "2006.14768v2"
    },
    {
      "index": 94,
      "title": "Data poisoning attacks on factorization-based collaborative filtering, 2016",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Bo¬†Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik."
    },
    {
      "index": 95,
      "title": "Principled approaches to robust machine learning and beyond",
      "abstract": "",
      "year": "2018",
      "venue": "PhD thesis, Massachusetts Institute of Technology",
      "authors": "Jerry¬†Zheng Li."
    },
    {
      "index": 96,
      "title": "RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume¬†33",
      "authors": "Liping Li, Wei Xu, Tianyi Chen, Georgios¬†B Giannakis, and Qing Ling.",
      "orig_title": "Rsa: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets",
      "paper_id": "1811.03761v2"
    },
    {
      "index": 97,
      "title": "Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization",
      "abstract": "",
      "year": "",
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": "Shaofeng Li, Minhui Xue, Benjamin Zhao, Haojin Zhu, and Xinpeng Zhang.",
      "orig_title": "Invisible backdoor attacks on deep neural networks via steganography and regularization",
      "paper_id": "1909.02742v3"
    },
    {
      "index": 98,
      "title": "Learning to Detect Malicious Clients for Robust Federated Learning",
      "abstract": "",
      "year": "2002",
      "venue": "arXiv preprint arXiv:",
      "authors": "Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen.",
      "orig_title": "Learning to detect malicious clients for robust federated learning",
      "paper_id": "2002.00211v1"
    },
    {
      "index": 99,
      "title": "Backdoor Learning: A Survey",
      "abstract": "",
      "year": "2007",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia.",
      "orig_title": "Backdoor learning: A survey",
      "paper_id": "2007.08745v5"
    },
    {
      "index": 100,
      "title": "Data poisoning attacks on stochastic bandits, 2019",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Fang Liu and Ness Shroff."
    },
    {
      "index": 101,
      "title": "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
      "abstract": "",
      "year": "",
      "venue": "International Symposium on Research in Attacks, Intrusions,\nand Defenses",
      "authors": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.",
      "orig_title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
      "paper_id": "1805.12185v1"
    },
    {
      "index": 102,
      "title": "Removing Backdoor-Based Watermarks in Neural Networks with Limited Data",
      "abstract": "",
      "year": "2008",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xuankai Liu, Fengting Li, Bihan Wen, and Qi¬†Li.",
      "orig_title": "Removing backdoor-based watermarks in neural networks with limited data",
      "paper_id": "2008.00407v2"
    },
    {
      "index": 103,
      "title": "Backdoor attacks and defenses in feature-partitioned collaborative learning",
      "abstract": "",
      "year": "2007",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yang Liu, Zhihao Yi, and Tianjian Chen."
    },
    {
      "index": 104,
      "title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Learning Representations",
      "authors": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.",
      "orig_title": "Delving into transferable adversarial examples and black-box attacks",
      "paper_id": "1611.02770v3"
    },
    {
      "index": 105,
      "title": "Trojaning attack on neural networks",
      "abstract": "",
      "year": "",
      "venue": "NDSS",
      "authors": "Yingqi Liu, Shiqing Ma, Yousra Aafer, W.¬†Lee, Juan Zhai, Weihang Wang, and\nX.¬†Zhang."
    },
    {
      "index": 106,
      "title": "Neural Trojans",
      "abstract": "",
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Design\n(ICCD)",
      "authors": "Yuntao Liu, Yang Xie, and Ankur Srivastava.",
      "orig_title": "Neural trojans",
      "paper_id": "1710.00942v1"
    },
    {
      "index": 107,
      "title": "Biometric backdoors: A poisoning attack against unsupervised template updating",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Giulio Lovisotto, Simon Eberz, and Ivan Martinovic."
    },
    {
      "index": 108,
      "title": "Threats to Federated Learning: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lingjuan Lyu, Han Yu, and Qiang Yang.",
      "orig_title": "Threats to federated learning: A survey",
      "paper_id": "2003.02133v1"
    },
    {
      "index": 109,
      "title": "Nic: Detecting adversarial samples with neural network invariant checking",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 26th Network and Distributed System\nSecurity Symposium (NDSS",
      "authors": "Shiqing Ma and Yingqi Liu."
    },
    {
      "index": 110,
      "title": "Data Poisoning Attacks in Contextual Bandits",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Decision and Game Theory for\nSecurity",
      "authors": "Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu.",
      "orig_title": "Data poisoning attacks in contextual bandits",
      "paper_id": "1808.05760v2"
    },
    {
      "index": 111,
      "title": "Data Poisoning against Differentially-Private Learners: Attacks and Defenses",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on\nArtificial Intelligence",
      "authors": "Yuzhe Ma, Xiaojin Zhu, and Justin Hsu.",
      "orig_title": "Data poisoning against differentially-private learners: attacks and defenses",
      "paper_id": "1903.09860v2"
    },
    {
      "index": 112,
      "title": "Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners",
      "abstract": "",
      "year": "2017",
      "venue": "Theory of Cryptography Conference, pages 245‚Äì279.\nSpringer",
      "authors": "Saeed Mahloujifar and Mohammad Mahmoody."
    },
    {
      "index": 113,
      "title": "Learning under ùëù-Tampering Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Algorithmic Learning Theory",
      "authors": "Saeed Mahloujifar, Dimitrios¬†I Diochnos, and Mohammad Mahmoody.",
      "orig_title": "Learning under pùëùp-tampering attacks",
      "paper_id": "1711.03707v4"
    },
    {
      "index": 114,
      "title": "The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume¬†33",
      "authors": "Saeed Mahloujifar, Dimitrios¬†I Diochnos, and Mohammad Mahmoody.",
      "orig_title": "The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure",
      "paper_id": "1809.03063v2"
    },
    {
      "index": 115,
      "title": "Data poisoning attacks in multi-party learning",
      "abstract": "",
      "year": "",
      "venue": "International Conference on Machine Learning",
      "authors": "Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed."
    },
    {
      "index": 116,
      "title": "Using machine teaching to identify optimal training-set attacks on machine learners",
      "abstract": "",
      "year": "2015",
      "venue": "AAAI",
      "authors": "Shike Mei and Xiaojin Zhu."
    },
    {
      "index": 117,
      "title": "Spambayes: Effective open-source, bayesian based, email classification system",
      "abstract": "",
      "year": "2004",
      "venue": "CEAS. Citeseer",
      "authors": "Tony¬†A Meyer and Brendon Whateley."
    },
    {
      "index": 118,
      "title": "The hidden vulnerability of distributed learning in byzantium",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "El¬†Mahdi¬†El Mhamdi, Rachid Guerraoui, and S√©bastien Rouault."
    },
    {
      "index": 119,
      "title": "Towards data poisoning attacks in crowd sensing systems",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the Eighteenth ACM International Symposium on\nMobile Ad Hoc Networking and Computing",
      "authors": "Chenglin Miao, Qi¬†Li, Houping Xiao, Wenjun Jiang, Mengdi Huai, and Lu¬†Su."
    },
    {
      "index": 120,
      "title": "Universal adversarial perturbations",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and\npattern recognition",
      "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\nFrossard.",
      "orig_title": "Universal adversarial perturbations",
      "paper_id": "1610.08401v3"
    },
    {
      "index": 121,
      "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 10th ACM Workshop on Artificial\nIntelligence and Security",
      "authors": "Luis Mu√±oz-Gonz√°lez, Battista Biggio, Ambra Demontis, Andrea Paudice,\nVasin Wongrassamee, Emil¬†C Lupu, and Fabio Roli.",
      "orig_title": "Towards poisoning of deep learning algorithms with back-gradient optimization",
      "paper_id": "1708.08689v1"
    },
    {
      "index": 122,
      "title": "Poisoning Attacks with Generative Adversarial Nets",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Luis Mu√±oz-Gonz√°lez, Bjarne Pfitzner, Matteo Russo, Javier\nCarnerero-Cano, and Emil¬†C Lupu.",
      "orig_title": "Poisoning attacks with generative adversarial nets",
      "paper_id": "1906.07773v2"
    },
    {
      "index": 123,
      "title": "Exploiting machine learning to subvert your spam filter",
      "abstract": "",
      "year": "2008",
      "venue": "LEET, 8:1‚Äì9",
      "authors": "Blaine Nelson, Marco Barreno, Fuching¬†Jack Chi, Anthony¬†D Joseph, Benjamin¬†IP\nRubinstein, Udam Saini, Charles¬†A Sutton, J¬†Doug Tygar, and Kai Xia."
    },
    {
      "index": 124,
      "title": "Sok: Security and privacy in machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE European Symposium on Security and Privacy\n(EuroS&P)",
      "authors": "Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael¬†P Wellman."
    },
    {
      "index": 125,
      "title": "Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection",
      "abstract": "",
      "year": "",
      "venue": "arXiv preprint arXiv:",
      "authors": "Andrea Paudice, Luis Mu√±oz-Gonz√°lez, Andras Gyorgy, and Emil¬†C Lupu.",
      "orig_title": "Detection of adversarial training examples in poisoning attacks through anomaly detection",
      "paper_id": "1802.03041v1"
    },
    {
      "index": 126,
      "title": "Label Sanitization against Label Flipping Poisoning Attacks",
      "abstract": "",
      "year": "",
      "venue": "Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases",
      "authors": "Andrea Paudice, Luis Mu√±oz-Gonz√°lez, and Emil¬†C Lupu.",
      "orig_title": "Label sanitization against label flipping poisoning attacks",
      "paper_id": "1803.00992v2"
    },
    {
      "index": 127,
      "title": "Deep k-NN Defense Against Clean-label Data Poisoning Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Neehar Peri, Neal Gupta, W.¬†Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom\nGoldstein, and John¬†P. Dickerson.",
      "orig_title": "Deep k-nn defense against clean-label data poisoning attacks",
      "paper_id": "1909.13374v3"
    },
    {
      "index": 128,
      "title": "Robust aggregation for federated learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Krishna Pillutla, Sham¬†M Kakade, and Zaid Harchaoui."
    },
    {
      "index": 129,
      "title": "A taxonomy and survey of attacks against machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "Computer Science Review, 34:",
      "authors": "Nikolaos Pitropakis, Emmanouil Panaousis, Thanassis Giannetsos, Eleftherios\nAnastasiadis, and George Loukas."
    },
    {
      "index": 130,
      "title": "Robust estimation via robust gradient estimation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Adarsh Prasad, Arun¬†Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar."
    },
    {
      "index": 131,
      "title": "Defending Neural Backdoors via Generative Distribution Modeling",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ximing Qiao, Yukun Yang, and Hai Li.",
      "orig_title": "Defending neural backdoors via generative distribution modeling",
      "paper_id": "1910.04749v2"
    },
    {
      "index": 132,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog, 1(8):9",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever."
    },
    {
      "index": 133,
      "title": "Backdoors in Neural Models of Source Code",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Goutham Ramakrishnan and Aws Albarghouthi.",
      "orig_title": "Backdoors in neural models of source code",
      "paper_id": "2006.06841v1"
    },
    {
      "index": 134,
      "title": "Certified Robustness to Label-Flipping Attacks via Randomized Smoothing",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and J¬†Zico Kolter.",
      "orig_title": "Certified robustness to label-flipping attacks via randomized smoothing",
      "paper_id": "2002.03018v4"
    },
    {
      "index": 135,
      "title": "Deepsigns: A generic watermarking framework for ip protection of deep learning models",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Bita¬†Darvish Rouhani, Huili Chen, and Farinaz Koushanfar."
    },
    {
      "index": 136,
      "title": "Hidden trigger backdoor attacks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash."
    },
    {
      "index": 137,
      "title": "BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang.",
      "orig_title": "Baaan: Backdoor attacks against autoencoder and gan-based machine learning models",
      "paper_id": "2010.03007v2"
    },
    {
      "index": 138,
      "title": "FaceHack: Triggering backdoored facial recognition systems using facial characteristics",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Esha Sarkar, Hadjer Benkraouda, and Michail Maniatakos.",
      "orig_title": "Facehack: Triggering backdoored facial recognition systems using facial characteristics",
      "paper_id": "2006.11623v1"
    },
    {
      "index": 139,
      "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion*",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov.",
      "orig_title": "You autocomplete me: Poisoning vulnerabilities in neural code completion",
      "paper_id": "2007.02220v3"
    },
    {
      "index": 140,
      "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John¬†P Dickerson, and Tom\nGoldstein.",
      "orig_title": "Just how toxic is data poisoning? A unified benchmark for backdoor and data poisoning attacks",
      "paper_id": "2006.12557v3"
    },
    {
      "index": 141,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer\nvision",
      "authors": "Ramprasaath¬†R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,\nDevi Parikh, and Dhruv Batra.",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 142,
      "title": "Exploring backdoor poisoning attacks against malware classifiers",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea."
    },
    {
      "index": 143,
      "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ali Shafahi, W¬†Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,\nTudor Dumitras, and Tom Goldstein.",
      "orig_title": "Poison frogs! Targeted clean-label poisoning attacks on neural networks",
      "paper_id": "1804.00792v2"
    },
    {
      "index": 144,
      "title": "Fawkes: Protecting personal privacy against unauthorized deep learning models",
      "abstract": "",
      "year": "2020",
      "venue": "USENIX Security Symposium",
      "authors": "Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, and Ben¬†Y\nZhao."
    },
    {
      "index": 145,
      "title": "Tensorclog: An imperceptible poisoning attack on deep neural network applications",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Access, 7:",
      "authors": "Juncheng Shen, Xiaolei Zhu, and De¬†Ma."
    },
    {
      "index": 146,
      "title": "Robust regression using repeated medians",
      "abstract": "",
      "year": "1982",
      "venue": "Biometrika, 69(1):242‚Äì244",
      "authors": "Andrew¬†F Siegel."
    },
    {
      "index": 147,
      "title": "Poisoning Attacks on Algorithmic Fairness",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "David Solans, Battista Biggio, and Carlos Castillo.",
      "orig_title": "Poisoning attacks on algorithmic fairness",
      "paper_id": "2004.07401v3"
    },
    {
      "index": 148,
      "title": "Robust learning: Information theory and algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "PhD thesis, Stanford University",
      "authors": "Jacob Steinhardt."
    },
    {
      "index": 149,
      "title": "Certified defenses for data poisoning attacks",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Jacob Steinhardt, Pang Wei¬†W Koh, and Percy¬†S Liang."
    },
    {
      "index": 150,
      "title": "Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers",
      "abstract": "",
      "year": "2018",
      "venue": "Innovations in Theoretical Computer Science Conference\n(ITCS)",
      "authors": "Jacob Steinhardt, Moses Charikar, and Gregory Valiant.",
      "orig_title": "Resilience: A criterion for learning in the presence of arbitrary outliers",
      "paper_id": "1703.04940v3"
    },
    {
      "index": 151,
      "title": "Data Poisoning Attacks on Federated Machine Learning",
      "abstract": "",
      "year": "2004",
      "venue": "arXiv preprint arXiv:",
      "authors": "Gan Sun, Yang Cong, Jiahua Dong, Qiang Wang, and Ji¬†Liu.",
      "orig_title": "Data poisoning attacks on federated machine learning",
      "paper_id": "2004.10020v1"
    },
    {
      "index": 152,
      "title": "Natural backdoor attack on text data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Lichao Sun."
    },
    {
      "index": 153,
      "title": "Poisoned classifiers are not only backdoored, they are fundamentally broken",
      "abstract": "",
      "year": "2010",
      "venue": "arXiv preprint arXiv:",
      "authors": "Mingjie Sun, Siddhant Agarwal, and J¬†Zico Kolter.",
      "orig_title": "Poisoned classifiers are not only backdoored, they are fundamentally broken",
      "paper_id": "2010.09080v2"
    },
    {
      "index": 154,
      "title": "Can you really backdoor federated learning?",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Ziteng Sun, Peter Kairouz, Ananda¬†Theertha Suresh, and H¬†Brendan McMahan."
    },
    {
      "index": 155,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:",
      "authors": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus."
    },
    {
      "index": 156,
      "title": "Bypassing Backdoor Detection Algorithms in Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Te¬†Juin¬†Lester Tan and Reza Shokri.",
      "orig_title": "Bypassing backdoor detection algorithms in deep learning",
      "paper_id": "1905.13409v2"
    },
    {
      "index": 157,
      "title": "An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining",
      "authors": "Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu.",
      "orig_title": "An embarrassingly simple approach for trojan attack in deep neural networks",
      "paper_id": "2006.08131v2"
    },
    {
      "index": 158,
      "title": "Data Poisoning Attacks Against Federated Learning Systems",
      "abstract": "",
      "year": "2020",
      "venue": "European Symposium on Research in Computer Security",
      "authors": "Vale Tolpegin, Stacey Truex, Mehmet¬†Emre Gursoy, and Ling Liu.",
      "orig_title": "Data poisoning attacks against federated learning systems",
      "paper_id": "2007.08432v2"
    },
    {
      "index": 159,
      "title": "The space of transferable adversarial examples",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Florian Tram√®r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick\nMcDaniel."
    },
    {
      "index": 160,
      "title": "Spectral Signatures in Backdoor Attacks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Brandon Tran, Jerry Li, and Aleksander Madry.",
      "orig_title": "Spectral signatures in backdoor attacks",
      "paper_id": "1811.00636v1"
    },
    {
      "index": 161,
      "title": "A survey of sampling from contaminated distributions",
      "abstract": "",
      "year": "1960",
      "venue": "Contributions to probability and statistics",
      "authors": "John¬†W Tukey."
    },
    {
      "index": 162,
      "title": "Label-Consistent Backdoor Attacks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Alexander Turner, Dimitris Tsipras, and Aleksander Madry.",
      "orig_title": "Label-consistent backdoor attacks",
      "paper_id": "1912.02771v2"
    },
    {
      "index": 163,
      "title": "Embedding watermarks into deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on International Conference on\nMultimedia Retrieval",
      "authors": "Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin‚Äôichi Satoh."
    },
    {
      "index": 164,
      "title": "Learning disjunction of conjunctions",
      "abstract": "",
      "year": "1985",
      "venue": "IJCAI",
      "authors": "Leslie¬†G Valiant."
    },
    {
      "index": 165,
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": "",
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine\nlearning",
      "authors": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol."
    },
    {
      "index": 166,
      "title": "Data poisoning attacks against differentially private recommender systems",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval, pages 1617‚Äì1620",
      "authors": "Soumya Wadhwa, Saurabh Agrawal, Harsh Chaudhari, Deepthi Sharma, and Kannan\nAchan."
    },
    {
      "index": 167,
      "title": "Microsoft chatbot is taught to swear on Twitter",
      "abstract": "",
      "year": "2016",
      "venue": "BBC News",
      "authors": "Jane Wakefield."
    },
    {
      "index": 168,
      "title": "Customizing triggers with concealed data poisoning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Eric Wallace, Tony¬†Z Zhao, Shi Feng, and Sameer Singh."
    },
    {
      "index": 169,
      "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao\nZheng, and Ben¬†Y Zhao."
    },
    {
      "index": 170,
      "title": "Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases",
      "abstract": "",
      "year": "",
      "venue": "Proceedings of the European Conference on Computer Vision\n(ECCV)",
      "authors": "Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang.",
      "orig_title": "Practical detection of trojan neural networks: Data-limited and data-free cases",
      "paper_id": "2007.15802v1"
    },
    {
      "index": 171,
      "title": "Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models",
      "abstract": "",
      "year": "2001",
      "venue": "arXiv preprint arXiv:",
      "authors": "Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, and\nTianle Chen.",
      "orig_title": "Backdoor attacks against transfer learning with pre-trained deep learning models",
      "paper_id": "2001.03274v2"
    },
    {
      "index": 172,
      "title": "Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-based Traffic Congestion Control Systems",
      "abstract": "",
      "year": "2003",
      "venue": "arXiv preprint arXiv:",
      "authors": "Yue Wang, Esha Sarkar, Michail Maniatakos, and Saif¬†Eddin Jabari.",
      "orig_title": "Stop-and-go: Exploring backdoor attacks on deep reinforcement learning-based traffic congestion control systems",
      "paper_id": "2003.07859v4"
    },
    {
      "index": 173,
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce¬†Zhang, and Bo¬†Li.",
      "orig_title": "Rab: Provable robustness against backdoor attacks",
      "paper_id": "2003.08904v8"
    },
    {
      "index": 174,
      "title": "Backdoor attacks on facial recognition in the physical world",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Emily Wenger, Josephine Passananti, Yuanshun Yao, Haitao Zheng, and Ben¬†Y Zhao."
    },
    {
      "index": 175,
      "title": "Mitigating Backdoor Attacks in Federated Learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chen Wu, Xian Yang, Sencun Zhu, and Prasenjit Mitra.",
      "orig_title": "Mitigating backdoor attacks in federated learning",
      "paper_id": "2011.01767v2"
    },
    {
      "index": 176,
      "title": "Graph Backdoor",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang.",
      "orig_title": "Graph backdoor",
      "paper_id": "2006.11890v5"
    },
    {
      "index": 177,
      "title": "Is Feature Selection Secure against Training Data Poisoning?",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and\nFabio Roli.",
      "orig_title": "Is feature selection secure against training data poisoning?",
      "paper_id": "1804.07933v1"
    },
    {
      "index": 178,
      "title": "DBA: Distributed backdoor attacks against federated learning",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo¬†Li."
    },
    {
      "index": 179,
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "abstract": "",
      "year": "2021",
      "venue": "Proceedings of the IEEE Symposium on Security and Privacy\n(May",
      "authors": "Xiaojun Xu, Qi¬†Wang, Huichen Li, Nikita Borisov, Carl¬†A Gunter, and Bo¬†Li.",
      "orig_title": "Detecting ai trojans using meta neural analysis",
      "paper_id": "1910.03137v4"
    },
    {
      "index": 180,
      "title": "Generative poisoning attack method against neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen."
    },
    {
      "index": 181,
      "title": "Design of Intentional Backdoors in Sequential Models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zhaoyuan Yang, Naresh Iyer, Johan Reimann, and Nurali Virani.",
      "orig_title": "Design of intentional backdoors in sequential models",
      "paper_id": "1902.09972v1"
    },
    {
      "index": 182,
      "title": "Latent backdoor attacks on deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the 2019 ACM SIGSAC Conference on Computer\nand Communications Security",
      "authors": "Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben¬†Y Zhao."
    },
    {
      "index": 183,
      "title": "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:",
      "authors": "Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.",
      "orig_title": "Byzantine-robust distributed learning: Towards optimal statistical rates",
      "paper_id": "1803.01498v2"
    },
    {
      "index": 184,
      "title": "Adversarial examples: Attacks and defenses for deep learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE transactions on neural networks and learning systems,\n30(9):",
      "authors": "Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li."
    },
    {
      "index": 185,
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "abstract": "",
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer\nVision",
      "authors": "Sangdoo Yun, Dongyoon Han, Seong¬†Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo."
    },
    {
      "index": 186,
      "title": "mixup: Beyond Empirical Risk Minimization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:",
      "authors": "Hongyi Zhang, Moustapha Cisse, Yann¬†N Dauphin, and David Lopez-Paz.",
      "orig_title": "mixup: Beyond empirical risk minimization",
      "paper_id": "1710.09412v2"
    },
    {
      "index": 187,
      "title": "Poisoning attack in federated learning using generative adversarial nets",
      "abstract": "",
      "year": "2019",
      "venue": "2019 18th IEEE International Conference on Trust, Security\nand Privacy in Computing and Communications/13th IEEE International\nConference on Big Data Science and Engineering (TrustCom/BigDataSE)",
      "authors": "Jiale Zhang, Junjun Chen, Di¬†Wu, Bing Chen, and Shui Yu."
    },
    {
      "index": 188,
      "title": "Protecting intellectual property of deep neural networks with watermarking",
      "abstract": "",
      "year": "2018",
      "venue": "Proceedings of the 2018 on Asia Conference on Computer and\nCommunications Security",
      "authors": "Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc¬†Ph Stoecklin, Heqing\nHuang, and Ian Molloy."
    },
    {
      "index": 189,
      "title": "A game-theoretic analysis of label flipping attacks on distributed support vector machines",
      "abstract": "",
      "year": "2017",
      "venue": "2017 51st Annual Conference on Information Sciences and\nSystems (CISS)",
      "authors": "Rui Zhang and Quanyan Zhu."
    },
    {
      "index": 190,
      "title": "Trojaning Language Models for Fun and Profit",
      "abstract": "",
      "year": "2008",
      "venue": "arXiv preprint arXiv:",
      "authors": "Xinyang Zhang, Zheng Zhang, and Ting Wang.",
      "orig_title": "Trojaning language models for fun and profit",
      "paper_id": "2008.00312v2"
    },
    {
      "index": 191,
      "title": "Online Data Poisoning Attacks",
      "abstract": "",
      "year": "",
      "venue": "Learning for Dynamics and Control, pages 201‚Äì210",
      "authors": "Xuezhou Zhang, Xiaojin Zhu, and Laurent Lessard.",
      "orig_title": "Online data poisoning attacks",
      "paper_id": "1903.01666v2"
    },
    {
      "index": 192,
      "title": "Backdoor Attacks to Graph Neural Networks",
      "abstract": "",
      "year": "2006",
      "venue": "arXiv preprint arXiv:",
      "authors": "Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil¬†Zhenqiang Gong.",
      "orig_title": "Backdoor attacks to graph neural networks",
      "paper_id": "2006.11165v4"
    },
    {
      "index": 193,
      "title": "Efficient label contamination attacks against black-box learning models",
      "abstract": "",
      "year": "2017",
      "venue": "IJCAI",
      "authors": "Mengchen Zhao, Bo¬†An, Wei Gao, and Teng Zhang."
    },
    {
      "index": 194,
      "title": "Generalized resilience and robust statistics",
      "abstract": "",
      "year": "1909",
      "venue": "arXiv preprint arXiv:",
      "authors": "Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt."
    },
    {
      "index": 195,
      "title": "Transferable Clean-Label Poisoning Attacks on Deep Neural Nets",
      "abstract": "",
      "year": "1905",
      "venue": "arXiv preprint arXiv:",
      "authors": "Chen Zhu, W¬†Ronny Huang, Ali Shafahi, Hengduo Li, Gavin Taylor, Christoph\nStuder, and Tom Goldstein.",
      "orig_title": "Transferable clean-label poisoning attacks on deep neural nets",
      "paper_id": "1905.05897v2"
    },
    {
      "index": 196,
      "title": "Deep Leakage from Gradients",
      "abstract": "",
      "year": "",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Ligeng Zhu, Zhijian Liu, and Song Han.",
      "orig_title": "Deep leakage from gradients",
      "paper_id": "1906.08935v2"
    },
    {
      "index": 197,
      "title": "Gangsweep: Sweep out neural backdoors by gan",
      "abstract": "",
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on\nMultimedia",
      "authors": "Liuwan Zhu, Rui Ning, Cong Wang, Chunsheng Xin, and Hongyi Wu."
    },
    {
      "index": 198,
      "title": "General notions of statistical depth function",
      "abstract": "",
      "year": "2000",
      "venue": "Annals of statistics",
      "authors": "Yijun Zuo and Robert Serfling."
    }
  ]
}