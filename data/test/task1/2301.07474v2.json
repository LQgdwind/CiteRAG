{
  "paper_id": "2301.07474v2",
  "title": "Threats, Vulnerabilities, and Controls of Machine Learning Based Systems: A Survey and Taxonomy",
  "abstract": "Abstract\nIn this article, we propose the Artificial Intelligence Security Taxonomy to systematize the knowledge of threats, vulnerabilities, and security controls of machine-learning-based (ML-based) systems.\nWe first classify the damage caused by attacks against ML-based systems, define ML-specific security, and discuss its characteristics.\nNext, we enumerate all relevant assets and stakeholders and provide a general taxonomy for ML-specific threats.\nThen, we collect a wide range of security controls against ML-specific threats\nthrough an extensive review of recent literature.\nFinally, we classify the vulnerabilities and controls of an ML-based system in terms of each vulnerable asset in the system’s entire lifecycle.",
  "reference_labels": [
    {
      "index": 0,
      "title": "인공지능 기반 공격 그래프 생성",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 1,
      "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "Akhtar, N., Mian, A.S.",
      "orig_title": "Threat of adversarial attacks on deep learning in computer vision: A survey",
      "paper_id": "1801.00553v3"
    },
    {
      "index": 2,
      "title": "Advances in adversarial attacks and defenses in computer vision: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Access",
      "authors": "Akhtar, N., Mian, A., Kardan, N., Shah, M.",
      "orig_title": "Advances in adversarial attacks and defenses in computer vision: A survey",
      "paper_id": "2108.00401v2"
    },
    {
      "index": 3,
      "title": "Adversarial attacks and defenses on AI in medical imaging informatics: A survey",
      "abstract": "",
      "year": "2022",
      "venue": "Expert Syst. Appl.",
      "authors": "Kaviani, S., Han, K.J., Sohn, I."
    },
    {
      "index": 4,
      "title": "Adversarial attacks and defenses in speaker recognition systems: A survey",
      "abstract": "",
      "year": "2022",
      "venue": "J. Syst. Archit.",
      "authors": "Lan, J., Zhang, R., Yan, Z., Wang, J., Chen, Y., Hou, R."
    },
    {
      "index": 5,
      "title": "Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection",
      "abstract": "",
      "year": "2021",
      "venue": "ACM Trans. Priv. Secur.",
      "authors": "Demetrio, L., Coull, S.E., Biggio, B., Lagorio, G., Armando, A., Roli, F.",
      "orig_title": "Adversarial exemples: A survey and experimental evaluation of practical attacks on machine learning for windows malware detection",
      "paper_id": "2008.07125v2"
    },
    {
      "index": 6,
      "title": "Adversarial attacks and defenses on cyber-physical systems: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Internet Things J.",
      "authors": "Li, J., Liu, Y., Chen, T., Xiao, Z., Li, Z., Wang, J."
    },
    {
      "index": 7,
      "title": "Adversarial attacks on deep-learning models in natural language processing: A survey",
      "abstract": "",
      "year": "2020",
      "venue": "ACM Trans. Intell. Syst. Technol.",
      "authors": "Zhang, W.E., Sheng, Q.Z., Alhazmi, A., Li, C."
    },
    {
      "index": 8,
      "title": "Adversarial attack and defense technologies in natural language processing: A survey",
      "abstract": "",
      "year": "2022",
      "venue": "Neurocomputing",
      "authors": "Qiu, S., Liu, Q., Zhou, S., Huang, W."
    },
    {
      "index": 9,
      "title": "Intriguing properties of neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "ICLR’14 Conference Track",
      "authors": "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.J., Fergus, R."
    },
    {
      "index": 10,
      "title": "Towards Security Threats of Deep Learning Systems: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Trans. Software Eng.",
      "authors": "He, Y., Meng, G., Chen, K., Hu, X., He, J.",
      "orig_title": "Towards security threats of deep learning systems: A survey",
      "paper_id": "1911.12562v2"
    },
    {
      "index": 11,
      "title": "Threats to training: A survey of poisoning attacks and defenses on machine learning systems",
      "abstract": "",
      "year": "2022",
      "venue": "ACM Journal of the ACM (JACM)",
      "authors": "Wang, Z., Ma, J., Wang, X., Hu, J., Qin, Z., Ren, K."
    },
    {
      "index": 12,
      "title": "A taxonomy and terminology of adversarial machine learning",
      "abstract": "",
      "year": "2019",
      "venue": "NIST IR",
      "authors": "Tabassi, E., Burns, K.J., Hadjimichael, M., Molina-Markham, A.D., Sexton, J.T."
    },
    {
      "index": 13,
      "title": "Artificial intelligence cybersecurity challenges; threat landscape for artificial intelligence",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "European Union Agency for Cybersecurity (ENISA)"
    },
    {
      "index": 14,
      "title": "Securing machine learning algorithms",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "European Union Agency for Cybersecurity (ENISA)"
    },
    {
      "index": 15,
      "title": "ATLAS - Adversarial Threat Landscape for Artificial-Intelligence Systems.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "The MITRE Corporation"
    },
    {
      "index": 16,
      "title": "Information technology - security techniques ― information security management systems ― overview and vocabulary",
      "abstract": "",
      "year": "2018",
      "venue": "ISO/IEC 27000:2018(E)",
      "authors": "ISO/IEC JTC 1/SC 27"
    },
    {
      "index": 17,
      "title": "Sponge Examples: Energy-Latency Attacks on Neural Networks",
      "abstract": "",
      "year": "2021",
      "venue": "EuroS&P’21",
      "authors": "Shumailov, I., Zhao, Y., Bates, D., Papernot, N., Mullins, R.D., Anderson, R.",
      "orig_title": "Sponge examples: Energy-latency attacks on neural networks",
      "paper_id": "2006.03463v2"
    },
    {
      "index": 18,
      "title": "Energy-Latency Attacks via Sponge Poisoning",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "Cinà, A.E., Demontis, A., Biggio, B., Roli, F., Pelillo, M.",
      "orig_title": "Energy-latency attacks via sponge poisoning",
      "paper_id": "2203.08147v5"
    },
    {
      "index": 19,
      "title": "Property Inference from Poisoning",
      "abstract": "",
      "year": "2022",
      "venue": "SP’22",
      "authors": "Mahloujifar, S., Ghosh, E., Chase, M.",
      "orig_title": "Property inference from poisoning",
      "paper_id": "2101.11073v1"
    },
    {
      "index": 20,
      "title": "Machine learning quality management guideline, 2nd english edition",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "National Institute of Advanced Industrial Science and Technology (AIST)"
    },
    {
      "index": 21,
      "title": "Proposal for a regulation of the European parliament and of the Council laying down harmonised rules on AI (artificial intelligence act) and amending certain union legislative acts",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "European Commission"
    },
    {
      "index": 22,
      "title": "National Security Commission on Artificial Intelligence (NSCAI) final report",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "National Security Commission on Artificial Intelligence"
    },
    {
      "index": 23,
      "title": "Artificial intelligence",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "ISO/IEC JTC 1/SC 42"
    },
    {
      "index": 24,
      "title": "Ai risk management framework: Second draft",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "National Institute of Standards and Technology (NIST)"
    },
    {
      "index": 25,
      "title": "Sok: Security and privacy in machine learning",
      "abstract": "",
      "year": "2018",
      "venue": "EuroS&P’18",
      "authors": "Papernot, N., McDaniel, P.D., Sinha, A., Wellman, M.P."
    },
    {
      "index": 26,
      "title": "A survey on security threats and defensive techniques of machine learning: A data driven view",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "Liu, Q., Li, P., Zhao, W., Cai, W., Yu, S., Leung, V.C.M."
    },
    {
      "index": 27,
      "title": "Cyber security meets artificial intelligence: a survey",
      "abstract": "",
      "year": "2018",
      "venue": "Frontiers Inf. Technol. Electron. Eng.",
      "authors": "Li, J."
    },
    {
      "index": 28,
      "title": "The security of machine learning in an adversarial setting: A survey",
      "abstract": "",
      "year": "2019",
      "venue": "J. Parallel Distributed Comput.",
      "authors": "Wang, X., Li, J., Kuang, X., Tan, Y., Li, J."
    },
    {
      "index": 29,
      "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
      "abstract": "",
      "year": "2020",
      "venue": "Comput. Sci. Rev.",
      "authors": "Huang, X., Kroening, D., Ruan, W., Sharp, J., Sun, Y., Thamo, E., Wu, M., Yi, X."
    },
    {
      "index": 30,
      "title": "Machine learning security: Threats, countermeasures, and evaluations",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Access",
      "authors": "Xue, M., Yuan, C., Wu, H., Zhang, Y., Liu, W."
    },
    {
      "index": 31,
      "title": "A system-driven taxonomy of attacks and defenses in adversarial machine learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": "Sadeghi, K., Banerjee, A., Gupta, S.K.S."
    },
    {
      "index": 32,
      "title": "Privacy and security issues in deep learning: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Access",
      "authors": "Liu, X., Xie, L., Wang, Y., Zou, J., Xiong, J., Ying, Z., Vasilakos, A.V."
    },
    {
      "index": 33,
      "title": "A Survey on Adversarial Attack in the Age of Artificial Intelligence",
      "abstract": "",
      "year": "2021",
      "venue": "Wireless Communications and Mobile Computing",
      "authors": "Kong, Z., Xue, J., Wang, Y., Huang, L., Niu, Z., Li, F."
    },
    {
      "index": 34,
      "title": "Ai-driven cybersecurity: An overview, security intelligence modeling and research directions",
      "abstract": "",
      "year": "2021",
      "venue": "SN Comput. Sci.",
      "authors": "Sarker, I.H., Furhad, M.H., Nowrozy, R."
    },
    {
      "index": 35,
      "title": "The Threat of Offensive AI to Organizations",
      "abstract": "",
      "year": "2023",
      "venue": "Computers & Security",
      "authors": "Mirsky, Y., Demontis, A., Kotak, J., Shankar, R., Gelei, D., Yang, L., Zhang, X., Pintor, M., Lee, W., Elovici, Y., Biggio, B.",
      "orig_title": "The threat of offensive ai to organizations",
      "paper_id": "2106.15764v1"
    },
    {
      "index": 36,
      "title": "”why do so?” - A practical perspective on machine learning security",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "Grosse, K., Bieringer, L., Besold, T.R., Biggio, B., Krombholz, K."
    },
    {
      "index": 37,
      "title": "Security for machine learning-based software systems: a survey of threats, practices and challenges",
      "abstract": "",
      "year": "2022",
      "venue": "CoRR",
      "authors": "Chen, H., Babar, M.A."
    },
    {
      "index": 38,
      "title": "Artificial Intelligence a Modern Approach",
      "abstract": "",
      "year": "2010",
      "venue": "Pearson Education, Inc.",
      "authors": "Russell, S.J."
    },
    {
      "index": 39,
      "title": "Decision tree methods: applications for classification and prediction",
      "abstract": "",
      "year": "2015",
      "venue": "Shanghai archives of psychiatry",
      "authors": "Song, Y.-Y., Ying, L."
    },
    {
      "index": 40,
      "title": "Discriminatory analysis-nonparametric discrimination: Small sample performance",
      "abstract": "",
      "year": "1952",
      "venue": "California Univ. Berkeley",
      "authors": "Fix, E., Hodges Jr, J.L."
    },
    {
      "index": 41,
      "title": "Support-vector networks",
      "abstract": "",
      "year": "1995",
      "venue": "Machine Learning",
      "authors": "Cortes, C., Vapnik, V."
    },
    {
      "index": 42,
      "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "abstract": "",
      "year": "2009",
      "venue": "Springer",
      "authors": "Hastie, T., Tibshirani, R., Friedman, J.H., Friedman, J.H."
    },
    {
      "index": 43,
      "title": "The regression analysis of binary sequences",
      "abstract": "",
      "year": "1958",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "Cox, D.R."
    },
    {
      "index": 44,
      "title": "A logical calculus of the ideas immanent in nervous activity",
      "abstract": "",
      "year": "1943",
      "venue": "The bulletin of mathematical biophysics",
      "authors": "McCulloch, W.S., Pitts, W."
    },
    {
      "index": 45,
      "title": "Information technology – security techniques – information security risk management",
      "abstract": "",
      "year": "2018",
      "venue": "ISO/IEC 27005:2018",
      "authors": "ISO/IEC JTC 1/SC 27"
    },
    {
      "index": 46,
      "title": "Demystifying the threat-modeling process",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Secur. Priv.",
      "authors": "Torr, P."
    },
    {
      "index": 47,
      "title": "Information security, cybersecurity and privacy protection - evaluation criteria for it security - part 1: Introduction and general model",
      "abstract": "",
      "year": "2009",
      "venue": "ISO/IEC 15408-1:2022(E)",
      "authors": "ISO/IEC JTC 1/SC 27"
    },
    {
      "index": 48,
      "title": "Guide for conducting risk assessments",
      "abstract": "",
      "year": "2012",
      "venue": "NIST Special Publication (SP) 800-30, Rev.1",
      "authors": "National Institute of Standards and Technology"
    },
    {
      "index": 49,
      "title": "Industrial communication networks - network and system security - part 1-1: Terminology, concepts and models",
      "abstract": "",
      "year": "2009",
      "venue": "IEC TS 62443-1-1:2009",
      "authors": "IEC TC 65"
    },
    {
      "index": 50,
      "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Pattern Recognition",
      "authors": "Biggio, B., Roli, F.",
      "orig_title": "Wild patterns: Ten years after the rise of adversarial machine learning",
      "paper_id": "1712.03141v2"
    },
    {
      "index": 51,
      "title": "A survey on adversarial attacks and defences",
      "abstract": "",
      "year": "2021",
      "venue": "CAAI Trans. Intell. Technol.",
      "authors": "Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., Mukhopadhyay, D."
    },
    {
      "index": 52,
      "title": "A Survey on Universal Adversarial Attack",
      "abstract": "",
      "year": "2021",
      "venue": "IJCAI’21",
      "authors": "Zhang, C., Benz, P., Lin, C., Karjauv, A., Wu, J., Kweon, I.S.",
      "orig_title": "A survey on universal adversarial attack",
      "paper_id": "2103.01498v2"
    },
    {
      "index": 53,
      "title": "Adversarial Attacks and Defences: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "Electronics",
      "authors": "Liang, H., He, E., Zhao, Y., Jia, Z., Li, H.",
      "orig_title": "Adversarial attack and defense: A survey",
      "paper_id": "1810.00069v1"
    },
    {
      "index": 54,
      "title": "Adversarial Example Detection for DNN Models: A Review and Experimental Comparison",
      "abstract": "",
      "year": "2022",
      "venue": "Artif. Intell. Rev.",
      "authors": "Aldahdooh, A., Hamidouche, W., Fezza, S.A., Déforges, O.",
      "orig_title": "Adversarial example detection for DNN models: a review and experimental comparison",
      "paper_id": "2105.00203v4"
    },
    {
      "index": 55,
      "title": "Adversarial deep learning: A survey on adversarial attacks and defense mechanisms on image classification",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Access",
      "authors": "Khamaiseh, S.Y., Bagagem, D., Al-Alaj, A., Mancino, M., Alomari, H.W."
    },
    {
      "index": 56,
      "title": "Y.: Secureml: A system for scalable privacy-preserving machine learning. In: SP’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "A.: BLAZE: blazing fast privacy-preserving machine learning. In: NDSS’20. The Internet Society",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "Adam in Private: Secure and Fast Training of Deep Neural Networks with Adaptive Moment Estimation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": Adam in private: Secure and fast training of deep neural networks with adaptive moment estimation",
      "paper_id": "2106.02203v1"
    },
    {
      "index": 59,
      "title": "C.: Poisoning attacks on algorithmic fairness. In: ECML-PKDD’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "A.: Exacerbating algorithmic bias through fairness attacks. In: AAAI’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": ": Detecting backdoor attacks on deep neural networks by activation clustering",
      "paper_id": "1811.03728v1"
    },
    {
      "index": 62,
      "title": "Backdoor Learning: A Survey",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "",
      "orig_title": ": Backdoor learning: A survey",
      "paper_id": "2007.08745v5"
    },
    {
      "index": 63,
      "title": "O.: Microsoft’s racist chatbot revealed the dangers of online conversation. IEEE Spectrum",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "J.: SNAP: Efficient extraction of private properties with poisoning. In: S&P’23",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "P.: Certified defenses for data poisoning attacks. In: NeurIPS’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "A.: Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In: ICASSP’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "N.Z.: MemGuard: Defending against black-box membership inference attacks via adversarial examples. In: CCS’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "Virtual Event (2020)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": Wild patterns reloaded: A survey of machine learning security against training data poisoning",
      "paper_id": "2205.01992v3"
    },
    {
      "index": 70,
      "title": ": Machine learning security against data poisoning: Are we there yet? CoRR abs/2204",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 71,
      "title": "V.: Machine learning models that remember too much. In: CCS’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "N.Z.: Robust and verifiable information embedding attacks to deep neural networks via error-correcting codes. In: AsiaCCS’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": ": Planting undetectable backdoors in machine learning models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "S.: Fine-pruning: Defending against backdooring attacks on deep neural networks. In: RAID’18. Lecture Notes in Computer Science",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "D.C.: Februus: Input purification defense against trojan attacks on deep neural network systems. In: ACSAC’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "S.: MISA: online defense of trojaned models using misattributions. In: ACSAC’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "M.: Towards reverse-engineering black-box neural networks. In: Explainable AI: Interpreting",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "N.Z.: Stealing hyperparameters in machine learning. In: S&P’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "T.: Stealing machine learning models via prediction apis. In: 25th USENIX Security Symposium",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 80,
      "title": "N.: PRADA: protecting against DNN model stealing attacks. In: EuroS&P",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "T.: Copycat CNN: stealing knowledge by persuading confession with random non-labeled data. In: IJCNN’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "M.: Knockoff nets: Stealing functionality of black-box models. In: CVPR’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "N.: Extraction of complex dnn models: Real threat or boogeyman? In: Engineering Dependable and Secure Machine Learning Systems",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "J.: Perturbing inputs to prevent model stealing. In: CNS’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "Y.: ML-Doctor: Holistic risk assessment of inference attacks against machine learning models. In: USENIX Security’22",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "Online Streaming (2021). https://doi.org/10.5220/0010373302580269",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "J.: BDPL: A boundary differentially private layer against machine learning model extraction attacks. In: ESORICS’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "S.: Embedding watermarks into deep neural networks. In: ICMR’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "J.: Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In: USENIX Security’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "M.: A survey of deep neural network watermarking techniques. Neurocomputing 461",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "F.: A systematic review on model watermarking for neural networks. Frontiers Big Data 4",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": I know what you trained last summer: A survey on stealing machine learning models and defences",
      "paper_id": "2206.08451v2"
    },
    {
      "index": 93,
      "title": "S.: Adversarial examples in the physical world. In: ICLR’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "A.: Practical black-box attacks against machine learning. In: AsiaCCS’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "D.: Robust physical-world attacks on deep learning visual classification. In: CVPR’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "J.: Robust audio adversarial example for a physical attack. In: IJCAI’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "P.: Universal adversarial perturbations. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 98,
      "title": "A.: The limitations of deep learning in adversarial settings. In: EuroS&P’16",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "C.: Explaining and harnessing adversarial examples. In: ICLR’15",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "X.: NIC: detecting adversarial samples with neural network invariant checking. In: NDSS’19. The Internet Society",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "Y.: Feature squeezing: Detecting adversarial examples in deep neural networks. In: NDSS’18. The Internet Society",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 102,
      "title": "A.L.: Mitigating adversarial effects through randomization. In: 6th International Conference on Learning Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 103,
      "title": "A study of the effect of JPG compression on adversarial images",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": ": A study of the effect of JPG compression on adversarial images",
      "paper_id": "1608.00853v1"
    },
    {
      "index": 104,
      "title": "H.: Comdefend: An efficient image compression model to defend adversarial examples. In: CVPR’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 105,
      "title": "R.: Defense-gan: Protecting classifiers against adversarial attacks using generative models. In: ICLR’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "Y.: Generative adversarial nets. In: NeurIPS’14",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": ": Image super-resolution as a defense against adversarial attacks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "H.: A unified gradient regularization family for adversarial examples. In: ICDM’15",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "S.: Adversarial machine learning at scale. In: ICLR’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "S.: Understanding adversarial training: Increasing local stability of supervised models through robust optimization. Neurocomputing 307",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "S.P.: Simple black-box adversarial attacks on deep neural networks. In: CVPR Workshops 2017",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "D.: Delving into transferable adversarial examples and black-box attacks. In: ICLR’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 113,
      "title": "P.D.: Ensemble adversarial training: Attacks and defenses. In: ICLR’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 114,
      "title": "J.: Defense against adversarial attacks using high-level representation guided denoiser. In: CVPR’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "M.J.: Reluplex: An efficient SMT solver for verifying deep neural networks. In: CAV’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": ": Evaluating robustness of neural networks with mixed integer programming",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "Towards Fast Computation of Certified Robustness for ReLU Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": ": Towards fast computation of certified robustness for relu networks",
      "paper_id": "1804.09699v4"
    },
    {
      "index": 118,
      "title": "L.: Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks. In: AAAI’19/IAAI’19/EAAI’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 119,
      "title": "L.: PROVEN: verifying robustness of neural networks with a probabilistic approach. In: ICML’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": ": Adversarial robustness toolbox v1",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "NeurIPS Datasets and Benchmarks 2021",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 122,
      "title": ": Technical report on the CleverHans v2",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 123,
      "title": "M.: Foolbox: A python toolbox to benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131 (2017)",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "S.: Protecting neural networks with hierarchical random switching: Towards better robustness-accuracy trade-off for stochastic defenses. In: IJCAI’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "C.: Towards robust neural networks via random self-ensemble. In: ECCV’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 126,
      "title": "P.: Explanations can be manipulated and geometry is to blame. In: NeurIPS’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "H.: Fooling LIME and SHAP: adversarial attacks on post hoc explanation methods. In: AIES ’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "A.: Distillation as a defense to adversarial perturbations against deep neural networks. In: SP’16",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": ": Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 130,
      "title": "D.A.: Towards evaluating the robustness of neural networks. In: SP’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 131,
      "title": "P.: Membership inference attacks against adversarially robust deep learning models. In: SP Workshops 2019",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 132,
      "title": "V.: Membership inference attacks against machine learning models. In: SP’17",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 133,
      "title": "T.: Model inversion attacks that exploit confidence information and basic countermeasures. In: CCS’15",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "J.F.: A methodology for formalizing model-inversion attacks. In: CSF’16",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "S.: Privacy risk in machine learning: Analyzing the connection to overfitting. In: CSF’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": Sok: Let the privacy games begin! a unified treatment of data inference privacy in machine learning",
      "paper_id": "2212.10986v2"
    },
    {
      "index": 137,
      "title": "D.: The secret sharer: Evaluating and testing unintended memorization in neural networks. In: USENIX Security’19",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 138,
      "title": "C.: Extracting training data from large language models. In: USENIX Security’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 139,
      "title": "Y.: Inference attacks against graph neural networks. In: Butler",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "J.: Reconstructing training data with informed adversaries. In: 43rd IEEE Symposium on Security and Privacy",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 141,
      "title": "N.: Property inference attacks on fully connected neural networks using permutation invariant representations. In: CCS’18",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "M.: Unleashing the tiger: Inference attacks on split learning. In: Kim",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 143,
      "title": "O.: Leakage of dataset properties in multi-party machine learning. In: Bailey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": ": Protecting global properties of datasets with distribution privacy mechanisms",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 145,
      "title": "A Survey of Privacy Attacks in Machine Learning",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": "",
      "orig_title": ": A survey of privacy attacks in machine learning",
      "paper_id": "2007.07646v3"
    },
    {
      "index": 146,
      "title": "Survey: Leakage and Privacy at Inference Time",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": ": Survey: Leakage and privacy at inference time",
      "paper_id": "2107.01614v2"
    },
    {
      "index": 147,
      "title": "P.: Generating high-fidelity synthetic patient data for assessing machine learning healthcare software. NPJ digital medicine 3(1)",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "Privacy-preserving Machine Learning through Data Obfuscation",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": ": Privacy-preserving machine learning through data obfuscation",
      "paper_id": "1807.01860v2"
    },
    {
      "index": 149,
      "title": "C.: Differential privacy. In: ICALP’06",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 150,
      "title": "A.: Private empirical risk minimization: Efficient algorithms and tight error bounds. In: FOCS’14",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 151,
      "title": "A.D.: Stochastic gradient descent with differentially private updates. In: GlobalSIP’13",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "V.: Privacy-preserving deep learning. In: CCS’15",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 153,
      "title": "L.: Deep learning with differential privacy. In: CCS’16",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 154,
      "title": ": ML privacy meter: Aiding regulatory compliance by quantifying the privacy risks of machine learning",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 155,
      "title": "J.: Towards making systems forget with machine unlearning. In: S&P’15",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "S.: Eternal sunshine of the spotless net: Selective forgetting in deep networks. In: CVPR’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "Machine Unlearning: Linear Filtration for Logit-based Classifiers",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": Machine unlearning: linear filtration for logit-based classifiers",
      "paper_id": "2002.02730v2"
    },
    {
      "index": 158,
      "title": "A Survey of Machine Unlearning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": A survey of machine unlearning",
      "paper_id": "2209.02299v6"
    },
    {
      "index": 159,
      "title": "A.: Membership privacy for machine learning models through knowledge transfer. In: AAAI’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "N.: Label-only membership inference attacks. In: ICML’21. Proceedings of Machine Learning Research",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "P.: Systematic evaluation of privacy risks of machine learning models. In: USENIX Security’21",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 162,
      "title": "Membership Inference Attacks on Machine Learning: A Survey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": ": Membership inference attacks on machine learning: A survey",
      "paper_id": "2103.07853v4"
    },
    {
      "index": 163,
      "title": "K.: Privacy-preserving deep learning on machine learning as a service - a comprehensive survey. IEEE Access 8",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 164,
      "title": "Privacy in Deep Learning: A Survey",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "",
      "orig_title": ": Privacy in deep learning: A survey",
      "paper_id": "2004.12254v5"
    },
    {
      "index": 165,
      "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": ": When machine learning meets privacy: A survey and outlook",
      "paper_id": "2011.11819v1"
    },
    {
      "index": 166,
      "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": ": Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning",
      "paper_id": "1812.00910v2"
    },
    {
      "index": 167,
      "title": "Advances and Open Problems in Federated Learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": ": Advances and open problems in federated learning",
      "paper_id": "1912.04977v3"
    },
    {
      "index": 168,
      "title": "D.: Federated learning and privacy. Commun. ACM 65(4)",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 169,
      "title": "P.: Explainable machine learning in deployment. In: FAT* ’20",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "A.: The problem of concept drift: definitions and related work. Computer Science Department",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    }
  ]
}