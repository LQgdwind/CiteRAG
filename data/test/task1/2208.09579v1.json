{
  "paper_id": "2208.09579v1",
  "title": "Learning in Audio-visual Context: A Review, Analysis, and New Perspective",
  "abstract": "Abstract\nSight and hearing are two senses that play a vital role in human communication and scene understanding.\nTo mimic human perception ability, audio-visual learning, aimed at developing computational approaches to learn from both audio and visual modalities, has been a flourishing field in recent years.\nA comprehensive survey that can systematically organize and analyze studies of the audio-visual field is expected.\nStarting from the analysis of audio-visual cognition foundations, we introduce several key findings that have inspired our computational studies. Then, we systematically review the recent audio-visual learning studies and divide them into three categories:\naudio-visual boosting, cross-modal perception and audio-visual collaboration.\nThrough our analysis, we discover that, the consistency of audio-visual data across semantic, spatial and temporal support the above studies.\nTo revisit the current development of the audio-visual learning field from a more macro view, we further propose a new perspective on audio-visual scene understanding, then discuss and analyze the feasible future direction of the audio-visual learning area.\nOverall, this survey reviews and outlooks the current audio-visual learning field from different aspects.\nWe hope it can provide researchers with a better understanding of this area. A website including constantly-updated survey is released: https://gewu-lab.github.io/audio-visual-learning/.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Brain biophysics: perception, consciousness, creativity. brain computer interface (bci)",
      "abstract": "",
      "year": "2018",
      "venue": "International Scientific Conference BCI 2018 Opole",
      "authors": "D. Man and R. Olchawa"
    },
    {
      "index": 1,
      "title": "The cognitive neuroscience of mind: a tribute to Michael S. Gazzaniga",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": "M. S. Gazzaniga"
    },
    {
      "index": 2,
      "title": "Deep audio-visual learning: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "International Journal of Automation and Computing",
      "authors": "H. Zhu, M.-D. Luo, R. Wang, A.-H. Zheng, and R. He"
    },
    {
      "index": 3,
      "title": "Recent Advances and Challenges in Deep Audio-Visual Correlation Learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2202.13673",
      "authors": "L. Vilaça, Y. Yu, and P. Viana",
      "orig_title": "Recent advances and challenges in deep audio-visual correlation learning",
      "paper_id": "2202.13673v1"
    },
    {
      "index": 4,
      "title": "A Survey of Multi-View Representation Learning",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on knowledge and data engineering",
      "authors": "Y. Li, M. Yang, and Z. Zhang",
      "orig_title": "A survey of multi-view representation learning",
      "paper_id": "1610.01206v5"
    },
    {
      "index": 5,
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "T. Baltrušaitis, C. Ahuja, and L.-P. Morency",
      "orig_title": "Multimodal machine learning: A survey and taxonomy",
      "paper_id": "1705.09406v2"
    },
    {
      "index": 6,
      "title": "Deep vision multimodal learning: Methodology, benchmark, and trend",
      "abstract": "",
      "year": "2022",
      "venue": "Applied Sciences",
      "authors": "W. Chai and G. Wang"
    },
    {
      "index": 7,
      "title": "Subcortical processing in auditory communication",
      "abstract": "",
      "year": "2015",
      "venue": "Hearing research",
      "authors": "A. Pannese, D. Grandjean, and S. Frühholz"
    },
    {
      "index": 8,
      "title": "Consciousness without a cerebral cortex: A challenge for neuroscience and medicine",
      "abstract": "",
      "year": "2007",
      "venue": "Behavioral and brain sciences",
      "authors": "B. Merker"
    },
    {
      "index": 9,
      "title": "Hearing lips and seeing voices",
      "abstract": "",
      "year": "1976",
      "venue": "Nature",
      "authors": "H. McGurk and J. MacDonald"
    },
    {
      "index": 10,
      "title": "Eye movements in auditory space perception",
      "abstract": "",
      "year": "1975",
      "venue": "Perception & Psychophysics",
      "authors": "B. Jones and B. Kabanoff"
    },
    {
      "index": 11,
      "title": "Crossmodal spatial interactions in subcortical and cortical circuits",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": "B. E. Stein, T. R. Stanford, M. T. Wallace, J. W. Vaughan, and W. Jiang"
    },
    {
      "index": 12,
      "title": "Polysensory properties of neurons in the anterior bank of the caudal superior temporal sulcus of the macaque monkey",
      "abstract": "",
      "year": "1988",
      "venue": "Journal of neurophysiology",
      "authors": "K. Hikosaka, E. Iwai, H. a. Saito, and K. Tanaka"
    },
    {
      "index": 13,
      "title": "Functional relevance of cross-modal plasticity in blind humans",
      "abstract": "",
      "year": "1997",
      "venue": "Nature",
      "authors": "L. G. Cohen, P. Celnik, A. Pascual-Leone, B. Corwell, L. Faiz, J. Dambrosia, M. Honda, N. Sadato, C. Gerloff, M. Hallett et al."
    },
    {
      "index": 14,
      "title": "Cortical hubs form a module for multisensory integration on top of the hierarchy of cortical networks",
      "abstract": "",
      "year": "2010",
      "venue": "Frontiers in neuroinformatics",
      "authors": "G. Zamora-López, C. Zhou, and J. Kurths"
    },
    {
      "index": 15,
      "title": "Visual music and musical vision",
      "abstract": "",
      "year": "2008",
      "venue": "Neurocomputing",
      "authors": "X. Li, D. Tao, S. J. Maybank, and Y. Yuan"
    },
    {
      "index": 16,
      "title": "Speech recognition by machine: A review",
      "abstract": "",
      "year": "1976",
      "venue": "IEEE",
      "authors": "D. R. Reddy"
    },
    {
      "index": 17,
      "title": "A survey on vision-based human action recognition",
      "abstract": "",
      "year": "2010",
      "venue": "Image and vision computing",
      "authors": "R. Poppe"
    },
    {
      "index": 18,
      "title": "A historical perspective of speech recognition",
      "abstract": "",
      "year": "2014",
      "venue": "Communications of the ACM",
      "authors": "X. Huang, J. Baker, and R. Reddy"
    },
    {
      "index": 19,
      "title": "Visual speech recognition using active shape models and hidden markov models",
      "abstract": "",
      "year": "1996",
      "venue": "ICASSP",
      "authors": "J. Luettin, N. A. Thacker, and S. W. Beet"
    },
    {
      "index": 20,
      "title": "Audiovisual speech processing",
      "abstract": "",
      "year": "2001",
      "venue": "IEEE signal processing magazine",
      "authors": "T. Chen"
    },
    {
      "index": 21,
      "title": "A comparison of model and transform-based visual features for audio-visual lvcsr",
      "abstract": "",
      "year": "2001",
      "venue": "ICME",
      "authors": "I. Matthews, G. Potamianos, C. Neti, and J. Luettin"
    },
    {
      "index": 22,
      "title": "Audio-visual speech modeling for continuous speech recognition",
      "abstract": "",
      "year": "2000",
      "venue": "IEEE transactions on multimedia",
      "authors": "S. Dupont and J. Luettin"
    },
    {
      "index": 23,
      "title": "Dynamic bayesian networks for audio-visual speech recognition",
      "abstract": "",
      "year": "2002",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "authors": "A. V. Nefian, L. Liang, X. Pi, X. Liu, and K. Murphy"
    },
    {
      "index": 24,
      "title": "Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition",
      "abstract": "",
      "year": "2008",
      "venue": "ICMI",
      "authors": "M. Gurban, J.-P. Thiran, T. Drugman, and T. Dutoit"
    },
    {
      "index": 25,
      "title": "Audio-visual speech recognition using deep learning",
      "abstract": "",
      "year": "2015",
      "venue": "Applied Intelligence",
      "authors": "K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata"
    },
    {
      "index": 26,
      "title": "End-to-end audiovisual fusion with lstms",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1709.04343",
      "authors": "S. Petridis, Y. Wang, Z. Li, and M. Pantic"
    },
    {
      "index": 27,
      "title": "Temporal multimodal learning in audiovisual speech recognition",
      "abstract": "",
      "year": "2016",
      "venue": "CVPR",
      "authors": "D. Hu, X. Li et al."
    },
    {
      "index": 28,
      "title": "Deep Audio-Visual Speech Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman",
      "orig_title": "Deep audio-visual speech recognition",
      "paper_id": "1809.02108v2"
    },
    {
      "index": 29,
      "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.11637",
      "authors": "G. Zhao, J. Lin, Z. Zhang, X. Ren, Q. Su, and X. Sun",
      "orig_title": "Explicit sparse transformer: Concentrated attention through explicit selection",
      "paper_id": "1912.11637v1"
    },
    {
      "index": 30,
      "title": "Multimodal sparse transformer network for audio-visual speech recognition",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": "Q. Song, B. Sun, and S. Li"
    },
    {
      "index": 31,
      "title": "Robust Self-Supervised Audio-Visual Speech Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2201.01763",
      "authors": "B. Shi, W.-N. Hsu, and A. Mohamed",
      "orig_title": "Robust self-supervised audio-visual speech recognition",
      "paper_id": "2201.01763v3"
    },
    {
      "index": 32,
      "title": "A survey of face recognition techniques",
      "abstract": "",
      "year": "2009",
      "venue": "journal of information processing systems",
      "authors": "R. Jafri and H. R. Arabnia"
    },
    {
      "index": 33,
      "title": "An overview of automatic speaker recognition technology",
      "abstract": "",
      "year": "2002",
      "venue": "ICASSP",
      "authors": "D. A. Reynolds"
    },
    {
      "index": 34,
      "title": "Fusion of face and speech data for person identity verification",
      "abstract": "",
      "year": "1999",
      "venue": "IEEE transactions on neural networks",
      "authors": "S. Ben-Yacoub, Y. Abdeljaoued, and E. Mayoraz"
    },
    {
      "index": 35,
      "title": "Multimodal person recognition using unconstrained audio and video",
      "abstract": "",
      "year": "1999",
      "venue": "AVBPA",
      "authors": "T. Choudhury, B. Clarkson, T. Jebara, and A. Pentland"
    },
    {
      "index": 36,
      "title": "Multi-level fusion of audio and visual features for speaker identification",
      "abstract": "",
      "year": "2006",
      "venue": "International Conference on Biometrics",
      "authors": "Z. Wu, L. Cai, and H. Meng"
    },
    {
      "index": 37,
      "title": "Audiovisual celebrity recognition in unconstrained web videos",
      "abstract": "",
      "year": "2009",
      "venue": "ICASSP",
      "authors": "M. E. Sargin, H. Aradhye, P. J. Moreno, and M. Zhao"
    },
    {
      "index": 38,
      "title": "A multi-view approach to audio-visual speaker verification",
      "abstract": "",
      "year": "2021",
      "venue": "ICASSP",
      "authors": "L. Sarı, K. Singh, J. Zhou, L. Torresani, N. Singhal, and Y. Saraf"
    },
    {
      "index": 39,
      "title": "Noise-Tolerant Audio-Visual Online Person Verification Using an Attention-based Neural Network Fusion",
      "abstract": "",
      "year": "2019",
      "venue": "ICASSP",
      "authors": "S. Shon, T.-H. Oh, and J. Glass",
      "orig_title": "Noise-tolerant audio-visual online person verification using an attention-based neural network fusion",
      "paper_id": "1811.10813v1"
    },
    {
      "index": 40,
      "title": "Audio-visual deep neural network for robust person verification",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "authors": "Y. Qian, Z. Chen, and S. Wang"
    },
    {
      "index": 41,
      "title": "Audio-visual person recognition in multimedia data from the iarpa janus program",
      "abstract": "",
      "year": "2018",
      "venue": "ICASSP",
      "authors": "G. Sell, K. Duh, D. Snyder, D. Etter, and D. Garcia-Romero"
    },
    {
      "index": 42,
      "title": "Audio-visual speaker diarization using fisher linear semi-discriminant analysis",
      "abstract": "",
      "year": "2016",
      "venue": "Multimedia Tools and Applications",
      "authors": "N. Sarafianos, T. Giannakopoulos, and S. Petridis"
    },
    {
      "index": 43,
      "title": "Multi-modal speaker diarization of real-world meetings using compressed-domain video features",
      "abstract": "",
      "year": "2009",
      "venue": "ICASSP",
      "authors": "G. Friedland, H. Hung, and C. Yeo"
    },
    {
      "index": 44,
      "title": "Multimodal speaker diarization",
      "abstract": "",
      "year": "2011",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "A. Noulas, G. Englebienne, and B. J. Krose"
    },
    {
      "index": 45,
      "title": "Audio segmentation and speaker localization in meeting videos",
      "abstract": "",
      "year": "2006",
      "venue": "ICPR",
      "authors": "H. Vajaria, T. Islam, S. Sarkar, R. Sankar, and R. Kasturi"
    },
    {
      "index": 46,
      "title": "Visual speaker localization aided by acoustic models",
      "abstract": "",
      "year": "2009",
      "venue": "ACM MM",
      "authors": "G. Friedland, C. Yeo, and H. Hung"
    },
    {
      "index": 47,
      "title": "Audio-visual speaker diarization based on spatiotemporal bayesian fusion",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "I. D. Gebru, S. Ba, X. Li, and R. Horaud"
    },
    {
      "index": 48,
      "title": "Who said that?: Audio-visual speaker diarisation of real-world meetings",
      "abstract": "",
      "year": "2019",
      "venue": "Interspeech",
      "authors": "J. S. Chung, B.-J. Lee, and I. Han",
      "orig_title": "Who said that?: Audio-visual speaker diarisation of real-world meetings",
      "paper_id": "1906.10042v1"
    },
    {
      "index": 49,
      "title": "Self-supervised learning for audio-visual speaker diarization",
      "abstract": "",
      "year": "2020",
      "venue": "ICASSP",
      "authors": "Y. Ding, Y. Xu, S.-X. Zhang, Y. Cong, and L. Wang",
      "orig_title": "Self-supervised learning for audio-visual speaker diarization",
      "paper_id": "2002.05314v1"
    },
    {
      "index": 50,
      "title": "Human action recognition from various data modalities: A review",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "authors": "Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu"
    },
    {
      "index": 51,
      "title": "“A short note on the kinetics-700 human action dataset",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 52,
      "title": "“Exploring multimodal video representation for action recognition",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 53,
      "title": "The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“The activitynet large-scale activity recognition challenge 2018 summary",
      "paper_id": "1808.03766v2"
    },
    {
      "index": 54,
      "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Epic-fusion: Audio-visual temporal binding for egocentric action recognition",
      "paper_id": "1908.08498v1"
    },
    {
      "index": 55,
      "title": "Audiovisual SlowFast Networks for Video Recognition",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Audiovisual slowfast networks for video recognition",
      "paper_id": "2001.08740v2"
    },
    {
      "index": 56,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 57,
      "title": "MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Mm-vit: Multi-modal video transformer for compressed video action recognition",
      "paper_id": "2108.09322v2"
    },
    {
      "index": 58,
      "title": "“Scsampler: Sampling salient clips from video for efficient action recognition",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "Listen to Look: Action Recognition by Previewing Audio",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Listen to look: Action recognition by previewing audio",
      "paper_id": "1912.04487v3"
    },
    {
      "index": 60,
      "title": "AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Adamml: Adaptive multi-modal learning for efficient video recognition",
      "paper_id": "2105.05165v2"
    },
    {
      "index": 61,
      "title": "Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learnable irrelevant modality dropout for multimodal action recognition on modality-specific annotated videos",
      "paper_id": "2203.03014v2"
    },
    {
      "index": 62,
      "title": "“Cross-domain first person audio-visual action recognition through relative norm alignment",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 63,
      "title": "Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Domain generalization through audio-visual relative norm alignment in first person action recognition",
      "paper_id": "2110.10101v1"
    },
    {
      "index": 64,
      "title": "Audio-Adaptive Activity Recognition Across Video Domains",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-adaptive activity recognition across video domains",
      "paper_id": "2203.14240v2"
    },
    {
      "index": 65,
      "title": "“Communication without words",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 66,
      "title": "“Emotion recognition based on joint visual and audio cues",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 67,
      "title": "“Error weighted semi-coupled hidden markov model for audio-visual emotion recognition",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 68,
      "title": "“Two-level hierarchical alignment for semi-coupled hmm-based audiovisual emotion recognition with temporal course",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "“Machine analysis of facial expressions",
      "abstract": "",
      "year": "2007",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "“Automatic facial expression analysis: a survey",
      "abstract": "",
      "year": "2003",
      "venue": "",
      "authors": ""
    },
    {
      "index": 71,
      "title": "“Context-sensitive learning for enhanced audiovisual emotion classification",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "“Audiovisual vocal outburst classification in noisy acoustic conditions",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "“Bimodal log-linear regression for fusion of audio and visual features",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "“Decision level combination of multiple modalities for recognition and analysis of emotional expression",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 75,
      "title": "“Modeling latent discriminative dynamic of multi-dimensional affective signals",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 76,
      "title": "“Audio–visual affective expression recognition through multistream fused hmm",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 77,
      "title": "“Audio visual emotion recognition based on triple-stream dynamic bayesian network models",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 78,
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Tensor fusion network for multimodal sentiment analysis",
      "paper_id": "1707.07250v1"
    },
    {
      "index": 79,
      "title": "Memory Fusion Network for Multi-view Sequential Learning",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Memory fusion network for multi-view sequential learning",
      "paper_id": "1802.00927v1"
    },
    {
      "index": 80,
      "title": "“Conversational memory network for emotion recognition in dyadic dialogue videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 81,
      "title": "“Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 82,
      "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "paper_id": "2006.15955v1"
    },
    {
      "index": 83,
      "title": "“Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 84,
      "title": "“Multi-modal sarcasm detection and humor classification in code-mixed conversations",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 85,
      "title": "“Contextual inter-modal attention for multi-modal sentiment analysis",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 86,
      "title": "“Multi-modal sarcasm detection in twitter with hierarchical fusion model",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 87,
      "title": "“Multilogue-net: A context aware rnn for multi-modal emotion detection and sentiment analysis in conversation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 88,
      "title": "“Audio-visual speech enhancement using multimodal deep convolutional neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "“Visually assisted time-domain speech enhancement",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "ON TRAINING TARGETS AND OBJECTIVE FUNCTIONS FOR DEEP-LEARNING-BASED AUDIO-VISUAL SPEECH ENHANCEMENT",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“On training targets and objective functions for deep-learning-based audio-visual speech enhancement",
      "paper_id": "1811.06234v1"
    },
    {
      "index": 91,
      "title": "The Conversation: Deep Audio-Visual Speech Enhancement",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“The conversation: Deep audio-visual speech enhancement",
      "paper_id": "1804.04121v2"
    },
    {
      "index": 92,
      "title": "My lips are concealed: Audio-visual speech enhancement through obstructions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“My lips are concealed: Audio-visual speech enhancement through obstructions",
      "paper_id": "1907.04975v1"
    },
    {
      "index": 93,
      "title": "Robust Unsupervised Audio-visual Speech Enhancement Using a Mixture of Variational Autoencoders",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders",
      "paper_id": "1911.03930v1"
    },
    {
      "index": 94,
      "title": "“Looking into your speech: Learning cross-modal affinity for audio-visual speech separation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 95,
      "title": "X. Alameda-Pineda",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "Learning to Separate Object Sounds by Watching Unlabeled Video",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to separate object sounds by watching unlabeled video",
      "paper_id": "1804.01665v2"
    },
    {
      "index": 97,
      "title": "The Sound of Pixels",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“The sound of pixels",
      "paper_id": "1804.03160v4"
    },
    {
      "index": 98,
      "title": "Self-Supervised Audio-Visual Co-Segmentation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised audio-visual co-segmentation",
      "paper_id": "1904.09013v1"
    },
    {
      "index": 99,
      "title": "The Sound of Motions",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“The sound of motions",
      "paper_id": "1904.05979v1"
    },
    {
      "index": 100,
      "title": "Recursive Visual Sound Separation Using Minus-Plus Net",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Recursive visual sound separation using minus-plus net",
      "paper_id": "1908.11602v2"
    },
    {
      "index": 101,
      "title": "Co-Separating Sounds of Visual Objects",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Co-separating sounds of visual objects",
      "paper_id": "1904.07750v2"
    },
    {
      "index": 102,
      "title": "Visual Scene Graphs for Audio Source Separation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual scene graphs for audio source separation",
      "paper_id": "2109.11955v1"
    },
    {
      "index": 103,
      "title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Cyclic co-learning of sounding object visual grounding and sound separation",
      "paper_id": "2104.02026v1"
    },
    {
      "index": 104,
      "title": "Learning to Have an Ear for Face Super-Resolution",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to have an ear for face super-resolution",
      "paper_id": "1909.12780v3"
    },
    {
      "index": 105,
      "title": "“Appearance matters",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "Audio source separation and speech enhancement",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "“Single-channel speech separation using sparse non-negative matrix factorization.” in Interspeech",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": ""
    },
    {
      "index": 108,
      "title": "“Learning joint statistical models for audio-visual fusion and segregation",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "“Audio-visual sound separation via hidden markov models",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "“Audio-visual enhancement of speech in noise",
      "abstract": "",
      "year": "2001",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "“Visually derived wiener filters for speech enhancement",
      "abstract": "",
      "year": "2010",
      "venue": "",
      "authors": ""
    },
    {
      "index": 112,
      "title": "Visual Speech Enhancement",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual speech enhancement",
      "paper_id": "1711.08789v3"
    },
    {
      "index": 113,
      "title": "Seeing Through Noise: Visually Driven Speaker Separation and Enhancement",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Seeing through noise: Visually driven speaker separation and enhancement",
      "paper_id": "1708.06767v3"
    },
    {
      "index": 114,
      "title": "“Multimodal speakerbeam: Single channel target speech extraction with audio-visual speaker clues.” in Interspeech",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 115,
      "title": "FaceFilter: Audio-visual speech separation using still images",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Facefilter: Audio-visual speech separation using still images",
      "paper_id": "2005.07074v1"
    },
    {
      "index": 116,
      "title": "Music Gesture for Visual Sound Separation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Music gesture for visual sound separation",
      "paper_id": "2004.09476v1"
    },
    {
      "index": 117,
      "title": "“What makes training multi-modal classification networks hard?” in CVPR",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "Balanced Multimodal Learning via On-the-fly Gradient Modulation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Balanced multimodal learning via on-the-fly gradient modulation",
      "paper_id": "2203.15332v1"
    },
    {
      "index": 119,
      "title": "“Reconstructing intelligible audio speech from visual speech features",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 120,
      "title": "“Generating intelligible audio speech from visual speech",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 121,
      "title": "VID2SPEECH: SPEECH RECONSTRUCTION FROM SILENT VIDEO",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Vid2speech: speech reconstruction from silent video",
      "paper_id": "1701.00495v2"
    },
    {
      "index": 122,
      "title": "Improved Speech Reconstruction from Silent Video",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Improved speech reconstruction from silent video",
      "paper_id": "1708.01204v3"
    },
    {
      "index": 123,
      "title": "Lip2AudSpec: Speech reconstruction from silent lip movements video",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Lip2audspec: Speech reconstruction from silent lip movements video",
      "paper_id": "1710.09798v1"
    },
    {
      "index": 124,
      "title": "Video-Driven Speech Reconstruction using Generative Adversarial Networks",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Video-driven speech reconstruction using generative adversarial networks",
      "paper_id": "1906.06301v1"
    },
    {
      "index": 125,
      "title": "End-to-End Video-To-Speech Synthesis using Generative Adversarial Networks",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end video-to-speech synthesis using generative adversarial networks",
      "paper_id": "2104.13332v3"
    },
    {
      "index": 126,
      "title": "“Harnessing ai for speech reconstruction using multi-view silent video feed",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 127,
      "title": "“Hush-hush speak: Speech reconstruction using silent videos.” in Interspeech",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "“Real-time piano music transcription based on computer vision",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": ""
    },
    {
      "index": 129,
      "title": "“Foley music: Learning to generate music from videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 130,
      "title": "Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Multi-instrumentalist net: Unsupervised generation of music from body movements",
      "paper_id": "2012.03478v1"
    },
    {
      "index": 131,
      "title": "Collaborative Learning to Generate Audio-Video Jointly",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Collaborative learning to generate audio-video jointly",
      "paper_id": "2104.02656v1"
    },
    {
      "index": 132,
      "title": "Video Background Music Generation with Controllable Music Transformer",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Video background music generation with controllable music transformer",
      "paper_id": "2111.08380v1"
    },
    {
      "index": 133,
      "title": "Visually Indicated Sounds",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "",
      "orig_title": "“Visually indicated sounds",
      "paper_id": "1512.08512v2"
    },
    {
      "index": 134,
      "title": "Visual to Sound: Generating Natural Sound for Videos in the Wild",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Visual to sound: Generating natural sound for videos in the wild",
      "paper_id": "1712.01393v2"
    },
    {
      "index": 135,
      "title": "“Generating visually aligned sound from videos",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 136,
      "title": "“Taming visually guided sound generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 137,
      "title": "Scene-Aware Audio for 360° Videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Scene-aware audio for 360 videos",
      "paper_id": "1805.04792v1"
    },
    {
      "index": 138,
      "title": "Self-Supervised Generation of Spatial Audio for 360∘ Video",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised generation of spatial audio for 360 video",
      "paper_id": "1809.02587v1"
    },
    {
      "index": 139,
      "title": "“2.5 d visual sound",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "Self-supervised Audio Spatialization with correspondence Classifier",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised audio spatialization with correspondence classifier",
      "paper_id": "1905.05375v1"
    },
    {
      "index": 141,
      "title": "Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Beyond mono to binaural: Generating binaural audio from mono audio with depth and cross modal attention",
      "paper_id": "2111.08046v1"
    },
    {
      "index": 142,
      "title": "Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Sep-stereo: Visually guided stereophonic audio generation by associating source separation",
      "paper_id": "2007.09902v1"
    },
    {
      "index": 143,
      "title": "Visually Informed Binaural Audio Generation without Binaural Audios",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Visually informed binaural audio generation without binaural audios",
      "paper_id": "2104.06162v1"
    },
    {
      "index": 144,
      "title": "Exploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Exploiting audio-visual consistency with partial supervision for spatial audio generation",
      "paper_id": "2105.00708v1"
    },
    {
      "index": 145,
      "title": "“You said that?: Synthesising talking faces from audio",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 146,
      "title": "“Synthesizing obama: learning lip sync from audio",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "Lip Movements Generation at a Glance",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Lip movements generation at a glance",
      "paper_id": "1803.10404v3"
    },
    {
      "index": 148,
      "title": "Realistic Speech-Driven Facial Animation with GANs",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Realistic speech-driven facial animation with gans",
      "paper_id": "1906.06337v1"
    },
    {
      "index": 149,
      "title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Few-shot adversarial learning of realistic neural talking head models",
      "paper_id": "1905.08233v2"
    },
    {
      "index": 150,
      "title": "MakeItTalk: Speaker-Aware Talking-Head Animation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Makelttalk: speaker-aware talking-head animation",
      "paper_id": "2004.12992v3"
    },
    {
      "index": 151,
      "title": "FReeNet: Multi-Identity Face Reenactment",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Freenet: Multi-identity face reenactment",
      "paper_id": "1905.11805v2"
    },
    {
      "index": 152,
      "title": "Neural Voice Puppetry: Audio-driven Facial Reenactment",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Neural voice puppetry: Audio-driven facial reenactment",
      "paper_id": "1912.05566v2"
    },
    {
      "index": 153,
      "title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation from Single-View Images",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Rotate-and-render: Unsupervised photorealistic face rotation from single-view images",
      "paper_id": "2003.08124v1"
    },
    {
      "index": 154,
      "title": "Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Write-a-speaker: Text-based emotional and rhythmic talking-head generation",
      "paper_id": "2104.07995v2"
    },
    {
      "index": 155,
      "title": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Pose-controllable talking face generation by implicitly modularized audio-visual representation",
      "paper_id": "2104.11116v1"
    },
    {
      "index": 156,
      "title": "“Audio-driven emotional video portraits",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "“Beat: the behavior expression animation toolkit",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 158,
      "title": "“Robot behavior toolkit: generating effective social behaviors for robots",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 159,
      "title": "“Learning individual styles of conversational gesture",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 160,
      "title": "“Seeg: Semantic energized co-speech gesture generation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "Analyzing Input and Output Representations for Speech-Driven Gesture Generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Analyzing input and output representations for speech-driven gesture generation",
      "paper_id": "1903.03369v4"
    },
    {
      "index": 162,
      "title": "“Evaluation of speech-to-gesture generation using bi-directional lstm network",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 163,
      "title": "To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“To react or not to react: End-to-end visual pose forecasting for personalized avatar during dyadic conversations",
      "paper_id": "1910.02181v1"
    },
    {
      "index": 164,
      "title": "“Style-controllable speech-driven gesture synthesis using normalising flows",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 165,
      "title": "S. Alexandersson",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach",
      "paper_id": "2007.12553v1"
    },
    {
      "index": 167,
      "title": "Audeo: Audio Generation for a Silent Performance Video",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Audeo: Audio generation for a silent performance video",
      "paper_id": "2006.14348v1"
    },
    {
      "index": 168,
      "title": "Dancing to Music",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Dancing to music",
      "paper_id": "1911.02001v1"
    },
    {
      "index": 169,
      "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Ai choreographer: Music conditioned 3d dance generation with aist++",
      "paper_id": "2101.08779v3"
    },
    {
      "index": 170,
      "title": "BatVision: Learning to See 3D Spatial Layout with Two Ears",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Batvision: Learning to see 3d spatial layout with two ears",
      "paper_id": "1912.07011v3"
    },
    {
      "index": 171,
      "title": "VisualEchoes: Spatial Image Representation Learning through Echolocation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Visualechoes: Spatial image representation learning through echolocation",
      "paper_id": "2005.01616v2"
    },
    {
      "index": 172,
      "title": "Beyond Image to Depth: Improving Depth Prediction using Echoes",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Beyond image to depth: Improving depth prediction using echoes",
      "paper_id": "2103.08468v2"
    },
    {
      "index": 173,
      "title": "“Co-attention-guided bilinear model for echo-based depth estimation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "“Soundnet: Learning sound representations from unlabeled video",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 175,
      "title": "“Ambient sound provides supervision for visual learning",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "Self-supervised Moving Vehicle Tracking with Stereo Sound",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised moving vehicle tracking with stereo sound",
      "paper_id": "1910.11760v1"
    },
    {
      "index": 177,
      "title": "“Enhanced audio tagging via multi-to single-modal teacher-student mutual learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 178,
      "title": "Multimodal Knowledge Expansion",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal knowledge expansion",
      "paper_id": "2103.14431v3"
    },
    {
      "index": 179,
      "title": "“Distilling audio-visual knowledge by compositional contrastive learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 180,
      "title": "“Example-based automatic music-driven conventional dance motion synthesis",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 181,
      "title": "“Music similarity-based approach to generating dance motion sequence",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 182,
      "title": "“Content-based video-music retrieval using soft intra-modal structure constraint",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "Image2song: Song Retrieval via Bridging Image Content and Lyric Words",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Image2song: Song retrieval via bridging image content and lyric words",
      "paper_id": "1708.05851v1"
    },
    {
      "index": 184,
      "title": "“Cross-modal embeddings for video and audio retrieval",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 185,
      "title": "“Learn2dance: Learning statistical music-to-dance mappings for choreography synthesis",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 186,
      "title": "“Dance with melody: An lstm-autoencoder approach to music-oriented dance synthesis",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "“Dance revolution: Long-term dance generation with music via curriculum learning",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 188,
      "title": "“Genre-conditioned long-term 3d dance generation driven by music",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 189,
      "title": "Audio to Body Dynamics",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio to body dynamics",
      "paper_id": "1712.09382v1"
    },
    {
      "index": 190,
      "title": "Sound-Guided Semantic Image Manipulation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Sound-guided semantic image manipulation",
      "paper_id": "2112.00007v1"
    },
    {
      "index": 191,
      "title": "Learning Visual Styles from Audio-Visual Associations",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning visual styles from audio-visual associations",
      "paper_id": "2205.05072v1"
    },
    {
      "index": 192,
      "title": "“There is more than meets the eye: Self-supervised multi-object detection and tracking with sound by distilling multimodal knowledge",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 193,
      "title": "“Knowledge distillation from multi-modality to single-modality for person verification",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 194,
      "title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Deep triplet neural networks with cluster-cca for audio-visual cross-modal retrieval",
      "paper_id": "1908.03737v3"
    },
    {
      "index": 195,
      "title": "“Deep cross-modal image–voice retrieval in remote sensing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 196,
      "title": "Labelling unlabelled videos from scratch with multi-modal self-supervision",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Labelling unlabelled videos from scratch with multi-modal self-supervision",
      "paper_id": "2006.13662v3"
    },
    {
      "index": 197,
      "title": "“Kernel and nonlinear canonical correlation analysis",
      "abstract": "",
      "year": "2000",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "“Deep canonical correlation analysis",
      "abstract": "",
      "year": "2013",
      "venue": "",
      "authors": ""
    },
    {
      "index": 199,
      "title": "“Cluster canonical correlation analysis",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "Audio-Visual Embedding for Cross-Modal Music Video Retrieval through Supervised Deep CCA",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual embedding for cross-modal music video retrieval through supervised deep cca",
      "paper_id": "1908.03744v1"
    },
    {
      "index": 201,
      "title": "“Learning classification with unlabeled data",
      "abstract": "",
      "year": "1994",
      "venue": "",
      "authors": ""
    },
    {
      "index": 202,
      "title": "listen and learn",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 203,
      "title": "Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Cooperative learning of audio and video models from self-supervised synchronization",
      "paper_id": "1807.00230v2"
    },
    {
      "index": 204,
      "title": "“Learning representations from audio-visual spatial alignment",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 205,
      "title": "Robust Audio-Visual Instance Discrimination",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Robust audio-visual instance discrimination",
      "paper_id": "2103.15916v1"
    },
    {
      "index": 206,
      "title": "Audio-Visual Instance Discrimination with Cross-Modal Agreement",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual instance discrimination with cross-modal agreement",
      "paper_id": "2004.12943v3"
    },
    {
      "index": 207,
      "title": "“Deep multimodal clustering for unsupervised audiovisual learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Self-supervised learning by cross-modal audio-video clustering",
      "paper_id": "1911.12667v3"
    },
    {
      "index": 209,
      "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Multimodal clustering networks for self-supervised learning from unlabeled videos",
      "paper_id": "2104.12671v3"
    },
    {
      "index": 210,
      "title": "I. Sutskever et al",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "A. Askell et al",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 212,
      "title": "“Opt: omni-perception pre-trainer for cross-modal understanding and generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "“Vatt: Transformers for multimodal self-supervised learning from raw video",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 214,
      "title": "“Audioclip: Extending clip to image",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 215,
      "title": "“Merlot reserve: Neural script knowledge through vision and language and sound",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 216,
      "title": "“Audio vision: Using audio-visual synchrony to locate sounds",
      "abstract": "",
      "year": "1999",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "“Pixels that sound",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": ""
    },
    {
      "index": 218,
      "title": "“Multimodal analysis for identification and segmentation of moving-sounding objects",
      "abstract": "",
      "year": "2012",
      "venue": "",
      "authors": ""
    },
    {
      "index": 219,
      "title": "Objects that Sound",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Objects that sound",
      "paper_id": "1712.06651v2"
    },
    {
      "index": 220,
      "title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual scene analysis with self-supervised multisensory features",
      "paper_id": "1804.03641v2"
    },
    {
      "index": 221,
      "title": "Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to localize sound sources in visual scenes: Analysis and applications",
      "paper_id": "1911.09649v1"
    },
    {
      "index": 222,
      "title": "Mix and Localize: Localizing Sound Sources in Mixtures",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Mix and localize: Localizing sound sources in mixtures",
      "paper_id": "2211.15058v1"
    },
    {
      "index": 223,
      "title": "“Class-aware sounding objects localization via audiovisual correspondence.” IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 224,
      "title": "Audio-Visual Segmentation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual segmentation",
      "paper_id": "2207.05042v3"
    },
    {
      "index": 225,
      "title": "“Video saliency detection via spatial-temporal fusion and low-rank coherency diffusion",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 226,
      "title": "“Bilevel feature learning for video saliency detection",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 227,
      "title": "J. Santos-Victor",
      "abstract": "",
      "year": "2008",
      "venue": "",
      "authors": ""
    },
    {
      "index": 228,
      "title": "“Multimodal saliency-based attention for object-based scene analysis",
      "abstract": "",
      "year": "2011",
      "venue": "",
      "authors": ""
    },
    {
      "index": 229,
      "title": "“Fixation prediction through multimodal analysis",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 230,
      "title": "DAVE: Deep Audio-Visual Embedding for Dynamic Saliency Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“Dave: A deep audio-visual embedding for dynamic saliency prediction",
      "paper_id": "1905.10693v2"
    },
    {
      "index": 231,
      "title": "STAViS: Spatio-Temporal AudioVisual Saliency Network",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Stavis: Spatio-temporal audiovisual saliency network",
      "paper_id": "2001.03063v2"
    },
    {
      "index": 232,
      "title": "“A multimodal saliency model for videos with high audio-visual correspondence",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 233,
      "title": "ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Vinet: Pushing the limits of visual modality for audio-visual saliency prediction",
      "paper_id": "2012.06170v3"
    },
    {
      "index": 234,
      "title": "“From semantic categories to fixations: A novel weakly-supervised visual-auditory saliency detection approach",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 235,
      "title": "“Cognitive mapping and planning for visual navigation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 236,
      "title": "Unifying Map and Landmark Based Representations for Visual Navigation",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Unifying map and landmark based representations for visual navigation",
      "paper_id": "1712.08125v1"
    },
    {
      "index": 237,
      "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Soundspaces: Audio-visual navigation in 3d environments",
      "paper_id": "1912.11474v3"
    },
    {
      "index": 238,
      "title": "and act: Towards audio-visual embodied navigation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 239,
      "title": "Learning to Set Waypoints for Audio-Visual Navigation",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to set waypoints for audio-visual navigation",
      "paper_id": "2008.09622v3"
    },
    {
      "index": 240,
      "title": "Semantic Audio-Visual Navigation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Semantic audio-visual navigation",
      "paper_id": "2012.11583v2"
    },
    {
      "index": 241,
      "title": "Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped Environments with Moving Sounds",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Catch me if you hear me: Audio-visual navigation in complex unmapped environments with moving sounds",
      "paper_id": "2111.14843v4"
    },
    {
      "index": 242,
      "title": "Sound Adversarial Audio-Visual Navigation",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Sound adversarial audio-visual navigation",
      "paper_id": "2202.10910v1"
    },
    {
      "index": 243,
      "title": "Move2Hear: Active Audio-Visual Source Separation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Move2hear: Active audio-visual source separation",
      "paper_id": "2105.07142v2"
    },
    {
      "index": 244,
      "title": "Audio-Visual Event Localization in Unconstrained Videos",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual event localization in unconstrained videos",
      "paper_id": "1803.08842v1"
    },
    {
      "index": 245,
      "title": "“Dual-modality seq2seq network for audio-visual event localization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 246,
      "title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Unified multisensory perception: Weakly-supervised audio-visual video parsing",
      "paper_id": "2007.10558v1"
    },
    {
      "index": 247,
      "title": "Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual event localization via recursive fusion by joint co-attention",
      "paper_id": "2008.06581v1"
    },
    {
      "index": 248,
      "title": "Positive Sample Propagation along the Audio-Visual Event Line",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Positive sample propagation along the audio-visual event line",
      "paper_id": "2104.00239v2"
    },
    {
      "index": 249,
      "title": "“Exploring cross-video and cross-modality signals for weakly-supervised audio-visual video parsing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 250,
      "title": "“Exploring heterogeneous clues for weakly-supervised audio-visual video parsing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 251,
      "title": "Investigating Modality Bias in Audio Visual Video Parsing",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Investigating modality bias in audio visual video parsing",
      "paper_id": "2203.16860v2"
    },
    {
      "index": 252,
      "title": "P. Anderson et al",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 253,
      "title": "“Joint student-teacher learning for audio-visual scene-aware dialog.” in Interspeech",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 254,
      "title": "A Simple Baseline for Audio-Visual Scene-Aware Dialog",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“A simple baseline for audio-visual scene-aware dialog",
      "paper_id": "1904.05876v1"
    },
    {
      "index": 255,
      "title": "Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Dynamic graph representation learning for video dialog via multi-modal shuffled transformers",
      "paper_id": "2007.03848v2"
    },
    {
      "index": 256,
      "title": "Pano-AVQA: Grounded Audio-Visual Question Answering on 360^∘ Videos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "“Pano-avqa: Grounded audio-visual question answering on 360deg videos",
      "paper_id": "2110.05122v1"
    },
    {
      "index": 257,
      "title": "Learning to Answer Questions in Dynamic Audio-Visual Scenarios",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Learning to answer questions in dynamic audio-visual scenarios",
      "paper_id": "2203.14072v2"
    },
    {
      "index": 258,
      "title": "Audio-Visual Scene-Aware Dialog and Reasoning using Audio-Visual Transformers with Joint Student-Teacher Learning",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "“Audio-visual scene-aware dialog and reasoning using audio-visual transformers with joint student-teacher learning",
      "paper_id": "2110.06894v1"
    },
    {
      "index": 259,
      "title": "“Dual attention matching for audio-visual event localization",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 260,
      "title": "“Audiovisual transformer with instance attention for audio-visual event localization",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 261,
      "title": "“Cross-modal background suppression for audio-visual event localization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 262,
      "title": "“Distributed audio-visual parsing based on multimodal transformer and deep joint source channel coding",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 263,
      "title": "End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "“End-to-end audio visual scene-aware dialog using multimodal attention-based video features",
      "paper_id": "1806.08409v2"
    },
    {
      "index": 264,
      "title": "“Lip reading in the wild",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 265,
      "title": "LRS3-TED: a large-scale dataset for visual speech recognition",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Lrs3-ted: a large-scale dataset for visual speech recognition",
      "paper_id": "1809.00496v2"
    },
    {
      "index": 266,
      "title": "VoxCeleb: a large-scale speaker identification dataset",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "“Voxceleb: a large-scale speaker identification dataset",
      "paper_id": "1706.08612v2"
    },
    {
      "index": 267,
      "title": "VoxCeleb2: Deep Speaker Recognition",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "“Voxceleb2: Deep speaker recognition",
      "paper_id": "1806.05622v2"
    },
    {
      "index": 268,
      "title": "AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Ava active speaker: An audio-visual dataset for active speaker detection",
      "paper_id": "1901.01342v2"
    },
    {
      "index": 269,
      "title": "S. Vijayanarasimhan",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 270,
      "title": "“Scaling egocentric vision: The epic-kitchens dataset",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 271,
      "title": "“Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 272,
      "title": "“Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 273,
      "title": "VGG-Sound: A Large-scale Audio-Visual Dataset",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "“Vggsound: A large-scale audio-visual dataset",
      "paper_id": "2004.14368v2"
    },
    {
      "index": 274,
      "title": "“Audio set: An ontology and human-labeled dataset for audio events",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": ""
    },
    {
      "index": 275,
      "title": "“The replica dataset: A digital replica of indoor spaces",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 276,
      "title": "“Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 277,
      "title": "H. Riemenschneider",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 278,
      "title": "“A behaviorally inspired fusion approach for computational audiovisual saliency modeling",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 279,
      "title": "A. Banki-Horvath",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    }
  ]
}