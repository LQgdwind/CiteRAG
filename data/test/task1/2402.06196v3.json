{
  "paper_id": "2402.06196v3",
  "title": "Large Language Models: A Survey",
  "abstract": "Abstract\nLarge Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022.\nLLMs‚Äô ability of general-purpose language understanding and generation is acquired by training billions of model‚Äôs parameters on massive amounts of text data, as predicted by scaling laws [1, 2].\nThe research area of LLMs, while very recent, is evolving rapidly in many different ways.\nIn this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations.\nWe also give an overview of techniques developed to build, and augment LLMs.\nWe then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks.\nFinally, we conclude the paper by discussing open challenges and future research directions.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Scaling Laws for Neural Language Models",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.08361",
      "authors": "J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei",
      "orig_title": "Scaling laws for neural language models",
      "paper_id": "2001.08361v1"
    },
    {
      "index": 1,
      "title": "Training Compute-Optimal Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.15556",
      "authors": "J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al.",
      "orig_title": "Training compute-optimal large language models",
      "paper_id": "2203.15556v1"
    },
    {
      "index": 2,
      "title": "Prediction and entropy of printed english",
      "abstract": "",
      "year": "1951",
      "venue": "Bell system technical journal",
      "authors": "C. E. Shannon"
    },
    {
      "index": 3,
      "title": "Statistical methods for speech recognition",
      "abstract": "",
      "year": "1998",
      "venue": "MIT press",
      "authors": "F. Jelinek"
    },
    {
      "index": 4,
      "title": "Foundations of statistical natural language processing",
      "abstract": "",
      "year": "1999",
      "venue": "MIT press",
      "authors": "C. Manning and H. Schutze"
    },
    {
      "index": 5,
      "title": "An introduction to information retrieval",
      "abstract": "",
      "year": "2009",
      "venue": "Cambridge university press",
      "authors": "C. D. Manning"
    },
    {
      "index": 6,
      "title": "A Survey of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.18223",
      "authors": "W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al.",
      "orig_title": "A survey of large language models",
      "paper_id": "2303.18223v16"
    },
    {
      "index": 7,
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.09419",
      "authors": "C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He et al."
    },
    {
      "index": 8,
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "abstract": "",
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "authors": "P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig"
    },
    {
      "index": 9,
      "title": "A survey for in-context learning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2301.00234",
      "authors": "Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui"
    },
    {
      "index": 10,
      "title": "Towards Reasoning in Large Language Models: A Survey",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.10403",
      "authors": "J. Huang and K. C.-C. Chang",
      "orig_title": "Towards reasoning in large language models: A survey",
      "paper_id": "2212.10403v2"
    },
    {
      "index": 11,
      "title": "An empirical study of smoothing techniques for language modeling",
      "abstract": "",
      "year": "1999",
      "venue": "Computer Speech & Language",
      "authors": "S. F. Chen and J. Goodman"
    },
    {
      "index": 12,
      "title": "A neural probabilistic language model",
      "abstract": "",
      "year": "2000",
      "venue": "Advances in neural information processing systems",
      "authors": "Y. Bengio, R. Ducharme, and P. Vincent"
    },
    {
      "index": 13,
      "title": "Continuous space language models for statistical machine translation",
      "abstract": "",
      "year": "2006",
      "venue": "COLING/ACL 2006 Main Conference Poster Sessions",
      "authors": "H. Schwenk, D. D√©chelotte, and J.-L. Gauvain"
    },
    {
      "index": 14,
      "title": "Recurrent neural network based language model.",
      "abstract": "",
      "year": "2010",
      "venue": "Interspeech",
      "authors": "T. Mikolov, M. Karafi√°t, L. Burget, J. Cernock·ª≥, and S. Khudanpur"
    },
    {
      "index": 15,
      "title": "Generating sequences with recurrent neural networks",
      "abstract": "",
      "year": "2013",
      "venue": "arXiv preprint arXiv:1308.0850",
      "authors": "A. Graves"
    },
    {
      "index": 16,
      "title": "Learning deep structured semantic models for web search using clickthrough data",
      "abstract": "",
      "year": "2013",
      "venue": "22nd ACM international conference on Information & Knowledge Management",
      "authors": "P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck"
    },
    {
      "index": 17,
      "title": "Neural Approaches to Conversational Information Retrieval",
      "abstract": "",
      "year": "2023",
      "venue": "Springer Nature",
      "authors": "J. Gao, C. Xiong, P. Bennett, and N. Craswell",
      "orig_title": "Neural Approaches to Conversational Information Retrieval",
      "paper_id": "2201.05176v1"
    },
    {
      "index": 18,
      "title": "Sequence to sequence learning with neural networks",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in neural information processing systems",
      "authors": "I. Sutskever, O. Vinyals, and Q. V. Le"
    },
    {
      "index": 19,
      "title": "On the properties of neural machine translation: Encoder-decoder approaches",
      "abstract": "",
      "year": "2014",
      "venue": "arXiv preprint arXiv:1409.1259",
      "authors": "K. Cho, B. Van Merri√´nboer, D. Bahdanau, and Y. Bengio"
    },
    {
      "index": 20,
      "title": "From captions to visual concepts and back",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll√°r, J. Gao, X. He, M. Mitchell, J. C. Platt et al."
    },
    {
      "index": 21,
      "title": "Show and Tell: A Neural Image Caption Generator",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "O. Vinyals, A. Toshev, S. Bengio, and D. Erhan",
      "orig_title": "Show and tell: A neural image caption generator",
      "paper_id": "1411.4555v2"
    },
    {
      "index": 22,
      "title": "Deep contextualized word representations. corr abs/1802.05365 (2018)",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.05365",
      "authors": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer"
    },
    {
      "index": 23,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1810.04805",
      "authors": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
      "orig_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 24,
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.11692",
      "authors": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov",
      "orig_title": "Roberta: A robustly optimized bert pretraining approach",
      "paper_id": "1907.11692v1"
    },
    {
      "index": 25,
      "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.03654",
      "authors": "P. He, X. Liu, J. Gao, and W. Chen",
      "orig_title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "paper_id": "2006.03654v6"
    },
    {
      "index": 26,
      "title": "Pre-Trained Models: Past, Present and Future",
      "abstract": "",
      "year": "2021",
      "venue": "AI Open",
      "authors": "X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang et al.",
      "orig_title": "Pre-trained models: Past, present and future",
      "paper_id": "2106.07139v3"
    },
    {
      "index": 27,
      "title": "Pre-trained Models for Natural Language Processing: A Survey",
      "abstract": "",
      "year": "2020",
      "venue": "Science China Technological Sciences",
      "authors": "X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang",
      "orig_title": "Pre-trained models for natural language processing: A survey",
      "paper_id": "2003.08271v4"
    },
    {
      "index": 28,
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "A. Gu, K. Goel, and C. R√©",
      "orig_title": "Efficiently modeling long sequences with structured state spaces",
      "paper_id": "2111.00396v3"
    },
    {
      "index": 29,
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2312.00752",
      "authors": "A. Gu and T. Dao"
    },
    {
      "index": 30,
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2204.02311",
      "authors": "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.",
      "orig_title": "Palm: Scaling language modeling with pathways",
      "paper_id": "2204.02311v5"
    },
    {
      "index": 31,
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.13971",
      "authors": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar et al.",
      "orig_title": "Llama: Open and efficient foundation language models",
      "paper_id": "2302.13971v1"
    },
    {
      "index": 32,
      "title": "GPT-4 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "OpenAI",
      "orig_title": "GPT-4 Technical Report",
      "paper_id": "2303.08774v6"
    },
    {
      "index": 33,
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou"
    },
    {
      "index": 34,
      "title": "Augmented Language Models: a Survey",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.07842",
      "authors": "G. Mialon, R. Dess√¨, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozi√®re, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz et al.",
      "orig_title": "Augmented language models: a survey",
      "paper_id": "2302.07842v1"
    },
    {
      "index": 35,
      "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2302.12813",
      "authors": "B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao"
    },
    {
      "index": 36,
      "title": "React: Synergizing reasoning and acting in language models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.03629",
      "authors": "S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao"
    },
    {
      "index": 37,
      "title": "Learning internal representations by error propagation",
      "abstract": "",
      "year": "1985",
      "venue": "",
      "authors": "D. E. Rumelhart, G. E. Hinton, R. J. Williams et al."
    },
    {
      "index": 38,
      "title": "Finding structure in time",
      "abstract": "",
      "year": "1990",
      "venue": "Cognitive science",
      "authors": "J. L. Elman"
    },
    {
      "index": 39,
      "title": "Fast text compression with neural networks.",
      "abstract": "",
      "year": "2000",
      "venue": "FLAIRS conference",
      "authors": "M. V. Mahoney"
    },
    {
      "index": 40,
      "title": "Strategies for training large scale neural network language models",
      "abstract": "",
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding",
      "authors": "T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ƒåernock·ª≥"
    },
    {
      "index": 41,
      "title": "tmikolov. rnnlm.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Deep Learning Based Text Classification: A Comprehensive Review",
      "abstract": "",
      "year": "2021",
      "venue": "ACM computing surveys (CSUR)",
      "authors": "S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao",
      "orig_title": "Deep learning‚Äìbased text classification: a comprehensive review",
      "paper_id": "2004.03705v3"
    },
    {
      "index": 43,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 44,
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1909.11942",
      "authors": "Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut",
      "orig_title": "Albert: A lite bert for self-supervised learning of language representations",
      "paper_id": "1909.11942v6"
    },
    {
      "index": 45,
      "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2003.10555",
      "authors": "K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning",
      "orig_title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "paper_id": "2003.10555v1"
    },
    {
      "index": 46,
      "title": "Cross-lingual Language Model Pretraining",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.07291",
      "authors": "G. Lample and A. Conneau",
      "orig_title": "Cross-lingual language model pretraining",
      "paper_id": "1901.07291v1"
    },
    {
      "index": 47,
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le",
      "orig_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "paper_id": "1906.08237v2"
    },
    {
      "index": 48,
      "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in neural information processing systems",
      "authors": "L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon",
      "orig_title": "Unified language model pre-training for natural language understanding and generation",
      "paper_id": "1905.03197v3"
    },
    {
      "index": 49,
      "title": "Improving language understanding by generative pre-training",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al."
    },
    {
      "index": 50,
      "title": "Language models are unsupervised multitask learners",
      "abstract": "",
      "year": "2019",
      "venue": "OpenAI blog",
      "authors": "A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al."
    },
    {
      "index": 51,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúExploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 52,
      "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚Äúmt5: A massively multilingual pre-trained text-to-text transformer",
      "paper_id": "2010.11934v3"
    },
    {
      "index": 53,
      "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMass: Masked sequence to sequence pre-training for language generation",
      "paper_id": "1905.02450v5"
    },
    {
      "index": 54,
      "title": "M. Ghazvininejad",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 55,
      "title": "A. Askell et al",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 56,
      "title": "G. Brockman et al",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 57,
      "title": "W. Saunders et al",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 58,
      "title": "‚ÄúTraining language models to follow instructions with human feedback",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 59,
      "title": "(2022) Introducing chatgpt",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 60,
      "title": "S. Bhosale et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 61,
      "title": "‚ÄúAlpaca: A strong",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 62,
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúQlora: Efficient finetuning of quantized llms",
      "paper_id": "2305.14314v1"
    },
    {
      "index": 63,
      "title": "‚ÄúKoala: A dialogue model for academic research",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 64,
      "title": "L. Saulnier et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 65,
      "title": "Code Llama: Open Foundation Models for Code",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCode llama: Open foundation models for code",
      "paper_id": "2308.12950v3"
    },
    {
      "index": 66,
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGorilla: Large language model connected with massive apis",
      "paper_id": "2305.15334v1"
    },
    {
      "index": 67,
      "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGiraffe: Adventures in expanding context lengths in llms",
      "paper_id": "2308.10882v1"
    },
    {
      "index": 68,
      "title": "‚ÄúVigogne: French instruction-following and chat models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 69,
      "title": "I. Beltagy et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 70,
      "title": "Focused Transformer: Contrastive Training for Context Scaling",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúFocused transformer: Contrastive training for context scaling",
      "paper_id": "2307.03170v2"
    },
    {
      "index": 71,
      "title": "‚ÄúStable beluga models",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 72,
      "title": "A. Chowdhery et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 73,
      "title": "S. Brahma et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 74,
      "title": "PaLM 2 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúPalm 2 technical report",
      "paper_id": "2305.10403v3"
    },
    {
      "index": 75,
      "title": "Large Language Models Encode Clinical Knowledge",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLarge language models encode clinical knowledge",
      "paper_id": "2212.13138v1"
    },
    {
      "index": 76,
      "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTowards expert-level medical question answering with large language models",
      "paper_id": "2305.09617v1"
    },
    {
      "index": 77,
      "title": "Finetuned Language Models Are Zero-Shot Learners",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúFinetuned language models are zero-shot learners",
      "paper_id": "2109.01652v5"
    },
    {
      "index": 78,
      "title": "‚ÄúScaling language models: Methods",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 79,
      "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMultitask prompted training enables zero-shot task generalization",
      "paper_id": "2110.08207v3"
    },
    {
      "index": 80,
      "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúErnie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
      "paper_id": "2107.02137v1"
    },
    {
      "index": 81,
      "title": "Improving language models by retrieving from trillions of tokens",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúImproving language models by retrieving from trillions of tokens",
      "paper_id": "2112.04426v3"
    },
    {
      "index": 82,
      "title": "‚ÄúJurassic-1: Technical details and evaluation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 83,
      "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGlam: Efficient scaling of language models with mixture-of-experts",
      "paper_id": "2112.06905v2"
    },
    {
      "index": 84,
      "title": "LaMDA: Language Models for Dialog Applications",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLamda: Language models for dialog applications",
      "paper_id": "2201.08239v3"
    },
    {
      "index": 85,
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúOpt: Open pre-trained transformer language models",
      "paper_id": "2205.01068v4"
    },
    {
      "index": 86,
      "title": "Galactica: A Large Language Model for Science",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGalactica: A large language model for science",
      "paper_id": "2211.09085v1"
    },
    {
      "index": 87,
      "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCodegen: An open large language model for code with multi-turn program synthesis",
      "paper_id": "2203.13474v5"
    },
    {
      "index": 88,
      "title": "S. Ananthakrishnan",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 89,
      "title": "P. Thacker et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 90,
      "title": "T. Gutman-Solo et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 91,
      "title": "‚ÄúUnifying language learning paradigms",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 92,
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBloom: A 176b-parameter open-access multilingual language model",
      "paper_id": "2211.05100v4"
    },
    {
      "index": 93,
      "title": "‚ÄúGlm-130b: An open bilingual pre-trained model",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 94,
      "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúPythia: A suite for analyzing large language models across training and scaling",
      "paper_id": "2304.01373v2"
    },
    {
      "index": 95,
      "title": "‚ÄúOrca: Progressive learning from complex explanation traces of gpt-4",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 96,
      "title": "‚ÄúStarcoder: may the source be with you!‚Äù arXiv preprint arXiv:2305.06161",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 97,
      "title": "Language Is Not All You Need: Aligning Perception with Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLanguage is not all you need: Aligning perception with language models",
      "paper_id": "2302.14045v2"
    },
    {
      "index": 98,
      "title": "‚ÄúGemini: a family of highly capable multimodal models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 99,
      "title": "Y. Chebotar et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 100,
      "title": "V. Korthikanti et al",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 101,
      "title": "Longformer: The Long-Document Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLongformer: The long-document transformer",
      "paper_id": "2004.05150v2"
    },
    {
      "index": 102,
      "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúOpt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "paper_id": "2212.12017v3"
    },
    {
      "index": 103,
      "title": "Language Models are General-Purpose Interfaces",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLanguage models are general-purpose interfaces",
      "paper_id": "2206.06336v1"
    },
    {
      "index": 104,
      "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúPrinciple-driven self-alignment of language models from scratch with minimal human supervision",
      "paper_id": "2305.03047v2"
    },
    {
      "index": 105,
      "title": "‚ÄúPalmyra-base Parameter Autoregressive Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 106,
      "title": "‚ÄúCamel-5b instructgpt",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 107,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 108,
      "title": "‚ÄúIntroducing mpt-7b: a new standard for open-source",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 109,
      "title": "A. Razdaibiedina",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 110,
      "title": "‚ÄúPal: Program-aided language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 111,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 112,
      "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCodegen2: Lessons for training llms on programming and natural languages",
      "paper_id": "2305.02309v2"
    },
    {
      "index": 113,
      "title": "Zephyr: Direct Distillation of LM Alignment",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúZephyr: Direct distillation of lm alignment",
      "paper_id": "2310.16944v1"
    },
    {
      "index": 114,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 115,
      "title": "‚ÄúQwen-vl: A frontier large vision-language model with versatile abilities",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 116,
      "title": "Available: https://mistral",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 117,
      "title": "‚ÄúDocllm: A layout-aware generative language model for multimodal document understanding",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 118,
      "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeepseek-coder: When the large language model meets programming ‚Äì the rise of code intelligence",
      "paper_id": "2401.14196v2"
    },
    {
      "index": 119,
      "title": "Knowledge Fusion of Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúKnowledge fusion of large language models",
      "paper_id": "2401.10491v2"
    },
    {
      "index": 120,
      "title": "TinyLlama: An Open-Source Small Language Model",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTinyllama: An open-source small language model",
      "paper_id": "2401.02385v2"
    },
    {
      "index": 121,
      "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLlama pro: Progressive llama with block expansion",
      "paper_id": "2401.02415v2"
    },
    {
      "index": 122,
      "title": "Transformer models: an introduction and catalog",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTransformer models: an introduction and catalog",
      "paper_id": "2302.07730v4"
    },
    {
      "index": 123,
      "title": "‚ÄúThe refinedweb dataset for falcon llm: outperforming curated corpora with web data",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 124,
      "title": "Z. Hatfield-Dodds",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 125,
      "title": "Self-Attention with Relative Position Representations",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSelf-attention with relative position representations",
      "paper_id": "1803.02155v2"
    },
    {
      "index": 126,
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRoformer: Enhanced transformer with rotary position embedding",
      "paper_id": "2104.09864v5"
    },
    {
      "index": 127,
      "title": "test long: Attention with linear biases enables input length extrapolation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 128,
      "title": "Rethinking Positional Encoding in Language Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRethinking positional encoding in language pre-training",
      "paper_id": "2006.15595v4"
    },
    {
      "index": 129,
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúOutrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "paper_id": "1701.06538v1"
    },
    {
      "index": 130,
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "paper_id": "2101.03961v3"
    },
    {
      "index": 131,
      "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúParameter-efficient multi-task fine-tuning for transformers via shared hypernetworks",
      "paper_id": "2106.04489v1"
    },
    {
      "index": 132,
      "title": "Instruction Tuning for Large Language Models: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúInstruction tuning for large language models: A survey",
      "paper_id": "2308.10792v8"
    },
    {
      "index": 133,
      "title": "‚ÄúCross-task generalization via natural language crowdsourcing instructions",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 134,
      "title": "‚ÄúSelf-instruct: Aligning language model with self generated instructions",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 135,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 136,
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDeep reinforcement learning from human preferences",
      "paper_id": "1706.03741v4"
    },
    {
      "index": 137,
      "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "paper_id": "2309.00267v3"
    },
    {
      "index": 138,
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDirect preference optimization: Your language model is secretly a reward model",
      "paper_id": "2305.18290v3"
    },
    {
      "index": 139,
      "title": "‚ÄúZero: Memory optimizations toward training trillion parameter models",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": ""
    },
    {
      "index": 140,
      "title": "RWKV: Reinventing RNNs for the Transformer Era",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRwkv: Reinventing rnns for the transformer era",
      "paper_id": "2305.13048v2"
    },
    {
      "index": 141,
      "title": "‚ÄúLora: Low-rank adaptation of large language models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 142,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDistilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 143,
      "title": "‚ÄúKnowledge distillation: A survey",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 144,
      "title": "Survey of Hallucination in Natural Language Generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSurvey of hallucination in natural language generation",
      "paper_id": "2202.03629v7"
    },
    {
      "index": 145,
      "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSources of hallucination by large language models on inference tasks",
      "paper_id": "2305.14552v2"
    },
    {
      "index": 146,
      "title": "‚ÄúROUGE: A package for automatic evaluation of summaries",
      "abstract": "",
      "year": "2004",
      "venue": "",
      "authors": ""
    },
    {
      "index": 147,
      "title": "‚ÄúBleu: a method for automatic evaluation of machine translation",
      "abstract": "",
      "year": "2002",
      "venue": "",
      "authors": ""
    },
    {
      "index": 148,
      "title": "‚ÄúHandling divergent reference texts when evaluating table-to-text generation",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 149,
      "title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTowards faithful neural table-to-text generation with content-matching constraints",
      "paper_id": "2005.00969v1"
    },
    {
      "index": 150,
      "title": "Generating Persona Consistent Dialogues by Exploiting Natural Language Inference",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúGenerating persona consistent dialogues by exploiting natural language inference",
      "paper_id": "1911.05889v4"
    },
    {
      "index": 151,
      "title": "‚Äúq2superscriptùëû2q^{2}: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 152,
      "title": "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúEvaluating attribution in dialogue systems: The BEGIN benchmark",
      "paper_id": "2105.00071v3"
    },
    {
      "index": 153,
      "title": "Rome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRome was built in 1776: A case study on factual correctness in knowledge-grounded response generation",
      "paper_id": "2110.05456v2"
    },
    {
      "index": 154,
      "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúFactscore: Fine-grained atomic evaluation of factual precision in long form text generation",
      "paper_id": "2305.14251v2"
    },
    {
      "index": 155,
      "title": "‚ÄúMachine learning: The high interest credit card of technical debt",
      "abstract": "",
      "year": "2014",
      "venue": "",
      "authors": ""
    },
    {
      "index": 156,
      "title": "‚ÄúAutomatic chain of thought prompting in large language models",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": ""
    },
    {
      "index": 157,
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTree of thoughts: Deliberate problem solving with large language models",
      "paper_id": "2305.10601v2"
    },
    {
      "index": 158,
      "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSelfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "paper_id": "2303.08896v3"
    },
    {
      "index": 159,
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúReflexion: Language agents with verbal reinforcement learning",
      "paper_id": "2303.11366v4"
    },
    {
      "index": 160,
      "title": "‚ÄúExploring the mit mathematics and eecs curriculum using large language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 161,
      "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúPromptchainer: Chaining large language model prompts through visual programming",
      "paper_id": "2203.06566v1"
    },
    {
      "index": 162,
      "title": "Large Language Models are Human-Level Prompt Engineers",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLarge language models are human-level prompt engineers",
      "paper_id": "2211.01910v2"
    },
    {
      "index": 163,
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "abstract": "",
      "year": "2005",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRetrieval-augmented generation for knowledge-intensive NLP tasks",
      "paper_id": "2005.11401v4"
    },
    {
      "index": 164,
      "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRetrieval-augmented generation for large language models: A survey",
      "paper_id": "2312.10997v5"
    },
    {
      "index": 165,
      "title": "2023) Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Accessed: Date of access",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 166,
      "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúUnifying large language models and knowledge graphs: A roadmap",
      "paper_id": "2306.08302v3"
    },
    {
      "index": 167,
      "title": "Active Retrieval Augmented Generation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúActive retrieval augmented generation",
      "paper_id": "2305.06983v2"
    },
    {
      "index": 168,
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúToolformer: Language models can teach themselves to use tools",
      "paper_id": "2302.04761v1"
    },
    {
      "index": 169,
      "title": "‚ÄúArt: Automatic multi-step reasoning and tool-use for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 170,
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "paper_id": "2303.17580v4"
    },
    {
      "index": 171,
      "title": "‚ÄúThe rise and potential of large language model based agents: A survey",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 172,
      "title": "A Survey on Large Language Model based Autonomous Agents",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA survey on large language model based autonomous agents",
      "paper_id": "2308.11432v7"
    },
    {
      "index": 173,
      "title": "‚ÄúAgent ai: Surveying the horizons of multimodal interaction",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 174,
      "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRewoo: Decoupling reasoning from observations for efficient augmented language models",
      "paper_id": "2305.18323v1"
    },
    {
      "index": 175,
      "title": "‚ÄúReact: Synergizing reasoning and acting in language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 176,
      "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúDera: Enhancing large language model completions with dialog-enabled resolving agents",
      "paper_id": "2303.17071v1"
    },
    {
      "index": 177,
      "title": "A Survey on Evaluation of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúA survey on evaluation of large language models",
      "paper_id": "2307.03109v9"
    },
    {
      "index": 178,
      "title": "‚ÄúNatural questions: A benchmark for question answering research",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 179,
      "title": "Measuring Massive Multitask Language Understanding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMeasuring massive multitask language understanding",
      "paper_id": "2009.03300v3"
    },
    {
      "index": 180,
      "title": "Program Synthesis with Large Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúProgram synthesis with large language models",
      "paper_id": "2108.07732v1"
    },
    {
      "index": 181,
      "title": "QuAC : Question Answering in Context",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúQuAC: Question answering in context",
      "paper_id": "1808.07036v3"
    },
    {
      "index": 182,
      "title": "‚ÄúMeasuring coding challenge competence with apps",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 183,
      "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúSeq2sql: Generating structured queries from natural language using reinforcement learning",
      "paper_id": "1709.00103v7"
    },
    {
      "index": 184,
      "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "paper_id": "1705.03551v2"
    },
    {
      "index": 185,
      "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúRACE: Large-scale ReAding comprehension dataset from examinations",
      "paper_id": "1704.04683v5"
    },
    {
      "index": 186,
      "title": "000+ questions for machine comprehension of text",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 187,
      "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
      "abstract": "",
      "year": "1905",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúBoolq: Exploring the surprising difficulty of natural yes/no questions",
      "paper_id": "1905.10044v1"
    },
    {
      "index": 188,
      "title": "‚ÄúLooking beyond the surface:a challenge set for reading comprehension over multiple sentences",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 189,
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTraining verifiers to solve math word problems",
      "paper_id": "2110.14168v2"
    },
    {
      "index": 190,
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMeasuring mathematical problem solving with the MATH dataset",
      "paper_id": "2103.03874v2"
    },
    {
      "index": 191,
      "title": "‚ÄúHellaswag: Can a machine really finish your sentence?‚Äù 2019",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 192,
      "title": "‚ÄúThink you have solved question answering? try arc",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 193,
      "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
      "abstract": "",
      "year": "1911",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúPIQA: reasoning about physical commonsense in natural language",
      "paper_id": "1911.11641v1"
    },
    {
      "index": 194,
      "title": "‚ÄúSocialiqa: Commonsense reasoning about social interactions",
      "abstract": "",
      "year": "1904",
      "venue": "",
      "authors": ""
    },
    {
      "index": 195,
      "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúCan a suit of armor conduct electricity? A new dataset for open book question answering",
      "paper_id": "1809.02789v1"
    },
    {
      "index": 196,
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTruthfulqa: Measuring how models mimic human falsehoods",
      "paper_id": "2109.07958v2"
    },
    {
      "index": 197,
      "title": "R. Salakhutdinov",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": ""
    },
    {
      "index": 198,
      "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúToolqa: A dataset for llm question answering with external tools",
      "paper_id": "2306.13304v1"
    },
    {
      "index": 199,
      "title": "‚ÄúA thorough examination of the cnn/daily mail reading comprehension task",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 200,
      "title": "‚ÄúAbstractive text summarization using sequence-to-sequence rnns and beyond",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": ""
    },
    {
      "index": 201,
      "title": "More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMore than reading comprehension: A survey on datasets and metrics of textual question answering",
      "paper_id": "2109.12264v2"
    },
    {
      "index": 202,
      "title": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúFlowqa: Grasping flow in history for conversational machine comprehension",
      "paper_id": "1810.06683v3"
    },
    {
      "index": 203,
      "title": "‚ÄúA survey on evaluation metrics for machine translation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 204,
      "title": "‚ÄúHalueval: A large-scale hallucination evaluation benchmark for large language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 205,
      "title": "‚ÄúHughes hallucination evaluation model (hhem) leaderboard",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": ""
    },
    {
      "index": 206,
      "title": "Challenges and Applications of Large Language Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúChallenges and applications of large language models",
      "paper_id": "2307.10169v1"
    },
    {
      "index": 207,
      "title": "O. Saarikivi et al",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 208,
      "title": "Textbooks Are All You Need II: phi-1.5 technical report",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTextbooks are all you need ii: phi-1.5 technical report",
      "paper_id": "2309.05463v1"
    },
    {
      "index": 209,
      "title": "‚ÄúHyena hierarchy: Towards larger convolutional language models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 210,
      "title": "‚ÄúStripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 211,
      "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúMonarch mixer: A simple sub-quadratic gemm-based architecture",
      "paper_id": "2310.12109v1"
    },
    {
      "index": 212,
      "title": "‚ÄúFinite mixture models",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": ""
    },
    {
      "index": 213,
      "title": "Visual Instruction Tuning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúVisual instruction tuning",
      "paper_id": "2304.08485v2"
    },
    {
      "index": 214,
      "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúLlava-plus: Learning to use tools for creating multimodal agents",
      "paper_id": "2311.05437v1"
    },
    {
      "index": 215,
      "title": "NExT-GPT: Any-to-Any Multimodal LLM",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúNext-gpt: Any-to-any multimodal llm",
      "paper_id": "2309.05519v3"
    },
    {
      "index": 216,
      "title": "M. Asgari-Chenaghlu",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": ""
    },
    {
      "index": 217,
      "title": "Automated Unit Test Improvement using Large Language Models at Meta",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúAutomated unit test improvement using large language models at meta",
      "paper_id": "2402.09171v1"
    },
    {
      "index": 218,
      "title": "TrustLLM: Trustworthiness in Large Language Models",
      "abstract": "",
      "year": "2024",
      "venue": "",
      "authors": "",
      "orig_title": "‚ÄúTrustllm: Trustworthiness in large language models",
      "paper_id": "2401.05561v6"
    },
    {
      "index": 219,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 220,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 221,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 222,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 223,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 224,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 225,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 226,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 227,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 228,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 229,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 230,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 231,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 232,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 233,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 234,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 235,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 236,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 237,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 238,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 239,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 240,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 241,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    },
    {
      "index": 242,
      "title": "Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "",
      "orig_title": "",
      "paper_id": "2311.14342v2"
    }
  ]
}