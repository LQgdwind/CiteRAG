{
  "paper_id": "2101.09429v1",
  "title": "Explainable Artificial Intelligence Approaches: A Survey",
  "abstract": "Abstract\nThe lack of explainability of a decision from an Artificial Intelligence (AI) based “black box” system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Explanation methods in deep learning: Users, values, concerns and challenges",
      "abstract": "",
      "year": "2018",
      "venue": "Explainable and Interpretable Models in Computer Vision and Machine Learning",
      "authors": "G. Ras, M. van Gerven, and P. Haselager"
    },
    {
      "index": 1,
      "title": "Eu regulations on algorithmic decision-making and a “right to explanation”",
      "abstract": "",
      "year": "2016",
      "venue": "ICML workshop on human interpretability in machine learning (WHI 2016)",
      "authors": "B. Goodman and S. Flaxman"
    },
    {
      "index": 2,
      "title": "Algorithmic accountability",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "B. Wyden"
    },
    {
      "index": 3,
      "title": "Ai ethical principles",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "M. T. Esper"
    },
    {
      "index": 4,
      "title": "Explaining the attributes of a deep learning based intrusion detection system for industrial control networks",
      "abstract": "",
      "year": "2020",
      "venue": "Sensors",
      "authors": "Z. Wang, Y. Lai, Z. Liu, and J. Liu"
    },
    {
      "index": 5,
      "title": "Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1708.08296",
      "authors": "W. Samek, T. Wiegand, and K.-R. Müller",
      "orig_title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models",
      "paper_id": "1708.08296v1"
    },
    {
      "index": 6,
      "title": "Evolutionary fuzzy systems for explainable artificial intelligence: why, when, what for, and where to?",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine",
      "authors": "A. Fernandez, F. Herrera, O. Cordon, M. J. del Jesus, and F. Marcelloni"
    },
    {
      "index": 7,
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "abstract": "",
      "year": "2018",
      "venue": "IEEE Access",
      "authors": "A. Adadi and M. Berrada"
    },
    {
      "index": 8,
      "title": "Explanation in human-ai systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable ai",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.01876",
      "authors": "S. T. Mueller, R. R. Hoffman, W. Clancey, A. Emrey, and G. Klein"
    },
    {
      "index": 9,
      "title": "Quantifying model complexity via functional decomposition for better post-hoc interpretability",
      "abstract": "",
      "year": "2019",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "C. Molnar, G. Casalicchio, and B. Bischl"
    },
    {
      "index": 10,
      "title": "Explanations of model predictions with live and breakDown packages",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.01955",
      "authors": "M. Staniak and P. Biecek",
      "orig_title": "Explanations of model predictions with live and breakdown packages",
      "paper_id": "1804.01955v2"
    },
    {
      "index": 11,
      "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)",
      "authors": "L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal",
      "orig_title": "Explaining explanations: An overview of interpretability of machine learning",
      "paper_id": "1806.00069v3"
    },
    {
      "index": 12,
      "title": "Instance-level explanations for fraud detection: A case study",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1806.07129",
      "authors": "D. Collaris, L. M. Vink, and J. J. van Wijk"
    },
    {
      "index": 13,
      "title": "Explainable artificial intelligence: A survey",
      "abstract": "",
      "year": "2018",
      "venue": "2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",
      "authors": "F. K. Došilović, M. Brčić, and N. Hlupić"
    },
    {
      "index": 14,
      "title": "A survey on explainable artificial intelligence (xai): towards medical xai",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.07374",
      "authors": "E. Tjoa and C. Guan"
    },
    {
      "index": 15,
      "title": "Towards a rigorous science of interpretable machine learning",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1702.08608",
      "authors": "F. Doshi-Velez and B. Kim"
    },
    {
      "index": 16,
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "abstract": "",
      "year": "2019",
      "venue": "Nature Machine Intelligence",
      "authors": "C. Rudin"
    },
    {
      "index": 17,
      "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
      "abstract": "",
      "year": "2020",
      "venue": "Information Fusion",
      "authors": "A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins et al.",
      "orig_title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
      "paper_id": "1910.10045v2"
    },
    {
      "index": 18,
      "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "abstract": "",
      "year": "2018",
      "venue": "Artificial Intelligence",
      "authors": "T. Miller",
      "orig_title": "Explanation in artificial intelligence: Insights from the social sciences",
      "paper_id": "1706.07269v3"
    },
    {
      "index": 19,
      "title": "Visual Interpretability for Deep Learning: a Survey",
      "abstract": "",
      "year": "2018",
      "venue": "Frontiers of Information Technology & Electronic Engineering",
      "authors": "Q.-s. Zhang and S.-C. Zhu",
      "orig_title": "Visual interpretability for deep learning: a survey",
      "paper_id": "1802.00614v2"
    },
    {
      "index": 20,
      "title": "Explaining control strategies in problem solving",
      "abstract": "",
      "year": "1989",
      "venue": "IEEE Intelligent Systems",
      "authors": "B. Chandrasekaran, M. C. Tanner, and J. R. Josephson"
    },
    {
      "index": 21,
      "title": "Explanation in second generation expert systems",
      "abstract": "",
      "year": "1993",
      "venue": "Second Generation Expert Systems",
      "authors": "W. R. Swartout and J. D. Moore"
    },
    {
      "index": 22,
      "title": "Rule-based expert systems: The mycin experiments of the stanford heuristic programming project: Bg buchanan and eh shortliffe,(addison-wesley, reading, ma, 1984); 702 pages",
      "abstract": "",
      "year": "1985",
      "venue": "",
      "authors": "W. R. Swartout"
    },
    {
      "index": 23,
      "title": "Tip: Typifying the interpretability of procedures",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1706.02952",
      "authors": "A. Dhurandhar, V. Iyengar, R. Luss, and K. Shanmugam"
    },
    {
      "index": 24,
      "title": "Assessing the Local Interpretability of Machine Learning Models",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.03501",
      "authors": "S. A. Friedler, C. D. Roy, C. Scheidegger, and D. Slack",
      "orig_title": "Assessing the local interpretability of machine learning models",
      "paper_id": "1902.03501v2"
    },
    {
      "index": 25,
      "title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models",
      "abstract": "",
      "year": "2011",
      "venue": "Decision Support Systems",
      "authors": "J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens"
    },
    {
      "index": 26,
      "title": "Manipulating and Measuring Model Interpretability",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.07810",
      "authors": "F. Poursabzi-Sangdeh, D. G. Goldstein, J. M. Hofman, J. W. Vaughan, and H. Wallach",
      "orig_title": "Manipulating and measuring model interpretability",
      "paper_id": "1802.07810v5"
    },
    {
      "index": 27,
      "title": "Measuring interpretability for different types of machine learning models",
      "abstract": "",
      "year": "2018",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
      "authors": "Q. Zhou, F. Liao, C. Mou, and P. Wang"
    },
    {
      "index": 28,
      "title": "Single family loan level dataset - freddie mac.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 29,
      "title": "Iml-cran package.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": ""
    },
    {
      "index": 30,
      "title": "Interpretable machine learning: A guide for making black box models explainable",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "C. Molnar et al."
    },
    {
      "index": 31,
      "title": "Predictive learning via rule ensembles",
      "abstract": "",
      "year": "2008",
      "venue": "The Annals of Applied Statistics",
      "authors": "J. H. Friedman, B. E. Popescu et al."
    },
    {
      "index": 32,
      "title": "Greedy function approximation: a gradient boosting machine",
      "abstract": "",
      "year": "2001",
      "venue": "Annals of Statistics",
      "authors": "J. H. Friedman"
    },
    {
      "index": 33,
      "title": "Package ‘icebox’",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "A. Goldstein, A. Kapelner, J. Bleich, and M. A. Kapelner"
    },
    {
      "index": 34,
      "title": "Random forests",
      "abstract": "",
      "year": "2001",
      "venue": "Machine Learning",
      "authors": "L. Breiman"
    },
    {
      "index": 35,
      "title": "Model class reliance: Variable importance measures for any machine learning model class, from the “rashomon” perspective",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1801.01489",
      "authors": "A. Fisher, C. Rudin, and F. Dominici"
    },
    {
      "index": 36,
      "title": "“Why Should I Trust You?” Explaining the Predictions of Any Classifier",
      "abstract": "",
      "year": "2016",
      "venue": "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "authors": "M. T. Ribeiro, S. Singh, and C. Guestrin",
      "orig_title": "Why should i trust you?: Explaining the predictions of any classifier",
      "paper_id": "1602.04938v3"
    },
    {
      "index": 37,
      "title": "A value for n-person games",
      "abstract": "",
      "year": "1953",
      "venue": "Contributions to the Theory of Games",
      "authors": "L. S. Shapley"
    },
    {
      "index": 38,
      "title": "An unexpected unity among methods for interpreting model predictions",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1611.07478",
      "authors": "S. Lundberg and S.-I. Lee",
      "orig_title": "An unexpected unity among methods for interpreting model predictions",
      "paper_id": "1611.07478v3"
    },
    {
      "index": 39,
      "title": "Chapter 16 interpretable machine learning — hands-on machine learning with r",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "B. B. . B. Greenwell"
    },
    {
      "index": 40,
      "title": "Causal interpretability for machine learning-problems, methods and evaluation",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGKDD Explorations Newsletter",
      "authors": "R. Moraffah, M. Karami, R. Guo, A. Raglin, and H. Liu"
    },
    {
      "index": 41,
      "title": "Explainability and Adversarial Robustness for RNNs",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.09855",
      "authors": "A. Hartl, M. Bachl, J. Fabini, and T. Zseby",
      "orig_title": "Explainability and adversarial robustness for rnns",
      "paper_id": "1912.09855v2"
    },
    {
      "index": 42,
      "title": "An Adversarial Approach for Explainable AI in Intrusion Detection Systems",
      "abstract": "",
      "year": "2018",
      "venue": "IECON 2018-44th Annual Conference of the IEEE Industrial Electronics Society",
      "authors": "D. L. Marino, C. S. Wickramasinghe, and M. Manic",
      "orig_title": "An adversarial approach for explainable ai in intrusion detection systems",
      "paper_id": "1811.11705v1"
    },
    {
      "index": 43,
      "title": "Examples are not enough, learn to criticize! criticism for interpretability",
      "abstract": "",
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "B. Kim, R. Khanna, and O. O. Koyejo"
    },
    {
      "index": 44,
      "title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1802.07814",
      "authors": "J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan",
      "orig_title": "Learning to explain: An information-theoretic perspective on model interpretation",
      "paper_id": "1802.07814v2"
    },
    {
      "index": 45,
      "title": "An Information-Theoretic Approach to Personalized Explainable Machine Learning",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE Signal Processing Letters",
      "authors": "A. Jung and P. H. J. Nardelli",
      "orig_title": "An information-theoretic approach to personalized explainable machine learning",
      "paper_id": "2003.00484v2"
    },
    {
      "index": 46,
      "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.06104",
      "authors": "M. Ancona, E. Ceolini, C. Öztireli, and M. Gross",
      "orig_title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "paper_id": "1711.06104v4"
    },
    {
      "index": 47,
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.11279",
      "authors": "B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and R. Sayres"
    },
    {
      "index": 48,
      "title": "Knowledge Infused Learning (K-IL): Towards Deep Incorporation of Knowledge in Deep Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1912.00512",
      "authors": "U. Kursuncu, M. Gaur, and A. Sheth",
      "orig_title": "Knowledge infused learning (k-il): Towards deep incorporation of knowledge in deep learning",
      "paper_id": "1912.00512v2"
    },
    {
      "index": 49,
      "title": "Infusing domain knowledge in ai-based” black box” models for better explainability with application in bankruptcy prediction",
      "abstract": "",
      "year": "2019",
      "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Anomaly Detection in Finance Workshop",
      "authors": "S. R. Islam, W. Eberle, S. Bundy, and S. K. Ghafoor"
    },
    {
      "index": 50,
      "title": "Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.09853",
      "authors": "S. R. Islam, W. Eberle, S. K. Ghafoor, A. Siraj, and M. Rogers",
      "orig_title": "Domain knowledge aided explainable artificial intelligence for intrusion detection and response",
      "paper_id": "1911.09853v2"
    },
    {
      "index": 51,
      "title": "A neural network approach for credit risk evaluation",
      "abstract": "",
      "year": "2008",
      "venue": "The Quarterly Review of Economics and Finance",
      "authors": "E. Angelini, G. di Tollo, and A. Roli"
    },
    {
      "index": 52,
      "title": "Five cs of credit.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "J. Segal"
    },
    {
      "index": 53,
      "title": "Introduction to computer security",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "B. Matt et al."
    },
    {
      "index": 54,
      "title": "Rule learning in a nutshell",
      "abstract": "",
      "year": "2012",
      "venue": "Foundations of Rule Learning",
      "authors": "J. Fürnkranz, D. Gamberger, and N. Lavrač"
    },
    {
      "index": 55,
      "title": "Scalable Bayesian Rule Lists",
      "abstract": "",
      "year": "2017",
      "venue": "34th International Conference on Machine Learning-Volume 70",
      "authors": "H. Yang, C. Rudin, and M. Seltzer",
      "orig_title": "Scalable bayesian rule lists",
      "paper_id": "1602.08610v2"
    },
    {
      "index": 56,
      "title": "Learning interpretable models",
      "abstract": "",
      "year": "2006",
      "venue": "",
      "authors": "S. Rüping et al."
    },
    {
      "index": 57,
      "title": "Towards Quantification of Explainability in Explainable Artificial Intelligence Methods",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1911.10104",
      "authors": "S. R. Islam, W. Eberle, and S. K. Ghafoor",
      "orig_title": "Towards quantification of explainability in explainable artificial intelligence methods",
      "paper_id": "1911.10104v1"
    },
    {
      "index": 58,
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information.",
      "abstract": "",
      "year": "1956",
      "venue": "Psychological Review",
      "authors": "G. A. Miller"
    },
    {
      "index": 59,
      "title": "The Mythos of Model Interpretability",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv preprint arXiv:1606.03490",
      "authors": "Z. C. Lipton",
      "orig_title": "The mythos of model interpretability",
      "paper_id": "1606.03490v3"
    },
    {
      "index": 60,
      "title": "Explainable artificial intelligence.",
      "abstract": "",
      "year": "",
      "venue": "",
      "authors": "P. Gandhi"
    }
  ]
}