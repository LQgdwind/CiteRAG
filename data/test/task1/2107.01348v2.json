{
  "paper_id": "2107.01348v2",
  "title": "Examining average and discounted reward optimality criteria in reinforcement learning",
  "abstract": "Abstract\nIn reinforcement learning (RL), the goal is to obtain an optimal policy,\nfor which the optimality criterion is fundamentally important.\nTwo major optimality criteria are average and discounted rewards.\nWhile the latter is more popular, it is problematic to apply in environments\nwithout an inherent notion of discounting.\nThis motivates us to revisit\na) the progression of optimality criteria in dynamic programming,\nb) justification for and complication of an artificial discount factor, and\nc) benefits of directly maximizing the average reward criterion, which is discounting-free.\nOur contributions include a thorough examination of the relationship between\naverage and discounted rewards, as well as a discussion of their pros and cons in RL.\nWe emphasize that average-reward RL methods possess the ingredient and mechanism\nfor applying a family of discounting-free optimality criteria\n(Veinott,, 1969) to RL.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Learning algorithms for Markov decision processes with average cost",
      "abstract": "",
      "year": "2001",
      "venue": "SIAM Journal on Control and Optimization",
      "authors": "Abounadi, J., Bertsekas, D., and Borkar, V. S."
    },
    {
      "index": 1,
      "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1908.00261",
      "authors": "Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G.",
      "orig_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift",
      "paper_id": "1908.00261v5"
    },
    {
      "index": 2,
      "title": "Constrained Markov Decision Processes",
      "abstract": "",
      "year": "1999",
      "venue": "Taylor & Francis",
      "authors": "Altman, E."
    },
    {
      "index": 3,
      "title": "Temporal Representation Learning",
      "abstract": "",
      "year": "2018",
      "venue": "PhD thesis, School of Computer Science, McGill University",
      "authors": "Bacon, P.-L."
    },
    {
      "index": 4,
      "title": "Infinite-horizon policy-gradient estimation",
      "abstract": "",
      "year": "2001",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Baxter, J. and Bartlett, P. L."
    },
    {
      "index": 5,
      "title": "Comparing value-function estimation algorithms in undiscounted problems",
      "abstract": "",
      "year": "1999",
      "venue": "Technical report",
      "authors": "Beleznay, F., Grobler, T., and Szepesvari, C."
    },
    {
      "index": 6,
      "title": "Dynamic Programming",
      "abstract": "",
      "year": "1957",
      "venue": "Princeton University Press",
      "authors": "Bellman, R."
    },
    {
      "index": 7,
      "title": "Discrete dynamic programming",
      "abstract": "",
      "year": "1962",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "Blackwell, D."
    },
    {
      "index": 8,
      "title": "OpenAI Gym",
      "abstract": "",
      "year": "2016",
      "venue": "arXiv:1606.01540",
      "authors": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W."
    },
    {
      "index": 9,
      "title": "Scalable methods for computing state similarity in deterministic Markov Decision Processes",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Castro, P. S.",
      "orig_title": "Scalable methods for computing state similarity in deterministic Markov decision processes",
      "paper_id": "1911.09291v1"
    },
    {
      "index": 10,
      "title": "Simulation-Based Algorithms for Markov Decision Processes",
      "abstract": "",
      "year": "2013",
      "venue": "Springer",
      "authors": "Chang, H. S., Hu, J., Fu, M. C., and Marcus, S. I."
    },
    {
      "index": 11,
      "title": "Adaptive step-sizes for reinforcement learning",
      "abstract": "",
      "year": "2014",
      "venue": "PhD thesis, Computer Science Department, University of Massachusetts Amherst",
      "authors": "Dabney, W. C."
    },
    {
      "index": 12,
      "title": "Policy evaluation with temporal differences: A survey and comparison",
      "abstract": "",
      "year": "2014",
      "venue": "Journal of Machine Learning Research",
      "authors": "Dann, C., Neumann, G., and Peters, J."
    },
    {
      "index": 13,
      "title": "Q-learning with uniformly bounded variance: Large discounting is not a barrier to fast learning",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2002.10301",
      "authors": "Devraj, A. M. and Meyn, S. P."
    },
    {
      "index": 14,
      "title": "Average-reward model-free reinforcement learning: a systematic review and literature mapping",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2010.08920",
      "authors": "Dewanto, V., Dunn, G., Eshragh, A., Gallagher, M., and Roosta, F.",
      "orig_title": "Average-reward model-free reinforcement learning: a systematic review and literature mapping",
      "paper_id": "2010.08920v2"
    },
    {
      "index": 15,
      "title": "Markov Chains",
      "abstract": "",
      "year": "2018",
      "venue": "Springer Series in Operations Research and Financial Engineering, Springer International Publishing",
      "authors": "Douc, R., Moulines, E., Priouret, P., and Soulier, P."
    },
    {
      "index": 16,
      "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.",
      "orig_title": "Benchmarking deep reinforcement learning for continuous control",
      "paper_id": "1604.06778v3"
    },
    {
      "index": 17,
      "title": "A Theoretical Analysis of Deep Q-Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning for Dynamics and Control",
      "authors": "Fan, J., Wang, Z., Xie, Y., and Yang, Z.",
      "orig_title": "A theoretical analysis of deep Q-learning",
      "paper_id": "1901.00137v3"
    },
    {
      "index": 18,
      "title": "Handbook of Markov Decision Processes: Methods and Applications, volume 40",
      "abstract": "",
      "year": "2002",
      "venue": "Springer US",
      "authors": "Feinberg, E. A. and Shwartz, A."
    },
    {
      "index": 19,
      "title": "Methods for computing state similarity in Markov decision processes",
      "abstract": "",
      "year": "2006",
      "venue": "Conference in Uncertainty in Artificial Intelligence",
      "authors": "Ferns, N., Castro, P. S., Precup, D., and Panangaden, P."
    },
    {
      "index": 20,
      "title": "Deep Reinforcement Learning that Matters",
      "abstract": "",
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.",
      "orig_title": "Deep reinforcement learning that matters",
      "paper_id": "1709.06560v3"
    },
    {
      "index": 21,
      "title": "Sensitivity analysis in discounted Markovian decision problems",
      "abstract": "",
      "year": "1985",
      "venue": "Operations Research Spektrum",
      "authors": "Hordijk, A., Dekker, R., and Kallenberg, L. C. M."
    },
    {
      "index": 22,
      "title": "Dynamic Programming and Markov Processes",
      "abstract": "",
      "year": "1960",
      "venue": "Technology Press of the Massachusetts Institute of Technology",
      "authors": "Howard, R. A."
    },
    {
      "index": 23,
      "title": "General discounting versus average reward",
      "abstract": "",
      "year": "2006",
      "venue": "Algorithmic Learning Theory, Springer Berlin Heidelberg",
      "authors": "Hutter, M."
    },
    {
      "index": 24,
      "title": "On structural properties of MDPs that bound loss due to shallow planning",
      "abstract": "",
      "year": "2016",
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": "Jiang, N., Singh, S., and Tewari, A."
    },
    {
      "index": 25,
      "title": "Towards Tight Bounds on the Sample Complexity of Average-reward MDPs",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Machine Learning",
      "authors": "Jin, Y. and Sidford, A.",
      "orig_title": "Towards tight bounds on the sample complexity of average-reward MDPs",
      "paper_id": "2106.07046v1"
    },
    {
      "index": 26,
      "title": "Optimizing average reward using discounted rewards",
      "abstract": "",
      "year": "2001",
      "venue": "Annual Conference on Computational Learning Theory",
      "authors": "Kakade, S."
    },
    {
      "index": 27,
      "title": "Zeroth-order Deterministic Policy Gradient",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv:2006.07314",
      "authors": "Kumar, H., Kalogerias, D. S., Pappas, G. J., and Ribeiro, A.",
      "orig_title": "Zeroth-order deterministic policy gradient",
      "paper_id": "2006.07314v2"
    },
    {
      "index": 28,
      "title": "General time consistent discounting",
      "abstract": "",
      "year": "2014",
      "venue": "Theoretical Computer Science",
      "authors": "Lattimore, T. and Hutter, M."
    },
    {
      "index": 29,
      "title": "Bandit Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Cambridge University Press",
      "authors": "Lattimore, T. and Szepesvári, C."
    },
    {
      "index": 30,
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M. J., and Bowling, M.",
      "orig_title": "Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents",
      "paper_id": "1709.06009v2"
    },
    {
      "index": 31,
      "title": "To discount or not to discount in reinforcement learning: A case study comparing R-learning and Q-learning",
      "abstract": "",
      "year": "1994",
      "venue": "International Conference on Machine Learning",
      "authors": "Mahadevan, S."
    },
    {
      "index": 32,
      "title": "Average reward reinforcement learning: Foundations, algorithms, and empirical results",
      "abstract": "",
      "year": "1996",
      "venue": "Machine Learning",
      "authors": "Mahadevan, S."
    },
    {
      "index": 33,
      "title": "Sensitive discount optimality: Unifying discounted and average reward reinforcement learning",
      "abstract": "",
      "year": "1996",
      "venue": "International Conference on Machine Learning",
      "authors": "Mahadevan, S."
    },
    {
      "index": 34,
      "title": "Q-learning with linear function approximation",
      "abstract": "",
      "year": "2007",
      "venue": "Computational Learning Theory",
      "authors": "Melo, F. S. and Ribeiro, M. I."
    },
    {
      "index": 35,
      "title": "Cognitive and attentional mechanisms in delay of gratification",
      "abstract": "",
      "year": "1972",
      "venue": "Journal of personality and social psychology",
      "authors": "Mischel, W., Ebbesen, E. B., and Zeiss, A. R."
    },
    {
      "index": 36,
      "title": "Derivatives of logarithmic stationary distributions for policy gradient reinforcement learning",
      "abstract": "",
      "year": "2010",
      "venue": "Neural Computation",
      "authors": "Morimura, T., Uchibe, E., Yoshimoto, J., Peters, J., and Doya, K."
    },
    {
      "index": 37,
      "title": "Is the policy gradient a gradient?",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Nota, C. and Thomas, P. S."
    },
    {
      "index": 38,
      "title": "Time Limits in Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Pardo, F., Tavakoli, A., Levdik, V., and Kormushev, P.",
      "orig_title": "Time limits in reinforcement learning",
      "paper_id": "1712.00378v4"
    },
    {
      "index": 39,
      "title": "Fast efficient hyperparameter tuning for policy gradient methods",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Paul, S., Kurin, V., and Whiteson, S."
    },
    {
      "index": 40,
      "title": "Biasing approximate dynamic programming with a lower discount factor",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Petrik, M. and Scherrer, B."
    },
    {
      "index": 41,
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "abstract": "",
      "year": "1994",
      "venue": "John Wiley & Sons, Inc.",
      "authors": "Puterman, M. L."
    },
    {
      "index": 42,
      "title": "A note on measurement of utility",
      "abstract": "",
      "year": "1937",
      "venue": "Review of Economic Studies",
      "authors": "Samuelson, P. A."
    },
    {
      "index": 43,
      "title": "Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view",
      "abstract": "",
      "year": "2010",
      "venue": "International Conference on Machine Learning",
      "authors": "Scherrer, B."
    },
    {
      "index": 44,
      "title": "A reinforcement learning method for maximizing undiscounted rewards",
      "abstract": "",
      "year": "1993",
      "venue": "International Conference on Machine Learning",
      "authors": "Schwartz, A."
    },
    {
      "index": 45,
      "title": "Learning without state-estimation in partially observable Markovian decision processes",
      "abstract": "",
      "year": "1994",
      "venue": "International Conference on Machine Learning",
      "authors": "Singh, S. P., Jaakkola, T. S., and Jordan, M. I."
    },
    {
      "index": 46,
      "title": "Optimum policy regions for Markov processes with discounting",
      "abstract": "",
      "year": "1966",
      "venue": "Operations Research",
      "authors": "Smallwood, R. D."
    },
    {
      "index": 47,
      "title": "A Bayesian framework for reinforcement learning",
      "abstract": "",
      "year": "2000",
      "venue": "International Conference on Machine Learning",
      "authors": "Strens, M. J. A."
    },
    {
      "index": 48,
      "title": "Introduction to Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "MIT Press",
      "authors": "Sutton, R. S. and Barto, A. G."
    },
    {
      "index": 49,
      "title": "Algorithms for Reinforcement Learning",
      "abstract": "",
      "year": "2010",
      "venue": "Morgan & Claypool",
      "authors": "Szepesvári, C."
    },
    {
      "index": 50,
      "title": "On a connection between importance sampling and the likelihood ratio policy gradient",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tang, J. and Abbeel, P."
    },
    {
      "index": 51,
      "title": "Bias in natural actor-critic algorithms",
      "abstract": "",
      "year": "2014",
      "venue": "International Conference on Machine Learning",
      "authors": "Thomas, P."
    },
    {
      "index": 52,
      "title": "Issues in using function approximation for reinforcement learning",
      "abstract": "",
      "year": "1993",
      "venue": "Connectionist Models Summer School",
      "authors": "Thrun, S. and Schwartz, A."
    },
    {
      "index": 53,
      "title": "On average versus discounted reward temporal-difference learning",
      "abstract": "",
      "year": "2002",
      "venue": "Machine Learning",
      "authors": "Tsitsiklis, J. N. and Van Roy, B."
    },
    {
      "index": 54,
      "title": "Insights in Reinforcement Learning: formal analysis and empirical evaluation of temporal-difference learning algorithms",
      "abstract": "",
      "year": "2011",
      "venue": "PhD thesis, Univ. Utrecht",
      "authors": "van Hasselt, H. P."
    },
    {
      "index": 55,
      "title": "Learning and Value Function Approximation in Complex Decision Processes",
      "abstract": "",
      "year": "1998",
      "venue": "PhD thesis, MIT",
      "authors": "Van Roy, B."
    },
    {
      "index": 56,
      "title": "Using a logarithmic mapping to enable lower discount factors in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Van Seijen, H., Fatemi, M., and Tavakoli, A."
    },
    {
      "index": 57,
      "title": "Discrete Dynamic Programming with Sensitive Discount Optimality Criteria",
      "abstract": "",
      "year": "1969",
      "venue": "The Annals of Mathematical Statistics",
      "authors": "Veinott, A. F."
    },
    {
      "index": 58,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "White, M.",
      "orig_title": "Unifying task specification in reinforcement learning",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 59,
      "title": "Meta-Gradient Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Xu, Z., van Hasselt, H. P., and Silver, D.",
      "orig_title": "Meta-gradient reinforcement learning",
      "paper_id": "1805.09801v1"
    },
    {
      "index": 60,
      "title": "A Self-Tuning Actor-Critic Algorithm",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H. P., Silver, D., and Singh, S.",
      "orig_title": "A self-tuning actor-critic algorithm",
      "paper_id": "2002.12928v5"
    }
  ]
}