{
  "paper_id": "2402.04615v3",
  "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
  "abstract": "Abstract\nScreen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction.\nWe introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding.\nOur model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets.\nAt the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements.\nWe use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale.\nWe run ablation studies to demonstrate the impact of these design choices.\nAt only 5B parameters, ScreenAI achieves new state-of-the-art results on UI- and infographics-based tasks (Multipage DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size.\nFinally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.",
  "reference_labels": [
    {
      "index": 0,
      "title": "DUBLIN–document understanding by language-image network",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.14218",
      "authors": "Kriti Aggarwal et al."
    },
    {
      "index": 1,
      "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Armen Aghajanyan et al.",
      "orig_title": "HTLM: Hyper-text pre-training and prompting of language models",
      "paper_id": "2107.06955v1"
    },
    {
      "index": 2,
      "title": "Flamingo: a visual language model for few-shot learning",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Jean-Baptiste Alayrac et al."
    },
    {
      "index": 3,
      "title": "Gemini: a family of highly capable multimodal models",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2312.11805",
      "authors": "Rohan Anil et al."
    },
    {
      "index": 4,
      "title": "PaLM 2 Technical Report",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.10403",
      "authors": "Rohan Anil et al.",
      "orig_title": "PaLM 2 technical report",
      "paper_id": "2305.10403v3"
    },
    {
      "index": 5,
      "title": "UIBert: Learning generic multimodal representations for UI understanding",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Chongyang Bai et al."
    },
    {
      "index": 6,
      "title": "A dataset for interactive vision language navigation with unknown command feasibility",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "Andrea Burns et al."
    },
    {
      "index": 7,
      "title": "End-to-end object detection with transformers",
      "abstract": "",
      "year": "2020",
      "venue": "European conference on computer vision",
      "authors": "Nicolas Carion et al."
    },
    {
      "index": 8,
      "title": "Pix2seq: A Language Modeling Framework for Object Detection",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2109.10852",
      "authors": "Ting Chen et al.",
      "orig_title": "Pix2seq: A language modeling framework for object detection",
      "paper_id": "2109.10852v2"
    },
    {
      "index": 9,
      "title": "WebSRC: A Dataset for Web-Based Structural Reading Comprehension",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Xingyu Chen et al.",
      "orig_title": "WebSRC: A dataset for web-based structural reading comprehension",
      "paper_id": "2101.09465v2"
    },
    {
      "index": 10,
      "title": "\\NAME: A Jointly-Scaled Multilingual Language-Image Model",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.06794",
      "authors": "Xi Chen et al.",
      "orig_title": "PaLi: A jointly-scaled multilingual language-image model",
      "paper_id": "2209.06794v4"
    },
    {
      "index": 11,
      "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2305.18565",
      "authors": "Xi Chen et al.",
      "orig_title": "PaLI-X: On scaling up a multilingual vision and language model",
      "paper_id": "2305.18565v1"
    },
    {
      "index": 12,
      "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2310.09199",
      "authors": "Xi Chen et al.",
      "orig_title": "PaLI-3 vision language models: Smaller, faster, stronger",
      "paper_id": "2310.09199v2"
    },
    {
      "index": 13,
      "title": "Rico: A mobile app dataset for building data-driven design applications",
      "abstract": "",
      "year": "2017",
      "venue": "30th annual ACM symposium on user interface software and technology",
      "authors": "Biplab Deka et al."
    },
    {
      "index": 14,
      "title": "Mind2web: Towards a generalist agent for the web",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2306.06070",
      "authors": "Xiang Deng et al."
    },
    {
      "index": 15,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.11929",
      "authors": "Alexey Dosovitskiy et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 16,
      "title": "TaTA: A Multilingual Table-to-Text Dataset for African Languages",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Sebastian Gehrmann et al.",
      "orig_title": "Tata: A multilingual table-to-text dataset for african languages",
      "paper_id": "2211.00142v1"
    },
    {
      "index": 17,
      "title": "Understanding HTML with Large Language Models",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.03945",
      "authors": "Izzeddin Gur et al.",
      "orig_title": "Understanding HTML with large language models",
      "paper_id": "2210.03945v2"
    },
    {
      "index": 18,
      "title": "ActionBert: Leveraging user actions for semantic understanding of user interfaces",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Zecheng He et al."
    },
    {
      "index": 19,
      "title": "ScreenQA: Large-scale question-answer pairs over mobile app screenshots",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.08199",
      "authors": "Yu-Chung Hsiao et al."
    },
    {
      "index": 20,
      "title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking",
      "abstract": "",
      "year": "2022",
      "venue": "30th ACM International Conference on Multimedia",
      "authors": "Yupan Huang et al.",
      "orig_title": "LayoutLMv3: Pre-training for document ai with unified text and image masking",
      "paper_id": "2204.08387v3"
    },
    {
      "index": 21,
      "title": "DVQA: Understanding Data Visualizations via Question Answering",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Kushal Kafle et al.",
      "orig_title": "Dvqa: Understanding data visualizations via question answering",
      "paper_id": "1801.08163v2"
    },
    {
      "index": 22,
      "title": "PreSTU: Pre-Training for Scene-Text Understanding",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF International Conference on Computer Vision",
      "authors": "Jihyung Kil et al.",
      "orig_title": "PreSTU: Pre-training for scene-text understanding",
      "paper_id": "2209.05534v3"
    },
    {
      "index": 23,
      "title": "Donut: Document understanding transformer without OCR",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2111.15664",
      "authors": "Geewook Kim et al."
    },
    {
      "index": 24,
      "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2303.16839",
      "authors": "Weicheng Kuo et al.",
      "orig_title": "MaMMUT: A simple architecture for joint learning for multimodal tasks",
      "paper_id": "2303.16839v3"
    },
    {
      "index": 25,
      "title": "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
      "abstract": "",
      "year": "2023",
      "venue": "International Conference on Machine Learning",
      "authors": "Kenton Lee et al."
    },
    {
      "index": 26,
      "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2209.14927",
      "authors": "Gang Li and Yang Li",
      "orig_title": "Spotlight: Mobile UI understanding using vision-language models with a focus",
      "paper_id": "2209.14927v4"
    },
    {
      "index": 27,
      "title": "Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Yang Li et al.",
      "orig_title": "Widget captioning: Generating natural language description for mobile user interface elements",
      "paper_id": "2010.04295v1"
    },
    {
      "index": 28,
      "title": "VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2112.05692",
      "authors": "Yang Li et al.",
      "orig_title": "VUT: Versatile ui transformer for multi-modal multi-task user interface modeling",
      "paper_id": "2112.05692v1"
    },
    {
      "index": 29,
      "title": "Learning to Denoise Raw Mobile UI Layouts for Improving Datasets at Scale",
      "abstract": "",
      "year": "2022",
      "venue": "2022 CHI Conference on Human Factors in Computing Systems",
      "authors": "Gang Li et al.",
      "orig_title": "Learning to denoise raw mobile UI layouts for improving datasets at scale",
      "paper_id": "2201.04100v2"
    },
    {
      "index": 30,
      "title": "Mug: Interactive Multimodal Grounding on User Interfaces",
      "abstract": "",
      "year": "2022",
      "venue": "",
      "authors": "Tao Li et al.",
      "orig_title": "MUG: Interactive multimodal grounding on user interfaces",
      "paper_id": "2209.15099v1"
    },
    {
      "index": 31,
      "title": "MatCha : Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2212.09662",
      "authors": "Fangyu Liu et al.",
      "orig_title": "MatCha: Enhancing visual language pretraining with math reasoning and chart derendering",
      "paper_id": "2212.09662v2"
    },
    {
      "index": 32,
      "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Fangyu Liu et al.",
      "orig_title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
      "paper_id": "2212.10505v2"
    },
    {
      "index": 33,
      "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2203.10244",
      "authors": "Ahmed Masry et al.",
      "orig_title": "ChartQA: A benchmark for question answering about charts with visual and logical reasoning",
      "paper_id": "2203.10244v1"
    },
    {
      "index": 34,
      "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Ahmed Masry et al.",
      "orig_title": "Unichart: A universal vision-language pretrained model for chart comprehension and reasoning",
      "paper_id": "2305.14761v3"
    },
    {
      "index": 35,
      "title": "DocVQA: A Dataset for VQA on Document Images",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE/CVF winter conference on applications of computer vision",
      "authors": "Minesh Mathew et al.",
      "orig_title": "DocVQA: A dataset for VQA on document images",
      "paper_id": "2007.00398v3"
    },
    {
      "index": 36,
      "title": "InfographicVQA",
      "abstract": "",
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision",
      "authors": "Minesh Mathew et al.",
      "orig_title": "InfographicVQA",
      "paper_id": "2104.12756v2"
    },
    {
      "index": 37,
      "title": "PlotQA: Reasoning over Scientific Plots",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Nitesh Methani et al.",
      "orig_title": "PlotQA: Reasoning over scientific plots",
      "paper_id": "1909.00997v3"
    },
    {
      "index": 38,
      "title": "OCR-VQA: Visual question answering by reading text in images",
      "abstract": "",
      "year": "2019",
      "venue": "ICDAR",
      "authors": "Anand Mishra et al."
    },
    {
      "index": 39,
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2112.09332",
      "authors": "Reiichiro Nakano et al.",
      "orig_title": "WebGPT: Browser-assisted question-answering with human feedback",
      "paper_id": "2112.09332v3"
    },
    {
      "index": 40,
      "title": "Going full-tilt boogie on document understanding with text-image-layout transformer",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": "Rafał Powalski et al."
    },
    {
      "index": 41,
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Colin Raffel et al.",
      "orig_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "paper_id": "1910.10683v4"
    },
    {
      "index": 42,
      "title": "SQuAD: 100,000+ questions for machine comprehension of text",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Pranav Rajpurkar et al."
    },
    {
      "index": 43,
      "title": "Android in the Wild: A Large-Scale Dataset for Android Device Control",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2307.10088",
      "authors": "Christopher Rawles et al.",
      "orig_title": "Android in the wild: A large-scale dataset for android device control",
      "paper_id": "2307.10088v2"
    },
    {
      "index": 44,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "authors": "Piyush Sharma et al."
    },
    {
      "index": 45,
      "title": "Towards Better Semantic Understanding of Mobile Interfaces",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv preprint arXiv:2210.02663",
      "authors": "Srinivas Sunkara et al.",
      "orig_title": "Towards better semantic understanding of mobile interfaces",
      "paper_id": "2210.02663v1"
    },
    {
      "index": 46,
      "title": "Unifying Vision, Text, and Layout for Universal Document Processing",
      "abstract": "",
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "Zineng Tang et al.",
      "orig_title": "Unifying vision, text, and layout for universal document processing",
      "paper_id": "2212.02623v3"
    },
    {
      "index": 47,
      "title": "UL2: Unifying Language Learning Paradigms",
      "abstract": "",
      "year": "2022",
      "venue": "Eleventh International Conference on Learning Representations",
      "authors": "Yi Tay et al.",
      "orig_title": "UL2: Unifying language learning paradigms",
      "paper_id": "2205.05131v3"
    },
    {
      "index": 48,
      "title": "Hierarchical multimodal transformers for Multi-Page DocVQA",
      "abstract": "",
      "year": "2023",
      "venue": "Pattern Recognition",
      "authors": "Rubèn Tito et al.",
      "orig_title": "Hierarchical multimodal transformers for multipage DocVQA",
      "paper_id": "2212.05935v2"
    },
    {
      "index": 49,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in neural information processing systems",
      "authors": "Ashish Vaswani et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 50,
      "title": "CIDEr: Consensus-based Image Description Evaluation",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Ramakrishna Vedantam et al.",
      "orig_title": "CIDEr: Consensus-based image description evaluation",
      "paper_id": "1411.5726v2"
    },
    {
      "index": 51,
      "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
      "abstract": "",
      "year": "2021",
      "venue": "34th Annual ACM Symposium on User Interface Software and Technology",
      "authors": "Bryan Wang et al.",
      "orig_title": "Screen2words: Automatic mobile ui summarization with multimodal learning",
      "paper_id": "2108.03353v1"
    },
    {
      "index": 52,
      "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "authors": "Peng Wang et al.",
      "orig_title": "OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
      "paper_id": "2202.03052v2"
    },
    {
      "index": 53,
      "title": "DocLLM: A layout-aware generative language model for multimodal document understanding",
      "abstract": "",
      "year": "2023",
      "venue": "arXiv preprint arXiv:2401.00908",
      "authors": "Dongsheng Wang et al."
    },
    {
      "index": 54,
      "title": "Resolving referring expressions in images with labeled elements",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)",
      "authors": "Nevan Wichers et al.",
      "orig_title": "Resolving referring expressions in images with labeled elements",
      "paper_id": "1810.10165v2"
    },
    {
      "index": 55,
      "title": "Screen parsing: Towards reverse engineering of ui models from screenshots",
      "abstract": "",
      "year": "2021",
      "venue": "34th Annual ACM Symposium on User Interface Software and Technology",
      "authors": "Jason Wu et al."
    },
    {
      "index": 56,
      "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Jialin Wu et al.",
      "orig_title": "Omni-SMoLA: Boosting generalist multimodal models with soft mixture of low-rank experts",
      "paper_id": "2312.00968v2"
    },
    {
      "index": 57,
      "title": "DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering",
      "abstract": "",
      "year": "2023",
      "venue": "",
      "authors": "Sijin Wu et al.",
      "orig_title": "DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering",
      "paper_id": "2308.10959v2"
    },
    {
      "index": 58,
      "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2010.11934",
      "authors": "Linting Xue et al.",
      "orig_title": "mT5: A massively multilingual pre-trained text-to-text transformer",
      "paper_id": "2010.11934v3"
    },
    {
      "index": 59,
      "title": "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling",
      "abstract": "",
      "year": "2022",
      "venue": "European Conference on Computer Vision",
      "authors": "Zhengyuan Yang et al.",
      "orig_title": "UniTAB: Unifying text and box outputs for grounded vision-language modeling",
      "paper_id": "2111.12085v2"
    },
    {
      "index": 60,
      "title": "Multimodal Icon Annotation For Mobile Applications",
      "abstract": "",
      "year": "2021",
      "venue": "23rd International Conference on Mobile Human-Computer Interaction",
      "authors": "Xiaoxue Zang et al.",
      "orig_title": "Multimodal icon annotation for mobile applications",
      "paper_id": "2107.04452v1"
    },
    {
      "index": 61,
      "title": "Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels",
      "abstract": "",
      "year": "2021",
      "venue": "2021 CHI Conference on Human Factors in Computing Systems",
      "authors": "Xiaoyi Zhang et al.",
      "orig_title": "Screen recognition: Creating accessibility metadata for mobile applications from pixels",
      "paper_id": "2101.04893v1"
    }
  ]
}