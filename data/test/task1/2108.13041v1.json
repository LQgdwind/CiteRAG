{
  "paper_id": "2108.13041v1",
  "title": "Auto-Split: A General Framework of Collaborative Edge-Cloud AI",
  "abstract": "Abstract.\nIn many industry scale applications, large and resource consuming machine learning models reside in powerful cloud servers. At the same time, large amounts of input data are collected at the edge of cloud. The inference results are also communicated to users or passed to downstream tasks at the edge. The edge often consists of a large number of low-power devices. It is a big challenge to design industry products to support sophisticated deep model deployment and conduct model inference in an efficient manner so that the model accuracy remains high and the end-to-end latency is kept low. This paper describes the techniques and engineering practice behind Auto-Split, an edge-cloud collaborative prototype of Huawei Cloud. This patented technology is already validated on selected applications, is on its way for broader systematic edge-cloud application integration, and is being made available for public use as an automated pipeline service for end-to-end cloud-edge collaborative intelligence deployment. To the best of our knowledge, there is no existing industry product that provides the capability of Deep Neural Network (DNN) splitting. 111Code and demo are available at: https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=5fad1eb4-50b2-4ac9-bcb0-a1f744cf85c7",
  "reference_labels": [
    {
      "index": 0,
      "title": "ModelArts: Deploying a Model as an Edge Service",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 1,
      "title": "Google Anthos",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 2,
      "title": "Alibaba Edge Computing APIs",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 3,
      "title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "R. Banner, Y. Nahshan, E. Hoffer, and D. Soudry"
    },
    {
      "index": 4,
      "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "R. Banner, Y. Nahshan, and D. Soudry",
      "orig_title": "Post training 4-bit quantization of convolutional networks for rapid-deployment",
      "paper_id": "1810.05723v3"
    },
    {
      "index": 5,
      "title": "Language Models are Few-Shot Learners",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint",
      "authors": "T. B. Brown et al.",
      "orig_title": "Language models are few-shot learners",
      "paper_id": "2005.14165v4"
    },
    {
      "index": 6,
      "title": "ZeroQ: A Novel Zero Shot Quantization Framework",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF CVPR",
      "authors": "Y. Cai, Zh. Yao, Zh. Dong, et al.",
      "orig_title": "ZeroQ: A Novel Zero Shot Quantization Framework",
      "paper_id": "2001.00281v1"
    },
    {
      "index": 7,
      "title": "Software-Defined Camera",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 8,
      "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE Micro",
      "authors": "Y.H. Chen, J. Emer, and V. Sze"
    },
    {
      "index": 9,
      "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv",
      "authors": "Y. Cheng, D. Wang, P. Zhou, and T. Zhang",
      "orig_title": "A survey of model compression and acceleration for deep neural networks",
      "paper_id": "1710.09282v9"
    },
    {
      "index": 10,
      "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "J. Choi, Z. Wang, S. Venkataramani, et al."
    },
    {
      "index": 11,
      "title": "PREMA: A Predictive Multi-task Scheduling Algorithm For Preemptible Neural Processing Units",
      "abstract": "",
      "year": "2020",
      "venue": "HPCA",
      "authors": "Yujeong Choi and Minsoo Rhu",
      "orig_title": "Prema: A predictive multi-task scheduling algorithm for preemptible neural processing units",
      "paper_id": "1909.04548v1"
    },
    {
      "index": 12,
      "title": "Low-bit Quantization of Neural Networks for Efficient Inference",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF ICCVW",
      "authors": "Y. Choukroun, E. Kravchik, and P. Kisilev"
    },
    {
      "index": 13,
      "title": "Alibaba Edge Computing",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 14,
      "title": "ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "X. Dai, P. Zhang, B. Wu, et al.",
      "orig_title": "ChamNet: Towards Efficient Network Design Through Platform-Aware Model Adaptation",
      "paper_id": "1812.08934v1"
    },
    {
      "index": 15,
      "title": "ModelArts Pro Model Deployment",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 16,
      "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Zh. Dong, Zh. Yao, A. Gholami, et al."
    },
    {
      "index": 17,
      "title": "MindX Edge",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 18,
      "title": "Learned Step Size Quantization",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "S. Esser, J. McKinstry, et al.",
      "orig_title": "Learned step size quantization",
      "paper_id": "1902.08153v3"
    },
    {
      "index": 19,
      "title": "Astraea: Deploy AI Services at the Edge in Elegant Ways",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE International Conference on Edge Computing (EDGE)",
      "authors": "Zh. Fu, J. Yang, Ch. Bai, et al."
    },
    {
      "index": 20,
      "title": "Rethinking Pruning for Accelerating Deep Inference At the Edge",
      "abstract": "",
      "year": "2020",
      "venue": "ACM SIGKDD",
      "authors": "D. Gao, X. He, Z. Zhou, et al."
    },
    {
      "index": 21,
      "title": "PULP-NN: Accelerating Quantized Neural Networks on Parallel Ultra-Low-Power RISC-V Processors",
      "abstract": "",
      "year": "2020",
      "venue": "Philosophical Transactions of the Royal Society A",
      "authors": "Angelo Garofalo, Manuele Rusci, Francesco Conti, Davide Rossi, and Luca Benini",
      "orig_title": "PULP-NN: accelerating quantized neural networks on parallel ultra-low-power RISC-V processors",
      "paper_id": "1908.11263v1"
    },
    {
      "index": 22,
      "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding",
      "abstract": "",
      "year": "2016",
      "venue": "CoRR",
      "authors": "S. Han, H. Mao, and W. Dally"
    },
    {
      "index": 23,
      "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Y. He, X. Zhang, and J. Sun",
      "orig_title": "Channel Pruning for Accelerating Very Deep Neural Networks",
      "paper_id": "1707.06168v2"
    },
    {
      "index": 24,
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "D. Hendrycks and Th. Dietterich",
      "orig_title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "paper_id": "1903.12261v1"
    },
    {
      "index": 25,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "ArXiv",
      "authors": "G. E. Hinton, O. Vinyals, and J. Dean",
      "orig_title": "Distilling the Knowledge in a Neural Network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 26,
      "title": "Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE INFOCOM",
      "authors": "Ch. Hu, W. Bao, D. Wang, and F. Liu"
    },
    {
      "index": 27,
      "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Gao Huang, Danlu Chen, T. Li, et al.",
      "orig_title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
      "paper_id": "1703.09844v5"
    },
    {
      "index": 28,
      "title": "Intel Nervana. Nervana’s Early Exit Inference",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Intel"
    },
    {
      "index": 29,
      "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "B. Jacob, S. Kligys, Bo Chen, et al.",
      "orig_title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
      "paper_id": "1712.05877v1"
    },
    {
      "index": 30,
      "title": "Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge",
      "abstract": "",
      "year": "2017",
      "venue": "ASPLOS",
      "authors": "Y. Kang, J. Hauswald, C. Gao, et al."
    },
    {
      "index": 31,
      "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "R. Krishnamoorthi",
      "orig_title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
      "paper_id": "1806.08342v1"
    },
    {
      "index": 32,
      "title": "SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud",
      "abstract": "",
      "year": "2020",
      "venue": "MobiCom",
      "authors": "Stefanos Laskaridis, Stylianos I. Venieris, et al.",
      "orig_title": "SPINN: synergistic progressive inference of neural networks over device and cloud",
      "paper_id": "2008.06402v2"
    },
    {
      "index": 33,
      "title": "Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing",
      "abstract": "",
      "year": "2020",
      "venue": "TWC",
      "authors": "En Li, Liekang Zeng, , et al.",
      "orig_title": "Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing",
      "paper_id": "1910.05316v1"
    },
    {
      "index": 34,
      "title": "Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint",
      "authors": "C. Michaelis, B. Mitzkus, R. Geirhos, et al.",
      "orig_title": "Benchmarking robustness in object detection: Autonomous driving when winter is coming",
      "paper_id": "1907.07484v2"
    },
    {
      "index": 35,
      "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "A. Mishra and D. Marr",
      "orig_title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
      "paper_id": "1711.05852v1"
    },
    {
      "index": 36,
      "title": "Loss Aware Post-training Quantization",
      "abstract": "",
      "year": "2019",
      "venue": "ArXiv",
      "authors": "Y. Nahshan, B. Chmiel, Ch. Baskin, et al.",
      "orig_title": "Loss Aware Post-training Quantization",
      "paper_id": "1911.07190v2"
    },
    {
      "index": 37,
      "title": "Amazon SageMaker Neo",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 38,
      "title": "Baidu’s PaddlePaddle",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 39,
      "title": "Timeloop: A systematic approach to dnn accelerator evaluation",
      "abstract": "",
      "year": "2019",
      "venue": "ISPASS",
      "authors": "Angshuman Parashar et al."
    },
    {
      "index": 40,
      "title": "HiLens Platform and Edge Device",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 41,
      "title": "Tencent PocketFlow",
      "abstract": "",
      "year": "2021",
      "venue": "",
      "authors": ""
    },
    {
      "index": 42,
      "title": "Model Compression via Distillation and Quantization",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "A. Polino, R. Pascanu, and Dan Alistarh",
      "orig_title": "Model compression via distillation and quantization",
      "paper_id": "1802.05668v1"
    },
    {
      "index": 43,
      "title": "Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers",
      "abstract": "",
      "year": "2020",
      "venue": "ArXiv",
      "authors": "M. Rusci, A. Capotondi, and L. Benini",
      "orig_title": "Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers",
      "paper_id": "1905.13082v1"
    },
    {
      "index": 44,
      "title": "SCALE-Sim: Systolic CNN Accelerator",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "A. Samajdar, Y. Zhu, P. Whatmough, et al."
    },
    {
      "index": 45,
      "title": "Efficient bit allocation for an arbitrary set of quantizers",
      "abstract": "",
      "year": "1988",
      "venue": "IEEE Trans. Acoustics, Speech, and Signal Processing",
      "authors": "Y. Shoham and A. Gersho"
    },
    {
      "index": 46,
      "title": "Platform-Aware Neural Architecture Search for Mobile",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "M. Tan, Bo Chen, R. Pang, V. Vasudevan, and Quoc V. Le"
    },
    {
      "index": 47,
      "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "ICPR",
      "authors": "S. Teerapittayanon, B. McDanel, and H.-T. Kung",
      "orig_title": "Branchynet: Fast inference via early exiting from deep neural networks",
      "paper_id": "1709.01686v1"
    },
    {
      "index": 48,
      "title": "Tenstorrent’s Grayskull AI Chip",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "Tenstorrent"
    },
    {
      "index": 49,
      "title": "Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud",
      "abstract": "",
      "year": "2018",
      "venue": "ACM SIGKDD",
      "authors": "J. Wang, J. Zhang, W. Bao, et al.",
      "orig_title": "Not just privacy: Improving performance of private deep learning in mobile cloud",
      "paper_id": "1809.03428v3"
    },
    {
      "index": 50,
      "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "K. Wang, Zh. Liu, Y. Lin, J. Lin, and Song H.",
      "orig_title": "Haq: Hardware-aware automated quantization with mixed precision",
      "paper_id": "1811.08886v3"
    },
    {
      "index": 51,
      "title": "Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "B. Wu, Y. Wang, P. Zhang, et al.",
      "orig_title": "Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search",
      "paper_id": "1812.00090v1"
    },
    {
      "index": 52,
      "title": "Accelergy: An architecture-level energy estimation methodology for accelerator designs",
      "abstract": "",
      "year": "2019",
      "venue": "ICCAD",
      "authors": "Y. N. Wu, J. S. Emer, and V. Sze"
    },
    {
      "index": 53,
      "title": "Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint",
      "authors": "Haichuan Yang et al.",
      "orig_title": "Energy-constrained compression for deep neural networks via weighted sparse projection and layer input masking",
      "paper_id": "1806.04321v3"
    },
    {
      "index": 54,
      "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "abstract": "",
      "year": "2018",
      "venue": "ArXiv",
      "authors": "T.-J. Yang, A. Howard, B. Chen, et al.",
      "orig_title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "paper_id": "1804.03230v2"
    },
    {
      "index": 55,
      "title": "Lq-nets: Learned quantization for highly accurate and compact deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "European conference on computer vision (ECCV)",
      "authors": "Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua"
    },
    {
      "index": 56,
      "title": "SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Linfeng Zhang, Zhanhong Tan, et al."
    },
    {
      "index": 57,
      "title": "Towards Real-time Cooperative Deep Inference over the Cloud and Edge End Devices",
      "abstract": "",
      "year": "2020",
      "venue": "IMWUT",
      "authors": "Shigeng Zhang, Yinggang Li, et al."
    },
    {
      "index": 58,
      "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting",
      "abstract": "",
      "year": "2019",
      "venue": "ICML",
      "authors": "R. Zhao, Y. Hu, J. Dotzel, C. D. Sa, and Z. Zhang"
    },
    {
      "index": 59,
      "title": "Towards Effective 2-bit Quantization: Pareto-optimal Bit Allocation for Deep CNNs Compression",
      "abstract": "",
      "year": "2020",
      "venue": "",
      "authors": "W. Zhe, J. Lin, M. M. Sabry Aly, S. Young, V. Chandrasekhar, and B. Girod"
    },
    {
      "index": 60,
      "title": "Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "H.Y. Zhou, B.B. Gao, and J. Wu",
      "orig_title": "Adaptive feeding: Achieving fast and accurate detections by adaptively combining object detectors",
      "paper_id": "1707.06399v1"
    },
    {
      "index": 61,
      "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients",
      "abstract": "",
      "year": "2016",
      "venue": "ArXiv",
      "authors": "Sh. Zhou, Z. Ni, et al.",
      "orig_title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients",
      "paper_id": "1606.06160v3"
    },
    {
      "index": 62,
      "title": "Neural Network Distiller: A Python Package For DNN Compression Research",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "N. Zmora, G. Jacob, et al.",
      "orig_title": "Neural Network Distiller: A Python Package For DNN Compression Research",
      "paper_id": "1910.12232v1"
    },
    {
      "index": 63,
      "title": "Neural Network Distiller: A Python Package For DNN Compression Research",
      "abstract": "",
      "year": "2019",
      "venue": "(October",
      "authors": "N. Zmora, G. Jacob,\net al. 2019.",
      "orig_title": "Neural Network Distiller: A Python Package For DNN Compression Research",
      "paper_id": "1910.12232v1"
    }
  ]
}