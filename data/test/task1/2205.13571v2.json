{
  "paper_id": "2205.13571v2",
  "title": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
  "abstract": "Abstract\nNeural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training.\nTo derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy.\nThe efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Optimization algorithms on matrix manifolds",
      "abstract": "",
      "year": "2009",
      "venue": "Princeton University Press",
      "authors": "P.-A. Absil, R. Mahony, and R. Sepulchre"
    },
    {
      "index": 1,
      "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani",
      "orig_title": "N2n learning: Network to network compression via policy gradient reinforcement learning",
      "paper_id": "1709.06030v2"
    },
    {
      "index": 2,
      "title": "What is the state of neural network pruning?",
      "abstract": "",
      "year": "2020",
      "venue": "machine learning and systems",
      "authors": "D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag"
    },
    {
      "index": 3,
      "title": "A rank-adaptive robust integrator for dynamical low-rank approximation",
      "abstract": "",
      "year": "2022",
      "venue": "BIT Numerical Mathematics",
      "authors": "G. Ceruti, J. Kusch, and C. Lubich",
      "orig_title": "A rank-adaptive robust integrator for dynamical low-rank approximation",
      "paper_id": "2104.05247v1"
    },
    {
      "index": 4,
      "title": "Time integration of symmetric and anti-symmetric low-rank matrices and Tucker tensors",
      "abstract": "",
      "year": "2020",
      "venue": "BIT Numerical Mathematics",
      "authors": "G. Ceruti and C. Lubich",
      "orig_title": "Time integration of symmetric and anti-symmetric low-rank matrices and Tucker tensors",
      "paper_id": "1906.01369v1"
    },
    {
      "index": 5,
      "title": "An unconventional robust integrator for dynamical low-rank approximation",
      "abstract": "",
      "year": "2022",
      "venue": "BIT. Numerical Mathematics",
      "authors": "G. Ceruti and C. Lubich",
      "orig_title": "An unconventional robust integrator for dynamical low-rank approximation",
      "paper_id": "2010.02022v1"
    },
    {
      "index": 6,
      "title": "Rank-adaptive time integration of tree tensor networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2201.10291",
      "authors": "G. Ceruti, C. Lubich, and D. Sulz",
      "orig_title": "Rank-adaptive time integration of tree tensor networks",
      "paper_id": "2201.10291v2"
    },
    {
      "index": 7,
      "title": "Time integration of tree tensor networks",
      "abstract": "",
      "year": "2021",
      "venue": "SIAM Journal on Numerical Analysis",
      "authors": "G. Ceruti, C. Lubich, and H. Walach",
      "orig_title": "Time integration of tree tensor networks",
      "paper_id": "2002.11392v2"
    },
    {
      "index": 8,
      "title": "An exploration of parameter redundancy in deep networks with circulant projections",
      "abstract": "",
      "year": "2015",
      "venue": "IEEE international conference on computer vision",
      "authors": "Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang"
    },
    {
      "index": 9,
      "title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1",
      "abstract": "",
      "year": "2016",
      "venue": "neural information processing systems",
      "authors": "M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio"
    },
    {
      "index": 10,
      "title": "The MNIST database of handwritten digit images for machine learning research",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE Signal Processing Magazine",
      "authors": "L. Deng"
    },
    {
      "index": 11,
      "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
      "abstract": "",
      "year": "2014",
      "venue": "neural information processing systems",
      "authors": "E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus",
      "orig_title": "Exploiting linear structure within convolutional networks for efficient evaluation",
      "paper_id": "1404.0736v2"
    },
    {
      "index": 12,
      "title": "On smooth decompositions of matrices",
      "abstract": "",
      "year": "1999",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "L. Dieci and T. Eirola"
    },
    {
      "index": 13,
      "title": "The principles of quantum mechanics",
      "abstract": "",
      "year": "1981",
      "venue": "Oxford university press",
      "authors": "P. A. M. Dirac et al."
    },
    {
      "index": 14,
      "title": "Rank Diminishing in Deep Neural Networks",
      "abstract": "",
      "year": "2022",
      "venue": "arXiv:2206.06072",
      "authors": "R. Feng, K. Zheng, Y. Huang, D. Zhao, M. Jordan, and Z.-J. Zha",
      "orig_title": "Rank diminishing in deep neural networks",
      "paper_id": "2206.06072v1"
    },
    {
      "index": 15,
      "title": "A geometric approach to dynamical model order reduction",
      "abstract": "",
      "year": "2018",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "F. Feppon and P. F. Lermusiaux"
    },
    {
      "index": 16,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Frankle and M. Carbin",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 17,
      "title": "Wave mechanics, advanced general theory, volume 1",
      "abstract": "",
      "year": "1934",
      "venue": "Oxford",
      "authors": "J. Frenkel"
    },
    {
      "index": 18,
      "title": "A riemannian rank-adaptive method for low-rank matrix completion",
      "abstract": "",
      "year": "2022",
      "venue": "Computational Optimization and Applications",
      "authors": "B. Gao and P.-A. Absil"
    },
    {
      "index": 19,
      "title": "Dynamic Network Surgery for Efficient DNNs",
      "abstract": "",
      "year": "2016",
      "venue": "neural information processing systems",
      "authors": "Y. Guo, A. Yao, and Y. Chen",
      "orig_title": "Dynamic network surgery for efficient dnns",
      "paper_id": "1608.04493v2"
    },
    {
      "index": 20,
      "title": "Solving ordinary differential equations. I. Nonstiff problems, volume 8 of Springer Series in Computational Mathematics",
      "abstract": "",
      "year": "1993",
      "venue": "Springer-Verlag",
      "authors": "E. Hairer, S. P. NÃ¸rsett, and G. Wanner"
    },
    {
      "index": 21,
      "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "",
      "authors": "Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang",
      "orig_title": "Soft filter pruning for accelerating deep convolutional neural networks",
      "paper_id": "1808.06866v1"
    },
    {
      "index": 22,
      "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "abstract": "",
      "year": "2018",
      "venue": "European conference on computer vision",
      "authors": "Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han",
      "orig_title": "AMC: AutoML for model compression and acceleration on mobile devices",
      "paper_id": "1802.03494v4"
    },
    {
      "index": 23,
      "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "Y. He, X. Zhang, and J. Sun",
      "orig_title": "Channel pruning for accelerating very deep neural networks",
      "paper_id": "1707.06168v2"
    },
    {
      "index": 24,
      "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Y. He, X. Zhang, and J. Sun",
      "orig_title": "Channel pruning for accelerating very deep neural networks",
      "paper_id": "1707.06168v2"
    },
    {
      "index": 25,
      "title": "Densely connected convolutional networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger"
    },
    {
      "index": 26,
      "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "European conference on computer vision (ECCV)",
      "authors": "Z. Huang and N. Wang",
      "orig_title": "Data-driven sparse structure selection for deep neural networks",
      "paper_id": "1707.01213v3"
    },
    {
      "index": 27,
      "title": "Low-rank compression of neural nets: Learning the rank of each layer",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Y. Idelbayev and M. A. Carreira-PerpiÃ±Ã¡n"
    },
    {
      "index": 28,
      "title": "Training CNNs with low-rank filters for efficient image classification",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Learning Representations",
      "authors": "Y. Ioannou, D. Robertson, J. Shotton, R. Cipolla, and A. Criminisi"
    },
    {
      "index": 29,
      "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions",
      "abstract": "",
      "year": "2014",
      "venue": "British Machine Vision Conference",
      "authors": "M. Jaderberg, A. Vedaldi, and A. Zisserman",
      "orig_title": "Speeding up convolutional neural networks with low rank expansions",
      "paper_id": "1405.3866v1"
    },
    {
      "index": 30,
      "title": "Initialization and Regularization of Factorized Neural Layers",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Learning Representations",
      "authors": "M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi",
      "orig_title": "Initialization and regularization of factorized neural layers",
      "paper_id": "2105.01029v2"
    },
    {
      "index": 31,
      "title": "Discretized dynamical low-rank approximation in the presence of small singular values",
      "abstract": "",
      "year": "2016",
      "venue": "SIAM Journal on Numerical Analysis",
      "authors": "E. Kieri, C. Lubich, and H. Walach"
    },
    {
      "index": 32,
      "title": "Efficient Neural Network Compression",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition",
      "authors": "H. Kim, M. U. K. Khan, and C.-M. Kyung",
      "orig_title": "Efficient neural network compression",
      "paper_id": "1811.12781v3"
    },
    {
      "index": 33,
      "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications",
      "abstract": "",
      "year": "2015",
      "venue": "",
      "authors": "Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin",
      "orig_title": "Compression of deep convolutional neural networks for fast and low power mobile applications",
      "paper_id": "1511.06530v2"
    },
    {
      "index": 34,
      "title": "Dynamical low-rank approximation",
      "abstract": "",
      "year": "2007",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "O. Koch and C. Lubich"
    },
    {
      "index": 35,
      "title": "Dynamical tensor approximation",
      "abstract": "",
      "year": "2010",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "O. Koch and C. Lubich"
    },
    {
      "index": 36,
      "title": "A robust collision source method for rank adaptive dynamical low-rank approximation in radiation therapy",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv:2111.07160",
      "authors": "J. Kusch and P. Stammer",
      "orig_title": "A robust collision source method for rank adaptive dynamical low-rank approximation in radiation therapy",
      "paper_id": "2111.07160v1"
    },
    {
      "index": 37,
      "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky",
      "orig_title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition",
      "paper_id": "1412.6553v3"
    },
    {
      "index": 38,
      "title": "Gradient-based learning applied to document recognition",
      "abstract": "",
      "year": "1998",
      "venue": "IEEE",
      "authors": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner"
    },
    {
      "index": 39,
      "title": "Constrained optimization based low-rank approximation of deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "authors": "C. Li and C. J. R. Shi"
    },
    {
      "index": 40,
      "title": "Runtime neural pruning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "J. Lin, Y. Rao, J. Lu, and J. Zhou"
    },
    {
      "index": 41,
      "title": "Towards Optimal Structured CNN Pruning via Generative Adversarial Learning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "authors": "S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and D. Doermann",
      "orig_title": "Towards optimal structured CNN pruning via generative adversarial learning",
      "paper_id": "1903.09291v1"
    },
    {
      "index": 42,
      "title": "Hierarchical Representations for Efficient Architecture Search",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu",
      "orig_title": "Hierarchical representations for efficient architecture search",
      "paper_id": "1711.00436v2"
    },
    {
      "index": 43,
      "title": "A projector-splitting integrator for dynamical low-rank approximation",
      "abstract": "",
      "year": "2014",
      "venue": "BIT Numerical Mathematics",
      "authors": "C. Lubich and I. V. Oseledets"
    },
    {
      "index": 44,
      "title": "Dynamical approximation by hierarchical Tucker and tensor-train tensors",
      "abstract": "",
      "year": "2013",
      "venue": "SIAM Journal on Matrix Analysis and Applications",
      "authors": "C. Lubich, T. Rohwedder, R. Schneider, and B. Vandereycken"
    },
    {
      "index": 45,
      "title": "Time integration of rank-constrained Tucker tensors",
      "abstract": "",
      "year": "2018",
      "venue": "SIAM Journal on Numerical Analysis",
      "authors": "C. Lubich, B. Vandereycken, and H. Walach"
    },
    {
      "index": 46,
      "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "J.-H. Luo, J. Wu, and W. Lin",
      "orig_title": "Thinet: A filter level pruning method for deep neural network compression",
      "paper_id": "1707.06342v1"
    },
    {
      "index": 47,
      "title": "Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "C. H. Martin and M. W. Mahoney",
      "orig_title": "Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning",
      "paper_id": "1810.01075v1"
    },
    {
      "index": 48,
      "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz",
      "orig_title": "Pruning convolutional neural networks for resource efficient inference",
      "paper_id": "1611.06440v2"
    },
    {
      "index": 49,
      "title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets",
      "abstract": "",
      "year": "2013",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "authors": "T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran"
    },
    {
      "index": 50,
      "title": "Integration methods and accelerated optimization algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "Neural Information Processing Systems",
      "authors": "D. Scieur, V. Roulet, F. Bach, and A. dâAspremont"
    },
    {
      "index": 51,
      "title": "Play and prune: Adaptive filter pruning for deep model compression",
      "abstract": "",
      "year": "2019",
      "venue": "Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "authors": "P. Singh, V. Kumar Verma, P. Rai, and V. P. Namboodiri"
    },
    {
      "index": 52,
      "title": "Analytic insights into structure and rank of neural network Hessian maps",
      "abstract": "",
      "year": "2021",
      "venue": "Neural Information Processing Systems",
      "authors": "S. P. Singh, G. Bachmann, and T. Hofmann"
    },
    {
      "index": 53,
      "title": "Low-Memory Neural Network Training: A Technical Report",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv:1904.10631",
      "authors": "N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. RÃ©",
      "orig_title": "Low-memory neural network training: A technical report",
      "paper_id": "1904.10631v2"
    },
    {
      "index": 54,
      "title": "Compressing Recurrent Neural Network with Tensor Train",
      "abstract": "",
      "year": "2017",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "authors": "A. Tjandra, S. Sakti, and S. Nakamura",
      "orig_title": "Compressing recurrent neural network with tensor train",
      "paper_id": "1705.08052v1"
    },
    {
      "index": 55,
      "title": "Why are Big Data Matrices Approximately Low Rank?",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Mathematics of Data Science",
      "authors": "M. Udell and A. Townsend",
      "orig_title": "Why are big data matrices approximately low rank?",
      "paper_id": "1705.07474v2"
    },
    {
      "index": 56,
      "title": "Pufferfish: communication-efficient models at no extra cost",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning and Systems",
      "authors": "H. Wang, S. Agarwal, and D. Papailiopoulos"
    },
    {
      "index": 57,
      "title": "Learning Structured Sparsity in Deep Neural Networks",
      "abstract": "",
      "year": "2016",
      "venue": "neural information processing systems",
      "authors": "W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li",
      "orig_title": "Learning structured sparsity in deep neural networks",
      "paper_id": "1608.03665v4"
    },
    {
      "index": 58,
      "title": "Coordinating filters for faster deep neural networks",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision",
      "authors": "W. Wen, C. Xu, C. Wu, Y. Wang, Y. Chen, and H. Li"
    },
    {
      "index": 59,
      "title": "Quantized convolutional neural networks for mobile devices",
      "abstract": "",
      "year": "2016",
      "venue": "IEEE conference on computer vision and pattern recognition",
      "authors": "J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng"
    },
    {
      "index": 60,
      "title": "Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification",
      "abstract": "",
      "year": "2020",
      "venue": "IEEE/CVF conference on computer vision and pattern recognition workshops",
      "authors": "H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y. Chen",
      "orig_title": "Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification",
      "paper_id": "2004.09031v1"
    },
    {
      "index": 61,
      "title": "Rethinking the Smaller-Norm-Less- Informative Assumption in Channel Pruning of Convolution Layers",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Ye, X. Lu, Z. Lin, and J. Z. Wang",
      "orig_title": "Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers",
      "paper_id": "1802.00124v2"
    }
  ]
}