{
  "paper_id": "2107.07651v2",
  "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
  "abstract": "Abstract\nLarge-scale vision and language representation learning has shown promising improvements on various vision-language tasks.\nMost existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens.\nBecause the visual tokens and word tokens are unaligned,\nit is challenging for the multimodal encoder to learn image-text interactions.\nIn this paper,\nwe introduce a contrastive loss to ALign the image and text representations\nBEfore Fusing (ALBEF) them through cross-modal attention,\nwhich enables more grounded vision and language representation learning.\nUnlike most existing methods, our method does not require bounding box annotations nor high-resolution images.\nTo improve learning from noisy web data,\nwe propose momentum distillation,\na self-training method which learns from pseudo-targets produced by a momentum model.\nWe provide a theoretical analysis of ALBEF from a mutual information maximization perspective,\nshowing that different training tasks can be interpreted as different ways to generate views for an image-text pair.\nALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks.\nOn image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets.\nOn VQA and NLVR2, ALBEF achieves absolute improvements of 2.37%percent2.372.37\\% and 3.84%percent3.843.84\\% compared to the state-of-the-art, while enjoying faster inference speed.\nCode and models are available at\nhttps://github.com/salesforce/ALBEF.",
  "reference_labels": [
    {
      "index": 0,
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "abstract": "",
      "year": "2019",
      "venue": "EMNLP",
      "authors": "Tan, H., M. Bansal",
      "orig_title": "LXMERT: learning cross-modality encoder representations from transformers",
      "paper_id": "1908.07490v3"
    },
    {
      "index": 1,
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Chen, Y., L. Li, L. Yu, et al.",
      "orig_title": "UNITER: universal image-text representation learning",
      "paper_id": "1909.11740v3"
    },
    {
      "index": 2,
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
      "abstract": "",
      "year": "2020",
      "venue": "ECCV",
      "authors": "Li, X., X. Yin, C. Li, et al.",
      "orig_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "paper_id": "2004.06165v5"
    },
    {
      "index": 3,
      "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "abstract": "",
      "year": "2018",
      "venue": "ACL",
      "authors": "Sharma, P., N. Ding, S. Goodman, et al."
    },
    {
      "index": 4,
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "abstract": "",
      "year": "2011",
      "venue": "NIPS",
      "authors": "Ordonez, V., G. Kulkarni, T. L. Berg"
    },
    {
      "index": 5,
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2103.00020",
      "authors": "Radford, A., J. W. Kim, C. Hallacy, et al.",
      "orig_title": "Learning transferable visual models from natural language supervision",
      "paper_id": "2103.00020v1"
    },
    {
      "index": 6,
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.05918",
      "authors": "Jia, C., Y. Yang, Y. Xia, et al."
    },
    {
      "index": 7,
      "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Gan, Z., Y. Chen, L. Li, et al.",
      "orig_title": "Large-scale adversarial training for vision-and-language representation learning",
      "paper_id": "2006.06195v2"
    },
    {
      "index": 8,
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "abstract": "",
      "year": "2017",
      "venue": "ICCV",
      "authors": "Selvaraju, R. R., M. Cogswell, A. Das, et al.",
      "orig_title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "paper_id": "1610.02391v4"
    },
    {
      "index": 9,
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Su, W., X. Zhu, Y. Cao, et al."
    },
    {
      "index": 10,
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "abstract": "",
      "year": "2019",
      "venue": "NeurIPS",
      "authors": "Lu, J., D. Batra, D. Parikh, et al."
    },
    {
      "index": 11,
      "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "Lu, J., V. Goswami, M. Rohrbach, et al.",
      "orig_title": "12-in-1: Multi-task vision and language representation learning",
      "paper_id": "1912.02315v2"
    },
    {
      "index": 12,
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1908.03557",
      "authors": "Li, L. H., M. Yatskar, D. Yin, et al.",
      "orig_title": "Visualbert: A simple and performant baseline for vision and language",
      "paper_id": "1908.03557v1"
    },
    {
      "index": 13,
      "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.07966",
      "authors": "Qi, D., L. Su, J. Song, et al.",
      "orig_title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data",
      "paper_id": "2001.07966v2"
    },
    {
      "index": 14,
      "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
      "abstract": "",
      "year": "2020",
      "venue": "AAAI",
      "authors": "Li, G., N. Duan, Y. Fang, et al.",
      "orig_title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "paper_id": "1908.06066v3"
    },
    {
      "index": 15,
      "title": "Ernie-vil: Knowledge enhanced vision-language representations through scene graph",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2006.16934",
      "authors": "Yu, F., J. Tang, W. Yin, et al."
    },
    {
      "index": 16,
      "title": "Vinvl: Making visual representations matter in vision-language models",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2101.00529",
      "authors": "Zhang, P., X. Li, X. Hu, et al."
    },
    {
      "index": 17,
      "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.03135",
      "authors": "Huang, Z., Z. Zeng, Y. Huang, et al.",
      "orig_title": "Seeing out of the box: End-to-end pre-training for vision-language representation learning",
      "paper_id": "2104.03135v2"
    },
    {
      "index": 18,
      "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
      "abstract": "",
      "year": "2019",
      "venue": "ACL",
      "authors": "Suhr, A., S. Zhou, A. Zhang, et al.",
      "orig_title": "A corpus for reasoning about natural language grounded in photographs",
      "paper_id": "1811.00491v3"
    },
    {
      "index": 19,
      "title": "VQA: visual question answering",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Antol, S., A. Agrawal, J. Lu, et al."
    },
    {
      "index": 20,
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.03334",
      "authors": "Kim, W., B. Son, I. Kim",
      "orig_title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "paper_id": "2102.03334v2"
    },
    {
      "index": 21,
      "title": "VSE++: improving visual-semantic embeddings with hard negatives",
      "abstract": "",
      "year": "2018",
      "venue": "BMVC",
      "authors": "Faghri, F., D. J. Fleet, J. R. Kiros, et al."
    },
    {
      "index": 22,
      "title": "Visual Semantic Reasoning for Image-Text Matching",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Li, K., Y. Zhang, K. Li, et al.",
      "orig_title": "Visual semantic reasoning for image-text matching",
      "paper_id": "1909.02701v1"
    },
    {
      "index": 23,
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR",
      "authors": "He, K., H. Fan, Y. Wu, et al.",
      "orig_title": "Momentum contrast for unsupervised visual representation learning",
      "paper_id": "1911.05722v3"
    },
    {
      "index": 24,
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "abstract": "",
      "year": "2020",
      "venue": "ICML",
      "authors": "Chen, T., S. Kornblith, M. Norouzi, et al.",
      "orig_title": "A simple framework for contrastive learning of visual representations",
      "paper_id": "2002.05709v3"
    },
    {
      "index": 25,
      "title": "Prototypical Contrastive Learning of Unsupervised Representations",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Li, J., P. Zhou, C. Xiong, et al.",
      "orig_title": "Prototypical contrastive learning of unsupervised representations",
      "paper_id": "2005.04966v5"
    },
    {
      "index": 26,
      "title": "MoPro: Webly Supervised Learning with Momentum Prototypes",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Li, J., C. Xiong, S. C. Hoi",
      "orig_title": "Mopro: Webly supervised learning with momentum prototypes",
      "paper_id": "2009.07995v1"
    },
    {
      "index": 27,
      "title": "Distilling the Knowledge in a Neural Network",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv preprint arXiv:1503.02531",
      "authors": "Hinton, G., O. Vinyals, J. Dean",
      "orig_title": "Distilling the knowledge in a neural network",
      "paper_id": "1503.02531v1"
    },
    {
      "index": 28,
      "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
      "abstract": "",
      "year": "2017",
      "venue": "ICLR",
      "authors": "Zagoruyko, S., N. Komodakis",
      "orig_title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "paper_id": "1612.03928v3"
    },
    {
      "index": 29,
      "title": "Born-Again Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "ICML",
      "authors": "Furlanello, T., Z. C. Lipton, M. Tschannen, et al.",
      "orig_title": "Born-again neural networks",
      "paper_id": "1805.04770v2"
    },
    {
      "index": 30,
      "title": "Training data-efficient image transformers & distillation through attention",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2012.12877",
      "authors": "Touvron, H., M. Cord, M. Douze, et al.",
      "orig_title": "Training data-efficient image transformers & distillation through attention",
      "paper_id": "2012.12877v2"
    },
    {
      "index": 31,
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1910.01108",
      "authors": "Sanh, V., L. Debut, J. Chaumond, et al.",
      "orig_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "paper_id": "1910.01108v4"
    },
    {
      "index": 32,
      "title": "Deep Mutual Learning",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Zhang, Y., T. Xiang, T. M. Hospedales, et al.",
      "orig_title": "Deep mutual learning",
      "paper_id": "1706.00384v1"
    },
    {
      "index": 33,
      "title": "Large scale distributed neural network training through online distillation",
      "abstract": "",
      "year": "2018",
      "venue": "ICLR",
      "authors": "Anil, R., G. Pereyra, A. Passos, et al.",
      "orig_title": "Large scale distributed neural network training through online distillation",
      "paper_id": "1804.03235v2"
    },
    {
      "index": 34,
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Tarvainen, A., H. Valpola",
      "orig_title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "paper_id": "1703.01780v6"
    },
    {
      "index": 35,
      "title": "Dividemix: Learning with noisy labels as semi-supervised learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Li, J., R. Socher, S. C. Hoi"
    },
    {
      "index": 36,
      "title": "Data-Efficient Language-Supervised Zero-Shot Learning with Self-Distillation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2104.08945",
      "authors": "Cheng, R., B. Wu, P. Zhang, et al.",
      "orig_title": "Data-efficient language-supervised zero-shot learning with self-distillation",
      "paper_id": "2104.08945v1"
    },
    {
      "index": 37,
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "",
      "year": "2021",
      "venue": "ICLR",
      "authors": "Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al.",
      "orig_title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "paper_id": "2010.11929v2"
    },
    {
      "index": 38,
      "title": "Attention Is All You Need",
      "abstract": "",
      "year": "2017",
      "venue": "NIPS",
      "authors": "Vaswani, A., N. Shazeer, N. Parmar, et al.",
      "orig_title": "Attention is all you need",
      "paper_id": "1706.03762v7"
    },
    {
      "index": 39,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "NAACL",
      "authors": "Devlin, J., M. Chang, K. Lee, et al.",
      "orig_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "paper_id": "1810.04805v2"
    },
    {
      "index": 40,
      "title": "Microsoft COCO: Common Objects in Context",
      "abstract": "",
      "year": "2014",
      "venue": "ECCV",
      "authors": "Lin, T., M. Maire, S. J. Belongie, et al.",
      "orig_title": "Microsoft COCO: common objects in context",
      "paper_id": "1405.0312v3"
    },
    {
      "index": 41,
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "abstract": "",
      "year": "2017",
      "venue": "IJCV",
      "authors": "Krishna, R., Y. Zhu, O. Groth, et al."
    },
    {
      "index": 42,
      "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
      "abstract": "",
      "year": "2021",
      "venue": "CVPR",
      "authors": "Changpinyo, S., P. Sharma, N. Ding, et al.",
      "orig_title": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
      "paper_id": "2102.08981v2"
    },
    {
      "index": 43,
      "title": "Decoupled Weight Decay Regularization",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1711.05101",
      "authors": "Loshchilov, I., F. Hutter",
      "orig_title": "Decoupled weight decay regularization",
      "paper_id": "1711.05101v3"
    },
    {
      "index": 44,
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "abstract": "",
      "year": "2020",
      "venue": "CVPR Workshops",
      "authors": "Cubuk, E. D., B. Zoph, J. Shlens, et al."
    },
    {
      "index": 45,
      "title": "What Makes for Good Views for Contrastive Learning?",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Tian, Y., C. Sun, B. Poole, et al.",
      "orig_title": "What makes for good views for contrastive learning?",
      "paper_id": "2005.10243v3"
    },
    {
      "index": 46,
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1807.03748",
      "authors": "Oord, A. v. d., Y. Li, O. Vinyals",
      "orig_title": "Representation learning with contrastive predictive coding",
      "paper_id": "1807.03748v2"
    },
    {
      "index": 47,
      "title": "A Mutual Information Maximization Perspective of Language Representation Learning",
      "abstract": "",
      "year": "2020",
      "venue": "ICLR",
      "authors": "Kong, L., C. de Masson d’Autume, L. Yu, et al.",
      "orig_title": "A mutual information maximization perspective of language representation learning",
      "paper_id": "1910.08350v2"
    },
    {
      "index": 48,
      "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
      "abstract": "",
      "year": "2015",
      "venue": "ICCV",
      "authors": "Plummer, B. A., L. Wang, C. M. Cervantes, et al.",
      "orig_title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "paper_id": "1505.04870v4"
    },
    {
      "index": 49,
      "title": "e-snli-ve: Corrected visual-textual entailment with natural language explanations",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2004.03744",
      "authors": "Do, V., O.-M. Camburu, Z. Akata, et al."
    },
    {
      "index": 50,
      "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1901.06706",
      "authors": "Xie, N., F. Lai, D. Doran, et al.",
      "orig_title": "Visual entailment: A novel task for fine-grained image understanding",
      "paper_id": "1901.06706v1"
    },
    {
      "index": 51,
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "abstract": "",
      "year": "2017",
      "venue": "CVPR",
      "authors": "Goyal, Y., T. Khot, D. Summers-Stay, et al.",
      "orig_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
      "paper_id": "1612.00837v3"
    },
    {
      "index": 52,
      "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
      "abstract": "",
      "year": "2018",
      "venue": "CVPR",
      "authors": "Yu, L., Z. Lin, X. Shen, et al.",
      "orig_title": "Mattnet: Modular attention network for referring expression comprehension",
      "paper_id": "1801.08186v3"
    },
    {
      "index": 53,
      "title": "Unifying vision-and-language tasks via text generation",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2102.02779",
      "authors": "Cho, J., J. Lei, H. Tan, et al."
    },
    {
      "index": 54,
      "title": "Bilinear Attention Networks",
      "abstract": "",
      "year": "2018",
      "venue": "NIPS",
      "authors": "Kim, J., J. Jun, B. Zhang",
      "orig_title": "Bilinear attention networks",
      "paper_id": "1805.07932v2"
    },
    {
      "index": 55,
      "title": "Modeling context in referring expressions",
      "abstract": "",
      "year": "2016",
      "venue": "ECCV",
      "authors": "Yu, L., P. Poirson, S. Yang, et al."
    },
    {
      "index": 56,
      "title": "Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding",
      "abstract": "",
      "year": "2019",
      "venue": "ICCV",
      "authors": "Liu, X., L. Li, S. Wang, et al.",
      "orig_title": "Adaptive reconstruction network for weakly supervised referring expression grounding",
      "paper_id": "1908.10568v1"
    },
    {
      "index": 57,
      "title": "Counterfactual contrastive learning fo weakly-supervised vision-language grounding",
      "abstract": "",
      "year": "2020",
      "venue": "NeurIPS",
      "authors": "Zhang, Z., Z. Zhao, Z. Lin, et al."
    },
    {
      "index": 58,
      "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "abstract": "",
      "year": "2015",
      "venue": "CVPR",
      "authors": "Karpathy, A., F. Li",
      "orig_title": "Deep visual-semantic alignments for generating image descriptions",
      "paper_id": "1412.2306v2"
    },
    {
      "index": 59,
      "title": "A large annotated corpus for learning natural language inference",
      "abstract": "",
      "year": "2015",
      "venue": "EMNLP",
      "authors": "Bowman, S. R., G. Angeli, C. Potts, et al.",
      "orig_title": "A large annotated corpus for learning natural language inference",
      "paper_id": "1508.05326v1"
    },
    {
      "index": 60,
      "title": "Deep Modular Co-Attention Networks for Visual Question Answering",
      "abstract": "",
      "year": "2019",
      "venue": "CVPR",
      "authors": "Yu, Z., J. Yu, Y. Cui, et al.",
      "orig_title": "Deep modular co-attention networks for visual question answering",
      "paper_id": "1906.10770v1"
    },
    {
      "index": 61,
      "title": "Referitgame: Referring to objects in photographs of natural scenes",
      "abstract": "",
      "year": "2014",
      "venue": "EMNLP",
      "authors": "Kazemzadeh, S., V. Ordonez, M. Matten, et al."
    },
    {
      "index": 62,
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "abstract": "",
      "year": "2016",
      "venue": "",
      "authors": "Das, A., H. Agrawal, C. L. Zitnick, et al.",
      "orig_title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "paper_id": "1606.03556v2"
    }
  ]
}