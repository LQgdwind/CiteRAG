{
  "paper_id": "2202.11133v1",
  "title": "Continual Auxiliary Task Learning",
  "abstract": "Abstract\nLearning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Addressing environment non-stationarity by repeating q-learning updates",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Sherief Abdallah and Michael Kaisers"
    },
    {
      "index": 1,
      "title": "Hindsight experience replay",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba"
    },
    {
      "index": 2,
      "title": "Never Give Up: Learning Directed Exploration Strategies",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andew Bolt, et al.",
      "orig_title": "Never give up: Learning directed exploration strategies",
      "paper_id": "2002.06038v1"
    },
    {
      "index": 3,
      "title": "Successor features for transfer in reinforcement learning",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver"
    },
    {
      "index": 4,
      "title": "Transfer in deep reinforcement learning using successor features and generalised policy improvement",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos"
    },
    {
      "index": 5,
      "title": "The option keyboard: Combining skills in reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Andre Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygün, Philippe Hamel, Daniel Toyama, Jonathan hunt, Shibl Mourad, David Silver, and Doina Precup"
    },
    {
      "index": 6,
      "title": "Fast reinforcement learning with generalized policy updates",
      "abstract": "",
      "year": "2020",
      "venue": "National Academy of Sciences",
      "authors": "André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup"
    },
    {
      "index": 7,
      "title": "Stochastic multi-armed-bandit problem with non-stationary rewards",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Omar Besbes, Yonatan Gur, and Assaf Zeevi"
    },
    {
      "index": 8,
      "title": "Universal Successor Features Approximators",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Diana Borsa, André Barreto, John Quan, Daniel Mankowitz, Rémi Munos, Hado van Hasselt, David Silver, and Tom Schaul",
      "orig_title": "Universal successor features approximators",
      "paper_id": "1812.07626v1"
    },
    {
      "index": 9,
      "title": "Exploration by Random Network Distillation",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov",
      "orig_title": "Exploration by random network distillation",
      "paper_id": "1810.12894v1"
    },
    {
      "index": 10,
      "title": "Towards Safe Policy Improvement for Non-Stationary MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yash Chandak, Scott M Jordan, Georgios Theocharous, Martha White, and Philip S Thomas",
      "orig_title": "Towards safe policy improvement for non-stationary mdps",
      "paper_id": "2010.12645v2"
    },
    {
      "index": 11,
      "title": "Optimizing for the Future in Non-Stationary MDPs",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Yash Chandak, Georgios Theocharous, Shiv Shanka, Martha White, Sridhar Mahadevan, and Philip S Thomas",
      "orig_title": "Optimizing for the future in non-stationary mdps",
      "paper_id": "2005.08158v4"
    },
    {
      "index": 12,
      "title": "Intrinsically motivated reinforcement learning",
      "abstract": "",
      "year": "2005",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nuttapong Chentanez, Andrew Barto, and Satinder Singh"
    },
    {
      "index": 13,
      "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu",
      "orig_title": "Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism",
      "paper_id": "2006.14389v1"
    },
    {
      "index": 14,
      "title": "CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "International conference on machine learning",
      "authors": "Cédric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer",
      "orig_title": "Curious: intrinsically motivated modular multi-goal reinforcement learning",
      "paper_id": "1810.06284v4"
    },
    {
      "index": 15,
      "title": "Improving reinforcement learning with context detection",
      "abstract": "",
      "year": "2006",
      "venue": "Autonomous agents and multiagent systems",
      "authors": "Bruno C Da Silva, Eduardo W Basso, Filipo S Perotto, Ana L C. Bazzan, and Paulo M Engel"
    },
    {
      "index": 16,
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine",
      "orig_title": "Diversity is all you need: Learning skills without a reward function",
      "paper_id": "1802.06070v6"
    },
    {
      "index": 17,
      "title": "Search on the replay buffer: Bridging planning and reinforcement learning",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine"
    },
    {
      "index": 18,
      "title": "On upper-confidence bound policies for non-stationary bandit problems",
      "abstract": "",
      "year": "2008",
      "venue": "arXiv",
      "authors": "Aurélien Garivier and Eric Moulines"
    },
    {
      "index": 19,
      "title": "Online Off-policy Prediction",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv",
      "authors": "Sina Ghiassian, Andrew Patterson, Martha White, Richard S Sutton, and Adam White",
      "orig_title": "Online off-policy prediction",
      "paper_id": "1811.02597v1"
    },
    {
      "index": 20,
      "title": "Variational Intrinsic Control",
      "abstract": "",
      "year": "2017",
      "venue": "Interational Conference on Learning Representations Workshop",
      "authors": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra",
      "orig_title": "Variational intrinsic control",
      "paper_id": "1611.07507v1"
    },
    {
      "index": 21,
      "title": "Consistent On-Line Off-Policy Evaluation",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Assaf Hallak and Shie Mannor",
      "orig_title": "Consistent on-line off-policy evaluation",
      "paper_id": "1702.07121v1"
    },
    {
      "index": 22,
      "title": "Meta-descent for Online, Continual Prediction",
      "abstract": "",
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Andrew Jacobsen, Matthew Schlegel, Cameron Linke, Thomas Degris, Adam White, and Martha White",
      "orig_title": "Meta-descent for online, continual prediction",
      "paper_id": "1907.07751v2"
    },
    {
      "index": 23,
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu",
      "orig_title": "Reinforcement learning with unsupervised auxiliary tasks",
      "paper_id": "1611.05397v1"
    },
    {
      "index": 24,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "Diederik P Kingma and Jimmy Ba"
    },
    {
      "index": 25,
      "title": "Reinforcement learning and evolutionary algorithms for non-stationary multi-armed bandit problems",
      "abstract": "",
      "year": "2008",
      "venue": "Applied Mathematics and Computation",
      "authors": "Dimitris E Koulouriotis and A Xanthopoulos"
    },
    {
      "index": 26,
      "title": "Rotting Bandits",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Nir Levine, Koby Crammer, and Shie Mannor",
      "orig_title": "Rotting bandits",
      "paper_id": "1702.07274v4"
    },
    {
      "index": 27,
      "title": "Adapting behavior via intrinsic reward: A survey and empirical study",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Cam Linke, Nadia M Ady, Martha White, Thomas Degris, and Adam White"
    },
    {
      "index": 28,
      "title": "Proximal Gradient Temporal Difference Learning: Stable Reinforcement Learning with Polynomial Sample Complexity",
      "abstract": "",
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Bo Liu, Ian Gemp, Mohammad Ghavamzadeh, Ji Liu, Sridhar Mahadevan, and Marek Petrik",
      "orig_title": "Proximal gradient temporal difference learning: Stable reinforcement learning with polynomial sample complexity",
      "paper_id": "2006.03976v1"
    },
    {
      "index": 29,
      "title": "Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou",
      "orig_title": "Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation",
      "paper_id": "1810.12429v1"
    },
    {
      "index": 30,
      "title": "Off-Policy Policy Gradient with Stationary Distribution Correction",
      "abstract": "",
      "year": "2020",
      "venue": "Uncertainty in Artificial Intelligence",
      "authors": "Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill"
    },
    {
      "index": 31,
      "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Marlos C Machado, Marc G Bellemare, and Michael Bowling",
      "orig_title": "A laplacian framework for option discovery in reinforcement learning",
      "paper_id": "1703.00956v2"
    },
    {
      "index": 32,
      "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios",
      "abstract": "",
      "year": "2017",
      "venue": "",
      "authors": "Ashique Mahmood, Huizhen Yu, and Richard Sutton",
      "orig_title": "Multi-step off-policy learning without importance sampling ratios",
      "paper_id": "1702.03006v1"
    },
    {
      "index": 33,
      "title": "Tuning-free step-size adaptation",
      "abstract": "",
      "year": "2012",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "authors": "Ashique Rupam Mahmood, Richard S Sutton, Thomas Degris, and Patrick M Pilarski"
    },
    {
      "index": 34,
      "title": "Contextual Imagined Goals for Self-Supervised Robotic Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Robot Learning",
      "authors": "Ashvin Nair, Shikhar Bahl, Alexander Khazatsky, Vitchyr Pong, Glen Berseth, and Sergey Levine",
      "orig_title": "Contextual imagined goals for self-supervised robotic learning",
      "paper_id": "1910.11670v1"
    },
    {
      "index": 35,
      "title": "A modern introduction to online learning",
      "abstract": "",
      "year": "2019",
      "venue": "",
      "authors": "Francesco Orabona"
    },
    {
      "index": 36,
      "title": "Intrinsic motivation systems for autonomous mental development",
      "abstract": "",
      "year": "2007",
      "venue": "IEEE Transactions on Evolutionary Computation",
      "authors": "Pierre-Yves Oudeyer, Frédéric Kaplan, and Verena V. Hafner"
    },
    {
      "index": 37,
      "title": "Reinforcement learning algorithm for non-stationary environments",
      "abstract": "",
      "year": "2020",
      "venue": "Applied Intelligence",
      "authors": "Sindhu Padakandla, KJ Prabuchandran, and Shalabh Bhatnagar"
    },
    {
      "index": 38,
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell",
      "orig_title": "Curiosity-driven exploration by self-supervised prediction",
      "paper_id": "1705.05363v1"
    },
    {
      "index": 39,
      "title": "A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Andrew Patterson, Adam White, Sina Ghiassian, and Martha White",
      "orig_title": "A generalized projected bellman error for off-policy value estimation in reinforcement learning",
      "paper_id": "2104.13844v3"
    },
    {
      "index": 40,
      "title": "Long-horizon visual planning with goal-conditioned hierarchical predictors",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Karl Pertsch, Oleh Rybkin, Frederik Ebert, Shenghao Zhou, Dinesh Jayaraman, Chelsea Finn, and Sergey Levine"
    },
    {
      "index": 41,
      "title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba"
    },
    {
      "index": 42,
      "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv",
      "authors": "Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine",
      "orig_title": "Skew-fit: State-covering self-supervised reinforcement learning",
      "paper_id": "1903.03698v4"
    },
    {
      "index": 43,
      "title": "Eligibility traces for off-policy policy evaluation",
      "abstract": "",
      "year": "2000",
      "venue": "Computer Science Department Faculty Publication Series",
      "authors": "Doina Precup"
    },
    {
      "index": 44,
      "title": "Learning by Playing – Solving Sparse Reward Tasks from Scratch",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg",
      "orig_title": "Learning by playing solving sparse reward tasks from scratch",
      "paper_id": "1802.10567v1"
    },
    {
      "index": 45,
      "title": "Better generalization with forecasts",
      "abstract": "",
      "year": "2013",
      "venue": "International Joint Conferences on Artificial Intelligence",
      "authors": "Tom Schaul and Mark B Ring"
    },
    {
      "index": 46,
      "title": "Universal value function approximators",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Machine Learning",
      "authors": "Tom Schaul, Dan Horgan, Karol Gregor, and David Silver"
    },
    {
      "index": 47,
      "title": "Improved and generalized upper bounds on the complexity of policy iteration",
      "abstract": "",
      "year": "2016",
      "venue": "Mathematics of Operations Research",
      "authors": "Bruno Scherrer"
    },
    {
      "index": 48,
      "title": "General Value Function Networks",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research",
      "authors": "Matthew Schlegel, Andrew Jacobsen, Zaheer Abbas, Andrew Patterson, Adam White, and Martha White",
      "orig_title": "General value function networks",
      "paper_id": "1807.06763v4"
    },
    {
      "index": 49,
      "title": "Rotting bandits are no harder than stochastic ones",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko"
    },
    {
      "index": 50,
      "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
      "abstract": "",
      "year": "2015",
      "venue": "arXiv",
      "authors": "Bradly C Stadie, Sergey Levine, and Pieter Abbeel",
      "orig_title": "Incentivizing exploration in reinforcement learning with deep predictive models",
      "paper_id": "1507.00814v3"
    },
    {
      "index": 51,
      "title": "Reinforcement Learning: An Introduction",
      "abstract": "",
      "year": "2018",
      "venue": "A Bradford Book",
      "authors": "Richard S. Sutton and Andrew G. Barto"
    },
    {
      "index": 52,
      "title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "abstract": "",
      "year": "2011",
      "venue": "International Conference on Autonomous Agents and Multiagent Systems",
      "authors": "Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup"
    },
    {
      "index": 53,
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "Richard S Sutton, A Rupam Mahmood, and Martha White",
      "orig_title": "An emphatic approach to the problem of off-policy temporal-difference learning",
      "paper_id": "1503.04269v2"
    },
    {
      "index": 54,
      "title": "Introduction and removal of reward, and maze performance in rats",
      "abstract": "",
      "year": "1930",
      "venue": "University of California publications in psychology",
      "authors": "Edward Chace Tolman and Charles H Honzik"
    },
    {
      "index": 55,
      "title": "Discovery of Useful Questions as Auxiliary Tasks",
      "abstract": "",
      "year": "2019",
      "venue": "Neural Information Processing Systems",
      "authors": "Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L. Lewis, Junhyuk Oh, Hado van Hasselt, David Silver, and Satinder Singh",
      "orig_title": "Discovery of useful questions as auxiliary tasks",
      "paper_id": "1909.04607v1"
    },
    {
      "index": 56,
      "title": "Discovery of Options via Meta-Learned Subgoals",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, and Satinder Singh",
      "orig_title": "Discovery of options via meta-learned subgoals",
      "paper_id": "2102.06741v1"
    },
    {
      "index": 57,
      "title": "Unifying Task Specification in Reinforcement Learning",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "Martha White",
      "orig_title": "Unifying task specification in reinforcement learning",
      "paper_id": "1609.01995v4"
    },
    {
      "index": 58,
      "title": "Discovering Diverse Nearly Optimal Policies with Successor Features",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv",
      "authors": "Tom Zahavy, Brendan O’Donoghue, Andre Barreto, Volodymyr Mnih, Sebastian Flennerhag, and Satinder Singh",
      "orig_title": "Discovering diverse nearly optimal policies with successor features",
      "paper_id": "2106.00669v2"
    },
    {
      "index": 59,
      "title": "Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen",
      "orig_title": "Generating adjacency-constrained subgoals in hierarchical reinforcement learning",
      "paper_id": "2006.11485v4"
    }
  ]
}