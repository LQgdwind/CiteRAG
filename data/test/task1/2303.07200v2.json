{
  "paper_id": "2303.07200v2",
  "title": "Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks",
  "abstract": "Abstract\nFeature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named “NeuroFS”. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on 111111 low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code is available on GitHub111https://github.com/zahraatashgahi/NeuroFS.",
  "reference_labels": [
    {
      "index": 0,
      "title": "A brain-inspired algorithm for training highly sparse neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1903.07138",
      "authors": "Zahra Atashgahi, Joost Pieterse, Shiwei Liu, Decebal Constantin Mocanu, Raymond Veldhuis, and Mykola Pechenizkiy"
    },
    {
      "index": 1,
      "title": "Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders",
      "abstract": "",
      "year": "2021",
      "venue": "Machine Learning",
      "authors": "Zahra Atashgahi, Ghada Sokar, Tim van der Lee, Elena Mocanu, Decebal Constantin Mocanu, Raymond Veldhuis, and Mykola Pechenizkiy"
    },
    {
      "index": 2,
      "title": "Concrete autoencoders: Differentiable feature selection and reconstruction",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Machine Learning",
      "authors": "Muhammed Fatih Balın, Abubakar Abid, and James Zou"
    },
    {
      "index": 3,
      "title": "Using mutual information for selecting features in supervised neural net learning",
      "abstract": "",
      "year": "1994",
      "venue": "IEEE Transactions on neural networks",
      "authors": "Roberto Battiti"
    },
    {
      "index": 4,
      "title": "Deep Rewiring: Training very sparse deep networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein",
      "orig_title": "Deep rewiring: Training very sparse deep networks",
      "paper_id": "1711.05136v5"
    },
    {
      "index": 5,
      "title": "Exploring autoencoders for unsupervised feature selection",
      "abstract": "",
      "year": "2015",
      "venue": "2015 International Joint Conference on Neural Networks (IJCNN)",
      "authors": "B Chandra and Rajesh K Sharma"
    },
    {
      "index": 6,
      "title": "A survey on feature selection methods",
      "abstract": "",
      "year": "2014",
      "venue": "Computers & Electrical Engineering",
      "authors": "Girish Chandrashekar and Ferat Sahin"
    },
    {
      "index": 7,
      "title": "Keras",
      "abstract": "",
      "year": "2015",
      "venue": "https://keras.io",
      "authors": "François Chollet et al."
    },
    {
      "index": 8,
      "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE Transactions on Computers",
      "authors": "Xiaoliang Dai, Hongxu Yin, and Niraj K Jha",
      "orig_title": "Nest: A neural network synthesis tool based on a grow-and-prune paradigm",
      "paper_id": "1711.02017v3"
    },
    {
      "index": 9,
      "title": "Sparse Networks from Scratch: Faster Training without Losing Performance",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1907.04840",
      "authors": "Tim Dettmers and Luke Zettlemoyer",
      "orig_title": "Sparse networks from scratch: Faster training without losing performance",
      "paper_id": "1907.04840v2"
    },
    {
      "index": 10,
      "title": "Agnostic feature selection",
      "abstract": "",
      "year": "2019",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
      "authors": "Guillaume Doquet and Michèle Sebag"
    },
    {
      "index": 11,
      "title": "Rigging the lottery: Making all tickets winners",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen"
    },
    {
      "index": 12,
      "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
      "abstract": "",
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": "Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin",
      "orig_title": "Gradient flow in sparse neural networks and how lottery tickets win",
      "paper_id": "2010.03533v2"
    },
    {
      "index": 13,
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Learning Representations",
      "authors": "Jonathan Frankle and Michael Carbin",
      "orig_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "paper_id": "1803.03635v5"
    },
    {
      "index": 14,
      "title": "The state of sparsity in deep neural networks",
      "abstract": "",
      "year": "2019",
      "venue": "arXiv preprint arXiv:1902.09574",
      "authors": "Trevor Gale, Erich Elsen, and Sara Hooker"
    },
    {
      "index": 15,
      "title": "Deep Learning",
      "abstract": "",
      "year": "2016",
      "venue": "MIT press",
      "authors": "Ian Goodfellow, Yoshua Bengio, and Aaron Courville",
      "orig_title": "Deep learning",
      "paper_id": "1807.07987v2"
    },
    {
      "index": 16,
      "title": "Generalized fisher score for feature selection",
      "abstract": "",
      "year": "2011",
      "venue": "27th Conference on Uncertainty in Artificial Intelligence, UAI 2011",
      "authors": "Quanquan Gu, Zhenhui Li, and Jiawei Han"
    },
    {
      "index": 17,
      "title": "An introduction to variable and feature selection",
      "abstract": "",
      "year": "2003",
      "venue": "Journal of machine learning research",
      "authors": "Isabelle Guyon and André Elisseeff"
    },
    {
      "index": 18,
      "title": "Gene selection for cancer classification using support vector machines",
      "abstract": "",
      "year": "2002",
      "venue": "Machine learning",
      "authors": "Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik"
    },
    {
      "index": 19,
      "title": "Autoencoder inspired unsupervised feature selection",
      "abstract": "",
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": "Kai Han, Yunhe Wang, Chao Zhang, Chao Li, and Chao Xu"
    },
    {
      "index": 20,
      "title": "Learning both Weights and Connections for Efficient Neural Networks",
      "abstract": "",
      "year": "2015",
      "venue": "Advances in neural information processing systems",
      "authors": "Song Han, Jeff Pool, John Tran, and William Dally",
      "orig_title": "Learning both weights and connections for efficient neural network",
      "paper_id": "1506.02626v3"
    },
    {
      "index": 21,
      "title": "Second order derivatives for network pruning: Optimal brain surgeon",
      "abstract": "",
      "year": "1993",
      "venue": "Advances in neural information processing systems",
      "authors": "Babak Hassibi and David G Stork"
    },
    {
      "index": 22,
      "title": "Laplacian score for feature selection",
      "abstract": "",
      "year": "2006",
      "venue": "Advances in neural information processing systems",
      "authors": "Xiaofei He, Deng Cai, and Partha Niyogi"
    },
    {
      "index": 23,
      "title": "Deep Learning Scaling is Predictable, Empirically",
      "abstract": "",
      "year": "2017",
      "venue": "arXiv preprint arXiv:1712.00409",
      "authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou",
      "orig_title": "Deep learning scaling is predictable, empirically",
      "paper_id": "1712.00409v1"
    },
    {
      "index": 24,
      "title": "Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste"
    },
    {
      "index": 25,
      "title": "The hardware lottery",
      "abstract": "",
      "year": "2021",
      "venue": "Communications of the ACM",
      "authors": "Sara Hooker"
    },
    {
      "index": 26,
      "title": "Machine learning based on attribute interactions",
      "abstract": "",
      "year": "2005",
      "venue": "University of Ljubljana",
      "authors": "Aleks Jakulin"
    },
    {
      "index": 27,
      "title": "Top-KAST: Top-K Always Sparse Training",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen",
      "orig_title": "Top-kast: Top-k always sparse training",
      "paper_id": "2106.03517v1"
    },
    {
      "index": 28,
      "title": "Improvements to platt’s smo algorithm for svm classifier design",
      "abstract": "",
      "year": "2001",
      "venue": "Neural computation",
      "authors": "S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and Karuturi Radha Krishna Murthy"
    },
    {
      "index": 29,
      "title": "Wrappers for feature subset selection",
      "abstract": "",
      "year": "1997",
      "venue": "Artificial intelligence",
      "authors": "Ron Kohavi and George H John"
    },
    {
      "index": 30,
      "title": "Artificial neurogenesis: An introduction and selective review",
      "abstract": "",
      "year": "2014",
      "venue": "Growing Adaptive Machines",
      "authors": "Taras Kowaliw, Nicolas Bredeche, Sylvain Chevallier, and René Doursat"
    },
    {
      "index": 31,
      "title": "Effective nonlinear feature selection method based on hsic lasso and with variational inference",
      "abstract": "",
      "year": "2022",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "Kazuki Koyama, Keisuke Kiritoshi, Tomomi Okawachi, and Tomonori Izumitani"
    },
    {
      "index": 32,
      "title": "Optimal brain damage",
      "abstract": "",
      "year": "1990",
      "venue": "Advances in neural information processing systems",
      "authors": "Yann LeCun, John S Denker, and Sara A Solla"
    },
    {
      "index": 33,
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Learning Representations",
      "authors": "Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr",
      "orig_title": "Snip: Single-shot network pruning based on connection sensitivity",
      "paper_id": "1810.02340v2"
    },
    {
      "index": 34,
      "title": "Lassonet: A neural network with feature sparsity",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Ismael Lemhadri, Feng Ruan, Louis Abraham, and Robert Tibshirani"
    },
    {
      "index": 35,
      "title": "Feature selection: A data perspective",
      "abstract": "",
      "year": "2018",
      "venue": "ACM Computing Surveys (CSUR)",
      "authors": "Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and Huan Liu"
    },
    {
      "index": 36,
      "title": "Conditional infomax learning: An integrated framework for feature extraction and fusion",
      "abstract": "",
      "year": "2006",
      "venue": "European conference on computer vision",
      "authors": "Dahua Lin and Xiaoou Tang"
    },
    {
      "index": 37,
      "title": "A probabilistic approach to feature selection-a filter solution",
      "abstract": "",
      "year": "1996",
      "venue": "ICML",
      "authors": "Huan Liu, Rudy Setiono, et al."
    },
    {
      "index": 38,
      "title": "Topological Insights into Sparse Neural Networks",
      "abstract": "",
      "year": "2020",
      "venue": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)",
      "authors": "Shiwei Liu, Tim van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferraro, Ghada Sokar, Mykola Pechenizkiy, and Decebal Constantin Mocanu",
      "orig_title": "Topological insights into sparse neural networks",
      "paper_id": "2006.14085v2"
    },
    {
      "index": 39,
      "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS 2021)",
      "authors": "Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu",
      "orig_title": "Sparse training via boosting pruning plasticity with neuroregeneration",
      "paper_id": "2106.10404v4"
    },
    {
      "index": 40,
      "title": "Do we actually need dense over-parameterization? in-time over-parameterization in sparse training",
      "abstract": "",
      "year": "2021",
      "venue": "38th International Conference on Machine Learning",
      "authors": "Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy"
    },
    {
      "index": 41,
      "title": "DeepPINK: reproducible feature selection in deep neural networks",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Yang Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble",
      "orig_title": "Deeppink: reproducible feature selection in deep neural networks",
      "paper_id": "1809.01185v2"
    },
    {
      "index": 42,
      "title": "A topological insight into restricted boltzmann machines",
      "abstract": "",
      "year": "2016",
      "venue": "Machine Learning",
      "authors": "Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta"
    },
    {
      "index": 43,
      "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
      "abstract": "",
      "year": "2018",
      "venue": "Nature communications",
      "authors": "Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta"
    },
    {
      "index": 44,
      "title": "Sparse Training Theory for Scalable and Efficient Agents",
      "abstract": "",
      "year": "2021",
      "venue": "20th International Conference on Autonomous Agents and MultiAgent Systems",
      "authors": "Decebal Constantin Mocanu, Elena Mocanu, Tiago Pinto, Selima Curci, Phuong H Nguyen, Madeleine Gibescu, Damien Ernst, and Zita A Vale",
      "orig_title": "Sparse training theory for scalable and efficient agents",
      "paper_id": "2103.01636v1"
    },
    {
      "index": 45,
      "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Learning Representations",
      "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz",
      "orig_title": "Pruning convolutional neural networks for resource efficient inference",
      "paper_id": "1611.06440v2"
    },
    {
      "index": 46,
      "title": "Importance Estimation for Neural Network Pruning",
      "abstract": "",
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": "Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz",
      "orig_title": "Importance estimation for neural network pruning",
      "paper_id": "1906.10771v1"
    },
    {
      "index": 47,
      "title": "Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization",
      "abstract": "",
      "year": "2019",
      "venue": "36th International Conference on Machine Learning",
      "authors": "Hesham Mostafa and Xin Wang",
      "orig_title": "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization",
      "paper_id": "1902.05967v3"
    },
    {
      "index": 48,
      "title": "Efficient and robust feature selection via joint l2, 1-norms minimization",
      "abstract": "",
      "year": "2010",
      "venue": "Advances in neural information processing systems",
      "authors": "Feiping Nie, Heng Huang, Xiao Cai, and Chris Ding"
    },
    {
      "index": 49,
      "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "abstract": "",
      "year": "2005",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence",
      "authors": "Hanchuan Peng, Fuhui Long, and Chris Ding"
    },
    {
      "index": 50,
      "title": "Neural-network feature selector",
      "abstract": "",
      "year": "1997",
      "venue": "IEEE transactions on neural networks",
      "authors": "Rudy Setiono and Huan Liu"
    },
    {
      "index": 51,
      "title": "Deep Unsupervised Feature Selection by Discarding Nuisance and Correlated Features",
      "abstract": "",
      "year": "2022",
      "venue": "Neural Networks",
      "authors": "Uri Shaham, Ofir Lindenbaum, Jonathan Svirsky, and Yuval Kluger",
      "orig_title": "Deep unsupervised feature selection by discarding nuisance and correlated features",
      "paper_id": "2110.05306v1"
    },
    {
      "index": 52,
      "title": "FsNet: Feature Selection Network on High-dimensional Biological Data",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2001.08322",
      "authors": "Dinesh Singh and Makoto Yamada",
      "orig_title": "Fsnet: Feature selection network on high-dimensional biological data",
      "paper_id": "2001.08322v3"
    },
    {
      "index": 53,
      "title": "Dynamic Sparse Training for Deep Reinforcement Learning",
      "abstract": "",
      "year": "2021",
      "venue": "arXiv preprint arXiv:2106.04217",
      "authors": "Ghada Sokar, Elena Mocanu, Decebal Constantin Mocanu, Mykola Pechenizkiy, and Peter Stone",
      "orig_title": "Dynamic sparse training for deep reinforcement learning",
      "paper_id": "2106.04217v3"
    },
    {
      "index": 54,
      "title": "Evolving neural networks through augmenting topologies",
      "abstract": "",
      "year": "2002",
      "venue": "Evolutionary computation",
      "authors": "Kenneth O Stanley and Risto Miikkulainen"
    },
    {
      "index": 55,
      "title": "Regression shrinkage and selection via the lasso",
      "abstract": "",
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "authors": "Robert Tibshirani"
    },
    {
      "index": 56,
      "title": "Feature Importance Ranking for Deep Learning",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Maksymilian Wojtas and Ke Chen",
      "orig_title": "Feature importance ranking for deep learning",
      "paper_id": "2010.08973v1"
    },
    {
      "index": 57,
      "title": "High-dimensional feature selection by feature-wise kernelized lasso",
      "abstract": "",
      "year": "2014",
      "venue": "Neural computation",
      "authors": "Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P Xing, and Masashi Sugiyama"
    },
    {
      "index": 58,
      "title": "Feature selection using Stochastic Gates",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger",
      "orig_title": "Feature selection using stochastic gates",
      "paper_id": "1810.04247v7"
    },
    {
      "index": 59,
      "title": "Feature selection with multi-view data: A survey",
      "abstract": "",
      "year": "2019",
      "venue": "Information Fusion",
      "authors": "Rui Zhang, Feiping Nie, Xuelong Li, and Xian Wei"
    }
  ]
}