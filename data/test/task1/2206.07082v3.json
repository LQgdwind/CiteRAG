{
  "paper_id": "2206.07082v3",
  "title": "Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems",
  "abstract": "Abstract\nStochastic optimization has found wide applications in minimizing objective functions in machine learning, which motivates a lot of theoretical studies to understand its practical success. Most of existing studies focus on the convergence of optimization errors, while the generalization analysis of stochastic optimization is much lagging behind. This is especially the case for nonconvex and nonsmooth problems often encountered in practice. In this paper, we initialize a systematic stability and generalization analysis of stochastic optimization on nonconvex and nonsmooth problems. We introduce novel algorithmic stability measures and establish their quantitative connection on the gap between population gradients and empirical gradients, which is then further extended to study the gap between the Moreau envelope of the empirical risk and that of the population risk. To our knowledge, these quantitative connection between stability and generalization in terms of either gradients or Moreau envelopes have not been studied in the literature. We introduce a class of sampling-determined algorithms, for which we develop bounds for three stability measures. Finally, we apply these results to derive error bounds for stochastic gradient descent and its adaptive variant, where we show how to achieve an implicit regularization by tuning the step sizes and the number of iterations.",
  "reference_labels": [
    {
      "index": 0,
      "title": "Uniform stability for first-order empirical risk minimization",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Learning Theory",
      "authors": "A. Attia and T. Koren"
    },
    {
      "index": 1,
      "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "P. Bartlett and S. Mendelson"
    },
    {
      "index": 2,
      "title": "Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "R. Bassily, V. Feldman, C. Guzm√°n, and K. Talwar",
      "orig_title": "Stability of stochastic gradient descent on nonsmooth convex losses",
      "paper_id": "2006.06914v1"
    },
    {
      "index": 3,
      "title": "Optimization Methods for Large-Scale Machine Learning",
      "abstract": "",
      "year": "2018",
      "venue": "SIAM Review",
      "authors": "L. Bottou, F. E. Curtis, and J. Nocedal",
      "orig_title": "Optimization methods for large-scale machine learning",
      "paper_id": "1606.04838v3"
    },
    {
      "index": 4,
      "title": "The tradeoffs of large scale learning",
      "abstract": "",
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "O. Bousquet and L. Bottou"
    },
    {
      "index": 5,
      "title": "Stability and generalization",
      "abstract": "",
      "year": "2002",
      "venue": "Journal of Machine Learning Research",
      "authors": "O. Bousquet and A. Elisseeff"
    },
    {
      "index": 6,
      "title": "Sharper Bounds for Uniformly Stable Algorithms",
      "abstract": "",
      "year": "2020",
      "venue": "Conference on Learning Theory",
      "authors": "O. Bousquet, Y. Klochkov, and N. Zhivotovskiy",
      "orig_title": "Sharper bounds for uniformly stable algorithms",
      "paper_id": "1910.07833v2"
    },
    {
      "index": 7,
      "title": "On the generalization ability of on-line learning algorithms",
      "abstract": "",
      "year": "2004",
      "venue": "IEEE Transactions on Information Theory",
      "authors": "N. Cesa-Bianchi, A. Conconi, and C. Gentile"
    },
    {
      "index": 8,
      "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "Z. Charles and D. Papailiopoulos",
      "orig_title": "Stability and generalization of learning algorithms that converge to global optima",
      "paper_id": "1710.08402v1"
    },
    {
      "index": 9,
      "title": "Stability and Convergence Trade-off of Iterative Optimization Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1804.01619",
      "authors": "Y. Chen, C. Jin, and B. Yu",
      "orig_title": "Stability and convergence trade-off of iterative optimization algorithms",
      "paper_id": "1804.01619v1"
    },
    {
      "index": 10,
      "title": "Stochastic model-based minimization of weakly convex functions",
      "abstract": "",
      "year": "2019",
      "venue": "SIAM Journal on Optimization",
      "authors": "D. Davis and D. Drusvyatskiy"
    },
    {
      "index": 11,
      "title": "Graphical Convergence of Subgradients in Nonconvex Optimization and Learning",
      "abstract": "",
      "year": "2021",
      "venue": "Mathematics of Operations Research",
      "authors": "D. Davis and D. Drusvyatskiy",
      "orig_title": "Graphical convergence of subgradients in nonconvex optimization and learning",
      "paper_id": "1810.07590v2"
    },
    {
      "index": 12,
      "title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "abstract": "",
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Defazio, F. Bach, and S. Lacoste-Julien"
    },
    {
      "index": 13,
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "abstract": "",
      "year": "2010",
      "venue": "Conference on Learning Theory",
      "authors": "J. Duchi, E. Hazan, and Y. Singer"
    },
    {
      "index": 14,
      "title": "Differential privacy: A survey of results",
      "abstract": "",
      "year": "2008",
      "venue": "International conference on theory and applications of models of computation",
      "authors": "C. Dwork"
    },
    {
      "index": 15,
      "title": "Stability of randomized learning algorithms",
      "abstract": "",
      "year": "2005",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Elisseeff, T. Evgeniou, and M. Pontil"
    },
    {
      "index": 16,
      "title": "Near-optimal non-convex optimization via stochastic path integrated differential estimator",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "C. Fang, C. Li, Z. Lin, and T. Zhang"
    },
    {
      "index": 17,
      "title": "Generalization Bounds for Uniformly Stable Algorithms",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "V. Feldman and J. Vondrak",
      "orig_title": "Generalization bounds for uniformly stable algorithms",
      "paper_id": "1812.09859v2"
    },
    {
      "index": 18,
      "title": "High probability generalization bounds for uniformly stable algorithms with nearly optimal rate",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory",
      "authors": "V. Feldman and J. Vondrak"
    },
    {
      "index": 19,
      "title": "Uniform convergence of gradients for non-convex learning and optimization",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "D. J. Foster, A. Sekhari, and K. Sridharan"
    },
    {
      "index": 20,
      "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
      "abstract": "",
      "year": "2013",
      "venue": "SIAM Journal on Optimization",
      "authors": "S. Ghadimi and G. Lan"
    },
    {
      "index": 21,
      "title": "Learning theory of distributed spectral algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "Inverse Problems",
      "authors": "Z.-C. Guo, S.-B. Lin, and D.-X. Zhou"
    },
    {
      "index": 22,
      "title": "Train faster, generalize better: Stability of stochastic gradient descent",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "M. Hardt, B. Recht, and Y. Singer",
      "orig_title": "Train faster, generalize better: Stability of stochastic gradient descent",
      "paper_id": "1509.01240v2"
    },
    {
      "index": 23,
      "title": "Introduction to online convex optimization",
      "abstract": "",
      "year": "2016",
      "venue": "Foundations and Trends¬Æ in Optimization",
      "authors": "E. Hazan"
    },
    {
      "index": 24,
      "title": "Accelerating stochastic gradient descent using predictive variance reduction",
      "abstract": "",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "R. Johnson and T. Zhang"
    },
    {
      "index": 25,
      "title": "Adam: A method for stochastic optimization",
      "abstract": "",
      "year": "2015",
      "venue": "International Conference on Learning Representations",
      "authors": "D. P. Kingma and J. Ba"
    },
    {
      "index": 26,
      "title": "Stability and deviation optimal risk bounds with convergence rate o‚Äã(1/n)ùëú1ùëõo(1/n)",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Klochkov and N. Zhivotovskiy"
    },
    {
      "index": 27,
      "title": "Benign Underfitting of Stochastic Gradient Descent",
      "abstract": "",
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "T. Koren, R. Livni, Y. Mansour, and U. Sherman",
      "orig_title": "Benign underfitting of stochastic gradient descent",
      "paper_id": "2202.13361v4"
    },
    {
      "index": 28,
      "title": "Data-Dependent Stability of Stochastic Gradient Descent",
      "abstract": "",
      "year": "2018",
      "venue": "International Conference on Machine Learning",
      "authors": "I. Kuzborskij and C. Lampert",
      "orig_title": "Data-dependent stability of stochastic gradient descent",
      "paper_id": "1703.01678v4"
    },
    {
      "index": 29,
      "title": "Learning rates for stochastic gradient descent with nonconvex objectives",
      "abstract": "",
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": "Y. Lei and K. Tang"
    },
    {
      "index": 30,
      "title": "Fine-grained analysis of stability and generalization for stochastic gradient descent",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Machine Learning",
      "authors": "Y. Lei and Y. Ying"
    },
    {
      "index": 31,
      "title": "Generalization performance of multi-pass stochastic gradient descent with convex loss functions",
      "abstract": "",
      "year": "2021",
      "venue": "Journal of Machine Learning Research",
      "authors": "Y. Lei, T. Hu, and K. Tang"
    },
    {
      "index": 32,
      "title": "On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning",
      "abstract": "",
      "year": "2020",
      "venue": "International Conference on Learning Representations",
      "authors": "J. Li, X. Luo, and M. Qiao",
      "orig_title": "On generalization error bounds of noisy gradient methods for non-convex learning",
      "paper_id": "1902.00621v4"
    },
    {
      "index": 33,
      "title": "On the convergence of stochastic gradient descent with adaptive stepsizes",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "X. Li and F. Orabona"
    },
    {
      "index": 34,
      "title": "Exploring private federated learning with laplacian smoothing",
      "abstract": "",
      "year": "2020",
      "venue": "arXiv preprint arXiv:2005.00218",
      "authors": "Z. Liang, B. Wang, Q. Gu, S. Osher, and Y. Yao"
    },
    {
      "index": 35,
      "title": "Generalization properties and implicit regularization for multiple passes SGM",
      "abstract": "",
      "year": "2016",
      "venue": "International Conference on Machine Learning",
      "authors": "J. Lin, R. Camoriano, and L. Rosasco"
    },
    {
      "index": 36,
      "title": "Algorithmic Stability and Hypothesis Complexity",
      "abstract": "",
      "year": "2017",
      "venue": "International Conference on Machine Learning",
      "authors": "T. Liu, G. Lugosi, G. Neu, and D. Tao",
      "orig_title": "Algorithmic stability and hypothesis complexity",
      "paper_id": "1702.08712v2"
    },
    {
      "index": 37,
      "title": "Stability and generalization in structured prediction",
      "abstract": "",
      "year": "2016",
      "venue": "The Journal of Machine Learning Research",
      "authors": "B. London, B. Huang, and L. Getoor"
    },
    {
      "index": 38,
      "title": "Algorithmic stability and meta-learning",
      "abstract": "",
      "year": "2005",
      "venue": "Journal of Machine Learning Research",
      "authors": "A. Maurer"
    },
    {
      "index": 39,
      "title": "The landscape of empirical risk for nonconvex losses",
      "abstract": "",
      "year": "2018",
      "venue": "The Annals of Statistics",
      "authors": "S. Mei, Y. Bai, and A. Montanari"
    },
    {
      "index": 40,
      "title": "R√©nyi differential privacy",
      "abstract": "",
      "year": "2017",
      "venue": "IEEE 30th computer security foundations symposium (CSF)",
      "authors": "I. Mironov"
    },
    {
      "index": 41,
      "title": "Foundations of Machine Learning",
      "abstract": "",
      "year": "2012",
      "venue": "MIT press",
      "authors": "M. Mohri, A. Rostamizadeh, and A. Talwalkar"
    },
    {
      "index": 42,
      "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints",
      "abstract": "",
      "year": "2018",
      "venue": "Conference on Learning Theory",
      "authors": "W. Mou, L. Wang, X. Zhai, and K. Zheng",
      "orig_title": "Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints",
      "paper_id": "1707.05947v1"
    },
    {
      "index": 43,
      "title": "Beating sgd saturation with tail-averaging and minibatching",
      "abstract": "",
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "N. M√ºcke, G. Neu, and L. Rosasco"
    },
    {
      "index": 44,
      "title": "A method for solving the convex programming problem with convergence rate o (1/k21superscriptùëò21/k^{2})",
      "abstract": "",
      "year": "1983",
      "venue": "Dokl. akad. nauk Sssr",
      "authors": "Y. E. Nesterov"
    },
    {
      "index": 45,
      "title": "Generalization Bounds via Convex Analysis",
      "abstract": "",
      "year": "2022",
      "venue": "Conference on Learning Theory",
      "authors": "G. Neu and G. Lugosi",
      "orig_title": "Generalization bounds via convex analysis",
      "paper_id": "2202.04985v3"
    },
    {
      "index": 46,
      "title": "Information-Theoretic Generalization Bounds for Stochastic Gradient Descent",
      "abstract": "",
      "year": "2021",
      "venue": "Conference on Learning Theory",
      "authors": "G. Neu, G. K. Dziugaite, M. Haghifam, and D. M. Roy",
      "orig_title": "Information-theoretic generalization bounds for stochastic gradient descent",
      "paper_id": "2102.00931v3"
    },
    {
      "index": 47,
      "title": "Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes",
      "abstract": "",
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "L. Pillaud-Vivien, A. Rudi, and F. Bach",
      "orig_title": "Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes",
      "paper_id": "1805.10074v3"
    },
    {
      "index": 48,
      "title": "Stability results in learning theory",
      "abstract": "",
      "year": "2005",
      "venue": "Analysis and Applications",
      "authors": "A. Rakhlin, S. Mukherjee, and T. Poggio"
    },
    {
      "index": 49,
      "title": "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel",
      "abstract": "",
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "D. Richards and I. Kuzborskij",
      "orig_title": "Stability & generalisation of gradient descent for shallow neural networks without the neural tangent kernel",
      "paper_id": "2107.12723v2"
    },
    {
      "index": 50,
      "title": "Learning with Gradient Descent and Weakly Convex Losses",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "D. Richards and M. Rabbat",
      "orig_title": "Learning with gradient descent and weakly convex losses",
      "paper_id": "2101.04968v2"
    },
    {
      "index": 51,
      "title": "Controlling bias in adaptive data analysis using information theory",
      "abstract": "",
      "year": "2016",
      "venue": "Artificial Intelligence and Statistics",
      "authors": "D. Russo and J. Zou"
    },
    {
      "index": 52,
      "title": "Minimizing finite sums with the stochastic average gradient",
      "abstract": "",
      "year": "2017",
      "venue": "Mathematical Programming",
      "authors": "M. Schmidt, N. Le Roux, and F. Bach"
    },
    {
      "index": 53,
      "title": "Learnability, stability and uniform convergence",
      "abstract": "",
      "year": "2010",
      "venue": "Journal of Machine Learning Research",
      "authors": "S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan"
    },
    {
      "index": 54,
      "title": "Learning theory estimates via integral operators and their approximations",
      "abstract": "",
      "year": "2007",
      "venue": "Constructive approximation",
      "authors": "S. Smale and D.-X. Zhou"
    },
    {
      "index": 55,
      "title": "Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron",
      "abstract": "",
      "year": "2019",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "S. Vaswani, F. Bach, and M. Schmidt"
    },
    {
      "index": 56,
      "title": "AdaGrad stepsizes: Sharp convergence over nonconvex landscapes",
      "abstract": "",
      "year": "2020",
      "venue": "Journal of Machine Learning Research",
      "authors": "R. Ward, X. Wu, and L. Bottou",
      "orig_title": "Adagrad stepsizes: Sharp convergence over nonconvex landscapes",
      "paper_id": "1806.01811v8"
    },
    {
      "index": 57,
      "title": "Information-theoretic analysis of generalization capability of learning algorithms",
      "abstract": "",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "A. Xu and M. Raginsky",
      "orig_title": "Information-theoretic analysis of generalization capability of learning algorithms",
      "paper_id": "1705.07809v2"
    },
    {
      "index": 58,
      "title": "Stability and Risk Bounds of Iterative Hard Thresholding",
      "abstract": "",
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": "X. Yuan and P. Li",
      "orig_title": "Stability and risk bounds of iterative hard thresholding",
      "paper_id": "2203.09413v1"
    },
    {
      "index": 59,
      "title": "Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the ùëÇ‚Å¢(1/ùëá) Convergence Rate",
      "abstract": "",
      "year": "2019",
      "venue": "Conference on Learning Theory",
      "authors": "L. Zhang and Z.-H. Zhou",
      "orig_title": "Stochastic approximation of smooth and strongly convex functions: Beyond the $o(1/t)$ convergence rate",
      "paper_id": "1901.09344v1"
    },
    {
      "index": 60,
      "title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "abstract": "",
      "year": "2004",
      "venue": "International Conference on Machine Learning",
      "authors": "T. Zhang"
    },
    {
      "index": 61,
      "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization",
      "abstract": "",
      "year": "2018",
      "venue": "arXiv preprint arXiv:1808.05671",
      "authors": "D. Zhou, J. Chen, Y. Cao, Y. Tang, Z. Yang, and Q. Gu",
      "orig_title": "On the convergence of adaptive gradient methods for nonconvex optimization",
      "paper_id": "1808.05671v4"
    },
    {
      "index": 62,
      "title": "Towards better generalization of adaptive gradient methods",
      "abstract": "",
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems",
      "authors": "Y. Zhou, B. Karimi, J. Yu, Z. Xu, and P. Li"
    }
  ]
}