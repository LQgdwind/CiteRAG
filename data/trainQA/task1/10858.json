{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 60 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems Warning: This paper may contain content that has the potential to be offensive and harmful\n\nPaper Abstract: Abstract Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user’s chat history, all without the need to manipulate the user’s input or gain direct access to OpenAI GPT4. We have reported the discovered vulnerabilities to OpenAI and our project demo is placed in the following link: https://fzwark.github.io/LLM-System-Attack-Demo/\n\n{retrieved_text}\n\nGenerate exactly 60 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 60\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Bing Chat\",\n        \"Canva GPT\",\n        \"DocMaker ChatGPT Plugin\",\n        \"Github Copilot - Your AI pair programmer\",\n        \"Google images\",\n        \"GPTs Statistic Data\",\n        \"GPTs Store\",\n        \"Introducting ChatGPT\",\n        \"OpenAI Dev Day\",\n        \"OpenAI Plugins\",\n        \"Third-Party GPTs Store\",\n        \"WebPilot ChatGPT Plugin\",\n        \"Secure computer system: Unified exposition and multics interpretation\",\n        \"Sard: A software assurance reference dataset\",\n        \"Compositional security for reentrant applications\",\n        \"Jailbreaking black box large language models in twenty queries\",\n        \"Evaluation of chatgpt model for vulnerability detection\",\n        \"A lattice model of secure information flow\",\n        \"Mathematical capabilities of chatgpt\",\n        \"Security policies and security models\",\n        \"Chatgpt is not all you need. a state of the art review of large generative ai models\",\n        \"Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection\",\n        \"Markdown: Syntax\",\n        \"From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy\",\n        \"Look before you leap: An exploratory study of uncertainty measurement for large language models\",\n        \"Llm platform security: Applying a systematic evaluation framework to openai’s chatgpt plugins\",\n        \"Could an artificial-intelligence agent pass an introductory physics course?\",\n        \"Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt\",\n        \"Stolen memories: Leveraging model memorization for calibrated {{\\\\{White-Box}}} membership inference\",\n        \"Prompt Injection attack against LLM-integrated Applications, June 2023\",\n        \"Prompt Injection Attacks and Defenses in LLM-Integrated Applications, October 2023\",\n        \"X. 509 internet public key infrastructure online certificate status protocol-ocsp\",\n        \"Department of Defense Trusted Computer System Evaluation Criteria, volume 83\",\n        \"Asleep at the keyboard? assessing the security of github copilot’s code contributions\",\n        \"Examining zero-shot vulnerability repair with large language models\",\n        \"From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application, August 2023\",\n        \"Ignore Previous Prompt: Attack Techniques For Language Models, November 2022\",\n        \"Jatmo: Prompt Injection Defense by Task-Specific Finetuning, January 2024\",\n        \"Language models are unsupervised multitask learners\",\n        \"Smoothllm: Defending large language models against jailbreaking attacks\",\n        \"Language-based information-flow security\",\n        \"Maatphor: Automated Variant Analysis for Prompt Injection Attacks, December 2023\",\n        \"Access control: principle and practice\",\n        \"Privacy and data protection in chatgpt and other ai chatbots: Strategies for securing user information\",\n        \"An independent evaluation of chatgpt on mathematical word problems (mwp)\",\n        \"Survey of vulnerabilities in large language models revealed by adversarial attacks\",\n        \"Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications, January 2024\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game, November 2023\",\n        \"Decodingtrust: A comprehensive assessment of trustworthiness in gpt models\",\n        \"Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection, June 2023\",\n        \"Jailbreak and guard aligned language models with only few in-context demonstrations\",\n        \"Unveiling security, privacy, and ethical concerns of chatgpt\",\n        \"A language for automatically enforcing privacy policies\",\n        \"Benchmarking and defending against indirect prompt injection attacks on large language models\",\n        \"A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models, January 2024\",\n        \"Assessing Prompt Injection Risks in 200+ Custom GPTs, November 2023\",\n        \"Secure program partitioning\",\n        \"Using replication and partitioning to build secure distributed systems\",\n        \"Universal and transferable adversarial attacks on aligned language models\"\n    ],\n    \"reasoning\": \"Selected these 60 references because they represent foundational work and recent advances in A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems Warning: This paper may contain content that has the potential to be offensive and harmful, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}