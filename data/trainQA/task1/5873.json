{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 128 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey of Deep Learning Models for Structural Code Understanding\n\nPaper Abstract: Abstract. In recent years, the rise of deep learning and automation requirements in the software industry has elevated Intelligent Software Engineering to new heights. The number of approaches and applications in code understanding is growing, with deep learning techniques being used in many of them to better capture the information in code data. In this survey, we present a comprehensive overview of the structures formed from code data. We categorize the models for understanding code in recent years into two groups: sequence-based and graph-based models, further make a summary and comparison of them111We provide a paper collection about deep learning models and datasets for structural code understanding in https://github.com/codingClaire/Structural-Code-Understanding. We also introduce metrics, datasets and the downstream tasks. Finally, we make some suggestions for future research in structural code understanding field.\n\n{retrieved_text}\n\nGenerate exactly 128 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 128\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Can Machines Read Coding Manuals Yet? – A Benchmark for Building Better Language Models for Code Understanding\",\n        \"A transformer-based approach for source code summarization\",\n        \"The adverse effects of code duplication in machine learning models of code\",\n        \"A survey of machine learning for big code and naturalness\",\n        \"Marc Brockschmidt\",\n        \"Self-Supervised Bug Detection and Repair\",\n        \"A Convolutional Attention Network for Extreme Summarization of Source Code\",\n        \"code2seq: Generating Sequences from Structured Representations of Code\",\n        \"A General Path-Based Representation for Predicting Program Properties\",\n        \"Meital Zilberstein\",\n        \"Code2Vec: Learning Distributed Representations of Code\",\n        \"SCC: Automatic Classification of Code Snippets\",\n        \"Neural Attribute Machines for Program Generation\",\n        \"Assessing the Effectiveness of Syntactic Structure to Learn Code Edit Representations\",\n        \"Code Completion using Neural AAention and Byte Pair Encoding\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Satanjeev Banerjee and Alon Lavie\",\n        \"BULNER: BUg Localization with word embeddings and NEtwork Regularization\",\n        \"Comparison and Evaluation of Clone Detection Tools\",\n        \"Neural Code Comprehension: A Learnable Representation of Code Semantics\",\n        \"Learning Deep Architectures for AI\",\n        \"Learning Python Code Suggestion with a Sparse Pointer Network\",\n        \"Miltiadis Allamanis\",\n        \"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation\",\n        \"CODIT: Code Editing with Tree-Based Neural Models\",\n        \"DeepCPDP: Deep Learning Based Cross-Project Defect Prediction\",\n        \"SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair\",\n        \"A Literature Study of Embeddings on Source Code\",\n        \"Learning phrase representations using RNN encoder-decoder for statistical machine translation\",\n        \"Funnel-transformer: Filtering out sequential redundancy for efficient language processing\",\n        \"A deep language model for software code\",\n        \"CoNCRA: A Convolutional Neural Network Code Retrieval Approach\",\n        \"Modelling‚ Visualising and Summarising Documents with a Single Convolutional Neural Network\",\n        \"Asm2Vec: Boosting static representation robustness for binary clone search against code obfuscation and compiler optimization\",\n        \"Self-Attention Networks for Code Search\",\n        \"CodeBERT: A Pre-Trained Model for Programming and Natural Languages\",\n        \"The program dependence graph and its use in optimization\",\n        \"Using convolutional neural networks for classification of malware represented as images\",\n        \"Samuel S Schoenholz\",\n        \"Graphcodebert: Pre-training code representations with data flow\",\n        \"CoDesc: A Large Code-Description Parallel Dataset\",\n        \"Momentum Contrast for Unsupervised Visual Representation Learning\",\n        \"Teaching machines to read and comprehend\",\n        \"Long Short-Term Memory\",\n        \"Neural joint attention code search over structure embeddings for software Q&A sites\",\n        \"Deep code comment generation with hybrid lexical and syntactical information\",\n        \"CoSQA: 20,000+ Web Queries for Code Search and Question Answering\",\n        \"Codesearchnet challenge: Evaluating the state of semantic code search\",\n        \"CodeGRU: Context-aware deep learning with gated recurrent unit for source code modeling\",\n        \"Code Clone Detection with Hierarchical Attentive Graph Embedding\",\n        \"BugGraph: Differentiating Source-Binary Code Similarity with Graph Triplet-Loss Network\",\n        \"TreeBERT: A Tree-Based Pre-Trained Model for Programming Language\",\n        \"Edward Grefenstette\",\n        \"Learning and Evaluating Contextual Embedding of Source Code\",\n        \"Gogul Balakrishnan\",\n        \"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\",\n        \"Code Prediction by Feeding Trees to Transformers\",\n        \"Semi-Supervised Classification with Graph Convolutional Networks\",\n        \"Chris Lattner and Vikram Adve\",\n        \"Deep learning at the shallow end: Malware classification for non-domain experts\",\n        \"Deep Learning for Source Code Modeling and Generation: Models, Applications, and Challenges\",\n        \"Alexander LeClair and Collin McMillan\",\n        \"Semantic Code Clone Detection Via Event Embedding Tree and GAT Network\",\n        \"Neural Code Search Evaluation Dataset\",\n        \"Code Completion with Neural Attention and Pointer Networks\",\n        \"CodeRetriever: Unimodal and Bimodal Contrastive Learning\",\n        \"Gated graph sequence neural networks\",\n        \"Improving bug detection via context-based code representation learning and attention-based neural networks\",\n        \"VulDeePecker: A Deep Learning-Based System for Vulnerability Detection\",\n        \"Seml: A semantic LSTM model for software defect prediction\",\n        \"Luke Zettlemoyer\",\n        \"Eui Chul Richard Shin\",\n        \"Neural Code Completion\",\n        \"A self-attentional neural architecture for code completion with multi-task learning\",\n        \"Modeling programs hierarchically with stack-augmented LSTM\",\n        \"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN\",\n        \"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks\",\n        \"Combining Graph Neural Networks with Expert Knowledge for Smart Contract Vulnerability Detection\",\n        \"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\",\n        \"Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"TBCNN: A tree-based convolutional neural network for programming language processing\",\n        \"Tung Thanh Nguyen\",\n        \"Auto-detection of sophisticated malware using lazy-binding control flow graph and deep learning\",\n        \"How could Neural Networks understand Programs?\",\n        \"CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks\",\n        \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n        \"Probabilistic model for code with decision trees\",\n        \"Code completion with statistical language models\",\n        \"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis\",\n        \"Learning representations by back-propagating errors\",\n        \"Retrieval on source code: a neural code search\",\n        \"Self-attention with relative position representations\",\n        \"CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees\",\n        \"PathPair2Vec: An AST path pair-based code representation method for defect prediction\",\n        \"Yasuaki Kobayashi\",\n        \"Neural Code Summarization\",\n        \"Flow2Vec: value-flow-based precise code embedding\",\n        \"Pythia: AI-assisted Code Completion System\",\n        \"Subhodeep Moitra\",\n        \"Language-agnostic Representation Learning of Source Code from Structure and Context\",\n        \"Neural Program Repair by Jointly Learning to Localize and Repair\",\n        \"Arantxa Casanova\",\n        \"Multi-modal attention network learning for semantic source code retrieval\",\n        \"Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding\",\n        \"Learning Scalable and Precise Representation of Program Semantics\",\n        \"Deep Semantic Feature Learning for Software Defect Prediction\",\n        \"Towards Full-line Code Completion with Neural Language Models\",\n        \"Learning to Represent Programs with Heterogeneous Graphs\",\n        \"CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network\",\n        \"Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs\",\n        \"Learning semantic program embeddings with graph interval neural network\",\n        \"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation\",\n        \"Christopher Vendome\",\n        \"GGF: A graph-based method for programming language syntax error correction\",\n        \"A Comprehensive Survey on Graph Neural Networks\",\n        \"Improving Tree-Structured Decoder Training for Code Generation via Mutual Learning\",\n        \"Method name suggestion with hierarchical attention networks\",\n        \"CCMC: Code Completion with a Memory Mechanism and a Copy Mechanism; CCMC: Code Completion with a Memory Mechanism and a Copy Mechanism\",\n        \"Graph embedding code prediction model integrating semantic features\",\n        \"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\",\n        \"Michihiro Yasunaga and Percy Liang\",\n        \"Bogdan Vasilescu\",\n        \"Pengcheng Yin and Graham Neubig\",\n        \"Retrieval-based neural source code summarization\",\n        \"A Novel Neural Source Code Representation Based on Abstract Syntax Tree\",\n        \"DeepSim: Deep Learning Code Functional Similarity\",\n        \"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\"\n    ],\n    \"reasoning\": \"Selected these 128 references because they represent foundational work and recent advances in A Survey of Deep Learning Models for Structural Code Understanding, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}