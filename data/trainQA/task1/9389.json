{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: On the Adversarial Robustness of Generative Autoencoders in the Latent Space\n\nPaper Abstract: Abstract The generative autoencoders, such as the variational autoencoders or the adversarial autoencoders, have achieved great success in lots of real-world applications, including image generation, and signal communication. However, little concern has been devoted to their robustness during practical deployment. Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent. This leaves a back door for malicious attackers to collapse VAEs from the latent space, especially in scenarios where the encoder and decoder are used separately, such as communication and compressed sensing. In this work, we provide the first study on the adversarial robustness of generative autoencoders in the latent space. Specifically, we empirically demonstrate the latent vulnerability of popular generative autoencoders through attacks in the latent space. We also evaluate the difference between variational autoencoders and their deterministic variants and observe that the latter performs better in latent robustness. Meanwhile, we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes. Additionally, we also verify the feasibility of improvement for the latent robustness of VAEs through adversarial training. In summary, we suggest concerning the adversarial latent robustness of the generative autoencoders, analyze several robustness-relative issues, and give some insights into a series of key challenges.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Auto-encoding variational bayes\",\n        \"Stochastic backpropagation and approximate inference in deep generative models\",\n        \"Adversarial autoencoders\",\n        \"From variational to deterministic autoencoders\",\n        \"Variational autoencoder for low bit-rate image compression\",\n        \"Variational autoencoder for reference based image super-resolution\",\n        \"Recurrent neural network-based semantic variational autoencoder for sequence-to-sequence learning\",\n        \"A hybrid convolutional variational autoencoder for text generation\",\n        \"Anomaly detection of time series with smoothness-inducing sequential variational auto-encoder\",\n        \"Pfvae: a planar flow-based variational auto-encoder prediction model for time series data\",\n        \"Towards the first adversarially robust neural network model on mnist\",\n        \"Improving vaes’ robustness to adversarial attack\",\n        \"Interventional assays for the latent space of autoencoders\",\n        \"Taming vaes\",\n        \"Variational autoencoder with learned latent structure\",\n        \"Understanding posterior collapse in generative latent variable models\",\n        \"Characterization of minimum error linear coding with sensory and neural noise\",\n        \"Opening the black box of deep neural networks in physical layer communication\",\n        \"Adversarial examples for generative models\",\n        \"Resisting adversarial attacks using gaussian mixture variational autoencoders\",\n        \"Type i attack for generative models\",\n        \"Diagnosing vulnerability of variational auto-encoders to adversarial attacks\",\n        \"Regularization with latent space virtual adversarial training\",\n        \"Lafeat: Piercing through adversarial defenses with latent features\",\n        \"Reliably fast adversarial training via latent adversarial perturbation\",\n        \"Beta-vae: Learning basic visual concepts with a constrained variational framework\",\n        \"Isolating sources of disentanglement in variational autoencoders\",\n        \"Recent advances in autoencoder-based representation learning\",\n        \"A kernel two-sample test\",\n        \"Information theoretic learning: Renyi’s entropy and kernel perspectives\",\n        \"Information theoretic-learning auto-encoder\",\n        \"Understanding autoencoders with information theoretic concepts\",\n        \"On the adversarial robustness of hypothesis testing\",\n        \"Approximating the shapiro-wilk w-test for non-normality\",\n        \"Deep generative models: Survey\",\n        \"Semi-deterministic and contrastive variational graph autoencoder for recommendation\",\n        \"Couple-vae: Mitigating the encoder-decoder incompatibility in variational text modeling with coupled deterministic networks\",\n        \"Deterministic decoding for discrete data in variational autoencoders\",\n        \"Exemplar vae: Linking generative models, nearest neighbor retrieval, and data augmentation\",\n        \"Adversarial attacks on variational autoencoders\",\n        \"Understanding and improving interpolation in autoencoders via an adversarial regularizer\",\n        \"Towards deep learning models resistant to adversarial attacks\",\n        \"Image quality metrics: Psnr vs. ssim\",\n        \"Image quality assessment: from error visibility to structural similarity\",\n        \"Improved techniques for training gans\",\n        \"Gans trained by a two time-scale update rule converge to a local nash equilibrium\",\n        \"The unreasonable effectiveness of deep features as a perceptual metric\",\n        \"TorchMetrics-Measuring Reproducibility in PyTorch\",\n        \"Visualizing data using t-sne\",\n        \"Representation learning: A review and new perspectives\",\n        \"Disentangling improves vaes’ robustness to adversarial attacks\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in On the Adversarial Robustness of Generative Autoencoders in the Latent Space, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}