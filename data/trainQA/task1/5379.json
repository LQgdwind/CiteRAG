{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 52 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Polynomial Interpolation and Division in Soft-linear Time\n\nPaper Abstract: Abstract Given a way to evaluate an unknown polynomial with integer coefficients, we present new algorithms to recover its nonzero coefficients and corresponding exponents. As an application, we adapt this interpolation algorithm to the problem of computing the exact quotient of two given polynomials. These methods are efficient in terms of the bit-length of the sparse representation, that is, the number of nonzero terms, the size of coefficients, the number of variables, and the logarithm of the degree. At the core of our results is a new Monte Carlo randomized algorithm to recover a polynomial f‚Äã(x)ùëìùë•f(x) with integer coefficients given a way to evaluate f‚Äã(Œ∏)modmmoduloùëìùúÉùëöf(\\theta)\\bmod m for any chosen integers Œ∏ùúÉ\\theta and mùëöm. This algorithm has nearly-optimal bit complexity, meaning that the total bit-length of the probes, as well as the computational running time, is softly linear (ignoring logarithmic factors) in the bit-length of the resulting sparse polynomial. To our knowledge, this is the first sparse interpolation algorithm with soft-linear bit complexity in the total output size. For polynomials with integer coefficients, the best previously known results have at least a cubic dependency on the bit-length of the exponents.\n\n{retrieved_text}\n\nGenerate exactly 52 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 52\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A variant of the Bombieri-Vinogradov theorem with explicit constants and applications\",\n        \"epsilon-discrepancy sets and their application for interpolation of sparse polynomials\",\n        \"Sparse Polynomial Interpolation and Testing\",\n        \"Sparse interpolation over finite fields via low-order roots of unity\",\n        \"Faster sparse multivariate polynomial interpolation of straight-line programs\",\n        \"Output-Sensitive Algorithms for Sumset and Sparse Polynomial Multiplication\",\n        \"A Deterministic Algorithm for Sparse Multivariate Polynomial Interpolation\",\n        \"Deterministically Testing Sparse Polynomial Identities of Unbounded Degree\",\n        \"A Linear Filtering Approach to the Computation of Discrete Fourier Transform\",\n        \"A new deterministic algorithm for sparse multivariate polynomial interpolation\",\n        \"Tellegen‚Äôs Principle into Practice\",\n        \"Sparse interpolation of multivariate rational functions\",\n        \"On the Equivalence Between Berlekamp‚Äôs and Euclid‚Äôs Algorithms\",\n        \"Interpolation of polynomials given by straight-line programs\",\n        \"Parallel sparse multivariate polynomial division\",\n        \"Diversification improves interpolation\",\n        \"Random primes in arithmetic progressions\",\n        \"Essentially optimal sparse polynomial multiplication\",\n        \"On exact division and divisibility testing for sparse polynomials\",\n        \"Polynomial modular product verification and its implications\",\n        \"Fast parallel algorithms for sparse multivariate polynomial interpolation over finite fields\",\n        \"Probably faster multiplication of sparse polynomials\",\n        \"Sparse Polynomial Interpolation in Practice\",\n        \"Sparse polynomial interpolation. Exploring fast heuristic algorithms over finite fields\",\n        \"On sparse interpolation of rational functions and gcds\",\n        \"Interpolation of Sparse Multivariate Polynomials over Large Finite Fields with Applications\",\n        \"Sparse Polynomial Interpolation over Fields with Large or Zero Characteristic\",\n        \"Sparse Polynomial Interpolation Based on Derivative\",\n        \"Sparse polynomial interpolation based on diversification\",\n        \"Revisit Sparse Polynomial Interpolation Based on Randomized Kronecker Substitution\",\n        \"Faster interpolation algorithms for sparse multivariate polynomials given by straight-line programs\",\n        \"Parallel sparse polynomial interpolation over finite fields\",\n        \"Sparse polynomial arithmetic\",\n        \"Early termination in sparse interpolation algorithms\",\n        \"Fifteen years after DSC and WLSS2: What parallel computations I do today [invited lecture at PASCO 2010]\",\n        \"Modular rational sparse multivariate polynomial interpolation\",\n        \"Supersparse black box rational function interpolation\",\n        \"Improved Sparse Multivariate Polynomial Interpolation Algorithms\",\n        \"On exact and approximate interpolation of sparse rational functions\",\n        \"Grundz√ºge einer arithmetischen Theorie der algebraischen Gr√∂ssen\",\n        \"Randomized Interpolation and Approximation of Sparse Polynomials\",\n        \"Polynomial Division Using Dynamic Arrays, Heaps, and Packed Exponent Vectors\",\n        \"Parallel sparse polynomial multiplication using heaps\",\n        \"Sparse polynomial division using a heap\",\n        \"Modular Algorithm for Sparse Multivariate Polynomial Interpolationand its Parallel Implementation\",\n        \"Nearly Optimal Sparse Polynomial Multiplication\",\n        \"Public-Key Cryptosystems Based on Composite Degree Residuosity Classes\",\n        \"What Can (and Can‚Äôt) we Do with Sparse Polynomials?\",\n        \"Estimations du type Brun-Titchmarsh\",\n        \"Schnelle Berechnung von Kettenbruchentwicklungen\",\n        \"A partial Bombieri‚ÄìVinogradov theorem with explicit constants\",\n        \"Interpolating polynomials from their values\"\n    ],\n    \"reasoning\": \"Selected these 52 references because they represent foundational work and recent advances in Sparse Polynomial Interpolation and Division in Soft-linear Time, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}