{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 61 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Testing the Quantitative Spacetime Hypothesis using Artificial Narrative Comprehension (I) Bootstrapping Meaning from Episodic Narrative viewed as a Feature Landscape\n\nPaper Abstract: Abstract The problem of extracting important and meaningful parts of a sensory data stream, without prior training, is studied for symbolic sequences, by using textual narrative as a test case. This is part of a larger study concerning the extraction of concepts from spacetime processes, and their knowledge representations within hybrid symbolic-learning ‘Artificial Intelligence’. Most approaches to text analysis make extensive use of the evolved human sense of language and semantics. In this work, streams are parsed without knowledge of semantics, using only measurable patterns (size and time) within the changing stream of symbols—as an event ‘landscape’. This is a form of interferometry. Using lightweight procedures that can be run in just a few seconds on a single CPU, this work studies the validity of the Semantic Spacetime Hypothesis, for the extraction of concepts as process invariants. This ‘semantic preprocessor’ may then act as a front-end for more sophisticated long-term graph-based learning techniques. The results suggest that what we consider important and interesting about sensory experience is not solely based on higher reasoning, but on simple spacetime process cues, and this may be how cognitive processing is bootstrapped in the beginning.\n\n{retrieved_text}\n\nGenerate exactly 61 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 61\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A spacetime approach to generalized cognitive reasoning in multi-scale learning\",\n        \"Patterns and Meanings: Using corpora for English language research and teaching\",\n        \"The Text Mining Handbook\",\n        \"Automated Text Summarization\",\n        \"A survey of automated text summarization\",\n        \"Spacetimes with semantics (i)\",\n        \"Spacetimes with semantics (ii)\",\n        \"Spacetimes with semantics (iii)\",\n        \"The Mathematical Theory of Communication\",\n        \"Prediction and entropy of printed english\",\n        \"Byond the zipf-mandelbrot law in quantitative linguistics\",\n        \"Complexity and universality in the long-range order of words\",\n        \"Towards the quantification of the semantic information encoded in written language\",\n        \"The Psychobiology of Language\",\n        \"An informational theory of the statistical structure of language\",\n        \"A latent semantic analysis framework for large–span language modeling\",\n        \"A neural probabilistic language model\",\n        \"Efficient estimation of word representations in vector space\",\n        \"Genes, Peoples and Languages\",\n        \"Computer immunology\",\n        \"Principles of a computer immune system\",\n        \"Intrusion detection using sequences of system calls\",\n        \"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\",\n        \"Immunology for physicists\",\n        \"The immune system, adaptation, and machine learning\",\n        \"Tolerance, danger and the extended family\",\n        \"The danger theory and its application to artificial immune systems\",\n        \"Configurable immunity model of evolving configuration management\",\n        \"Probabilistic anomaly detection in distributed computer networks\",\n        \"Promise Theory: Principles and Applications (second edition)\",\n        \"Human-understandable inference of causal relationships\",\n        \"Compass and direction in topic maps\",\n        \"Project gutenberg\",\n        \"Implementing a generalized tool for network monitoring\",\n        \"Bootstrapping a data mining intrusion detection system\",\n        \"Unsupervised learning techniques for an intrusion detection system\",\n        \"Data mining-based intrusion detectors: an overview of the columbia ids project\",\n        \"Approaches to the Typology of Word Classes, chapter Lexical prototypes as a universal basis for cross-linguistic identification for “parts of speech”\",\n        \"Approaches to the Typology of Word Classes, chapter When can a language have adjectives? An implication universal\",\n        \"Slogans: The End of Sympathy\",\n        \"The structure and function of complex networks\",\n        \"Zipf’s word frequency law in natural language: A critical review and future directions\",\n        \"Small Worlds\",\n        \"Linked\",\n        \"On certain formal properties of grammars\",\n        \"Automated reponse using system-call delays\",\n        \"Biological Sequence Analysis\",\n        \"Bioinformatics: The Machine Learning Approach\",\n        \"All the words from google books, sorted by frequency, March 2019\",\n        \"Timeseries interferometry in distributed systems (proposal)\",\n        \"Elements of Information Theory\",\n        \"Two dimensional time-series for anomaly detection and regulation in adaptive systems\",\n        \"Unsupervised emotion detection from text using semantic and syntactic relations\",\n        \"Interdisciplinary approach to emotion detection from text\",\n        \"Thinking, Fast and Slow\",\n        \"From observability to significance in distributed systems\",\n        \"Presidential inaugural speech\",\n        \"Universal grammar, statistics or both?\",\n        \"The Language Instinct\",\n        \"The Unfolding of Language\",\n        \"Cognitive Grammar, A Basic Introduction\"\n    ],\n    \"reasoning\": \"Selected these 61 references because they represent foundational work and recent advances in Testing the Quantitative Spacetime Hypothesis using Artificial Narrative Comprehension (I) Bootstrapping Meaning from Episodic Narrative viewed as a Feature Landscape, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}