{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Modes of Homogeneous Gradient FlowsIdo would like to thank Prof. Andrea Bertozzi for the opportunity of studying in mathematics science department, UCLA in general and for helpful conversations, related to this work, in particular. We thank Prof. Ronen Talmon for stimulating discussions. And we also would like to thank Shachar Praisler for his helpfull advice. Funding: This work was supported by the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No. 777826 (NoMADS). GG acknowledges support by the Israel Science Foundation (Grant No. 534/19) and by the Ollendorff Minerva Center.\n\nPaper Abstract: Abstract Finding latent structures in data is drawing increasing attention in diverse fields such as image and signal processing, fluid dynamics, and machine learning. In this work we examine the problem of finding the main modes of gradient flows. Gradient descent is a fundamental process in optimization where its stochastic version is prominent in training of neural networks. Here our aim is to establish a consistent theory for gradient flows œàt=P‚Äã(œà)subscriptùúìùë°ùëÉùúì\\psi_{t}=P(\\psi), where PùëÉP is a nonlinear homogeneous operator. Our proposed framework stems from analytic solutions of homogeneous flows, previously formalized by Cohen-Gilboa, where the initial condition œà0subscriptùúì0\\psi_{0} admits the nonlinear eigenvalue problem P‚Äã(œà0)=Œª‚Äãœà0ùëÉsubscriptùúì0ùúÜsubscriptùúì0P(\\psi_{0})=\\lambda\\psi_{0}. We first present an analytic solution for Dynamic Mode Decomposition (DMD) in such cases. We show an inherent flaw of DMD, which is unable to recover the essential dynamics of the flow. It is evident that DMD is best suited for homogeneous flows of degree one. We propose an adaptive time sampling scheme and show its dynamics are analogue to homogeneous flows of degree one with a fixed step size. Moreover, we adapt DMD to yield a real spectrum, using symmetric matrices. Our analytic solution of the proposed scheme recovers the dynamics perfectly and yields zero error. We then proceed to show that in the general case the orthogonal modes {œïi}subscriptitalic-œïùëñ\\{\\phi_{i}\\} are approximately nonlinear eigenfunctions P‚Äã(œïi)‚âàŒªi‚ÄãœïiùëÉsubscriptitalic-œïùëñsubscriptùúÜùëñsubscriptitalic-œïùëñP(\\phi_{i})\\approx\\lambda_{i}\\phi_{i}. We formulate Orthogonal Nonlinear Spectral decomposition (OrthoNS), which recovers the essential latent structures of the gradient descent process. Definitions for spectrum and filtering are given, and a Parseval-type identity is shown. Experimental results on images, show the resemblance to direct computations of nonlinear sepctral decomposition. A significant speedup (by about two orders of magnitude) is achieved for this application using the proposed method.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Wavelet contraction, wavelet expansion, and the control of seismic resolution\",\n        \"Image denoising via learned dictionaries and sparse representation\",\n        \"The finite harmonic oscillator and its applications to sequences, communication, and radar\",\n        \"Sparsity-promoting dynamic mode decomposition\",\n        \"Preconditioned dynamic mode decomposition and mode selection algorithms for large datasets using incremental proper orthogonal decomposition\",\n        \"Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks\",\n        \"On spectral clustering: Analysis and an algorithm\",\n        \"Self-supervised unconstrained illumination invariant representation\",\n        \"Spectralnet: Spectral clustering using deep neural networks\",\n        \"Singan: Learning a generative model from a single natural image\",\n        \"Spectral decompositions using one-homogeneous functionals\",\n        \"Optoacoustic model-based inversion using anisotropic adaptive total-variation regularization\",\n        \"Dynamic mode decomposition of numerical and experimental data\",\n        \"Crime topic modeling\",\n        \"Online control with adversarial disturbances\",\n        \"Optimization methods for large-scale machine learning\",\n        \"Laplacian smoothing gradient descent\",\n        \"Non-stochastic control with bandit feedback\",\n        \"Towards provable control for unknown linear dynamical systems\",\n        \"Variable projection methods for an optimized dynamic mode decomposition\",\n        \"Dynamic mode decomposition for non-uniformly sampled data\",\n        \"A dynamic mode decomposition approach for large and arbitrarily sampled systems\",\n        \"Hamiltonian systems and transformation in hilbert space\",\n        \"Spectral properties of dynamical systems, model reduction and decompositions\",\n        \"Introducing the p-laplacian spectra\",\n        \"Stable explicit p-laplacian flows based on nonlinear eigenvalue analysis\",\n        \"A spectral approach to total variation\",\n        \"A total variation spectral framework for scale and texture analysis\",\n        \"Nonlinear spectral decompositions by gradient flows of one-homogeneous functionals\",\n        \"On the scale-space of filters and their applications\",\n        \"Nonlinear Eigenproblems in Image Processing and Computer Vision\",\n        \"Numerical methods for the solution of ill-posed problems, volume 328\",\n        \"Nonlinear total variation based noise removal algorithms\",\n        \"p-laplacian driven image processing\",\n        \"PDE evolutions for M-smoothers in one, two, and three dimensions\",\n        \"Linear algebra and learning from data\",\n        \"Numerical linear algebra, volume 50\",\n        \"Dynamic mode decomposition: data-driven modeling of complex systems\",\n        \"De-biasing the dynamic mode decomposition for applied koopman spectral analysis of noisy datasets\",\n        \"Effects of small noise on the dmd/koopman spectrum\",\n        \"Characterizing and correcting for the effect of sensor noise in the dynamic mode decomposition\",\n        \"Consistent dynamic mode decomposition\",\n        \"Dynamic mode decomposition using a kalman filter for parameter estimation\",\n        \"Extended-kalman-filter-based dynamic mode decomposition for simultaneous system identification and denoising\",\n        \"A data‚Äìdriven approximation of the koopman operator: Extending dynamic mode decomposition\",\n        \"Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the koopman operator\",\n        \"Forecasting sequential data using consistent koopman autoencoders\",\n        \"Energy dissipating flows for solving nonlinear eigenpair problems\",\n        \"Anisotropic diffusion in image processing, volume 1\",\n        \"Nonlinear spectral theory, volume 10\",\n        \"The optimal hard threshold for singular values is 4/3434/\\\\sqrt{3}\",\n        \"Solution of the equation ax+xb=c by inversion of an m*m or n*n matrix\",\n        \"Solution of the matrix equation ax- xb= c\",\n        \"Solution of the matrix equation ax+ xb= c [f4]\",\n        \"Stability by lyapunov‚Äôs direct method\",\n        \"Matrix calculations for liapunov quadratic forms\",\n        \"Analysis and synthesis of stability matrices\",\n        \"Op√©rateurs maximaux monotones et semi-groupes de contractions dans les espaces de Hilbert, volume 5\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Modes of Homogeneous Gradient FlowsIdo would like to thank Prof. Andrea Bertozzi for the opportunity of studying in mathematics science department, UCLA in general and for helpful conversations, related to this work, in particular. We thank Prof. Ronen Talmon for stimulating discussions. And we also would like to thank Shachar Praisler for his helpfull advice. Funding: This work was supported by the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No. 777826 (NoMADS). GG acknowledges support by the Israel Science Foundation (Grant No. 534/19) and by the Ollendorff Minerva Center., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}