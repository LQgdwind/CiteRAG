{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 362 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey of Monte Carlo Methods for Parameter Estimation\n\nPaper Abstract: Abstract Statistical signal processing applications usually require the estimation of some parameters of interest given a set of observed data. These estimates are typically obtained either by solving a multi-variate optimization problem, as in the maximum likelihood (ML) or maximum a posteriori (MAP) estimators, or by performing a multi-dimensional integration, as in the minimum mean squared error (MMSE) estimators. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and the Monte Carlo (MC) methodology is one feasible approach. MC methods proceed by drawing random samples, either from the desired distribution or from a simpler one, and using them to compute consistent estimators. The most important families of MC algorithms are Markov chain MC (MCMC) and importance sampling (IS). On the one hand, MCMC methods draw samples from a proposal density, building then an ergodic Markov chain whose stationary distribution is the desired distribution by accepting or rejecting those candidate samples as the new state of the chain. On the other hand, IS techniques draw samples from a simple proposal density, and then assign them suitable weights that measure their quality in some appropriate way. In this paper, we perform a thorough review of MC methods for the estimation of static parameters in signal processing applications. A historical note on the development of MC schemes is also provided, followed by the basic MC method and a brief description of the rejection sampling (RS) algorithm, as well as three sections describing many of the most relevant MCMC and IS algorithms, and their combined use. Finally, five numerical examples (including the estimation of the parameters of a chaotic system, a localization problem in wireless sensor networks and a spectral analysis application) are provided in order to demonstrate the performance of the described approaches.\n\n{retrieved_text}\n\nGenerate exactly 362 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 362\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"R.L.: Statistical Inference. Duxbury\",\n        \"Statistical Inference\",\n        \"Statistical Signal Processing\",\n        \"Fundamentals of Statistical Signal Processing: Estimation Theory\",\n        \"Detection, Estimation, and Modulation Theory, Part I: Detection, Estimation, and Linear Modulation Theory\",\n        \"Digital Audio Restoration, 1st edn.\",\n        \"Blind Deconvolution\",\n        \"Blind Equalization and Identification\",\n        \"Handbook of Blind Source Separation: Independent Component Analysis and Applications\",\n        \"Wireless Sensor Networks: An Information Processing Approach\",\n        \"Wireless Sensor Networks: Signal Processing and Communications\",\n        \"Bayesian theory\",\n        \"Methods for approximating integrals in statistics with special emphasis on Bayesian integration problems\",\n        \"The variational approximation for Bayesian inference\",\n        \"A tutorial on variational Bayesian inference\",\n        \"The St. Petersburg paradox\",\n        \"Solutions des probl√©mes qui regardoient le jeu du franc carreau\",\n        \"Essai d‚Äôarithm√©tique morale\",\n        \"Th√©orie Analytique des Probabilit√©s vol. 7\",\n        \"A Budget of Paradoxes\",\n        \"On an experimental determination of œÄùúã\\\\pi\",\n        \"Un‚Äôapplicazione del calcolo della probabilit√†\",\n        \"Rudolf Wolf‚Äôs contribution to the Buffon needle problem (an early Monte Carlo experiment) and application of least squares\",\n        \"Geometric probability and the number œÄùúã\\\\pi\",\n        \"Puzzles and Paradoxes\",\n        \"Lazzarini‚Äôs lucky approximation of œÄùúã\\\\pi\",\n        \"Stochastic simulation in the nineteenth century\",\n        \"The probable error of a mean\",\n        \"Probable error of a correlation coefficient\",\n        \"British statistics and statisticians today\",\n        \"On Student‚Äôs 1908 Article ‚ÄúThe Probable Error of a Mean‚Äù\",\n        \"On the extreme individuals and the range of samples taken from a normal population\",\n        \"Random sampling numbers\",\n        \"Dice for statistical experiments\",\n        \"From X-rays to Quarks: Modern Physicists and Their Discoveries\",\n        \"Metropolis, Monte Carlo and the MANIAC\",\n        \"The beginning of the Monte Carlo method\",\n        \"Model world: tales from the time line‚Äîthe definition of OR and the origins of Monte Carlo simulation\",\n        \"Stan Ulam, John von Neumann, and the Monte Carlo method\",\n        \"The Monte Carlo method\",\n        \"A note on early Monte Carlo computations and scientific meetings\",\n        \"Various techniques used in connection with random digits\",\n        \"Mathematical methods in large-scale computing units\",\n        \"Equation of state calculations by fast computing machines\",\n        \"Monte Carlo sampling methods using Markov chains and their applications\",\n        \"Monte Carlo calculation of the radial distribution functions for a proton‚Äìelectron plasma\",\n        \"Optimum Monte-Carlo sampling using Markov chains\",\n        \"A history of the Metropolis‚ÄìHastings algorithm\",\n        \"Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images\",\n        \"Sampling-based approaches to calculating marginal densities\",\n        \"Iterative and non-iterative simulation algorithms\",\n        \"E.I.: Explaining the Gibbs sampler. The American Statistician 46(3)\",\n        \"E.: Understanding the Metropolis-Hastings algorithm. The American Statistician 49(4)\",\n        \": Modelling complexity: applications of Gibbs sampling in medicine\",\n        \"J.: Bayesian comparison of econometric models. Technical report\",\n        \"D.B.: Markov chain Monte Carlo methods in biostatistics. Statistical Methods in Medical Research 5(4)\",\n        \"B.: Bayesian phylogenetic inference via Markov chain Monte Carlo methods. Biometrics 55(1)\",\n        \"F.: Guest editors‚Äô introduction: The top 10 algorithms. Computing in Science & Engineering 2(1)\",\n        \"T.-H.: Blind deconvolution of discrete-valued signals. In: Proceedings of the 27th Asilomar Conference on Signals\",\n        \"T.-H.: Blind restoration of linearly degraded discrete signals by Gibbs sampling. IEEE Transactions on Signal Processing 43(10)\",\n        \"P.: Fully Bayesian analysis of hidden Markov models. In: Proceedings of the 8th European Signal Processing Conference (EUSIPCO)\",\n        \"In: Proceedings of the IEE Colloquium on Adaptive Signal Processing for Mobile Communication Systems\",\n        \"E.: Simulation-based methods for blind maximum-likelihood filter identification. Signal processing 73(1)\",\n        \"W.J.: Interpolation of missing samples for audio restoration. IEE Electronics Letters 30(8)\",\n        \"In: Proceedings of the IEEE International Conference on Acoustics\",\n        \"S.J.: Bayesian enhancement of speech and audio signals which can be modelled as ARMA processes. International Statistical Review 65(1)\",\n        \"S.S.: Tractable models and efficient algorithms for Bayesian tomography. In: Proceedings of the IEEE International Conference on Acoustics\",\n        \"K.: ML parameter estimation for Markov random fields with applications to Bayesian tomography. IEEE Transactions on Image Processing 7(7)\",\n        \"W.: Parameter estimation of time-varying autoregressive models using the Gibbs sampler. IEE Electronics Letters 31(13)\",\n        \"Image and Signal Processing 144(4)\",\n        \"W.J.: Markov chain Monte Carlo methods with applications to signal processing. Signal Processing 81(1)\",\n        \"S.J.: Guest editorial special issue on Monte Carlo methods for statistical signal processing. IEEE Transactions on Signal Processing 50(2)\",\n        \"J.S.: Monte Carlo Bayesian signal processing for wireless communications. The Journal of VLSI Signal Processing 30(1)\",\n        \"M.I.: An introduction to MCMC for machine learning. Machine learning 50(1)\",\n        \"X.: Monte Carlo methods for signal processing: a review in the statistical signal processing context. IEEE Signal Processing Magazine 22(6)\",\n        \"H.: Random sampling (Monte Carlo) techniques in neutron attenuation problems. Nucleonics 6(5)\",\n        \"A.W.: Methods of reducing sample size in Monte Carlo computations. Journal of the Operations Research Society of America 1(5)\",\n        \"W.A.: Sampling techniques for increasing the efficiency of simulations of queuing systems. PhD thesis\",\n        \": Analytical results of variance reduction characteristics of biased Monte Carlo for deep-penetration problems\",\n        \"T.E.: A Monte Carlo learning/biasing experiment with intelligent random numbers. Nuclear Science and Engineering 92(3)\",\n        \"B.R.: An improved importance sampling method for digital communication system simulations. IEEE Transactions on Communications 34(7)\",\n        \"M.C.: Developments in the theory and application of importance sampling. IEEE Transactions on Communications 35(7)\",\n        \"A.: Monte Carlo simulation of computer system availability/reliability models. In: Proceedings of the 17th Symposium on Fault-Tolerant Computing\",\n        \"T.C.: Importance sampling in multivariate problems. In: Proceedings of the Statistical Computing Section\",\n        \": Bayesian estimates of equation system parameters: an application of integration by Monte Carlo\",\n        \"L.: Multiparameter univariate Bayesian analysis. Journal of the American Statistical Association 74(367)\",\n        \"T.C.: Importance sampling for Bayesian estimation. In: Buja\",\n        \": Weighted Average Importance Sampling and Defensive Mixture Distributions\",\n        \"T.C.: Weighted average importance sampling and defensive mixture distributions. Technometrics 37(2)\",\n        \"In: Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques\",\n        \"Y.: Safe and effective importance sampling. Journal of the American Statistical Association 95(449)\",\n        \": Novel approach to nonlinear/non-Gaussian Bayesian state estimation\",\n        \"A.M.: A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering 12(656-704)\",\n        \"Y.: Population Monte Carlo algorithms. Transactions of the Japanese Society for Artificial Intelligence 16(2)\",\n        \"C.P.: Population Monte Carlo. Journal of Computational and Graphical Statistics 13(4)\",\n        \"C.P.: Adaptive importance sampling in general mixture classes. Statistics and Computing 18\",\n        \"M.F.: Improving population Monte Carlo: Alternative weighting and resampling schemes. Signal Processing 131\",\n        \"C.P.: Adaptive multiple importance sampling. Scandinavian Journal of Statistics 39(4)\",\n        \"J.: An adaptive population importance sampler: Learning from the uncertanity. IEEE Transactions on Signal Processing 63(16)\",\n        \"et al.: The pseudo-marginal approach for efficient monte carlo computations. The Annals of Statistics 37(2)\",\n        \"R.J.: Approximate Bayesian computational methods. Statistics and Computing\",\n        \"C.: Approximate Bayesian computation. PLoS Computational Biology 9(1)\",\n        \"W.: Big learning with Bayesian methods. arXiv:1411.6370 (2014)\",\n        \"G.: Monte Carlo Statistical Methods. Springer\",\n        \"H.F.: Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. Chapman and Hall/CRC\",\n        \"J.S.: Monte Carlo Strategies in Scientific Computing. Springer\",\n        \"R.: Advanced Markov Chain Monte Carlo Methods: Learning from Past Samples vol. 714. John Wiley & Sons\",\n        \"X.-L.: Handbook of Markov Chain Monte Carlo. CRC press\",\n        \": Handbook of Monte Carlo Methods vol\",\n        \"N.: Beyond the Kalman Filter: Particle Filters for Tracking Applications. Artech House\",\n        \"B.: Particle Filters for Random Set Models vol. 798. Springer\",\n        \"J.V.: Bayesian Signal Processing: Classical\",\n        \"N.: Sequential Monte Carlo Methods in Practice\",\n        \"S.: Bayesian Filtering and Smoothing vol. 3. Cambridge University Press\",\n        \"L.: Non-uniform random variate generation. Springer\",\n        \"J.: Principles of Random Variate Generation. Clarendon Press (Oxford and New York)\",\n        \"G.: Automatic Nonuniform Random Variate Generation. Springer\",\n        \"J.E.: Random Number Generation and Monte Carlo Methods. Springer\",\n        \"J.: Independent Random Sampling Methods. Springer\",\n        \"P.M.: Monte Carlo methods for signal processing: recent advances. In: 12th European Signal Processing Conference (EUSIPCO)\",\n        \"C.P.: Bayesian computation: a summary of the current state\",\n        \"L.: A review of Multiple Try MCMC algorithms for signal processing. Digital Signal Processing (2018)\",\n        \"C.: Accelerating mcmc algorithms. arXiv:1804.02719 (2018)\",\n        \"J.: Adaptive importance sampling in signal processing. Digital Signal Processing 47\",\n        \"F.: Adaptive population importance samplers: A general perspective. IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM)\",\n        \"P.: Adaptive importance sampling. The past\",\n        \"D.: Winbugs-a bayesian modelling framework: concepts\",\n        \"N.: The bugs project: evolution\",\n        \"D.: The BUGS Book: A Practical Introduction to Bayesian Analysis. CRC press\",\n        \"K.: Jags: A program for analysis of bayesian graphical models using gibbs sampling. In: Proceedings of DSC\",\n        \"J.: Stan: A probabilistic programming language for bayesian inference and optimization. Journal of Educational and Behavioral Statistics 40(5)\",\n        \"A.: Stan: A probabilistic programming language. Journal of statistical software 76(1) (2017)\",\n        \"U.: Computer methods for sampling from gamma\",\n        \"S.G.: Sampling truncated normal\",\n        \"Series C (Applied Statistics) 26\",\n        \"C.: Efficient Nakagami-m fading channel simulation. IEEE Transaction on Vehicular Technology 54(2)\",\n        \"L.: Almost rejectionless sampling from Nakagami-mùëöm distributions (m‚â•1ùëö1m\\\\geq 1). IET Electronics Letters 48(24)\",\n        \"X.M.: Highly efficient rejection method for generating Nakagami-m sequences. IET Electronics Letters 47(19)\",\n        \": Extremely efficient acceptance-rejection method for simulating uncorrelated Nakagami fading channels\",\n        \"P.: Adaptive Rejection Sampling for Gibbs Sampling. Applied Statistics 41(2)\",\n        \"J.: Transformed density rejection with inflection points. Statistics and computing 23\",\n        \"J.: A novel rejection sampling scheme for posterior probability distributions. Proc. of the 34th IEEE ICASSP (2009)\",\n        \"W.: A rejection technique for sampling from T-concave distributions. ACM Transactions on Mathematical Software 21(2)\",\n        \"J.: Generalized rejection sampling schemes and applications in signal processing. Signal Processing 90(11)\",\n        \"Y.W.: Concave convex adaptive rejection sampling. Journal of Computational and Graphical Statistics 20(3)\",\n        \"J.: A generalization of the adaptive rejection sampling algorithm. Statistics and Computing 21(4)\",\n        \"T.: Random variate generation using concavity properties of transformed densities. Journal of Computational and Graphical Statistics 7(4)\",\n        \"F.: Adaptive rejection sampling with fixed number of nodes. (To appear) Communications in Statistics - Simulation and Computation (2017)\",\n        \"L.: Parsimonious Adaptive Rejection Sampling. IET Electronics Letters 53(16)\",\n        \"W.: A universal generator for bivariate log-concave distributions. Computing 52\",\n        \"V.: Metropolis Sampling. Wiley StatsRef: Statistics Reference Online (2017)\",\n        \"B.P.: Markov chain Monte Carlo convergence diagnostics: a comparative review. Journal of the American Statistical Association 91(434)\",\n        \"et al.: Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability 7(1)\",\n        \"et al.: Efficient metropolis jumping rules. Bayesian statistics 5(599-608)\",\n        \"J.: Optimal scaling of metropolis algorithms: is 0.234 as robust as is believed? Technical report (2007)\",\n        \"M.: Optimal acceptance rates for metropolis algorithms: Moving beyond 0.234. Stochastic Processes and their Applications 118(12)\",\n        \"M.: Efficient sampling using metropolis algorithms: Applications of optimal scaling results. Journal of Computational and Graphical Statistics 17(2)\",\n        \"et al.: Optimal scalings for local metropolis‚Äìhastings chains on nonproduct targets in high dimensions. The Annals of Applied Probability 19(3)\",\n        \"et al.: Optimal scaling and diffusion limits for the langevin algorithm in high dimensions. The Annals of Applied Probability 22(6)\",\n        \"S.M.: Markov chain Monte Carlo in small worlds. Statistics and Computing 16(2)\",\n        \"S.M.: Small-world MCMC and convergence to multi-modal distributions: From slow mixing to fast mixing. The Annals of Applied Probability 17(1)\",\n        \"R.L.: Rates of convergence of the Hastings and Metropolis algorithms. The Annals of Statistics 24(1)\",\n        \"R.L.: Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms. Biometrika 83(1)\",\n        \"M.P.: Optimization by simulated annealing. Science 220(4598)\",\n        \"J.S.: Surprising convergence properties of some simple gibbs samplers under various scans. International Journal of Statistics and Probability 5(1)\",\n        \"J.S.: Metropolized Gibbs sampler: an improvement. Technical report\",\n        \"G.: The Recycling Gibbs Sampler for efficient learning. Digital Signal Processing 74\",\n        \"B.P.: On the convergence of successive substitution sampling. Journal of Computational and Graphical statistics 1(2)\",\n        \": The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem\",\n        \": Polya urn latent Dirichlet allocation: a doubly sparse massively parallel sampler\",\n        \"M.R.: Generalized linear models with random effects; a Gibbs sampling approach. Journal of the American statistical association 86(413)\",\n        \"M.A.: The griddy Gibbs sampler. Technical report\",\n        \"M.A.: Facilitating the Gibbs sampler: the Gibbs stopper and the griddy-Gibbs sampler. Journal of the American Statistical Association 87(419)\",\n        \": On Markov chain Monte Carlo methods for nonlinear and non-Gaussian state-space models\",\n        \": Bayesian estimation of state-space models using the Metropolis-Hastings algorithm within Gibbs sampling\",\n        \"D.: Independent doubly adaptive rejection Metropolis sampling within Gibbs sampling. IEEE Transactions on Signal Processing 63(12)\",\n        \"et al.: Optimal scaling for partially updating mcmc algorithms. The Annals of Applied Probability 16(2)\",\n        \"J.: A fast universal self-tuned sampler within Gibbs sampling. Digital Signal Processing 47\",\n        \"W.H.: The calculation of posterior distributions by data augmentation. Journal of the American statistical Association 82(398)\",\n        \": Maximum likelihood from incomplete data via the EM algorithm\",\n        \"R.M.: Slice sampling. The Annals of Statistics 31(3)\",\n        \"Amsterdam (Netherland) (1979)\",\n        \"R.L.: Monte Carlo techniques for generating random feasible solutions to mathematical programs. In: Proceedings of the ORSA/TIMS Conference\",\n        \"R.L.: Efficient Monte Carlo procedures for generating points uniformly distributed over bounded regions. Operations Research 32(6)\",\n        \"J.: Hit-and-run algorithms for the identification of nonredundant linear inequalities. Mathematical Programming 37(2)\",\n        \"R.L.: Direction choice for accelerated convergence in hit-and-run sampling. Operations Research 46(1)\",\n        \"B.: Performance of the Gibbs\",\n        \"E.I.: Adaptive direction sampling. The Statistician\",\n        \"W.R.: Convergence of adaptive direction sampling. Journal of Multivariate Analysis 49(2)\",\n        \"C.J.: Practical Markov chain Monte Carlo. Statistical science\",\n        \"L.M.: Subsampling the Gibbs sampler. The American Statistician 48(3)\",\n        \"M.J.: On thinning of chains in MCMC. Methods in Ecology and Evolution 3(1)\",\n        \": Statistically efficient thinning of a Markov chain sampler\",\n        \"J.: A tutorial on adaptive MCMC. Statistics and Computing 18(4)\",\n        \"J.: An adaptive Metropolis algorithm. Bernoulli 7(2)\",\n        \"E.: DRAM: efficient adaptive MCMC. Statistics and Computing 16(4)\",\n        \"R.: Adaptive independent Metropolis‚ÄìHastings by fast estimation of mixtures of normals. Journal of Computational and Graphical Statistics 19(2)\",\n        \"L.: Fully adaptive Gaussian mixture Metropolis-Hastings algorithm. In: Proceedings of the IEEE International Conference on Acoustics\",\n        \"H.: Adaptive Metropolis algorithm using variational Bayesian adaptive Kalman filter. Computational Statistics and Data Analysis 83\",\n        \"G.: Limit theorems for some adaptive MCMC algorithms with subgeometric kernels. Bernoulli 16(1)\",\n        \"M.: On the stability and ergodicity of adaptive scaling Metropolis algorithms. Stochastic Processes and their Applications 121(12)\",\n        \"M.: Robust adaptive Metropolis algorithm with coerced acceptance rate. Statistics and Computing 22(5)\",\n        \": Adaptive Rejection Metropolis Sampling within Gibbs Sampling\",\n        \"F.: Adaptive rejection Metropolis sampling using Lagrange interpolation polynomials of degree 2. Computational Statistics and Data Analysis 52(7)\",\n        \"D.: Independent doubly adaptive rejection Metropolis sampling. In: Proceedings of the IEEE International Conference on Acoustics\",\n        \"D.: Adaptive Independent Sticky MCMC algorithms. (to appear) EURASIP Journal on Advances in Signal Processing (2017)\",\n        \"J.S.: Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms. Journal of Applied Probability 44(2)\",\n        \"G.: Air Markov chain Monte Carlo. Technical report (2018)\",\n        \"O.: Langevin diffusions and Metropolis-Hastings algorithms. Methodology and Computing in Applied Probability 4\",\n        \"B.: Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73(2)\",\n        \"E.: Numerical Solution of Stochastic Differential Equations. Springer\",\n        \"Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60(1)\",\n        \"D.: Hybrid Monte Carlo. Physics Letters B 195(2)\",\n        \"In: Handbook of Markov Chain Monte Carlo\",\n        \"et al.: Optimal tuning of the hybrid monte carlo algorithm. Bernoulli 19(5A)\",\n        \"Y.: Manifold stochastic dynamics for Bayesian learning. Neural computation 13(11)\",\n        \"A.: The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research 15(1)\",\n        \"J.S.: Exploring hybrid Monte Carlo in Bayesian computation. In: Proceedings of the International Symposium on Bayesian Analysis (ISBA) (2000)\",\n        \"S.: Explicit variable step-size and time-reversible integration. Applied Numerical Mathematics 39(3)\",\n        \"A.: Optimal tuning of the hybrid Monte Carlo algorithm. Bernoulli 19(5A)\",\n        \"M.: Optimizing the integrator step size for Hamiltonian Monte Carlo. arXiv:1411.6669 (2014)\",\n        \"M.: The geometric foundations of Hamiltonian Monte Carlo. arXiv:1410.5110 (2014)\",\n        \"M.: A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434 (2017)\",\n        \"D.: Geometrically Tempered Hamiltonian Monte Carlo. arXiv:1604.00872\",\n        \"B.: Wormhole Hamiltonian Monte Carlo. in Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence\",\n        \"H.: Random Number Generation and Quasi-Monte Carlo Methods. Society for Industrial Mathematics\",\n        \"C.: Learn from thy neighbor: Parallel-chains and regional adaptive MCMC. Journal of the American Statistical Association 104(448)\",\n        \"F.: Interacting multiple try algorithms with different proposal distributions. Statistics and Computing 23\",\n        \"F.: Orthogonal parallel MCMC methods for sampling and optimization. Digital Signal Processing 58\",\n        \"F.: Parallel metropolis coupled Markov Chain Monte Carlo for bayesian phylogenetic inference. Bioinformatics 20(3)\",\n        \"C.J.: Markov Chain Monte Carlo maximum likelihood. Computing Science and Statistics: Proceedings of the 23rd Symposium on the Interface\",\n        \"J.: Smelly parallel MCMC chains. IEEE International Conference on Acoustics\",\n        \"F.: Parallel Metropolis chains with cooperative adaptation. IEEE International Conference on Acoustics\",\n        \"E.A.: Annealing Markov Chain Monte Carlo with applications to ancestral inference. Journal of the American Statistical Association 90\",\n        \"C.C.: On population-based simulation for static inference. Statistics and Computing 17(3)\",\n        \"T.: Bayesian model learning based on a parallel MCMC strategy. Statistics Computing 16\",\n        \"T.: Parallel interacting MCMC for learning of topologies of graphical models. Data Mining and Knowledge Discovery 17(3)\",\n        \": A general construction for parallelizing Metropolis-Hastings algorithms\",\n        \"C.: On Markov chain Monte Carlo methods for tall data. arXiv:1505.02827 (2015)\",\n        \"E.: Asymptotically exact\",\n        \": Parallelizing MCMC via Weierstrass sampler\",\n        \": Parallelizing MCMC with random partition trees\",\n        \"A.E.: Parallel Markov chain Monte Carlo simulation by pre-fetching. Journal of Computational and Graphical Statistics 15(1)\",\n        \"et al.: Mrbayes: Bayesian inference of phylogenetic trees. Bioinformatics 17(8)\",\n        \"J.S.: Parallel computing and Monte Carlo algorithms. Far east journal of theoretical statistics 4(2)\",\n        \"I.: Efficient parallelisation of Metropolis-Hastings algorithms using a prefetching approach. Computational Statistics & Data Analysis 54(11)\",\n        \"D.: GPU-accelerated Gibbs sampling: a case study of the Horseshoe Probit model. Statistics and Computing 29(2)\",\n        \"D.: Asynchronous Gibbs sampling. arXiv:1509.08999v5 (2018)\",\n        \"A.: Some adaptive Monte Carlo methods for Bayesian inference. Statistics in Medicine 18\",\n        \"J.: Non-reversible Metropolis-Hastings. Statistics and Computing 26(6)\",\n        \"R.M.: Improving asymptotic variance of MCMC estimators: Non-reversible chains are better. Technical report\",\n        \"M.: Non-reversible Monte Carlo simulations of spin models. Computer Physics Communications 182(9)\",\n        \"M.: Irreversible Monte Carlo algorithms for efficient sampling. Physica D: Nonlinear Phenomena 240(4)\",\n        \"G.T.: Monte carlo methods beyond detailed balance. Physica A: Statistical Mechanics and its Applications 418\",\n        \"M.: Lifting ‚Äî a non-reversible Markov Chain Carlo algorithm. arXiv:1412.8762\",\n        \"M.: Irreversible Monte Carlo algorithms for efficient sampling. Physica D: Nonlinear Phenomena 240(4-5)\",\n        \"D.B.: Bayesian Data Analysis\",\n        \"A.: General methods for monitoring convergence of iterative simulations. Journal of Computational and Graphical Statistics 7(4)\",\n        \"A.: Nonparametric convergence assessment for MCMC model selection. Journal of Computational and Graphical Statistics 12(1)\",\n        \"D.B.: Inference from iterative simulation using multiple sequences. Statistical Science 7(4)\",\n        \"L.: Measuring sample quality with Stein‚Äôs method. In: Advances in Neural Information Processing Systems (NIPS)\",\n        \"L.: Measuring sample quality with diffusions. arXiv:1611.06972v6 (2018)\",\n        \"A.: A kernel test of goodness of fit. In: International Conference on Machine Learning (ICML) (2016). JMLR: Workshop and Conference Proceedings\",\n        \"M.: A kernelized Stein discrepancy for goodness-of-fit tests. In: International Conference on Machine Learning (ICML)\",\n        \"N.: Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 79(3)\",\n        \"L.: Measuring sample quality with kernels. In: Proceedings of the 34th International Conference on Machine Learning (ICML)\",\n        \"D.B.: MCMC for imbalanced categorical data. Journal of the American Statistical Association\",\n        \"L.: Optimally combining sampling techniques for Monte Carlo rendering. In SIGGRAPH 1995 Proceedings\",\n        \"J.: Layered Adaptive Importance Sampling. Statistics and Computing 27(3)\",\n        \"M.F.: Improving Population Monte Carlo: Alternative weighting and resampling schemes. Signal Processing 131(12)\",\n        \": Generalized multiple importance sampling\",\n        \"M.F.: Efficient multiple importance sampling estimators. IEEE Signal Processing Letters 22(10)\",\n        \"M.F.: Heretical multiple importance sampling. IEEE Signal Processing Letters 23(10)\",\n        \"J.: A gradient adaptive population importance sampler. In: Proceedings of the IEEE International Conference on Acoustics\",\n        \"J.: A population Monte Carlo scheme with transformed weights and its application to stochastic kinetic models. Statistics and Computing (2014)\",\n        \"J.: An adaptive population importance sampler. IEEE International Conference on Acoustics\",\n        \"I.: Gradient importance sampling. arXiv:1507.05781\",\n        \"J.: MCMC-driven adaptive multiple importance sampling. In: Polpo\",\n        \"A.L.: Regenerative Markov chain importance sampling. Communications in Statistics - Simulation and Computation (2015)\",\n        \"B.: Markov chain importance sampling with applications to rare event probability estimation. Statistics and Computing 23\",\n        \"C.Z.: A novel adaptive importance sampling algorithm based on Markov chain and low-discrepancy sequence. Aerospace Science and Technology 29\",\n        \"A.: Sequential Monte Carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68(3)\",\n        \"J.O.: Adaptive importance sampling in Monte Carlo integration. Journal of Statistical Computation and Simulation 41(3-4)\",\n        \"J.: Bayesian inference in econometric models using Monte Carlo integration. Econometrica: Journal of the Econometric Society\",\n        \"R.: Another look at rejection sampling through importance sampling. Statistics & Probability Letters 72\",\n        \"A.: A note on importance sampling using standardized weights. University of Chicago\",\n        \"W.H.: Sequential imputations and Bayesian missing data problems. Journal of the American Statistical Association 89(425)\",\n        \"N. (eds.): Sequential Monte Carlo Methods in Practice. Springer\",\n        \"G.: Introducing Monte Carlo Methods With R. Springer\",\n        \"com/2010/09/24/effective-sample-size/ (2010)\",\n        \"F.: Effective Sample Size for importance sampling based on discrepancy measures. Signal Processing 131\",\n        \": Importance sampling: Computational complexity and intrinsic dimension\",\n        \"K.: On the role of interaction in sequential Monte Carlo algorithms. Bernoulli 22(1)\",\n        \": Convergence of sequential Monte Carlo based sampling methods\",\n        \"F.: An improved sir-based sequential monte carlo algorithm. In: IEEE Statistical Signal Processing Workshop (SSP)\",\n        \"F.: Independent resampling sequential monte carlo algorithms. IEEE Transactions on Signal Processing 65(20)\",\n        \"F.: Weighting a resampled particle in Sequential Monte Carlo. IEEE Statistical Signal Processing Workshop\",\n        \"T.B.: Nested Sequential Monte Carlo methods. Proceedings of theInternational Conference on Machine Learning 37\",\n        \"G.: Group Importance Sampling for particle filtering and MCMC. Digital Signal Processing 82\",\n        \"F.: Cooperative parallel particle filters for on-line model selection and applications to urban mobility. Digital Signal Processing 60\",\n        \"S.: Resampling algorithms and architectures for distributed particle filters. IEEE Transactions Signal Processing 53(7)\",\n        \"M.A.: A proof of uniform convergence over time for a distributed particle filter. Signal Processing 122\",\n        \"J.: A distributed particle filter for nonlinear tracking in wireless sensor networks. Signal Processing 98\",\n        \"E.: On parallel implementation of sequential Monte Carlo methods: the island particle model. Statistics and Computing 25(2)\",\n        \"J.: Convergence properties of weighted particle islands with application to the double bootstrap algorithm. arXiv:1410.4231\",\n        \": A sequential Monte Carlo algorithm to incorporate model uncertainty in Bayesian sequential design\",\n        \"P.M.: Sequential Monte Carlo methods under model uncertainty. In: 2016 IEEE Statistical Signal Processing Workshop (SSP)\",\n        \": High-dimensional filtering using nested sequential Monte Carlo\",\n        \"R.B.: A statistical contribution to historical linguistics. Phd Thesis (2015)\",\n        \"J.: Particle filtering. IEEE Signal Processing Magazine 20(5)\",\n        \"A.M.: A tutorial on particle filtering and smoothing: fifteen years later. technical report (2008)\",\n        \": A multi-model particle filtering algorithm for indoor tracking of mobile terminals using rss data\",\n        \"R.: Particle Markov chain Monte Carlo methods. Journal of the Royal Statistical Society B 72(3)\",\n        \"T.B.: Particle Gibbs with ancestor sampling. Journal of Machine Learning Research 15(1)\",\n        \"J.: Particle and sigma-point methods for state and parameter estimation in nonlinear dynamic systems. Phd Thesis\",\n        \"W.H.: The multiple-try method and local optimization in Metropolis sampling. Journal of the American Statistical Association 95(449)\",\n        \"J.: On the flexibility of the design of multiple try Metropolis schemes. Computational Statistics 28(6)\",\n        \"J.: A multi-point Metropolis scheme with generic weight functions. Statistics & Probability Letters 82(7)\",\n        \"E.: Scaling analysis of multiple-try MCMC methods. Stochastic Processes and their Applications 122\",\n        \"C.: Acceleration of the Multiple Try Metropolis algorithm using antithetic and stratified sampling. Statistics and Computing 17(2)\",\n        \"F.: Issues in the Multiple Try Metropolis mixing. Computational Statistics 32(1)\",\n        \"J.: On multiple try schemes and the Particle Metropolis-Hastings algorithm. viXra:1409.0051 (2014)\",\n        \"C.P.: Rao-Blackwellisation of sampling schemes. Biometrika 83(1)\",\n        \"R.: MCMC using ensembles of states for problems with fast and slow variables such as Gaussian process regression. arXiv:1101.0387 (2011)\",\n        \": Combining particle MCMC with Rao-Blackwellized Monte Carlo data association for parameter estimation in multiple target tracking\",\n        \"G.O.: The pseudo-marginal approach for efficient monte carlo computations. The Annals of Statistics 37(2)\",\n        \"D.: MCMC for doubly-intractable distributions. Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)\",\n        \"K.: An efficient Markov Chain Monte Carlo method for distributions with intractable normalising constants. Biometrika 93\",\n        \": Markov Chain Monte Carlo without likelihoods\",\n        \"M.A.: Estimation of population growth or decline in genetically monitored populations. Genetics 164\",\n        \"R.J.: Approximate Bayesian computational methods. Statistics and Computing 22(6)\",\n        \": Towards scaling up Markov Chain Monte Carlo: an adaptive subsampling approach\",\n        \"M.: Austerity in MCMC land: Cutting the Metropolis-Hastings budget. Proceedings of the 31st International Conference on Machine Learning\",\n        \"C.: Markov Chain Monte Carlo using an approximation. Journal of Computational and Graphical Statistics 14\",\n        \"delayed-acceptance MCMC for targets with expensive likelihoods. Journal of Computational and Graphical Statistics 26(2)\",\n        \"H.: Efficient MCMC for climate model parameter estimation: Parallel adaptive chains and Early Rejection. Bayesian Analysis 7(3)\",\n        \"In: IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)\",\n        \": Firefly Monte Carlo: Exact MCMC with subsets of data\",\n        \"M.F.: Efficient linear fusion of partial estimators. Digital Signal Processing 78\",\n        \"R.E.: Bayes and big data: The consensus Monte Carlo algorithm. In: EFaBBayes 250th Conference\",\n        \": Coupled MCMC with a randomized acceptance probability\",\n        \"A.: Noisy Monte Carlo: Convergence of Markov chains with approximate transition kernels. Statistics and Computing 26(2)\",\n        \"G.O.: Stability of noisy Metropolis-Hastings. Statistics and Computing 26(6)\",\n        \"S.: Bayesian Filtering and Smoothing. Cambridge University Press\",\n        \": Parameter estimation in stochastic differential equations with markov chain monte carlo and non-linear kalman filtering\",\n        \": Posterior inference on parameters of stochastic differential equations via non-linear Gaussian filtering and adaptive MCMC\",\n        \"D.: Optimal approximating Markov chains for Bayesian inference. arXiv:1508.03387 (2015)\",\n        \": Error bounds for approximations of geometrically ergodic Markov chains\",\n        \"M.: Adiabatic Monte Carlo. arXiv:1405.3489 (2014)\",\n        \": Model-free forecasting outperforms the correct mechanistic model for simulated and experimental data\",\n        \"G.: Reply to Hartig and Dormann: The true model myth. Proceedings of the National Academy of Sciences (PNAS) 110(42)\",\n        \"P.: Law of Chaos. Birkh√∂user\",\n        \"A.S.: Nonparametric belief propagation for self-localization of sensor networks. IEEE Journal on Selected Areas in Communications 23(4)\",\n        \"D.T.: An empirical study of collaborative acoustic source localization. Journal of Signal Processing Systems 57(3)\",\n        \"P.: List of references on spectral line analysis. Signal Processing 31(3)\",\n        \"K.C.: Linear prediction approach for efficient frequency estimation of multiple real sinusoids: algorithms and analyses. Signal Processing\",\n        \"S.C.: Adaptive algorithms for the rejection of sinusoidal disturbances with unknown frequency. Automatica 33(12)\",\n        \": Cancelation of unknown multiharmonic disturbance for nonlinear plant with input delay\",\n        \"A.: Suppression of multiple narrowband interferers in a spread-spectrum communication system. IEEE Journal on Selected Areas in Communications 18(8)\",\n        \"E.: A nonlinear adaptive filter for narrowband interference mitigation in spread spectrum systems. Signal processing 85(3)\",\n        \"J.G.: Digital Communications. McGraw-Hill\",\n        \"Series B (Methodological)\"\n    ],\n    \"reasoning\": \"Selected these 362 references because they represent foundational work and recent advances in A Survey of Monte Carlo Methods for Parameter Estimation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}