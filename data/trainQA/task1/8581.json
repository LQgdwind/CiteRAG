{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 69 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: SCV-GNN: Sparse Compressed Vector-based Graph Neural Network Aggregation\n\nPaper Abstract: Abstract Graph neural networks (GNNs) have emerged as a powerful tool to process graph-based data in fields like communication networks, molecular interactions, chemistry, social networks, and neuroscience. GNNs are characterized by the ultra-sparse nature of their adjacency matrix that necessitates the development of dedicated hardware beyond general-purpose sparse matrix multipliers. While there has been extensive research on designing dedicated hardware accelerators for GNNs, few have extensively explored the impact of the sparse storage format on the efficiency of the GNN accelerators. This paper proposes SCV-GNN with the novel sparse compressed vectors (SCV) format optimized for the aggregation operation. We use Z-Morton ordering to derive a data-locality-based computation ordering and partitioning scheme. The paper also presents how the proposed SCV-GNN is scalable on a vector processing system. Experimental results over various datasets show that the proposed method achieves a geometric mean speedup of 7.96×7.96\\times and 7.04×7.04\\times over CSC and CSR aggregation operations, respectively. The proposed method also reduces the memory traffic by a factor of 3.29×3.29\\times and 4.37×4.37\\times over compressed sparse column (CSC) and compressed sparse row (CSR), respectively. Thus, the proposed novel aggregation format reduces the latency and memory access for GNN inference.\n\n{retrieved_text}\n\nGenerate exactly 69 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 69\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Brain-inspired computing: Models and architectures\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Deep residual learning for image recognition\",\n        \"Going deeper with convolutions\",\n        \"Audio-visual speech recognition using deep learning\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"RoBERTa: A robustly optimized BERT pretraining approach\",\n        \"Deep learning for image-based cancer detection and diagnosis - A survey\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Graph attention networks\",\n        \"How powerful are graph neural networks?\",\n        \"Inductive representation learning on large graphs\",\n        \"Computing graph neural networks: A survey from algorithms to accelerators\",\n        \"RouteNet: Leveraging graph neural networks for network modeling and optimization in sdn\",\n        \"Neural message passing for quantum chemistry\",\n        \"Fake news detection on social media using geometric deep learning\",\n        \"Pooling regularized graph neural network for FMRI biomarker analysis\",\n        \"NVIDIA A100 Tensor Core GPU Architecture\",\n        \"In-datacenter performance analysis of a tensor processing unit\",\n        \"Ten lessons from three generations shaped Google’s TPUv4i : Industrial product\",\n        \"PipeDream: Generalized pipeline parallelism for DNN training\",\n        \"GPipe: Efficient training of giant neural networks using pipeline parallelism\",\n        \"LayerPipe: Accelerating deep neural network training by intra-layer and inter-layer gradient pipelining and multiprocessor scheduling\",\n        \"PermDNN: Efficient compressed DNN architecture with permuted diagonal matrices\",\n        \"Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices\",\n        \"EIE: Efficient inference engine on compressed deep neural network\",\n        \"A gradient-interleaved scheduler for energy-efficient backpropagation for training neural networks\",\n        \"SIGMA: A sparse and irregular GEMM accelerator with flexible interconnects for DNN training\",\n        \"FPGA-based training accelerator utilizing sparseness of convolutional neural network\",\n        \"SPRING: A sparsity-aware reduced-precision monolithic 3D CNN accelerator architecture for training and inference\",\n        \"ExTensor: An accelerator for sparse tensor algebra\",\n        \"Deep graph library: A graph-centric, highly-performant package for graph neural networks\",\n        \"Fast graph representation learning with PyTorch Geometric\",\n        \"GenGNN: A generic FPGA framework for graph neural network acceleration\",\n        \"GNNerator: A hardware/software framework for accelerating graph neural networks\",\n        \"Hardware acceleration of graph neural networks\",\n        \"VersaGNN: a versatile accelerator for graph neural networks\",\n        \"Rubik: A hierarchical architecture for efficient graph neural network training\",\n        \"Accelerating graph connected component computation with emerging processing-in-memory architecture\",\n        \"GCoD: Graph convolutional network acceleration via dedicated algorithm and accelerator co-design\",\n        \"GraphLily: Accelerating graph linear algebra on HBM-equipped FPGAs\",\n        \"ZIPPER: Exploiting tile- and operator-level parallelism for general and scalable graph neural network acceleration\",\n        \"HyGCN: A GCN accelerator with hybrid architecture\",\n        \"Cambricon-G: A polyvalent energy-efficient accelerator for dynamic graph neural networks\",\n        \"GNNIE: GNN inference engine with load-balancing and graph-specific caching\",\n        \"AWB-GCN: A graph convolutional network accelerator with runtime workload rebalancing\",\n        \"Efficient neighbor-sampling-based GNN training on CPU-FPGA heterogeneous platform\",\n        \"A computer oriented geodetic data base; and a new technique in file sequencing\",\n        \"Efficient processing of deep neural networks: A tutorial and survey\",\n        \"EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks\",\n        \"Efficient gather and scatter operations on graphics processors\",\n        \"Revisiting multi-pass scatter and gather on GPUs\",\n        \"Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks\",\n        \"Designware multi-port memory\",\n        \"Introduction to idt’s FourPort sram\",\n        \"Composing multi-ported memories on fpgas\",\n        \"Ramulator: A fast and extensible DRAM simulator\",\n        \"Systolic arrays (for VLSI)\",\n        \"Accelerating matrix multiplication with block sparse format and nvidia tensor cores\",\n        \"GCNAX: A flexible and energy-efficient accelerator for graph convolutional neural networks\",\n        \"A survey of accelerator architectures for deep neural networks\",\n        \"A template-based design methodology for graph-parallel hardware accelerators\",\n        \"I-GCN: A graph convolutional network accelerator with runtime locality enhancement through islandization\",\n        \"Rabbit order: Just-in-time parallel reordering for fast graph analysis\",\n        \"BoostGCN: A framework for optimizing gcn inference on fpga\",\n        \"GROW: A row-stationary sparse-dense GEMM accelerator for memory-efficient graph convolutional neural networks\",\n        \"Adaptive sparse tiling for sparse matrix multiplication\",\n        \"Understanding the design-space of sparse/dense multiphase GNN dataflows on spatial accelerators\"\n    ],\n    \"reasoning\": \"Selected these 69 references because they represent foundational work and recent advances in SCV-GNN: Sparse Compressed Vector-based Graph Neural Network Aggregation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}