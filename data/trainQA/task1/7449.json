{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient Zero-shot Visual Search via Target and Context-aware Transformer\n\nPaper Abstract: Abstract Visual search is a ubiquitous challenge in natural vision, including daily tasks such as finding a friend in a crowd or searching for a car in a parking lot. Human rely heavily on relevant target features to perform goal-directed visual search. Meanwhile, context is of critical importance for locating a target object in complex scenes as it helps narrow down the search area and makes the search process more efficient. However, few works have combined both target and context information in visual search computational models. Here we propose a zero-shot deep learning architecture, TCT (Target and Context-aware Transformer), that modulates self attention in the Vision Transformer with target and contextual relevant information to enable human-like zero-shot visual search performance. Target modulation is computed as patch-wise local relevance between the target and search images, whereas contextual modulation is applied in a global fashion. We conduct visual search experiments on TCT and other competitive visual search models on three natural scene datasets with varying levels of difficulty. TCT demonstrates human-like performance in terms of search efficiency and beats the SOTA models in challenging visual search tasks. Importantly, TCT generalizes well across datasets with novel objects without retraining or fine-tuning. Furthermore, we also introduce a new dataset to benchmark models for invariant visual search under incongruent contexts. TCT manages to search flexibly via target and context modulation, even under incongruent contexts.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep-bcn: Deep networks meet biased competition to create a brain-inspired model of attention control\",\n        \"Recognition in terra incognita\",\n        \"Understanding robustness of transformers for image classification\",\n        \"A source for feature-based attention in the prefrontal cortex\",\n        \"The neural basis of visual attention\",\n        \"Anchoring visual search in scenes: Assessing the role of anchor objects on eye movements during visual search\",\n        \"When pigs fly: Contextual reasoning in synthetic and natural scenes\",\n        \"Saliency, attention, and visual search: An information theoretic approach\",\n        \"Serial, covert shifts of attention during visual search are reflected by the frontal eye fields and correlated with population oscillations\",\n        \"Coco-search18 fixation dataset for predicting goal-directed attention control\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Neural mechanisms of selective visual attention\",\n        \"An empirical study of context in object detection\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Humans, but not deep neural networks, often miss giant targets in scenes\",\n        \"Graph-based visual saliency\",\n        \"Mask r-cnn\",\n        \"Image signature: Highlighting sparse salient regions\",\n        \"Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks\",\n        \"A model of saliency-based visual attention for rapid scene analysis\",\n        \"Panoptic feature pyramid networks\",\n        \"Panoptic segmentation\",\n        \"Inhibition of return\",\n        \"Shifts in selective visual attention: towards the underlying neural circuitry\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Deepfix: A fully convolutional neural network for predicting human eye fixations\",\n        \"Deepgaze iii: Modeling free-viewing human scanpaths with deep learning\",\n        \"Microsoft coco: Common objects in context. ineuropean conference on computer vision 2014 sep 6 (pp. 740-755).\",\n        \"Saliency detection within a deep convolutional architecture.\",\n        \"Searching for the neural mechanisms of feature-based attention in the primate brain\",\n        \"Thereâ€™s waldo! a normalization model of visual search predicts single-trial human fixations in an object search task\",\n        \"An integrative theory of prefrontal cortex function\",\n        \"Could simplified stimuli change how the brain performs visual search tasks? a deep neural network study\",\n        \"Scegram: An image database for semantic and syntactic inconsistencies in scenes\",\n        \"Eye movements in iconic visual search\",\n        \"Attention and visual search\",\n        \"Noticing familiar objects in real world scenes: the role of temporal cortical neurons in natural vision\",\n        \"Seeing what is not there: Learning context to determine where objects are missing\",\n        \"Reading scenes: How scene grammar guides attention and aids perception in real-world environments\",\n        \"Learned region sparsity and diversity also predicts visual attention\",\n        \"Five factors that guide attention in visual search\",\n        \"Detectron2\",\n        \"Predicting goal-directed human attention using inverse reinforcement learning\",\n        \"Saliency detection: A boolean map approach\",\n        \"Sun: A bayesian framework for saliency using natural statistics\",\n        \"Look twice: A generalist model predicts return fixations across tasks and species\",\n        \"Finding any waldo with zero-shot invariant and efficient visual search\",\n        \"Putting visual object recognition in context\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Efficient Zero-shot Visual Search via Target and Context-aware Transformer, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}