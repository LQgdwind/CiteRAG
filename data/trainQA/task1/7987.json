{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Word class representations spontaneously emerge in a deep neural network trained on next word prediction\n\nPaper Abstract: Abstract How do humans learn language, and can the first language be learned at all? These fundamental questions are still hotly debated. In contemporary linguistics, there are two major schools of thought that give completely opposite answers. According to Chomsky’s theory of universal grammar, language cannot be learned because children are not exposed to sufficient data in their linguistic environment. In contrast, usage-based models of language assume a profound relationship between language structure and language use. In particular, contextual mental processing and mental representations are assumed to have the cognitive capacity to capture the complexity of actual language use at all levels. The prime example is syntax, i.e., the rules by which words are assembled into larger units such as sentences. Typically, syntactic rules are expressed as sequences of word classes. However, it remains unclear whether word classes are innate, as implied by universal grammar, or whether they emerge during language acquisition, as suggested by usage-based approaches. Here, we address this issue from a machine learning and natural language processing perspective. In particular, we trained an artificial deep neural network on predicting the next word, provided sequences of consecutive words as input. Subsequently, we analyzed the emerging activation patterns in the hidden layers of the neural network. Strikingly, we find that the internal representations of nine-word input sequences cluster according to the word class of the tenth word to be predicted as output, even though the neural network did not receive any explicit information about syntactic rules or word classes during training. This surprising result suggests, that also in the human brain, abstract representational categories such as word classes may naturally emerge as a consequence of predictive coding and processing during language acquisition.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"So kommt der Mensch zur Sprache: über Spracherwerb, Sprachentstehung und Sprache & Denken, volume 16\",\n        \"The history of Herodotus, volume 1\",\n        \"Language acquisition: A linguistic introduction\",\n        \"Course in general linguistics\",\n        \"On the nature, use and acquisition of language\",\n        \"Aspects of the Theory of Syntax, volume 11\",\n        \"The growth of language: Universal grammar, experience, and principles of computation\",\n        \"Constructions: A construction grammar approach to argument structure\",\n        \"Constructions: A new theoretical approach to language\",\n        \"Constructing a language: A usage-based theory of language acquisition\",\n        \"Cognitive grammar\",\n        \"The Evolution of Grammar: Tense, Aspect, and Modality in the Languages of the World\",\n        \"Frequency and the emergence of linguistic structure\",\n        \"Usage-based theory and exemplar representations of constructions\",\n        \"Usage-based construction grammar\",\n        \"Explain me this\",\n        \"The dynamics of the linguistic system: Usage, conventionalization, and entrenchment\",\n        \"14. usage-based construction grammar\",\n        \"Predictive coding: an account of the mirror neuron system\",\n        \"Canonical microcircuits for predictive coding\",\n        \"Predictive processing: a canonical cortical computation\",\n        \"Predictive coding and stochastic resonance: Towards a unified theory of auditory (phantom) perception\",\n        \"Neural correlates of linguistic collocations during continuous speech perception\",\n        \"Neural network based successor representations to form cognitive maps of space and language\",\n        \"Neural network based formation of cognitive maps of semantic spaces and the emergence of abstract concepts\",\n        \"Navigating cognition: Spatial codes for human thinking\",\n        \"spacy-industrial-strength natural language processing in python\",\n        \"A universal part-of-speech tagset\",\n        \"Visualizing data using t-sne\",\n        \"How to use t-sne effectively\",\n        \"Exploring a world of a thousand dimensions\",\n        \"Visualizing structure and transitions in high-dimensional biological data\",\n        \"Multidimensional scaling: I. theory and method\",\n        \"Nonmetric multidimensional scaling: a numerical method\",\n        \"Multidimensional scaling, volume 11\",\n        \"Multidimensional scaling\",\n        \"Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods\",\n        \"Quantifying the separability of data classes in neural networks\",\n        \"Analysis and visualization of sleep stages based on deep neural networks\",\n        \"Analysis of structure and dynamics in three-neuron motifs\",\n        \"Recurrence resonance” in three-neuron motifs\",\n        \"Weight statistics controls dynamics in recurrent neural networks\",\n        \"Quantifying and maximizing the information flux in recurrent neural networks\",\n        \"A statistical method for analyzing and comparing spatiotemporal cortical activation patterns\",\n        \"Analysis of multichannel eeg patterns during human sleep: a novel approach\",\n        \"Microstructure of cortical activity during sleep reflects respiratory events and state of daytime vigilance\",\n        \"Classification at the accuracy limit: facing the problem of data ambiguity\",\n        \"Extracting continuous sleep depth from eeg data without machine learning\",\n        \"Keras\",\n        \"keras-rl\",\n        \"Array programming with NumPy\",\n        \"Scikit-learn: Machine learning in Python\",\n        \"Matplotlib: A 2d graphics environment\",\n        \"Exploring network structure, dynamics, and function using networkx\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Word class representations spontaneously emerge in a deep neural network trained on next word prediction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}