{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 56 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Probabilistic Constraint for Safety-Critical Reinforcement Learning\n\nPaper Abstract: Abstract In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that leads to a lower variance than SPG-REINFORCE, which is substantiated by our theoretical results. A noteworthy aspect of both SPGs is their inherent algorithm independence, rendering them versatile for application across a range of policy-based algorithms. Furthermore, we propose a Safe Primal-Dual algorithm that can leverage both SPGs to learn safe policies. It is subsequently followed by theoretical analyses that encompass the convergence of the algorithm, as well as the near-optimality and feasibility on average. In addition, we test the proposed approaches by a series of empirical experiments. These experiments aim to examine and analyze the inherent trade-offs between the optimality and safety, and serve to substantiate the efficacy of two SPGs, as well as our theoretical contributions.\n\n{retrieved_text}\n\nGenerate exactly 56 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 56\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Playing atari with deep reinforcement learning\",\n        \"Mastering the game of go without human knowledge\",\n        \"Alternative theoretical frameworks for finite horizon discrete-time stochastic optimal control\",\n        \"End-to-end training of deep visuomotor policies\",\n        \"Benchmarking deep reinforcement learning for continuous control\",\n        \"Neuro-dynamic programming\",\n        \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"Continuous control with deep reinforcement learning\",\n        \"Trust region policy optimization\",\n        \"Proximal policy optimization algorithms\",\n        \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\",\n        \"Q-learning\",\n        \"Policy gradient methods for reinforcement learning with function approximation\",\n        \"A comprehensive survey on safe reinforcement learning\",\n        \"Voltage instability: phenomena, countermeasures, and analysis methods\",\n        \"Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation\",\n        \"Reinforcement learning for mdps with constraints\",\n        \"Discounted markov decision processes with utility constraints\",\n        \"Risk-constrained reinforcement learning with percentile risk criteria\",\n        \"Risk-sensitive markov decision processes\",\n        \"Td algorithm for the variance of return and mean-variance reinforcement learning\",\n        \"Risk-sensitive reinforcement learning applied to control under constraints\",\n        \"Ai safety gridworlds\",\n        \"Simple random search provides a competitive approach to reinforcement learning\",\n        \"Constrained Markov decision processes\",\n        \"An actor-critic algorithm for constrained markov decision processes\",\n        \"An online actorâ€“critic algorithm with function approximation for constrained markov decision processes\",\n        \"Accelerated primal-dual policy optimization for safe reinforcement learning\",\n        \"Constrained policy optimization\",\n        \"Reward constrained policy optimization\",\n        \"Projection-based constrained policy optimization\",\n        \"First order constrained optimization in policy space\",\n        \"Ipo: Interior-point policy optimization under constraints\",\n        \"Penalized proximal policy optimization for safe reinforcement learning\",\n        \"Pareto optimality in multiobjective problems\",\n        \"End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks\",\n        \"Cautious adaptation for reinforcement learning in safety-critical settings\",\n        \"Formal verification of neural networks for safety-critical tasks in deep reinforcement learning\",\n        \"Percentile optimization for markov decision processes with parameter uncertainty\",\n        \"Safe policies for reinforcement learning via primal-dual methods\",\n        \"Towards safe continuing task reinforcement learning\",\n        \"Policy gradients for probabilistic constrained reinforcement learning\",\n        \"Openai gym\",\n        \"Reinforcement learning: An introduction\",\n        \"Studies in linear and non-linear programming\",\n        \"Convex optimization\",\n        \"Nonlinear programming\",\n        \"Safe reinforcement learning using advantage-based intervention\",\n        \"Universal approximation using radial-basis-function networks\",\n        \"On the relation between universality, characteristic kernels and rkhs embedding of measures\",\n        \"Multilayer feedforward networks are universal approximators\",\n        \"A stochastic approximation method\",\n        \"Actor-critic algorithms\",\n        \"Probability: theory and examples\",\n        \"State augmented constrained reinforcement learning: Overcoming the limitations of learning with rewards\",\n        \"Real analysis on intervals\"\n    ],\n    \"reasoning\": \"Selected these 56 references because they represent foundational work and recent advances in Probabilistic Constraint for Safety-Critical Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}