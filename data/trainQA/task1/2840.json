{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 57 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient Sparse Artificial Neural Networks\n\nPaper Abstract: Abstract The brain, as the source of inspiration for Artificial Neural Networks (ANN), is based on a sparse structure. This sparse structure helps the brain to consume less energy, learn easier and generalize patterns better than any other ANN. In this paper, two evolutionary methods for adopting sparsity to ANNs are proposed. In the proposed methods, the sparse structure of a network as well as the values of its parameters are trained and updated during the learning process. The simulation results show that these two methods have better accuracy and faster convergence while they need fewer training samples compared to their sparse and non-sparse counterparts. Furthermore, the proposed methods significantly improve the generalization power and reduce the number of parameters. For example, the sparsification of the ResNet47 network by exploiting our proposed methods for the image classification of ImageNet dataset uses 40% fewer parameters while the top-1 accuracy of the model improves by 12% and 5% compared to the dense network and their sparse counterpart, respectively. As another example, the proposed methods for the CIFAR10 dataset converge to their final structure 7 times faster than its sparse counterpart, while the final accuracy increases by 6%.\n\n{retrieved_text}\n\nGenerate exactly 57 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 57\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Highly selective receptive fields in mouse visual cortex\",\n        \"Cortical connectivity and sensory coding\",\n        \"Sparse orthogonal population representation of spatial context in the retrosplenial cortex\",\n        \"Cortical drive and thalamic feed-forward inhibition control thalamic output synchrony during absence seizures\",\n        \"Decoupling of timescales reveals sparse convergent cpg network in the adult spinal cord\",\n        \"Sparse temporal coding of elementary tactile features during active whisker sensation\",\n        \"Emergence of scaling in random networks\",\n        \"Collective dynamics of ‚Äòsmall-world‚Äônetworks\",\n        \"Differential connectivity and response dynamics of excitatory and inhibitory neurons in visual cortex\",\n        \"Handbook of brain connectivity\",\n        \"The evolutionary origins of modularity\",\n        \"Nonspecific synaptic plasticity improves the recognition of sparse patterns degraded by local noise\",\n        \"Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis\",\n        \"Complex brain networks: graph theoretical analysis of structural and functional systems\",\n        \"Improved classification of blood-brain-barrier drugs using deep learning\",\n        \"A survey on deep learning in medical image analysis\",\n        \"Deep learning for cellular image analysis\",\n        \"Integrating biomedical research and electronic health records to create knowledge based biologically meaningful machine readable embeddings\",\n        \"Predicting protein-protein interactions from matrix-based protein sequence using convolution neural network and feature-selective rotation forest\",\n        \"Deep learning and alternative learning strategies for retrospective real-world clinical data\",\n        \"Evaluation of deep learning in non-coding rna classification\",\n        \"Deep learning: new computational modelling techniques for genomics\",\n        \"A machine learning approach to predicting psychosis using semantic density and latent content analysis\",\n        \"Coherent transport of quantum states by deep reinforcement learning\",\n        \"Searching for exotic particles in high-energy physics with deep learning\",\n        \"A statistical learning framework for materials science: application to elastic moduli of k-nary inorganic polycrystalline compounds\",\n        \"Effect of edge pruning on structural controllability and observability of complex networks\",\n        \"Learning both weights and connections for efficient neural network\",\n        \"Structured pruning of deep convolutional neural networks\",\n        \"Group sparse regularization for deep neural networks\",\n        \"Learning sparse neural networks through l‚Äã_‚Äã0ùëô_0l\\\\_0 regularization\",\n        \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science\",\n        \"Scnn: An accelerator for compressed-sparse convolutional neural networks\",\n        \"K-sparse autoencoders\",\n        \"Learning efficient convolutional networks through network slimming\",\n        \"Evolving deep neural networks\",\n        \"Sparsifying neural network connections for face recognition\",\n        \"Channel pruning for accelerating very deep neural networks\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"The lottery ticket hypothesis: Finding sparse, trainable neural networks\",\n        \"Stabilizing the lottery ticket hypothesis\",\n        \"Deconstructing lottery tickets: Zeros, signs, and the supermask\",\n        \"Sparse networks from scratch: Faster training without losing performance\",\n        \"Rigging the lottery: Making all tickets winners\",\n        \"Morphnet: Fast & simple resource-constrained structure learning of deep networks\",\n        \"Evolving neural networks through augmenting topologies\",\n        \"A neuroevolution approach to general atari game playing\",\n        \"Neural networks with differentiable structure\",\n        \"Constraining connectivity to encourage modularity in hyperneat\",\n        \"Deep learning\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"On the evolution of random graphs\",\n        \"Random graphs with arbitrary degree distributions and their applications\",\n        \"Control systems engineering\",\n        \"Deep residual learning for image recognition\",\n        \"Rectified linear units improve restricted boltzmann machines\",\n        \"Improving neural networks by preventing co-adaptation of feature detectors\"\n    ],\n    \"reasoning\": \"Selected these 57 references because they represent foundational work and recent advances in Efficient Sparse Artificial Neural Networks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}