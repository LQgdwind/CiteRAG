{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Emergence of shared sensory-motor graphical language from visual input\n\nPaper Abstract: Abstract The framework of Language Games studies the emergence of languages in populations of agents. Recent contributions relying on deep learning methods focused on agents communicating via an idealized communication channel, where utterances produced by a speaker are directly perceived by a listener. This comes in contrast with human communication, which instead relies on a sensory-motor channel, where motor commands produced by the speaker (e.g. vocal or gestural articulators) result in sensory effects perceived by the listener (e.g. audio or visual). Here, we investigate if agents can evolve a shared language when they are equipped with a continuous sensory-motor system to produce and perceive signs, e.g. drawings. To this end, we introduce the Graphical Referential Game (greg) where a speaker must produce a graphical utterance to name a visual referent object consisting of combinations of MNIST digits while a listener has to select the corresponding object among distractor referents, given the produced message. The utterances are drawing images produced using dynamical motor primitives combined with a sketching library. To tackle greg we present curves: a multimodal contrastive deep learning mechanism that represents the energy (alignment) between named referents and utterances generated through gradient ascent on the learned energy landscape. We, then, present a set of experiments and metrics based on a systematic compositional dataset to evaluate the resulting language. We show that our method allows the emergence of a shared, graphical language with compositional properties.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Measuring compositionality in representation learning\",\n        \"Learning to guide and to be guided in the architect-builder problem\",\n        \"Understanding linguistic evolution by visualizing the emergence of topographic mappings\",\n        \"Language models are few-shot learners\",\n        \"Simulating the evolution of language\",\n        \"Compositionality and generalization in emergent languages\",\n        \"Communicating artificial neural networks develop efficient color-naming systems\",\n        \"A simple framework for contrastive learning of visual representations\",\n        \"Constraints and preadaptations in the earliest stages of language evolution\",\n        \"Multi-agent compositional communication learning from raw visual input\",\n        \"Language evolution: consensus and controversies\",\n        \"Self-organization in vowel systems\",\n        \"Aux origines du langage\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Experimental semiotics: A review\",\n        \"Compositionality and capacity in emergent languages\",\n        \"Emergence of language with multi-agent games: Learning to communicate with sequences of symbols\",\n        \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\",\n        \"Deep residual learning for image recognition\",\n        \"The origin of speech\",\n        \"Iterated learning and the evolution of language\",\n        \"Natural language does not emerge ‘naturally’ in multi-agent dialog\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Emergent multi-agent communication in the deep learning era\",\n        \"Multi-agent cooperation and the emergence of (natural) language\",\n        \"Emergence of linguistic communication from referential games with symbolic and pixel input\",\n        \"Gradient-based learning applied to document recognition\",\n        \"Ease-of-teaching and language structure from emergent communication\",\n        \"Differentiable drawing and sketching\",\n        \"Learning to draw: Emergent communication through sketching\",\n        \"Emergence of grounded compositional language in multi-agent populations\",\n        \"Multi-agent reinforcement learning as a computational tool for language evolution research: Historical context and future challenges\",\n        \"Cosmo (“communicating about objects using sensory–motor operations”): A bayesian modeling framework for studying speech communication and the emergence of phonological systems\",\n        \"Emergent communication of generalizations\",\n        \"Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications\",\n        \"Language origins viewed in spontaneous and interactive vocal rates of human and bonobo infants\",\n        \"The self-organization of speech sounds\",\n        \"Self-organization in the evolution of speech\",\n        \"The emergence of the shape bias results from communicative efficiency\",\n        \"Language models are unsupervised multitask learners\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Zero-shot text-to-image generation\",\n        \"Compositional languages emerge in a neural iterated learning model\",\n        \"Internal and external pressures on language emergence: least effort, object constancy and frequency\",\n        \"High-resolution image synthesis with latent diffusion models\",\n        \"Photorealistic text-to-image diffusion models with deep language understanding\",\n        \"Dynamic movement primitives -a framework for motor control in humans and humanoid robotics\",\n        \"A self-organizing spatial vocabulary\",\n        \"Language games for autonomous robots\",\n        \"The Talking Heads experiment\",\n        \"The grounded naming game\",\n        \"On the quantal nature of speech\",\n        \"The evolution of combinatorial phonology\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Emergence of shared sensory-motor graphical language from visual input, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}