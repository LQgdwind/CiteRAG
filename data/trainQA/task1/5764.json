{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Experimentally realized memristive memory augmented neural network\n\nPaper Abstract: Abstract Abstract Lifelong on-device learning is a key challenge for machine intelligence, and this requires learning from few, often single, samples. Memory augmented neural network has been proposed to achieve the goal, but the memory module has to be stored in an off-chip memory due to its size. Therefore the practical use has been heavily limited. Previous works on emerging memory-based implementation have difficulties in scaling up because different modules with various structures are difficult to integrate on the same chip and the small sense margin of the content addressable memory for the memory module heavily limited the degree of mismatch calculation. In this work, we implement the entire memory augmented neural network architecture in a fully integrated memristive crossbar platform and achieve an accuracy that closely matches standard software on digital hardware for the Omniglot dataset. The successful demonstration is supported by implementing new functions in crossbars in addition to widely reported matrix multiplications. For example, the locality-sensitive hashing operation is implemented in crossbar arrays by exploiting the intrinsic stochasticity of memristor devices. Besides, the content-addressable memory module is realized in crossbars, which also supports the degree of mismatches. Simulations based on experimentally validated models show such an implementation can be efficiently scaled up for one-shot learning on the Mini-ImageNet dataset. The successful demonstration paves the way for practical on-device lifelong learning and opens possibilities for novel attention-based algorithms not possible in conventional hardware.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Meta-learning with memory-augmented neural networks\",\n        \"Manna: An accelerator for memory-augmented neural networks\",\n        \"Scaling memory-augmented neural networks with sparse reads and writes\",\n        \"Energy and policy considerations for deep learning in NLP\",\n        \"Evaluating the energy efficiency of deep convolutional neural networks on cpus and gpus\",\n        \"X-MANN: A Crossbar based Architecture for Memory Augmented Neural Networks\",\n        \"First draft of a report on the edvac\",\n        \"Memristor-the missing circuit element\",\n        \"Memristive crossbar arrays for brain-inspired computing\",\n        \"Long short-term memory networks in memristor crossbar arrays\",\n        \"In situ training of feed-forward and recurrent convolutional memristor networks\",\n        \"Training and operation of an integrated neuromorphic network based on metal-oxide memristors\",\n        \"Equivalent-accuracy accelerated neural-network training using analogue memory\",\n        \"CMOS-integrated memristive non-volatile computing-in-memory for AI edge processors\",\n        \"Analogue signal and image processing with large memristor crossbars\",\n        \"Sparse coding with memristor networks\",\n        \"Mixed-precision in-memory computing\",\n        \"A general memristor-based partial differential equation solver\",\n        \"Transiently chaotic simulated annealing based on intrinsic nonlinearity of memristors for efficient solution of optimization problems\",\n        \"Power-efficient combinatorial optimization using intrinsic noise in memristor Hopfield neural networks\",\n        \"Dot-product engine for neuromorphic computing: Programming 1t1m crossbar to accelerate matrix-vector multiplication\",\n        \"In-memory hyperdimensional computing\",\n        \"Robust high-dimensional memory-augmented neural networks\",\n        \"Ferroelectric ternary content-addressable memory for one-shot learning\",\n        \"Ferroelectric FET based in-memory computing for few-shot learning\",\n        \"Sapiens: A 64-kb rram-based non-volatile associative memory for one-shot learning and inference at the edge\",\n        \"One shot learning of simple visual concepts\",\n        \"Matching networks for one shot learning\",\n        \"Memristor crossbar arrays with 6-nm half-pitch and 2-nm critical dimension\",\n        \"A fully integrated reprogrammable memristor‚Äìcmos system for efficient multiply‚Äìaccumulate operations\",\n        \"Fully hardware-implemented memristor convolutional neural network\",\n        \"Isaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars\",\n        \"Learning to remember rare events\",\n        \"Locality-sensitive hashing scheme based on p-stable distributions\",\n        \"Low-conductance and multilevel cmos-integrated nanoscale oxide memristors\",\n        \"CMOS-integrated nanoscale memristive crossbars for CNN and optimization acceleration\",\n        \"Similarity search in high dimensions via hashing\",\n        \"Similarity search and locality sensitive hashing using ternary content addressable memories\",\n        \"Query-aware locality-sensitive hashing for approximate nearest neighbor search\",\n        \"A device non-ideality resilient approach for mapping neural networks to crossbar arrays\",\n        \"Geometric conductive filament confinement by nanotips for resistive switching of hfo 2-rram devices with high performance\",\n        \"1 mb 0.41 Œºùúá\\\\mum2 2t-2r cell nonvolatile tcam with two-bit encoding and clocked self-referenced sensing\",\n        \"7.4 a 256b-wordlength reram-based tcam with 1ns search-time and 14√ó\\\\times improvement in wordlength-energyefficiency-density product using 2.5 t1r cell\",\n        \"An area-efficient ternary cam design using floating gate transistors\",\n        \"Magnetoresistive random access memory\",\n        \"Characterizing endurance degradation of incremental switching in analog rram for neuromorphic systems\",\n        \"Endurance and retention degradation of intermediate levels in filamentary analog rram\",\n        \"Statistical fluctuations in hfo x resistive-switching memory: part i-set/reset variability\",\n        \"Simpleshot: Revisiting nearest-neighbor classification for few-shot learning\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Experimentally realized memristive memory augmented neural network, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}