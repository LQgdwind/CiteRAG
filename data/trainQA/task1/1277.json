{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 557 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Neural Language Generation: Formulation, Methods, and Evaluation\n\nPaper Abstract: Abstract Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.\n\n{retrieved_text}\n\nGenerate exactly 557 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 557\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A learning algorithm for boltzmann machines\",\n        \"A re-examination of machine learning approaches for sentence-level mt evaluation\",\n        \"Regression for sentence-level mt evaluation with pseudo references\",\n        \"Adversarial generation of handwritten text images conditioned on sequences\",\n        \"Rethinking attribute representation and injection for sentiment classification\",\n        \"Spice: Semantic propositional image caption evaluation\",\n        \"Guided open vocabulary image captioning with constrained beam search\",\n        \"Vqa: Visual question answering\",\n        \"Towards principled methods for training generative adversarial networks. arxiv e-prints, art.\",\n        \"Affective neural response generation\",\n        \"Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features\",\n        \"Text-based measures of document diversity\",\n        \"An architecture for deep, hierarchical generative models\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Polarized-vae: Proximity based disentangled representation learning for text generation\",\n        \"Meteor: An automatic metric for mt evaluation with improved correlation with human judgments\",\n        \"Generating sentences from disentangled syntactic and semantic spaces\",\n        \"Commonsense for generative multi-hop question answering tasks\",\n        \"What do neural machine translation models learn about morphology?\",\n        \"Towards comparative evaluation and shared tasks for nlg in interactive systems\",\n        \"Climbing towards nlu: On meaning, form, and understanding in the age of data\",\n        \"Scheduled sampling for sequence prediction with recurrent neural networks\",\n        \"A neural probabilistic language model\",\n        \"Estimating or propagating gradients through stochastic neurons for conditional computation\",\n        \"Curriculum learning\",\n        \"Learning long-term dependencies with gradient descent is difficult\",\n        \"Bidirectional recurrent neural networks as generative models\",\n        \"Uncc qa: biomedical question answering system\",\n        \"Personalized and Adaptive Text Simplification\",\n        \"Demystifying mmd gans\",\n        \"Language (technology) is power: A critical survey of” bias” in nlp\",\n        \"Better rewards yield better summaries: Learning to summarise without references\",\n        \"Pros and cons of gan evaluation measures\",\n        \"Generating sentences from a continuous space\",\n        \"Language models are few-shot learners\",\n        \"Artificial intelligence and responsible innovation\",\n        \"Sub-domain modelling for dialogue management with hierarchical reinforcement learning\",\n        \"Language gans falling short\",\n        \"Faithful to the original: Fact aware neural abstractive summarization\",\n        \"Generating and evaluating evaluative arguments\",\n        \"Deep communicating agents for abstractive summarization\",\n        \"Evaluation of text generation: A survey\",\n        \"The price of debiasing automatic metrics in natural language evaluation\",\n        \"Kermit: Generative insertion-based modeling for sequences\",\n        \"Maximum-likelihood augmented discrete generative adversarial networks\",\n        \"Learning to predict explainable plots for neural story generation\",\n        \"A survey on dialogue systems: Recent advances and new frontiers\",\n        \"Adversarial text generation via feature-mover’s distance\",\n        \"Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning\",\n        \"Generative pretraining from pixels\",\n        \"Controllable paraphrase generation with a syntactic exemplar\",\n        \"Enhanced lstm for natural language inference\",\n        \"Fast abstractive summarization with reinforce-selected sentence rewriting\",\n        \"A stable and effective learning strategy for trainable greedy decoding\",\n        \"Long short-term memory-networks for machine reading\",\n        \"Neural summarization by extracting sentences and words\",\n        \"Generating long sequences with sparse transformers\",\n        \"Noisy parallel approximate decoding for conditional recurrent language model\",\n        \"Learning phrase representations using rnn encoder-decoder for statistical machine translation\",\n        \"On the weaknesses of reinforcement learning for neural machine translation\",\n        \"Empirical evaluation of gated recurrent neural networks on sequence modeling\",\n        \"A recurrent latent variable model for sequential data\",\n        \"Sentence mover’s similarity: Automatic evaluation for multi-sentence texts\",\n        \"Neural text generation in stories using entity representations as context\",\n        \"Creative writing with a machine in the loop: Case studies on slogans and stories\",\n        \"Revisiting summarization evaluation for scientific articles\",\n        \"Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality\",\n        \"A machine learning approach to the automatic evaluation of machine translation\",\n        \"Inference suboptimality in variational autoencoders\",\n        \"Deep reinforcement learning for multi-domain dialogue systems\",\n        \"Attention-over-attention neural networks for reading comprehension\",\n        \"Learning to evaluate image captioning\",\n        \"Towards diverse and natural image descriptions via a conditional gan\",\n        \"Transformer-xl: Attentive language models beyond a fixed-length context\",\n        \"Frustratingly short attention spans in neural language modeling\",\n        \"Plug and play language models: a simple approach to controlled text generation\",\n        \"Language modeling with gated convolutional networks\",\n        \"Training language gans from scratch\",\n        \"Question answering by reasoning across documents with graph convolutional networks\",\n        \"Universal transformers\",\n        \"Meteor universal: Language specific translation evaluation for any target language\",\n        \"Survey on evaluation methods for dialogue systems\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Simplifying natural language for aphasic readers\",\n        \"The diag experiments: Natural language generation for intelligent tutoring systems\",\n        \"Wizard of wikipedia: Knowledge-powered conversational agents\",\n        \"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics\",\n        \"Tutorial on variational autoencoders\",\n        \"Enabling language models to fill in the blanks\",\n        \"Multi-task learning for multiple language translation\",\n        \"Learning to generate product reviews from attributes\",\n        \"Unified language model pre-training for natural language understanding and generation\",\n        \"Banditsum: Extractive summarization as a contextual bandit\",\n        \"Hierarchical text generation using an outline\",\n        \"A context-aware natural language generator for dialogue systems\",\n        \"Sequence modeling with unconstrained generation order\",\n        \"Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model\",\n        \"Controllable abstractive summarization\",\n        \"Hierarchical neural story generation\",\n        \"Strategies for structuring story generation\",\n        \"Implicit deep latent variable models for text generation\",\n        \"Maskgan: Better text generation via filling in the _\",\n        \"Unsupervised sentence compression using denoising auto-encoders\",\n        \"Controlling linguistic style aspects in neural language generation\",\n        \"Sentence compression by deletion with lstms\",\n        \"How to write plain English: A book for lawyers and consumers\",\n        \"A new readability yardstick\",\n        \"Taking mt evaluation metrics to extremes: Beyond correlation with human judgments\",\n        \"The importance of generation order in language modeling\",\n        \"Towards making nlg a voice for interpretable machine learning\",\n        \"Natural answer generation with heterogeneous memory\",\n        \"Style transfer in text: Exploration and evaluation\",\n        \"Evaluating effects of machine translation accuracy on cross-lingual patent retrieval\",\n        \"Attention, please! a critical review of neural attention models in natural language processing\",\n        \"deltableu: A discriminative metric for generation tasks with intrinsically diverse targets\",\n        \"Evaluating natural language processing systems\",\n        \"Sentence-level mt evaluation without reference translations: Beyond language modeling\",\n        \"Judge the judges: A large-scale evaluation study of neural language models for online review generation\",\n        \"Survey of the state of the art in natural language generation: Core tasks, applications and evaluation\",\n        \"Image style transfer using convolutional neural networks\",\n        \"Convolutional sequence to sequence learning\",\n        \"Computational approaches to storytelling and creativity\",\n        \"Neural poetry translation\",\n        \"Mask-predict: Parallel decoding of conditional masked language models\",\n        \"Semi-autoregressive training improves mask-predict decoding\",\n        \"Generating topical poetry\",\n        \"Hafez: an interactive poetry generation system\",\n        \"Affect-lm: A neural language model for customizable affective text generation\",\n        \"Heterogeneous automatic mt evaluation through non-parametric metric combinations\",\n        \"A smorgasbord of features for automatic mt evaluation\",\n        \"A systematic exploration of diversity in machine translation\",\n        \"Natural language generation enhances human decision-making with uncertain information\",\n        \"A snapshot of nlg evaluation practices 2005-2014\",\n        \"Plan, write, and revise: an interactive system for open-domain story generation\",\n        \"Generative adversarial nets\",\n        \"Improving neural language models with a continuous cache\",\n        \"Generating sequences with recurrent neural networks\",\n        \"Speech recognition with deep recurrent neural networks\",\n        \"Neural turing machines\",\n        \"Learning to transduce with unbounded memory\",\n        \"Non-autoregressive neural machine translation\",\n        \"Insertion-based decoding with automatically inferred generation order\",\n        \"Levenshtein transformer\",\n        \"Improved training of wasserstein gans\",\n        \"Pixelvae: A latent variable model for natural images\",\n        \"Long text generation via adversarial training with leaked information\",\n        \"Non-autoregressive neural machine translation with enhanced decoder input\",\n        \"Don’t stop pretraining: Adapt language models to domains and tasks\",\n        \"Generating sentences by editing prototypes\",\n        \"Textkd-gan: Text generation using knowledge distillation and generative adversarial networks\",\n        \"Lepor: A robust evaluation metric for machine translation with augmented factors\",\n        \"Automation of summary evaluation by the pyramid method\",\n        \"Unifying human and statistical evaluation for natural language generation\",\n        \"Evaluation of nlg in an end-to-end spoken dialogue system-is it worth it?\",\n        \"Pretrained transformers improve out-of-distribution robustness\",\n        \"Teaching machines to read and comprehend\",\n        \"Gans trained by a two time-scale update rule converge to a local nash equilibrium\",\n        \"Supervised automatic evaluation for summarization with voted regression model\",\n        \"Boundary-seeking generative adversarial networks\",\n        \"Long short-term memory\",\n        \"Focused evaluation for image description with binary forced-choice tasks\",\n        \"Framing image description as a ranking task: Data, models and evaluation metrics\",\n        \"Lexically constrained decoding for sequence generation using grid beam search\",\n        \"Learning to write with cooperative discriminators\",\n        \"The curious case of neural text degeneration\",\n        \"A comprehensive survey of deep learning for image captioning\",\n        \"Improved lexically constrained decoding for translation and monolingual rewriting\",\n        \"Toward controlled generation of text\",\n        \"Natural language to structured query generation via meta-learning\",\n        \"How (not) to train your generative model: Scheduled sampling, likelihood, adversary?\",\n        \"Quantitatively evaluating gans with divergences proposed for training\",\n        \"Automatic detection of generated text is easiest when humans are fooled\",\n        \"Comparison of diverse decoding methods from conditional language models\",\n        \"Automatic evaluation of translation quality for distant language pairs\",\n        \"A neural network for factoid question answering over paragraphs\",\n        \"Adversarial example generation with syntactically controlled paraphrase networks\",\n        \"Categorical reparametrization with gumble-softmax\",\n        \"Way off-policy batch deep reinforcement learning of implicit human preferences in dialog\",\n        \"On using very large target vocabulary for neural machine translation\",\n        \"Perplexity—a measure of the difficulty of speech recognition tasks\",\n        \"Adversarial examples for evaluating reading comprehension systems\",\n        \"Neural crf model for sentence alignment in text simplification\",\n        \"How can we know what language models know?\",\n        \"Hooks in the headline: Learning to generate headlines with controlled styles\",\n        \"Disentangled representation learning for non-parallel text style transfer\",\n        \"Google’s multilingual neural machine translation system: Enabling zero-shot translation\",\n        \"Ilr-based mt comprehension test with multi-level questions\",\n        \"Recurrent continuous translation models\",\n        \"Neural machine translation in linear time\",\n        \"A semantic and syntactic text simplification tool for health content\",\n        \"Towards neural language evaluators\",\n        \"Adversarial evaluation of dialogue models\",\n        \"Scaling laws for neural language models\",\n        \"Generating informative responses with controlled sentence function\",\n        \"Deep reinforcement learning for sequence-to-sequence models\",\n        \"Ctrl: A conditional transformer language model for controllable generation\",\n        \"Sharp nearby, fuzzy far away: How neural language models use context\",\n        \"Generalization through memorization: Nearest neighbor language models\",\n        \"Controlling output length in neural encoder-decoders\",\n        \"A tutorial on deep latent variable models of natural language\",\n        \"Adversarially regularized autoencoders for generating discrete structures\",\n        \"Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel\",\n        \"Auto-encoding variational bayes\",\n        \"An introduction to variational autoencoders\",\n        \"Reformer: The efficient transformer\",\n        \"Opennmt: Open-source toolkit for neural machine translation\",\n        \"Summarization beyond sentence extraction: A probabilistic approach to sentence compression\",\n        \"Six challenges for neural machine translation\",\n        \"An adversarial approach to high-quality, sentiment-controlled neural dialogue generation\",\n        \"Neural machine translation with adequacy-oriented learning\",\n        \"Rankqa: Neural question answering with answer re-ranking\",\n        \"Multiplicative lstm for sequence modelling\",\n        \"A hierarchical approach for generating descriptive image paragraphs\",\n        \"All the news that’s fit to fabricate: Ai-generated text as a tool of media misinformation\",\n        \"Complexity-weighted loss and diverse reranking for sentence simplification\",\n        \"Neural text summarization: A critical evaluation\",\n        \"Improving abstraction in text summarization\",\n        \"A learning approach to improving sentence-level mt evaluation\",\n        \"Importance of search and evaluation strategies in neural dialogue modeling\",\n        \"Babytalk: Understanding and generating simple image descriptions\",\n        \"Ask me anything: Dynamic memory networks for natural language processing\",\n        \"From word embeddings to document distances\",\n        \"Gans for sequences of discrete elements with the gumbel-softmax distribution\",\n        \"The summary loop: Learning to write abstractive summaries without examples\",\n        \"Professor forcing: A new algorithm for training recurrent networks\",\n        \"Multiple-attribute text rewriting\",\n        \"Backward-forward sequence generative network for multiple lexical constraints\",\n        \"The meteor metric for automatic evaluation of machine translation\",\n        \"Best practices for the human evaluation of automatically generated text\",\n        \"Deterministic non-autoregressive neural sequence modeling by iterative refinement\",\n        \"Testing statistical hypotheses\",\n        \"Interpretable Neural Models for Natural Language Processing\",\n        \"Cder: Efficient mt evaluation using block movements\",\n        \"Binary codes capable of correcting deletions, insertions, and reversals\",\n        \"Pre-training via paraphrasing\",\n        \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Retrieval-augmented generation for knowledge-intensive nlp tasks\",\n        \"A diversity-promoting objective function for neural conversation models\",\n        \"Mutual information and diverse decoding improve neural machine translation\",\n        \"A simple, fast diverse decoding algorithm for neural generation\",\n        \"Deep reinforcement learning for dialogue generation\",\n        \"Adversarial learning for neural dialogue generation\",\n        \"Generating long and informative reviews with aspect-aware coarse-to-fine decoding\",\n        \"Visualbert: A simple and performant baseline for vision and language\",\n        \"A topic augmented text generation model: Joint learning of semantics and structural features\",\n        \"End-to-end task-completion neural dialogue systems\",\n        \"Paraphrase generation with deep reinforcement learning\",\n        \"Dialogue generation: From imitation learning to inverse reinforcement learning\",\n        \"End-to-end non-autoregressive neural machine translation with connectionist temporal classification\",\n        \"Commongen: A constrained text generation dataset towards generative commonsense reasoning\",\n        \"An information-theoretic approach to automatic evaluation of summaries\",\n        \"Looking for a few good metrics: Rouge and its evaluation\",\n        \"Orange: a method for evaluating automatic evaluation metrics for machine translation\",\n        \"Global encoding for abstractive summarization\",\n        \"Adversarial ranking for language generation\",\n        \"How can we accelerate progress towards human-like linguistic generalization?\",\n        \"A critical review of recurrent neural networks for sequence learning\",\n        \"Beyond narrative description: Generating poetry from images by multi-adversarial training\",\n        \"Sentiment analysis: Mining opinions, sentiments, and emotions\",\n        \"Iterative policy learning in end-to-end trainable task-oriented neural dialog models\",\n        \"End-to-end optimization of task-oriented dialogue model with deep reinforcement learning\",\n        \"How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation\",\n        \"Bfgan: Backward and forward generative adversarial networks for lexically constrained sentence generation\",\n        \"Coupled generative adversarial networks\",\n        \"Generating wikipedia by summarizing long sequences\",\n        \"Improved image captioning via policy gradient optimization of spider\",\n        \"Table-to-text generation by structure-aware seq2seq learning\",\n        \"Adversarial training for large neural language models\",\n        \"Hierarchical transformers for multi-document summarization\",\n        \"Text summarization with pretrained encoders\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Rhetorically controlled encoder-decoder for modern chinese poetry generation\",\n        \"Yisi-a unified semantic mt quality evaluation and estimation metric for languages with different levels of available resources\",\n        \"Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles\",\n        \"Content preserving text generation with attribute controls\",\n        \"Towards multilingual neural question answering\",\n        \"Revisiting classifier two-sample tests\",\n        \"Automatically assessing machine summary content without a gold standard\",\n        \"Towards an automatic turing test: Learning to evaluate dialogue responses\",\n        \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Lstm based conversation models\",\n        \"Are gans created equal? a large-scale study\",\n        \"Multi-task sequence to sequence learning\",\n        \"Stanford neural machine translation systems for spoken language domains\",\n        \"Effective approaches to attention-based neural machine translation\",\n        \"Results of the wmt19 metrics shared task: Segment-level and strong mt systems pose big challenges\",\n        \"Flowseq: Non-autoregressive conditional sequence generation with generative flow\",\n        \"The concrete distribution: A continuous relaxation of discrete random variables\",\n        \"Controllable sentence simplification: Employing syntactic and lexical constraints\",\n        \"Paraphrasing revisited with neural machine translation\",\n        \"Sentence compression for arbitrary languages via multilingual pivoting\",\n        \"The tipster summac text summarization evaluation\",\n        \"Reference-less quality estimation of text simplification systems\",\n        \"Controllable sentence simplification\",\n        \"Tangled up in bleu: Reevaluating the evaluation of automatic machine translation evaluation metrics\",\n        \"Putting evaluation in context: Contextual embeddings improve machine translation evaluation\",\n        \"Equity beyond bias in language technologies for education\",\n        \"Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference\",\n        \"Middle-out decoding\",\n        \"Coherent dialogue with attention-based language models\",\n        \"What to talk about and how? selective generation using lstms with coarse-to-fine alignment\",\n        \"Mogrifier lstm\",\n        \"A deep memory-based architecture for sequence-to-sequence learning\",\n        \"Unrolled generative adversarial networks\",\n        \"Cgmh: Constrained sentence generation by metropolis-hastings sampling\",\n        \"Language as a latent variable: Discrete generative models for sentence compression\",\n        \"Neural variational inference for text processing\",\n        \"Turing-NLG: A 17-billion-parameter language model by Microsoft\",\n        \"Statistical language models based on neural networks\",\n        \"Recurrent neural network based language model\",\n        \"Conditional generative adversarial nets\",\n        \"Recurrent models of visual attention\",\n        \"Handwriting styles: benchmarks and evaluation metrics\",\n        \"Nested lstms\",\n        \"Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation\",\n        \"Backward and forward language modeling for constrained sentence generation\",\n        \"Sequence to better sequence: continuous revision of combinatorial structures\",\n        \"Neural semantic encoders\",\n        \"Correcting length bias in neural machine translation\",\n        \"Reliable fidelity and diversity metrics for generative models\",\n        \"Theoretical insights into memorization in gans\",\n        \"Summarunner: A recurrent neural network based sequence model for extractive summarization of documents\",\n        \"Abstractive text summarization using sequence-to-sequence rnns and beyond\",\n        \"There’s no comparison: Reference-less evaluation metrics in grammatical error correction\",\n        \"Ranking sentences for extractive summarization with reinforcement learning\",\n        \"On memory in human and artificial language processing systems\",\n        \"Better summarization evaluation with word embeddings for rouge\",\n        \"Relgan: Relational generative adversarial networks for text generation\",\n        \"An evaluation tool for machine translation: Fast evaluation for mt research\",\n        \"Abstractive document summarization without parallel data\",\n        \"Controllable text simplification with lexical constraint loss\",\n        \"Exploring neural text simplification models\",\n        \"Polite dialogue generation without parallel data\",\n        \"Why we need new evaluation metrics for nlg\",\n        \"A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation\",\n        \"Parallel wavenet: Fast high-fidelity speech synthesis\",\n        \"An assessment of the accuracy of automatic evaluation in summarization\",\n        \"Iter: Improving translation edit rate through optimizable edit costs\",\n        \"Bleu: a method for automatic evaluation of machine translation\",\n        \"User modelling in text generation\",\n        \"Who, what, when, where, why?: comparing multiple approaches to the cross-lingual 5w task\",\n        \"E-rating machine translation\",\n        \"Multi-reward reinforced summarization with saliency and entailment\",\n        \"A deep reinforced model for abstractive summarization\",\n        \"Effective estimation of deep generative language models\",\n        \"Adversarial advantage actor-critic model for task-completion dialogue policy learning\",\n        \"Deep dyna-q: Integrating planning for task-completion dialogue policy learning\",\n        \"Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning\",\n        \"Language models as knowledge bases?\",\n        \"Learning to score system summaries for better content selection evaluation\",\n        \"chrf: character n-gram f-score for automatic mt evaluation\",\n        \"chrf++: words helping character n-grams\",\n        \"Fast lexically constrained decoding with dynamic beam allocation for neural machine translation\",\n        \"Language generation with recurrent generative adversarial networks without pre-training\",\n        \"Variational autoencoder for deep learning of images, labels and captions\",\n        \"Counterfactual story reasoning and generation\",\n        \"Conversing by reading: Contentful neural conversation with on-demand machine reading\",\n        \"Training a sentence-level machine translation confidence measure\",\n        \"Improving language understanding with unsupervised learning\",\n        \"Language models are unsupervised multitask learners\",\n        \"Compressive transformers for long-range sequence modelling\",\n        \"Do transformers need deep long-range memory\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Techniques for learning binary stochastic feedforward neural networks\",\n        \"Unsupervised pretraining for sequence to sequence learning\",\n        \"Stand-alone self-attention in vision models\",\n        \"A decade of automatic content evaluation of news summaries: Reassessing the state of the art\",\n        \"Sequence level training with recurrent neural networks\",\n        \"Classification accuracy score for conditional generative models\",\n        \"Natural language generation challenges for explainable ai\",\n        \"Why do we still use 18-year old BLEU\",\n        \"An investigation into the validity of some metrics for automatically evaluating natural language generation systems\",\n        \"Stochastic backpropagation and approximate inference in deep generative models\",\n        \"Tine: A metric to assess mt adequacy\",\n        \"How much knowledge can you pack into the parameters of a language model?\",\n        \"What’s in a name? reducing bias in bios without access to protected attributes\",\n        \"Knowledge-aware language model pretraining\",\n        \"Insertion-deletion transformer\",\n        \"Learning representations by back-propagating errors\",\n        \"A neural attention model for abstractive sentence summarization\",\n        \"Non-autoregressive machine translation with latent alignments\",\n        \"Convolutional, long short-term memory, fully connected deep neural networks\",\n        \"Assessing generative models via precision and recall\",\n        \"Hierarchical reinforcement learning for open-domain dialog\",\n        \"Improved techniques for training gans\",\n        \"Deep reinforcement learning for modeling chit-chat dialog with discrete attributes\",\n        \"Relational recurrent neural networks\",\n        \"Atomic: An atlas of machine commonsense for if-then reasoning\",\n        \"Introducing mathqa: a math-aware question answering system\",\n        \"Bidirectional recurrent neural networks\",\n        \"Get to the point: Summarization with pointer-generator networks\",\n        \"What makes a good conversation? how controllable attributes affect human judgments\",\n        \"Bleurt: Learning robust metrics for text generation\",\n        \"A hybrid convolutional variational autoencoder for text generation\",\n        \"On accurate evaluation of gans for language generation\",\n        \"Controlling politeness in neural machine translation via side constraints\",\n        \"Building end-to-end dialogue systems using generative hierarchical neural network models\",\n        \"A hierarchical latent variable encoder-decoder model for generating dialogues\",\n        \"Automatic fact-guided sentence modification\",\n        \"Neural responding machine for short-text conversation\",\n        \"Generating long and diverse responses with neural conversation models\",\n        \"Neural text simplification of clinical letters with a domain specific phrase table\",\n        \"Style transfer from non-parallel text by cross-alignment\",\n        \"Toward diverse text generation with inverse reinforcement learning\",\n        \"Ruse: Regressor using sentence embeddings for automatic machine translation evaluation\",\n        \"How good is my gan?\",\n        \"Megatron-lm: Training multi-billion parameter language models using gpu model parallelism\",\n        \"Deep headline generation for clickbait detection\",\n        \"Syntactic simplification and text cohesion\",\n        \"Learning an unreferenced metric for online dialogue evaluation\",\n        \"A study of translation edit rate with targeted human annotation\",\n        \"Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric\",\n        \"The evolved transformer\",\n        \"Matching the blanks: Distributional similarity for relation learning\",\n        \"Ladder variational autoencoders\",\n        \"Mass: Masked sequence to sequence pre-training for language generation\",\n        \"Latent space expanded variational autoencoder for sentence generation\",\n        \"Two are better than one: An ensemble of retrieval-and generation-based dialog systems\",\n        \"A neural network approach to context-sensitive generation of conversational responses\",\n        \"Machine translation evaluation versus quality estimation\",\n        \"Estimating the sentence-level quality of machine translation systems\",\n        \"Natural language generation using reinforcement learning with external rewards\",\n        \"Cold fusion: Training seq2seq models together with language models\",\n        \"A deeper exploration of the standard pb-smt approach to text simplification and its evaluation\",\n        \"Eed: Extended edit distance measure for machine translation\",\n        \"Beer: Better evaluation as ranking\",\n        \"Narrative generation in the wild: Methods from nanogenmo\",\n        \"Evaluation measures for text summarization\",\n        \"Evaluating evaluation methods for generation in the presence of variation\",\n        \"Insertion transformer: Flexible sequence generation via insertion operations\",\n        \"Variational recurrent neural machine translation\",\n        \"Can unconditional language models recover arbitrary sentences?\",\n        \"On extractive and abstractive neural document summarization with transformer language models\",\n        \"Towards text generation with adversarially learned neural outlines\",\n        \"End-to-end memory networks\",\n        \"Bleu is not suitable for the evaluation of text simplification\",\n        \"Semantic structural evaluation for text simplification\",\n        \"Simple and effective text simplification using semantic and neural methods\",\n        \"On the importance of diversity in question generation for qa\",\n        \"Videobert: A joint model for video and language representation learning\",\n        \"Joint learning of a dual smt system for paraphrase generation\",\n        \"Unsupervised neural text simplification\",\n        \"Generating text with recurrent neural networks\",\n        \"Sequence to sequence learning with neural networks\",\n        \"Improved semantic representations from tree-structured long short-term memory networks\",\n        \"olmpics–on what language model pre-training captures\",\n        \"Cluster-based beam search for pointer-generator chatbot grounded by knowledge\",\n        \"Subgoal discovery for hierarchical dialogue policy learning\",\n        \"Context-aware natural language generation with recurrent neural networks\",\n        \"Target-guided open-domain conversation\",\n        \"Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems\",\n        \"Text complexity and text simplification\",\n        \"A note on the evaluation of generative models\",\n        \"How to make context more useful? an empirical study on context-aware neural conversational models\",\n        \"Low-resource neural headline generation\",\n        \"Accelerated dp based search for statistical translation\",\n        \"Recurrent memory networks for language modeling\",\n        \"A simple method for commonsense reasoning\",\n        \"Transformer dissection: An unified understanding for transformer’s attention via the lens of kernel\",\n        \"Capturing greater context for question generation\",\n        \"Improving conditional sequence generative adversarial networks by stepwise evaluation\",\n        \"Computing machinery and intelligence-am turing\",\n        \"Attention is all you need\",\n        \"Cider: Consensus-based image description evaluation\",\n        \"Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge\",\n        \"Diverse beam search: Decoding diverse solutions from neural sequence models\",\n        \"Extracting and composing robust features with denoising autoencoders\",\n        \"Order matters: Sequence to sequence for sets\",\n        \"Pointer networks\",\n        \"Grammar as a foreign language\",\n        \"A neural conversational model\",\n        \"Show and tell: A neural image caption generator\",\n        \"Towards adaptive text summarization: How does compression rate affect summary readability of l2 texts?\",\n        \"The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives\",\n        \"Paradise: A framework for evaluating spoken dialogue agents\",\n        \"Bert has a mouth, and it must speak: Bert as a markov random field language model\",\n        \"Improving neural language modeling via adversarial training\",\n        \"Paperrobot: Incremental draft generation of scientific ideas\",\n        \"Text simplification using neural machine translation\",\n        \"Character: Translation edit rate on character level\",\n        \"Topic-guided variational auto-encoder for text generation\",\n        \"Non-monotonic sequential text generation\",\n        \"Neural text generation with unlikelihood training\",\n        \"Loss functions for multiset prediction\",\n        \"Backpropagation through time: What it is and how to do it\",\n        \"Memory networks\",\n        \"Neural domain adaptation for biomedical question answering\",\n        \"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"A learning algorithm for continually running fully recurrent neural networks\",\n        \"Sequence-to-sequence learning as beam-search optimization\",\n        \"Challenges in data-to-document generation\",\n        \"Transfertransfo: A transfer learning approach for neural network based conversational agents\",\n        \"Google’s neural machine translation system: Bridging the gap between human and machine translation\",\n        \"On multiplicative integration with recurrent neural networks\",\n        \"Sentence simplification by monolingual machine translation\",\n        \"Deliberation networks: Sequence generation beyond one-pass decoding\",\n        \"Copy or rewrite: Hybrid summarization with hierarchical reinforcement learning\",\n        \"Topic augmented neural response generation with a joint attention mechanism\",\n        \"Convolutional lstm network: A machine learning approach for precipitation nowcasting\",\n        \"Neural extractive text summarization with syntactic compression\",\n        \"Dp-gan: Diversity-promoting generative adversarial network for generating informative and diversified text\",\n        \"A skeleton-based model for promoting coherence among sentences in narrative story generation\",\n        \"Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Clickbait? sensational headline generation with auto-tuned reinforcement learning\",\n        \"Optimizing statistical machine translation for text simplification\",\n        \"I, poet: automatic chinese poetry composition through a generative summarization framework under constrained optimization\",\n        \"Peak: Pyramid evaluation via automated knowledge extraction\",\n        \"Video captioning by adversarial lstm\",\n        \"Breaking the beam search curse: A study of (re-) scoring methods and stopping criteria for neural machine translation\",\n        \"Improving neural machine translation with conditional sequence generative adversarial nets\",\n        \"Review networks for caption generation\",\n        \"Unsupervised text style transfer using language models as discriminators\",\n        \"Improved variational autoencoders for text modeling using dilated convolutions\",\n        \"Depth-gated lstm\",\n        \"Plan-and-write: Towards better automatic storytelling\",\n        \"Epitomic variational autoencoders\",\n        \"Automatic poetry generation with mutual reinforcement learning\",\n        \"Neural generative question answering\",\n        \"Learning and evaluating general linguistic intelligence\",\n        \"Memory architectures in recurrent neural network language models\",\n        \"Using grice’s maxim of quantity to select the content of plan descriptions\",\n        \"Seqgan: Sequence generative adversarial nets with policy gradient\",\n        \"A review of recurrent neural networks: Lstm cells and network architectures\",\n        \"Big bird: Transformers for longer sequences\",\n        \"Visualizing and understanding convolutional networks\",\n        \"Variational neural machine translation\",\n        \"Pretraining-based natural language generation for text summarization\",\n        \"Multimodal hierarchical reinforcement learning policy for task-oriented visual dialog\",\n        \"Flexible and creative chinese poetry generation using neural memory\",\n        \"Extract domain-specific paraphrase from monolingual corpus for automatic evaluation of machine translation\",\n        \"Bertscore: Evaluating text generation with bert\",\n        \"Chinese poetry generation with recurrent neural networks\",\n        \"Neural latent extractive document summarization\",\n        \"Generating informative and diverse conversational responses via adversarial information maximization\",\n        \"Generating text via adversarial training\",\n        \"Adversarial feature matching for text generation\",\n        \"Pointer: Constrained text generation via insertion-based generative pre-training\",\n        \"Style transfer as unsupervised machine translation\",\n        \"Adversarially regularized autoencoders\",\n        \"Integrating transformer and paraphrase rules for sentence simplification\",\n        \"Attention-via-attention neural machine translation\",\n        \"Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning\",\n        \"Zero-shot dialog generation with cross-domain latent actions\",\n        \"Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance\",\n        \"Personalized dialogue generation with diversified traits\",\n        \"Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction\",\n        \"A c-lstm neural network for text classification\",\n        \"Emotional chatting machine: Emotional conversation generation with internal and external memory\",\n        \"Selective encoding for abstractive sentence summarization\",\n        \"Hype: A benchmark for human eye perceptual evaluation of generative models\",\n        \"Self-adversarial learning with comparative discrimination for text generation\",\n        \"Unpaired image-to-image translation using cycle-consistent adversarial networks\",\n        \"Long short-term memory over recursive structures\",\n        \"Texygen: A benchmarking platform for text generation models\",\n        \"Fine-tuning language models from human preferences\",\n        \"Strokebank: Automating personalized chinese handwriting generation\"\n    ],\n    \"reasoning\": \"Selected these 557 references because they represent foundational work and recent advances in Neural Language Generation: Formulation, Methods, and Evaluation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}