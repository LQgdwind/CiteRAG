{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 207 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models\n\nPaper Abstract: Abstract While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.\n\n{retrieved_text}\n\nGenerate exactly 207 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 207\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Evaluating correctness and faithfulness of instruction-following models for question answering\",\n        \"Do language models know when they’re hallucinating references?\",\n        \"Mega: Multilingual evaluation of generative ai\",\n        \"Tracing knowledge in language models back to the training data\",\n        \"Self-consuming generative models go mad\",\n        \"The internal state of an llm knows when its lying\",\n        \"Training a helpful and harmless assistant with reinforcement learning from human feedback\",\n        \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\",\n        \"Predicting sentences using n-gram language models\",\n        \"Improving language models by retrieving from trillions of tokens\",\n        \"Language models are few-shot learners\",\n        \"Neural machine translation with monolingual translation memory\",\n        \"Factual error correction for abstractive summarization models\",\n        \"Instruction mining: High-quality instruction data selection for large language models\",\n        \"Bias and fairness in natural language processing\",\n        \"A survey on evaluation of large language models\",\n        \"Purr: Efficiently editing language model hallucinations by denoising language model corruptions\",\n        \"A survey on dialogue systems: Recent advances and new frontiers\",\n        \"Alpagasus: Training a better alpaca with fewer data\",\n        \"Factool: Factuality detection in generative ai – a tool augmented framework for multi-task and multi-domain scenarios\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Dola: Decoding by contrasting layers improves factuality in large language models\",\n        \"Scaling instruction-finetuned language models\",\n        \"Lm vs lm: Detecting factual errors via cross examination\",\n        \"Free dolly: Introducing the world’s first truly open instruction-tuned llm\",\n        \"Knowledge enhanced fine-tuning for better handling unseen entities in dialogue generation\",\n        \"Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better\",\n        \"Editing factual knowledge in language models\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Chain-of-verification reduces hallucination in large language models\",\n        \"A survey for in-context learning\",\n        \"Understanding iterative revision from human-written text\",\n        \"Improving factuality and reasoning in language models through multiagent debate\",\n        \"Shifting attention to relevance: Towards the uncertainty estimation of large language models\",\n        \"FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization\",\n        \"On the origin of hallucinations in conversational models: Is it the datasets or the models?\",\n        \"Evaluating groundedness in dialogue systems: The BEGIN benchmark\",\n        \"Improving factual consistency in summarization with compression-based post-editing\",\n        \"Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs\",\n        \"Bridging the gap: A survey on integrating (human) feedback for natural language generation\",\n        \"Scaling laws for reward model overoptimization\",\n        \"Rarr: Researching and revising what language models say, using language models\",\n        \"Enabling large language models to generate text with citations\",\n        \"Creating training corpora for NLG micro-planners\",\n        \"A survey on bias in deep nlp\",\n        \"Reinforcement learning for language models\",\n        \"Critic: Large language models can self-correct with tool-interactive critiquing\",\n        \"Hallucinations in large multilingual translation models\",\n        \"Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation\",\n        \"Detecting and preventing hallucinations in large vision language models\",\n        \"On calibration of modern neural networks\",\n        \"The curious case of neural text degeneration\",\n        \"Look before you leap: An exploratory study of uncertainty measurement for large language models\",\n        \"Transformer-patcher: One mistake worth one neuron\",\n        \"Survey of hallucination in natural language generation\",\n        \"How can we know when language models know? on the calibration of language models for question answering\",\n        \"Language models (mostly) know what they know\",\n        \"Challenges and applications of large language models\",\n        \"Openassistant conversations–democratizing large language model alignment\",\n        \"Evaluating the factual consistency of abstractive text summarization\",\n        \"Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning\",\n        \"Albert: A lite bert for self-supervised learning of language representations\",\n        \"Measuring faithfulness in chain-of-thought reasoning\",\n        \"Internet-augmented language models through few-shot prompting for open-domain question answering\",\n        \"Platypus: Quick, cheap, and powerful refinement of llms\",\n        \"Hallucinations in neural machine translation\",\n        \"Factuality enhanced language models for open-ended text generation\",\n        \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Retrieval-augmented generation for knowledge-intensive nlp tasks\",\n        \"A survey on retrieval-augmented text generation\",\n        \"Halueval: A large-scale hallucination evaluation benchmark for large language models\",\n        \"Pretrained language models for text generation: A survey\",\n        \"Inference-time intervention: Eliciting truthful answers from a language model\",\n        \"Self-checker: Plug-and-play modules for fact-checking with large language models\",\n        \"How pre-trained language models capture factual knowledge? a causal-inspired analysis\",\n        \"Chain of knowledge: A framework for grounding large language models with structured knowledge bases\",\n        \"Evaluating object hallucination in large vision-language models\",\n        \"Sébastien Bubeck\",\n        \"Batgpt: A bidirectional autoregessive talker from generative pre-trained transformer\",\n        \"Let’s verify step by step\",\n        \"Rouge: A package for automatic evaluation of summaries\",\n        \"Truthfulqa: Measuring how models mimic human falsehoods\",\n        \"Generating with confidence: Uncertainty quantification for black-box large language models\",\n        \"Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models\",\n        \"Aligning large multi-modal model with robust instruction tuning\",\n        \"Visual instruction tuning\",\n        \"Reta-llm: A retrieval-augmented large language model toolkit\",\n        \"Lost in the middle: How language models use long contexts\",\n        \"A token-level reference-free hallucination detection benchmark for free-form text generation\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Zero-resource hallucination prevention for large language models\",\n        \"Chatgpt as a factual inconsistency evaluator for abstractive text summarization\",\n        \"Augmented large language models with parametric knowledge guiding\",\n        \"Time waits for no one! analysis and challenges of temporal misalignment\",\n        \"Video-chatgpt: Towards detailed video understanding via large vision and language models\",\n        \"When not to trust language models: Investigating effectiveness of parametric and non-parametric memories\",\n        \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\",\n        \"On faithfulness and factuality in abstractive summarization\",\n        \"Sources of hallucination by large language models on inference tasks\",\n        \"Locating and editing factual associations in gpt\",\n        \"Mass-editing memory in a transformer\",\n        \"Augmented language models: a survey\",\n        \"Recurrent neural network based language model\",\n        \"Recent advances in natural language processing via large pre-trained language models: A survey\",\n        \"Factscore: Fine-grained atomic evaluation of factual precision in long form text generation\",\n        \"Memory-based model editing at scale\",\n        \"Halo: Estimation and reduction of hallucinations in open-source weak large language models\",\n        \"Generating benchmarks for factuality evaluation of language models\",\n        \"Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation\",\n        \"Fixing model bugs with natural language patches\",\n        \"Webgpt: Browser-assisted question-answering with human feedback\",\n        \"Summarunner: A recurrent neural network based sequence model for extractive summarization of documents\",\n        \"Jfleg: A fluency corpus and benchmark for grammatical error correction\",\n        \"Biases in large language models: Origins, inventory and discussion\",\n        \"Large dual encoders are generalizable retrievers\",\n        \"Skeleton-of-thought: Large language models can do parallel decoding\",\n        \"Can lms learn new entities from descriptions? challenges in propagating injected knowledge\",\n        \"Gpt-4 technical report\",\n        \"Training language models to follow instructions with human feedback\",\n        \"ToTTo: A controlled table-to-text generation dataset\",\n        \"Generative agents: Interactive simulacra of human behavior\",\n        \"Faster and smaller n-gram language models\",\n        \"The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only\",\n        \"Check your facts and try again: Improving large language models with external knowledge and automated feedback\",\n        \"Instruction tuning with gpt-4\",\n        \"Discovering language model behaviors with model-written evaluations\",\n        \"Summarization is (almost) dead\",\n        \"merge conflicts!\\\" exploring the impacts of external distractors to parametric knowledge graphs\",\n        \"Making language models better tool learners with execution feedback\",\n        \"Tool learning with foundation models\",\n        \"Pre-trained models for natural language processing: A survey\",\n        \"Language models are unsupervised multitask learners\",\n        \"Question decomposition improves the faithfulness of model-generated reasoning\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"In-context retrieval-augmented language models\",\n        \"Exploring the relationship between llm hallucinations and prompt linguistic nuances: Readability, formality, and concreteness\",\n        \"Controlling hallucinations at word level in data-to-text generation\",\n        \"Investigating the factual knowledge boundary of large language models with retrieval augmentation\",\n        \"How much knowledge can you pack into the parameters of a language model?\",\n        \"The probabilistic relevance framework: Bm25 and beyond\",\n        \"Bloom: A 176b-parameter open-access multilingual language model\",\n        \"Reinforcement learning from human feedback: Progress and challenges\",\n        \"Proximal policy optimization algorithms\",\n        \"Large language models can be easily distracted by irrelevant context\",\n        \"Natural language to code translation with execution\",\n        \"Trusting your evidence: Hallucinate less with context-aware decoding\",\n        \"Replug: Retrieval-augmented black-box language models\",\n        \"Prompting gpt-3 to be reliable\",\n        \"Editable neural networks\",\n        \"Pandagpt: One model to instruction-follow them all\",\n        \"Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs?\",\n        \"Black-box tuning for language-model-as-a-service\",\n        \"Moss: Training conversational language models from synthetic data\",\n        \"Task ambiguity in humans and language models\",\n        \"Stanford alpaca: An instruction-following llama model\",\n        \"Behavioral cloning from observation\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Llama 2: Open foundation and fine-tuned chat models\",\n        \"Med-halt: Medical domain hallucination test for large language models\",\n        \"A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation\",\n        \"Attention is all you need\",\n        \"On exposure bias, hallucination and domain shift in neural machine translation\",\n        \"Voyager: An open-ended embodied agent with large language models\",\n        \"Revisiting challenges in data-to-text generation with fact grounding\",\n        \"Self-consistency improves chain of thought reasoning in language models\",\n        \"How far can camels go? exploring the state of instruction tuning on open resources\",\n        \"Self-instruct: Aligning language models with self-generated instructions\",\n        \"Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration\",\n        \"Jailbroken: How does llm safety training fail?\",\n        \"Finetuned language models are zero-shot learners\",\n        \"Chain-of-thought prompting elicits reasoning in large language models\",\n        \"Simple synthetic data reduces sycophancy in large language models\",\n        \"Qafacteval: Improved qa-based factual consistency evaluation for summarization\",\n        \"On decoder-only architecture for speech-to-text and large language model integration\",\n        \"Do plms know and understand ontological knowledge?\",\n        \"On hallucination and predictive uncertainty in conditional language generation\",\n        \"Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts\",\n        \"Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms\",\n        \"Baize: An open-source chat model with parameter-efficient tuning on self-chat data\",\n        \"React: Synergizing reasoning and acting in language models\",\n        \"mplug-owl: Modularization empowers large language models with multimodality\",\n        \"Do large language models know what they don’t know?\",\n        \"Kola: Carefully benchmarking world knowledge of large language models\",\n        \"Improving language models via plug-and-play retrieval feedback\",\n        \"Automatic evaluation of attribution by large language models\",\n        \"Decoding methods in neural language generation: a survey\",\n        \"Glm-130b: An open bilingual pre-trained model\",\n        \"AlignScore: Evaluating factual consistency with a unified alignment function\",\n        \"How language model hallucinations can snowball\",\n        \"Instruction tuning for large language models: A survey\",\n        \"Mitigating language model hallucination with interactive question-knowledge alignment\",\n        \"Bertscore: Evaluating text generation with bert\",\n        \"Hybrid retrieval-augmented generation for real-time composition assistance\",\n        \"Verify-and-edit: A knowledge-enhanced chain-of-thought framework\",\n        \"Automatic calibration and error correction for large language models via pareto optimal self-supervision\",\n        \"Dense text retrieval based on pretrained language models: A survey\",\n        \"A survey of large language models\",\n        \"Can we edit factual knowledge by in-context learning?\",\n        \"Secrets of rlhf in large language models part i: Ppo\",\n        \"Why does chatgpt fall short in providing truthful answers\",\n        \"Qmsum: A new benchmark for query-based multi-domain meeting summarization\",\n        \"Mquake: Assessing knowledge editing in language models via multi-hop questions\",\n        \"Lima: Less is more for alignment\",\n        \"Context-faithful prompting for large language models\",\n        \"Enhancing factual consistency of abstractive summarization\",\n        \"Promptbench: Towards evaluating the robustness of large language models on adversarial prompts\",\n        \"Universal and transferable adversarial attacks on aligned language models\"\n    ],\n    \"reasoning\": \"Selected these 207 references because they represent foundational work and recent advances in Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}