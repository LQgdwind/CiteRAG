{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 150 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning\n\nPaper Abstract: Abstract. The success of machine learning is fueled by the increasing availability of computing power and large training datasets. The training data is used to learn new models or update existing ones, assuming that it is sufficiently representative of the data that will be encountered at test time. This assumption is challenged by the threat of poisoning, an attack that manipulates the training data to compromise the model’s performance at test time. Although poisoning has been acknowledged as a relevant threat in industry applications, and a variety of different attacks and defenses have been proposed so far, a complete systematization and critical review of the field is still missing. In this survey, we provide a comprehensive systematization of poisoning attacks and defenses in machine learning, reviewing more than 100 papers published in the field in the last 15 years. We start by categorizing the current threat models and attacks, and then organize existing defenses accordingly. While we focus mostly on computer-vision applications, we argue that our systematization also encompasses state-of-the-art attacks and defenses for other data modalities. Finally, we discuss existing resources for research in poisoning, and shed light on the current limitations and open research questions in this research field.\n\n{retrieved_text}\n\nGenerate exactly 150 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 150\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"VENOMAVE: Clean-Label Poisoning Against Speech Recognition\",\n        \"Bullseye polytope: A scalable clean-label poisoning attack with improved transferability\",\n        \"Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers\",\n        \"How To Backdoor Federated Learning\",\n        \"Baseline Pruning-Based Approach to Trojan Detection in Neural Networks\",\n        \"Defense Against Reward Poisoning Attacks in Reinforcement Learning\",\n        \"Detecting poisoning attacks on ML in iot environments\",\n        \"A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning\",\n        \"Can ML be secure?\",\n        \"Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks\",\n        \"Random search for hyper-parameter optimization\",\n        \"Analyzing Federated Learning through an Adversarial Lens\",\n        \"Bagging classifiers for fighting poisoning attacks in adversarial classification tasks\",\n        \"Evasion Attacks against ML at Test Time\",\n        \"Support Vector Machines Under Adversarial Label Noise\",\n        \"Poisoning Attacks against Support Vector Machines\",\n        \"Is data clustering in adversarial settings secure?\",\n        \"Poisoning behavioral malware clustering\",\n        \"Wild patterns: Ten years after the rise of adversarial ML\",\n        \"Adversarial Attacks on Node Embeddings via Graph Poisoning\",\n        \"Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff\",\n        \"DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations\",\n        \"Understanding Distributed Poisoning Attack in Federated Learning\",\n        \"On evaluating adversarial robustness\",\n        \"Regularization Can Help Mitigate Poisoning Attacks… with the Right Hyperparameters\",\n        \"Adversarial attacks and defences: A survey\",\n        \"Property Inference From Poisoning\",\n        \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering\",\n        \"Mitigating backdoor attacks in LSTM-based Text Classification Sys. by Backdoor Keyword Identification\",\n        \"DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks\",\n        \"ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks Without Training Substitute Models\",\n        \"On Collective Robustness of Bagging Against Data Poisoning\",\n        \"Targeted backdoor attacks on deep learning systems using data poisoning\",\n        \"Badnl: Backdoor attacks against nlp models\",\n        \"Defending against backdoor attack on deep neural networks\",\n        \"Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification\",\n        \"Sentinet: Detecting localized universal attacks against deep learning systems\",\n        \"Energy-Latency Attacks via Sponge Poisoning\",\n        \"Machine Learning Security against Data Poisoning: Are We There Yet?\",\n        \"Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions\",\n        \"A black-box adversarial attack for poisoning clustering\",\n        \"The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers?\",\n        \"Casting out Demons: Sanitizing Training Data for Anomaly Sensors\",\n        \"Infinity-Norm Support Vector Machines Against Adversarial Label Contamination\",\n        \"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks\",\n        \"Sever: A robust meta-algorithm for stochastic optimization\",\n        \"Februus: Input purification defense against trojan attacks on deep neural network systems\",\n        \"LIRA: Learnable, Imperceptible and Robust Backdoor Attacks\",\n        \"Generic Methods for Optimization-Based Modeling\",\n        \"Black-box Detection of Backdoor Attacks with Limited Information and Data\",\n        \"Victoria Krakovna\",\n        \"Preventing unauthorized use of proprietary data: Poisoning for secure dataset release\",\n        \"Adversarial Examples Make Strong Poisons\",\n        \"Backdoor attacks and countermeasures on deep learning: A comprehensive review\",\n        \"Damith C Ranasinghe\",\n        \"What Doesn’t Kill You Makes You Robust (er): Adversarial Training against Poisons and Backdoors\",\n        \"Motivating the rules of the game for adversarial example research\",\n        \"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses\",\n        \"Planting undetectable backdoors in machine learning models\",\n        \"Advbox: a toolbox to generate adversarial examples that fool neural networks\",\n        \"Machine Learning Security in Industry: A Quantitative Survey\",\n        \"Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks\",\n        \"Badnets: Identifying vulnerabilities in the ML model supply chain\",\n        \"Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in AI Systems\",\n        \"Benjamin IP Rubinstein\",\n        \"Contamination Attacks and Mitigation in Multi-Party ML\",\n        \"On the effectiveness of mitigating data poisoning attacks with gradient shaping\",\n        \"Michael Cogswell\",\n        \"Neuroninspect: Detecting backdoors in neural networks via output explanations\",\n        \"Yunhan Huang and Quanyan Zhu\",\n        \"Cristina Nita-Rotaru\",\n        \"Niklas Pousette Harger\",\n        \"Mohammad Samragh\",\n        \"Certified robustness of nearest neighbors against data poisoning attacks\",\n        \"Efficient global optimization of expensive black-box functions\",\n        \"Defense against neural trojan attacks: A survey\",\n        \"Marius Kloft and Pavel Laskov\",\n        \"Can You Hear It? Backdoor Attacks via Ultrasonic Triggers\",\n        \"Pang Wei Koh and Percy Liang\",\n        \"Stronger data poisoning attacks break data sanitization defenses\",\n        \"Hamed Pirsiavash\",\n        \"Adversarial ML-industry perspectives\",\n        \"Curie: A method for protecting SVM Classifier from Poisoning Attack\",\n        \"Alexander Levine and Soheil Feizi\",\n        \"Hyperband: A novel bandit-based approach to hyperparameter optimization\",\n        \"Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization\",\n        \"Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks\",\n        \"Anti-backdoor learning: Training clean models on poisoned data\",\n        \"Backdoor learning: A survey\",\n        \"Rethinking the trigger of backdoor attack\",\n        \"Yevgeniy Vorobeychik\",\n        \"Brendan Dolan-Gavitt\",\n        \"Luciano Sanchez Ramos\",\n        \"Excess capacity and backdoor poisoning\",\n        \"Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database\",\n        \"Bhavya Kailkhura\",\n        \"secml: A Python Library for Secure and Explainable ML\",\n        \"Vasin Wongrassamee\",\n        \"Misleading learners: Co-opting your spam filter\",\n        \"Fuching Jack Chi\",\n        \"Tuan Anh Nguyen and Anh Tran\",\n        \"Tuan Anh Nguyen and Anh Tuan Tran\",\n        \"Valentina Zantedeschi\",\n        \"Transferability in ML: from phenomena to black-box attacks using adversarial samples\",\n        \"Luis Muñoz-González\",\n        \"Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples\",\n        \"ONION: A Simple and Effective Defense Against Textual Backdoor Attacks\",\n        \"Pradeep Ravikumar\",\n        \"Anthony D Joseph\",\n        \"Akshayvarun Subramanya\",\n        \"Dynamic backdoor attacks against ML models\",\n        \"Backdoor suppression in neural networks using input fuzzing and majority voting\",\n        \"Hadjer Benkraouda\",\n        \"John P Dickerson\",\n        \"Christoph Studer\",\n        \"Arjun Nitin Bhagoji\",\n        \"Backdoor Scanning for Deep Neural Networks through K-Arm Optimization\",\n        \"Robin Sommer and Vern Paxson\",\n        \"Exposing backdoors in robust ML models\",\n        \"Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch\",\n        \"Adversarial attack and defense on graph data: A survey\",\n        \"Can you really backdoor federated learning?\",\n        \"On defending against label flipping attacks on malware detection system\",\n        \"Demon in the Variant: Statistical Analysis of {{\\\\{DNNs}}\\\\} for Robust Backdoor Contamination Detection\",\n        \"A Comprehensive Survey on Poisoning Attacks and Countermeasures in Machine Learning\",\n        \"Mehmet Emre Gursoy\",\n        \"Antonio Torralba and Alexei A Efros\",\n        \"Label-consistent backdoor attacks\",\n        \"Model agnostic defence against backdoor attacks in ML\",\n        \"Prashanth Krishnamurthy\",\n        \"Confoc: Content-focus protection against trojan attacks on neural networks\",\n        \"Alexander J Levine\",\n        \"Rab: Provable robustness against backdoor attacks\",\n        \"With Great Dispersion Comes Greater Resilience: Efficient Poisoning Attacks and Defenses for Linear Regression Models\",\n        \"On the Trade-off between Adversarial and Backdoor Robustness\",\n        \"Josephine Passananti\",\n        \"Dongxian Wu and Yisen Wang\",\n        \"Detection of Backdoors in Trained Classifiers Without Access to the Training Set\",\n        \"Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing\",\n        \"Characterizing attacks on deep reinforcement learning\",\n        \"Support vector machines under adversarial label contamination\",\n        \"Generative Poisoning Attack Method Against Neural Networks\",\n        \"Kota Yoshida and Takeshi Fujino\",\n        \"Thomas P Karnowski\",\n        \"DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation\",\n        \"TAD: Trigger Approximation based Black-box Trojan Detection for AI\",\n        \"Trojaning Language Models for Fun and Profit\",\n        \"Karthikeyan Natesan Ramamurthy\",\n        \"Anna Cinzia Squicciarini\",\n        \"Daniel Zügner and Stephan Günnemann\"\n    ],\n    \"reasoning\": \"Selected these 150 references because they represent foundational work and recent advances in Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}