{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 127 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Gaze-based intention estimation: principles, methodologies, and applications in HRI\n\nPaper Abstract: Abstract. Intention prediction has become a relevant field of research in Human-Machine and Human-Robot Interaction. Indeed, any artificial system (co)-operating with and along humans, designed to assist and coordinate its actions with a human partner, would benefit from first inferring the human’s current intention. To spare the user the cognitive burden of explicitly uttering their goals, this inference relies mostly on behavioral cues deemed indicative of the current action. It has been long known that eye movements are highly anticipatory of the single steps unfolding during a task, hence they can serve as a very early and reliable behavioural cue for intention recognition. This review aims to draw a line between insights in the psychological literature on visuomotor control and relevant applications of gaze-based intention recognition in technical domains, with a focus on teleoperated and assistive robotic systems. Starting from the cognitive principles underlying the relationship between intentions, eye movements, and action, the use of eye tracking and gaze-based models for intent recognition in Human-Robot Interaction is considered, with prevalent methodologies and their diverse applications. Finally, special consideration is given to relevant human factors issues and current limitations to be factored in when designing such systems.\n\n{retrieved_text}\n\nGenerate exactly 127 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 127\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Motion intention recognition in robot assisted applications\",\n        \"Social eye gaze in human-robot interaction: a review\",\n        \"Predicting user intent through eye gaze for shared autonomy\",\n        \"A Gaze-Based Driver Distraction Warning System and Its Effect on Visual Behavior\",\n        \"Human adaptation to human–robot shared control\",\n        \"Gaze for Error Detection During Human-Robot Shared Manipulation\",\n        \"Semantic gaze labeling for human-robot shared manipulation\",\n        \"Inferring Goals with Gaze during Teleoperated Manipulation\",\n        \"Eye-hand behavior in human-robot shared manipulation\",\n        \"Multimodal Integration of Natural Gaze Behavior for Intention Recognition during Object Manipulation\",\n        \"Modeling human plan recognition using Bayesian theory of mind\",\n        \"Attentional landscapes in reaching and grasping\",\n        \"Toward a framework for levels of robot autonomy in human-robot interaction\",\n        \"Visual search is modulated by action intentions\",\n        \"Anticipatory eye fixations reveal tool knowledge for tool interaction\",\n        \"Goal-oriented gaze strategies afforded by object interaction\",\n        \"Intention estimation from gaze and motion features for human-robot shared-control object manipulation\",\n        \"It’s in the eyes: Planning precise manual actions before execution\",\n        \"Advanced statistical methods for eye movement analysis and modelling: a gentle introduction\",\n        \"Predicting task from eye movements: On the importance of spatial distribution, dynamics, and image features\",\n        \"Defending Yarbus: Eye movements reveal observers’ task\",\n        \"Intending and acting: Toward a naturalized action theory\",\n        \"Intention, plans, and practical reason. Vol. 10\",\n        \"Differences in fixations between grasping and viewing objects\",\n        \"Features of planned hand actions influence identification of graspable objects\",\n        \"Eye movement analysis for activity recognition using electrooculography\",\n        \"Cognition-aware computing\",\n        \"Why do the eyes prefer the index finger? Simultaneous recording of eye and hand movements during precision grasping\",\n        \"Scanpath modeling and classification with hidden Markov models\",\n        \"The integration of top-down and bottom-up factors in visual search during driving\",\n        \"Towards gaze-based prediction of the intent to interact in virtual reality\",\n        \"Essays on Actions and Events: Philosophical Essays Volume 1. Vol. 1\",\n        \"The intentional stance\",\n        \"Multi-modal intention prediction with probabilistic movement primitives\",\n        \"Selective dorsal and ventral processing: Evidence for a common attentional mechanism in reaching and perception\",\n        \"On the Roles of Eye Gaze and Head Dynamics in Predicting Driver’s Intent to Change Lanes\",\n        \"A policy-blending formalism for shared control\",\n        \"Action anticipation: Reading the intentions of humans and robots\",\n        \"Human eye-head co-ordination in natural exploration\",\n        \"The eyes have it: the neuroethology, function and evolution of social gaze\",\n        \"Learning to recognize daily actions using gaze\",\n        \"Prediction in goal-directed action\",\n        \"Active inference and epistemic value\",\n        \"Gaze-Based Intention Estimation for Shared Autonomy in Pick-and-Place Tasks\",\n        \"Progressive co-adaptation in human-machine interaction\",\n        \"A Theory of Human Action\",\n        \"Perceptually-guided deep neural networks for ego-action prediction: Object grasping\",\n        \"Teleoperation and beyond for assistive humanoid robots\",\n        \"Human-in-the-loop optimization of shared autonomy in assistive robotics\",\n        \"Robust online time series prediction with recurrent neural networks\",\n        \"An inverse Yarbus process: Predicting observers’ task from eye movement patterns\",\n        \"Exploiting three-dimensional gaze tracking for action recognition during bimanual manipulation to enhance human–robot collaboration\",\n        \"Recognition, prediction, and planning for assisted teleoperation of freeform tasks\",\n        \"Modeling task control of eye movements\",\n        \"Vision and action\",\n        \"Visual memory and motor planning in a natural task\",\n        \"Gaze control as prediction\",\n        \"Predicting cognitive state from eye movements\",\n        \"Eye tracking: A comprehensive guide to methods and measures\",\n        \"Using gaze patterns to predict task intent in collaboration\",\n        \"Chien-Ming Huang and Bilge Mutlu\",\n        \"Bayesian surprise attracts human attention\",\n        \"The use of eye movements in human-computer interaction techniques: what you look at is what you get\",\n        \"Siddarth Jain and Brenna Argall\",\n        \"Probabilistic Human Intent Recognition for Shared Autonomy in Assistive Robotics\",\n        \"Shared autonomy via hindsight optimization for teleoperation and teaming\",\n        \"Siddhartha S Srinivasa\",\n        \"Eye–hand coordination in object manipulation\",\n        \"Anete Aumeistere\",\n        \"Recent Trends in Driver Safety Monitoring Systems: State of the Art and Challenges\",\n        \"Human action recognition and prediction: A survey\",\n        \"Behavioral Research and Practical Models of Drivers’ Attention\",\n        \"Mathieu Marchand\",\n        \"Vision, eye movements, and natural behavior\",\n        \"The roles of vision and eye movements in the control of activities of daily living\",\n        \"Looking and acting: vision and eye movements in natural behaviour\",\n        \"Eye movements and the control of actions in everyday life\",\n        \"Where we look when we steer\",\n        \"A comparison of selected simple supervised learning algorithms to predict driver intent based on gaze data\",\n        \"Inference of Manipulation Intent in Teleoperation for Robotic Assistance\",\n        \"3-D-gaze-based robotic grasping through mimicking human visuomotor function for people with motion impairments\",\n        \"Christian Ruhhammer\",\n        \"3D Gaze Estimation for Head-Mounted Eye Tracking System With Auto-Calibration Method\",\n        \"A review of intent detection, arbitration, and communication aspects of shared control for physical human–robot interaction\",\n        \"Driving around bends with manipulated eye-steering coordination\",\n        \"Driver behavior and situation aware brake assistance for intelligent vehicles\",\n        \"Springs of Action: Understanding Intentional Behavior\",\n        \"The cerebellum and the timing of coordinated eye and hand tracking\",\n        \"Kristy Elizabeth Boyer\",\n        \"Recognition of activities of daily living with egocentric vision: A review\",\n        \"Learning Arbitration for Shared Autonomy by Hindsight Data Aggregation\",\n        \"The phenomenology of action: A conceptual framework\",\n        \"Oculomotor behavior and perceptual strategies in complex tasks\",\n        \"Multi-parameter prediction of drivers’ lane-changing behaviour with neural network model\",\n        \"Shared control–based bimanual robot manipulation\",\n        \"Gaze and motion information fusion for human intention inference\",\n        \"Yosef Razin and Karen Feigh\",\n        \"Bimanual aiming and overt attention: one law for two hands\",\n        \"Reorienting attention across the horizontal and vertical meridians: evidence in favor of a premotor theory of attention\",\n        \"Eye Movements in Vehicle Control\",\n        \"Eye–hand coordination during learning of a novel visuomotor task\",\n        \"Kentaro Kawamura\",\n        \"Dario D Salvucci and Joseph H Goldberg\",\n        \"The time course of a lane change: Driver control and eye-movement behavior\",\n        \"Action-effect binding and agency\",\n        \"Intentionality: An Essay in the Philosophy of Mind\",\n        \"Combining gaze and AI planning for online human intention recognition\",\n        \"Eye–hand coordination of symmetric bimanual reaching tasks: Temporal aspects\",\n        \"Combining low and mid-level gaze features for desktop activity recognition\",\n        \"Mind the step: complementary effects of an implicit task on eye and head movements in real-life gaze allocation\",\n        \"Wolfgang Rosenstiel\",\n        \"Intelligent human-machine interaction based on dynamic Bayesian networks probabilistic intention recognition\",\n        \"A system for three-dimensional gaze fixation analysis using eye tracking glasses\",\n        \"Ajay Kumar Tanwani and Sylvain Calinon\",\n        \"Understanding and sharing intentions: The origins of cultural cognition\",\n        \"A possible reason for why data-driven beats theory-driven computer vision\",\n        \"Defining eye-fixation sequences across individuals and tasks: the Binocular-Individual Threshold (BIT) algorithm\",\n        \"The role of intention in cognitive robotics\",\n        \"Toward shared autonomy control schemes for human-robot systems: Action primitive recognition using eye gaze features\",\n        \"Gaze3DFix: Detecting 3D fixations with an ellipsoidal bounding volume\",\n        \"Who is the Controller of Controlled Processes?\",\n        \"What does the driver look at? The influence of intersection characteristics on attention allocation and driving behavior\",\n        \"Driver Lane Change Intention Inference for Intelligent Vehicles: Framework, Survey, and Challenges\",\n        \"Eye movements during perception of complex objects\",\n        \"Recognizing behavior in hand-eye coordination patterns\",\n        \"Using machine learning to detect events in eye-tracking data\",\n        \"Semi-Autonomous Robotic Arm Reaching With Hybrid Gaze–Brain Machine Interface\"\n    ],\n    \"reasoning\": \"Selected these 127 references because they represent foundational work and recent advances in Gaze-based intention estimation: principles, methodologies, and applications in HRI, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}