{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 60 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Modeling Nonlinear Dynamics in Continuous Time with Inductive Biases on Decay Rates and/or Frequencies\n\nPaper Abstract: Abstract We propose a neural network-based model for nonlinear dynamics in continuous time that can impose inductive biases on decay rates and/or frequencies. Inductive biases are helpful for training neural networks especially when training data are small. The proposed model is based on the Koopman operator theory, where the decay rate and frequency information is used by restricting the eigenvalues of the Koopman operator that describe linear evolution in a Koopman space. We use neural networks to find an appropriate Koopman space, which are trained by minimizing multi-step forecasting and backcasting errors using irregularly sampled time-series data. Experiments on various time-series datasets demonstrate that the proposed method achieves higher forecasting performance given a single short training sequence than the existing methods.\n\n{retrieved_text}\n\nGenerate exactly 60 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 60\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems\",\n        \"Forecasting sequential data using consistent Koopman autoencoders\",\n        \"Applied koopmanism\",\n        \"Neural lyapunov control\",\n        \"Numerical differentiation of noisy, nonsmooth data\",\n        \"Recurrent neural networks for multivariate time series with missing values\",\n        \"Neural ordinary differential equations\",\n        \"Learning neural event functions for ordinary differential equations\",\n        \"Weak form generalized hamiltonian learning\",\n        \"Periodic autoregressive moving average methods based on fourier representation of periodic coefficients\",\n        \"Learning interpretable continuous-time models of latent stochastic dynamical systems\",\n        \"One-parameter semigroups for linear evolution equations, volume 194\",\n        \"Using fourier series to estimate periodic patterns in dynamic occupancy models\",\n        \"Hamiltonian neural networks\",\n        \"DeSKO: Stability-assured robust control with a deep stochastic koopman operator\",\n        \"Long short-term memory\",\n        \"Improvements of the daily optimum interpolation sea surface temperature (doisst) version 2.1\",\n        \"Neural dynamic mode decomposition for end-to-end modeling of nonlinear dynamics\",\n        \"Non-linear dynamics of biological systems\",\n        \"Physics-informed machine learning\",\n        \"Learning stable nonlinear dynamical systems with gaussian mixture models\",\n        \"Adam: A method for stochastic optimization\",\n        \"Hamiltonian systems and transformation in Hilbert space\",\n        \"Dynamic mode decomposition: data-driven modeling of complex systems\",\n        \"Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders\",\n        \"Learning compositional koopman operators for model-based control\",\n        \"Deep learning for universal linear embeddings of nonlinear dynamics\",\n        \"Learning stable deep dynamics models\",\n        \"Stable neural flows\",\n        \"Symplectic adjoint method for exact gradient of neural ODE with minimal memory\",\n        \"Koopman-based lifting techniques for nonlinear systems identification\",\n        \"Spectral properties of dynamical systems, model reduction and decompositions\",\n        \"Construction of eigenfunctions for scalar-type operators via laplace averages with connections to the koopman operator\",\n        \"Neural learning of stable dynamical systems based on data-driven lyapunov candidates\",\n        \"Koopman spectrum nonlinear regulator and provably efficient online learning\",\n        \"Automatic differentiation in PyTorch\",\n        \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\",\n        \"Daily high-resolution-blended analyses for sea surface temperature\",\n        \"Spectral analysis of nonlinear flows\",\n        \"Latent ODEs for irregularly-sampled time series\",\n        \"Long expressive memory for sequence modeling\",\n        \"Energy balance in feedback synchronization of chaotic systems\",\n        \"Sparse model selection via integral terms\",\n        \"Dynamic mode decomposition of numerical and experimental data\",\n        \"Monotonic networks\",\n        \"A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting\",\n        \"Physics-integrated variational autoencoders for robust and interpretable generative modeling\",\n        \"Learning dynamics models with stable invariant sets\",\n        \"Learning Koopman invariant subspaces for dynamic mode decomposition\",\n        \"Forecasting at scale\",\n        \"Hamiltonian generative networks\",\n        \"On dynamic mode decomposition: Theory and applications\",\n        \"Learning stable stochastic nonlinear dynamical systems\",\n        \"Frequency demultiplication\",\n        \"Maximum likelihood estimation for periodic autoregressive moving average models\",\n        \"Unconstrained monotonic neural networks\",\n        \"A dataâ€“driven approximation of the Koopman operator: Extending dynamic mode decomposition\",\n        \"Short-term demand forecasting for bike sharing system based on machine learning\",\n        \"Learning deep neural network representations for Koopman operators of nonlinear dynamical systems\",\n        \"Periodic time series data analysis by deep learning methodology\"\n    ],\n    \"reasoning\": \"Selected these 60 references because they represent foundational work and recent advances in Modeling Nonlinear Dynamics in Continuous Time with Inductive Biases on Decay Rates and/or Frequencies, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}