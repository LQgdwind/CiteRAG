{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Online Subset Selection using ùõº-Core with no Augmented Regret\n\nPaper Abstract: Abstract We revisit the classic problem of optimal subset selection in the online learning set-up. Assume that the set [N]delimited-[]ùëÅ[N] consists of NùëÅN distinct elements. On the tùë°tth round, an adversary chooses a monotone reward function ft:2[N]‚Üí‚Ñù+:subscriptùëìùë°‚Üísuperscript2delimited-[]ùëÅsubscript‚Ñùf_{t}:2^{[N]}\\to\\mathbb{R}_{+} that assigns a non-negative reward to each subset of [N].delimited-[]ùëÅ[N]. An online policy selects (perhaps randomly) a subset St‚äÜ[N]subscriptùëÜùë°delimited-[]ùëÅS_{t}\\subseteq[N] consisting of kùëòk elements before the reward function ftsubscriptùëìùë°f_{t} for the tùë°tth round is revealed to the learner. As a consequence of its choice, the policy receives a reward of ft‚Äã(St)subscriptùëìùë°subscriptùëÜùë°f_{t}(S_{t}) on the tùë°tth round. Our goal is to design an online sequential subset selection policy to maximize the expected cumulative reward accumulated over a time horizon. In this connection, we propose an online learning policy called SCore (Subset Selection with Core) that solves the problem for a large class of reward functions. The proposed SCore policy is based on a new polyhedral characterization of the reward functions called Œ±ùõº\\alpha-Core - a generalization of Core from the cooperative game theory literature. We establish a learning guarantee for the SCore policy in terms of a new performance metric called Œ±ùõº\\alpha-augmented regret. In this new metric, the performance of the online policy is compared with an unrestricted offline benchmark that can select all NùëÅN elements at every round. We show that a large class of reward functions, including submodular, can be efficiently optimized with the SCore policy. We also extend the proposed policy to the optimistic learning set-up where the learner has access to additional untrusted hints regarding the reward functions. Finally, we conclude the paper with a list of open problems.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Greedy column subset selection: New bounds and distributed algorithms\",\n        \"Online learning with imperfect hints\",\n        \"Fundamental limits on the regret of online network-caching\",\n        \"Guarantees for greedy maximization of non-submodular functions with applications\",\n        \"Submodularity in machine learning and artificial intelligence\",\n        \"Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation\",\n        \"Robust maximization of non-submodular objectives\",\n        \"Coresets via bilevel optimization for continual learning and streaming\",\n        \"An improved approximation algorithm for the column subset selection problem\",\n        \"Greedy minimization of weakly supermodular set functions\",\n        \"Prediction, learning, and games\",\n        \"Greedy dictionary selection for sparse representation\",\n        \"Complexity of determining nonemptiness of the core\",\n        \"Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection\",\n        \"Online learning with a hint\",\n        \"Optimal approximation for unconstrained non-submodular minimization\",\n        \"Combinatorial penalties: Which structures are preserved by convex relaxations?\",\n        \"Online sparse linear regression\",\n        \"Online learning of assignments that maximize submodular functions\",\n        \"Online submodular maximization under a matroid constraint with application to learning assignments\",\n        \"Near-optimal sensor placements in gaussian processes\",\n        \"Improved algorithms for online submodular maximization via first-order regret bounds\",\n        \"Introduction to online convex optimization\",\n        \"Online learning with prior knowledge\",\n        \"Efficient learning algorithms for changing environments\",\n        \"The shapley value and average convex games\",\n        \"The coalitional rationality of the shapley value\",\n        \"Adaptive feature selection: Computationally efficient online sparse linear regression under rip\",\n        \"Maximizing the spread of influence through a social network\",\n        \"Scalable greedy feature selection via weak submodularity\",\n        \"The hungarian method for the assignment problem\",\n        \"Fast maximization of non-submodular, monotonic functions on the integer lattice\",\n        \"On the global linear convergence of frank-wolfe optimization variants\",\n        \"On the theory of systematic sampling, ii\",\n        \"Geometric properties of the kernel, nucleolus, and related solution concepts\",\n        \"Game theory\",\n        \"Tracking regret bounds for online submodular optimization\",\n        \"Optimistic no-regret algorithms for discrete caching\",\n        \"Sampling permutations for shapley value estimation\",\n        \"Accelerating online convex optimization via adaptive prediction\",\n        \"Online optimization in dynamic environments: Improved regret rates for strongly convex problems\",\n        \"k-experts-online policies and fundamental limits\",\n        \"Approximate core for committee selection via multilinear extension and market clearing\",\n        \"Maximize a monotone function with a generic submodularity ratio\",\n        \"Leadcache: Regret-optimal caching in networks\",\n        \"Learning to cache with no regrets\",\n        \"Optimization, learning, and games with predictable sequences\",\n        \"The graph of the hypersimplex\",\n        \"An optimal learning algorithm for online unconstrained submodular maximization\",\n        \"Greedy and iht algorithms for non-convex optimization with monotone costs of non-zeros\",\n        \"Weakly submodular function maximization using local submodularity ratio\",\n        \"The nucleolus of a characteristic function game\",\n        \"Theory of linear and integer programming\",\n        \"Cores of convex games\",\n        \"The assignment game i: The core\",\n        \"Amortized efficiency of list update and paging rules\",\n        \"Population monotonic allocation schemes for cooperative games with transferable utility\",\n        \"An online algorithm for maximizing submodular functions\",\n        \"Two-sided weak submodularity for matroid constrained optimization and regression\",\n        \"The general graph matching game: Approximate core\",\n        \"Projection onto the capped simplex\",\n        \"Online convex programming and generalized infinitesimal gradient ascent\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in Online Subset Selection using ùõº-Core with no Augmented Regret, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}