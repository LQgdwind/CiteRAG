{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: The Missing Indicator Method: From Low to High Dimensions\n\nPaper Abstract: Abstract. Missing data is common in applied data science, particularly for tabular data sets found in healthcare, social sciences, and natural sciences. Most supervised learning methods only work on complete data, thus requiring preprocessing such as missing value imputation to work on incomplete data sets. However, imputation alone does not encode useful information about the missing values themselves. For data sets with informative missing patterns, the Missing Indicator Method (MIM), which adds indicator variables to indicate the missing pattern, can be used in conjunction with imputation to improve model performance. While commonly used in data science, MIM is surprisingly understudied from an empirical and especially theoretical perspective. In this paper, we show empirically and theoretically that MIM improves performance for informative missing values, and we prove that MIM does not hurt linear models asymptotically for uninformative missing values. Additionally, we find that for high-dimensional data sets with many uninformative indicators, MIM can induce model overfitting and thus test performance. To address this issue, we introduce Selective MIM (SMIM), a novel MIM extension that adds missing indicators only for features that have informative missing patterns. We show empirically that SMIM performs at least as well as MIM in general, and improves MIM for high-dimensional data. Lastly, to demonstrate the utility of MIM on real-world data science tasks, we demonstrate the effectiveness of MIM and SMIM on clinical tasks generated from the MIMIC-III database of electronic health records.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"An introduction to modern missing data analyses\",\n        \"Mapping patient trajectories using longitudinal extraction and deep learning in the MIMIC-III critical care database\",\n        \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\",\n        \"Prediction with Missing Data\",\n        \"Random forests\",\n        \"Classification and regression trees\",\n        \"Xgboost: A scalable tree boosting system\",\n        \"Maximum likelihood from incomplete data via the EM algorithm\",\n        \"An artificial neural networks model for early predicting in-hospital mortality in acute pancreatitis in MIMIC-III\",\n        \"Auto-Sklearn 2.0: Hands-free automl via meta-learning\",\n        \"An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers\",\n        \"Evolving imputation strategies for missing data in classification problems with TPOT\",\n        \"Predicting hospital length of stay using neural networks on mimic iii data\",\n        \"Mida: Multiple imputation using denoising autoencoders\",\n        \"Missing covariate data in clinical research: when and when not to use the missing-indicator method for analysis\",\n        \"Econometrics\",\n        \"Multitask learning and benchmarking with clinical time series data\",\n        \"Amelia II: A program for missing data\",\n        \"How to deal with missing data in supervised deep learning?\",\n        \"MIMIC-III, a freely accessible critical care database\",\n        \"On the consistency of supervised learning with missing values\",\n        \"Unpredictable bias when using the missing indicator method or complete case analysis for missing confounder values: an empirical example\",\n        \"MIRACLE: Causally-Aware Imputation via Learning Missing Data Mechanisms\",\n        \"NeuMiss networks: differentiable programming for supervised learning with missing values\",\n        \"What’s a good imputation to predict with missing values?\",\n        \"Linear predictor on linearly-generated data with missing values: non consistency and solutions\",\n        \"Don’t do imputation: Dealing with informative missing values in EHR data analysis\",\n        \"Missing value imputation: a review and analysis of the literature (2006–2017)\",\n        \"A test of missing completely at random for multivariate data with missing values\",\n        \"Statistical analysis with missing data. Vol. 793\",\n        \"MIWAE: Deep generative modelling and imputation of incomplete data sets\",\n        \"Natural language processing of MIMIC-III clinical notes for identifying diagnosis and procedures with neural networks\",\n        \"Scikit-learn: Machine Learning in Python\",\n        \"Benchmarking missing-values approaches for predictive models on health databases\",\n        \"Benchmarking deep learning models on large healthcare datasets\",\n        \"Propensity score estimation with missing values using a multiple imputation missingness pattern (MIMP) approach\",\n        \"C4. 5: programs for machine learning\",\n        \"Inference and missing data\",\n        \"Missing data: our view of the state of the art\",\n        \"Predicting sepsis with a recurrent neural network using the MIMIC III database\",\n        \"A new insight into missing data in intensive care unit patient profiles: observational study\",\n        \"Multiple imputation with missing indicators as proxies for unmeasured variables: simulation study\",\n        \"Missing data should be handled differently for prediction than for description or causal explanation\",\n        \"Estimation and imputation in probabilistic principal component analysis with missing not at random data\",\n        \"MissForest—non-parametric missing value imputation for mixed-type data\",\n        \"Democratizing EHR analyses with FIDDLE: a flexible data-driven preprocessing pipeline for structured clinical data\",\n        \"Good methods for coping with missing data in decision trees\",\n        \"Flexible imputation of missing data\",\n        \"mice: Multivariate imputation by chained equations in R\",\n        \"OpenML: networked science in machine learning\",\n        \"Mimic-extract: A data extraction, preprocessing, and representation pipeline for mimic-iii\",\n        \"Does imputation matter? Benchmark for predictive models\",\n        \"Automl pipeline selection: Efficiently navigating the combinatorial space\",\n        \"Gain: Missing data imputation using generative adversarial nets\",\n        \"Matrix completion with quantified uncertainty through low rank gaussian copula\",\n        \"Missing value imputation for mixed data via gaussian copula\",\n        \"gcimpute: A Package for Missing Data Imputation\",\n        \"Machine learning prediction models for mechanically ventilated patients: analyses of the MIMIC-III database\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in The Missing Indicator Method: From Low to High Dimensions, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}