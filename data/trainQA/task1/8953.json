{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 192 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets\n\nPaper Abstract: Abstract The development of large language models (LLMs) such as ChatGPT111https://chat.openai.com/ has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT’s performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT’s performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.\n\n{retrieved_text}\n\nGenerate exactly 192 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 192\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Towards a human-like open-domain chatbot\",\n        \"Mega: Multilingual evaluation of generative ai\",\n        \"Can we trust the evaluation on chatgpt?\",\n        \"Publicly available clinical bert embeddings\",\n        \"Program synthesis with large language models\",\n        \"Refined: An efficient zero-shot-capable approach to end-to-end entity linking\",\n        \"Promptsource: An integrated development environment and repository for natural language prompts\",\n        \"Chatgpt: Applications, opportunities, and threats\",\n        \"A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\",\n        \"The second PASCAL recognising textual entailment challenge\",\n        \"Findings of the 2019 conference on machine translation (wmt19)\",\n        \"Ask and you shall receive (a graph drawing): Testing chatgpt’s potential to apply graph layout algorithms\",\n        \"Better by you, better than me, chatgpt3 as writing assistance in students essays\",\n        \"Scibert: A pretrained language model for scientific text\",\n        \"The fifth PASCAL recognizing textual entailment challenge\",\n        \"Semantic parsing on Freebase from question-answer pairs\",\n        \"Piqa: Reasoning about physical commonsense in natural language\",\n        \"Gpt-neox-20b: An open-source autoregressive language model\",\n        \"Findings of the 2014 workshop on statistical machine translation\",\n        \"Findings of the 2016 conference on machine translation\",\n        \"A categorical archive of chatgpt failures\",\n        \"Language models are few-shot learners\",\n        \"Does chatgpt resemble humans in language use?\",\n        \"Evaluating large language models trained on code\",\n        \"Dialogsum: A real-life scenario dialogue summarization dataset\",\n        \"Evaluation of chatgpt model for vulnerability detection\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Scaling instruction-finetuned language models\",\n        \"Boolq: Exploring the surprising difficulty of natural yes/no questions\",\n        \"Electra: Pre-training text encoders as discriminators rather than generators\",\n        \"Think you have solved question answering? try arc, the ai2 reasoning challenge\",\n        \"Training verifiers to solve math word problems\",\n        \"The PASCAL recognising textual entailment challenge\",\n        \"Auggpt: Leveraging chatgpt for text data augmentation\",\n        \"The CommitmentBank: Investigating projection in naturally occurring discourse\",\n        \"Results of the WNUT2017 shared task on novel and emerging entity recognition\",\n        \"Analyzing chatgpt’s aptitude in an introductory computer engineering course\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"A survey for in-context learning\",\n        \"Glam: Efficient scaling of language models with mixture-of-experts\",\n        \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\",\n        \"What does chatgpt return about human values? exploring value bias in chatgpt using a descriptive value theory\",\n        \"Mathematical capabilities of chatgpt\",\n        \"An effective, performant named entity recognition system for noisy business telephone conversation transcripts\",\n        \"The Pile: An 800gb dataset of diverse text for language modeling\",\n        \"Human-like summarization evaluation with chatgpt\",\n        \"Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection\",\n        \"The third PASCAL recognizing textual entailment challenge\",\n        \"Samsum corpus: A human-annotated dialogue dataset for abstractive summarization\",\n        \"Assessing working memory capacity of chatgpt\",\n        \"Palm 2 technical report\",\n        \"News summarization and evaluation in the era of gpt-3\",\n        \"Domain-specific language model pretraining for biomedical natural language processing\",\n        \"Semantic communications with ordered importance using chatgpt\",\n        \"The political ideology of conversational ai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation\",\n        \"Unifying human and statistical evaluation for natural language generation\",\n        \"A survey on recent approaches for natural language processing in low-resource scenarios\",\n        \"Aligning ai with shared human values\",\n        \"Measuring massive multitask language understanding\",\n        \"Measuring mathematical problem solving with the math dataset\",\n        \"How good are gpt models at machine translation? a comprehensive evaluation\",\n        \"Teaching machines to read and comprehend\",\n        \"Training compute-optimal large language models\",\n        \"Lora: Low-rank adaptation of large language models\",\n        \"Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation\",\n        \"Zero-shot clinical entity recognition using chatgpt\",\n        \"Is ChatGPT better than human annotators? potential and limitations of ChatGPT in explaining implicit hate speech\",\n        \"Evaluation of chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers\",\n        \"Consistency analysis of chatgpt\",\n        \"Is chatgpt a good translator? yes with gpt-4 as the engine\",\n        \"Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension\",\n        \"Chart-to-text: A large-scale benchmark for chart summarization\",\n        \"Evaluating gpt-4 and chatgpt on japanese medical licensing examinations\",\n        \"Chatgpt for programming numerical methods\",\n        \"Looking beyond the surface: A challenge set for reading comprehension over multiple sentences\",\n        \"Mind the gap! injecting commonsense knowledge for abstractive dialogue summarization\",\n        \"Chatgpt: Jack of all trades, master of none\",\n        \"The moral authority of chatgpt\",\n        \"Natural questions: a benchmark for question answering research\",\n        \"Race: Large-scale reading comprehension dataset from examinations\",\n        \"Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning\",\n        \"Albert: A lite bert for self-supervised learning of language representations\",\n        \"Improving named entity recognition in telephone conversations via effective active learning with human in the loop\",\n        \"An auto encoder-based dimensionality reduction technique for efficient entity linking in business phone conversations\",\n        \"BLINK with Elasticsearch for efficient entity linking in business conversations\",\n        \"Domain adaptation with pre-trained transformers for query-focused abstractive text summarization\",\n        \"WSL-DS: Weakly supervised learning with distant supervision for query focused multi-document abstractive summarization\",\n        \"Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task\",\n        \"Cqsumdp: A chatgpt-annotated resource for query-focused abstractive summarization based on debatepedia\",\n        \"Biobert: a pre-trained biomedical language representation model for biomedical text mining\",\n        \"The power of scale for parameter-efficient prompt tuning\",\n        \"The Winograd schema challenge\",\n        \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Solving quantitative reasoning problems with language models\",\n        \"Evaluating chatgpt’s information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness\",\n        \"Multi-step jailbreaking privacy attacks on chatgpt\",\n        \"hot\\\" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media\",\n        \"Prefix-tuning: Optimizing continuous prompts for generation\",\n        \"Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation\",\n        \"Differentiate chatgpt-generated and human-written medical texts\",\n        \"Rouge: A package for automatic evaluation of summaries\",\n        \"Truthfulqa: Measuring how models mimic human falsehoods\",\n        \"A comprehensive evaluation of chatgpt’s zero-shot text-to-sql capability\",\n        \"How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation\",\n        \"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\",\n        \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation\",\n        \"Towards an automatic turing test: Learning to evaluate dialogue responses\",\n        \"Codexglue: A machine learning benchmark dataset for code understanding and generation\",\n        \"Biogpt: generative pre-trained transformer for biomedical text generation and mining\",\n        \"Learning word vectors for sentiment analysis\",\n        \"Semeval-2022 task 11: Multilingual complex named entity recognition (multiconer)\",\n        \"Is chatgpt really a “code red” for google search?\",\n        \"Can a suit of armor conduct electricity? a new dataset for open book question answering\",\n        \"Neurips 2020 efficientqa competition: Systems, analyses and lessons learned\",\n        \"Cross-task generalization via natural language crowdsourcing instructions\",\n        \"Davide Andreoletti\",\n        \"Crosslingual generalization through multitask finetuning\",\n        \"Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization\",\n        \"Adversarial NLI: A new benchmark for natural language understanding\",\n        \"Industrial engineering with large language models: A case study of chatgpt’s performance on oil & gas problems\",\n        \"Gpt-4 technical report\",\n        \"Chatgpt: Optimizing language models for dialogue\",\n        \"Linguistic ambiguity analysis in chatgpt\",\n        \"Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Bleu: a method for automatic evaluation of machine translation\",\n        \"Using chatgpt for entity matching\",\n        \"To chatgpt, or not to chatgpt: That is the question!\",\n        \"Towards making the most of chatgpt for machine translation\",\n        \"Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets\",\n        \"Inverse scaling prize: Round 1 winners\",\n        \"AdapterFusion: Non-destructive task composition for transfer learning\",\n        \"WiC: The word-in-context dataset for evaluating context-sensitive meaning representations\",\n        \"Collecting diverse natural language inference problems for sentence representation evaluation\",\n        \"Is chatgpt a general-purpose natural language processing task solver?\",\n        \"Language models are unsupervised multitask learners\",\n        \"Scaling language models: Methods, analysis & insights from training gopher\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Chartsumm: A comprehensive benchmark for automatic chart summarization of long and short summaries\",\n        \"Know what you don’t know: Unanswerable questions for squad\",\n        \"Chatgpt-crawler: Find out if chatgpt really knows what it’s talking about\",\n        \"Can chatgpt assess human personalities? a general evaluation framework\",\n        \"Summareranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization\",\n        \"Choice of plausible alternatives: An evaluation of commonsense causal reasoning\",\n        \"Generating phishing attacks using chatgpt\",\n        \"Gender bias in coreference resolution\",\n        \"Winogrande: An adversarial winograd schema challenge at scale\",\n        \"The moral side of murder\",\n        \"Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter\",\n        \"Multitask prompted training enables zero-shot task generalization\",\n        \"Social iqa: Commonsense reasoning about social interactions\",\n        \"Bloom: A 176b-parameter open-access multilingual language model\",\n        \"Get to the point: Summarization with pointer-generator networks\",\n        \"Language models are multilingual chain-of-thought reasoners\",\n        \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\",\n        \"TaCL: Improving BERT pre-training with token-aware contrastive learning\",\n        \"Black-box tuning for language-model-as-a-service\",\n        \"Is chatgpt good at search? investigating large language models as re-ranking agent\",\n        \"Chatgpt: The end of online exam integrity?\",\n        \"Challenging big-bench tasks and whether chain-of-thought can solve them\",\n        \"Chatgpt4pcg competition: Character-like level generation for science birds\",\n        \"Unifying language learning paradigms\",\n        \"The Trolley Problem/Das Trolley-Problem (Englisch/Deutsch): Reclam Great Papers Philosophie\",\n        \"Lamda: Language models for dialog applications\",\n        \"Introduction to the conll-2003 shared task: language-independent named entity recognition\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Chatlog: Recording and analyzing chatgpt across time\",\n        \"Attention is all you need\",\n        \"Superglue: A stickier benchmark for general-purpose language understanding systems\",\n        \"GLUE: A multi-task benchmark and analysis platform for natural language understanding\",\n        \"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX\",\n        \"Is chatgpt a good nlg evaluator? a preliminary study\",\n        \"Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks\",\n        \"Is chatgpt a good sentiment analyzer? a preliminary study\",\n        \"Finetuned language models are zero-shot learners\",\n        \"Emergent abilities of large language models\",\n        \"Inverse scaling can become u-shaped\",\n        \"Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark\",\n        \"Scalable zero-shot entity linking with dense entity retrieval\",\n        \"Zero-shot temporal relation extraction with chatgpt\",\n        \"Hellaswag: Can a machine really finish your sentence?\",\n        \"Glm-130b: An open bilingual pre-trained model\",\n        \"How would stance detection techniques evolve after the launch of chatgpt?\",\n        \"ReCoRD: Bridging the gap between human and machine commonsense reading comprehension\",\n        \"Dialogpt: Large-scale generative pre-training for conversational response generation\",\n        \"Gender bias in coreference resolution: Evaluation and debiasing methods\",\n        \"Is chatgpt equipped with emotional dialogue capabilities?\",\n        \"QMSum: A new benchmark for query-based multi-domain meeting summarization\",\n        \"Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert\",\n        \"A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\"\n    ],\n    \"reasoning\": \"Selected these 192 references because they represent foundational work and recent advances in A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}