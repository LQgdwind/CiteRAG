{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning\n\nPaper Abstract: Abstract The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients‚Äô models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this work, we first expose vulnerabilities of the CC framework, and introduce a novel attack strategy that can circumvent the defences of CC and other robust aggregators and reduce their test accuracy up to %33 on best-case scenarios in image classification tasks. Then, we propose a new robust and fast defence mechanism that is effective against the proposed and other existing Byzantine attacks.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity\",\n        \"How to backdoor federated learning\",\n        \"A little is enough: Circumventing defenses for distributed learning\",\n        \"signsgd with majority vote is communication efficient and fault tolerant\",\n        \"Analyzing federated learning through an adversarial lens\",\n        \"Poisoning attacks against support vector machines\",\n        \"Machine learning with adversaries: Byzantine tolerant gradient descent\",\n        \"Intriguing perties of neural networks\",\n        \"Fltrust: Byzantine-robust federated learning via trust bootstrapping\",\n        \"Data poisoning attacks to local differential privacy protocols\",\n        \"Towards evaluating the robustness of neural networks\",\n        \"Towards taming the resource and data heterogeneity in federated learning\",\n        \"Targeted backdoor attacks on deep learning systems using data poisoning\",\n        \"Distributed statistical machine learning in adversarial settings: Byzantine gradient descent\",\n        \"Distributed momentum for byzantine-resilient stochastic gradient descent\",\n        \"The hidden vulnerability of distributed learning in byzantium\",\n        \"Local model poisoning attacks to byzantine-robust federated learning\",\n        \"Byzantine machine learning made easy by resilient averaging of momentums\",\n        \"The limitations of federated learning in sybil settings\",\n        \"Robust aggregation for adaptive privacy preserving federated learning in healthcare\",\n        \"Federated learning for mobile keyboard prediction\",\n        \"Byzantine-robust decentralized learning via self-centered clipping\",\n        \"Advances and open problems in federated learning\",\n        \"Secure, privacy-preserving and federated machine learning in medical imaging\",\n        \"Learning from history for byzantine robust optimization\",\n        \"Byzantine-robust learning on heterogeneous datasets via bucketing\",\n        \"Cifar-10 (canadian institute for advanced research)\",\n        \"The byzantine generals problem\",\n        \"MNIST handwritten digit database\",\n        \"Abnormal client behavior detection in federated learning\",\n        \"Privacy-preserving federated brain tumour segmentation\",\n        \"Towards deep learning models resistant to adversarial attacks\",\n        \"Dopamine: Differentially private federated learning on medical data\",\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Estimating a dirichlet distribution\",\n        \"A method for solving the convex programming problem with convergence rate o‚Äã(1/k2)ùëú1superscriptùëò2o(1/k^{2})\",\n        \"FLGUARD: secure and private federated learning\",\n        \"Semi-federated learning for collaborative intelligence in massive iot networks\",\n        \"Robust aggregation for federated learning\",\n        \"Some methods of speeding up the convergence of iteration methods\",\n        \"Federated learning for emoji prediction in a mobile keyboard\",\n        \"Can decentralized learning be more robust than federated learning?\",\n        \"Toward communication-learning trade-off for federated learning at the network edge\",\n        \"The future of digital health with federated learning\",\n        \"Hidden trigger backdoor attacks\",\n        \"Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning\",\n        \"Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning\",\n        \"On the importance of initialization and momentum in deep learning\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"Attack of the tails: Yes, you really can backdoor federated learning\",\n        \"SlOWMO: Improving communication-efficient distributed SGD with slow momentum\",\n        \"Escaping saddle points faster with stochastic momentum\",\n        \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\",\n        \"Dba: Distributed backdoor attacks against federated learning\",\n        \"Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulation\",\n        \"Byzantine-robust distributed learning: Towards optimal statistical rates\",\n        \"BatchCrypt: Efficient homomorphic encryption for cross-silo federated learning\",\n        \"Semi-federated learning: Convergence analysis and optimization of a hybrid learning framework\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}