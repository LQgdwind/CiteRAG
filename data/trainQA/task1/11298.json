{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform\n\nPaper Abstract: Abstract Transformer-based foundation models have become crucial for various domains, most notably natural language processing (NLP) or computer vision (CV). These models are predominantly deployed on high-performance GPUs or hardwired accelerators with highly customized, proprietary instruction sets. Until now, limited attention has been given to RISC-V-based general-purpose platforms. In our work, we present the first end-to-end inference results of transformer models on an open-source many-tiny-core RISC-V platform implementing distributed Softmax primitives and leveraging ISA extensions for SIMD floating-point operand streaming and instruction repetition, as well as specialized DMA engines to minimize costly main memory accesses and to tolerate their latency. We focus on two foundational transformer topologies, encoder-only and decoder-only models. For encoder-only models, we demonstrate a speedup of up to 12.8√ó\\times between the most optimized implementation and the baseline version. We reach over 79% FPU utilization and 294 GFLOPS/W, outperforming State-of-the-Art (SoA) accelerators by more than 2√ó\\times utilizing the HW platform while achieving comparable throughput per computational unit. For decoder-only topologies, we achieve 16.1√ó\\times speedup in the Non-Autoregressive (NAR) mode and up to 35.6√ó\\times speedup in the Autoregressive (AR) mode compared to the baseline implementation. Compared to the best SoA dedicated accelerator, we achieve 2.04√ó\\times higher FPU utilization.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A Survey of Large Language Models\",\n        \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n        \"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\",\n        \"Attention is All you Need\",\n        \"On the Opportunities and Risks of Foundation Models\",\n        \"Language Models are Few-Shot Learners\",\n        \"Whisper\",\n        \"SparQ Attention: Bandwidth-Efficient LLM Inference\",\n        \"Transformers are RNNs: Fast autoregressive transformers with linear attention\",\n        \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\",\n        \"NVIDIA A100 Tensor Core GPU: Performance and Innovation\",\n        \"NVIDIA Hopper H100 GPU: Scaling Performance\",\n        \"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster\",\n        \"A Software-Defined Tensor Streaming Multiprocessor for Large-Scale Machine Learning\",\n        \"A High-Performance, Energy-Efficient Modular DMA Engine Architecture\",\n        \"Snitch: A Tiny Pseudo Dual-Issue Processor for Area and Energy Efficient Execution of Floating-Point Intensive Workloads\",\n        \"A Study of BFLOAT16 for Deep Learning Training\",\n        \"FP8 Formats for Deep Learning\",\n        \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n        \"Llama 2: Open Foundation and Fine-Tuned Chat Models\",\n        \"GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale\",\n        \"Visual Transformers: Token-based Image Representation and Processing for Computer Vision\",\n        \"CvT: Introducing Convolutions to Vision Transformers\",\n        \"Efficiently Scaling Transformer Inference\",\n        \"MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases\",\n        \"Luna: Linear Unified Nested Attention\",\n        \"Linformer: Self-Attention with Linear Complexity\",\n        \"Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time\",\n        \"QLoRA: Efficient Finetuning of Quantized LLMs\",\n        \"NVIDIA Data Center Deep Learning Product Performance AI Inference\",\n        \"SambaNova DataScale¬Æ | The AI Platform for Innovation\",\n        \"Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning\",\n        \"Intel Gaudi 2 Neural Network Deep Learning Inference Processor\",\n        \"AMD Instinct‚Ñ¢ MI200 Series Accelerator and Node Architectures\",\n        \"AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference With Transformers\",\n        \"An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention\",\n        \"20.5 C-Transformer: A 2.6-18.1Œºùúá\\\\muJ/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models\",\n        \"FreFlex: A High-Performance Processor for Convolution and Attention Computations via Sparsity-Adaptive Dynamic Frequency Boosting\",\n        \"Ayaka: A Versatile Transformer Accelerator With Low-Rank Estimation and Heterogeneous Dataflow\",\n        \"22.9 A 12nm 18.1TFLOPs/W Sparse Transformer Processor with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power Management\",\n        \"Graphcore\",\n        \"Occamy: A 432-core 28.1 dp-gflop/s/w 83% FPU utilization dual-chiplet, dual-HBM2E RISC-V-based accelerator for stencil and sparse linear algebra computations with 8-to-64-bit floating-point support in 12nm FinFET\",\n        \"MiniFloats on RISC-V Cores: ISA Extensions with Mixed-Precision Short Dot Products\",\n        \"Manticore: A 4096-Core RISC-V Chiplet Architecture for Ultraefficient Floating-Point Computing\",\n        \"HBM2E\",\n        \"I-BERT: Integer-only BERT quantization\",\n        \"Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra\",\n        \"A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators\",\n        \"Benchmarking Large Language Models on NVIDIA H100 GPUs with CoreWeave (Part 1)\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}