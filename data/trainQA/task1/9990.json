{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design\n\nPaper Abstract: Abstract Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DNN training, namely SAT, is developed to neatly support both the regular dense operations and the computation-efficient N:M sparse operations. At the dataflow level, multiple optimization methods ranging from interleave mapping, pre-generation of N:M sparse weights, and offline scheduling, are proposed to boost the computational efficiency of SAT. Finally, the effectiveness of our training scheme is evaluated on a Xilinx VCU1525 FPGA card using various DNN models (ResNet9, ViT, VGG19, ResNet18, and ResNet50) and datasets (CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet). Experimental results show the SAT accelerator with the BDWP sparse training method under 2:8 sparse ratio achieves an average speedup of 1.75×\\times over that with the dense training, accompanied by a negligible accuracy loss of 0.56% on average. Furthermore, our proposed training scheme significantly improves the training throughput by 2.97∼similar-to\\sim25.22×\\times and the energy efficiency by 1.36∼similar-to\\sim3.58×\\times over prior FPGA-based accelerators.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\",\n        \"Language Models Are Few-shot Learners\",\n        \"Accelerating DNN Training with Structured Data Gradient Pruning\",\n        \"XST: A Crossbar Column-wise Sparse Training for Efficient Continual Learning\",\n        \"Approximate Random Dropout for DNN Training Acceleration in GPGPU\",\n        \"Growing Efficient Deep Networks by Structured Continuous Sparsification\",\n        \"StructADMM: Achieving Ultrahigh Efficiency in Structured Pruning for DNNs\",\n        \"Accelerating CNN Training by Pruning Activation Gradients\",\n        \"Rigging the Lottery: Making All Tickets Winners\",\n        \"Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning\",\n        \"SCANN: Synthesis of Compact and Accurate Neural Networks\",\n        \"Quantized Sparse Training: A Unified Trainable Framework for Joint Pruning and Quantization in DNNs\",\n        \"TinyADC: Peripheral Circuit-aware Weight Pruning Framework for Mixed-signal DNN Accelerators\",\n        \"OMNI: A Framework for Integrating Hardware and Software Optimizations for Sparse CNNs\",\n        \"SDP: Co-Designing Algorithm, Dataflow, and Architecture for In-SRAM Sparse NN Acceleration\",\n        \"FSA: A Fine-Grained Systolic Accelerator for Sparse CNNs\",\n        \"Search-Free Inference Acceleration for Sparse Convolutional Neural Networks\",\n        \"Energy-Efficient CNN Personalized Training by Adaptive Data Reformation\",\n        \"A Low-latency Sparse-Winograd Accelerator for Convolutional Neural Networks\",\n        \"NVIDIA A100 Tensor Core GPU Architecture\",\n        \"Accelerating Sparse Deep Neural Networks\",\n        \"Pre-defined Sparsity for Low-Complexity Convolutional Neural Networks\",\n        \"PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning\",\n        \"Unleashing the Potential of Sparse DNNs Through Synergistic Hardware-Sparsity Co-Design\",\n        \"Structured Term Pruning for Computational Efficient Neural Networks Inference\",\n        \"Efficient Layer-Wise N:M Sparse CNN Accelerator with Flexible SPEC: Sparse Processing Element Clusters\",\n        \"An Algorithm–Hardware Co-optimized Framework for Accelerating N:M Sparse Transformers\",\n        \"Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism\",\n        \"1xN Pattern for Pruning Convolutional Neural Networks\",\n        \"SPDY: Accurate Pruning with Speedup Guarantees\",\n        \"DominoSearch: Find layer-wise fine-grained N:M sparse schemes from dense neural networks\",\n        \"Learning N:M Fine-grained Structured Sparse Neural Networks from Scratch\",\n        \"FPGA-based Low-batch Training Accelerator for Modern CNNs Featuring High Bandwidth Memory\",\n        \"EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization\",\n        \"FeCaffe: FPGA-enabled Caffe with OpenCL for Deep Learning Training and Inference on Intel Stratix 10\",\n        \"An FPGA-based Processor for Training Convolutional Neural Networks\",\n        \"Automatic Compiler Based FPGA Accelerator for CNN Training\",\n        \"EILE: Efficient Incremental Learning on the Edge\",\n        \"An On-Chip Fully Connected Neural Network Training Hardware Accelerator Based on Brain Float Point and Sparsity Awareness\",\n        \"Pooling Acceleration in the DaVinci Architecture Using Im2col and Col2im Instructions\",\n        \"PyTorch Profiler\",\n        \"SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional Neural Networks Training\",\n        \"THETA: A High-Efficiency Training Accelerator for DNNs With Triple-Side Sparsity Exploration\",\n        \"CEST: Computation-Efficient N:M Sparse Training for Deep Neural Networks\",\n        \"Trainer: An Energy-Efficient Edge-Device Training Processor Supporting Dynamic Weight Pruning\",\n        \"An FPGA-Based Reconfigurable Accelerator for Low-Bit DNN Training\",\n        \"Towards Efficient Deep Neural Network Training by FPGA-based Batch-level Parallelism\",\n        \"ETA: An Efficient Training Accelerator for DNNs Based on Hardware-algorithm Co-optimization\",\n        \"FPGA-based Training Accelerator Utilizing Sparseness of Convolutional Neural Network\",\n        \"Automatic Mixed Precision\",\n        \"In-Datacenter Performance Analysis of A Tensor Processing Unit\",\n        \"uSystolic: Byte-Crawling Unary Systolic Array\",\n        \"Maestro: A Memory-on-Logic Architecture for Coordinated Parallel Use of Many Systolic Arrays\",\n        \"Deep Residual Learning for Image Recognition\",\n        \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n        \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n        \"Low-bit Quantization of Neural Networks for Efficient Inference\",\n        \"PULP: A Parallel Ultra Low Power Platform for Next Generation IoT Applications\",\n        \"Basejump STL: Systemverilog Needs a Standard Template Library for Hardware Design\",\n        \"Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design\",\n        \"Fast: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding\",\n        \"Intel Performance Counter Monitor\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}