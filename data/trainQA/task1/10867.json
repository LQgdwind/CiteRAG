{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 289 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing\n\nPaper Abstract: Abstract Processing-using-DRAM (PUD) is a processing-in-memory (PIM) approach that uses a DRAM array’s massive internal parallelism to execute very-wide (e.g., 16,384–262,144-bit-wide) data-parallel operations, in a single-instruction multiple-data (SIMD) fashion. However, DRAM rows’ large and rigid granularity limit the effectiveness and applicability of PUD in three ways. First, since applications have varying degrees of SIMD parallelism (which is often smaller than the DRAM row granularity), PUD execution often leads to underutilization, throughput loss, and energy waste. Second, due to the high area cost of implementing interconnects that connect columns in a wide DRAM row, most PUD architectures are limited to the execution of parallel map operations, where a single operation is performed over equally-sized input and output arrays. Third, the need to feed the wide DRAM row with tens of thousands of data elements combined with the lack of adequate compiler support for PUD systems create a programmability barrier, since programmers need to manually extract SIMD parallelism from an application and map computation to the PUD hardware. Our goal is to design a flexible PUD system that overcomes the limitations caused by the large and rigid granularity of PUD. To this end, we propose MIMDRAM, a hardware/software co-designed PUD system that introduces new mechanisms to allocate and control only the necessary resources for a given PUD operation. The key idea of MIMDRAM is to leverage fine-grained DRAM (i.e., the ability to independently access smaller segments of a large DRAM row) for PUD computation. disable,disable,todo: disable,#D1MIMDRAM exploits this key idea to enable a multiple-instruction multiple-data (MIMD) execution model in each DRAM subarray (and SIMD execution within each DRAM row segment). We evaluate MIMDRAM using twelve real-world applications and 495 multi-programmed application mixes. Our evaluation shows that MIMDRAM provides 34×\\times the performance, 14.3×\\times the energy efficiency, 1.7×\\times the throughput, and 1.3×\\times the fairness of a state-of-the-art PUD framework, along with 30.6×\\times and 6.8×\\times the energy efficiency of a high-end CPU and GPU, respectively. MIMDRAM adds small area cost to a DRAM chip (1.11%) and CPU die (0.6%). We hope and believe that MIMDRAM’s ideas and results will help to enable more efficient and easy-to-program PUD systems. To this end, we open source MIMDRAM at https://github.com/CMU-SAFARI/MIMDRAM.\n\n{retrieved_text}\n\nGenerate exactly 289 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 289\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Memory Scaling: A Systems Architecture Perspective\",\n        \"Research Problems and Opportunities in Memory Systems\",\n        \"The Tail at Scale\",\n        \"Profiling a Warehouse-Scale Computer\",\n        \"Enabling Practical Processing in and near Memory for Data-Intensive Computing\",\n        \"Processing Data Where It Makes Sense: Enabling In-Memory Computation\",\n        \"Intelligent Architectures for Intelligent Machines\",\n        \"Processing-in-Memory: A Workload-Driven Perspective\",\n        \"A Modern Primer on Processing in Memory\",\n        \"Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks\",\n        \"Reducing Data Movement Energy via Online Data Clustering and Encoding\",\n        \"Reflections on the Memory Wall\",\n        \"The Memory Gap and the Future of High Performance Memories\",\n        \"A Case for Exploiting Subarray-Level Parallelism (SALP) in DRAM\",\n        \"Hitting the Memory Wall: Implications of the Obvious\",\n        \"Demystifying Complex Workload–DRAM Interactions: An Experimental Study\",\n        \"A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing\",\n        \"PIM-Enabled Instructions: A Low-Overhead, Locality-Aware Processing-in-Memory Architecture\",\n        \"It’s the Memory, Stupid!\",\n        \"DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks\",\n        \"Compute Trends Across Three Eras of Machine Learning\",\n        \"AI and Memory Wall\",\n        \"NDA: Near-DRAM Acceleration Architecture Leveraging Commodity DRAM Devices and Standard Memory Modules\",\n        \"JAFAR: Near-Data Processing for Databases\",\n        \"The True Processing in Memory Accelerator\",\n        \"GenStore: A High-Performance and Energy-Efficient In-Storage Computing System for Genome Sequence Analysis\",\n        \"Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-in-Memory Hardware\",\n        \"Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture\",\n        \"Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System\",\n        \"SynCron: Efficient Synchronization Support for Near-Data-Processing Architectures\",\n        \"NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling\",\n        \"A 1ynm 1.25V 8Gb, 16Gb/s/pin GDDR6-Based Accelerator-in-Memory Supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning Applications\",\n        \"Near-Memory Processing in Action: Accelerating Personalized Recommendation with AxDIMM\",\n        \"SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-in-Memory Architectures\",\n        \"McDRAM: Low Latency and Energy-Efficient Matrix Computations in DRAM\",\n        \"McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge\",\n        \"Casper: Accelerating Stencil Computation using Near-Cache Processing\",\n        \"Chameleon: Versatile and Practical Near-DRAM Acceleration Architecture for Large Memory Systems\",\n        \"A Case for Intelligent RAM\",\n        \"Computational RAM: Implementing Processors in Memory\",\n        \"Saving Memory Movements Through Vector Processing in the DRAM\",\n        \"Beyond the Wall: Near-Data Processing for Databases\",\n        \"ABC-DIMM: Alleviating the Bottleneck of Communication in DIMM-Based Near-Memory Processing with Inter-DIMM Broadcast\",\n        \"GraphSSD: Graph Semantics Aware SSD\",\n        \"Processing in Memory: The Terasys Massively Parallel PIM Array\",\n        \"Mapping Irregular Applications to DIVA, a PIM-Based Data-Intensive Architecture\",\n        \"Opportunities and Challenges of Performing Vector Operations Inside the DRAM\",\n        \"Livia: Data-Centric Computing Throughout the Memory Hierarchy\",\n        \"GraphPIM: Enabling Instruction-Level PIM Offloading in Graph Computing Frameworks\",\n        \"LazyPIM: An Efficient Cache Coherence Mechanism for Processing-in-Memory\",\n        \"TOP-PIM: Throughput-Oriented Programmable Processing in Memory\",\n        \"“HRL: Efficient and Flexible Reconfigurable Logic for Near-Data Processing\",\n        \"“GRIM-Filter: Fast Seed Location Filtering in DNA Read Mapping Using Processing-in-Memory Technologies\",\n        \"A. Daglis et al\",\n        \"“Operand Size Reconfiguration for Big Data Processing in Memory\",\n        \"“NIM: An HMC-Based Machine for Neuron Computation\",\n        \"“TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory\",\n        \"“Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory\",\n        \"“Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges\",\n        \"“CoNDA: Efficient Cache Coherence Support for Near-Data Accelerators\",\n        \"E. Ebrahimi et al\",\n        \"“GenASM: A High-Performance\",\n        \"J. Jestes et al\",\n        \"“Scheduling Techniques for GPU Architectures with Processing-in-Memory Capabilities\",\n        \"“Data Reorganization in Memory Using 3D-Stacked DRAM\",\n        \"“Accelerating Pointer Chasing in 3D-Stacked Memory: Challenges\",\n        \"“BSSync: Processing Near Memory for Machine Learning Workloads with Bounded Staleness Consistency Models\",\n        \"“Mitigating Edge Machine Learning Inference Bottlenecks: An Empirical Study on Accelerating Google Edge Models\",\n        \"“Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks\",\n        \"“Polynesia: Enabling High-Performance and Energy-Efficient Hybrid Transactional/Analytical Databases with Hardware/Software Co-Design\",\n        \"“Polynesia: Enabling Effective Hybrid Transactional/Analytical Databases with Specialized Hardware/Software Co-Design\",\n        \"“Practical Mechanisms for Reducing Processor-Memory Data Movement in Modern Workloads\",\n        \"R. Kanakagiri et al\",\n        \"R. Quislant et al\",\n        \"“NAPEL: Near-Memory Computing Application Performance Prediction via Ensemble Learning\",\n        \"“A 20nm 6GB Function-in-Memory DRAM\",\n        \"S.-h. Kang et al\",\n        \"“184QPS/W 64Mb/m​m2𝑚superscript𝑚2mm^{2} 3D Logic-to-DRAM Hybrid Bonding with Process-Near-Memory Engine for Recommendation System\",\n        \"“Accelerating Sparse Matrix-Matrix Multiplication with 3D-Stacked Logic-in-Memory Hardware\",\n        \"C. Pfister et al\",\n        \"“Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes\",\n        \"N. Alachiotis et al\",\n        \"“Design Space Exploration for PIM Architectures in 3D-Stacked Memories\",\n        \"“HAMLeT: Hardware Accelerated Memory Layout Transform within 3D-Stacked DRAM\",\n        \"“A Heterogeneous PIM Hardware-Software Co-Design for Energy-Efficient Graph Processing\",\n        \"“GraphH: A Processing-in-Memory Architecture for Large-Scale Graph Processing\",\n        \"“Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach\",\n        \"“Adaptive Scheduling for Systems with Asymmetric Memory Hierarchies\",\n        \"“iPIM: Programmable In-Memory Image Processing Accelerator using Near-Bank Architecture\",\n        \"“DRAMA: An Architecture for Accelerated Processing Near Memory\",\n        \"A. Farmahini-Farahani et al\",\n        \"“Active-Routing: Compute on the Way for Near-Data Processing\",\n        \"“Lightweight SIMT Core Designs for Intelligent 3D Stacked DRAM\",\n        \"“PIMS: A Lightweight Processing-in-Memory Accelerator for Stencil Computations\",\n        \"“GRIM-Filter: Fast Seed Filtering in Read Mapping using Emerging Memory Technologies\",\n        \"“LazyPIM: Efficient Support for Cache Coherence in Processing-in-Memory Architectures\",\n        \"“GraphQ: Scalable PIM-Based Graph Processing\",\n        \"“GraphP: Reducing Communication for PIM-Based Graph Processing with Efficient Data Partition\",\n        \"“Triple Engine Processor (TEP): A Heterogeneous Near-Memory Processor for Diverse Kernel Operations\",\n        \"“A Case for Near Memory Computation Inside the Smart Memory Cube\",\n        \"M. Diener et al\",\n        \"“Charon: Specialized Near-Memory Processing Architecture for Clearing Dead Objects in Memory\",\n        \"“Active Memory Cube: A Processing-in-Memory Architecture for Exascale Systems\",\n        \"“CAIRO: A Compiler-Assisted Technique for Enabling Instruction-Level Offloading of Processing-in-Memory\",\n        \"“Processing in 3D Memories to Speed Up Operations on Complex Data Structures\",\n        \"“PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory\",\n        \"“ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars\",\n        \"“Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology\",\n        \"“In-DRAM Bulk Bitwise Execution Engine\",\n        \"“DRISA: A DRAM-Based Reconfigurable In-Situ Accelerator\",\n        \"“RowClone: Fast and Energy-Efficient In-DRAM Bulk Data Copy and Initialization\",\n        \"“The Processing Using Memory Paradigm: In-DRAM Bulk Copy\",\n        \"“DrAcc: A DRAM Based Accelerator for Accurate CNN Inference\",\n        \"“ELP2IM: Efficient and Low Power Bitwise Operation Processing in DRAM\",\n        \"“GraphR: Accelerating Graph Processing Using ReRAM\",\n        \"“PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning\",\n        \"G. Tziantzioulis\",\n        \"“Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks\",\n        \"S. Jeloka et al\",\n        \"“Duality Cache for Data Parallel Acceleration\",\n        \"“Buddy-RAM: Improving the Performance and Efficiency of Bulk Bitwise Operations Using DRAM\",\n        \"“Simple Operations in Memory to Reduce Data Movement\",\n        \"“RowClone: Accelerating Data Movement and Initialization Using DRAM\",\n        \"“Fast Bulk Bitwise AND and OR in DRAM\",\n        \"“Pinatubo: A Processing-in-Memory Architecture for Bulk Bitwise Operations in Emerging Non-Volatile Memories\",\n        \"G. Falcao et al\",\n        \"“FloatPIM: In-Memory Acceleration of Deep Neural Network Training with High Precision\",\n        \"“Sparse BD-Net: A Multiplication-Less DNN with Sparse Binarized Depth-Wise Separable Convolution\",\n        \"“Flash-Cosmos: In-Flash Bulk Bitwise Operations Using Inherent Computation Capability of NAND Flash Memory\",\n        \"“Adapting the RACER Architecture to Integrate Improved In-ReRAM Logic Primitives\",\n        \"“RACER: Bit-Pipelined Processing Using Resistive Memory\",\n        \"“QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple Row Activation in Commodity DRAMs\",\n        \"“D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers With Low Latency and High Throughput\",\n        \"“The DRAM Latency PUF: Quickly Evaluating Physical Unclonable Functions by Exploiting the Latency-Reliability Tradeoff in Modern Commodity DRAM Devices\",\n        \"“DR-STRaNGe: End-to-End System Design for DRAM-Based True Random Number Generators\",\n        \"“PiDRAM: A Holistic End-to-End FPGA-Based Framework for Processing-in-DRAM\",\n        \"“In-Memory Low-Cost Bit-Serial Addition Using Commodity DRAM Technology\",\n        \"“GraphiDe: A Graph Processing Accelerator Leveraging In-DRAM-Computing\",\n        \"“SCOPE: A Stochastic Computing Engine for DRAM-Based In-Situ Accelerator\",\n        \"“Parallel Automata Processor\",\n        \"“Hyper-AP: Enhancing Associative Processing Through A Full-Stack Optimization\",\n        \"“In-Memory Data Parallel Processor\",\n        \"“CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities and Optimizations\",\n        \"“Ultra Low Power Associative Computing with Spin Neurons and Resistive Crossbar Memory\",\n        \"M. Modarressi et al\",\n        \"“ParaBit: Processing Parallel Bitwise Operations in NAND Flash Memory Based SSDs\",\n        \"P.-F. Chiu et al\",\n        \"“A Novel Convolution Computing Paradigm Based on NOR Flash Array with High Computing Speed and Energy Efficiency\",\n        \"“High-Performance Mixed-Signal Neurocomputing with Nanoscale Floating-Gate Memory Cell Arrays\",\n        \"“Three-Dimensional NAND Flash for Vector–Matrix Multiplication\",\n        \"P.-K. Hsu et al\",\n        \"“Behemoth: A Flash-Centric Training Accelerator for Extreme-Scale {{\\\\{DNNs}}\\\\}\",\n        \"“MemCore: Computing-in-Flash Design for Deep Neural Network Acceleration\",\n        \"“Flash Memory Array for Efficient Implementation of Deep Neural Networks\",\n        \"“S-FLASH: A NAND Flash-Based Deep Neural Network Accelerator Exploiting Bit-Level Sparsity\",\n        \"“Neuromorphic Computing Using NAND Flash Memory Architecture with Pulse Width Modulation Scheme\",\n        \"“3D-FPIM: An Extreme Energy-Efficient DNN Acceleration System Using 3D NAND Flash-Based In-Situ PIM Unit\",\n        \"W.-S. Khwa et al\",\n        \"“BLADE: An In-Cache Computing Architecture for Edge Devices\",\n        \"C. Ramachandra et al\",\n        \"“Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks\",\n        \"“Towards a Reconfigurable Bit-Serial/Bit-Parallel Vector Accelerator Using In-Situ Processing-in-SRAM\",\n        \"M.-S. Keel et al\",\n        \"“Colonnade: A Reconfigurable SRAM-Based Digital Bit-Serial Compute-in-Memory Macro for Processing Neural Networks\",\n        \"“C3SRAM: An In-Memory-Computing SRAM Macro Based on Robust Capacitive Coupling Computing Mechanism\",\n        \"“A 28 nm Configurable Memory (TCAM/BCAM/SRAM) Using Push-Rule 6T Bit Cell Enabling Logic-in-Memory\",\n        \"“Infinity Stream: Portable and Programmer-Friendly In-/Near-Memory Fusion\",\n        \"“Energy-Efficient and High Throughput Sparse Distributed Memory Architecture\",\n        \"S. Pampana et al\",\n        \"“Low-Cost Inter-Linked Subarrays (LISA): Enabling Fast Inter-Subarray Data Movement in DRAM\",\n        \"“SIMDRAM: A Framework for Bit-Serial SIMD Processing Using DRAM\",\n        \"“LAcc: Exploiting Lookup Table-Based Fast and Accurate Vector Multiplication in DRAM-Based CNN Accelerator\",\n        \"S. Bavikadi et al\",\n        \"M. Connolly et al\",\n        \"P. Gonzalez et al\",\n        \"“CHOPPER: A Compiler Infrastructure for Programmable Bit-Serial SIMD Processing Using Memory In DRAM\",\n        \"J. Gómez-Luna et al\",\n        \"“FPGA-Based Near-Memory Acceleration of Modern Data-Intensive Applications\",\n        \"“DaPPA: A Data-Parallel Framework for Processing-in-Memory Architectures\",\n        \"A. Boroumand et al\",\n        \"“Swordfish: A Framework for Evaluating Deep Neural Network-Based Basecalling Using Computation-In-Memory with Non-Ideal Memristors\",\n        \"“Evaluating Homomorphic Operations on a Real-World Processing-In-Memory System\",\n        \"“Evaluating Machine LearningWorkloads on Memory-Centric Computing Systems\",\n        \"A. Nassereldine et al\",\n        \"“GenPIP: In-Memory Acceleration of Genome Analysis via Tight Integration of Basecalling and Read Mapping\",\n        \"D. Diamantopoulos et al\",\n        \"“HMC Specification Rev. 2.0\",\n        \"“A 1.2V 8Gb 8-Channel 128GB/s High-Bandwidth Memory (HBM) Stacked DRAM with Effective Microbump I/O Test Methods Using 29nm Process and TSV\",\n        \"“Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth at Low Cost\",\n        \"x16 DDR4 SDRAM Features\",\n        \"“Fine-Grained Activation for Power Reduction in DRAM\",\n        \"N. Muralimanohar et al\",\n        \"“Half-DRAM: A High-Bandwidth and Low-Power DRAM Architecture from the Rethinking of Fine-Grained Activation\",\n        \"A. Pedram et al\",\n        \"“Partial Row Activation for Low-Power DRAM System\",\n        \"F. Bostanci et al\",\n        \"“Energy Efficient High Bandwidth DRAM for Throughput Processors\",\n        \"N. Chatterjee et al\",\n        \"M. Buxton et al\",\n        \"Shared Resource MIMD Computer\",\n        \"“Very High-Speed Computing Systems\",\n        \"“Parallel Operation in the Control Data 6600\",\n        \"“Phoenix Rebirth: Scalable MapReduce on a Large-Scale Shared-Memory System\",\n        \"“PolyBench: The Polyhedral Benchmark Suite\",\n        \"“Rodinia: A Benchmark Suite for Heterogeneous Computing\",\n        \"” https://t.ly/_ibe2\",\n        \"“Adaptive-Latency DRAM: Optimizing DRAM Timing for the Common-Case\",\n        \"“Field-Effect Transistor Memory\",\n        \"DRAM Circuit Design: Fundamental and High-Speed Topics. John Wiley & Sons\",\n        \"“HiRA: Hidden Row Activation for Reducing Refresh Latency of Off-the-Shelf DRAM Chips\",\n        \"“RowPress: Amplifying Read Disturbance in Modern DRAM Chips\",\n        \"DRAM Circuit Design: A Tutorial\",\n        \"“Solar-DRAM: Reducing DRAM Access Latency by Exploiting the Variation in Local Bitlines\",\n        \"“Design-Induced Latency Variation in Modern DRAM Chips: Characterization\",\n        \"Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches\",\n        \"“Cost Modelling for Vectorization on ARM\",\n        \"D. Nuzman et al\",\n        \"S. R. dos Santos\",\n        \"“LLVM and Clang: Next Generation Compiler Technology\",\n        \"LLVM Essentials. Packt Publishing Ltd\",\n        \"Getting Started with LLVM Core Libraries. Packt Publishing Ltd\",\n        \"“LLVM for Grad Students\",\n        \"” https://github.com/IITH-Compilers/LLVM-Loop-Profiler\",\n        \"“FIGARO: Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching\",\n        \"K.-D. Kang et al\",\n        \"“Worst-Case Analysis of Memory Allocation Algorithms\",\n        \"“Auto-Vectorization in LLVM\",\n        \"“A Compiler for Automatic Selection of Suitable Processing-in-Memory Instructions\",\n        \"“To PIM or Not for Emerging General Purpose Processing in DDR Memory Systems\",\n        \"Introduction to Algorithms. MIT Press\",\n        \"“Edge-Disjoint Spanning Trees and Depth-First Search\",\n        \"“OpenMP: An Industry Standard API for Shared-Memory Programming\",\n        \"J. Nickolls et al\",\n        \"“NVIDIA;s Next Generation CUDA Compute Architecture: Fermi\",\n        \"“ISA Aging: A X86 Case Study\",\n        \"“SHRINK: Reducing the ISA Complexity via Instruction Recycling\",\n        \"O. Lostes-Cazorla et al\",\n        \"“A High-Performance Microarchitecture with Hardware-Programmable Functional Units\",\n        \"Intel® 64 and IA-32 Architectures Software Developer’s Manual\",\n        \"Cortex-A8 Technical Reference Manual\",\n        \"“Linux and the Devicetree — The Linux Kernel Documentation\",\n        \"“Revisiting RowHammer: An Experimental Analysis of Modern DRAM Devices and Mitigation Techniques\",\n        \"“A Deeper Look into RowHammer’s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses\",\n        \"“Understanding RowHammer Under Reduced Wordline Voltage: An Experimental Study Using Real DRAM Devices\",\n        \"J. Rosenblum et al\",\n        \"“Near-Optimal Bin Packing Algorithms\",\n        \"“A Programmer’s Description of L6\",\n        \"B. Beckmann et al\",\n        \"“6th Generation Intel Core Processor Family Datasheet\",\n        \"“NVIDIA A100 Tensor Core GPU Architecture\",\n        \"“Measuring Energy Consumption for Short Code Paths Using RAPL\",\n        \"“NVIDIA Management Library (NVML)\",\n        \"“CACTI 3.0: An Integrated Cache Timing\",\n        \"R. Balasubramonian\",\n        \"“Stall-Time Fair Memory Access Scheduling for Chip Multiprocessors\",\n        \"“Controller for a Synchronous DRAM That Maximizes Throughput by Allowing Memory Requests and Commands to Be Issued Out of Order\",\n        \"“SPEC CPU2006 Benchmarks\",\n        \"C. Rodrigues et al\",\n        \"“The SPLASH-2 Programs: Characterization and Methodological Considerations\",\n        \"“DAMOV Benchmark Suite and Simulation Framework\",\n        \"L. Chelini et al\",\n        \"“NDP-RANK: Prediction and Ranking of NDP Systems Performance using Machine Learning\",\n        \"D. Mahajan et al\",\n        \"E. Manogaran et al\",\n        \"“Symbiotic Jobscheduling for a Simultaneous Multithreaded Processor\",\n        \"“System-Level Performance Metrics for Multiprogram Workloads\",\n        \"“Demystifying Multicore Throughput Metrics\",\n        \"“Balancing Thoughput and Fairness in SMT Processors\",\n        \"M. Papamichael et al\",\n        \"“ATLAS: A Scalable and High-Performance Scheduling Algorithm for Multiple Memory Controllers\",\n        \"“The Blacklisting Memory Scheduler: Achieving High Performance and Fairness at Low Cost\",\n        \"“BLISS: Balancing Performance\",\n        \"V. Seshadri et al\",\n        \"“Fairness via Source Throttling: A Configurable and High-Performance Fairness Substrate for Multi-Core Memory Systems\",\n        \"“Prefetch-Aware Shared Resource Management for Multi-Core Systems\",\n        \"“Application-Aware Prioritization Mechanisms for On-Chip Networks\",\n        \"R. Ausavarungnirun et al\",\n        \"“Parallelism-Aware Batch Scheduling: Enhancing Both Performance and Fairness of Shared DRAM Systems\",\n        \"“Ramulator: A Fast and Extensible DRAM Simulator\",\n        \"V. Narasiman et al\",\n        \"“Improving System Throughput and Fairness Simultaneously in Shared Memory CMP Systems via Dynamic Bank Partitioning\",\n        \"“MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput\",\n        \"D. Leitersdorf et al\",\n        \"“Scaling Equations for the Accurate Prediction of CMOS Device Performance from 180 Nm to 7 Nm\",\n        \"“DIMMining: Pruning-Efficient and Parallel Graph Mining on Near-Memory-Computing\",\n        \"“SARA: Scaling a Reconfigurable Dataflow Accelerator\",\n        \"“FlexiDRAM: A Flexible In-DRAM Framework to Enable Parallel General-Purpose Computation\",\n        \"Professional CUDA C Programming. John Wiley & Sons\",\n        \"“The OpenACC® Application Programming Interface\"\n    ],\n    \"reasoning\": \"Selected these 289 references because they represent foundational work and recent advances in MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}