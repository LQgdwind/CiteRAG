{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Memory-Sample Lower Bounds for Learning with Classical-Quantum Hybrid Memory\n\nPaper Abstract: Abstract In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for parity learning on nùëõn bits requires either Œ©‚Äã(n2)Œ©superscriptùëõ2\\Omega(n^{2}) bits of classical memory or an exponential number (in nùëõn) of random samples. A line of recent works continued that research direction and showed that for a large collection of classical learning tasks, either super-linear classical memory size or super-polynomially many samples are needed. All these works consider learning algorithms as classical branching programs, which perform classical computation within bounded memory. However, these results do not capture all physical computational models, remarkably, quantum computers and the use of quantum memory. It leaves the possibility that a small piece of quantum memory could significantly reduce the need for classical memory or samples and thus completely change the nature of the classical learning task. Despite the recent research on the necessity of quantum memory for intrinsic quantum learning problems like shadow tomography and purity testing, the role of quantum memory in classical learning tasks remains obscure. In this work, we study classical learning tasks in the presence of quantum memory. We prove that any quantum algorithm with both, classical memory and quantum memory, for parity learning on nùëõn bits, requires either Œ©‚Äã(n2)Œ©superscriptùëõ2\\Omega(n^{2}) bits of classical memory or Œ©‚Äã(n)Œ©ùëõ\\Omega(n) bits of quantum memory or an exponential number of samples. In other words, the memory-sample lower bound for parity learning remains qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size c‚Äãnùëêùëõcn (for some constant c>0ùëê0c>0). Our result is more general and applies to many other classical learning tasks. Following previous works, we represent by the matrix M:A√óX‚Üí{‚àí1,1}:ùëÄ‚Üíùê¥ùëã11M:A\\times X\\to\\{-1,1\\} the following learning task. An unknown xùë•x is sampled uniformly at random from a concept class XùëãX, and a learning algorithm tries to uncover xùë•x by seeing streaming of random samples (ai,bi=M‚Äã(ai,x))subscriptùëéùëñsubscriptùëèùëñùëÄsubscriptùëéùëñùë•(a_{i},b_{i}=M(a_{i},x)) where for every iùëñi, ai‚ààAsubscriptùëéùëñùê¥a_{i}\\in A is chosen uniformly at random. Assume that k,‚Ñì,rùëò‚Ñìùëük,\\ell,r are integers such that any submatrix of MùëÄM of at least 2‚àík‚ãÖ|A|‚ãÖsuperscript2ùëòùê¥2^{-k}\\cdot|A| rows and at least 2‚àí‚Ñì‚ãÖ|X|‚ãÖsuperscript2‚Ñìùëã2^{-\\ell}\\cdot|X| columns, has a bias of at most 2‚àírsuperscript2ùëü2^{-r}. We prove that any algorithm with classical and quantum hybrid memory for the learning problem corresponding to MùëÄM needs either (1) Œ©‚Äã(k‚ãÖ‚Ñì)Œ©‚ãÖùëò‚Ñì\\Omega(k\\cdot\\ell) bits of classical memory, or (2) Œ©‚Äã(r)Œ©ùëü\\Omega(r) qubits of quantum memory, or (3) 2Œ©‚Äã(r)superscript2Œ©ùëü2^{\\Omega(r)} random samples, to achieve a success probability at least 2‚àíO‚Äã(r)superscript2ùëÇùëü2^{-O(r)}. Our results refute the possibility that a small amount of quantum memory significantly reduces the size of classical memory needed for efficient learning on these problems. Our results also imply improved security of several existing cryptographical protocols in the bounded-storage model (protocols that are based on parity learning on nùëõn bits), proving that security holds even in the presence of a quantum adversary with at most c‚Äãn2ùëêsuperscriptùëõ2cn^{2} bits of classical memory and c‚Äãnùëêùëõcn bits of quantum memory (for some constant c>0ùëê0c>0).\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Quantum supremacy using a programmable superconducting processor\",\n        \"Shadow tomography of quantum states\",\n        \"Quantum algorithmic measurement\",\n        \"Everlasting security in the bounded storage model\",\n        \"Information theoretically secure communication in the limited storage space model\",\n        \"A sharp continuity estimate for the von neumann entropy\",\n        \"Entanglement is necessary for optimal quantum property testing\",\n        \"Time-space tradeoffs for learning finite functions from random evaluations, with applications to polynomials\",\n        \"Device-independent oblivious transfer from the bounded-quantum-storage-model and computational assumptions\",\n        \"Exponential separations between learning with and without quantum memory\",\n        \"Toward instance-optimal state certification with incoherent measurements\",\n        \"Unconditional security against memory-bounded adversaries\",\n        \"Distributional and LqsuperscriptùêøùëûL^{q} norm inequalities for polynomials over convex bodies in ‚Ñùnsuperscript‚Ñùùëõ\\\\mathbb{R}^{n}\",\n        \"A tight high-order entropic quantum uncertainty relation with applications\",\n        \"Secure identification and qkd in the bounded-quantum-storage model\",\n        \"Cryptography in the bounded-quantum-storage model\",\n        \"Tight security proofs for the bounded-storage model\",\n        \"On generating the initial key in the bounded-storage model\",\n        \"Speak much, remember little: Cryptography in the bounded storage model, revisited\",\n        \"Authentication in the bounded storage model\",\n        \"Hyper-encryption and everlasting security\",\n        \"Cryptographic distinguishability measures for quantum-mechanical states\",\n        \"Exponential separation for one-way quantum communication complexity, with applications to cryptography\",\n        \"Memory-sample lower bounds for learning parity with noise\",\n        \"Time-space tradeoffs for distinguishing distributions and applications to security of goldreich‚Äôs PRG\",\n        \"Extractor-based time-space lower bounds for learning\",\n        \"Simple schemes in the bounded storage model\",\n        \"Sample-optimal tomography of quantum states\",\n        \"Predicting many properties of a quantum system from very few measurements\",\n        \"On everlasting security in the hybrid bounded storage model\",\n        \"Two-source extractors secure against quantum adversaries\",\n        \"Time-space hardness of learning sparse parities\",\n        \"Adaptive estimation of a quadratic functional by model selection\",\n        \"Hyper-encryption against space-bounded adversaries from on-line strong extractors\",\n        \"Secure multiparty computation in the bounded storage model\",\n        \"Conditionally-perfect secrecy and a provably-secure randomized cipher\",\n        \"Entropy samplers and strong generic lower bounds for space bounded learning\",\n        \"Non-interactive timestamping in the bounded storage model\",\n        \"Quantum entropy and its use\",\n        \"Security of device-independent quantum key distribution in the bounded-quantum-storage model\",\n        \"A time-space lower bound for a large class of learning problems\",\n        \"Fast learning requires good memory: A time-space lower bound for parity learning\",\n        \"Linear models in statistics\",\n        \"Fundamental limits of online and distributed algorithms for statistical learning and estimation\",\n        \"Memory-sample tradeoffs for linear regression with small error\",\n        \"Memory, communication, and statistical queries\",\n        \"The ‚Äútransition probability‚Äù in the state space of a *-algebra\",\n        \"How to learn a quantum state\",\n        \"Composable security in the bounded-quantum-storage model\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Memory-Sample Lower Bounds for Learning with Classical-Quantum Hybrid Memory, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}