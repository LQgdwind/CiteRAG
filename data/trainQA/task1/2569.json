{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 221 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis\n\nPaper Abstract: \n\n{retrieved_text}\n\nGenerate exactly 221 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 221\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"TensorFlow: Large-scale machine learning on heterogeneous distributed systems\",\n        \"Towards the unification and robustness of perturbation and gradient based explanations\",\n        \"Contextual string embeddings for sequence labeling\",\n        \"The illustrated BERT, ELMo, and co. (How NLP cracked transfer learning)\",\n        \"The illustrated Transformer\",\n        \"Visualizing a neural machine translation model (mechanics of seq2seq models with attention)\",\n        \"Recurrent neural networks cheatsheet [Lecture Notes]\",\n        \"Fine-grained analysis of diversity levels in the news\",\n        \"Understanding delegation through machine learning: A method and application to the European Union\",\n        \"Towards better understanding of gradient-based attribution methods for deep neural networks\",\n        \"Ext5: Towards extreme multi-task scaling for transfer learning\",\n        \"On the comparability of pre-trained language models\",\n        \"Layer Normalization\",\n        \"XLS-R: Self-supervised cross-lingual speech representation learning at scale\",\n        \"wav2vec 2.0: A framework for self-supervised learning of speech representations\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"SLAM: A unified encoder for speech and language modeling via speech-text joint pre-training\",\n        \"Automated text classification of news articles: A practical guide\",\n        \"Analysis methods in neural language processing: A survey\",\n        \"Longformer: The long-document Transformer\",\n        \"A neural probabilistic language model\",\n        \"Text as data: An overview\",\n        \"quanteda: An R package for the quantitative analysis of textual data\",\n        \"Enriching word vectors with subword information\",\n        \"On the opportunities and risks of foundation models\",\n        \"A training algorithm for optimal margin classifiers\",\n        \"OpenAI API\",\n        \"Language models are few-shot learners\",\n        \"How to control the stability of training neural networks with the batch size\",\n        \"Random oversampling and undersampling for imbalanced classification\",\n        \"Predicting the vote using legislative speech\",\n        \"What’s the difference between a CPU and a GPU?\",\n        \"Using sentiment analysis to monitor electoral campaigns: Method matters—Evidence from the United States and Italy\",\n        \"Every tweet counts? How sentiment analysis of social media can improve our knowledge of citizens’ political preferences with an application to Italy and France\",\n        \"Using word order in political text classification with long short-term memory models\",\n        \"XGBoost: A scalable tree boosting system\",\n        \"Generating long sequences with sparse Transformers\",\n        \"Learning phrase representations using RNN encoder–decoder for statistical machine translation\",\n        \"What does BERT look at? An analysis of BERT’s attention\",\n        \"ELECTRA: Pre-training text encoders as discriminators rather than generators\",\n        \"More efficient NLP model pre-training with ELECTRA\",\n        \"Fast and accurate deep network learning by Exponential Linear Units (ELUs)\",\n        \"Echo chamber or public sphere? Predicting political orientation and measuring political homophily in Twitter using big data\",\n        \"Unsupervised cross-lingual representation learning at scale\",\n        \"Support-vector networks\",\n        \"Transformer-XL: Attentive language models beyond a fixed-length context\",\n        \"New pipeline for zero-shot text classification\",\n        \"Zero-shot learning in modern NLP\",\n        \"Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it\",\n        \"Multilingual BERT\",\n        \"BERT: Pre-training of deep bidirectional Transformers for language understanding\",\n        \"How populist are parties? Measuring degrees of populism in party manifestos using supervised machine learning\",\n        \"Language and ideology in Congress\",\n        \"Separating the wheat from the chaff: Applications of automated document classification using support vector machines\",\n        \"Towards a rigorous science of interpretable machine learning\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Adaptive subgradient methods for online learning and stochastic optimization\",\n        \"A deep modular RNN approach for ethos mining\",\n        \"Finding structure in time\",\n        \"The impact of party cues on manual coding of political texts\",\n        \"Studies in linguistic analysis\",\n        \"Political advertising online and offline\",\n        \"VIOLET: End-to-end video-language Transformers with masked visual-token modeling\",\n        \"Survey of the state of the art in natural language generation: Core tasks, applications and evaluation\",\n        \"Cross-lingual classification of topics in political texts\",\n        \"A primer on neural network models for natural language processing\",\n        \"Deep learning\",\n        \"Google Colaboratory Frequently Asked Questions\",\n        \"Self-supervised pretraining of visual features in the wild\",\n        \"Machine learning human rights and wrongs: How the successes and failures of supervised learning algorithms can inform the debate about information effects\",\n        \"Text as data: The promise and pitfalls of automatic content analysis methods for political texts\",\n        \"Conditional word embedding and hypothesis testing via Bayes-by-backprop\",\n        \"Activation functions explained - GELU, SELU, ELU, ReLU and more\",\n        \"Deep residual learning for image recognition\",\n        \"Gaussian Error Linear Units (GELUs)\",\n        \"Long short-term memory\",\n        \"Universal language model fine-tuning for text classification\",\n        \"Open-domain targeted sentiment analysis via span-based extraction and classification\",\n        \"PyTorch BERT model\",\n        \"Summary of the models\",\n        \"Tokenizer summary\",\n        \"Matplotlib: A 2D graphics environment\",\n        \"Political ideology detection using recursive neural networks\",\n        \"Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?\",\n        \"Perceiver IO: A general architecture for structured inputs & outputs\",\n        \"Perceiver: General perception with iterative attention\",\n        \"An introduction to statistical learning with applications in R\",\n        \"What does BERT learn about the structure of language?\",\n        \"Toxic comment classification challenge\",\n        \"The credibility of public and private signals: A document-based approach\",\n        \"gdown: Download a large file from Google Drive\",\n        \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n        \"Learning bill similarity with annotated and augmented corpora of bills\",\n        \"Adam: A method for stochastic optimization\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Reformer: The efficient Transformer\",\n        \"Attention is not only a weight: Analyzing Transformers with vector norms\",\n        \"Captum: A unified and generic model interpretability library for PyTorch\",\n        \"The geometry of culture: Analyzing the meanings of class through word embeddings\",\n        \"Disentangling user samples: A supervised machine learning approach to proxy-population mismatch in Twitter research\",\n        \"ALBERT: A lite BERT for self-supervised learning of language representations\",\n        \"Extracting policy positions from political texts using words as data\",\n        \"Distributed representations of sentences and documents\",\n        \"Imbalanced-learn: A Python toolbox to tackle the curse of imbalanced datasets in machine learning\",\n        \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"RoBERTa: A robustly optimized BERT pretraining approach\",\n        \"Decoupled weight decay regularization\",\n        \"UniVL: A unified video and language pre-training model for multimodal understanding and generation\",\n        \"Effective approaches to attention-based neural machine translation\",\n        \"Understanding natural language understanding [Presentation]\",\n        \"Introduction to information retrieval\",\n        \"CamemBERT: a tasty French language model\",\n        \"Revisiting small batch training for deep neural networks\",\n        \"Learned in translation: Contextualized word vectors\",\n        \"BERT fine-tuning tutorial with PyTorch\",\n        \"Data structures for statistical computing in Python\",\n        \"A new benchmark for NLP in social sciences: Evaluating the usefulness of pre-trained language models for classifying open-ended survey responses\",\n        \"Michael Waskom and Team (2020)\",\n        \"Coder reliability and misclassification in the human coding of party manifestos\",\n        \"Efficient estimation of word representations in vector space\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Active learning approaches for labeling text: Review and assessment of the performance of active learning approaches\",\n        \"Explanation in artificial intelligence: Insights from the social sciences\",\n        \"Open domain targeted sentiment\",\n        \"From isolation to radicalization: Anti-Muslim hostility and support for ISIS in the West\",\n        \"Stance and sentiment in tweets\",\n        \"Interpretable machine learning\",\n        \"We need to go deeper: Measuring electoral violence using convolutional neural networks and social media\",\n        \"Model card for bert-base-german-uncased from dbmdz\",\n        \"Rectified linear units improve Restricted Boltzmann machines\",\n        \"The future of coding: A comparison of hand-coding and three types of computer-assisted text analysis methods\",\n        \"PhoBERT: Pre-trained language models for Vietnamese\",\n        \"What the [MASK]? Making sense of language-specific BERT models\",\n        \"A Guide to NumPy\",\n        \"Cross-domain topic classification for political texts\",\n        \"A survey on transfer learning\",\n        \"Thumbs up? Sentiment classification using machine learning techniques\",\n        \"Human rights are (increasingly) plural: Learning the changing taxonomy of human rights from large-scale text reveals information effects\",\n        \"PyTorch: An imperative style, high-performance deep learning library\",\n        \"Scikit-learn: Machine learning in Python\",\n        \"GloVe: Global vectors for word representation\",\n        \"Scaling text with the Class Affinity Model\",\n        \"Deep contextualized word representations\",\n        \"Dissecting contextual word embeddings: Architecture and representation\",\n        \"To tune or not to tune? Adapting pretrained representations to diverse tasks\",\n        \"Embeddings in natural language processing: Theory and advances in vector representations of meaning\",\n        \"Using supervised machine learning in automated content analysis: An example using relational uncertainty\",\n        \"Placebo selection in survey experiments: An agnostic approach\",\n        \"About WordNet\",\n        \"R: A language and environment for statistical computing\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Improving language understanding by generative pre-training\",\n        \"Language models are unsupervised multitask learners\",\n        \"Exploring the limits of transfer learning with a unified text-to-text Transformer\",\n        \"Zero-shot text-to-image generation\",\n        \"Measuring elite personality using speech\",\n        \"Come hither or go away? Recognising pre-electoral coalition signals in the news\",\n        \"Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates\",\n        \"Sentence-BERT: Sentence embeddings using Siamese BERT-networks\",\n        \"Measuring emotion in parliamentary debates with automated textual analysis\",\n        \"Word embeddings for the analysis of ideological placement in parliamentary corpora\",\n        \"Beyond accuracy: Behavioral testing of NLP models with CheckList\",\n        \"AI democratization in the era of GPT-3\",\n        \"A timely intervention: Tracking the changing meanings of political concepts with word vectors\",\n        \"Enlisting supervised machine learning in mapping scientific uncertainty expressed in food risk analysis\",\n        \"AutoExtend: Extending word embeddings to embeddings for synsets and lexemes\",\n        \"NLP’s ImageNet moment has arrived\",\n        \"Neural transfer learning for natural language processing\",\n        \"Unsupervised cross-lingual representation learning\",\n        \"Recent advances in language model fine-tuning\",\n        \"More than bags of words: Sentiment analysis with word embeddings\",\n        \"Learning representations by back-propagating errors\",\n        \"How good is your tokenizer? on the monolingual performance of multilingual language models\",\n        \"Japanese and Korean voice search\",\n        \"Automatic word sense discrimination\",\n        \"scikit-learn Developers (2020a)\",\n        \"Metrics and scoring: quantifying the quality of predictions\",\n        \"RBF SVM Parameters\",\n        \"The multiclass classification of newspaper articles with machine learning: The hybrid binary snowball approach\",\n        \"text2vec: Modern text mining framework for R\",\n        \"Neural machine translation of rare words with subword units\",\n        \"The sound of rebellion: Voting dissent and legislative speech in the UK House of Commons\",\n        \"Linguistic structure prediction\",\n        \"Recursive deep models for semantic compositionality over a sentiment treebank\",\n        \"Word embeddings: What works, what doesn’t, and how to tell the difference for applied research\",\n        \"Dropout: A simple way to prevent neural networks from overfitting\",\n        \"VideoBERT: A joint model for video and language representation learning\",\n        \"How to fine-tune BERT for text classification?\",\n        \"Sequence to sequence learning with neural networks\",\n        \"Long Range Arena: A benchmark for efficient Transformers\",\n        \"BERT rediscovers the classical NLP pipeline\",\n        \"What do you learn from context? Probing for sentence structure in contextualized word representations\",\n        \"A bad workman blames his tweets: The consequences of citizens’ uncivil Twitter use when interacting with party candidates\",\n        \"From frequency to meaning: Vector space models of semantics\",\n        \"rstudioapi: Safely access the RStudio API\",\n        \"Python 3 Reference Manual\",\n        \"Principles of risk minimization for learning theory\",\n        \"Attention is all you need\",\n        \"SuperGLUE: A stickier benchmark for general-purpose language understanding systems\",\n        \"Linformer: Self-attention with linear complexity\",\n        \"A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis\",\n        \"How to estimate continuous sentiments from texts using binary training data\",\n        \"Latent semantic scaling: A semisupervised text analysis technique for new domains and languages\",\n        \"Finetuned language models are zero-shot learners\",\n        \"Text analysis in R\",\n        \"stringr: Simple, consistent wrappers for common string operations\",\n        \"A broad-coverage challenge corpus for sentence understanding through inference\",\n        \"Hugging Face’s Transformers: State-of-the-art natural language processing\",\n        \"MARMOT: A deep learning framework for constructing multimodal representations for vision-and-language tasks\",\n        \"Google’s neural machine translation system: Bridging the gap between human and machine translation\",\n        \"Python API reference\",\n        \"mT5: A massively multilingual pre-trained text-to-text transformer\",\n        \"XLNet: Generalized autoregressive pretraining for language understanding\",\n        \"TaBERT: Pretraining for joint understanding of textual and tabular data\",\n        \"Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach\",\n        \"How transferable are features in deep neural networks?\",\n        \"Big Bird: Transformers for longer sequences\",\n        \"MITRE at SemEval-2016 task 6: Transfer learning for stance detection\",\n        \"CASM: A deep-learning approach for identifying collective action events with text and image data from social media\",\n        \"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books\"\n    ],\n    \"reasoning\": \"Selected these 221 references because they represent foundational work and recent advances in Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}