{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference\n\nPaper Abstract: Abstract Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce ℓℓ\\ell-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret – e.g. graphical – diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, ℓℓ\\ell-C2ST can be specialized to offer better statistical power, while being computationally more efficient. On standard SBI benchmarks, ℓℓ\\ell-C2ST provides comparable results to C2ST and outperforms alternative local approaches such as coverage tests based on highest predictive density (HPD). We further highlight the importance of local evaluation and the benefit of interpretability of ℓℓ\\ell-C2ST on a challenging application from computational neuroscience.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Interrogating theoretical models of neural computation with emergent property inference\",\n        \"Stan: A probabilistic programming language\",\n        \"HNPE: Leveraging Global Parameters for Neural Posterior Estimation\",\n        \"The frontier of simulation-based inference\",\n        \"Confidence sets and hypothesis testing in a likelihood-free inference setting\",\n        \"Real-time gravitational wave science with neural posterior estimation\",\n        \"Group equivariant neural posterior estimation\",\n        \"Towards reliable simulation-based inference with balanced neural ratio estimation\",\n        \"Calibrated predictive distributions via diagnostics for conditional coverage\",\n        \"A large-scale study of probabilistic calibration in neural network regression\",\n        \"Neural spline flows\",\n        \"nflows: normalizing flows in PyTorch\",\n        \"Institute of mathematical statistics monographs: Computer age statistical inference: Algorithms, evidence, and data science series number 5\",\n        \"Model misspecification in approximate Bayesian computation: consequences and diagnostics\",\n        \"Bayesian workflow\",\n        \"Training deep neural density estimators to identify mechanistic models of neural dynamics\",\n        \"Generative adversarial networks\",\n        \"Automatic posterior transformation for likelihood-free inference\",\n        \"Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics\",\n        \"Simulation-based inference for whole-brain network modeling of epilepsy using deep neural density estimators\",\n        \"The elements of statistical learning\",\n        \"Likelihood-free MCMC with amortized approximate ratio estimators\",\n        \"A crisis in simulation-based inference? beware, your posterior approximations can be unfaithful\",\n        \"Inverting brain grey matter models with likelihood-free inference: a tool for trustable cytoarchitecture measurements\",\n        \"Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns\",\n        \"Global and local two-sample tests via regression\",\n        \"Robust simulation-based inference in cosmology with bayesian neural networks\",\n        \"The virtual brain: a simulator of primate brain network dynamics\",\n        \"Validation diagnostics for SBI algorithms based on normalizing flows\",\n        \"Revisiting classifier two-sample tests\",\n        \"Benchmarking simulation-based inference\",\n        \"Lossless, scalable implicit likelihood inference for cosmological fields\",\n        \"Simulation-based calibration checking for bayesian computation: The choice of test quantities shapes sensitivity\",\n        \"E-valuating classifier two-sample tests\",\n        \"Masked autoregressive flow for density estimation\",\n        \"Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows\",\n        \"Normalizing flows for probabilistic modeling and inference\",\n        \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n        \"Scikit-learn: Machine learning in Python\",\n        \"Variational inference with normalizing flows\",\n        \"Monte Carlo statistical methods\",\n        \"Validating bayesian inference algorithms with simulation-based calibration\",\n        \"Inferring coalescence times from DNA sequence data\",\n        \"sbi: A toolkit for simulation-based inference\",\n        \"Neural posterior estimation for exoplanetary atmospheric retrieval\",\n        \"Robust neural posterior estimation and statistical model criticism\",\n        \"Discriminative calibration\",\n        \"Diagnostics for conditional density models and bayesian inference algorithms\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}