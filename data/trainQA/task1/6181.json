{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Memory-efficient model-based deep learning with convergence and robustness guarantees\n\nPaper Abstract: Abstract Computational imaging has been revolutionized by compressed sensing algorithms, which offer guaranteed uniqueness, convergence, and stability properties. Model-based deep learning methods that combine imaging physics with learned regularization priors have emerged as more powerful alternatives for image recovery. The main focus of this paper is to introduce a memory efficient model-based algorithm with similar theoretical guarantees as CS methods. The proposed iterative algorithm alternates between a gradient descent involving the score function and a conjugate gradient algorithm to encourage data consistency. The score function is modeled as a monotone convolutional neural network. Our analysis shows that the monotone constraint is necessary and sufficient to enforce the uniqueness of the fixed point in arbitrary inverse problems. In addition, it also guarantees the convergence to a fixed point, which is robust to input perturbations. We introduce two implementations of the proposed MOL framework, which differ in the way the monotone property is imposed. The first approach enforces a strict monotone constraint, while the second one relies on an approximation. The guarantees are not valid for the second approach in the strict sense. However, our empirical studies show that the convergence and robustness of both approaches are comparable, while the less constrained approximate implementation offers better performance. The proposed deep equilibrium formulation is significantly more memory efficient than unrolled methods, which allows us to apply it to 3D or 2D+time problems that current unrolled algorithms cannot handle.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Model-based image reconstruction for MRI\",\n        \"Statistical image reconstruction for polyenergetic X-ray computed tomography\",\n        \"Dynamic PET reconstruction using wavelet regularization with adapted basis functions\",\n        \"Model-based 2.5-D deconvolution for extended depth of field in brightfield microscopy\",\n        \"Sparse MRI: The application of compressed sensing for rapid MR imaging\",\n        \"Plug-and-play priors for model based reconstruction\",\n        \"Plug-and-play unplugged: Optimization-free reconstruction using consensus equilibrium\",\n        \"The little engine that could: Regularization by denoising (RED)\",\n        \"An online plug-and-play algorithm for regularized image reconstruction\",\n        \"Plug-and-play methods provably converge with properly trained denoisers\",\n        \"Scalable plug-and-play ADMM with convergence guarantees\",\n        \"BM3D image denoising with shape-adaptive principal component analysis\",\n        \"Plug-and-Play Methods for Integrating Physical and Learned Models in Computational Imaging: Theory, algorithms, and applications\",\n        \"Learning fast approximations of sparse coding\",\n        \"Learning a variational network for reconstruction of accelerated MRI data\",\n        \"MoDL: Model-based deep learning architecture for inverse problems\",\n        \"FISTA-net: Learning a fast iterative shrinkage thresholding network for inverse problems in imaging\",\n        \"A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image Reconstruction\",\n        \"Deep ADMM-Net for compressive sensing MRI\",\n        \"Deep learning techniques for inverse problems in imaging\",\n        \"Training deep nets with sublinear memory cost\",\n        \"Memory-efficient learning for large-scale computational imaging\",\n        \"Deep equilibrium models\",\n        \"Deep equilibrium architectures for inverse problems in imaging\",\n        \"On instabilities of deep learning in image reconstruction and the potential costs of AI\",\n        \"Measuring robustness in deep learning based compressive sensing\",\n        \"Solving inverse problems with deep neural networks-robustness included\",\n        \"Machine learning for image reconstruction\",\n        \"U-net: Convolutional networks for biomedical image segmentation\",\n        \"A fast alternating direction method for TVL1-L2 signal reconstruction from partial Fourier data\",\n        \"An iterative thresholding algorithm for linear inverse problems with a sparsity constraint\",\n        \"A fast iterative shrinkage-thresholding algorithm for linear inverse problems\",\n        \"ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing\",\n        \"Primer on monotone operator methods\",\n        \"Spectral normalization for generative adversarial networks\",\n        \"CLIP: Cheap Lipschitz training of neural networks\",\n        \"Learning maximally monotone operators for image recovery\",\n        \"Lipschitz constant estimation of neural networks via sparse polynomial optimization\",\n        \"Exactly computing the local lipschitz constant of relu networks\",\n        \"SENSE: sensitivity encoding for fast MRI\",\n        \"Image quality assessment: from error visibility to structural similarity\",\n        \"An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement\",\n        \"fastMRI: An open dataset and benchmarks for accelerated MRI\",\n        \"Compressed sensing singleâ€“breath-hold CMR for fast quantification of LV function, volumes, and mass\",\n        \"PSF model-based reconstruction with sparsity constraint: Algorithm and application to real-time cardiac MRI\",\n        \"Accelerating cardiac cine MRI using a deep learning-based ESPIRiT reconstruction\",\n        \"Free-breathing Accelerated Cardiac MRI Using Deep Learning: Validation in Children and Young Adults\",\n        \"OCMR (v1. 0)â€“Open-Access Multi-Coil k-Space Dataset for Cardiovascular Magnetic Resonance Imaging\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Memory-efficient model-based deep learning with convergence and robustness guarantees, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}