{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Learning Task-General Representations with Generative Neuro-Symbolic Modeling\n\nPaper Abstract: Abstract People can learn rich, general-purpose conceptual representations from only raw perceptual inputs. Current machine learning approaches fall well short of these human standards, although different modeling traditions often have complementary strengths. Symbolic models can capture the compositional and causal knowledge that enables flexible generalization, but they struggle to learn from raw inputs, relying on strong abstractions and simplifying assumptions. Neural network models can learn directly from raw data, but they struggle to capture compositional and causal structure and typically must retrain to tackle new tasks. We bring together these two traditions to learn generative models of concepts that capture rich compositional and causal structure, while learning from raw data. We develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. The distributions of parts (strokes), and correlations between parts, are modeled with neural network subroutines, allowing the model to learn directly from raw data and express nonparametric statistical relationships. We apply our model to the Omniglot challenge of human-level concept learning, using a background set of alphabets to learn an expressive prior distribution over character drawings. In a subsequent evaluation, our GNS model uses probabilistic inference to learn rich conceptual representations from a single training image that generalize to 4 unique tasks, succeeding where previous work has fallen short.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Perception of dynamic information in static handwritten forms\",\n        \"Building machines that learn and think for themselves\",\n        \"Robustfill: Neural program learning under noisy i/o\",\n        \"Learning to infer graphics programs from hand-drawn images\",\n        \"Attend, infer, repeat: Fast scene understanding with generative models\",\n        \"Generating new concepts with hybrid neuro-symbolic models\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"Synthesizing programs for images using reinforced adversarial learning\",\n        \"Few-shot learning with graph neural networks\",\n        \"Bayesian Data Analysis (3rd ed.)\",\n        \"Neural networks and the bias/variance dilemma\",\n        \"A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs\",\n        \"A rational analysis of rule-based concept learning\",\n        \"Learning a theory of causality\",\n        \"Concepts in a probabilistic language of thought\",\n        \"Generating sequences with recurrent neural networks\",\n        \"Neural turing machines\",\n        \"Multi-object representation learning with iterative variational inference\",\n        \"DRAW: A recurrent neural network for image generation\",\n        \"A neural representation of sketch drawings\",\n        \"The Variational Homoencoder: Learning to learn high capacity generative models from few examples\",\n        \"Learning to learn generative programs with Memoised Wake-Sleep\",\n        \"When writing impairs reading: Letter perceptionâ€™s susceptibility to motor interference\",\n        \"Structured statistical models of inductive reasoning\",\n        \"Stacked Capsule Autoencoders\",\n        \"Picture: A probabilistic programming language for scene perception\",\n        \"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\",\n        \"People infer recursive visual concepts from just a few examples\",\n        \"Human-level concept learning through probabilistic program induction\",\n        \"Building machines that learn and think like people\",\n        \"The Omniglot challenge: A 3-year progress report\",\n        \"Deep learning\",\n        \"Visual presentation of single letters activates a premotor area involved in writing\",\n        \"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision\",\n        \"The Algebraic Mind: Integrating Connectionism and Cognitive Science\",\n        \"Letting structure emerge: Connectionist and dynamical systems approaches to cognition\",\n        \"The big book of concepts\",\n        \"The role of theories in conceptual coherence\",\n        \"Learning compositional rules via neural program synthesis\",\n        \"The learnability of abstract syntactic principles\",\n        \"The logical primitives of thought: Empirical foundations for compositional cognitive models\",\n        \"Neural programmer-interpreters\",\n        \"One-Shot generalization in deep generative models\",\n        \"Attentive recurrent comparators\",\n        \"Prototypical networks for few-shot learning\",\n        \"Learning Structured Generative Concepts\",\n        \"How to grow a mind: Statistics, structure, and abstraction\",\n        \"HOUDINI: Lifelong learning as program synthesis\",\n        \"Matching networks for one shot learning\",\n        \"Neural-symbolic VQA: Disentangling reasoning from vision and language understanding\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Learning Task-General Representations with Generative Neuro-Symbolic Modeling, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}