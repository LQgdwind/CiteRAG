{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 46 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: You Only Spike Once: Improving Energy-Efficient Neuromorphic Inference to ANN-Level Accuracy\n\nPaper Abstract: Abstract In the past decade, advances in Artificial Neural Networks (ANNs) have allowed them to perform extremely well for a wide range of tasks. In fact, they have reached human parity when performing image recognition, for example. Unfortunately, the accuracy of these ANNs comes at the expense of a large number of cache and/or memory accesses and compute operations. Spiking Neural Networks (SNNs), a type of neuromorphic, or brain-inspired network, have recently gained significant interest as power-efficient alternatives to ANNs, because they are sparse, accessing very few weights, and typically only use addition operations instead of the more power-intensive multiply-and-accumulate (MAC) operations. The vast majority of neuromorphic hardware designs support rate-encoded SNNs, where the information is encoded in spike rates. Rate-encoded SNNs could be seen as inefficient as an encoding scheme because it involves the transmission of a large number of spikes. A more efficient encoding scheme, Time-To-First-Spike (TTFS) encoding, encodes information in the relative time of arrival of spikes. While TTFS-encoded SNNs are more efficient than rate-encoded SNNs, they have, up to now, performed poorly in terms of accuracy compared to previous methods. Hence, in this work, we aim to overcome the limitations of TTFS-encoded neuromorphic systems. To accomplish this, we propose: (1) a novel optimization algorithm for TTFS-encoded SNNs converted from ANNs and (2) a novel hardware accelerator for TTFS-encoded SNNs, with a scalable and low-power design. Overall, our work in TTFS encoding and training improves the accuracy of SNNs to achieve state-of-the-art results on MNIST MLPs, while reducing power consumption by 1.46Ã—\\bm{\\times} over the state-of-the-art neuromorphic hardware.\n\n{retrieved_text}\n\nGenerate exactly 46 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 46\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\",\n        \"ImageNet large scale visual recognition challenge\",\n        \"EfficientNet: Rethinking model scaling for convolutional neural networks\",\n        \"Searching for MobileNetV3\",\n        \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <<1MB model size.\",\n        \"A million spiking-neuron integrated circuit with a scalable communication network and interface\",\n        \"Introduction to spiking neural networks: Information processing, learning and applications\",\n        \"Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition\",\n        \"Spiking Neuron Models: Single Neurons, Populations, Plasticity\",\n        \"Pattern recognition computation using action potential timing for stimulus representation\",\n        \"The evidence for neural information processing with precise spike-times: A survey\",\n        \"Deep learning with spiking neurons: Opportunities and challenges\",\n        \"Programming spiking neural networks on Intel Loihi\",\n        \"Minitaur, an event-driven FPGA-based spiking network accelerator\",\n        \"Fast classification using sparsely active spiking networks\",\n        \"TrueNorth: Design and tool flow of a 65 mW 1 million neuron programmable neurosynaptic chip\",\n        \"Temporal coding in spiking neural networks with alpha synaptic function\",\n        \"Conversion of analog to spiking neural networks using sparse temporal coding\",\n        \"A review of learning in biologically plausible spiking neural networks\",\n        \"Phase-of-firing code\",\n        \"A quantitative description of membrane current and its application to conduction and excitation in nerve\",\n        \"Chapter 12 a framework for spiking neuron models: The spike response model\",\n        \"How spike generation mechanisms determine the neuronal response to fluctuating inputs\",\n        \"Slayer: Spike layer error reassignment in time\",\n        \"Direct training for spiking neural networks: faster, larger, better\",\n        \"Surrogate gradient learning in spiking neural networks\",\n        \"Conversion of continuous-valued deep networks to efficient event-driven networks for image classification\",\n        \"Supervised learning based on temporal coding in spiking neural networks\",\n        \"Spike-timing-dependent back propagation in deep spiking neural networks\",\n        \"An ultra-high energy-efficient reconfigurable processor for deep neural networks with binary/ternary weights in 28nm CMOS\",\n        \"Spiking neural networks hardware implementations and challenges: A survey\",\n        \"Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing\",\n        \"Spatio-temporal backpropagation for training high-performance spiking neural networks\",\n        \"Error-backpropagation in temporally encoded networks of spiking neurons\",\n        \"Robust spike-train learning in spike-event based weight update\",\n        \"Training spiking neural networks for cognitive tasks: A versatile framework compatible with various temporal codes\",\n        \"OpenSMART: Single-cycle multi-hop noc generator in BSV and Chisel\",\n        \"VTA: an open hardware-software stack for deep learning\",\n        \"Decoupled access/execute computer architectures\",\n        \"MNIST handwritten digit database\",\n        \"Backpropagation for energy-efficient neuromorphic computing\",\n        \"Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches\",\n        \"Spinnaker: mapping neural networks onto a massively-parallel chip multiprocessor\",\n        \"Bridge the gap between neural networks and neuromorphic hardware with a neural network compiler\",\n        \"Shenjing: A low power reconfigurable neuromorphic accelerator with partial-sum and spike networks-on-chip\",\n        \"Learning multiple layers of features from tiny images\"\n    ],\n    \"reasoning\": \"Selected these 46 references because they represent foundational work and recent advances in You Only Spike Once: Improving Energy-Efficient Neuromorphic Inference to ANN-Level Accuracy, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}