{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 118 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies\n\nPaper Abstract: Abstract. As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.\n\n{retrieved_text}\n\nGenerate exactly 118 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 118\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations\",\n        \"Sanity checks for saliency maps\",\n        \"Evaluating saliency map explanations for convolutional neural networks: a user study\",\n        \"Permutation importance: a corrected feature importance measure\",\n        \"Power to the people: The role of humans in interactive machine learning\",\n        \"Machine bias\",\n        \"Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency\",\n        \"System and method for predicting consumer credit risk using income risk based credit score\",\n        \"FactSheets: Increasing trust in AI services through supplier’s declarations of conformity\",\n        \"Investigating user confidence for uncertainty presentation in predictive decision making\",\n        \"Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance\",\n        \"Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff\",\n        \"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance\",\n        \"A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy\",\n        \"’It’s Reducing a Human Being to a Percentage’ Perceptions of Justice in Algorithmic Decisions\",\n        \"Human-Centric Justification of Machine Learning Predictions.\",\n        \"The values encoded in machine learning research\",\n        \"Random forests\",\n        \"Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making in child welfare services\",\n        \"Proxy tasks and subjective measures can be misleading in evaluating explainable ai systems\",\n        \"To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making\",\n        \"Martin Wattenberg\",\n        \"Hello AI: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making\",\n        \"Do explanations make VQA models more predictable to a human?\",\n        \"F Maxwell Harper\",\n        \"Felicitas Buchner\",\n        \"Anne Spencer Ross\",\n        \"Construct validity in psychological tests\",\n        \"Devleena Das and Sonia Chernova\",\n        \"Riccardo Fogliato\",\n        \"Algorithm aversion: People erroneously avoid algorithms after seeing them err\",\n        \"Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them\",\n        \"Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment\",\n        \"Rachel KE Bellamy\",\n        \"Towards a rigorous science of interpretable machine learning\",\n        \"The accuracy, fairness, and limits of predicting recidivism\",\n        \"Jamie C Rasmussen\",\n        \"What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play\",\n        \"All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously\",\n        \"Assessing the Local Interpretability of Machine Learning Models\",\n        \"Yarin Gal and Zoubin Ghahramani\",\n        \"Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience\",\n        \"Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI\",\n        \"The Flaws of Policies Requiring Human Oversight of Government Algorithms\",\n        \"The principles and limits of algorithm-in-the-loop decision making\",\n        \"Grouped variable importance with random forests and application to multiple functional data analysis\",\n        \"Correlation and variable importance in random forests\",\n        \"Human decision making with machine assistance: An experiment on bailing and jailing\",\n        \"Christine Jacinto\",\n        \"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?\",\n        \"Visual analytics in deep learning: An interrogative survey for the next frontiers\",\n        \"Jennifer Wortman Vaughan\",\n        \"Attention is not explanation\",\n        \"xGEMs: Generating examplars to explain black-box models\",\n        \"The limits of human predictions of recidivism\",\n        \"and Jennifer Wortman Vaughan\",\n        \"Jessica R Hullman\",\n        \"Consumer credit-risk models via machine-learning algorithms\",\n        \"Impact of a deep learning assistant on the histopathologic classification of liver cancer\",\n        \"Pang Wei Koh and Percy Liang\",\n        \"Margaret Burnett\",\n        \"Catalin-Mihai Barbu\",\n        \"An evaluation of the human-interpretability of explanation\",\n        \"Vivian Lai and Chenhao Tan\",\n        \"Alexander Pritzel\",\n        \"A quantitative approach to content validity\",\n        \"Trust in automation: Designing for appropriate reliance\",\n        \"Co-Design and Evaluation of an Intelligent Decision Support System for Stroke Rehabilitation Assessment\",\n        \"Daniel PP Siewiorek\",\n        \"Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation\",\n        \"Explanation-Based Human Debugging of NLP Models: A Survey\",\n        \"Arvind Satyanarayan\",\n        \"Questioning the AI: Informing Design Practices for Explainable AI User Experiences\",\n        \"The mythos of model interpretability\",\n        \"Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making\",\n        \"Algorithm appreciation: People prefer algorithmic to human judgment\",\n        \"Scott M Lundberg and Su-In Lee\",\n        \"An integrative model of organizational trust\",\n        \"When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making\",\n        \"Inioluwa Deborah Raji\",\n        \"David Piorkowski\",\n        \"How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation\",\n        \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\",\n        \"Donald R Honeycutt\",\n        \"Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning\",\n        \"A Slow Algorithm Improves Users’ Assessments of the Algorithm’s Accuracy\",\n        \"Manipulating and measuring model interpretability\",\n        \"Adult Data Set\",\n        \"Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans\",\n        \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n        \"Robots in society, society in robots\",\n        \"Simple means to improve the interpretability of regression coefficients\",\n        \"Predicting recidivism in north carolina, 1978 and 1980\",\n        \"Is attention interpretable?\",\n        \"Maximilian Granz\",\n        \"Jordan Boyd-Graber\",\n        \"Melissa Birchfield\",\n        \"Mennatallah El-Assady\",\n        \"Should We Trust (X) AI? Design Dimensions for Structured Experimental Evaluations\",\n        \"Progressive disclosure: designing for effective transparency\",\n        \"Lending Club\",\n        \"Insights into human-agent teaming: Intelligent agent transparency and uncertainty\",\n        \"Conditional variable importance for random forests\",\n        \"Supreme Court of Wisconsin\",\n        \"Martijn Millecamp\",\n        \"Investigating human+ machine complementarity for recidivism predictions\",\n        \"Mark van der Wilk\",\n        \"State Court Processing Statistics, 1990-2009: Felony Defendants in Large Urban Counties\",\n        \"How to Evaluate Trust in AI-Assisted Decision Making? A Survey of Empirical Methodologies\",\n        \"A human-grounded evaluation of shap for alert processing\",\n        \"Attention is not not explanation\",\n        \"A Survey of Human-in-the-loop for Machine Learning\",\n        \"Insurance premium prediction via gradient tree-boosted Tweedie compound Poisson models\",\n        \"Representer point selection for explaining deep neural networks\",\n        \"Shlomo Berkovsky\",\n        \"Advances in Variational Inference\",\n        \"Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making\",\n        \"Reinforcement learning trees\"\n    ],\n    \"reasoning\": \"Selected these 118 references because they represent foundational work and recent advances in Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}