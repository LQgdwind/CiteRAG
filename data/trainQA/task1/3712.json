{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature\n\nPaper Abstract: Abstract. Information overload is a prevalent challenge in many high-value domains. A prominent case in point is the explosion of the biomedical literature on COVID-19, which swelled to hundreds of thousands of papers in a matter of months. In general, biomedical literature expands by two papers every minute, totalling over a million new papers every year. Search in the biomedical realm, and many other vertical domains is challenging due to the scarcity of direct supervision from click logs. Self-supervised learning has emerged as a promising direction to overcome the annotation bottleneck. We propose a general approach for vertical search based on domain-specific pretraining and present a case study for the biomedical domain. Despite being substantially simpler and not using any relevance labels for training or development, our method performs comparably or better than the best systems in the official TREC-COVID evaluation, a COVID-related biomedical search competition. Using distributed computing in modern cloud infrastructure, our system can scale to tens of millions of articles on PubMed and has been deployed as Microsoft Biomedical Search, a new search experience for biomedical literature: https://aka.ms/biomedsearch.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Publicly Available Clinical BERT Embeddings\",\n        \"Modern information retrieval\",\n        \"SciBERT: A Pretrained Language Model for Scientific Text\",\n        \"gevent\",\n        \"The unified medical language system (UMLS): integrating biomedical terminology\",\n        \"Language models are few-shot learners\",\n        \"UnitedQA: A Hybrid Approach for Open Domain Question Answering\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Unified language model pre-training for natural language understanding and generation\",\n        \"Elasticsearch: the definitive guide: a distributed real-time search and analytics engine\",\n        \"Domain-specific language model pretraining for biomedical natural language processing\",\n        \"Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks\",\n        \"Long short-term memory\",\n        \"locust\",\n        \"ClinicalBERT: Modeling clinical notes and predicting hospital readmission\",\n        \"Learning deep structured semantic models for web search using clickthrough data\",\n        \"Dense passage retrieval for open-domain question answering\",\n        \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\",\n        \"Natural Questions: A Benchmark for Question Answering Research\",\n        \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining\",\n        \"Pretrained transformers for text ranking: Bert and beyond\",\n        \"Medical subject headings (MeSH)\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"SLEDGE: A Simple Yet Effective Baseline for Coronavirus Scientific Knowledge Search\",\n        \"SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search\",\n        \"MS MARCO: A human generated machine reading comprehension dataset\",\n        \"Passage Re-ranking with BERT\",\n        \"Language models are unsupervised multitask learners\",\n        \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n        \"TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19\",\n        \"Relevance weighting of search terms\",\n        \"Flask\",\n        \"Neural Machine Translation of Rare Words with Subword Units\",\n        \"Enhancing clinical concept extraction with contextual embeddings\",\n        \"An overview of microsoft academic service (mas) and applications\",\n        \"An evaluation of two commercial deep learning-based information retrieval systems for covid-19 literature\",\n        \"Attention is all you need\",\n        \"TREC-COVID: constructing a pandemic information retrieval test collection\",\n        \"Superglue: A stickier benchmark for general-purpose language understanding systems\",\n        \"GLUE: A Multi-task Benchmark and Analaysis Platform for Natural Language Understanding\",\n        \"Microsoft Academic Graph: When experts are not enough\",\n        \"A Review of Microsoft Academic Services for Science of Science Studies\",\n        \"Cord-19: The covid-19 open research dataset\",\n        \"CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web to Special Domain Search\",\n        \"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\",\n        \"Simple applications of BERT for ad hoc document retrieval\",\n        \"Applying BERT to document retrieval with birch\",\n        \"Selective weak supervision for neural information retrieval\",\n        \"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}