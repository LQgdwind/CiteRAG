{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: B’MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory\n\nPaper Abstract: Abstract We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (“context” in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B’MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory “in-context,” permanent structural memory “in-weights,” fading memory “in-state,” and long-term eidetic memory “in-storage” by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B’MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B’MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B’MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we test whether models trained inductively on a-priori bounded sequences (up to 8K tokens) can still perform transductive inference on sequences many-fold longer. B’MOJO’s ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"On the learnability of physical concepts: Can a neural network understand what’s real?\",\n        \"Simple linear attention language models balance the recall-throughput tradeoff\",\n        \"xlstm: Extended long short-term memory\",\n        \"Piqa: Reasoning about physical commonsense in natural language\",\n        \"On the algebraic structure of bilinear systems. theory and applica-tions of variable structure systems\",\n        \"Language models are few-shot learners\",\n        \"Transductive inference for estimating values of functions\",\n        \"Think you have solved question answering? try arc, the ai2 reasoning challenge\",\n        \"Flashattention: Fast and memory-efficient exact attention with io-awareness\",\n        \"Griffin: Mixing gated linear recurrences with local attention for efficient language models\",\n        \"Realization and structure theory of bilinear dynamical systems\",\n        \"Bilinear control systems: matrices in action, volume 169\",\n        \"Inductive learning of transductive inference:.\",\n        \"Multi-modal hallucination control by visual information grounding\",\n        \"Hungry hungry hippos: Towards language modeling with state space models\",\n        \"A framework for few-shot language model evaluation\",\n        \"Mamba: Linear-time sequence modeling with selective state spaces\",\n        \"Training compute-optimal large language models\",\n        \"Nonlinear control systems: an introduction\",\n        \"Stochastic processes and filtering theory\",\n        \"Mistral 7b\",\n        \"Linear systems, volume 156\",\n        \"A new approach to linear filtering and prediction problems\",\n        \"Scaling laws for neural language models\",\n        \"Bilinear and nonlinear realizations of input-output maps\",\n        \"Jamba: A hybrid transformer-mamba language model\",\n        \"On the stochastic realization problem\",\n        \"On a measure of lack of fit in time series models\",\n        \"System identification toolbox: User’s guide\",\n        \"Megalodon: Efficient llm pretraining and inference with unlimited context length\",\n        \"Results on the non existence of finite dimensional filters\",\n        \"Landmark attention: Random-access infinite context length for transformers\",\n        \"In-context learning and induction heads\",\n        \"The lambada dataset: Word prediction requiring a broad discourse context\",\n        \"Mechanistic design and scaling of hybrid architectures\",\n        \"Winogrande: An adversarial winograd schema challenge at scale\",\n        \"SCROLLS: Standardized CompaRison over long language sequences\",\n        \"A formal theory of inductive inference. i.\",\n        \"Test-time training with self-supervision for generalization under distribution shifts\",\n        \"Subspace identification for linear systems: Theory—Implementation—Applications\",\n        \"Transductive inference and semi-supervised learning\",\n        \"A technique for high-performance data compression\",\n        \"Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications\",\n        \"On a matrix riccati equation of stochastic control\",\n        \"Efficient streaming language models with attention sinks\",\n        \"Gated linear attention transformers with hardware-efficient training\",\n        \"Megabyte: Predicting million-byte sequences with multiscale transformers\",\n        \"Stacked residuals of dynamic layers for time series anomaly detection\",\n        \"Train/test-time adaptation with retrieval\",\n        \"A novel deep neural network architecture for non-linear system identification\",\n        \"Hellaswag: Can a machine really finish your sentence?\",\n        \"Understanding deep learning (still) requires rethinking generalization\",\n        \"Compression of individual sequences via variable-rate coding\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in B’MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}