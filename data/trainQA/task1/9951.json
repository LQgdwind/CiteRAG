{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings.\n\nPaper Abstract: Abstract Since the release of ChatGPT, and subsequently GPT-4, there has been an increased interest in AI safety and alignment research. As Large Language Models are deployed within Artificial Intelligence systems, that are increasingly integrated with human society, it becomes more important than ever to study their internal structures. Higher level abilities of LLMs such as GPT-3.5 emerge in large part due to informative language representations they induce from raw text data during pre-training on trillions of words. These embeddings exist in vector spaces of several thousand dimensions, and their processing involves mapping between multiple vector spaces, with total number of parameters on the order of trillions. Furthermore, these language representations are induced by gradient optimization, resulting in a black box system that is hard to interpret. In this paper, we take a look at the topological structure of neuronal activity in the “brain” of Chat-GPT’s foundation language model, and analyze it with respect to a metric representing the notion of fairness. We develop a novel approach to visualize GPT’s moral dimensions. We first compute a fairness metric, inspired by social psychology literature, to identify factors that typically influence fairness assessments in humans, such as legitimacy, need, and responsibility. Subsequently, we summarize the manifold’s shape using a lower-dimensional simplicial complex, whose topology is derived from this metric. We color it with a heat map associated with this fairness metric, producing human-readable visualizations of the high-dimensional sentence manifold. Our results show that sentence embeddings based on GPT-3.5 can be decomposed into two submanifolds corresponding to fair and unfair moral judgments. This indicates that GPT-based language models develop a moral dimension within their representation spaces and induce an understanding of fairness during their training process.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Attention is all you need\",\n        \"Moral concerns are differentially observable in language\",\n        \"Communication and collective action: Language and the evolution of human cooperation\",\n        \"integrating wordnet for multiple sense embeddings in vector semantics\",\n        \"Associative judgment and vector space semantics\",\n        \"Vector space models of word meaning and phrase meaning: A survey\",\n        \"On approximation of concept similarity measure in description logic elh with pre-trained word embedding\",\n        \"Using word embeddings to generate data-driven human agent decision-making from natural language\",\n        \"investigating language universal and specific properties in word embeddings\",\n        \"glove: Global vectors for word representation\",\n        \"word2vec parameter learning explained\",\n        \"bert has a moral compass: Improvements of ethical and moral values of machines\",\n        \"semantics derived automatically from language corpora contain human-like moral choices\",\n        \"Word vector embeddings hold social ontological relations capable of reflecting meaningful fairness assessments\",\n        \"the geometry of culture: Analyzing the meanings of class through word embeddings\",\n        \"Patterns in language: Text analysis of government reports on the irish industrial school system with word embedding\",\n        \"moral foundations theory: The pragmatic validity of moral pluralism\",\n        \"Moral foundations twitter corpus: A collection of 35k tweets annotated for moral sentiment\",\n        \"enhancing the measurement of social effects by capturing morality\",\n        \"incorporating the measurement of moral foundations theory into analyzing stances on controversial topics\",\n        \"Moralstrength: Exploiting a moral lexicon and embedding similarity for moral foundations prediction\",\n        \"linguistic inquiry and word count: Liwc 2001\",\n        \"Dictator games: A meta study\",\n        \"exploring the meaning of significance in experimental economics (no. 2013–32; discussion papers)\",\n        \"The effects of the take-option in dictator-game experiments: A comment on engel’s (2011) meta-study\",\n        \"Promoting helping behavior with framing in dictator games\",\n        \"A neural signature of fairness in altruism: A game of theta?\",\n        \"altruistic or strategic considerations: A meta-analysis on the ultimatum and dictator games\",\n        \"Personality change over 40 years of adulthood: Hierarchical linear modeling analyses of two longitudinal samples\",\n        \"personality and social relations\",\n        \"Less power or powerless? egocentric empathy gaps and the irony of having little versus no power in social decision making\",\n        \"Fair and unfair punishers coexist in the ultimatum game\",\n        \"no harm done? an experimental approach to the nonidentity problem\",\n        \"Ethical orientation versus short-term ethics training: Effects on ethical behavior in the prisoner’s dilemma game and dictator game experiments\",\n        \"Cruel to be kind but not cruel for cash: Harm aversion in the dictator game\",\n        \"The tragedy of the commons revisited: The importance of group decision-making\",\n        \"Modeling the commons as a game with vector payoffs\",\n        \"reward and punishment\",\n        \"Empathic joy and the empathy-altruism hypothesis\",\n        \"Explaining altruistic sharing in the dictator game: The role of affective empathy, cognitive empathy, and justice sensitivity\",\n        \"Individual differences in decision making: Drive and reward responsiveness affect strategic bargaining in economic games\",\n        \"Fairness and cooperation are rewarding: Evidence from social cognitive neuroscience\",\n        \"Shifting the blame: On delegation and responsibility\",\n        \"the evolution of altruistic punishment\",\n        \"what do humans maximize?\",\n        \"Cooperation, reciprocity and punishment in fifteen small-scale societies\",\n        \"Evolutionary explanations of emotions\",\n        \"human cooperation and its underlying mechanisms\",\n        \"Responsibility: The tie that binds\",\n        \"Self-interest through delegation: An additional rationale for the principal-agent relationship\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}