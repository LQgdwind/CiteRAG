{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Discovering and Exploiting Sparse Rewards in a Learned Behavior Space\n\nPaper Abstract: Abstract Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Exploration algorithms have been proposed that require the definition of a low-dimension behavior space, in which the behavior generated by the agentâ€™s policy can be represented. The need to design a priori this space such that it is worth exploring is a major limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alternating two-step process. In the first step, STAX builds a repertoire of diverse policies while learning a low-dimensional representation of the high-dimensional observations generated during the policies evaluation. In the exploitation step, emitters optimize the performance of the discovered rewarding solutions. Experiments conducted on three different sparse reward environments show that STAX performs comparably to existing baselines while requiring much less prior information about the task as it autonomously builds the behavior space it explores.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Hindsight experience replay\",\n        \"A survey on intrinsic motivation in reinforcement learning\",\n        \"Active learning of inverse models with intrinsically motivated goal exploration in robots\",\n        \"Unifying count-based exploration and intrinsic motivation\",\n        \"Dota 2 with large scale deep reinforcement learning\",\n        \"Exploration by random network distillation\",\n        \"Qd-rl: Efficient mixing of quality and diversity in reinforcement learning\",\n        \"Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms\",\n        \"Autonomous skill discovery with quality-diversity and unsupervised descriptors\",\n        \"Multi-emitter map-elites: improving quality, diversity and data efficiency with heterogeneous sets of emitters\",\n        \"Robots that can adapt like animals\",\n        \"Quality and diversity optimization: A unifying modular framework\",\n        \"A fast and elitist multiobjective genetic algorithm: Nsga-ii\",\n        \"Go-explore: a new approach for hard-exploration problems\",\n        \"Diversity is all you need: Learning skills without a reward function\",\n        \"Covariance matrix adaptation for the rapid illumination of behavior space\",\n        \"Intrinsically motivated goal exploration processes with automatic curriculum learning\",\n        \"Are quality diversity algorithms better at generating stepping stones than objective-based search?\",\n        \"Unsupervised behaviour discovery with quality-diversity optimisation\",\n        \"An analysis of phenotypic diversity in multi-solution optimization\",\n        \"Prediction of neural network performance by phenotypic modeling\",\n        \"The cma evolution strategy: A tutorial\",\n        \"Correlation matrix distance, a meaningful measure for evaluation of non-stationary mimo channels\",\n        \"A simple sequentially rejective multiple test procedure\",\n        \"Learning to utilize shaping rewards: A new approach of reward shaping\",\n        \"Adam: A method for stochastic optimization\",\n        \"Self-normalizing neural networks\",\n        \"Curiosity driven exploration of learned disentangled goal spaces\",\n        \"Exploiting open-endedness to solve problems through the search for novelty\",\n        \"Evolving a diversity of virtual creatures through novelty search and local competition\",\n        \"Transforming exploratory creativity with delenox,.\",\n        \"Online-learning and planning in high dimensions with finite element goal babbling\",\n        \"On a test of whether one of two random variables is stochastically larger than the other\",\n        \"Reward functions for accelerated learning\",\n        \"Illuminating search spaces by mapping elites\",\n        \"Visual reinforcement learning with imagined goals\",\n        \"Policy invariance under reward transformations: Theory and application to reward shaping\",\n        \"What is intrinsic motivation? a typology of computational approaches\",\n        \"Billiard\",\n        \"Sparse reward exploration via novelty search and emitters\",\n        \"Unsupervised learning and exploration of reachable outcome space\",\n        \"Quality diversity: A new frontier for evolutionary computation\",\n        \"Br-ns: an archive-less approach to novelty search\",\n        \"Combining evolution and deep reinforcement learning for policy search: a survey\",\n        \"Understanding the behavior of reinforcement learning agents\",\n        \"Reinforcement learning: An introduction\",\n        \"Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in Discovering and Exploiting Sparse Rewards in a Learned Behavior Space, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}