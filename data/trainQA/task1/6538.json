{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Theoretical Framework for Inference and Learning in Predictive Coding Networks\n\nPaper Abstract: Abstract Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network’s response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity — an algorithm called prospective configuration. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on tasks such as online, few-shot, and continual learning, where brains are known to excel. Despite this promising empirical performance, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Tightening the Biological Constraints on Gradient-Based Predictive Coding\",\n        \"Canonical microcircuits for predictive coding\",\n        \"Automatic differentiation in machine learning: a survey\",\n        \"Variational Algorithms for Approximate Bayesian Inference\",\n        \"Deriving differential target propagation from iterating approximate inverses\",\n        \"Early inference in energy-based models approximates back-propagation\",\n        \"A tutorial on the free-energy framework for modelling perception and learning\",\n        \"The free energy principle for action and perception: A mathematical review\",\n        \"Radical predictive processing\",\n        \"Surfing Uncertainty: Prediction, Action, and the Embodied Mind\",\n        \"The recent excitement about neural networks\",\n        \"On variational message passing on factor graphs\",\n        \"Maximum likelihood from incomplete data via the EM algorithm\",\n        \"Learning and inference in the brain\",\n        \"A theory of cortical responses\",\n        \"Hierarchical models in the brain\",\n        \"The free-energy principle: A unified brain theory?\",\n        \"A free energy principle for the brain\",\n        \"Active inference and agency: Optimal control without cost functions\",\n        \"Graphical models and variational methods\",\n        \"Deep Learning\",\n        \"On automatic differentiation\",\n        \"Stable fixed points of loopy belief propagation are minima of the Bethe free energy\",\n        \"Predictive coding explains binocular rivalry: An epistemological review\",\n        \"Predictive coding\",\n        \"ImageNet classification with deep convolutional neural networks\",\n        \"Factor graphs and the sum-product algorithm\",\n        \"A theoretical framework for back-propagation\",\n        \"Difference target propagation\",\n        \"The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors\",\n        \"Deep predictive coding networks for video prediction and unsupervised learning\",\n        \"Iterative amortized inference\",\n        \"A Theoretical Framework for Target Propagation\",\n        \"Credit assignment in neural networks through deep feedback control\",\n        \"Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?\",\n        \"Predictive Coding Approximates Backprop along Arbitrary Computation Graphs\",\n        \"Relaxing the constraints on predictive coding models\",\n        \"Expectation-Maximization as lower bound maximization\",\n        \"Divergence measures and message passing\",\n        \"On the computational architecture of the neocortex\",\n        \"A view of the EM algorithm that justifies incremental, sparse, and other variants\",\n        \"Biological credit assignment through dynamic inversion of feedforward networks\",\n        \"Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects\",\n        \"Learning representations by back-propagating errors\",\n        \"Feature discovery by competitive learning\",\n        \"Learning on Arbitrary Graph Topologies via Predictive Coding\",\n        \"Predictive Coding Can Do Exact Backpropagation on Convolutional and Recurrent Neural Networks\",\n        \"Reverse Differentiation via Predictive Coding\",\n        \"The cybernetic Bayesian brain\",\n        \"Mastering the game of Go with deep neural networks and tree search\",\n        \"Can the Brain Do Backpropagation?—Exact Implementation of Backpropagation in Predictive Coding Networks\",\n        \"Inferring Neural Activity Before Plasticity: A Foundation for Learning Beyond Backpropagation\",\n        \"Hybrid Predictive Coding: Inferring, Fast and Slow\",\n        \"Attention is all you need\",\n        \"Evaluating the neurophysiological evidence for predictive processing as a model of perception\",\n        \"Bayesian learning via stochastic gradient Langevin dynamics\",\n        \"An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity\",\n        \"Variational message passing\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in A Theoretical Framework for Inference and Learning in Predictive Coding Networks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}