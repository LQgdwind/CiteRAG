{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: Abstract In open-ended continuous environments, robots need to learn multiple parameterised control tasks in hierarchical reinforcement learning. We hypothesise that the most complex tasks can be learned more easily by transferring knowledge from simpler tasks, and faster by adapting the complexity of the actions to the task. We propose a task-oriented representation of complex actions, called procedures, to learn online task relationships and unbounded sequences of action primitives to control the different observables of the environment. Combining both goal-babbling with imitation learning, and active learning with transfer of knowledge based on intrinsic motivation, our algorithm self-organises its learning process. It chooses at any given time a task to focus on; and what, how, when and from whom to transfer knowledge. We show with a simulation and a real industrial robot arm, in cross-task and cross-learner transfer settings, that task composition is key to tackle highly complex tasks. Task decomposition is also efficiently transferred across different embodied learners and by active imitation, where the robot requests just a small amount of demonstrations and the adequate type of information. The robot learns and exploits task dependencies so as to learn tasks of every complexity.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Reinforcement Learning: An introduction\",\n        \"Action representations in robotics: A taxonomy and systematic classification\",\n        \"Learning and development in neural networks: The importance of starting small\",\n        \"Curriculum Learning\",\n        \"A survey on transfer learning\",\n        \"Transfer learning for reinforcement learning domains: A survey\",\n        \"A survey of transfer learning\",\n        \"Primate culture and social learning\",\n        \"Imitation in Animals and Artifacts; Chapter Three Sources of Information in Social Learning\",\n        \"Shared intentionality\",\n        \"The origins of intelligence in children (M. Cook, Trans.)\",\n        \"Intrinsic Motivation and Self-Determination in Human Behavior\",\n        \"Intrinsic Motivation Systems for Autonomous Mental Development\",\n        \"Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)\",\n        \"Intrinsically motivated goal exploration for active motor learning in robots: A case study\",\n        \"Goal Babbling permits Direct Learning of Inverse Kinematics\",\n        \"Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning\",\n        \"CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning\",\n        \"Motor primitives—new data and future questions\",\n        \"Imitating others by composition of primitive actions: A neuro-dynamic model\",\n        \"Learning by Playing Solving Sparse Reward Tasks from Scratch\",\n        \"Behavioral hierarchy: exploration and representation\",\n        \"Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining\",\n        \"Recent advances in hierarchical reinforcement learning\",\n        \"Hierarchical affordance discovery using intrinsic motivation\",\n        \"Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation\",\n        \"Learning a set of interrelated tasks by using sequences of motor policies for a strategic intrinsically motivated learner\",\n        \"Learning from demonstration\",\n        \"Handbook of Robotics; Number 59; Chapter Robot Programming by Demonstration\",\n        \"Learning table tennis with a mixture of motor primitives\",\n        \"Autonomous exploration of motor skills by skill babbling\",\n        \"Integrating reinforcement learning with human demonstrations of varying ability\",\n        \"Experiments in Socially Guided Exploration: Lessons learned in building robots that learn with and without human teachers\",\n        \"Incremental learning of subtasks from unsegmented demonstration\",\n        \"Learning a Set of Interrelated Tasks by Using a Succession of Motor Policies for a Socially Guided Intrinsically Motivated Learner\",\n        \"Learning robot motion control with demonstration and advice-operators\",\n        \"Interactive Policy Learning through Confidence-Based Autonomy\",\n        \"Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner\",\n        \"Designing interactions for robot active learners\",\n        \"Active Learning from Infancy to Childhood; Chapter Curious Learners: How Infants’ Motivation to Learn Shapes and Is Shaped by Infants’ Interactions with the Social World\",\n        \"Infants prefer to imitate a reliable person\",\n        \"CLIC: Curriculum Learning and Imitation for object Control in non-rewarding environments\",\n        \"Effects of social guidance on a robot learning sequences of policies in hierarchical learning\",\n        \"Cognitive developmental robotics: A survey\",\n        \"Developmental Robotics: From Babies to Robots\",\n        \"Socially Guided Intrinsic Motivation for Robot Learning of Motor Skills\",\n        \"Using a tangible interactive tabletop to learn at school: empirical studies in the wild\",\n        \"Learning and generalization of motor skills by learning from demonstration\",\n        \"Hierarchical reinforcement learning with movement primitives\",\n        \"Learning Parameterized Skills\",\n        \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\",\n        \"Mind in Society: the development of higher psychological processes\",\n        \"Interactive Learning Gives the Tempo to an Intrinsically Motivated Robot Learner\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in 1 Introduction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}