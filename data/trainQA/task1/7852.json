{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 525 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Comprehensive Survey of Continual Learning: Theory, Method and Application\n\nPaper Abstract: Abstract To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.\n\n{retrieved_text}\n\nGenerate exactly 525 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 525\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Conditional channel gated networks for task-aware continual learning\",\n        \"Iirc: Incremental implicitly-refined classification\",\n        \"Metaplasticity: tuning synapses and networks for plasticity\",\n        \"Metaplasticity: the plasticity of synaptic plasticity\",\n        \"Life-long disentangled representation learning with cross-domain latent homologies\",\n        \"Gp-tree: A gaussian process classifier for few-shot incremental learning\",\n        \"Continual learning with adaptive weights (claw)\",\n        \"Semantics-driven generative replay for few-shot class incremental learning\",\n        \"Uncertainty-based continual learning with adaptive regularization\",\n        \"Ss-il: Separated softmax for incremental learning\",\n        \"Subspace regularizers for few-shot class incremental learning\",\n        \"Memory aware synapses: Learning what (not) to forget\",\n        \"Online continual learning with maximal interfered retrieval\",\n        \"Expert gate: Lifelong learning with a network of experts\",\n        \"Task-free continual learning\",\n        \"Gradient based sample selection for online continual learning\",\n        \"Learning fast, learning slow: A general continual learning method based on complementary learning system\",\n        \"Varigrow: Variational architecture growing for task-agnostic continual learning based on bayesian novelty\",\n        \"Activity-dependent gating of lateral inhibition in the mouse olfactory bulb\",\n        \"A closer look at memorization in deep networks\",\n        \"Class-incremental learning with cross-space clustering and controlled transfer\",\n        \"Dopaminergic neurons write and update memories with cell-type-specific rules\",\n        \"Mushroom body output neurons encode valence and guide memory-based action selection in drosophila\",\n        \"Few-shot continual active learning by a robot\",\n        \"Eec: Learning to encode and regenerate images for continual learning\",\n        \"Structural components of synaptic plasticity and memory consolidation\",\n        \"Rainbow memory: Continual learning with a memory of diverse samples\",\n        \"Online continual learning on a contaminated data stream with blurry task boundaries\",\n        \"Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models\",\n        \"Learning to continually learn\",\n        \"Il2m: Class incremental learning with dual memory\",\n        \"Scail: Classifier weights scaling for class incremental learning\",\n        \"Generalisation guarantees for continual learning with orthogonal gradient descent\",\n        \"Unifying importance based regularisation methods for continual learning\",\n        \"Comps: Continual meta policy search\",\n        \"Task-aware information routing from common representation space in lifelong learning\",\n        \"Doodle it yourself: Class incremental learning by drawing a few sketches\",\n        \"Continual lifelong learning in natural language processing: A survey\",\n        \"On the effectiveness of lipschitz-driven rehearsal in continual learning\",\n        \"Coresets via bilevel optimization for continual learning and streaming\",\n        \"Class-incremental continual learning into the extended der-verse\",\n        \"Transfer without forgetting\",\n        \"Continual semi-supervised learning through contrastive interpolation consistency\",\n        \"Machine unlearning\",\n        \"Language models are few-shot learners\",\n        \"Dark experience for general continual learning: a strong, simple baseline\",\n        \"New insights on reducing abrupt representation change in online continual learning\",\n        \"Online learned continual compression with adaptive quantization modules\",\n        \"Online fast adaptation and knowledge accumulation (osaka): a new approach to continual learning\",\n        \"Online continual learning with natural distribution shifts: An empirical study with visual data\",\n        \"Continual learning for neural machine translation\",\n        \"Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval\",\n        \"End-to-end incremental learning\",\n        \"Re-evaluating circuit mechanisms underlying pattern separation\",\n        \"Incremental learning in semantic segmentation from image labels\",\n        \"Modeling the background for incremental learning in semantic segmentation\",\n        \"Co2l: Contrastive continual learning\",\n        \"Swad: Domain generalization by seeking flat minima\",\n        \"Task-balanced batch normalization for exemplar-based class-incremental learning\",\n        \"Cpr: Classifier-projection regularization for continual learning\",\n        \"Ssul: Semantic segmentation with unknown label for exemplar-based class-incremental learning\",\n        \"Active bias: Training more accurate neural networks by emphasizing high variance samples\",\n        \"Riemannian walk for incremental learning: Understanding forgetting and intransigence\",\n        \"Using hindsight to anchor past knowledge in continual learning\",\n        \"Continual learning in low-rank orthogonal subspaces\",\n        \"Efficient lifelong learning with a-gem\",\n        \"On tiny episodic memories in continual learning\",\n        \"Class gradient projection for continual learning\",\n        \"Mitigating forgetting in online continual learning via instance-aware parameterization\",\n        \"Few-shot incremental learning for label-to-image translation\",\n        \"Lifelong machine learning\",\n        \"Semantic-aware knowledge distillation for few-shot class-incremental learning\",\n        \"Synthesized feature based few-shot class-incremental learning on a mixture of subspaces\",\n        \"Metafscil: A meta-learning approach for few-shot class incremental learning\",\n        \"Embodied artificial intelligence\",\n        \"Online continual learning from imbalanced data\",\n        \"Continual learning for affective robotics: Why, what and how?\",\n        \"Ongoing in vivo experience triggers synaptic metaplasticity in the neocortex\",\n        \"Coordinated and compartmentalized neuromodulation shapes sensory processing in drosophila\",\n        \"Routing networks with co-training for continual learning\",\n        \"Gan memory with no forgetting\",\n        \"Continual pre-training mitigates forgetting in language and vision\",\n        \"Uncertainty-based out-of-distribution detection requires suitable function space priors\",\n        \"Probing representation forgetting in supervised and unsupervised continual learning\",\n        \"Hippocampal replay of extended experience\",\n        \"A continual learning survey: Defying forgetting in classification tasks\",\n        \"Continual prototype evolution: Learning online from non-stationary data streams\",\n        \"Episodic memory in lifelong language learning\",\n        \"Ratt: Recurrent attention to transient tasks for continual image captioning\",\n        \"Flattening sharpness for dynamic gradient projection memory benefits continual learning\",\n        \"Kernel continual learning\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Learning without memorizing\",\n        \"Diffusion models beat gans on image synthesis\",\n        \"A theoretical analysis of catastrophic forgetting through the ntk overlap matrix\",\n        \"Efficient continual learning ensembles in neural network subspaces\",\n        \"Federated class-incremental learning\",\n        \"Bridging non co-occurrence with unlabeled in-the-wild data for incremental object detection\",\n        \"Few-shot class-incremental learning via relation knowledge distillation\",\n        \"Inability to activate rac1-dependent forgetting contributes to behavioral inflexibility in mutants of multiple autism-risk genes\",\n        \"Plop: Learning without forgetting for continual semantic segmentation\",\n        \"Podnet: Pooled outputs distillation for small-tasks incremental learning\",\n        \"Dytox: Transformers for continual learning with dynamic token expansion\",\n        \"What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?\",\n        \"Complementary roles of basal ganglia and cerebellum in learning and motor control\",\n        \"Efficient perturbation inference and expandable network for continual learning\",\n        \"A survey of embodied ai: From simulators to research tasks\",\n        \"Uncertainty-guided continual learning with bayesian neural networks\",\n        \"Adversarial continual learning\",\n        \"Remembering for the right reasons: Explanations reduce catastrophic forgetting\",\n        \"The turking test: Can language models understand instructions?\",\n        \"Boovae: Boosting approach for continual learning of vae\",\n        \"Memory efficient continual learning with transformers\",\n        \"Parvalbumin+ interneurons obey unique connectivity rules and establish a powerful lateral-inhibition microcircuit in dentate gyrus\",\n        \"Orthogonal gradient descent for continual learning\",\n        \"Overcoming catastrophic forgetting in incremental object detection via elastic response distillation\",\n        \"Pathnet: Evolution channels gradient descent in super neural networks\",\n        \"Self-supervised models are continual learners\",\n        \"Online continual learning under extreme memory constraints\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"Orthogonal representations for robust context-dependent task performance in brains and neural networks\",\n        \"Sharpness-aware minimization for efficiently improving generalization\",\n        \"Continual semantic segmentation leveraging image-level labels and rehearsal\",\n        \"The organization of recent and remote memories\",\n        \"Autonomous agents as embodied ai\",\n        \"Continuous scene representations for embodied ai\",\n        \"Self-supervised training enhances online continual learning\",\n        \"Incremental few-shot instance segmentation\",\n        \"R-dfcil: Relation-guided representation learning for data-free class incremental learning\",\n        \"Towards continual learning for multilingual machine translation via vocabulary substitution\",\n        \"Loss surfaces, mode connectivity, and fast ensembling of dnns\",\n        \"Dynamic dialogue policy for continual reinforcement learning\",\n        \"Continual learning for task-oriented dialogue system with iterative network pruning, expanding and masking\",\n        \"Dynamic vaes with generative replay for continual zero-shot learning\",\n        \"Fast r-cnn\",\n        \"Mixed-privacy forgetting in deep networks\",\n        \"Eternal sunshine of the spotless net: Selective forgetting in deep networks\",\n        \"Continual learning via neural pruning\",\n        \"Continual pre-training of language models for math problem understanding with syntax-aware memory network\",\n        \"Knowledge capture and replay for continual learning\",\n        \"Knowledge distillation: A survey\",\n        \"Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering\",\n        \"Investigating catastrophic forgetting during continual training for neural machine translation\",\n        \"Class-incremental instance segmentation via multi-teacher networks\",\n        \"Not just selection, but exploration: Online class-incremental continual learning via dual view consistency\",\n        \"Adaptive orthogonal projection for batch and online continual learning\",\n        \"Online continual learning through mutual information maximization\",\n        \"Look-ahead meta learning for continual learning\",\n        \"Nispa: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks\",\n        \"Embracing change: Continual learning in deep neural networks\",\n        \"Econet: Effective continual pretraining of language models for event temporal reasoning\",\n        \"Visualizing and understanding the effectiveness of bert\",\n        \"An end-to-end architecture for class-incremental object detection with knowledge distillation\",\n        \"The elements of statistical learning: data mining, inference, and prediction, volume 2\",\n        \"Labelling and optical erasure of synaptic memory traces in the motor cortex\",\n        \"Remind your neural network to prevent catastrophic forgetting\",\n        \"Replay in deep learning: Current approaches and missing biological elements\",\n        \"Exemplar-supported generative reproduction for class incremental learning\",\n        \"Incremental learning in online scenario\",\n        \"Online continual learning via candidates voting\",\n        \"Posterior meta-replay for continual learning\",\n        \"Constrained few-shot class-incremental learning\",\n        \"Flat minima\",\n        \"Lifelong learning via progressive distillation and retrospection\",\n        \"Learning a unified classifier incrementally via rebalancing\",\n        \"Parameter-efficient transfer learning for nlp\",\n        \"Re-evaluating continual learning scenarios: A categorization and case for strong baselines\",\n        \"How well does self-supervised pre-training perform with streaming data?\",\n        \"One pass imagenet\",\n        \"Overcoming catastrophic forgetting for continual learning via model adaptation\",\n        \"Continual learning by using information of each class holistically\",\n        \"Distilling causal effect of data in class-incremental learning\",\n        \"Continual learning for text classification with information disentanglement based regularization\",\n        \"Altersgd: Finding flat minima for continual learning by alternative training\",\n        \"Compacting, picking and growing for unforgetting continual learning\",\n        \"Optimizing reusable knowledge for continual learning via metalearning\",\n        \"On quadratic penalties in elastic weight consolidation\",\n        \"Incremental task learning with incremental rank updates\",\n        \"Origins of cell-type-specific olfactory processing in the drosophila mushroom body circuit\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"Memory-efficient incremental learning through feature adaptation\",\n        \"Selective experience replay for lifelong learning\",\n        \"Averaging weights leads to wider optima and better generalization\",\n        \"Adaptive mixtures of local experts\",\n        \"A simple baseline that questions the use of pretrained-models in continual learning\",\n        \"Meta-learning representations for continual learning\",\n        \"Contributions by metaplasticity to solving the catastrophic forgetting problem\",\n        \"Reconciling meta-learning and continual learning with online mixtures of tasks\",\n        \"Fedspeech: Federated text-to-speech with continual learning\",\n        \"Helpful or harmful: Inter-task association in continual learning\",\n        \"Learn continually, generalize rapidly: Lifelong knowledge accumulation for few-shot learning\",\n        \"Gradient-based editing of memory examples for online task-free continual learning\",\n        \"Energy-based latent aligner for incremental learning\",\n        \"Towards open world object detection\",\n        \"Novel class discovery without forgetting\",\n        \"Continual learning with node-importance based adaptive group sparse regularization\",\n        \"S3c: Self-supervised stochastic classifiers for few-shot class-incremental learning\",\n        \"Reparameterizing convolutions for incremental multi-task learning without task interference\",\n        \"Forget-free continual learning with winning subnetworks\",\n        \"Class-incremental learning by knowledge distillation with adaptive feature consolidation\",\n        \"A soft nearest-neighbor framework for continual semi-supervised learning\",\n        \"Natural continual learning: success is a journey, not (just) a destination\",\n        \"Continual reinforcement learning with multi-timescale replay\",\n        \"Continual reinforcement learning with complex synapses\",\n        \"Policy consolidation for continual reinforcement learning\",\n        \"Variational auto-regressive gaussian processes for continual learning\",\n        \"Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting\",\n        \"Continual training of language models for few-shot learning\",\n        \"Continual learning of natural language processing tasks: A survey\",\n        \"Continual learning of a mixed sequence of similar and dissimilar tasks\",\n        \"Fearnet: Brain-inspired model for incremental learning\",\n        \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n        \"Same state, different task: Continual reinforcement learning without interference\",\n        \"Towards continual reinforcement learning: A review and perspectives\",\n        \"Towards label-efficient incremental learning: A survey\",\n        \"Imbalanced continual learning with partitioning reservoir sampling\",\n        \"Continual learning on noisy data streams via self-purified replay\",\n        \"A theoretical study on solving continual learning\",\n        \"Split-and-bridge: Adaptable class incremental learning within a single neural network\",\n        \"Dygrain: An incremental learning framework for dynamic graphs\",\n        \"Segment anything\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Meta-consolidation for continual learning\",\n        \"Incremental object detection via meta-learning\",\n        \"Optimal continual learning has perfect memory and is np-hard\",\n        \"Online continual learning on class incremental blurry task configuration with anytime inference\",\n        \"Balancing stability and plasticity through advanced null space in continual learning\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Biological underpinnings for lifelong learning machines\",\n        \"Generalized and incremental few-shot learning by explicit learning and calibration without forgetting\",\n        \"Determinantal point processes for machine learning\",\n        \"Bayesian structural adaptation for continual learning\",\n        \"What learning systems do intelligent agents need? complementary learning systems theory updated\",\n        \"Retrospective adversarial replay for continual learning\",\n        \"Continual learning with bayesian neural networks for non-stationary data\",\n        \"Do not forget to attend to uncertainty while mitigating catastrophic forgetting\",\n        \"Semi-supervised class incremental learning\",\n        \"Few-shot and continual learning with attentive independent mechanisms\",\n        \"Continual learning with extended kronecker-factored approximate curvature\",\n        \"Residual continual learning\",\n        \"Overcoming catastrophic forgetting with unlabeled data in the wild\",\n        \"Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning\",\n        \"Continual learning in the teacher-student setup: Impact of task similarity\",\n        \"A neural dirichlet process mixture model for task-free continual learning\",\n        \"Overcoming catastrophic forgetting by incremental moment matching\",\n        \"Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges\",\n        \"The power of scale for parameter-efficient prompt tuning\",\n        \"Variational beam search for learning with distribution shifts\",\n        \"Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation\",\n        \"Rilod: Near real-time incremental learning for object detection at the edge\",\n        \"Continual few-shot intent detection\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection\",\n        \"Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting\",\n        \"Learning without forgetting\",\n        \"Total recall: a customized continual learning method for neural semantic parsers\",\n        \"Balancing between forgetting and acquisition in incremental subpopulation learning\",\n        \"Towards better plasticity-stability trade-off in incremental learning: A simple linear connector\",\n        \"Prototype-guided continual adaptation for class-incremental unsupervised domain adaptation\",\n        \"Trgp: Trust region gradient projection for continual learning\",\n        \"Beyond not-forgetting: Continual learning with backward knowledge transfer\",\n        \"Focal loss for dense object detection\",\n        \"Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models\",\n        \"Lifelong and continual learning dialogue systems: learning during conversation\",\n        \"Few-shot class-incremental learning via entropy-regularized data-free replay\",\n        \"Continual learning with recursive gradient optimization\",\n        \"Incremental prompting: Episodic memory prompt for lifelong event detection\",\n        \"Incremental meta-learning via indirect discriminant alignment\",\n        \"Lifelong intent detection via multi-strategy rebalancing\",\n        \"Continual learning for sentence representations using conceptors\",\n        \"Long-tailed class incremental learning\",\n        \"Rotate your networks: Better weight consolidation and less catastrophic forgetting\",\n        \"Generative feature replay for class-incremental learning\",\n        \"Adaptive aggregation networks for class-incremental learning\",\n        \"Rmm: Reinforced memory management for class-incremental learning\",\n        \"Mnemonics training: Multi-class incremental learning without forgetting\",\n        \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n        \"Continual reinforcement learning in 3d non-stationary environments\",\n        \"Core50: a new dataset and benchmark for continuous object recognition\",\n        \"Generalized variational continual learning\",\n        \"Gradient episodic memory for continual learning\",\n        \"Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps\",\n        \"Adaptive online planning for continual lifelong learning\",\n        \"Overcoming recency bias of normalization statistics in continual learning: Balance and adaptation\",\n        \"Continual federated learning based on knowledge distillation\",\n        \"Representational continuity for unsupervised continual learning\",\n        \"Continual learning in task-oriented dialogue systems\",\n        \"Online continual learning in image classification: An empirical survey\",\n        \"Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning\",\n        \"Piggyback: Adapting a single network to multiple tasks by learning to mask weights\",\n        \"Packnet: Adding multiple tasks to a single network by iterative pruning\",\n        \"Recall: Replay-based continual learning in semantic segmentation\",\n        \"Optimizing neural networks with kronecker-factored approximate curvature\",\n        \"On class orderings for incremental learning\",\n        \"Few-shot lifelong learning\",\n        \"Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory\",\n        \"Catastrophic interference in connectionist networks: The sequential learning problem\",\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Continual learning using a bayesian nonparametric dictionary of weight factors\",\n        \"An empirical investigation of the role of pre-training in lifelong learning\",\n        \"Lifelong policy gradient learning of factored policies for faster training without forgetting\",\n        \"Continual learning for natural language generation in task-oriented dialog systems\",\n        \"Generalized class incremental learning\",\n        \"Continual learning with filter atom swapping\",\n        \"Incremental learning techniques for semantic segmentation\",\n        \"Continual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations\",\n        \"Knowledge distillation for incremental learning in semantic segmentation\",\n        \"One-pass learning via bridging orthogonal gradient descent and recursive least-squares\",\n        \"Wide neural networks forget less catastrophically\",\n        \"Architecture matters in continual learning\",\n        \"Dropout as an implicit gating mechanism for continual learning\",\n        \"Linear mode connectivity in multitask and continual learning\",\n        \"Understanding the role of training regimes in continual learning\",\n        \"The drosophila mushroom body: From architecture to algorithm in a learning circuit\",\n        \"Continual learning for named entity recognition\",\n        \"Continuous coordination as a realistic scenario for lifelong learning\",\n        \"What is being transferred in transfer learning?\",\n        \"Variational continual learning\",\n        \"Alife: Adaptive logit regularizer and feature replay for incremental semantic segmentation\",\n        \"Gpt-4 technical report\",\n        \"Foundational models for continual learning: An empirical study of latent replay\",\n        \"Learning to remember: A synaptic plasticity driven framework for continual learning\",\n        \"Continual learning via local module composition\",\n        \"Dopamine and cognitive control in prefrontal cortex\",\n        \"Overcoming catastrophic forgetting by neuron-level plasticity control\",\n        \"Continual deep learning by functional regularisation of memorable past\",\n        \"First session adaptation: A strong replay-free baseline for class-incremental learning\",\n        \"Continual lifelong learning with neural networks: A review\",\n        \"Continual learning by asymmetric loss approximation with single-side overestimation\",\n        \"Class-incremental learning for action recognition in videos\",\n        \"Continual few-shot learning for text classification\",\n        \"Simpler is better: off-the-shelf continual learning through pretrained backbones\",\n        \"Continual learning: a feature extraction formalization, an efficient algorithm, and fundamental obstructions\",\n        \"Faster ilod: Incremental learning for object detectors based on faster rcnn\",\n        \"Sid: Incremental learning for anchor-free object detection via selective and inter-related distillation\",\n        \"Few-shot class-incremental learning from an open-set perspective\",\n        \"The ideal continual learner: An agent that never forgets\",\n        \"A pac-bayesian bound for lifelong learning\",\n        \"Film: Visual reasoning with a general conditioning layer\",\n        \"Incremental few-shot object detection\",\n        \"Fetril: Feature translation for exemplar-free class-incremental learning\",\n        \"Continual learning with fully probabilistic models\",\n        \"Dualnet: Continual learning, fast and slow\",\n        \"Contextual transformation networks for online continual learning\",\n        \"Continual normalization: Rethinking batch normalization for online continual learning\",\n        \"Online task-free continual learning with dynamic sparse distributed memory\",\n        \"Looking back on learned experiences for class/task incremental learning\",\n        \"Gdumb: A simple approach that questions our progress in continual learning\",\n        \"The challenges of continuous self-supervised learning\",\n        \"Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5\",\n        \"Continual few-shot relation learning via embedding space regularization and data augmentation\",\n        \"Bns: Building network structures dynamically for continual learning\",\n        \"Elle: Efficient lifelong pre-training for emerging data\",\n        \"Recent advances of continual learning in computer vision: An overview\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Random path selection for incremental learning\",\n        \"itaml: An incremental task-agnostic meta-learning approach\",\n        \"Relationship matters: Relation guided knowledge transfer for incremental learning of object detectors\",\n        \"Anatomy of catastrophic forgetting: Hidden representations and task semantics\",\n        \"Effect of scale on catastrophic forgetting in neural networks\",\n        \"Model zoo: A growing brain that learns continually\",\n        \"Encoder based lifelong learning\",\n        \"Continual unsupervised representation learning\",\n        \"Progressive prompts: Continual learning for language models\",\n        \"icarl: Incremental classifier and representation learning\",\n        \"A two-phase prototypical network model for incremental few-shot relation classification\",\n        \"Faster r-cnn: Towards real-time object detection with region proposal networks\",\n        \"The persistence and transience of memory\",\n        \"Learning to learn without forgetting by maximizing transfer and minimizing interference\",\n        \"Scalable recollections for continual lifelong learning\",\n        \"Online structured laplace approximations for overcoming catastrophic forgetting\",\n        \"Stream-51: Streaming classification and novelty detection from videos\",\n        \"Experience replay for continual learning\",\n        \"Generative continual concept learning\",\n        \"Complementary learning for overcoming catastrophic forgetting using experience replay\",\n        \"Detection and continual learning of novel face presentation attacks\",\n        \"Class-incremental novel class discovery\",\n        \"Continual learning via sequential function-space variational inference\",\n        \"Progressive neural networks\",\n        \"Gradient projection memory for continual learning\",\n        \"Towards a natural benchmark for continual learning\",\n        \"Progress & compress: A scalable framework for continual learning\",\n        \"Continual learning in generative adversarial nets\",\n        \"Overcoming catastrophic forgetting with hard attention to the task\",\n        \"Overcoming catastrophic forgetting beyond continual learning: Balanced training for neural machine translation\",\n        \"A progressive model to enable continual learning for semantic slot filling\",\n        \"Overcoming catastrophic forgetting in incremental few-shot learning by finding flat minima\",\n        \"Incremental few-shot semantic segmentation via embedding adaptive-update and hyper-class representation\",\n        \"Mimicking the oracle: An initial phase decorrelation approach for class incremental learning\",\n        \"Learning with selective forgetting\",\n        \"Online class-incremental continual learning with adversarial shapley value\",\n        \"Continual learning with deep generative replay\",\n        \"Incremental learning of object detectors without catastrophic forgetting\",\n        \"Dlcft: Deep linear continual fine-tuning for general incremental learning\",\n        \"On generalizing beyond domains in cross-domain continual learning\",\n        \"On learning the geodesic path for incremental learning\",\n        \"Rectification-based knowledge retention for continual learning\",\n        \"Calibrating cnns for lifelong learning\",\n        \"Always be dreaming: A new approach for data-free class-incremental learning\",\n        \"Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning\",\n        \"Score-based generative modeling through stochastic differential equations\",\n        \"Climb: A continual learning benchmark for vision-and-language tasks\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"Unsupervised model adaptation for continual semantic segmentation\",\n        \"Lamol: Language modeling for lifelong language learning\",\n        \"Distill and replay for continual language learning\",\n        \"Exploring example influence in continual learning\",\n        \"Information-theoretic online memory selection for continual learning\",\n        \"Improving and understanding variational continual learning\",\n        \"Layerwise optimization by gradient decomposition for continual learning\",\n        \"Learning to imagine: Diversify memory for incremental learning using unlabeled data\",\n        \"Topology-preserving class-incremental learning\",\n        \"Few-shot class-incremental learning\",\n        \"A deep hierarchical approach to lifelong learning in minecraft\",\n        \"Catastrophic forgetting and mode collapse in gans\",\n        \"Functional regularisation for continual learning with gaussian processes\",\n        \"Gcr: Gradient coreset based replay buffer selection for continual learning\",\n        \"Bring evanescent representations to life in lifelong class incremental learning\",\n        \"Natural variational continual learning\",\n        \"A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex\",\n        \"Brain-inspired replay for continual learning with artificial neural networks\",\n        \"Three scenarios for continual learning\",\n        \"Neural discrete representation learning\",\n        \"Prompt augmented generative replay via supervised contrastive learning for lifelong intent detection\",\n        \"Attention is all you need\",\n        \"Efficient continual learning with modular networks and task-driven priors\",\n        \"Rehearsal revealed: The limits and merits of revisiting samples in continual learning\",\n        \"vclimb: A novel video class incremental learning benchmark\",\n        \"Random sampling with a reservoir\",\n        \"Continual learning with hypernetworks\",\n        \"Neural plasticity: Dopamine tunes the mushroom body output network\",\n        \"Mell: Large-scale extensible user intent classification for dialogue systems with meta lifelong learning\",\n        \"Lifelong graph learning\",\n        \"Foster: Feature boosting and compression for class-incremental learning\",\n        \"Wanderlust: Online continual object detection in the real world\",\n        \"Acae-remind for online continual learning with compressed feature replay\",\n        \"Triple-memory networks: A brain-inspired method for continual learning\",\n        \"Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality\",\n        \"Ordisco: Effective and efficient usage of incremental unlabeled data for semi-supervised continual learning\",\n        \"Afec: Active forgetting of negative transfer in continual learning\",\n        \"Incorporating neuro-inspired adaptability for continual learning in artificial intelligence\",\n        \"Coscl: Cooperation of small continual learners is stronger than a big one\",\n        \"Memory replay with data compression for continual learning\",\n        \"Learngene: From open-world to your learning task\",\n        \"Anti-retroactive interference for lifelong learning\",\n        \"Few-shot class-incremental learning for named entity recognition\",\n        \"Training networks in null space of feature covariance for continual learning\",\n        \"Incremental learning from scratch for task-oriented dialogue systems\",\n        \"S-prompts learning with pre-trained transformers: An occamâ€™s razor for domain incremental learning\",\n        \"Continual learning with lifelong vision transformer\",\n        \"Continual learning through retrieval and imagination\",\n        \"Online continual learning with contrastive vision transformer\",\n        \"Efficient meta lifelong-learning with limited memory\",\n        \"Improving task-free continual learning by distributionally robust memory evolution\",\n        \"Meta-learning with less forgetting on large-scale non-stationary task distributions\",\n        \"Dualprompt: Complementary prompting for rehearsal-free continual learning\",\n        \"Learning to prompt for continual learning\",\n        \"Continual learning with guarantees via weight interval constraints\",\n        \"Disentangling transfer in continual reinforcement learning\",\n        \"Continual world: A robotic benchmark for continual reinforcement learning\",\n        \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time\",\n        \"Supermasks in superposition\",\n        \"Memory replay gans: Learning to generate new categories without forgetting\",\n        \"Striking a balance between stability and plasticity for class-incremental learning\",\n        \"Curriculum-meta learning for order-robust continual relation extraction\",\n        \"Class-incremental learning with strong pre-trained models\",\n        \"Large scale incremental learning\",\n        \"Deltagrad: Rapid retraining of machine learning models\",\n        \"Incremental learning via rate reduction\",\n        \"Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system\",\n        \"Incremental learning using conditional adversarial networks\",\n        \"General incremental learning with domain-aware categorical representations\",\n        \"Geometry of sequence working memory in macaque prefrontal cortex\",\n        \"Reinforced continual learning\",\n        \"Continual learning of control primitives: Skill discovery via reset-games\",\n        \"Meta-attention for vit-backed continual learning\",\n        \"Generative negative text replay for continual vision-language pretraining\",\n        \"Der: Dynamically expandable representation for class incremental learning\",\n        \"An em framework for online incremental learning of semantic segmentation\",\n        \"Uncertainty-aware contrastive distillation for incremental semantic segmentation\",\n        \"Stably maintained dendritic spines are associated with lifelong memories\",\n        \"Learning latent representations across multiple data domains using lifelong vaegan\",\n        \"Task-free continual learning via online discrepancy distance learning\",\n        \"Learning with recoverable forgetting\",\n        \"Mitigating forgetting in online continual learning with neuron calibration\",\n        \"Dreaming to distill: Data-free knowledge transfer via deepinversion\",\n        \"Contintin: Continual learning from task instructions\",\n        \"Federated continual learning with weighted inter-client transfer\",\n        \"Scalable and order-robust continual learning with additive parameter decomposition\",\n        \"Online coreset selection for rehearsal-based continual learning\",\n        \"Lifelong learning with dynamically expandable networks\",\n        \"Continual learning by modeling intra-class variation\",\n        \"Self-training for class-incremental semantic segmentation\",\n        \"Semantic drift compensation for class-incremental learning\",\n        \"Continual learning of context-dependent processing in neural networks\",\n        \"Continual learning through synaptic intelligence\",\n        \"Piggyback gan: Efficient lifelong learning for image conditioned generation\",\n        \"Hyper-lifelonggan: scalable lifelong learning for image conditioned generation\",\n        \"Lifelong gan: Continual learning for conditional image generation\",\n        \"Few-shot incremental learning with continually evolved classifiers\",\n        \"Representation compensation networks for continual semantic segmentation\",\n        \"Slca: Slow learner with classifier alignment for continual learning on a pre-trained model\",\n        \"Mixup: Beyond empirical risk minimization\",\n        \"Class-incremental learning via deep model consolidation\",\n        \"Side-tuning: a baseline for network adaptation via additive side networks\",\n        \"Active protection: Learning-activated raf/mapk activity protects labile memory from rac1-independent forgetting\",\n        \"Cglb: Benchmark tasks for continual graph learning\",\n        \"Epicker is an exemplar-based continual learning approach for knowledge accumulation in cryoem particle picking\",\n        \"A simple but strong baseline for online continual learning: Repeated augmented rehearsal\",\n        \"Continual sequence generation with adaptive compositional modules\",\n        \"Maintaining discrimination and fairness in class incremental learning\",\n        \"Genetic dissection of mutual interference between two consecutive learning tasks in drosophila\",\n        \"On leveraging pretrained gans for generation with limited data\",\n        \"Static-dynamic co-teaching for class-incremental 3d object detection\",\n        \"Forward compatible few-shot class-incremental learning\",\n        \"Few-shot class-incremental learning by sampling multi-phase tasks\",\n        \"Co-transport for class-incremental learning\",\n        \"Image de-raining via continual learning\",\n        \"Objects as points\",\n        \"Class-incremental learning via dual augmentation\",\n        \"Prototype augmentation and self-supervision for incremental learning\",\n        \"Self-promoted prototype refinement for few-shot class-incremental learning\",\n        \"Self-sustaining representation expansion for non-exemplar class-incremental learning\",\n        \"Continual prompt tuning for dialog state tracking\",\n        \"Margin-based few-shot class-incremental learning with class-level overfitting mitigation\"\n    ],\n    \"reasoning\": \"Selected these 525 references because they represent foundational work and recent advances in A Comprehensive Survey of Continual Learning: Theory, Method and Application, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}