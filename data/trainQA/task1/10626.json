{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 138 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Systematic Review of Data-to-Text NLG.\n\nPaper Abstract: Abstract This systematic review undertakes a comprehensive analysis of current research on data-to-text generation, identifying gaps, challenges, and future directions within the field. Relevant literature in this field on datasets, evaluation metrics, application areas, multilingualism, language models, and hallucination mitigation methods is reviewed. Various methods for producing high-quality text are explored, addressing the challenge of hallucinations in data-to-text generation. These methods include re-ranking, traditional and neural pipeline architecture, planning architectures, data cleaning, controlled generation, and modification of models and training techniques. Their effectiveness and limitations are assessed, highlighting the need for universally applicable strategies to mitigate hallucinations. The review also examines the usage, popularity, and impact of datasets, alongside evaluation metrics, with an emphasis on both automatic and human assessment. Additionally, the evolution of data-to-text models, particularly the widespread adoption of transformer models, is discussed. Despite advancements in text quality, the review emphasizes the importance of research in low-resourced languages and the engineering of datasets in these languages to promote inclusivity. Finally, several application domains of data-to-text are highlighted, emphasizing their relevance in such domains. Overall, this review serves as a guiding framework for fostering innovation and advancing data-to-text generation.\n\n{retrieved_text}\n\nGenerate exactly 138 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 138\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Building applied natural language generation systems\",\n        \"Guided neural language generation for abstractive summarization using abstract meaning representation.\",\n        \"Automatic and Human-AI Interactive Text Generation\",\n        \"Semantic Neural Machine Translation Using AMR.\",\n        \"ClipCap: CLIP Prefix for Image Captioning\",\n        \"Maximizing stylistic control and semantic accuracy in nlg: Personality variation and discourse contrast.\",\n        \"Neural natural language generation: A survey on multilinguality, multimodality, controllability and learning.\",\n        \"Transformer based natural language generation for question-answering.\",\n        \"Learning Neural Templates for Text Generation.\",\n        \"ToTTo: A controlled table-to-text generation dataset.\",\n        \"Scalable Micro-planned Generation of Discourse from Structured Data.\",\n        \"Damata: A robot-journalist covering the brazilian amazon deforestation.\",\n        \"Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model.\",\n        \"SQL-to-Text Generation with Graph-to-Sequence Model,\",\n        \"Survey of the state of the art in natural language generation: Core tasks, applications and evaluation.\",\n        \"PAN: Pipeline assisted neural networks model for data-to-text generation in social internet of things.\",\n        \"Dealing with hallucination and omission in neural Natural Language Generation: A use case on meteorology.\",\n        \"Towards constructing sports news from live text commentary,\",\n        \"Interacting with financial data using natural language.\",\n        \"Personalizing Weekly Diet Reports\",\n        \"Neural Text Generation from Structured Data with Application to the Biography Domain.\",\n        \"Getting to Production with Few-shot Natural Language Generation Models.\",\n        \"Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods.\",\n        \"AGGGEN: Ordering and aggregating while generating.\",\n        \"Learning Semantic Correspondences with Less Supervision\",\n        \"Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization.\",\n        \"Handling rare items in data-to-text generation.\",\n        \"ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation.\",\n        \"Logic2Text: High-Fidelity Natural Language Generation from Logical Forms.\",\n        \"Survey of Hallucination in Natural Language Generation.\",\n        \"A Systematic Literature Review on Text Generation Using Deep Neural Network Models.\",\n        \"Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods,\",\n        \"A Survey of Evaluation Metrics Used for NLG Systems.\",\n        \"Neural Text Generation: Past, Present and Beyond\",\n        \"Innovations in Neural Data-to-text Generation,\",\n        \"The WebNLG challenge: Generating text from RDF data.\",\n        \"Investigating Pretrained Language Models for Graph-to-Text Generation,\",\n        \"Lightweight, dynamic graph convolutional networks for AMR-to-text generation.\",\n        \"Neural text generation from rich semantic representations.\",\n        \"The PRISMA 2020 statement: An updated guideline for reporting systematic reviews.\",\n        \"Controlled hallucinations: learning to generate faithfully from noisy data.\",\n        \"Data-to-text generation with style imitation.\",\n        \"Enhancing content planning for table-to-text generation with data understanding and verification.\",\n        \"Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator.\",\n        \"Few-Shot Table-to-Text Generation with Prototype Memory.\",\n        \"Challenges in Data-to-Document Generation,\",\n        \"Operation-guided neural networks for high fidelity data-to-text generation.\",\n        \"Unsupervised natural language generation with denoising autoencoders.\",\n        \"Neural data-to-text generation: A comparison between pipeline and end-to-end architectures.\",\n        \"Long and diverse text generation with planning-based hierarchical variational model.\",\n        \"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing\",\n        \"T.: Table-to-text generation with effective hierarchical encoder on three dimensions (row\",\n        \": Enhancing neural data-to-text generation models with external background knowledge\",\n        \"W.Y.: KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation\",\n        \"C.: Multilingual AMR-to-Text Generation. arXiv preprint arXiv:2011.05443\",\n        \"Y.: Online Back-Parsing for AMR-to-Text Generation\",\n        \"EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing\",\n        \": Structural Adapters in Pretrained Language Models for AMR-to-Text Generation\",\n        \"K.: Data-to-text Generation by Splicing Together Nearest Neighbors. EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing\",\n        \": Neural data-to-text generation with LM-based text augmentation\",\n        \"EACL 2021 - 16th Conference of the European Chapter of the Association for Computational Linguistics\",\n        \": Neural AMR: Sequence-to-sequence models for parsing and generation\",\n        \"C.: A Graph-to-Sequence Model for AMR-to-Text Generation\",\n        \"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics\",\n        \"M.: Data-to-text generation with entity modeling. ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics\",\n        \": A simple recipe towards reducing hallucination in neural surface realisation\",\n        \"H.: Learning to select\",\n        \": Logical natural language generation from open-domain tables\",\n        \": Bridging the structural gap between encoding and decoding for data-to-text generation\",\n        \"F.: GPT-too : A Language-Model-First Approach for AMR-to-Text Generation\",\n        \": Towards faithful neural table-to-text generation with content-matching constraints\",\n        \": Neural data-to-text generation via jointly learning the segmentation and correspondence\",\n        \"H.: Fact-based text editing. Proceedings of the Annual Meeting of the Association for Computational Linguistics\",\n        \"ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\",\n        \": On Training Instance Selection for Few-Shot Neural Text Generation\",\n        \": XLPT-AMR: Cross-lingual pre-training via multi-task learning for zero-shot AMR parsing and text generation\",\n        \"Y.: Graph Pre-training for AMR Parsing and Generation. Proceedings of the Annual Meeting of the Association for Computational Linguistics 1\",\n        \"NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference 1\",\n        \": Step-by-step: Separating planning from realization in neural data-to-text generation\",\n        \": Structural neural encoders for AMR-to-text generation\",\n        \"NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n        \"P.: NEUROLOGIC A‚àóesque Decoding: Constrained Text Generation with Lookahead Heuristics\",\n        \"H.: Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics 8\",\n        \": Modeling global and local node contexts for text generation from knowledge graphs\",\n        \"M.: Data-to-text Generation with Macro Planning. Transactions of the Association for Computational Linguistics 9\",\n        \": JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\",\n        \": Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity\",\n        \": TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching\",\n        \"Y.: Learning with Contrastive Examples for Data-to-Text Generation. COLING 2020 - 28th International Conference on Computational Linguistics\",\n        \": Best Practices for Data-Efficient Modeling in NLG: How to Train Production-Ready Neural Models with Less Data\",\n        \"LREC 2018 - 11th International Conference on Language Resources and Evaluation\",\n        \"M.: Data-to-text generation with content selection and planning. 33rd AAAI Conference on Artificial Intelligence\",\n        \"Z.: Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View. 35th AAAI Conference on Artificial Intelligence\",\n        \"A.M.: End-to-End Content and Plan Selection for Data-to-Text Generation. E2E NLG Challenge System Descriptions\",\n        \"I.: E2E NLG challenge: Neural models vs. templates. INLG 2018 - 11th International Natural Language Generation Conference\",\n        \": A good sample is hard to find: Noise injection sampling and self-training for neural language generation models\",\n        \"H.: Revisiting challenges in data-to-text generation with fact grounding. INLG 2019 - 12th International Conference on Natural Language Generation\",\n        \": Semi-supervised neural text generation by joint learning of natural language generation and natural language understanding models\",\n        \"M.: T5Pretrain-Text-to-Text Pre-Training for Data-to-Text Tasks.pdf\",\n        \": PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation\",\n        \": Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG\",\n        \": NABU ‚Äì Multilingual Graph-Based Neural RDF Verbalizer\",\n        \"A.: The 2020 Bilingual\",\n        \": Machine Translation Aided Bilingual Data-to-Text Generation and Semantic Parsing\",\n        \"Jory Stevens-Guille\",\n        \"Z.: ùí´2subscriptùí´2\\\\mathcal{P}_{2} : A Plan-and-Pretrain Approach for Knowledge Graph-to-Text Generation (December)\",\n        \"P.: Controlling hallucinations at word level in data-to-text generation. Data Mining and Knowledge Discovery 36(1)\",\n        \"V.: The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254 (2017)\",\n        \"N.: Abstract meaning representation for sembanking. In: Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse\",\n        \"In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n        \"J.: Parallel data\",\n        \"M.: Findings of the 2017 conference on machine translation (wmt17). WMT 2017 - 2nd Conference on Machine Translation\",\n        \"M.: Findings of the 2019 conference on machine translation (wmt19). 5th Conference on Machine Translation\",\n        \"Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014)\",\n        \"I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\",\n        \"J.: Long short-term memory. Neural computation 9(8)\",\n        \"Y.: On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014)\",\n        \"P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1)\",\n        \"L.: Bart: Denoising sequence-to-sequence pre-training for natural language generation\",\n        \"A.: Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291 (2019)\",\n        \"et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8)\",\n        \": Bleu: a method for automatic evaluation of machine translation\",\n        \": Meteor universal: Language specific translation evaluation for any target language\",\n        \"C.-Y.: Rouge: A package for automatic evaluation of summaries. In: Text Summarization Branches Out\",\n        \"M.: chrf: character n-gram f-score for automatic mt evaluation. In: Proceedings of the Tenth Workshop on Statistical Machine Translation\",\n        \"In: Proceedings of the Second International Conference on Human Language Technology Research\",\n        \"In: Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers\",\n        \"Lawrence Zitnick\",\n        \"M.M.: A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) 55(2)\",\n        \"S.: Spice: Semantic propositional image caption evaluation. In: Computer Vision‚ÄìECCV 2016: 14th European Conference\",\n        \"Y.: Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019)\",\n        \"S.: Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622 (2019)\",\n        \": Bleurt: Learning robust metrics for text generation\",\n        \"In: Proceedings of the 12th International Conference on Natural Language Generation\",\n        \"D.M.: Disentangling the properties of human evaluation methods: A classification system to support comparability\",\n        \": A comprehensive capability analysis of gpt-3 and gpt-3\",\n        \"et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\",\n        \"et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\"\n    ],\n    \"reasoning\": \"Selected these 138 references because they represent foundational work and recent advances in A Systematic Review of Data-to-Text NLG., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}