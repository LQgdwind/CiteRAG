{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 57 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning\n\nPaper Abstract: Abstract In large-scale machine learning, recent works have studied the effects of compressing gradients in stochastic optimization in order to alleviate the communication bottleneck. These works have collectively revealed that stochastic gradient descent (SGD) is robust to structured perturbations such as quantization, sparsification, and delays. Perhaps surprisingly, despite the surge of interest in large-scale, multi-agent reinforcement learning, almost nothing is known about the analogous question: Are common reinforcement learning (RL) algorithms also robust to similar perturbations? In this paper, we investigate this question by studying a variant of the classical temporal difference (TD) learning algorithm with a perturbed update direction, where a general compression operator is used to model the perturbation. Our main technical contribution is to show that compressed TD algorithms, coupled with an error-feedback mechanism used widely in optimization, exhibit the same non-asymptotic theoretical guarantees as their SGD counterparts. We then extend our results significantly to nonlinear stochastic approximation algorithms and multi-agent settings. In particular, we prove that for multi-agent TD learning, one can achieve linear convergence speedups in the number of agents while communicating just O~‚Äã(1)~ùëÇ1\\tilde{O}(1) bits per agent at each time-step. Our work is the first to provide finite-time results in RL that account for general compression operators and error-feedback in tandem with linear function approximation and Markovian sampling. Our analysis hinges on studying the drift of a novel Lyapunov function that captures the dynamics of a memory variable introduced by error feedback.\n\n{retrieved_text}\n\nGenerate exactly 57 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 57\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Federated optimization: Distributed machine learning for on-device intelligence\",\n        \"1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns\",\n        \"Qsgd: Communication-efficient sgd via gradient quantization and encoding\",\n        \"Terngrad: Ternary gradients to reduce communication in distributed deep learning\",\n        \"Sparsified sgd with memory\",\n        \"Sparse communication for distributed gradient descent\",\n        \"signsgd: Compressed optimisation for non-convex problems\",\n        \"Learning to predict by the methods of temporal differences\",\n        \"Reinforcement learning: An introduction\",\n        \"Finite-time analysis of distributed td (0) with linear function approximation on multi-agent reinforcement learning\",\n        \"Federated reinforcement learning: techniques, applications, and open challenges\",\n        \"Federated reinforcement learning with environment heterogeneity\",\n        \"Federated reinforcement learning: Linear speedup under markovian sampling\",\n        \"A finite time analysis of temporal difference learning with linear function approximation\",\n        \"Finite-time error bounds for linear stochastic approximation andtd learning\",\n        \"Performance of q-learning with linear function approximation: Stability and finite-time analysis\",\n        \"Ratq: A universal fixed-length quantizer for stochastic optimization\",\n        \"vqsgd: Vector quantized stochastic gradient descent\",\n        \"Error feedback fixes signsgd and other gradient compression schemes\",\n        \"On biased compression for distributed learning\",\n        \"The convergence of sparsified gradient methods\",\n        \"Differentially quantized gradient methods\",\n        \"Source coding theory, volume 83\",\n        \"An analysis of temporal-difference learning with function approximation\",\n        \"Linear stochastic approximation: Constant step-size and iterate averaging\",\n        \"Finite sample analyses for td (0) with function approximation\",\n        \"Perturbed iterate analysis for asynchronous stochastic optimization\",\n        \"Stochastic approximation: a dynamical systems viewpoint, volume 48\",\n        \"Distributed td (0) with almost no communication\",\n        \"Scaffold: Stochastic controlled averaging for federated learning\",\n        \"Local sgd converges fast and communicates little\",\n        \"A unified theory of decentralized sgd with changing topology and local updates\",\n        \"Dissecting adam: The sign, magnitude and variance of stochastic gradients\",\n        \"The ode method for convergence of stochastic approximation and reinforcement learning\",\n        \"On td(0) with function approximation: Concentration bounds and a centered variant with exponential convergence\",\n        \"Finite time bounds for temporal difference learning with function approximation: Problems with some ‚Äústate-of-the-art‚Äù results\",\n        \"Temporal difference learning as gradient splitting\",\n        \"Scalable distributed dnn training using commodity gpu cloud computing\",\n        \"Distributed learning with compressed gradients\",\n        \"Distributed learning with compressed gradient differences\",\n        \"Acceleration for compressed gradient descent in distributed and federated optimization\",\n        \"Stochastic distributed learning with gradient quantization and variance reduction\",\n        \"Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization\",\n        \"Federated learning with compression: Unified analysis and sharp guarantees\",\n        \"Deep gradient compression: Reducing the communication bandwidth for distributed training\",\n        \"The error-feedback framework: Better rates for sgd with delayed gradients and compressed communication\",\n        \"Linearly converging error compensated sgd\",\n        \"Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients\",\n        \"Ef21: A new, simpler, theoretically better, and practically faster error feedback\",\n        \"Ef21-p and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression\",\n        \"Efficient randomized subspace embeddings for distributed optimization under a communication budget\",\n        \"Solving multi-arm bandit using a few bits of communication\",\n        \"Linear stochastic bandits over a bit-constrained channel\",\n        \"Remote contextual bandits\",\n        \"Matrix analysis\",\n        \"Constrained consensus and optimization in multi-agent networks\",\n        \"On communication compression for distributed optimization on heterogeneous data\"\n    ],\n    \"reasoning\": \"Selected these 57 references because they represent foundational work and recent advances in Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}