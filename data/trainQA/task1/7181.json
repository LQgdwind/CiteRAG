{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Facial Action Units Detection Aided by Global-Local Expression Embedding\n\nPaper Abstract: Abstract Since Facial Action Unit (AU) annotations require domain expertise, common AU datasets only contain a limited number of subjects. As a result, a crucial challenge for AU detection is addressing identity overfitting. We find that AUs and facial expressions are highly associated, and existing facial expression datasets often contain a large number of identities. In this paper, we aim to utilize the expression datasets without AU labels to facilitate AU detection. Specifically, we develop a novel AU detection framework aided by the Global-Local facial Expressions Embedding, dubbed GLEE-Net. Our GLEE-Net consists of three branches to extract identity-independent expression features for AU detection. We introduce a global branch for modeling the overall facial expression while eliminating the impacts of identities. We also design a local branch focusing on specific local face regions. The combined output of global and local branches is firstly pre-trained on an expression dataset as an identity-independent expression embedding, and then finetuned on AU datasets. Therefore, we significantly alleviate the issue of limited identities. Furthermore, we introduce a 3D global branch that extracts expression coefficients through 3D face reconstruction to consolidate 2D AU descriptions. Finally, a Transformer-based multi-label classifier is employed to fuse all the representations for AU detection. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art on the widely-used DISFA, BP4D and BP4D+ datasets.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Multiple view geometry in computer vision\",\n        \"Openface 2.0: Facial behavior analysis toolkit\",\n        \"Recognition of action units in the wild with deep nets and a new global-local loss\",\n        \"Large scale 3d morphable models\",\n        \"Vggface2: A dataset for recognising faces across pose and age\",\n        \"Multi-label image recognition with graph convolutional networks\",\n        \"Deep structure inference network for facial action unit recognition\",\n        \"Knowledge augmented deep neural networks for joint facial expression and action unit recognition\",\n        \"Facial action coding system: A technique for the measurement of facial movement\",\n        \"Liblinear: A library for large linear classification\",\n        \"3d guided fine-grained face manipulation\",\n        \"Deep residual learning for image recognition\",\n        \"Facial action unit detection with transformers\",\n        \"Subject identification across large expression variations using 3d facial landmarks\",\n        \"Facing imbalanced dataâ€“recommendations for the use of performance metrics\",\n        \"Iterative methods for optimization\",\n        \"Deep video portraits\",\n        \"Aff-wild2: Extending the aff-wild database for affect recognition\",\n        \"Sewa db: A rich database for audio-visual emotion and sentiment research in the wild\",\n        \"Semantic relationships guided representation learning for facial action unit recognition\",\n        \"Write-a-speaker: Text-based emotional and rhythmic talking-head generation\",\n        \"Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing\",\n        \"Eac-net: Deep nets with enhancing and cropping for facial action unit detection\",\n        \"Disfa: A spontaneous facial action intensity database\",\n        \"Affectnet: A database for facial expression, valence, and arousal computing in the wild\",\n        \"Local relationship learning with person-specific shape regularization for facial action unit detection\",\n        \"Facial action recognition for facial expression analysis from static face images\",\n        \"A 3d face model for pose and illumination invariant face recognition\",\n        \"Impaired recognition of affect in facial expression in depressed patients\",\n        \"Facenet: A unified embedding for face recognition and clustering\",\n        \"Jaa-net: joint facial action unit detection and face alignment via adaptive attention\",\n        \"Facial action unit detection using attention and relation learning\",\n        \"Uncertain graph neural networks for facial action unit detection\",\n        \"Hybrid message passing with performance-driven structures for facial action unit detection\",\n        \"Attention is all you need\",\n        \"A compact embedding for facial expression similarity\",\n        \"Region attention networks for pose and occlusion robust facial expression recognition\",\n        \"Exploiting semantic embedding and visual feature for facial action unit detection\",\n        \"Facs3d-net: 3d convolution based spatiotemporal representation for action unit detection\",\n        \"One-shot face reenactment using appearance adaptive normalization\",\n        \"Cross-modality attention with semantic graph embedding for multi-label classification\",\n        \"Learning a facial expression embedding disentangled from identity\",\n        \"Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database\",\n        \"Multimodal spontaneous emotion corpus for human behavior analysis\",\n        \"Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset\",\n        \"Automatic micro-expression analysis: Open challenges\",\n        \"Joint patch and multi-label learning for facial action unit and holistic expression recognition\",\n        \"Learning facial action units from web images with scalable weakly supervised clustering\",\n        \"Deep region and multi-label learning for facial action unit detection\",\n        \"Robust lightweight facial expression recognition network with label distribution training\",\n        \"Learning multiscale active facial patches for expression analysis\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Facial Action Units Detection Aided by Global-Local Expression Embedding, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}