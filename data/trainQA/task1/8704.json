{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 217 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy\n\nPaper Abstract: Abstract Machine unlearning (MU) is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions. Additionally, the paper emphasizes the need for researchers and practitioners to continue exploring and refining unlearning techniques to ensure that ML models can adapt to changing circumstances while maintaining user trust. The importance of unlearning is further highlighted in making Artificial Intelligence (AI) more trustworthy and transparent, especially with the increasing importance of AI in various domains that involve large amounts of personal user data.\n\n{retrieved_text}\n\nGenerate exactly 217 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 217\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Machine learning: Algorithms, real-world applications and research directions\",\n        \"The potential for artificial intelligence in healthcare\",\n        \"Machine learning and deep learning\",\n        \"Approximate data deletion from machine learning models\",\n        \"A review on machine unlearning\",\n        \"Towards making systems forget with machine unlearning\",\n        \"Verifiable and provably secure machine unlearning\",\n        \"A survey of machine unlearning\",\n        \"Remember what you want to forget: Algorithms for machine unlearning\",\n        \"Deep unlearning via randomized conditionally independent hessians\",\n        \"Medroberta. nl: a language model for dutch electronic health records\",\n        \"Markov chain monte carlo-based machine unlearning: Unlearning what needs to be forgotten\",\n        \"Preserving personal dignity: the vital role of the right to be forgotten\",\n        \"Decision making within a cancel culture environment\",\n        \"The right to be forgotten is taking shape: Cjeu judgments in gc and others (c-136/17) and google v cnil (c-507/17)\",\n        \"Humans forget, machines remember: Artificial intelligence and the right to be forgotten\",\n        \"The european union general data protection regulation: what it is and what it means\",\n        \"Machine unlearning\",\n        \"Coded machine unlearning\",\n        \"Machine unlearning for random forests\",\n        \"Adaptive machine unlearning\",\n        \"Fast yet effective machine unlearning\",\n        \"Can ai learn to forget?\",\n        \"Novel neural network repair methods for data privacy and individual fairness\",\n        \"A statistical framework for differential privacy\",\n        \"Adaptive laplace mechanism: Differential privacy preservation in deep learning\",\n        \"Proactively control privacy in recommender systems\",\n        \"Understanding and Measuring Privacy Risks in Machine Learning\",\n        \"Towards improving privacy of synthetic datasets\",\n        \"Consent management platforms under the gdpr: processors and/or controllers?\",\n        \"Personal data vulnerability: Constitutional issues with the california consumer privacy act\",\n        \"“it wouldn’t happen to me”: Privacy concerns and perspectives following the cambridge analytica scandal\",\n        \"Privacy laws and privacy by design schemes for the internet of things\",\n        \"Analyzing privacy and end user information exposure in digital communication environments\",\n        \"Discover and mitigate unknown biases with debiasing alternate networks\",\n        \"A high-bias, low-variance introduction to machine learning for physicists\",\n        \"Biased against debiasing: On the role of (institutionally sponsored) self-transformation in the struggle against prejudice\",\n        \"Fedharmony: unlearning scanner bias with distributed data\",\n        \"Trapdoor: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis\",\n        \"Learning to unlearn: Building immunity to dataset bias in medical imaging studies\",\n        \"Unlearn dataset bias in natural language inference by fitting the residual\",\n        \"Skin deep unlearning: Artefact and instrument debiasing in the context of melanoma classification\",\n        \"A survey of identification and mitigation of machine learning algorithmic biases in image analysis\",\n        \"Towards debiasing dnn models from spurious feature influence\",\n        \"Everybody needs good neighbours: An unsupervised locality-based method for bias mitigation\",\n        \"Transfer learning and deep domain adaptation\",\n        \"The pitfalls of transfer learning in computer vision for agriculture\",\n        \"Learning to learn without forgetting by maximizing transfer and minimizing interference\",\n        \"Transfer learning via test-time neural networks aggregation\",\n        \"NES-TL: Network embedding similarity-based transfer learning\",\n        \"“Adversarial transfer learning for deep learning based automatic modulation classification\",\n        \"“How transferable are features in deep neural networks?\",\n        \"“A survey on negative transfer\",\n        \"“On compressing deep models by low rank and sparse decomposition\",\n        \"“A survey of large language models\",\n        \"“’right to be forgotten and re-referencing’and the freedom of speech and expression: Indian resonances to google vs. cnil\",\n        \"“Hard to forget: Poisoning attacks on certified machine unlearning\",\n        \"“Data poisoning attacks on federated machine learning\",\n        \"“Federated multi-task learning\",\n        \"“Data poisoning attacks against machine learning algorithms\",\n        \"“On the robustness of ensemble-based machine learning against data poisoning\",\n        \"“On developing deep learning models with particle swarm optimization in the presence of poisoning attacks\",\n        \"“Particle swarm optimization for single objective continuous space problems: A review\",\n        \"“Bounding membership inference\",\n        \"“Practical differentially private hyperparameter tuning with subsampling\",\n        \"“Fast model update for iot traffic anomaly detection with machine unlearning\",\n        \"“A novel fractional order chaos-based image encryption using fisher yates algorithm and 3-d cat map\",\n        \"“On the fundamental limits of coded data shuffling\",\n        \"Edinburgh: Oliver and Boyd, 1963\",\n        \"“Block shuffling learning for deepfake detection\",\n        \"“A comparative study of data anonymization techniques\",\n        \"“Survey: Leakage and privacy at inference time\",\n        \"Springer International Publishing, 2017\",\n        \"“Extracting training data from large language models\",\n        \"“Personal information leakage by abusing the gdpr’right of access’\",\n        \"“Enabling realistic health data re-identification risk assessment through adversarial modeling\",\n        \"“Machine unlearning via algorithmic stability\",\n        \"“Machine unlearning of features and labels\",\n        \"“Zero-shot machine unlearning\",\n        \"“Learning multiple layers of features from tiny images\",\n        \"“Gradient-based learning applied to document recognition\",\n        \"“Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\",\n        \"“Making recommender systems forget: Learning and unlearning for erasable recommendation\",\n        \"J. Pouget-Abadie\",\n        \"“An introduction to variational autoencoders\",\n        \"“Acoustic modeling using deep belief networks\",\n        \"“Learn to forget: Machine unlearning via neuron masking\",\n        \"V. Chandrasekaran\",\n        \"“Adversarial unlearning: Reducing confidence along adversarial directions\",\n        \"“Self-adaptation for machine learning based systems\",\n        \"“Machine unlearning: Linear filtration for logit-based classifiers\",\n        \"“Dynamically selected mixup machine unlearning\",\n        \"“Federated unlearning via class-discriminative pruning\",\n        \"“Model sparsification can simplify machine unlearning\",\n        \"“Sanity checks for lottery tickets: Does your winning ticket really win the jackpot?\",\n        \"“The lottery ticket hypothesis: Finding sparse\",\n        \"“Pruning neural networks without any data by iteratively conserving synaptic flow\",\n        \"“Federated unlearning with knowledge distillation\",\n        \"“Heterogeneous federated knowledge graph embedding learning and unlearning\",\n        \"“Neural attention distillation: Erasing backdoor triggers from deep neural networks\",\n        \"“Amnesiac machine learning\",\n        \"“Few-shot unlearning by model inversion\",\n        \"“A novel deep learning model compression algorithm\",\n        \"“Pruning by explaining: A novel criterion for deep neural network pruning\",\n        \"“Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks\",\n        \"“Knowledge from the original network: restore a better pruned network with knowledge distillation\",\n        \"“Model inversion attacks against collaborative inference\",\n        \"“Deep residual learning for image recognition\",\n        \"“Reading digits in natural images with unsupervised feature learning\",\n        \"“Imagenet: A large-scale hierarchical image database\",\n        \"“Matching networks for one shot learning\",\n        \"“Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop\",\n        \"“Learning word vectors for sentiment analysis\",\n        \"“A probabilistic analysis of the rocchio algorithm with tfidf for text categorization\",\n        \"“Automated learning of decision rules for text categorization\",\n        \"000+ questions for machine comprehension of text\",\n        \"“Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid\",\n        \"“The multi-purpose incremental learning system aq15 and its testing application to three medical domains\",\n        \"“Using the adap learning algorithm to forecast the onset of diabetes mellitus\",\n        \"” Physical Review E\",\n        \"“A public domain dataset for human activity recognition using smartphones\",\n        \"“Open graph benchmark: Datasets for machine learning on graphs\",\n        \"“Collective classification in network data\",\n        \"“Character-level convolutional networks for text classification\",\n        \"“Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories\",\n        \"“Microsoft coco: Common objects in context\",\n        \"“Face recognition in unconstrained videos with matched background similarity\",\n        \"“Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification\",\n        \"“Real-time prediction of online shoppers’ purchasing intention using multilayer perceptron and lstm recurrent neural networks\",\n        \"“Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine\",\n        \"“The movielens datasets: History and context\",\n        \"“Eternal sunshine of the spotless net: Selective forgetting in deep networks\",\n        \"“Continual lifelong learning with neural networks: A review\",\n        \"“Mixed-privacy forgetting in deep networks\",\n        \"“Unfolded self-reconstruction lsh: Towards machine unlearning in approximate nearest neighbour search\",\n        \"“Ml attack models: Adversarial attacks and data poisoning attacks\",\n        \"“Towards probabilistic verification of machine unlearning\",\n        \"“Backdoor detection in reinforcement learning\",\n        \"“One4all: Manipulate one agent to poison the cooperative multi-agent reinforcement learning\",\n        \"“Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency\",\n        \"“Don’t watch me: A spatio-temporal trojan attack on deep-reinforcement-learning-augment autonomous driving\",\n        \"“Hedgecut: Maintaining randomised trees for low-latency machine unlearning\",\n        \"“Poison neural network-based mmwave beam selection and detoxification with machine unlearning\",\n        \"“Towards deep learning models resistant to adversarial attacks\",\n        \"“Adversarial robustness via runtime masking and cleansing\",\n        \"“Learning fair representations\",\n        \"“On fairness and calibration\",\n        \"“An empirical study on the relation between network interpretability and adversarial robustness\",\n        \"” Science robotics\",\n        \"“Machine unlearning: Its need and implementation strategies\",\n        \"“The right to be forgotten in federated learning: An efficient realization with rapid retraining\",\n        \"“Ai model disgorgement: Methods and choices\",\n        \"“Interpretable data-based explanations for fairness debugging\",\n        \"“On the necessity of auditable algorithmic definitions for machine unlearning\",\n        \"“Mindful unlearning in unprecedented times: Implications for management and organizations\",\n        \"“Feature unlearning for generative models via implicit feedback\",\n        \"“Exploring linkages between unlearning and human resource development: Revisiting unlearning cases\",\n        \"“Responsibility beyond design: Physicians’ requirements for ethical medical AI\",\n        \"“The dawn of the AI robots: Towards a new framework of AI robot accountability\",\n        \"“A methodology to build interpretable machine learning models in organizations\",\n        \"“Learning together: Adapting methods for family and community research during a pandemic\",\n        \"“A survey on explainable artificial intelligence techniques and challenges\",\n        \"“Training and preparing tomorrow’s workforce for the fourth industrial revolution\",\n        \"“Artificial intelligence as a disruptive technology—a systematic literature review\",\n        \"“Organizational unlearning: A risky food safety strategy?\",\n        \"“Sketching an AI marketplace: Tech\",\n        \"“Addressing fairness\",\n        \"“Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients\",\n        \"“Exit through the training data: A look into instance-attribution explanations and efficient data deletion in machine learning\",\n        \"“Analysis of explainers of black box deep neural networks for computer vision: A survey\",\n        \"“Variational interaction information maximization for cross-domain disentanglement\",\n        \"“Speaker-invariant adversarial domain adaptation for emotion recognition\",\n        \"“A 2-step deep learning method with domain adaptation for multi-centre\",\n        \"“Large-scale multi-modal pre-trained models: A comprehensive survey\",\n        \"PhD thesis, University of Sheffield, 2022\",\n        \"“Posterior differential regularization with f-divergence for improving model robustness\",\n        \"“Embodied communication: How robots and people communicate through physical interaction\",\n        \"A. Schwarzschild\",\n        \"“Deep regression unlearning\",\n        \"“Certified graph unlearning\",\n        \"“Feature-guided black-box safety testing of deep neural networks\",\n        \"“A zest of lime: Towards architecture-independent model distances\",\n        \"“Quantitative evaluation of machine learning explanations: A human-grounded benchmark\",\n        \"“A human-grounded evaluation benchmark for local explanations of machine learning\",\n        \"“Trustworthy artificial intelligence: a review\",\n        \"“Domain adaptation and multi-domain adaptation for neural machine translation: A survey\",\n        \"“A review of generative adversarial networks in cancer imaging: New applications\",\n        \"“A survey on GAN techniques for data augmentation to address the imbalanced data issues in credit card fraud detection\",\n        \"“Data augmentation with improved regularisation and sampling for imbalanced blood cell image classification\",\n        \"“Prompt certified machine unlearning with randomized gradient smoothing and quantization\",\n        \"“Theory building with big data-driven research – moving away from the “what” towards the “why”\",\n        \"“Incentive techniques for the internet of things: a survey\",\n        \"“Distributed learning in healthcare\",\n        \"“Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware\",\n        \"“Challenges and limitations of social data analysis approaches\",\n        \"“Next-generation digital ecosystem for climate data mining and knowledge discovery: A review of digital data collection technologies\",\n        \"“Machine unlearning via gan\",\n        \"“Federated unlearning for on-device recommendation\",\n        \"“Subspace based federated unlearning\",\n        \"“Incremental and decremental support vector machine learning\",\n        \"“Incremental and decremental learning for linear support vector machines\",\n        \"“Incremental and decremental training for linear classification\",\n        \"“Efficient machine unlearning using general adversarial network\",\n        \"“Hidden poison: Machine unlearning enables camouflaged poisoning attacks\",\n        \"“A review of the trends and challenges in adopting natural language processing methods for education feedback analysis\",\n        \"“A review: Data pre-processing and data augmentation techniques\",\n        \"“Supervising model attention with human explanations for robust natural language inference\",\n        \"“Multi-aspects ai-based modeling and adversarial learning for cybersecurity intelligence and robustness: A comprehensive overview\",\n        \"“Spiking neural networks and online learning: An overview and perspectives\",\n        \"“Lifelong anomaly detection through unlearning\",\n        \"PhD thesis, 2023\",\n        \"privacy-preserving and federated machine learning in medical imaging\",\n        \"“Recommendation unlearning\",\n        \"“A discriminative account of the learning\",\n        \"“Trustworthy recommendation and search: Introduction to the special issue-part 1\",\n        \"“Counterfactually evaluating explanations in recommender systems\",\n        \"“Sociotechnical envelopment of artificial intelligence: An approach to organizational deployment of inscrutable artificial intelligence systems\"\n    ],\n    \"reasoning\": \"Selected these 217 references because they represent foundational work and recent advances in Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}