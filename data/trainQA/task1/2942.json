{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Rethinking Neural Operations for Diverse Tasks\n\nPaper Abstract: Abstract An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while being much more expressive: we prove that it includes many named operations across multiple application areas. Starting with any standard backbone such as ResNet, we show how to transform it into a search space over XD-operations and how to traverse the space using a simple weight-sharing scheme. On a diverse set of tasks—solving PDEs, distance prediction for protein folding, and music modeling—our approach consistently yields models with lower error than baseline networks and often even lower error than expert-designed domain-specific approaches.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A fully open-source framework for deep learning protein real-valued distances\",\n        \"Sparse linear networks with a fixed butterfly structure: Theory and practice\",\n        \"Butterfly transform: An efficient FFT based neural architecture design\",\n        \"Harmonising chorales by probabilistic inference\",\n        \"An empirical evaluation of generic convolutional and recurrent networks for sequence modeling\",\n        \"Trellis networks for sequence modeling\",\n        \"Random search for hyper-parameter optimization\",\n        \"Once-for-all: Train one network and specialize it for efficient deployment\",\n        \"Learning fast algorithms for linear transforms using butterfly factorizations\",\n        \"Kaleidoscope: An efficient, learnable representation for all structured linear maps\",\n        \"NAS-Bench-201: Extending the scope of reproducible neural architecture search\",\n        \"Neural architecture search: A survey\",\n        \"Densely connected search space for more flexible neural architecture search\",\n        \"HiPPO: Recurrent memory with optimal polynomial projections\",\n        \"Deep residual learning for image recognition\",\n        \"Long short-term memory\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments\",\n        \"Highly accurate protein structure prediction with alphafold\",\n        \"Adam: A method for stochastic optimization\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Object recognition with gradient-based learning\",\n        \"Random search and reproducibility for neural architecture search\",\n        \"Hyperband: A novel bandit-based approach to hyperparameter optimization\",\n        \"Geometry-aware gradient algorithms for neural architecture search\",\n        \"Diffusion convolutional recurrent neural network: Data-driven traffic forecasting\",\n        \"Butterfly factorization\",\n        \"Multidimensional butterfly factorization\",\n        \"Fourier neural operator for parametric partial differential equations\",\n        \"Auto-DeepLab: Hierarchical neural architecture search for semantic image segmentation\",\n        \"DARTS: Differentiable architecture search\",\n        \"AtomNAS: Fine-grained end-to-end neural architecture search\",\n        \"ACDC: A structured efficient linear layer\",\n        \"Fast neural architecture search of compact semantic segmentation models via auxiliary cells\",\n        \"Efficient neural architecture search via parameter sharing\",\n        \"AutoML-Zero: Evolving machine learning algorithms from scratch\",\n        \"U-Net: Convolutional networks for biomedical image segmentation\",\n        \"ImageNet large scale visual recognition challenge\",\n        \"Improved protein structure prediction using potentials from deep learning\",\n        \"DGM: A deep learning algorithm for solving partial differential equations\",\n        \"Attention is all you need\",\n        \"Textnas: A neural architecture search space tailored for text representation\",\n        \"R-Transformer: Recurrent neural network enhanced transformer\",\n        \"PC-DARTS: Partial channel connections for memory-efficient architecture search\",\n        \"NAS evaluation is frustratingly hard\",\n        \"NAS-Bench-101: Towards reproducible neural architecture search\",\n        \"Wide residual networks\",\n        \"NAS-Bench-1Shot1: Benchmarking and dissecting one-shot neural architecture search\",\n        \"Learning transferable architectures for scalable image recognition\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Rethinking Neural Operations for Diverse Tasks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}