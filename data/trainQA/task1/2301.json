{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 190 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey\n\nPaper Abstract: Abstract Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (rl) methods has been leading to the emergence of a new field: developmental reinforcement learning. Developmental rl is concerned with the use of deep rl algorithms to tackle a developmental problem — the intrinsically motivated acquisition of open-ended repertoires of skills. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard rl algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental rl and proposes a computational framework based on goal-conditioned rl to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.\n\n{retrieved_text}\n\nGenerate exactly 190 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 190\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Imitating Interactive Intelligence.\",\n        \"Variational option discovery algorithms.\",\n        \"Surprise-based intrinsic motivation for deep reinforcement learning.\",\n        \"DECSTR: Learning goal-directed abstract behaviors using pre-verbal spatial predicates in intrinsically motivated agents.\",\n        \"Neural module networks.\",\n        \"Hindsight experience replay.\",\n        \"Cognitive developmental robotics: A survey.\",\n        \"The option-critic architecture.\",\n        \"Agent57: Outperforming the atari human benchmark.\",\n        \"Never give up: Learning directed exploration strategies.\",\n        \"Learning to understand goal specifications by modelling reward.\",\n        \"Systematic generalization: What is required and can it be learned?.\",\n        \"Proximo-distal competence based curiosity-driven exploration.\",\n        \"R-iac: Robust intrinsically motivated exploration and active learning.\",\n        \"Intrinsically motivated goal exploration for active motor learning in robots: A case study.\",\n        \"Active learning of inverse models with intrinsically motivated goal exploration in robots.\",\n        \"The option keyboard: Combining skills in reinforcement learning.\",\n        \"Successor features for transfer in reinforcement learning.\",\n        \"Fast reinforcement learning with generalized policy updates.\",\n        \"Autonomous navigation of stratospheric balloons using reinforcement learning.\",\n        \"Unifying count-based exploration and intrinsic motivation.\",\n        \"Why Children Talk to Themselves.\",\n        \"Curiosity and exploration.\",\n        \"Smirl: Surprise minimizing rl in dynamic environments.\",\n        \"Control what you can: Intrinsically motivated task-planning agent.\",\n        \"Language models are few-shot learners.\",\n        \"Exploration by random network distillation.\",\n        \"Learning with amigo: Adversarially motivated intrinsic goals.\",\n        \"Explore, discover and learn: Unsupervised discovery of state-covering skills.\",\n        \"Developmental Robotics: From Babies to Robots.\",\n        \"Modularity, Language, and the Flexibility of Thought.\",\n        \"Multitask learning.\",\n        \"Specializing Versatile Skill Libraries using Local Mixture of Experts.\",\n        \"Actrce: Augmenting experience via teacher’s advice for multi-goal reinforcement learning.\",\n        \"Gated-attention architectures for task-oriented language grounding.\",\n        \"Plangan: Model-based planning with sparse rewards and multiple goals.\",\n        \"BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop.\",\n        \"Glib: Efficient exploration for relational model-based reinforcement learning via goal-literal babbling.\",\n        \"Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning.\",\n        \"Exploratory play, rational action, and efficient search.\",\n        \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models.\",\n        \"Higher: Improving instruction following with hindsight generation for experience replay.\",\n        \"Being There: Putting Brain, Body, and World Together Again.\",\n        \"End-to-end driving via conditional imitation learning.\",\n        \"Language-conditioned goal generation: a new approach to language grounding for rl.\",\n        \"Epidemioptim: A toolbox for the optimization of control policies in epidemiological models.\",\n        \"Language as a cognitive tool to imagine goals in curiosity driven exploration.\",\n        \"Scaling map-elites to deep neuroevolution.\",\n        \"CURIOUS: intrinsically motivated modular multi-goal reinforcement learning.\",\n        \"GEP-PG: decoupling exploration and exploitation in deep reinforcement learning algorithms.\",\n        \"An Empowerment-based Solution to Robotic Manipulation Tasks with Sparse Rewards.\",\n        \"Embodied question answering\",\n        \"From Embodied to Socially Embedded Agents – Implications for Interaction-Aware Robots\",\n        \"Feudal reinforcement learning\",\n        \"The Helmholtz Machine\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Goal-conditioned imitation learning\",\n        \"First return, then explore\",\n        \"The goal construct in psychology\",\n        \"Learning and development in neural networks: the importance of starting small\",\n        \"Hierarchically organized latent modules for exploratory search in morphogenetic systems\",\n        \"Rewriting history with inverse RL: hindsight inference for policy improvement\",\n        \"Diversity is all you need: Learning skills without a reward function\",\n        \"Discovering Generalizable Skills via Automated Generation of Diverse Tasks\",\n        \"Self-supervised learning of image embedding for continuous control\",\n        \"Automatic goal generation for reinforcement learning agents\",\n        \"Modular active curiosity-driven discovery of tool use\",\n        \"Intrinsically motivated goal exploration processes with automatic curriculum learning\",\n        \"Clic: Curriculum learning and imitation for object control in nonrewarding environments\",\n        \"Accuracy-based curriculum learning in deep reinforcement learning\",\n        \"Meta learning shared hierarchies\",\n        \"Generative adversarial nets\",\n        \"The scientist in the crib: Minds, brains, and how children learn\",\n        \"Towards a neuroscience of active sampling and curiosity\",\n        \"Recurrent independent mechanisms\",\n        \"Variational intrinsic control\",\n        \"On the role of planning in model-based deep reinforcement learning\",\n        \"Dynamical distance learning for semi-supervised and unsupervised skill discovery\",\n        \"Learning an embedding space for transferable robot skills\",\n        \"Grounded Language Learning in a Simulated 3D World\",\n        \"Deep q-learning from demonstrations\",\n        \"Emergent systematic generalization in a situated agent\",\n        \"Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text\",\n        \"Grounded language learning fast and slow\",\n        \"Open-Endedness for the Sake of Open-Endedness\",\n        \"Generative adversarial imitation learning\",\n        \"VIME: variational information maximizing exploration\",\n        \"Reinforcement learning with unsupervised auxiliary tasks\",\n        \"Language as an abstraction for hierarchical deep reinforcement learning\",\n        \"Learning to achieve goals\",\n        \"Maximizing Learning Progress: An Internal Reward System for Development\",\n        \"Grounding spatio-temporal language with transformers\",\n        \"The psychology and neuroscience of curiosity\",\n        \"Active world model learning with progress curiosity\",\n        \"Grimgep: Learning progress for robust goal sampling in visual deep reinforcement learning\",\n        \"Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation\",\n        \"One solution is not all you need: Few-shot extrapolation via structured maxent RL\",\n        \"Curiosity-driven multi-criteria hindsight experience replay\",\n        \"Evolving a diversity of virtual creatures through novelty search and local competition\",\n        \"Hierarchical reinforcement learning with hindsight\",\n        \"Towards practical multi-object manipulation using relational reinforcement learning\",\n        \"Social Situatedness of Natural and Artificial Intelligence: Vygotsky and Beyond\",\n        \"Adapting behavior via intrinsic reward: a survey and empirical study\",\n        \"Robust active binocular vision through intrinsically motivated learning\",\n        \"Exploration in model-based reinforcement learning by empirically estimating learning progress\",\n        \"Working memory graphs\",\n        \"A survey of reinforcement learning informed by natural language\",\n        \"What Do Words Do? Toward a Theory of Language-Augmented Thought\",\n        \"Learning latent plans from play\",\n        \"Grounding language in play\",\n        \"Unicorn: Continual learning with a universal, off-policy agent\",\n        \"Information driven self-organization of complex robotic behaviors\",\n        \"Automatic discovery of subgoals in reinforcement learning using diverse density\",\n        \"Towards a Vygotskyan Cognitive Robotics: The Role of Language as a Cognitive Tool\",\n        \"Human-level control through deep reinforcement learning\",\n        \"The Intersection of Planning and Learning\",\n        \"Variational information maximisation for intrinsically motivated reinforcement learning\",\n        \"Self-organization of early vocal development in infants and machines: The role of intrinsic motivation\",\n        \"Illuminating search spaces by mapping elites\",\n        \"Data-efficient hierarchical reinforcement learning\",\n        \"Contextual imagined goals for self-supervised robotic learning\",\n        \"Overcoming exploration in reinforcement learning with demonstrations\",\n        \"Visual reinforcement learning with imagined goals\",\n        \"Planning with goal-conditioned policies\",\n        \"Socially guided intrinsic motivation for robot learning of motor skills\",\n        \"Model Learning for Robot Control: A Survey\",\n        \"Zero-shot task generalization with multi-task deep reinforcement learning\",\n        \"Discovering Diverse Solutions in Deep Reinforcement Learning\",\n        \"Intrinsic Motivation Systems for Autonomous Mental Development\",\n        \"What is intrinsic motivation? a typology of computational approaches\",\n        \"How evolution may work through curiosity-driven developmental process\",\n        \"Curiosity-driven exploration by self-supervised prediction\",\n        \"Film: Visual reasoning with a general conditioning layer\",\n        \"Maximum entropy gain exploration for long horizon multi-goal reinforcement learning\",\n        \"Multi-goal reinforcement learning: Challenging robotics environments and request for research\",\n        \"Skew-fit: State-covering self-supervised reinforcement learning\",\n        \"Automatic curriculum learning for deep RL: A short survey\",\n        \"Teacher Algorithms for Curriculum Learning of Deep Rl in Continuously Parameterized Environments\",\n        \"Temporal Abstraction in Reinforcement Learning\",\n        \"Automated curricula through setter-solver interactions\",\n        \"RIDE: rewarding impact-driven exploration for procedurally-generated environments\",\n        \"Goal-driven learning\",\n        \"Successor options: An option discovery framework for reinforcement learning\",\n        \"Learning by playing solving sparse reward tasks from scratch\",\n        \"Curious hierarchical actor-critic reinforcement learning\",\n        \"Efficient exploratory learning of inverse kinematics on a bionic elephant trunk\",\n        \"Goal babbling permits direct learning of inverse kinematics\",\n        \"A benchmark for systematic generalization in grounded language understanding\",\n        \"Sequential Thought Processes in Pdp Models\",\n        \"Evolution strategies as a scalable alternative to reinforcement learning\",\n        \"Grail: a goal-discovering robotic architecture for intrinsically-motivated learning\",\n        \"Intrinsically motivated open-ended learning in autonomous robots\",\n        \"Universal value function approximators\",\n        \"General value function networks\",\n        \"Curious model-building control systems\",\n        \"Learning to generate sub-goals for action sequences\",\n        \"A possibility for implementing curiosity and boredom in model-building neural controllers\",\n        \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\",\n        \"Parameter-exploring policy gradients\",\n        \"Planning to explore via self-supervised world models\",\n        \"Dynamics-aware unsupervised discovery of skills\",\n        \"Towards teachable autonomous agents\",\n        \"Mastering the game of go with deep neural networks and tree search\",\n        \"Using relative novelty to identify useful temporal abstractions in reinforcement learning\",\n        \"Skill characterization based on betweenness\",\n        \"Intrinsically motivated reinforcement learning: An evolutionary perspective\",\n        \"Why Open-Endedness Matters\",\n        \"The Role of Subjectivity in the Evaluation of Open-Endedness\",\n        \"Open-ended learning leads to generally capable agents\",\n        \"Intrinsic motivation and automatic curricula via asymmetric self-play\",\n        \"Reinforcement learning: An introduction\",\n        \"Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction\",\n        \"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\n        \"Temporal-difference networks\",\n        \"A boolean task algebra for reinforcement learning\",\n        \"The Cultural Origins of Human Cognition\",\n        \"Constructing a Language\",\n        \"Attention is all you need\",\n        \"Many-goals reinforcement learning\",\n        \"Feudal networks for hierarchical reinforcement learning\",\n        \"Thought and Language\",\n        \"Unsupervised control through non-parametric discriminative rewards\",\n        \"Language, Thought, and Reality: Selected Writings of Benjamin Lee Whorf\",\n        \"Natural evolution strategies\",\n        \"The Role of Tutoring in Problem Solving\",\n        \"The laplacian in RL: learning representations with efficient approximations\",\n        \"Interactive language learning by question answering\",\n        \"Automatic curriculum learning through value disagreement\",\n        \"Target-driven visual navigation in indoor scenes using deep reinforcement learning\",\n        \"The Epigenesis of Meaning in Human Beings, and Possibly in Robots\"\n    ],\n    \"reasoning\": \"Selected these 190 references because they represent foundational work and recent advances in Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}