{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: History for Visual Dialog: Do we really need it?\n\nPaper Abstract: Abstract Visual Dialog involves “understanding” the dialog history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to generate the correct response. In this paper, we show that co-attention models which explicitly encode dialog history outperform models that don’t, achieving state-of-the-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowd-sourcing dataset collection procedure by showing that history is indeed only required for a small amount of the data and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisDialConv) of the VisDial val set and provide a benchmark of 63% NDCG.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Ensemble based discriminative models for visual dialog challenge 2018\",\n        \"Bottom-up and top-down attention for image captioning and visual question answering\",\n        \"VQA: Visual question answering\",\n        \"Layer normalization\",\n        \"Curriculum learning\",\n        \"Statistical class theory and probability calculation\",\n        \"Learning end-to-end goal-oriented dialog\",\n        \"Evaluating visual conversational agents via cooperative human-AI games\",\n        \"UNITER: Learning universal image-text representations\",\n        \"Visual dialog\",\n        \"Response to “visual dialogue without vision or dialogue”(massiceti et al., 2018)\",\n        \"Guesswhat?! visual object discovery through multi-modal dialogue\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction\",\n        \"Learning and development in neural networks: The importance of starting small\",\n        \"Multi-step reasoning via recurrent dual attention for visual dialog\",\n        \"Image-question-answer synergistic network for visual dialog\",\n        \"Dialog-based interactive image retrieval\",\n        \"The photobook dataset: Building common ground through visually-grounded dialogue\",\n        \"Deep residual learning for image recognition\",\n        \"Long short-term memory\",\n        \"Extending a parser to distant domains using a few dozen partially annotated examples\",\n        \"Dual attention networks for visual reference resolution in visual dialog\",\n        \"Bilinear attention networks\",\n        \"Codraw: Visual dialog for collaborative drawing\",\n        \"Adam: A method for stochastic optimization\",\n        \"Visual coreference resolution in visual dialog using neural module networks\",\n        \"CLEVR-dialog: A diagnostic dataset for multi-round reasoning in visual dialog\",\n        \"Visual genome: Connecting language and vision using crowdsourced dense image annotations\",\n        \"Use of ranks in one-criterion variance analysis\",\n        \"A diversity-promoting objective function for neural conversation models\",\n        \"A multimodal dialogue system for conversational image editing\",\n        \"How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation\",\n        \"What is it? disambiguating the different readings of the pronoun ‘it’\",\n        \"VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Edit me: A corpus and a framework for understanding natural language image editing\",\n        \"Conversational image editing: Incremental intent identification in a new dialogue task\",\n        \"Visual dialogue without vision or dialogue\",\n        \"Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering\",\n        \"Recursive visual attention in visual dialog\",\n        \"Why we need new evaluation metrics for nlg\",\n        \"Automatic differentiation in PyTorch\",\n        \"Glove: Global vectors for word representation\",\n        \"A structured review of the validity of bleu\",\n        \"Faster R-CNN: Towards real-time object detection with region proposal networks\",\n        \"Grounded agreement games: Emphasizing conversational grounding in visual dialogue settings\",\n        \"Visual reference resolution using attention memory for visual dialog\",\n        \"A hierarchical latent variable encoder-decoder model for generating dialogues\",\n        \"Ask no more: Deciding when to guess in referential visual dialogue\",\n        \"Lxmert: Learning cross-modality encoder representations from transformers\",\n        \"Tips and tricks for visual question answering: Learnings from the 2017 challenge\",\n        \"Attention is all you need\",\n        \"Individual comparisons by ranking methods\",\n        \"Better conversations by modeling, filtering, and optimizing for coherence and diversity”\",\n        \"Making history matter: Gold-critic sequence training for visual dialog\",\n        \"What you see is what you get: Visual pronoun coreference resolution in dialogues\",\n        \"Deep modular co-attention networks for visual question answering\",\n        \"BERTScore: Evaluating text generation with bert\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in History for Visual Dialog: Do we really need it?, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}