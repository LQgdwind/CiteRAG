{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 45 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Virgo: Cluster-level Matrix Unit Integration in GPUs for Scalability and Energy Efficiency\n\nPaper Abstract: Abstract. Modern GPUs incorporate specialized matrix units such as Tensor Cores to accelerate GEMM operations central to deep learning workloads. However, existing matrix unit designs are tightly coupled to the SIMT core, limiting the size and energy efficiency of the operation due to capacity and bandwidth constraints from the register file. Such a limitation in scalability makes it difficult to simultaneously enhance compute throughput and improve energy efficiency in GPUs. To address this challenge, we propose Virgo, a new GPU microarchitecture that integrates dedicated matrix units at the SIMT core cluster level. By physically disaggregating the matrix unit from the SIMT core, Virgo eliminates scalability constraints imposed by the core microarchitecture. Consequently, Virgo increases the granularity of operations at the hardware which not only improves data reuse, but also reduces the number of instructions processed in the SIMT core. This reduction in instruction processing decreases energy consumption within the core pipeline, thereby improving the system-level energy efficiency. Our evaluations, implemented in synthesizable RTL, demonstrate that Virgo achieves up to 66.3% reduction in active power and 77.2% reduction in active energy consumption of the system-on-chip compared to the baseline core-coupled design.\n\n{retrieved_text}\n\nGenerate exactly 45 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 45\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Amd cdna 2 architecture\",\n        \"Chipyard: Integrated design, simulation, and implementation framework for custom socs\",\n        \"The rocket chip generator\",\n        \"A case study in cuda kernel fusion: Implementing flashattention-2 on nvidia hopper architecture using the cutlass library\",\n        \"Understanding gpu power: A survey of profiling, modeling, and simulation methods\",\n        \"Language models are few-shot learners\",\n        \"Productive Design of Extensible On-Chip Memory Hierarchies\",\n        \"Nvidia tesla v100 gpu architecture\",\n        \"Nvidia a100 tensor core gpu datasheet\",\n        \"Nvidia ampere gpu architecture tuning guide\",\n        \"Nvidia h100 tensor core gpu datasheet\",\n        \"Parallel thread execution isa version 8.5\",\n        \"Flashattention: Fast and memory-efficient exact attention with io-awareness\",\n        \"Memory access coalescing: A technique for eliminating redundant memory accesses\",\n        \"A journey of a 1,000 kernels begins with a single step: A retrospective of deep learning on gpus\",\n        \"New cublas 12.0 features and matrix multiplication performance on nvidia hopper gpus\",\n        \"Dtc-spmm: Bridging the gap in accelerating general sparse matrix multiplication with tensor cores\",\n        \"Gemmini: An agile systolic array generator enabling systematic evaluations of deep-learning architectures\",\n        \"Demystifying the placement policies of the nvidia gpu thread block scheduler for concurrent kernels\",\n        \"Berkeley hardfloat\",\n        \"Towards power efficiency in deep learning on data center hardware\",\n        \"Rm-stc: Row-merge dataflow inspired gpu sparse tensor core for energy-efficient sparse acceleration\",\n        \"Introduction to the xe-hpg architecture\",\n        \"FireSim: FPGA-accelerated cycle-exact scale-out system simulation in the public cloud\",\n        \"Cutlass: Fast linear algebra in cuda c++\",\n        \"Duplo: Lifting redundant memory accesses of deep neural networks for gpu tensor cores\",\n        \"Interpret: Inter-warp register reuse for gpu tensor core\",\n        \"Ai-enabling workloads on large-scale gpu-accelerated system: Characterization, opportunities, and implications\",\n        \"Benchmarking and dissecting the nvidia hopper gpu architecture\",\n        \"Towards improved power management in cloud gpus\",\n        \"Modeling deep learning accelerator enabled gpus\",\n        \"Searching for activation functions\",\n        \"Characterizing the performance, power efficiency, and programmability of amd matrix cores\",\n        \"Compute trends across three eras of machine learning\",\n        \"Diplomatic design patterns : A tilelink case study\",\n        \"Dissecting tensor cores via microbenchmarks: Latency, throughput and numeric behaviors\",\n        \"Mad macce: Supporting multiply-add operations for democratizing matrix-multiplication accelerators\",\n        \"Fast implementation of dgemm on fermi gpu\",\n        \"Vortex: Extending the risc-v isa for gpgpu and 3d-graphics\",\n        \"Optimizing gpgpu kernel summation for performance and energy efficiency\",\n        \"Dual-side sparse tensor core\",\n        \"Demystifying tensor cores to optimize half-precision matrix multiply\",\n        \"Understanding the gpu microarchitecture to achieve bare-metal performance tuning\",\n        \"Simd2: A generalized matrix instruction set for accelerating tensor computation beyond gemm\",\n        \"Sustainable supercomputing for ai: Gpu power capping at hpc scale\"\n    ],\n    \"reasoning\": \"Selected these 45 references because they represent foundational work and recent advances in Virgo: Cluster-level Matrix Unit Integration in GPUs for Scalability and Energy Efficiency, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}