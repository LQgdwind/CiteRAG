{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 177 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, and Challenges\n\nPaper Abstract: Abstract. Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into agent model-explaining, reward-explaining, state-explaining, and task-explaining methods. We also review and highlight RL methods that conversely leverage human knowledge to promote learning efficiency and performance of agents while this kind of method is often ignored in XRL field. Some challenges and opportunities in XRL are discussed. This survey intends to provide a high-level summarization of XRL and to motivate future research on more effective XRL solutions. Corresponding open source codes are collected and categorized at https://github.com/Plankson/awesome-explainable-reinforcement-learning.\n\n{retrieved_text}\n\nGenerate exactly 177 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 177\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Local explanation methods for deep neural networks lack sensitivity to parameter values\",\n        \"Sanity checks for saliency maps\",\n        \"Towards reinforcement learning of human readable policies\",\n        \"Neurosymbolic reinforcement learning with formally verified exploration\",\n        \"Towards better interpretability in deep q-networks\",\n        \"Intrinsic dimension of data representations in deep neural networks\",\n        \"Marta: Leveraging human rationales for explainable text classification\",\n        \"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI\",\n        \"Dynamic Inverse Reinforcement Learning for Characterizing Animal Behavior\",\n        \"Agent57: Outperforming the atari human benchmark\",\n        \"Beyond accuracy: The role of mental models in human-AI team performance\",\n        \"An introduction to first-order logic\",\n        \"Verifiable Reinforcement Learning via Policy Extraction\",\n        \"Explaining reinforcement learning with shapley values\",\n        \"A distributional perspective on reinforcement learning\",\n        \"Deep Learning\",\n        \"How to recommend? User trust factors in movie recommender systems\",\n        \"Dota 2 with large scale deep reinforcement learning\",\n        \"Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning\",\n        \"Dot-to-dot: Explainable hierarchical reinforcement learning for robotic manipulation\",\n        \"Explaining recommendations: Satisfaction vs. promotion\",\n        \"Analyzing and validating neural networks predictions\",\n        \"Toward Policy Explanations for Multi-Agent Reinforcement Learning\",\n        \"Explainable Multi-Agent Reinforcement Learning for Temporal Queries\",\n        \"The user’s mental model of an information retrieval system: An experiment on a prototype online catalog\",\n        \"Exploration by random network distillation\",\n        \"Interference management for cellular-connected UAVs: A deep reinforcement learning approach\",\n        \"Model-free deep reinforcement learning for urban autonomous driving\",\n        \"Conditional DQN-Based Motion Planning With Fuzzy Logic for Autonomous Driving\",\n        \"ES-DQN: A Learning Method for Vehicle Intelligent Speed Control Strategy Under Uncertain Cut-In Scenario\",\n        \"Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning\",\n        \"Attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving\",\n        \"Makespan Optimisation in Cloudlet Scheduling with Improved DQN Algorithm in Cloud Computing\",\n        \"Deep reinforcement learning from human preferences\",\n        \"Intellingo: An intelligible translation environment\",\n        \"Evolutionary learning of interpretable decision trees\",\n        \"Agent-based Learning for Auto-Navigation within the Virtual City\",\n        \"Explainable reinforcement learning for broad-xai: A conceptual framework and survey\",\n        \"A roadmap for a rigorous science of interpretability\",\n        \"Techniques for interpretable machine learning\",\n        \"Deep-reinforcement-learning-based autonomous voltage control for power grid operations\",\n        \"Automated rationale generation: a technique for explainable AI and its effects on human perceptions\",\n        \"Driverless car: Autonomous driving using deep reinforcement learning in urban environment\",\n        \"Markov decision processes: Discrete stochastic dynamic programming (martin l. puterman)\",\n        \"Actor-critic continuous state reinforcement learning for wind-turbine control robust optimization\",\n        \"Counterfactual multi-agent policy gradients\",\n        \"Deep fair models for complex data: Graphs labeling and explainable face recognition\",\n        \"Addressing function approximation error in actor-critic methods\",\n        \"Application of instruction-based behavior explanation to a reinforcement learning agent with changing policy\",\n        \"Autonomous self-explanation of behavior for interactive reinforcement learning agents\",\n        \"Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective\",\n        \"Skanda Vaidyanath\",\n        \"How should I explain? A comparison of different explanation types for recommender systems\",\n        \"Review and comparison of methods to study the contribution of variables in artificial neural network models\",\n        \"Explainable active learning (xal) toward ai explanations as interfaces for machine teachers\",\n        \"A Survey on Interpretable Reinforcement Learning\",\n        \"Estimating information flow in deep neural networks\",\n        \"Actor-critic with familiarity-based trajectory experience replay\",\n        \"Explanation augmented feedback in human-in-the-loop reinforcement learning\",\n        \"Mohammad Gheshlaghi Azar\",\n        \"Matthew Hausknecht and Peter Stone\",\n        \"Emergence of locomotion behaviours in rich environments\",\n        \"Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies\",\n        \"Interpretable policies for reinforcement learning by genetic programming\",\n        \"A questionnaire measure of achievement motivation\",\n        \"Hado Van Hasselt\",\n        \"Explainability in deep reinforcement learning\",\n        \"Collective explainable AI: Explaining cooperative strategies and agent contribution in multiagent reinforcement learning with shapley values\",\n        \"Distilling the knowledge in a neural network\",\n        \"Combining planning and deep reinforcement learning in tactical decision making for autonomous driving\",\n        \"Metrics for explainable AI: Challenges and prospects\",\n        \"Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps\",\n        \"Romain Vuillemot\",\n        \"Shixiang Shane Gu\",\n        \"Zhengyao Jiang and Shan Luo\",\n        \"Hankz Hankui Zhuo\",\n        \"Jessica R Hullman\",\n        \"Oluwasanmi Koyejo\",\n        \"Martin Wattenberg\",\n        \"The (un) reliability of saliency methods\",\n        \"Margaret Burnett\",\n        \"Menaka Narayanan\",\n        \"Brenden K Petersen\",\n        \"Russ R Salakhutdinov\",\n        \"An Environment for Autonomous Driving Decision-Making\",\n        \"Social attention for autonomous decision-making in dense traffic\",\n        \"Research into questionnaire design: A summary of the literature\",\n        \"Continuous control with deep reinforcement learning\",\n        \"Brian Y Lim and Anind K Dey\",\n        \"Deep reinforcement learning for economic dispatch of virtual power plant in internet of energy\",\n        \"The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery\",\n        \"Forecasting the Market with Machine Learning Algorithms: An Application of NMC-BERT-LSTM-DQN-X Algorithm in Quantitative Trading\",\n        \"Towards explainable NLP: A generative explanation framework for text classification\",\n        \"Transmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach Based on Multi-Task Attribution Map\",\n        \"Curricular Subgoals for Inverse Reinforcement Learning\",\n        \"Progressive decision-making framework for power system topology control\",\n        \"Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning\",\n        \"Sequence modeling of temporal credit assignment for episodic reinforcement learning\",\n        \"Using genetic programming with prior formula knowledge to solve symbolic regression problem\",\n        \"Raphael Fonteneau\",\n        \"Katherine Driggs-Campbell\",\n        \"Improving the Diversity of Bootstrapped DQN via Noisy Priors\",\n        \"Explainable Reinforcement Learning: A Survey and Comparative Review\",\n        \"MAVIPER: Learning Decision Tree Policies for Interpretable Multi-Agent Reinforcement Learning\",\n        \"Explanation in artificial intelligence: Insights from the social sciences\",\n        \"Siddharth Karamcheti\",\n        \"Adria Puigdomenech Badia\",\n        \"Playing atari with deep reinforcement learning\",\n        \"Human-level control through deep reinforcement learning\",\n        \"A multidisciplinary survey and framework for design and evaluation of explainable AI systems\",\n        \"Interpretable machine learning\",\n        \"Human trial-and-error learning\",\n        \"Counterfactual state explanations for reinforcement learning agents via generative deep learning\",\n        \"Applying DQN solutions in fog-based vehicular networks: Scheduling, caching, and collision control\",\n        \"Inductive logic programming via differentiable deep neural logic networks\",\n        \"Incorporating relational background knowledge into reinforcement learning via differentiable inductive logic programming\",\n        \"and Prithviraj Ammanabrolu\",\n        \"Aaron Powers and Sara Kiesler\",\n        \"Erika Puiutta and Eric Veith\",\n        \"Emilee Rader and Rebecca Gray\",\n        \"Huáscar Espinoza\",\n        \"Model-agnostic interpretability of machine learning\",\n        \"Christopher Clarizio\",\n        \"Leveraging human knowledge in tabular reinforcement learning: A study of human subjects\",\n        \"Introduction to the Shapley value\",\n        \"Conservative q-improvement: Reinforcement learning for an interpretable decision-tree policy\",\n        \"Measuring mental models: Choosing the right tools for the job\",\n        \"The secrets of machine learning: ten things you wish you had known earlier to be more effective at data analysis\",\n        \"Interpretable machine learning: Fundamental principles and 10 grand challenges\",\n        \"Tobias Schürmann\",\n        \"Prioritized experience replay\",\n        \"High-dimensional continuous control using generalized advantage estimation\",\n        \"Proximal policy optimization algorithms\",\n        \"Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations\",\n        \"Niviru Wijayaratne\",\n        \"Efficient hierarchical policy network with fuzzy rules\",\n        \"Andrew Silva and Matthew Gombolay\",\n        \"Matthew Gombolay\",\n        \"Mastering the game of go without human knowledge\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"autoBOT: evolving neuro-symbolic representations for explainable low resource text classification\",\n        \"Components and characteristics of the dopamine reward utility signal\",\n        \"Reinforcement learning: An introduction\",\n        \"Efficient processing of deep neural networks: A tutorial and survey\",\n        \"Dragomir Anguelov\",\n        \"Aaquib Tabrez and Bradley Hayes\",\n        \"Stephanie Milani\",\n        \"Nicholay Topin and Manuela Veloso\",\n        \"Contrastive explanations for reinforcement learning in terms of expected consequences\",\n        \"Sharan Vidash Vidya Shanmugham\",\n        \"Vijayaraghavan Murali\",\n        \"Explainable Deep Reinforcement Learning: State of the Art and Challenges\",\n        \"Sebastian Pokutta\",\n        \"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks\",\n        \"Pin Wang and Ching-Yao Chan\",\n        \"Deep reinforcement learning for autonomous driving\",\n        \"Explainable ai and reinforcement learning—a systematic review of current approaches and trends\",\n        \"‘Mental model’comparison of automated and human scoring\",\n        \"A survey of preference-based reinforcement learning methods\",\n        \"Model primitives for hierarchical lifelong reinforcement learning\",\n        \"Khimya Khetarpal\",\n        \"Alexandros Karatzoglou\",\n        \"Joey Tianyi Zhou\",\n        \"Perceiving the world: Question-guided reinforcement learning for text-based games\",\n        \"Artificial general intelligence and the human mental model\",\n        \"Two-timescale voltage control in distribution grids using deep reinforcement learning\",\n        \"Dynamic energy dispatch strategy for integrated energy system based on improved deep reinforcement learning\",\n        \"Learn to interpret atari agents\",\n        \"Explainable Reinforcement Learning via a Causal World Model\",\n        \"Learning state importance for preference-based reinforcement learning\",\n        \"Faster and safer training by embedding high-level knowledge into deep reinforcement learning\",\n        \"Explainable ai in deep reinforcement learning models for power system emergency control\",\n        \"KoGuN: accelerating deep reinforcement learning via integrating human suboptimal knowledge\",\n        \"Leveraging human guidance for deep reinforcement learning tasks\",\n        \"Luke Zettlemoyer\",\n        \"Combined heat and power system intelligent economic dispatch: A deep reinforcement learning approach\",\n        \"Differentiable logic machines\"\n    ],\n    \"reasoning\": \"Selected these 177 references because they represent foundational work and recent advances in A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, and Challenges, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}