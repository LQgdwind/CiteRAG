{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 61 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Solving morphological analogies: from retrieval to generation\n\nPaper Abstract: Abstract Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we summarize these findings and we extend them by combining ANNr and the AE embedding model, and checking the performance of ANNc as an retrieval method. The combination of ANNr and AE outperforms the other approaches in almost all cases, and ANNc as a retrieval method achieves competitive or better performance than 3CosMul. We conclude with general guidelines on using our framework to tackle APs with DL.\n\n{retrieved_text}\n\nGenerate exactly 61 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 61\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"On the measure of intelligence\",\n        \"Analogy making as a complex adaptive system\",\n        \"Learning to rank based on analogical reasoning\",\n        \"Analogical embedding for analogy-based learning to rank\",\n        \"Analogical proportion-based methods for recommendation - first investigations\",\n        \"Abstraction and analogy-making in artificial intelligence\",\n        \"Analogy-preserving functions: A way to extend boolean samples\",\n        \"Improvements in analogical learning: Application to translating multi-terms of the medical domain\",\n        \"Morphological predictability of unseen words using computational analogy\",\n        \"Solving word analogies: A machine learning perspective\",\n        \"Classifying and completing word analogies by machine learning\",\n        \"Solving analogies on words based on minimal complexity transformation\",\n        \"A neural approach for detecting morphological analogies\",\n        \"On the Transferability of Neural Models of Morphological Analogies\",\n        \"Tackling morphological analogies using deep learning - extended version\",\n        \"Solving Morphological Analogies Through Generation\",\n        \"A Deep Learning Approach to Solving Morphological Analogies\",\n        \"Transferring Learned Models of Morphological Analogy\",\n        \"Visalogy: Answering visual analogy questions\",\n        \"Detecting unseen visual relations using analogies\",\n        \"An analogy based approach for solving target sense verification\",\n        \"The copycat project: A model of mental fluidity and analogy-making\",\n        \"A complexity based approach for solving hofstadter’s analogies\",\n        \"Tools for the production of analogical grids and a resource of n-gram analogical grids in 11 languages\",\n        \"Siganalogies - Morphological Analogies from Sigmorphon 2016 and 2019\",\n        \"Analogy and formal languages\",\n        \"Analogical dissimilarity: Definition, algorithms and two experiments in machine learning\",\n        \"Analogical proportions\",\n        \"Analogy between concepts\",\n        \"Saussurian analogy: a theoretical account and its application\",\n        \"A short introduction to computational trends in analogical reasoning\",\n        \"De l’analogie rendant compte de la commutation en linguistique\",\n        \"Behavior of Analogical Inference w.r.t. Boolean Functions\",\n        \"Galois theory for analogical classifiers\",\n        \"Glove: Global vectors for word representation\",\n        \"Finite-state transducers solving analogies on words\",\n        \"Character-position arithmetic for analogy questions between word forms\",\n        \"Vector-to-sequence models for sentence analogies\",\n        \"On understanding character-level models for representing morphology\",\n        \"Dependency-based word embeddings\",\n        \"BERT: pre-training of deep bidirectional transformers for language understanding\",\n        \"Efficient estimation of word representations in vector space\",\n        \"Enriching word vectors with subword information\",\n        \"Backpropagation applied to handwritten zip code recognition\",\n        \"Nonlinear principal component analysis using autoassociative neural networks\",\n        \"Lstm can solve hard long time lag problems\",\n        \"Character-level recurrent sequence-to-sequence model\",\n        \"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\",\n        \"Linguistic regularities in continuous space word representations\",\n        \"wav2vec 2.0: A framework for self-supervised learning of speech representations\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"The sigmorphon 2016 shared task–morphological reinflection\",\n        \"The SIGMORPHON 2019 shared task: Morphological analysis in context and cross-lingual transfer for inflection\",\n        \"Subcharacter information in japanese embeddings: when is it worth it?\",\n        \"Boundaries in a morphological grammar of portuguese\",\n        \"Neural Networks: a Comprehensive Foundation\",\n        \"Random decision forests\",\n        \"Support-vector networks\",\n        \"Exploring analogical inference in healthcare\",\n        \"Adam: A method for stochastic optimization\",\n        \"Incorporating nesterov momentum into adam\"\n    ],\n    \"reasoning\": \"Selected these 61 references because they represent foundational work and recent advances in Solving morphological analogies: from retrieval to generation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}