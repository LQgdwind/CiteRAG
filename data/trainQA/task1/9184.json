{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 54 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning\n\nPaper Abstract: Abstract Thompson sampling (TS) is widely used in sequential decision making due to its ease of use and appealing empirical performance. However, many existing analytical and empirical results for TS rely on restrictive assumptions on reward distributions, such as belonging to conjugate families, which limits their applicability in realistic scenarios. Moreover, sequential decision making problems are often carried out in a batched manner, either due to the inherent nature of the problem or to serve the purpose of reducing communication and computation costs. In this work, we jointly study these problems in two popular settings, namely, stochastic multi-armed bandits (MABs) and infinite-horizon reinforcement learning (RL), where TS is used to learn the unknown reward distributions and transition dynamics, respectively. We propose batched Langevin Thompson Sampling algorithms that leverage MCMC methods to sample from approximate posteriors with only logarithmic communication costs in terms of batches. Our algorithms are computationally efficient and maintain the same order-optimal regret guarantees of ùí™‚Äã(log‚Å°T)ùí™ùëá\\mathcal{O}(\\log T) for stochastic MABs, and ùí™‚Äã(T)ùí™ùëá\\mathcal{O}(\\sqrt{T}) for RL. We complement our theoretical findings with experimental results.\n\n{retrieved_text}\n\nGenerate exactly 54 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 54\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Bayesian optimal control of smoothly parameterized systems\",\n        \"Analysis of thompson sampling for the multi-armed bandit problem\",\n        \"Further optimal regret bounds for thompson sampling\",\n        \"Optimistic posterior sampling for reinforcement learning: worst-case regret bounds\",\n        \"Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps\",\n        \"Dynamic programming and optimal control: Volume I, volume 1\",\n        \"Steady state analysis of episodic reinforcement learning\",\n        \"Safe imitation learning via fast bayesian reward inference from preferences\",\n        \"An empirical evaluation of thompson sampling\",\n        \"Convergence of langevin mcmc in kl-divergence\",\n        \"Batched multi-armed bandits problem\",\n        \"Thompson Sampling for Learning Parameterized Markov Decision Processes\",\n        \"Solving two-armed bernoulli bandit problems using a bayesian learning automaton\",\n        \"Better optimism by bayes: Adaptive planning with rich models\",\n        \"Soft actor-critic algorithms and applications\",\n        \"Optimality of thompson sampling for gaussian bandits depends on priors\",\n        \"Mirrored langevin dynamics\",\n        \"Bayesian control of large mdps with unknown dynamics in data-poor environments\",\n        \"Near-optimal regret bounds for reinforcement learning\",\n        \"Is q-learning provably efficient?\",\n        \"Provablylearning to optimize via posterior sampling efficient reinforcement learning with linear function approximation\",\n        \"Thompson sampling in non-episodic restless bandits\",\n        \"Batched thompson sampling\",\n        \"Parallelizing thompson sampling\",\n        \"Thompson sampling: An asymptotically optimal finite-time analysis\",\n        \"Stochastic first-order methods for average-reward markov decision processes\",\n        \"Ensemble sampling\",\n        \"A complete recipe for stochastic gradient mcmc\",\n        \"Optimistic bayesian sampling in contextual-bandit problems\",\n        \"On approximate Thompson sampling with Langevin algorithms\",\n        \"(more) efficient reinforcement learning via posterior sampling\",\n        \"Model-based reinforcement learning and the eluder dimension\",\n        \"Posterior sampling for reinforcement learning without episodes\",\n        \"Learning unknown markov decision processes: A thompson sampling approach\",\n        \"Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling\",\n        \"Learning to optimize via posterior sampling\",\n        \"Customer acquisition via display advertising using multi-armed bandit experiments\",\n        \"An analysis of model-based interval estimation for markov decision processes\",\n        \"A bayesian framework for reinforcement learning\",\n        \"An interactive points of interest guidance system\",\n        \"Posterior sampling for large scale reinforcement learning\",\n        \"On the likelihood that one unknown probability exceeds another in view of the evidence of two samples\",\n        \"Towards minimax optimal reinforcement learning in factored markov decision processes\",\n        \"Near-optimal optimistic reinforcement learning using empirical bernstein inequalities\",\n        \"Linear bandits with stochastic delayed feedback\",\n        \"Learning infinite-horizon average-reward mdps with linear function approximation\",\n        \"Model-free reinforcement learning in infinite-horizon average-reward markov decision processes\",\n        \"Bayesian learning via stochastic gradient langevin dynamics\",\n        \"Nearly minimax optimal regret for learning infinite-horizon average-reward mdps with linear function approximation\",\n        \"Langevin monte carlo for contextual bandits\",\n        \"Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound\",\n        \"On function approximation in reinforcement learning: Optimism in the face of large state spaces\",\n        \"Feel-good thompson sampling for contextual bandits and reinforcement learning\",\n        \"Neural thompson sampling\"\n    ],\n    \"reasoning\": \"Selected these 54 references because they represent foundational work and recent advances in Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}