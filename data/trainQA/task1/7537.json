{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Transformers for End-to-End InfoSec Tasks: A Feasibility Study\n\nPaper Abstract: Abstract. Training a machine learning (ML) model from raw information security (InfoSec) data involves utilizing distinct data types and input formats that require unique considerations compared to more conventional applications of ML like natural language processing (NLP) and computer vision (CV). In this paper, we assess the viability of transformer models in end-to-end InfoSec settings, in which no intermediate feature representations or processing steps occur outside the model. We implement transformer models for two distinct InfoSec data formats – specifically URLs and PE files – in a novel end-to-end approach, and explore a variety of architectural designs, training regimes, and experimental settings to determine the ingredients necessary for performant detection models. We show that in contrast to conventional transformers trained on more standard NLP–related tasks, our URL transformer model requires a different training approach to reach high performance levels. Specifically, we show that 1) pre-training on a massive corpus of unlabeled URL data for an auto-regressive task does not readily transfer to binary classification of malicious or benign URLs, but 2) that using an auxiliary auto-regressive loss improves performance when training from scratch. We introduce a method for mixed objective optimization, which dynamically balances contributions from both loss terms so that neither one of them dominates. We show that this method yields quantitative evaluation metrics comparable to that of several top-performing benchmark classifiers. Unlike URLs, binary executables contain longer and more distributed sequences of information-rich bytes. To accommodate such lengthy byte sequences, we introduce additional context length into the transformer by providing its self-attention layers with an adaptive span similar to Sukhbaatar et al. We demonstrate that this approach performs comparably to well-established malware detection models on benchmark PE file datasets, but also point out the need for further exploration into model improvements in scalability and compute efficiency.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Optuna: A next-generation hyperparameter optimization framework\",\n        \"Source code authorship attribution using long short-term memory based networks\",\n        \"Automated big text security classification\",\n        \"Automated big security text pruning and classification\",\n        \"Automated us diplomatic cables security classification: Topic model pruning vs. classification based on clusters\",\n        \"EMBER: an open dataset for training static pe malware machine learning models\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Longformer: The long-document transformer\",\n        \"Generating long sequences with sparse transformers\",\n        \"Activation analysis of a byte-based deep neural network for malware classification\",\n        \"Machine learning for email spam filtering: review, approaches and open research problems\",\n        \"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context\",\n        \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n        \"Automatic Malware Description via Attribute Tagging and Similarity Embedding\",\n        \"SMART: Semantic Malware Attribute Relevance Tagging\",\n        \"Fast and three-rious: Speeding up weak supervision with triplet methods\",\n        \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups\",\n        \"MtNet: a multi-task neural network for dynamic malware classification\",\n        \"Reformer: The Efficient Transformer\",\n        \"ImageNet classification with deep convolutional neural networks\",\n        \"Learning from Context: A Multi-View Deep Learning Architecture for Malware Detection\",\n        \"ALBERT: A lite bert for self-supervised learning of language representations\",\n        \"URLNet: Learning a URL representation with deep learning for malicious URL detection\",\n        \"I-MAD: A Novel Interpretable Malware Detector Using Hierarchical Transformer\",\n        \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n        \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"Explaining vulnerabilities of deep learning to adversarial malware binaries\",\n        \"Malware classification with recurrent networks\",\n        \"Scikit-learn: Machine learning in Python\",\n        \"Trex: Learning execution semantics from micro-traces for binary similarity\",\n        \"Improving language understanding by generative pre-training\",\n        \"Malware detection by eating a whole EXE\",\n        \"KiloGrams: Very Large N-Grams for Malware Classification\",\n        \"Mockingbird: Defending against deep-learning-based website fingerprinting attacks with adversarial traces\",\n        \"Detection of phishing websites using an efficient feature-based machine learning framework\",\n        \"Snorkel: Rapid training data creation with weak supervision\",\n        \"ALOHA: Auxiliary Loss Optimization for Hypothesis Augmentation\",\n        \"Moon: A mixed objective optimization network for the recognition of facial attributes\",\n        \"Meade: Towards a malicious email attachment detection engine\",\n        \"A survey of stealth malware attacks, mitigation measures, and steps toward autonomous open world solutions\",\n        \"Malicious URL detection using machine learning: A survey\",\n        \"eXpose: A character-level convolutional neural network with embeddings for detecting malicious URLs, file paths and registry keys\",\n        \"Weaponizing data science for social engineering: Automated E2E spear phishing on Twitter\",\n        \"Triplet Fingerprinting: More Practical and Portable Website Fingerprinting with N-shot Learning\",\n        \"Adaptive Attention Span in Transformers\",\n        \"Are Pretrained Convolutions Better than Pretrained Transformers?\",\n        \"Attention is all you need\",\n        \"Character level based detection of DGA domain names\",\n        \"Anderson Nascimento\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Transformers for End-to-End InfoSec Tasks: A Feasibility Study, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}