{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 68 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Super-resolution and Robust Sparse Continuous Fourier Transform in Any Constant Dimension: Nearly Linear Time and Sample Complexity1footnote 11footnote 1A preliminary version of this paper appears at ACM-SIAM Symposium on Discrete Algorithms (SODA 2023).\n\nPaper Abstract: The ability to resolve detail in the object that is being imaged, named by resolution, is the core parameter of an imaging system. Super-resolution is a class of techniques that can enhance the resolution of an imaging system and even transcend the diffraction limit of systems. Despite huge success in the application, super-resolution is not well understood on the theoretical side, especially for any dimension d‚â•2ùëë2d\\geq 2. In particular, in order to recover a kùëòk-sparse signal, all previous results suffer from either/both poly‚Å°(k)polyùëò\\operatorname{poly}(k) samples or running time. We design robust algorithms for any (constant) dimension under a strong noise model based on developing some new techniques in Sparse Fourier transform (Sparse FT), such as inverting a robust linear system, ‚Äúeggshell‚Äù sampling schemes, and partition and voting methods in high dimension. These algorithms are the first to achieve running time and sample complexity (nearly) linear in the number of source points and logarithmic in bandwidth for any constant dimension, and we believe the techniques developed in the work can find their further applications on the Super-resolution and Sparse FT problem.\n\n{retrieved_text}\n\nGenerate exactly 68 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 68\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Restricted isometry property for general p-norms\",\n        \"What‚Äôs the frequency, Kenneth?: Sublinear Fourier sampling off the grid\",\n        \"Iterative thresholding for sparse approximations\",\n        \"On a modification of chebyshev‚Äôs inequality and of the error formula of laplace\",\n        \"Nearly-optimal bounds for sparse recovery in generic norms, with applications to kùëòk-median sketching\",\n        \"Continuous analogues of series\",\n        \"Sparse fourier transform by traversing cooley-tukey fft computation graphs\",\n        \"An improved lower bound for sparse reconstruction from subsampled hadamard matrices\",\n        \"Solving tall dense linear programs in nearly linear time\",\n        \"An improved estimate in the restricted isometry problem\",\n        \"Spectral compressed sensing via structured matrix completion\",\n        \"Towards a mathematical theory of super-resolution\",\n        \"Super-resolution from noisy data\",\n        \"Distributions of angles in random packing on spheres\",\n        \"Restricted isometry of Fourier matrices and list decodability of random linear codes\",\n        \"A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations\",\n        \"Fourier-sparse interpolation without a frequency gap\",\n        \"An adaptive sublinear-time block sparse Fourier transform\",\n        \"Introduction to algorithms\",\n        \"Learning mixtures of linear regressions in subexponential time via Fourier moments\",\n        \"Algorithmic foundations for the diffraction limit\",\n        \"Active regression via linear-sample sparsification\",\n        \"Estimating the frequency of a clustered signal\",\n        \"Near-optimal signal recovery from random projections: Universal encoding strategies?\",\n        \"Lower bounds for sparse recovery\",\n        \"Compressed sensing\",\n        \"Faster integer multiplication\",\n        \"Approximate sparse recovery: optimizing time and measurements\",\n        \"Improved time bounds for near-optimal sparse Fourier representations\",\n        \"Nearly optimal sparse Fourier transform\",\n        \"Simple and practical algorithm for sparse Fourier transform\",\n        \"Super-resolution off the grid\",\n        \"Probability inequalities for sums of bounded random variables\",\n        \"The restricted isometry property of subsampled Fourier matrices\",\n        \"Matrix pencil method for estimating parameters of exponentially damped/undamped sinusoids in noise\",\n        \"Sample-optimal Fourier sampling in any constant dimension\",\n        \"(Nearly) Sample-optimal sparse Fourier transform\",\n        \"K-median clustering, model-based compressive sensing, and sparse recovery for earth mover distance\",\n        \"On the power of adaptivity in sparse recovery\",\n        \"On model-based rip-1 matrices\",\n        \"Faster dynamic matrix inverse for faster lps\",\n        \"Sparse Fourier transform in any constant dimension with nearly-optimal sample complexity in sublinear time\",\n        \"Sample efficient estimation and recovery in sparse FFT via isolation on average\",\n        \"Adaptive sparse recovery with limited adaptivity\",\n        \"A multivariate generalization of prony‚Äôs method\",\n        \"Computational geometry algorithms and applications\",\n        \"Dimension-independent sparse Fourier transform\",\n        \"Faster ridge regression via the subsampled randomized hadamard transform\",\n        \"Deterministic sparse Fourier transform with an ‚Ñì‚àûsubscript‚Ñì guarantee\",\n        \"Heavy hitters via cluster-preserving clustering\",\n        \"Solving empirical risk minimization in the current matrix multiplication time\",\n        \"The threshold for super-resolution via extremal functions\",\n        \"A new class of fully discrete sparse fourier transforms: Faster stable implementations with guarantees\",\n        \"Stronger L2/L2 compressed sensing; without iterating\",\n        \"(Nearly) Sample-optimal sparse Fourier transform in any dimension; RIPless and Filterless\",\n        \"The retrieval of harmonics from a covariance function\",\n        \"Efficient sketches for the set query problem\",\n        \"Sparse recovery and Fourier sampling\",\n        \"A robust sparse Fourier transform in the continuous setting\",\n        \"Lower bounds for adaptive sparse recovery\",\n        \"On sparse reconstruction from Fourier and Gaussian measurements\",\n        \"Matrix Theory: Optimization, Concentration and Algorithms\",\n        \"Understanding machine learning: From theory to algorithms\",\n        \"Sparse fourier transform over lattices: A unified approach to signal reconstruction\",\n        \"List of references on spectral line analysis\",\n        \"An introduction to matrix concentration inequalities\",\n        \"Introduction to the non-asymptotic analysis of random matrices\",\n        \"High-dimensional probability: An introduction with applications in data science, volume 47\"\n    ],\n    \"reasoning\": \"Selected these 68 references because they represent foundational work and recent advances in Super-resolution and Robust Sparse Continuous Fourier Transform in Any Constant Dimension: Nearly Linear Time and Sample Complexity1footnote 11footnote 1A preliminary version of this paper appears at ACM-SIAM Symposium on Discrete Algorithms (SODA 2023)., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}