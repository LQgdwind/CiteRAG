{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: Abstract Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as “rule-based reasoning” and “case-based reasoning”. Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Generalization on the unseen, logic reasoning and degree curriculum\",\n        \"What learning algorithm is in-context learning? investigations with linear models\",\n        \"Exploring length generalization in large language models\",\n        \"Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models\",\n        \"Language models are few-shot learners\",\n        \"Sparks of artificial general intelligence: Early experiments with gpt-4\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers\",\n        \"Faith and fate: Limits of transformers on compositionality\",\n        \"Towards revealing the mystery behind chain of thought: A theoretical perspective\",\n        \"What can transformers learn in-context? a case study of simple function classes\",\n        \"Towards a mechanistic interpretation of multi-step reasoning capabilities of language models\",\n        \"The impact of positional encoding on length generalization in transformers\",\n        \"Decomposed prompting: A modular approach for solving complex tasks\",\n        \"Large language models are zero-shot reasoners\",\n        \"Humans in humans out: On gpt converging toward common sense in both success and failure\",\n        \"Teaching arithmetic to small transformers\",\n        \"Loogle: Can long-context language models understand long contexts?\",\n        \"Towards understanding grokking: An effective theory of representation learning\",\n        \"Factscore: Fine-grained atomic evaluation of factual precision in long form text generation\",\n        \"Progress measures for grokking via mechanistic interpretability\",\n        \"Show your work: Scratchpads for intermediate computation with language models\",\n        \"Introducing chatgpt\",\n        \"Gpt-4 technical report\",\n        \"Grokking: Generalization beyond overfitting on small algorithmic datasets\",\n        \"Limitations of language models in arithmetic and symbolic induction\",\n        \"Is chatgpt a general-purpose natural language processing task solver?\",\n        \"Language models are unsupervised multitask learners\",\n        \"Positional description matters for transformers arithmetic\",\n        \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\",\n        \"Lamda: Language models for dialog applications\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Llama 2: Open foundation and fine-tuned chat models\",\n        \"Transformers learn in-context by gradient descent\",\n        \"Emergent abilities of large language models\",\n        \"Chain-of-thought prompting elicits reasoning in large language models\",\n        \"Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks\",\n        \"It ain’t that bad: Understanding the mysterious performance drop in ood generalization for generative transformer models\",\n        \"Explaining the complex task reasoning of large language models with template-content structure\",\n        \"Are transformers universal approximators of sequence-to-sequence functions?\",\n        \"Counterfactual memorization in neural language models\",\n        \"The clock and the pizza: Two stories in mechanistic explanation of neural networks\",\n        \"Least-to-most prompting enables complex reasoning in large language models\",\n        \"Teaching algorithmic reasoning via in-context learning\",\n        \"What algorithms can transformers learn? a study in length generalization\",\n        \"Transformers can achieve length generalization but not robustly\",\n        \"Large language models can learn rules\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in 1 Introduction, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}