{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 280 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Learning in Audio-visual Context: A Review, Analysis, and New Perspective\n\nPaper Abstract: Abstract Sight and hearing are two senses that play a vital role in human communication and scene understanding. To mimic human perception ability, audio-visual learning, aimed at developing computational approaches to learn from both audio and visual modalities, has been a flourishing field in recent years. A comprehensive survey that can systematically organize and analyze studies of the audio-visual field is expected. Starting from the analysis of audio-visual cognition foundations, we introduce several key findings that have inspired our computational studies. Then, we systematically review the recent audio-visual learning studies and divide them into three categories: audio-visual boosting, cross-modal perception and audio-visual collaboration. Through our analysis, we discover that, the consistency of audio-visual data across semantic, spatial and temporal support the above studies. To revisit the current development of the audio-visual learning field from a more macro view, we further propose a new perspective on audio-visual scene understanding, then discuss and analyze the feasible future direction of the audio-visual learning area. Overall, this survey reviews and outlooks the current audio-visual learning field from different aspects. We hope it can provide researchers with a better understanding of this area. A website including constantly-updated survey is released: https://gewu-lab.github.io/audio-visual-learning/.\n\n{retrieved_text}\n\nGenerate exactly 280 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 280\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Brain biophysics: perception, consciousness, creativity. brain computer interface (bci)\",\n        \"The cognitive neuroscience of mind: a tribute to Michael S. Gazzaniga\",\n        \"Deep audio-visual learning: A survey\",\n        \"Recent advances and challenges in deep audio-visual correlation learning\",\n        \"A survey of multi-view representation learning\",\n        \"Multimodal machine learning: A survey and taxonomy\",\n        \"Deep vision multimodal learning: Methodology, benchmark, and trend\",\n        \"Subcortical processing in auditory communication\",\n        \"Consciousness without a cerebral cortex: A challenge for neuroscience and medicine\",\n        \"Hearing lips and seeing voices\",\n        \"Eye movements in auditory space perception\",\n        \"Crossmodal spatial interactions in subcortical and cortical circuits\",\n        \"Polysensory properties of neurons in the anterior bank of the caudal superior temporal sulcus of the macaque monkey\",\n        \"Functional relevance of cross-modal plasticity in blind humans\",\n        \"Cortical hubs form a module for multisensory integration on top of the hierarchy of cortical networks\",\n        \"Visual music and musical vision\",\n        \"Speech recognition by machine: A review\",\n        \"A survey on vision-based human action recognition\",\n        \"A historical perspective of speech recognition\",\n        \"Visual speech recognition using active shape models and hidden markov models\",\n        \"Audiovisual speech processing\",\n        \"A comparison of model and transform-based visual features for audio-visual lvcsr\",\n        \"Audio-visual speech modeling for continuous speech recognition\",\n        \"Dynamic bayesian networks for audio-visual speech recognition\",\n        \"Dynamic modality weighting for multi-stream hmms inaudio-visual speech recognition\",\n        \"Audio-visual speech recognition using deep learning\",\n        \"End-to-end audiovisual fusion with lstms\",\n        \"Temporal multimodal learning in audiovisual speech recognition\",\n        \"Deep audio-visual speech recognition\",\n        \"Explicit sparse transformer: Concentrated attention through explicit selection\",\n        \"Multimodal sparse transformer network for audio-visual speech recognition\",\n        \"Robust self-supervised audio-visual speech recognition\",\n        \"A survey of face recognition techniques\",\n        \"An overview of automatic speaker recognition technology\",\n        \"Fusion of face and speech data for person identity verification\",\n        \"Multimodal person recognition using unconstrained audio and video\",\n        \"Multi-level fusion of audio and visual features for speaker identification\",\n        \"Audiovisual celebrity recognition in unconstrained web videos\",\n        \"A multi-view approach to audio-visual speaker verification\",\n        \"Noise-tolerant audio-visual online person verification using an attention-based neural network fusion\",\n        \"Audio-visual deep neural network for robust person verification\",\n        \"Audio-visual person recognition in multimedia data from the iarpa janus program\",\n        \"Audio-visual speaker diarization using fisher linear semi-discriminant analysis\",\n        \"Multi-modal speaker diarization of real-world meetings using compressed-domain video features\",\n        \"Multimodal speaker diarization\",\n        \"Audio segmentation and speaker localization in meeting videos\",\n        \"Visual speaker localization aided by acoustic models\",\n        \"Audio-visual speaker diarization based on spatiotemporal bayesian fusion\",\n        \"Who said that?: Audio-visual speaker diarisation of real-world meetings\",\n        \"Self-supervised learning for audio-visual speaker diarization\",\n        \"Human action recognition from various data modalities: A review\",\n        \"“A short note on the kinetics-700 human action dataset\",\n        \"“Exploring multimodal video representation for action recognition\",\n        \"“The activitynet large-scale activity recognition challenge 2018 summary\",\n        \"“Epic-fusion: Audio-visual temporal binding for egocentric action recognition\",\n        \"“Audiovisual slowfast networks for video recognition\",\n        \"“Attention is all you need\",\n        \"“Mm-vit: Multi-modal video transformer for compressed video action recognition\",\n        \"“Scsampler: Sampling salient clips from video for efficient action recognition\",\n        \"“Listen to look: Action recognition by previewing audio\",\n        \"“Adamml: Adaptive multi-modal learning for efficient video recognition\",\n        \"“Learnable irrelevant modality dropout for multimodal action recognition on modality-specific annotated videos\",\n        \"“Cross-domain first person audio-visual action recognition through relative norm alignment\",\n        \"“Domain generalization through audio-visual relative norm alignment in first person action recognition\",\n        \"“Audio-adaptive activity recognition across video domains\",\n        \"“Communication without words\",\n        \"“Emotion recognition based on joint visual and audio cues\",\n        \"“Error weighted semi-coupled hidden markov model for audio-visual emotion recognition\",\n        \"“Two-level hierarchical alignment for semi-coupled hmm-based audiovisual emotion recognition with temporal course\",\n        \"“Machine analysis of facial expressions\",\n        \"“Automatic facial expression analysis: a survey\",\n        \"“Context-sensitive learning for enhanced audiovisual emotion classification\",\n        \"“Audiovisual vocal outburst classification in noisy acoustic conditions\",\n        \"“Bimodal log-linear regression for fusion of audio and visual features\",\n        \"“Decision level combination of multiple modalities for recognition and analysis of emotional expression\",\n        \"“Modeling latent discriminative dynamic of multi-dimensional affective signals\",\n        \"“Audio–visual affective expression recognition through multistream fused hmm\",\n        \"“Audio visual emotion recognition based on triple-stream dynamic bayesian network models\",\n        \"“Tensor fusion network for multimodal sentiment analysis\",\n        \"“Memory fusion network for multi-view sequential learning\",\n        \"“Conversational memory network for emotion recognition in dyadic dialogue videos\",\n        \"“Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences\",\n        \"“A transformer-based joint-encoding for emotion recognition and sentiment analysis\",\n        \"“Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm\",\n        \"“Multi-modal sarcasm detection and humor classification in code-mixed conversations\",\n        \"“Contextual inter-modal attention for multi-modal sentiment analysis\",\n        \"“Multi-modal sarcasm detection in twitter with hierarchical fusion model\",\n        \"“Multilogue-net: A context aware rnn for multi-modal emotion detection and sentiment analysis in conversation\",\n        \"“Audio-visual speech enhancement using multimodal deep convolutional neural networks\",\n        \"“Visually assisted time-domain speech enhancement\",\n        \"“On training targets and objective functions for deep-learning-based audio-visual speech enhancement\",\n        \"“The conversation: Deep audio-visual speech enhancement\",\n        \"“My lips are concealed: Audio-visual speech enhancement through obstructions\",\n        \"“Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders\",\n        \"“Looking into your speech: Learning cross-modal affinity for audio-visual speech separation\",\n        \"X. Alameda-Pineda\",\n        \"“Learning to separate object sounds by watching unlabeled video\",\n        \"“The sound of pixels\",\n        \"“Self-supervised audio-visual co-segmentation\",\n        \"“The sound of motions\",\n        \"“Recursive visual sound separation using minus-plus net\",\n        \"“Co-separating sounds of visual objects\",\n        \"“Visual scene graphs for audio source separation\",\n        \"“Cyclic co-learning of sounding object visual grounding and sound separation\",\n        \"“Learning to have an ear for face super-resolution\",\n        \"“Appearance matters\",\n        \"Audio source separation and speech enhancement\",\n        \"“Single-channel speech separation using sparse non-negative matrix factorization.” in Interspeech\",\n        \"“Learning joint statistical models for audio-visual fusion and segregation\",\n        \"“Audio-visual sound separation via hidden markov models\",\n        \"“Audio-visual enhancement of speech in noise\",\n        \"“Visually derived wiener filters for speech enhancement\",\n        \"“Visual speech enhancement\",\n        \"“Seeing through noise: Visually driven speaker separation and enhancement\",\n        \"“Multimodal speakerbeam: Single channel target speech extraction with audio-visual speaker clues.” in Interspeech\",\n        \"“Facefilter: Audio-visual speech separation using still images\",\n        \"“Music gesture for visual sound separation\",\n        \"“What makes training multi-modal classification networks hard?” in CVPR\",\n        \"“Balanced multimodal learning via on-the-fly gradient modulation\",\n        \"“Reconstructing intelligible audio speech from visual speech features\",\n        \"“Generating intelligible audio speech from visual speech\",\n        \"“Vid2speech: speech reconstruction from silent video\",\n        \"“Improved speech reconstruction from silent video\",\n        \"“Lip2audspec: Speech reconstruction from silent lip movements video\",\n        \"“Video-driven speech reconstruction using generative adversarial networks\",\n        \"“End-to-end video-to-speech synthesis using generative adversarial networks\",\n        \"“Harnessing ai for speech reconstruction using multi-view silent video feed\",\n        \"“Hush-hush speak: Speech reconstruction using silent videos.” in Interspeech\",\n        \"“Real-time piano music transcription based on computer vision\",\n        \"“Foley music: Learning to generate music from videos\",\n        \"“Multi-instrumentalist net: Unsupervised generation of music from body movements\",\n        \"“Collaborative learning to generate audio-video jointly\",\n        \"“Video background music generation with controllable music transformer\",\n        \"“Visually indicated sounds\",\n        \"“Visual to sound: Generating natural sound for videos in the wild\",\n        \"“Generating visually aligned sound from videos\",\n        \"“Taming visually guided sound generation\",\n        \"“Scene-aware audio for 360 videos\",\n        \"“Self-supervised generation of spatial audio for 360 video\",\n        \"“2.5 d visual sound\",\n        \"“Self-supervised audio spatialization with correspondence classifier\",\n        \"“Beyond mono to binaural: Generating binaural audio from mono audio with depth and cross modal attention\",\n        \"“Sep-stereo: Visually guided stereophonic audio generation by associating source separation\",\n        \"“Visually informed binaural audio generation without binaural audios\",\n        \"“Exploiting audio-visual consistency with partial supervision for spatial audio generation\",\n        \"“You said that?: Synthesising talking faces from audio\",\n        \"“Synthesizing obama: learning lip sync from audio\",\n        \"“Lip movements generation at a glance\",\n        \"“Realistic speech-driven facial animation with gans\",\n        \"“Few-shot adversarial learning of realistic neural talking head models\",\n        \"“Makelttalk: speaker-aware talking-head animation\",\n        \"“Freenet: Multi-identity face reenactment\",\n        \"“Neural voice puppetry: Audio-driven facial reenactment\",\n        \"“Rotate-and-render: Unsupervised photorealistic face rotation from single-view images\",\n        \"“Write-a-speaker: Text-based emotional and rhythmic talking-head generation\",\n        \"“Pose-controllable talking face generation by implicitly modularized audio-visual representation\",\n        \"“Audio-driven emotional video portraits\",\n        \"“Beat: the behavior expression animation toolkit\",\n        \"“Robot behavior toolkit: generating effective social behaviors for robots\",\n        \"“Learning individual styles of conversational gesture\",\n        \"“Seeg: Semantic energized co-speech gesture generation\",\n        \"“Analyzing input and output representations for speech-driven gesture generation\",\n        \"“Evaluation of speech-to-gesture generation using bi-directional lstm network\",\n        \"“To react or not to react: End-to-end visual pose forecasting for personalized avatar during dyadic conversations\",\n        \"“Style-controllable speech-driven gesture synthesis using normalising flows\",\n        \"S. Alexandersson\",\n        \"“Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach\",\n        \"“Audeo: Audio generation for a silent performance video\",\n        \"“Dancing to music\",\n        \"“Ai choreographer: Music conditioned 3d dance generation with aist++\",\n        \"“Batvision: Learning to see 3d spatial layout with two ears\",\n        \"“Visualechoes: Spatial image representation learning through echolocation\",\n        \"“Beyond image to depth: Improving depth prediction using echoes\",\n        \"“Co-attention-guided bilinear model for echo-based depth estimation\",\n        \"“Soundnet: Learning sound representations from unlabeled video\",\n        \"“Ambient sound provides supervision for visual learning\",\n        \"“Self-supervised moving vehicle tracking with stereo sound\",\n        \"“Enhanced audio tagging via multi-to single-modal teacher-student mutual learning\",\n        \"“Multimodal knowledge expansion\",\n        \"“Distilling audio-visual knowledge by compositional contrastive learning\",\n        \"“Example-based automatic music-driven conventional dance motion synthesis\",\n        \"“Music similarity-based approach to generating dance motion sequence\",\n        \"“Content-based video-music retrieval using soft intra-modal structure constraint\",\n        \"“Image2song: Song retrieval via bridging image content and lyric words\",\n        \"“Cross-modal embeddings for video and audio retrieval\",\n        \"“Learn2dance: Learning statistical music-to-dance mappings for choreography synthesis\",\n        \"“Dance with melody: An lstm-autoencoder approach to music-oriented dance synthesis\",\n        \"“Dance revolution: Long-term dance generation with music via curriculum learning\",\n        \"“Genre-conditioned long-term 3d dance generation driven by music\",\n        \"“Audio to body dynamics\",\n        \"“Sound-guided semantic image manipulation\",\n        \"“Learning visual styles from audio-visual associations\",\n        \"“There is more than meets the eye: Self-supervised multi-object detection and tracking with sound by distilling multimodal knowledge\",\n        \"“Knowledge distillation from multi-modality to single-modality for person verification\",\n        \"“Deep triplet neural networks with cluster-cca for audio-visual cross-modal retrieval\",\n        \"“Deep cross-modal image–voice retrieval in remote sensing\",\n        \"“Labelling unlabelled videos from scratch with multi-modal self-supervision\",\n        \"“Kernel and nonlinear canonical correlation analysis\",\n        \"“Deep canonical correlation analysis\",\n        \"“Cluster canonical correlation analysis\",\n        \"“Audio-visual embedding for cross-modal music video retrieval through supervised deep cca\",\n        \"“Learning classification with unlabeled data\",\n        \"listen and learn\",\n        \"“Cooperative learning of audio and video models from self-supervised synchronization\",\n        \"“Learning representations from audio-visual spatial alignment\",\n        \"“Robust audio-visual instance discrimination\",\n        \"“Audio-visual instance discrimination with cross-modal agreement\",\n        \"“Deep multimodal clustering for unsupervised audiovisual learning\",\n        \"“Self-supervised learning by cross-modal audio-video clustering\",\n        \"“Multimodal clustering networks for self-supervised learning from unlabeled videos\",\n        \"I. Sutskever et al\",\n        \"A. Askell et al\",\n        \"“Opt: omni-perception pre-trainer for cross-modal understanding and generation\",\n        \"“Vatt: Transformers for multimodal self-supervised learning from raw video\",\n        \"“Audioclip: Extending clip to image\",\n        \"“Merlot reserve: Neural script knowledge through vision and language and sound\",\n        \"“Audio vision: Using audio-visual synchrony to locate sounds\",\n        \"“Pixels that sound\",\n        \"“Multimodal analysis for identification and segmentation of moving-sounding objects\",\n        \"“Objects that sound\",\n        \"“Audio-visual scene analysis with self-supervised multisensory features\",\n        \"“Learning to localize sound sources in visual scenes: Analysis and applications\",\n        \"“Mix and localize: Localizing sound sources in mixtures\",\n        \"“Class-aware sounding objects localization via audiovisual correspondence.” IEEE Transactions on Pattern Analysis and Machine Intelligence\",\n        \"“Audio-visual segmentation\",\n        \"“Video saliency detection via spatial-temporal fusion and low-rank coherency diffusion\",\n        \"“Bilevel feature learning for video saliency detection\",\n        \"J. Santos-Victor\",\n        \"“Multimodal saliency-based attention for object-based scene analysis\",\n        \"“Fixation prediction through multimodal analysis\",\n        \"“Dave: A deep audio-visual embedding for dynamic saliency prediction\",\n        \"“Stavis: Spatio-temporal audiovisual saliency network\",\n        \"“A multimodal saliency model for videos with high audio-visual correspondence\",\n        \"“Vinet: Pushing the limits of visual modality for audio-visual saliency prediction\",\n        \"“From semantic categories to fixations: A novel weakly-supervised visual-auditory saliency detection approach\",\n        \"“Cognitive mapping and planning for visual navigation\",\n        \"“Unifying map and landmark based representations for visual navigation\",\n        \"“Soundspaces: Audio-visual navigation in 3d environments\",\n        \"and act: Towards audio-visual embodied navigation\",\n        \"“Learning to set waypoints for audio-visual navigation\",\n        \"“Semantic audio-visual navigation\",\n        \"“Catch me if you hear me: Audio-visual navigation in complex unmapped environments with moving sounds\",\n        \"“Sound adversarial audio-visual navigation\",\n        \"“Move2hear: Active audio-visual source separation\",\n        \"“Audio-visual event localization in unconstrained videos\",\n        \"“Dual-modality seq2seq network for audio-visual event localization\",\n        \"“Unified multisensory perception: Weakly-supervised audio-visual video parsing\",\n        \"“Audio-visual event localization via recursive fusion by joint co-attention\",\n        \"“Positive sample propagation along the audio-visual event line\",\n        \"“Exploring cross-video and cross-modality signals for weakly-supervised audio-visual video parsing\",\n        \"“Exploring heterogeneous clues for weakly-supervised audio-visual video parsing\",\n        \"“Investigating modality bias in audio visual video parsing\",\n        \"P. Anderson et al\",\n        \"“Joint student-teacher learning for audio-visual scene-aware dialog.” in Interspeech\",\n        \"“A simple baseline for audio-visual scene-aware dialog\",\n        \"“Dynamic graph representation learning for video dialog via multi-modal shuffled transformers\",\n        \"“Pano-avqa: Grounded audio-visual question answering on 360deg videos\",\n        \"“Learning to answer questions in dynamic audio-visual scenarios\",\n        \"“Audio-visual scene-aware dialog and reasoning using audio-visual transformers with joint student-teacher learning\",\n        \"“Dual attention matching for audio-visual event localization\",\n        \"“Audiovisual transformer with instance attention for audio-visual event localization\",\n        \"“Cross-modal background suppression for audio-visual event localization\",\n        \"“Distributed audio-visual parsing based on multimodal transformer and deep joint source channel coding\",\n        \"“End-to-end audio visual scene-aware dialog using multimodal attention-based video features\",\n        \"“Lip reading in the wild\",\n        \"“Lrs3-ted: a large-scale dataset for visual speech recognition\",\n        \"“Voxceleb: a large-scale speaker identification dataset\",\n        \"“Voxceleb2: Deep speaker recognition\",\n        \"“Ava active speaker: An audio-visual dataset for active speaker detection\",\n        \"S. Vijayanarasimhan\",\n        \"“Scaling egocentric vision: The epic-kitchens dataset\",\n        \"“Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages\",\n        \"“Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph\",\n        \"“Vggsound: A large-scale audio-visual dataset\",\n        \"“Audio set: An ontology and human-labeled dataset for audio events\",\n        \"“The replica dataset: A digital replica of indoor spaces\",\n        \"“Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots\",\n        \"H. Riemenschneider\",\n        \"“A behaviorally inspired fusion approach for computational audiovisual saliency modeling\",\n        \"A. Banki-Horvath\"\n    ],\n    \"reasoning\": \"Selected these 280 references because they represent foundational work and recent advances in Learning in Audio-visual Context: A Review, Analysis, and New Perspective, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}