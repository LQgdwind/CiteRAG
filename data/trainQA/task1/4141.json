{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 44 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression\n\nPaper Abstract: Abstract. Organizing complex peer production projects and advancing scientific knowledge of open collaboration each depend on the ability to measure quality. Article quality ratings on English language Wikipedia have been widely used by both Wikipedia community members and academic researchers for purposes like tracking knowledge gaps and studying how political polarization shapes collaboration. Even so, measuring quality presents many methodological challenges. The most widely used systems use labels on discrete ordinal scales when assessing quality, but such labels can be inconvenient for statistics and machine learning. Prior work handles this by assuming that different levels of quality are “evenly spaced” from one another. This assumption runs counter to intuitions about the relative degrees of effort needed to raise Wikipedia encyclopedia articles to different quality levels. Furthermore, models from prior work are fit to datasets that oversample high-quality articles. This limits their accuracy for representative samples of articles or revisions. I describe a technique extending the Wikimedia Foundations’ ORES article quality model to address these limitations. My method uses weighted ordinal regression models to construct one-dimensional continuous measures of quality. While scores from my technique and from prior approaches are correlated, my approach improves accuracy for research datasets and provides evidence that the “evenly spaced” assumption is unfounded in practice on English Wikipedia. I conclude with recommendations for using quality scores in future research and include the full code, data, and models.\n\n{retrieved_text}\n\nGenerate exactly 44 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 44\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A Content-Driven Reputation System for the Wikipedia\",\n        \"A Breakdown of Quality Flaws in Wikipedia\",\n        \"Predicting Quality Flaws in User-Generated Content: The Case of Wikipedia\",\n        \"The Evolutionary Trajectories of Peer-Produced Artifacts: Group Composition, the Trajectories’ Exploration, and the Quality of Artifacts\",\n        \"How Wikipedia Works and How You Can Be a Part of It\",\n        \"Measuring the Quality of Edits to Wikipedia\",\n        \"Size Matters: Word Count as a Measure of Quality on Wikipedia\",\n        \"Brms: An R Package for Bayesian Multilevel Models Using Stan\",\n        \"Learning to Classify Ordinal Data: The Data Replication Method\",\n        \"Underproduction: An Approach for Measuring Risk in Open Source Software\",\n        \"Inventing Temperature\",\n        \"Quality Assessment of Wikipedia Articles Without Feature Engineering\",\n        \"Learning to Predict the Quality of Contributions to Wikipedia\",\n        \"Why Do People Write for Wikipedia? Incentives to Contribute to Open-Content Publishing\",\n        \"Problems of Monetary Management: The UK Experience\",\n        \"Interpolating Quality Dynamics in Wikipedia and Demonstrating the Keilana Effect\",\n        \"ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia\",\n        \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction\",\n        \"The Tower of Babel Meets Web 2.0: User-Generated Content and Its Applications in a Multilingual Context\",\n        \"Common Knowledge?: An Ethnography of Wikipedia\",\n        \"Inherent Trade-Offs in the Fair Determination of Risk Scores\",\n        \"Reciprocity and Donation: How Article Topic, Quality and Dwell Time Predict Banner Donation on Wikipedia\",\n        \"Why the World Reads Wikipedia: Beyond English Speakers\",\n        \"Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles\",\n        \"Statistical Rethinking\",\n        \"People Who Can Take It: How Women Wikipedians Negotiate and Navigate Safety\",\n        \"Scikit-Learn: Machine Learning in Python\",\n        \"How Wikipedia Works\",\n        \"Classifying Wikipedia Article Quality With Revision History Networks\",\n        \"Do ImageNet Classifiers Generalize to ImageNet?\",\n        \"A Taxonomy of Knowledge Gaps for Wikimedia Projects (Second Draft)\",\n        \"StRE: Self Attentive Edit Quality Prediction in Wikipedia\",\n        \"Article Quality Classification on Wikipedia: Introducing Document Embeddings and Content Features\",\n        \"The Wisdom of Polarized Crowds\",\n        \"‘Improving Ratings’: Audit in the British University System\",\n        \"Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers\",\n        \"Effects of Algorithmic Flagging on Fairness: Quasi-Experimental Evidence from Wikipedia\",\n        \"Ms. Categorized: Gender, Notability, and Inequality on Wikipedia\",\n        \"Decentering Design: Wikipedia and Indigenous Knowledge\",\n        \"Modern Applied Statistics with S\",\n        \"The Success and Failure of Quality Improvement Projects in Peer Production Communities\",\n        \"Tell Me More: An Actionable Quality Model for Wikipedia\",\n        \"Crowd Development: The Interplay between Crowd Evaluation and Collaborative Dynamics in Wikipedia\",\n        \"History-Based Article Quality Assessment on Wikipedia\"\n    ],\n    \"reasoning\": \"Selected these 44 references because they represent foundational work and recent advances in Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}