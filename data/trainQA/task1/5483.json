{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Interventions, Where and How? Experimental Design for Causal Models at Scale\n\nPaper Abstract: Abstract Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the in-silico single-cell gene regulatory network dataset, DREAM.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Abcd-strategy: Budgeted experimental design for targeted causal structure discovery\",\n        \"Variational causal networks: Approximate bayesian inference over causal structures\",\n        \"Emergence of scaling in random networks\",\n        \"Efficient generation of large random networks\",\n        \"A meta-transfer objective for learning to disentangle causal mechanisms\",\n        \"Differentiable causal discovery from interventional data\",\n        \"Bayesian experimental design: A review\",\n        \"Reconstructing causal biological networks through active learning\",\n        \"Bcd nets: Scalable variational approaches for bayesian causal discovery\",\n        \"On random graphs i. publicationes mathematicae (debrecen)\",\n        \"Variational bayesian optimal experimental design\",\n        \"A unified stochastic gradient approach to designing bayesian-optimal experiments\",\n        \"Deep adaptive design: Amortizing sequential bayesian experimental design\",\n        \"Data analysis with bayesian networks: A bootstrap approach\",\n        \"Active invariant causal prediction: Experiment selection through stability\",\n        \"Sample efficient active learning of causal trees\",\n        \"Dream4: Combining genetic and dynamic information to identify biological networks and dynamical models\",\n        \"Active learning of causal networks with intervention experiments and optimal designs\",\n        \"Bayesian active learning for classification and preference learning\",\n        \"Implicit deep adaptive design: policy-based experimental design without likelihoods\",\n        \"Learning neural causal models from unknown interventions\",\n        \"Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning\",\n        \"A simple baseline for batch active learning with stochastic acquisition functions\",\n        \"Efficient bayesian experimental design for implicit models\",\n        \"Cost-optimal learning of causal graphs\",\n        \"Near-optimal nonmyopic value of information in graphical models\",\n        \"A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise\",\n        \"Policy-based bayesian experimental design for non-differentiable implicit models\",\n        \"On a measure of the information provided by an experiment\",\n        \"Stein variational gradient descent: A general purpose bayesian inference algorithm\",\n        \"Dibs: Differentiable bayesian structure learning\",\n        \"Learning latent permutations with gumbel-sinkhorn networks\",\n        \"On bayesian methods for seeking the extremum\",\n        \"Active learning of causal bayes net structure\",\n        \"An analysis of approximations for maximizing submodular set functions—i\",\n        \"A bayesian active learning experimental design for inferring signaling networks\",\n        \"Causality\",\n        \"Structural intervention distance for evaluating causal graphs\",\n        \"Identifiability of causal graphs using functional models\",\n        \"Causal inference by using invariant prediction: identification and confidence intervals\",\n        \"Elements of causal inference: foundations and learning algorithms\",\n        \"On variational bounds of mutual information\",\n        \"On nesting monte carlo estimators\",\n        \"Gaussian processes in machine learning\",\n        \"Causal protein-signaling networks derived from multiparameter single-cell data\",\n        \"Genenetweaver: in silico benchmark generation and performance profiling of network inference methods\",\n        \"Learning neural causal models with active interventions\",\n        \"Learning graphical model structure using l1-regularization paths\",\n        \"Learning causal graphs with small interventions\",\n        \"Active structure learning of causal dags via directed clique trees\",\n        \"Gaussian process optimization in the bandit setting: No regret and experimental design\",\n        \"Near-optimal multi-perturbation experimental design for causal structure learning\",\n        \"Elements of information theory\",\n        \"Active learning for structure in bayesian networks\",\n        \"Optimal experimental design via bayesian optimization: active causal structure learning for gaussian process networks\",\n        \"Matching a desired causal state via shift interventions\",\n        \"Dags with no tears: Continuous optimization for structure learning\",\n        \"Single-step bayesian search method for an extremum of functions of a single variable\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in Interventions, Where and How? Experimental Design for Causal Models at Scale, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}