{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 46 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training\n\nPaper Abstract: Abstract Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier Œ≤ùõΩ\\beta. Traditionally, to find the trade-off to be learned, IB requires a search for Œ≤ùõΩ\\beta through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of Œ≤ùõΩ\\beta with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable Œ≤ùõΩ\\beta, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VIB. Furthermore, in terms of calibration performance, FVIB outperforms other IB and calibration methods by enabling continuous optimization of Œ≤ùõΩ\\beta. Our codes are available at https://github.com/sotakudo/fvib.\n\n{retrieved_text}\n\nGenerate exactly 46 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 46\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Information dropout: Learning optimal representations through noisy computation\",\n        \"Invariance principle meets information bottleneck for out-of-distribution generalization\",\n        \"Fixing a broken elbo\",\n        \"Deep variational information bottleneck\",\n        \"Uncertainty in the variational information bottleneck\",\n        \"Multi-rate vae: Train once, get the full rate-distortion curve\",\n        \"Smote: synthetic minority over-sampling technique\",\n        \"Start globally, optimize locally, predict globally: Improving performance on imbalanced data\",\n        \"Automated detection of atrial fibrillation using long short-term memory network with rr interval signals\",\n        \"An information theoretic tradeoff between complexity and accuracy\",\n        \"On calibration of modern neural networks\",\n        \"Freedom of information law comes to vietnam: How do human rights adapt to goals of economic development and political stability?\",\n        \"beta-vae: Learning basic visual concepts with a constrained variational framework\",\n        \"Distilling the knowledge in a neural network\",\n        \"Adam: A method for stochastic optimization\",\n        \"Auto-encoding variational bayes\",\n        \"Caveats for information bottleneck in deterministic scenarios\",\n        \"Nonlinear information bottleneck\",\n        \"Learning multiple layers of features from tiny images\",\n        \"A training pipeline of an arrhythmia classifier for atrial fibrillation detection using photoplethysmography signal\",\n        \"On implicit regularization in Œ≤ùõΩ\\\\beta-vaes\",\n        \"The neural autoregressive distribution estimator\",\n        \"Lagrangian relaxation\",\n        \"Invariant information bottleneck for domain generalization\",\n        \"On the limited memory bfgs method for large scale optimization\",\n        \"Don‚Äôt blame the elbo! a linear vae perspective on posterior collapse\",\n        \"Obtaining well calibrated probabilities using bayesian binning\",\n        \"Reading digits in natural images with unsupervised feature learning\",\n        \"Disentangled information bottleneck\",\n        \"Pytorch: An imperative style, high-performance deep learning library\",\n        \"Trainable undersampling for class-imbalance learning\",\n        \"Regularizing neural networks by penalizing confident output distributions\",\n        \"Abrupt changes in fibrillatory wave characteristics at the termination of paroxysmal atrial fibrillation in humans\",\n        \"A new specification of generalized linear models for categorical data\",\n        \"The convex information bottleneck lagrangian\",\n        \"Learning and generalization with the information bottleneck\",\n        \"A generalised linear model framework for Œ≤ùõΩ\\\\beta-variational autoencoders based on exponential dispersion families\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"Deep learning and the information bottleneck principle\",\n        \"The information bottleneck method\",\n        \"Recent advances in autoencoder-based representation learning\",\n        \"The role of the information bottleneck in representation learning\",\n        \"Learnability for the information bottleneck\",\n        \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\",\n        \"Deep deterministic information bottleneck with matrix-based entropy functional\",\n        \"Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning\"\n    ],\n    \"reasoning\": \"Selected these 46 references because they represent foundational work and recent advances in Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}