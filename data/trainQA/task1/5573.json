{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Prediction of speech intelligibility with DNN-based performance measures\n\nPaper Abstract: Abstract This paper presents a speech intelligibility model based on automatic speech recognition (ASR), combining phoneme probabilities from deep neural networks (DNN) and a performance measure that estimates the word error rate from these probabilities. This model does not require the clean speech reference nor the word labels during testing as the ASR decoding step — which finds the most likely sequence of words given phoneme posterior probabilities — is omitted. The model is evaluated via the root-mean-squared error between the predicted and observed speech reception thresholds from eight normal-hearing listeners. The recognition task consists of identifying noisy words from a German matrix sentence test. The speech material was mixed with eight noise maskers covering different modulation types, from speech-shaped stationary noise to a single-talker masker. The prediction performance is compared to five established models and an ASR-model using word labels. Two combinations of features and networks were tested. Both include temporal information either at the feature level (amplitude modulation filterbanks and a feed-forward network) or captured by the architecture (mel-spectrograms and a time-delay deep neural network, TDNN). The TDNN model is on par with the DNN while reducing the number of parameters by a factor of 373737; this optimization allows parallel streams on dedicated hearing aid hardware as a forward-pass can be computed within the 10 ms of each frame. The proposed model performs almost as well as the label-based model and produces more accurate predictions than the baseline models.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Methods for calculation of the speech intelligibility index\",\n        \"Modelling speaker intelligibility in noise\",\n        \"Efficient adaptive procedures for threshold and concurrent slope estimates for psychophysics and speech intelligibility tests\",\n        \"Dnn-based performance measures for predicting error rates in automatic speech recognition and optimizing hearing aid parameters\",\n        \"Should deep neural nets have ears? the role of auditory features in deep learning approaches\",\n        \"Why do asr systems despite neural nets still depend on robust features.\",\n        \"Prediction of speech intelligibility with DNN-based performance measures\",\n        \"Modeling auditory processing of amplitude modulation. i. detection and masking with narrow-band carriers\",\n        \"Modeling auditory processing of amplitude modulation. ii. spectral and temporal integration\",\n        \"Effect of reducing slow temporal modulations on speech reception\",\n        \"Effect of temporal envelope smearing on speech reception\",\n        \"The modulation transfer function for speech intelligibility\",\n        \"Characterizing frequency selectivity for envelope fluctuations\",\n        \"Kaldi-based dnn architectures for speech recognition in romanian\",\n        \"Customizing a vliw-simd application-specific instruction-set processor for hearing aid devices\",\n        \"Mean temporal distance: Predicting asr error from temporal properties of speech signal\",\n        \"A practical guide to training restricted boltzmann machines\",\n        \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups\",\n        \"Development and analysis of an International Speech Test Signal (ISTS)\",\n        \"Deeper time delay neural networks for effective acoustic modelling\",\n        \"Single-ended prediction of listening effort using deep neural networks\",\n        \"Objective assessment of a speech enhancement scheme with an automatic speech recognition-based system\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"A multi-resolution envelope-power based model for speech intelligibility\",\n        \"Microscopic prediction of speech recognition for listeners with normal hearing in noise using an auditory model\",\n        \"The hearing-aid speech perception index (haspi)\",\n        \"Tandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation\",\n        \"The multilingual matrix test: Principles, applications, and comparison across languages: A review\",\n        \"On information and sufficiency\",\n        \"Autonomous measurement of speech intelligibility utilizing automatic speech recognition\",\n        \"An auditory inspired amplitude modulation filter bank for robust feature extraction in automatic speech recognition\",\n        \"Integration of optimized modulation filter sets into deep neural networks for automatic speech recognition\",\n        \"Rectified linear units improve restricted boltzmann machines\",\n        \"Librispeech: an asr corpus based on public domain audio books\",\n        \"Performance analysis of the aurora large vocabulary baseline system\",\n        \"A time delay neural network architecture for efficient modeling of long temporal contexts\",\n        \"The kaldi speech recognition toolkit\",\n        \"Parallel training of dnns with natural gradient and parameter averaging\",\n        \"A speech intelligibility index-based approach to predict the speech reception threshold for sentences in fluctuating noise for normal-hearing listeners\",\n        \"Analyzing deep cnn-based utterance embeddings for acoustic model adaptation\",\n        \"Prediction of speech intelligibility based on deep machine listening: Influence of training data and simulation of hearing impairment\",\n        \"Matrix sentence intelligibility prediction using an automatic speech recognition system\",\n        \"Monaural speech intelligibility and detection in maskers with varying amounts of spectro-temporal speech features\",\n        \"Predicting speech intelligibility with deep neural networks\",\n        \"An algorithm for intelligibility prediction of time-frequency weighted noisy speech\",\n        \"A metric for predicting binaural speech intelligibility in stationary noise and competing speech maskers\",\n        \"Development and evaluation of a german sentence test part iii: evaluation of the oldenburg sentence test\",\n        \"Improving deep neural network acoustic models using generalized maxout networks\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Prediction of speech intelligibility with DNN-based performance measures, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}