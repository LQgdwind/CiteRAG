{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 333 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future\n\nPaper Abstract: Abstract Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the “black-box” nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at https://github.com/ShilinSun/mxai_review.\n\n{retrieved_text}\n\nGenerate exactly 333 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 333\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Attention is all you need\",\n        \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\",\n        \"Chatgpt: Get instant answers, find creative inspiration, learn something new\",\n        \"Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning\",\n        \"Unbox the black-box for the medical explainable ai via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond\",\n        \"Multimodal explainable artificial intelligence: A comprehensive review of methodological advances and future research directions\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Visualizing higher-layer features of a deep network\",\n        \"Beyond intuition: Rethinking token attributions inside transformers\",\n        \"Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers\",\n        \"Transformer interpretability beyond attention visualization\",\n        \"Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence\",\n        \"Rethinking interpretability in the era of large language models\",\n        \"Usable xai: 10 strategies towards exploiting explainability in the llm era\",\n        \"From understanding to utilization: A survey on explainability for large language models\",\n        \"Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy\",\n        \"Rule-based learning systems for support vector machines\",\n        \"Assessment of landslide susceptibility by decision trees in the metropolitan area of istanbul, turkey\",\n        \"Mining high-speed data streams\",\n        \"Auto-encoding variational bayes\",\n        \"beta-vae: Learning basic visual concepts with a constrained variational framework.\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Principal component analysis for special types of data.\",\n        \"Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks\",\n        \"Explainable generative ai (genxai): A survey, conceptualization, and research agenda\",\n        \"Explainability for large language models: A survey\",\n        \"Explainable artificial intelligence (xai) from a user perspective: A synthesis of prior literature and problematizing avenues for future research\",\n        \"A systematic review of explainable artificial intelligence models and applications: Recent developments and future trends\",\n        \"Explainability of vision transformers: A comprehensive review and new perspectives\",\n        \"Explainability and evaluation of vision transformers: An in-depth experimental study\",\n        \"Towards human-centered explainable ai: A survey of user studies for model explanations\",\n        \"Explainable ai (xai): A systematic meta-survey of current challenges and future opportunities\",\n        \"A survey on xai and natural language explanations\",\n        \"Knowledge graphs as tools for explainable machine learning: A survey\",\n        \"Explainable ai methods-a brief overview\",\n        \"Counterfactual explanations and how to find them: literature review and benchmarking\",\n        \"Explainable ai for time series classification: a review, taxonomy and research directions\",\n        \"A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence\",\n        \"Notions of explainability and evaluation approaches for explainable artificial intelligence\",\n        \"Explainable artificial intelligence: objectives, stakeholders, and future research opportunities\",\n        \"Explainable ai: A review of machine learning interpretability methods\",\n        \"Explainable artificial intelligence approaches: A survey\",\n        \"Argumentation and explainable artificial intelligence: a survey\",\n        \"A survey on the explainability of supervised machine learning\",\n        \"Reviewing the need for explainable artificial intelligence (xai)\",\n        \"Explainable artificial intelligence (xai): An engineering perspective\",\n        \"Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond\",\n        \"Minimum redundancy feature selection from microarray gene expression data\",\n        \"Feature selection for high-dimensional data—a pearson redundancy based filter\",\n        \"“Emotion recognition using brain activity\",\n        \"“Normalized mutual information feature selection\",\n        \"“Optimization method based extreme learning machine for classification\",\n        \"“Eye movement analysis for activity recognition using electrooculography\",\n        \"K. Kreutz-Delgado\",\n        \"“Gene selection for cancer classification using support vector machines\",\n        \"“Feature subset selection for blood pressure classification using orthogonal forward selection\",\n        \"“Feature selection for high-dimensional industrial data.” in ESANN\",\n        \"” Machine learning\",\n        \"“Exploration of a hybrid feature selection algorithm\",\n        \"“Ant colony optimization for feature selection in face recognition\",\n        \"“Optimization of intrusion detection through fast hybrid feature selection\",\n        \"“A wrapper for feature selection based on mutual information\",\n        \"“A hybrid feature selection approach for microarray gene expression data\",\n        \"“Application of a hybrid wavelet feature selection method in the design of a self-paced brain interface system\",\n        \"“Feature subset selection in large dimensionality domains\",\n        \"“Independent component analysis: algorithms and applications\",\n        \"“A global geometric framework for nonlinear dimensionality reduction\",\n        \"“Nonlinear dimensionality reduction by locally linear embedding\",\n        \"” IEEE transactions on pattern analysis and machine intelligence\",\n        \"“Two-dimensional linear discriminant analysis\",\n        \"“Two-dimensional linear discriminant analysis of principle component vectors for face recognition\",\n        \"“Visualizing data using t-sne.” Journal of machine learning research\",\n        \"“A note on two-dimensional linear discriminant analysis\",\n        \"“Two-dimensional direct and weighted linear discriminant analysis for face recognition\",\n        \"“On the relationship between feature selection and classification accuracy\",\n        \"“What is principal component analysis?” Nature biotechnology\",\n        \"“1d-lda vs. 2d-lda: When is vector-based linear discriminant analysis better than matrix-based?” Pattern Recognition\",\n        \"“Generalized linear discriminant analysis: a unified framework and efficient model selection\",\n        \"“Mpca: Multilinear principal component analysis of tensor objects\",\n        \"“Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization\",\n        \"“Structured sparse principal component analysis\",\n        \"“Two-stage image denoising by principal component analysis with local pixel grouping\",\n        \"“Unsupervised kernel dimension reduction\",\n        \"Interaction effects in logistic regression. Sage\",\n        \"“Purposeful selection of variables in logistic regression\",\n        \"“An introduction to logistic regression analysis and reporting\",\n        \"“Logistic regression: Why we cannot do what we think we can do\",\n        \"“Rocr: visualizing classifier performance in r\",\n        \"J. del Campo-Ávila\",\n        \"“Supporting the dynamic evolution of web service protocols in service-oriented architectures\",\n        \"“Learning to match xml schemas-a decision tree based approach\",\n        \"“An k nn model-based approach and its application in text categorization\",\n        \"“Application of the ga/knn method to seldi proteomics data\",\n        \"“The truth is in there-rule extraction from opaque models using genetic programming.” in FLAIRS\",\n        \"“Rule extraction from support vector machines.” in Esann\",\n        \"“Bayesian models of cognition\",\n        \"“A bayesian model for repeated measures zero-inflated count data with application to outpatient psychiatric service use\",\n        \"“Probabilistic climate change predictions applying bayesian model averaging\",\n        \"“Bayesian econometric methods cambridge\",\n        \"“Trust and tam in online shopping: An integrated model\",\n        \"Making things happen: A theory of causal explanation. Oxford university press\",\n        \"“Explanation and understanding\",\n        \"“Causal inference in statistics: An overview\",\n        \"Statistical learning from a regression perspective. Springer\",\n        \"“Predictive analytics in information systems research\",\n        \"“The interplay between theory and method\",\n        \"“To explain or to predict?” 2010\",\n        \"“Causal inference without counterfactuals\",\n        \"“Evidence-based public health: moving beyond randomized trials\",\n        \"“Varieties of causal intervention\",\n        \"“Causal reasoning through intervention\",\n        \"“Toward causal inference with interference\",\n        \"Causality. Cambridge university press\",\n        \"“Feature deduction and ensemble design of intrusion detection systems\",\n        \"“Building efficient intrusion detection model based on principal component analysis and c4. 5\",\n        \"“Transferring knowledge by prior feature sampling\",\n        \"“Density estimation trees\",\n        \"“Towards an effective cooperation of the user and the computer for classification\",\n        \"“Data mining and visualization for decision support and modeling of public health-care resources\",\n        \"“A bias correction algorithm for the gini variable importance measure in classification trees\",\n        \"” BMC bioinformatics\",\n        \"“Visualization method and tool for interactive learning of large decision trees\",\n        \"“Analysis and correction of bias in total decrease in node impurity measures for tree-based algorithms\",\n        \"“Visual classification: an interactive approach to decision tree construction\",\n        \"“Eclectic rule-extraction from support vector machines\",\n        \"“Extracting the knowledge embedded in support vector machines\",\n        \"“Support vector machines with symbolic interpretation\",\n        \"“Rule extraction from trained support vector machines\",\n        \"“Rule extraction from linear support vector machines\",\n        \"“A multiple kernel support vector machine scheme for feature selection and rule extraction from gene expression data of cancer tissue\",\n        \"“Support vector machine interpretation\",\n        \"“Paintingclass: interactive construction\",\n        \"“Linked independent component analysis for multimodal data fusion\",\n        \"“Simultaneous analysis of coupled data matrices subject to different amounts of noise\",\n        \"“Multisensor data fusion: A review of the state-of-the-art\",\n        \"“New algorithm for integration between wireless microwave sensor network and radar for improved rainfall measurement and mapping\",\n        \"“Multimodal data fusion: an overview of methods\",\n        \"“Dynamical reconfiguration strategy of a multi sensor data fusion algorithm based on information theory\",\n        \"“Scalable tensor factorizations for incomplete data\",\n        \"“Show and tell: A neural image caption generator\",\n        \"“Multimodal sentiment analysis with word-level fusion and reinforcement learning\",\n        \"“Sequence to sequence-video to text\",\n        \"“Human action recognition using factorized spatio-temporal convolutional networks\",\n        \"“Supersparse linear integer models for optimized medical scoring systems\",\n        \"“Interpretable decision sets: A joint framework for description and prediction\",\n        \"“Simple rules for complex decisions\",\n        \"“Interpretability of fuzzy systems: Current research trends and prospects\",\n        \"“Accurate intelligible models with pairwise interactions\",\n        \"“Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n        \"“Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\",\n        \"“Adaptive deconvolutional networks for mid and high level feature learning\",\n        \"“Visualizing and understanding convolutional networks\",\n        \"“Understanding deep image representations by inverting them\",\n        \"“Understanding neural networks through deep visualization\",\n        \"“Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks\",\n        \"“Convergent learning: Do different neural networks learn the same representations?” arXiv preprint arXiv:1511.07543\",\n        \"“Understanding black-box predictions via influence functions\",\n        \"“Opening the black box of deep neural networks via information\",\n        \"“The application of two-level attention models in deep convolutional neural network for fine-grained image classification\",\n        \"“Hierarchical question-image co-attention for visual question answering\",\n        \"“Human attention in visual question answering: Do humans and deep networks look at the same regions?” Computer Vision and Image Understanding\",\n        \"“Infogan: Interpretable representation learning by information maximizing generative adversarial nets\",\n        \"“Vqa: Visual question answering\",\n        \"“Generating visual explanations\",\n        \"“Multimodal compact bilinear pooling for visual question answering and visual grounding\",\n        \"“Right for the right reasons: Training differentiable models by constraining their explanations\",\n        \"“Interpretable deep models for icu outcome prediction\",\n        \"“Treeview: Peeking into deep neural networks via feature-space partitioning\",\n        \"“Explaining nonlinear classification decisions with deep taylor decomposition\",\n        \"“Not just a black box: Learning important features through propagating activation differences\",\n        \"“Axiomatic attribution for deep networks\",\n        \"“Learning how to explain neural networks: Patternnet and patternattribution\",\n        \"“On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n        \"“” why should i trust you?” explaining the predictions of any classifier\",\n        \"“Learning deep features for discriminative localization\",\n        \"“Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"“Explaining recurrent neural network predictions in sentiment analysis\",\n        \"“Visualizing and understanding recurrent networks\",\n        \"“Interpretable recurrent neural networks using sequential sparse recovery\",\n        \"“Increasing the interpretability of recurrent neural networks using hidden markov models\",\n        \"N. Díaz-Rodríguez\",\n        \"“Interweaving multimodal interaction with flexible unit visualizations for data exploration\",\n        \"“Multimodal data to design visual learning analytics for understanding regulation of learning\",\n        \"“An analysis of visual question answering algorithms\",\n        \"“Toward explainable affective computing: A review\",\n        \"“Emoco: Visual analysis of emotion coherence in presentation videos\",\n        \"“Emotioncues: Emotion-oriented visual summarization of classroom videos\",\n        \"“Dehumor: Visual analytics for decomposing humor\",\n        \"“Towards a better gold standard: Denoising and modelling continuous emotion annotations based on feature agglomeration and outlier regularisation\",\n        \"M. Madadi et al\",\n        \"“Co-clustering to reveal salient facial features for expression recognition\",\n        \"“Using a pca-based dataset similarity measure to improve cross-corpus emotion recognition\",\n        \"“A visual-physiology multimodal system for detecting outlier behavior of participants in a reality tv show\",\n        \"“Out of the box: Reasoning with graph convolution nets for factual visual question answering\",\n        \"“M3gat: A multi-modal\",\n        \"“Explainable video action reasoning via prior knowledge and state transitions\",\n        \"“Modeling high-order relationships: Brain-inspired hypergraph-induced multimodal-multitask framework for semantic comprehension\",\n        \"“Enhancing explanations in recommender systems with knowledge graphs\",\n        \"“Behind the scene: Revealing the secrets of pre-trained vision-and-language models\",\n        \"“Probing image-language transformers for verb understanding\",\n        \"“Uniter: Universal image-text representation learning\",\n        \"“Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers\",\n        \"“Learning to explain: A model-agnostic framework for explaining black box models\",\n        \"“Clip surgery for better explainability with enhancement in open-vocabulary tasks\",\n        \"“Tagclip: A local-to-global framework to enhance open-vocabulary multi-label classification of clip without training\",\n        \"“Interpreting clip’s image representation via text-based decomposition\",\n        \"“Black-box tuning of vision-language models with effective gradient approximation\",\n        \"“Unveiling hierarchical relationships for social image representation learning\",\n        \"“Multimodal analogical reasoning over knowledge graphs\",\n        \"“Dkn: Deep knowledge-aware network for news recommendation\",\n        \"“Learning heterogeneous knowledge base embeddings for explainable recommendation\",\n        \"“Knowledge-aware autoencoders for explainable recommender systems\",\n        \"“Cross-modal causal relational reasoning for event-level visual question answering\",\n        \"“Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing\",\n        \"“Counterfactual vqa: A cause-effect look at language bias\",\n        \"D. Parikh et al\",\n        \"“Interpretable visual question answering by reasoning on dependency trees\",\n        \"“Lcm-captioner: A lightweight text-based image captioning method with collaborative mechanism between vision and text\",\n        \"“Multimodal research in vision and language: A review of current and emerging trends\",\n        \"“Counterfactual samples synthesizing for robust visual question answering\",\n        \"“Question-conditioned counterfactual image generation for vqa\",\n        \"“Multimodal explanations by predicting counterfactuality in videos\",\n        \"“Generating counterfactual explanations with natural language\",\n        \"“Non-autoregressive image captioning with counterfactuals-critical multi-agent learning\",\n        \"“Counterfactual critic multi-agent training for scene graph generation\",\n        \"“Bias in multimodal ai: Testbed for fair automatic recruitment\",\n        \"“Explicit bias discovery in visual question answering models\",\n        \"“Women also snowboard: Overcoming bias in captioning models\",\n        \"“Overcoming language priors in visual question answering with adversarial regularization\",\n        \"R. Salakhutdinov\",\n        \"“Multiviz: Towards visualizing and understanding multimodal models\",\n        \"“Vl-interpret: An interactive visualization tool for interpreting vision-language transformers\",\n        \"“Towards better explanations of class activation mapping\",\n        \"“Did the model understand the question?” arXiv preprint arXiv:1805.05492\",\n        \"“Towards the interpretability of deep learning models for multi-modal neuroimaging: Finding structural changes of the ageing brain\",\n        \"“Interpretability for multimodal emotion recognition using concept activation vectors\",\n        \"“Rise: Randomized input sampling for explanation of black-box models\",\n        \"“Gpt-4 technical report\",\n        \"“Lida: A tool for automatic generation of grammar-agnostic visualizations and infographics using large language models\",\n        \"“Can foundation models wrangle your data?” arXiv preprint arXiv:2205.09911\",\n        \"“Benchmarking large language models as ai research agents\",\n        \"D. Rifinski Fainman\",\n        \"“Generative table pre-training empowers models for tabular prediction\",\n        \"“Data science with llms and interpretable models\",\n        \"“Multimodal c4: An open\",\n        \"J. Parker-Holder\",\n        \"“Quality not quantity: On the interaction between dataset design and robustness of clip\",\n        \"B. Mirzasoleiman\",\n        \"“Rendering graphs for graph reasoning in multimodal large language models\",\n        \"“Llm and gnn are complementary: Distilling llm for multimodal graph learning\",\n        \"“How multimodal integration boost the performance of llm for optimization: Case study on capacitated vehicle routing problems\",\n        \"“Generative multimodal models are in-context learners\",\n        \"“What makes multimodal in-context learning work?” in Proc. of CVPR\",\n        \"“Obelics: An open web-scale filtered dataset of interleaved image-text documents\",\n        \"S. Sagawa et al\",\n        \"“Mm-narrator: Narrating long-form videos with multimodal in-context learning\",\n        \"“Beyond task performance: Evaluating and reducing the flaws of large multimodal models with in-context learning\",\n        \"“How does the textual information affect the retrieval of multimodal in-context learning?” arXiv preprint arXiv:2404.12866\",\n        \"“Chain of thought prompt tuning in vision language models\",\n        \"“Multimodal chain-of-thought reasoning in language models\",\n        \"“Visual chain of thought: bridging logical gaps with multimodal infillings\",\n        \"“Let’s think frame by frame with vip: A video infilling and prediction dataset for evaluating video chain-of-thought\",\n        \"“Contrastive novelty-augmented learning: Anticipating outliers with large language models\",\n        \"“Does synthetic data generation of llms help clinical text mining?” arXiv preprint arXiv:2303.04360\",\n        \"“Q: How to specialize large vision-language models to data-scarce vqa tasks? a: Self-train on unlabeled images!” in Proc. of CVPR\",\n        \"“T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering\",\n        \"“Llm-powered data augmentation for enhanced cross-lingual performance\",\n        \"“Generalized category discovery with large language models in the loop\",\n        \"“Llava-next: Improved reasoning\",\n        \"“Llava-med: Training a large language-and-vision assistant for biomedicine in one day\",\n        \"“Videochat: Chat-centric video understanding\",\n        \"“Dolphins: Multimodal language model for driving\",\n        \"“Salmonn: Towards generic hearing abilities for large language models\",\n        \"“Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models\",\n        \"M. Reynolds et al\",\n        \"“Mm-react: Prompting chatgpt for multimodal reasoning and action\",\n        \"“X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\",\n        \"“X-instructblip: A framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning\",\n        \"“Anymal: An efficient and scalable any-modality augmented language model\",\n        \"“Probing multimodal large language models for global and local semantic representation\",\n        \"J. Watson-Daniels\",\n        \"“What is the limitation of multimodal llms? a deeper look into multimodal llms through prompt probing\",\n        \"“Emergent world representations: Exploring a sequence model trained on a synthetic task\",\n        \"“Probing multimodal llms as world models for driving\",\n        \"“From text to pixel: Advancing long-context understanding in mllms\",\n        \"“Agla: Mitigating object hallucinations in large vision-language models with assembly of global and local attention\",\n        \"“Active reasoning in an open-world environment\",\n        \"“Reckoning: reasoning through dynamic knowledge encoding\",\n        \"“Statler: State-maintaining language models for embodied reasoning\",\n        \"Y. Chebotar et al\",\n        \"plan and select: interactive planning with llms enables open-world multi-task agents\",\n        \"“Knowledge acquisition disentanglement for knowledge-based visual question answering with large language models\",\n        \"“What if…?: Counterfactual inception to mitigate hallucination effects in large multimodal models\",\n        \"“Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models\",\n        \"“Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks\",\n        \"“What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models\",\n        \"“Using counterfactual tasks to evaluate the generality of analogical reasoning in large language models\",\n        \"“Stop reasoning! when multimodal llms with chain-of-thought reasoning meets adversarial images\",\n        \"“On the robustness of large multimodal models against image adversarial attacks\",\n        \"“A survey on data selection for language models\",\n        \"“Label words are anchors: An information flow perspective for understanding in-context learning\",\n        \"“Link-context learning for multimodal llms\",\n        \"“A survey of multimodal large language model from a data-centric perspective\",\n        \"“Image-of-thought prompting for visual reasoning refinement in multimodal large language models\",\n        \"“Mm-llms: Recent advances in multimodal large language models\",\n        \"“Visual instruction tuning\",\n        \"“Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms\",\n        \"“Robustness to spurious correlations in text classification via automatically generated counterfactuals\",\n        \"V. Sindhwani et al\",\n        \"S. Levine et al\",\n        \"“Collaborating with language models for embodied reasoning\",\n        \"“Hawk: Learning to understand open-world video anomalies\",\n        \"“Multimodal explanations: Justifying decisions and pointing to the evidence\",\n        \"“Toward human-understandable\",\n        \"“Personalized showcases: Generating multi-modal explanations for recommendations\",\n        \"“A first look: Towards explainable textvqa models via visual and textual explanations\",\n        \"“Uncovering what why and how: A comprehensive benchmark for causation understanding of video anomaly\",\n        \"“Docvqa: A dataset for vqa on document images\",\n        \"“Learn to explain: Multimodal reasoning via thought chains for science question answering\",\n        \"“Lora: A logical reasoning augmented dataset for visual question answering\",\n        \"“Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond\",\n        \"“Textual explanations for self-driving vehicles\",\n        \"“Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset\",\n        \"“From recognition to cognition: Visual commonsense reasoning\",\n        \"“Common sense reasoning for deep fake detection\",\n        \"“Explainable outfit recommendation with joint outfit matching and comment generation\",\n        \"“Reasoner: An explainable recommendation dataset with comprehensive labeling ground truths\",\n        \"“Towards surveillance video-and-language understanding: New dataset baselines and challenges\",\n        \"“Hallucination of multimodal large language models: A survey\",\n        \"“Eyes wide shut? exploring the visual shortcomings of multimodal llms\",\n        \"“V?: Guided visual search as a core mechanism in multimodal llms\",\n        \"“Crespr: Modular sparsification of dnns to improve pruning performance and model interpretability\",\n        \"“Language is primarily a tool for communication rather than thought\"\n    ],\n    \"reasoning\": \"Selected these 333 references because they represent foundational work and recent advances in A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}