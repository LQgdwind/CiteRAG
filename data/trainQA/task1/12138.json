{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 246 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Code Generation and Runtime Techniques for Enabling Data-Efficient Deep Learning Training on GPUs\n\nPaper Abstract: Abstract As deep learning models scale, their training cost has surged significantly. Due to both hardware advancements and limitations in current software stacks, the need for data efficiency has risen. Data efficiency refers to the effective hiding of data access latency and the avoidance of unnecessary data movements. Significant challenges arise from the growing disparity between GPU memory bandwidth and computational throughput, imminent GPU memory capacity limitations, and inefficiencies in the PyTorch software stack, including a lack of device-specific PCIe transfer optimizations and high-level domain-specific abstractions. To effectively mitigate these data inefficiencies for deep learning training, this dissertation analyzes data inefficiency in representative deep training tasks, specifically in graph neural networks (GNNs) and large language models (LLMs). It then proposes novel runtime and code generation techniques to mitigate these challenges and implements these optimizations seamlessly within the PyTorch stack while maintaining strong programmability and interoperability. First, Hector intermediate representation (IR) and its code generator are devised to introduce domain-specific high-level abstraction and systematically address memory-intensive performance challenges for relational graph neural networks (RGNNs). The performance challenges stem from RGNN’s inherent memory intensiveness, the gap between the programming interface and the kernel APIs, and the high kernel optimization cost due to kernel coupling with layout and heterogeneity. Using a general matrix multiply (GEMM) template and a traversal template, Hector achieves up to a 43.7×\\times speed-up in training and inference compared to the state-of-the-art systems. Linear operator reordering and compact tensor materialization further achieve up to 3.8×\\times speed-up compared to the Hector unoptimized code. Second, PyTorch-Direct is introduced to incorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN training. PyTorch-Direct significantly reduces CPU utilization, resulting in higher end-to-end training performance. For the input datasets and GNN architectures evaluated, PyTorch-Direct decreases the overall training time by up to 38.2%. Finally, in LLM training, the throughput has been increasingly constrained by GPU memory capacity. To mitigate this, the SSDTrain offloading framework is designed and implemented. Since activations take most of the GPU memory, SSDTrain offloads activations to Non-Volatile Memory Express (NVMe) SSDs with a direct GPU–SSD data path and good interoperability. The evaluation shows that SSDTrain reduces activations peak memory use by up to 47% with negligible overhead. We further analyze how the reduced activation memory use may be leveraged to increase throughput by increasing micro-batch size and reducing pipeline parallelism bubbles. Together, these contributions demonstrate that code generation and runtime techniques can systematically mitigate the data management bottlenecks in deep learning training, which stem from the data-intensive nature of workloads and the oversimplification inherent in the deep learning training software stack.\n\n{retrieved_text}\n\nGenerate exactly 246 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 246\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"“Graph convolutional neural networks for web-scale recommender systems\",\n        \"Graph convolutional neural networks for web-scale recommender systems\",\n        \"Deep learning recommendation model for personalization and recommendation systems\",\n        \"“Trading off compute in training and inference\",\n        \"“Deep learning hardware: Past\",\n        \"“TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings\",\n        \"“LLM-Analysis: Latency and memory analysis of transformer models for training and inference\",\n        \"“LLM inference unveiled: Survey and roofline model insights\",\n        \"compute and data trends in machine learning\",\n        \"“GPU specs database\",\n        \"“Lots of questions on Google’s ‘Trillium’ TPU v6\",\n        \"“NVIDIA Blackwell architecture and B200/B100 accelerators announced: Going bigger with smaller data\",\n        \"“Tensor processing unit\",\n        \"R. Narayanaswami\",\n        \"“Large graph convolutional network training with GPU-oriented data communication architecture\",\n        \"“EMOGI: Efficient memory-access for out-of-memory graph-traversal in GPUs\",\n        \"“Fine-grained memory access over I/O interconnect for efficient remote sparse data access\",\n        \"“Memory capacity growth: A major contributor to the success of computers\",\n        \"“No silver bullet essence and accidents of software engineering\",\n        \"“cuBLAS library user guide v12.0\",\n        \"” NVIDIA Corporation\",\n        \"“Hector: An efficient programming and compilation framework for implementing relational graph neural networks in GPU architectures\",\n        \"“PyTorch-Direct: Enabling GPU centric data access for very large graph neural network training with irregular accesses\",\n        \"“Graph neural network training with data tiering\",\n        \"“TBA: Faster large language model training using SSD-based activation offloading\",\n        \"“Backpropagation applied to handwritten zip code recognition\",\n        \"“Spectral networks and locally connected networks on graphs\",\n        \"“Inductive representation learning on large graphs\",\n        \"“Convolutional neural networks on graphs with fast localized spectral filtering\",\n        \"“Semi-supervised classification with graph convolutional networks\",\n        \"“Variational graph auto-encoders\",\n        \"“Learning convolutional neural networks for graphs\",\n        \"“Representation learning on graphs: Methods and applications\",\n        \"“DeepWalk: Online learning of social representations\",\n        \"“Node2vec: Scalable feature learning for networks\",\n        \"“Bing Chat — Microsoft Edge\",\n        \"” 2022. [Online]. Available: https://github.com/langchain-ai/langchain\",\n        \"“Emergent abilities of large language models\",\n        \"“Language models are unsupervised multitask learners\",\n        \"“Attention is all you need\",\n        \"“BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"“Llama 2: Open foundation and fine-tuned chat models\",\n        \"“Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"“GSPMD: General and scalable parallelization for ML computation graphs\",\n        \"“Megatron-LM: Training multi-billion parameter language models using model parallelism\",\n        \"“DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters\",\n        \"“PyTorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation\",\n        \"“ZeRO: Memory optimizations toward training trillion parameter models\",\n        \"Programming Massively Parallel Processors\",\n        \"“Nvidia Tesla V100 GPU architecture whitepaper\",\n        \"“Dissecting the NVIDIA Volta GPU architecture via microbenchmarking\",\n        \"“Instructions for managing a parallel cache hierarchy\",\n        \"“EE 7722 GPU microarchitecture lecture notes\",\n        \"“2. kernel profiling guide — NsightCompute 12.6 documentation\",\n        \"“Sorting of vector of tuple in C++ (ascending order)\",\n        \"“Can pytorch by-pass Python GIL?” 2019\",\n        \"“PEP 703 – making the global interpreter lock optional in CPython — peps.python.org\",\n        \"“Automated GPU kernel fusion with XLA\",\n        \"“PyTorch 2 tutorial and paper presentation @ ASPLOS’2024\",\n        \"“Compiling high-level scripting languages to performant code\",\n        \"“GPU acceleration of automatic differentiation in C++ with Clad\",\n        \"“Reverse-mode automatic differentiation and optimization of GPU kernels via Enzyme\",\n        \"“pybind11 — seamless operability between c++11 and python\",\n        \"“Deep graph library: A graph-centric\",\n        \"“Fast graph representation learning with PyTorch Geometric\",\n        \"“FeatGraph: A flexible and efficient backend for graph neural network systems\",\n        \"“Efficient sparse matrix kernels based on adaptive workload-balancing and parallel-reduction\",\n        \"“SparseTIR: Composable abstractions for sparse compilation in deep learning\",\n        \"“Modeling relational data with graph convolutional networks\",\n        \"“Heterogeneous graph transformer\",\n        \"“Empirical analysis of performance bottlenecks in graph neural network training and inference with GPUs\",\n        \"“The nature of graph neural network workloads\",\n        \"“cublas<<t>>gemmBatched() — cuBLAS library user guide v12.2\",\n        \"“Accelerating matrix multiplication with block sparse format and NVIDIA tensor cores — NVIDIA technical blog\",\n        \"“[feature] gather mm by isratnisa ⋅⋅\\\\cdot pull request #3641 ⋅⋅\\\\cdot dmlc/dgl\",\n        \"“Seastar: Vertex-centric programming for graph neural networks\",\n        \"“Graphiler: Optimizing graph neural networks with message passing data flow graph\",\n        \"“HGL: Accelerating heterogeneous GNN training with holistic representation and optimization\",\n        \"“Relational graph attention networks\",\n        \"“Open graph benchmark: Datasets for machine learning on graphs\",\n        \"“Kernel methods for mining instance data in ontologies\",\n        \"” Journal of Medicinal Chemistry\",\n        \"“A fast approximation of the weisfeiler-lehman graph kernel for RDF data\",\n        \"J. van Ossenbruggen\",\n        \"“Observed versus latent features for knowledge base and text inference\",\n        \"“Fast algorithms for convolutional neural networks\",\n        \"“Optimizing irregular dense operators of heterogeneous gnn models on gpu\",\n        \"“TorchScript — PyTorch 2.2 documentation\",\n        \"“Numba: A LLVM-based Python JIT compiler\",\n        \"“TVM: An automated end-to-end optimizing compiler for deep learning\",\n        \"“GE-SpMM: General-purpose sparse matrix-matrix multiplication on GPUs for graph neural networks\",\n        \"“At-scale sparse deep neural network inference with efficient gpu implementation\",\n        \"“TLPGNN: A lightweight two-level parallelism paradigm for graph neural network computation on GPU\",\n        \"“The tensor algebra compiler\",\n        \"“MLIR: Scaling compiler infrastructure for domain specific computation\",\n        \"“FusedMM: A unified sddmm-spmm kernel for graph embedding and graph neural networks\",\n        \"“Dense dynamic blocks: Optimizing SpMM for processors with vector and matrix units using machine learning techniques\",\n        \"H. Asghari-Moghaddam\",\n        \"“Automatic performance tuning of sparse matrix kernels\",\n        \"“Design principles for sparse matrix multiplication on the GPU\",\n        \"“CSR5: An efficient storage format for cross-platform sparse matrix-vector multiplication\",\n        \"“yaSpMV: Yet another SpMV framework on GPUs\",\n        \"“Chapter 31 - abstraction for AoS and SoA layout in C++\",\n        \"“SoAx: A generic C++ structure of arrays for handling particles in HPC codes\",\n        \"“DistDGL: Distributed graph neural network training for billion-scale graphs\",\n        \"“Tensor algebra compilation with workspaces\",\n        \"“Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks\",\n        \"“GraphSAINT: Graph sampling based inductive learning method\",\n        \"“A comprehensive survey on graph neural networks\",\n        \"“Stellargraph machine learning library\",\n        \"“Graph neural networks in tensorflow and keras with spektral\",\n        \"“SIGN: Scalable inception graph neural networks\",\n        \"efficient and scalable graph embedding\",\n        \"“Improving the accuracy\",\n        \"“Leaderboards for node property prediction — open graph benchmark\",\n        \"“Towards efficient large-scale graph neural network computing\",\n        \"“Traversing large graphs on GPUs with unified memory\",\n        \"“Subway: Minimizing data transfer during out-of-GPU-memory graph processing\",\n        \"“Nvidia Tesla P100 whitepaper\",\n        \"“Nvidia A100 TensorCore GPU architecture whitepaper\",\n        \"“Evaluating characteristics of CUDA communication primitives on high-bandwidth interconnects\",\n        \"“Data transfer matters for GPU computing\",\n        \"“Developing a linux kernel module using GPUDirect RDMA\",\n        \"“Unified memory for CUDA beginners\",\n        \"“Performance evaluation of advanced features in CUDA unified memory\",\n        \"“ImageNet classification with deep convolutional neural networks\",\n        \"“Deep residual learning for image recognition\",\n        \"“Graph attention networks\",\n        \"“Torchvision the machine-vision package of torch\",\n        \"“Beyond GPU memory limits with unified memory on pascal\",\n        \"“Autograd - pytorch/pytorch\",\n        \"“Aten/aten/src/readme.md at master · zdevito/aten · GitHub\",\n        \"“The WebGraph framework I: Compression techniques\",\n        \"“What is Twitter\",\n        \"“Konect: The koblenz network collection\",\n        \"“Winner-take-all column row sampling for memory efficient adaptation of language model\",\n        \"“Reducing activation recomputation in large transformer models\",\n        \"“MegaScale: Scaling large language model training to more than 10\",\n        \"A. McMillan-Major\",\n        \"“Dissecting batching effects in GPT inference\",\n        \"“The case for co-designing model architectures with hardware\",\n        \"“DeepSpeed Inference: Enabling efficient inference of transformer models at unprecedented scale\",\n        \"“Scaling laws for neural language models\",\n        \"“An empirical model of large-batch training\",\n        \"“Training deep nets with sublinear memory cost\",\n        \"“Announcing Epoch AI’s updated parameter\",\n        \"“ND A100 V4-series - Azure virtual machines\",\n        \"“GPU machine types | compute engine documentation\",\n        \"“Delta project profile\",\n        \"“Fiddler: CPU-GPU orchestration for fast inference of mixture-of-experts models\",\n        \"“ZeRO-Offload: Democratizing billion-scale model training\",\n        \"“PowerInfer: Fast large language model serving with a consumer-grade GPU\",\n        \"“FlashNeuron: SSD-enabled large-batch training of very deep neural networks\",\n        \"“About Google cloud hyperdisk — compute engine documentation\",\n        \"“Architecture and performance of Perlmutter’s 35 PB ClusterStor E1000 all‐flash file system\",\n        \"“Megatron-DeepSpeed: Ongoing research training transformer language models at scale\",\n        \"“Training compute-optimal large language models\",\n        \"“Language models are few-shot learners\",\n        \"“All SPEC/OSG results.” [Online]. Available: http://spec.org/cgi-bin/osgresults?conf=cpu2017;op=dump;format=csvdump\",\n        \"“Ultra-low latency with Samsung Z-NAND SSD\",\n        \"JESD218B: Solid-State Drive (SSD) Requirements and Endurance Test Method\",\n        \"Available: https://thinksystem\",\n        \"“QNAP NAS solution: QTS SSD extra over-provisioning\",\n        \"“Why SMART’s over-provisioning?” 2024\",\n        \"“Solidigm™ SSD endurance estimator\",\n        \"“Over-provisioning NAND-based Intel® SSDs for better endurance\",\n        \"“Over-provisioning benefits for Samsung data center SSDs\",\n        \"“Operational characteristics of SSDs in enterprise storage systems: A large-scale field study\",\n        \"“D7-P5620 mid-endurance PCIe 4.0 NVMe SSD for data centers | Solidigm D7 SSD\",\n        \"“Solidigm™ solid state drive D7-P5620 series (12.8TB\",\n        \"“FL6 series (2.5-inch) | KIOXIA - United States (English)\",\n        \"“Kioxia fl6xhul1t60 1.6TB PCIe4 NVMe SSD brand new\",\n        \"“SOLIDIGM ssdpf2sq800gz01 D7-P5810 solid state drive – Dihuni – GPU server for AI\",\n        \"“Quantifying performance gains of GPUDirect Storage\",\n        \"“STRONGHOLD: Fast and affordable billion-scale deep learning model training\",\n        \"“ZeRO-Infinity: Breaking the GPU memory wall for extreme scale deep learning\",\n        \"“FlexGen: High-throughput generative inference of large language models with a single GPU\",\n        \"“LLM in a Flash: Efficient large language model inference with limited memory\",\n        \"“KvikIO - high performance file IO\",\n        \"“Flash Correct-and-Refresh: Retention-aware error management for increased Flash memory lifetime\",\n        \"“Error patterns in MLC NAND Flash memory: Measurement\",\n        \"“Optimizing NAND Flash-based SSDs via retention relaxation\",\n        \"“Behemoth: A Flash-centric training accelerator for extreme-scale DNNs\",\n        \"“A monolingual approach to contextualized word embeddings for mid-resource languages\",\n        \"“Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures\",\n        \"“FlashAttention-2: Faster attention with better parallelism and work partitioning\",\n        \"“Scaling large language model training with Pax on GPUs\",\n        \"“FlashAttention: Fast and memory-efficient exact attention with IO-awareness\",\n        \"“Paxml (aka Pax)\",\n        \"analytics and research – Dihuni – GPU server for AI\",\n        \"“Intel Optane DC P5800X series 1.6TB\",\n        \"“Samsung V-NAND SSD 980 pro 2021 data sheet revision 2.1\",\n        \"“Samsung 980 pro 1TB internal gaming SSD PCIe gen 4 x4 nvme mz-v8p1t0b/am\",\n        \"“Don’t be a blockhead: Zoned namespaces make work on conventional SSDs obsolete\",\n        \"“ZNS+: Advanced zoned namespace interface for supporting in-storage zone compaction\",\n        \"The Datacenter as a Computer: Designing Warehouse-Scale Machines\",\n        \"“Total cost of ownership (TCO) analysis\",\n        \"“Nvidia Blackwell perf TCO analysis – B100 vs B200 vs GB200NVL72 – SemiAnalysis\",\n        \"“SNIA enterprise TCO calculator\",\n        \"“Introduction to NVIDIA DGX H100/H200 systems — NVIDIA DGX H100/H200 user guide\",\n        \"“NVM Express moves into the future\",\n        \"“Software:Lustre (file system)\",\n        \"“GPU Direct I/O with HDF5\",\n        \"“Efficient memory management for large language model serving with PagedAttention\",\n        \"“Capuchin: Tensor-based GPU memory management for deep learning\",\n        \"“SuperNeurons: Dynamic GPU memory management for training deep neural networks\",\n        \"“vDNN: Virtualized deep neural networks for scalable\",\n        \"“SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping\",\n        \"“NVIDIA H100 tensor core GPU architecture\",\n        \"“Big Bird: Transformers for longer sequences\",\n        \"“Deja Vu: Contextual sparsity for efficient LLMs at inference time\",\n        \"“SqueezeLLM: Dense-and-sparse quantization\",\n        \"“SpQR: A sparse-quantized representation for near-lossless LLM weight compression\",\n        \"“SparseGPT: Massive language models can be accurately pruned in one-shot\",\n        \"“Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\",\n        \"“Learning n:m fine-grained structured sparse neural networks from scratch\",\n        \"“Channel permutations for N:M sparsity\",\n        \"“MegaBlocks: Efficient sparse training with mixture-of-experts\",\n        \"“SparTA: Deep-learning model sparsity via tensor-with-sparsity-attribute\",\n        \"“Sparse GPU kernels for deep learning\",\n        \"“Efficient quantized sparse matrix operations on tensor cores\",\n        \"“Dynamic N:M fine-grained structured sparse attention mechanism\",\n        \"“Accelerating sparse deep neural networks\",\n        \"“TensorRT-LLM: A TensorRT toolbox for optimized large language model inference\",\n        \"“Transformer engine\",\n        \"“Add new keys for Graphcore IPU (DispatchKey / Backend / DeviceType) by AnthonyBarbier · pull request #74763 · pytorch/pytorch\",\n        \"“Release v0.8.0 · dmlc/dgl\",\n        \"“[doc] add an official documentation of UnifiedTensor by davidmin7 · pull request #3194 · dmlc/dgl\",\n        \"“[feature] add multi-GPU UnifiedTensor unit test by davidmin7 · pull request #3184 · dmlc/dgl\",\n        \"” 2021. [Online]. Available: https://github.com/dmlc/dgl/pull/3086\",\n        \"“Bridging the gap between deep learning and sparse matrix format selection\",\n        \"“Parallelizing maximal clique enumeration on GPUs\",\n        \"“HyLAC: Hybrid linear assignment solver in CUDA\",\n        \"“Controlling data movement to boost performance on the NVIDIA ampere architecture\",\n        \"“Mixture-of-experts with expert choice routing\",\n        \"“2017 Kaggle machine learning & data science survey\",\n        \"“2017 data scientist report\",\n        \"“Doing a reality check on GPU-accelerated databases\",\n        \"“RAPIDS: GPU-accelerated data analytics & machine learning\",\n        \"“GPU database systems characterization and optimization\",\n        \"“GPU databases—the new modality of data analytics\",\n        \"“Database optimization techniques #1: Indexing\",\n        \"“MySQL :: MySQL 8.4 reference manual :: 10.3 optimization and indexes\",\n        \"“SQream’s unique architecture: Comparing and contrasting to leading data architectures\",\n        \"“Evaluating end-to-end optimization for data analytics applications in Weld\",\n        \"“A tensor compiler for uniﬁed machine learning prediction serving\"\n    ],\n    \"reasoning\": \"Selected these 246 references because they represent foundational work and recent advances in Code Generation and Runtime Techniques for Enabling Data-Efficient Deep Learning Training on GPUs, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}