{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 45 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: MulGT: Multi-Task Graph-Transformer with Task-Aware Knowledge Injection and Domain Knowledge-Driven Pooling for Whole Slide Image Analysis\n\nPaper Abstract: Abstract Whole slide image (WSI) has been widely used to assist automated diagnosis under the deep learning fields. However, most previous works only discuss the SINGLE task setting which is not aligned with real clinical setting, where pathologists often conduct multiple diagnosis tasks simultaneously. Also, it is commonly recognized that the multi-task learning paradigm can improve learning efficiency by exploiting commonalities and differences across multiple tasks. To this end, we present a novel multi-task framework (i.e., MulGT) for WSI analysis by the specially designed Graph-Transformer equipped with Task-aware Knowledge Injection and Domain Knowledge-driven Graph Pooling modules. Basically, with the Graph Neural Network and Transformer as the building commons, our framework is able to learn task-agnostic low-level local information as well as task-specific high-level global representation. Considering that different tasks in WSI analysis depend on different features and properties, we also design a novel Task-aware Knowledge Injection module to transfer the task-shared graph embedding into task-specific feature spaces to learn more accurate representation for different tasks. Further, we elaborately design a novel Domain Knowledge-driven Graph Pooling module for each task to improve both the accuracy and robustness of different tasks by leveraging different diagnosis patterns of multiple tasks. We evaluated our method on two public WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney carcinoma. Experimental results show that our method outperforms single-task counterparts and the state-of-the-art methods on both tumor typing and staging tasks.\n\n{retrieved_text}\n\nGenerate exactly 45 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 45\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Multiple instance classification: Review, taxonomy and comparative study\",\n        \"Layer normalization\",\n        \"Accurate learning of graph representations with graph multiset pooling\",\n        \"Spectral clustering with graph neural networks for graph pooling\",\n        \"Multitask learning\",\n        \"Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning\",\n        \"Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks\",\n        \"Multi-label image recognition with graph convolutional networks\",\n        \"Classification and mutation prediction from nonâ€“small cell lung cancer histopathology images using deep learning\",\n        \"Multi-Task Learning with Deep Neural Networks: A Survey\",\n        \"Fast graph representation learning with PyTorch Geometric\",\n        \"Graph u-nets\",\n        \"Node-Aligned Graph Convolutional Network for Whole-Slide Image Representation and Classification\",\n        \"Deep residual learning for image recognition\",\n        \"H2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis\",\n        \"Attention-based deep multiple instance learning\",\n        \"RotoGrad: Gradient Homogenization in Multitask Learning\",\n        \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\",\n        \"A generalized deep learning framework for whole-slide image segmentation and analysis\",\n        \"Self-attention graph pooling\",\n        \"Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning\",\n        \"Mesh graphormer\",\n        \"Conflict-averse gradient descent for multi-task learning\",\n        \"Data-efficient and weakly supervised computational pathology on whole-slide images\",\n        \"Attention is all you need\",\n        \"Graphit: Encoding graph structure in transformers\",\n        \"Transformer for Graphs: An Overview from Architecture Perspective\",\n        \"Center-focusing multi-task CNN with injected features for classification of glioma nuclear images\",\n        \"Dropgnn: random dropouts increase the expressiveness of graph neural networks\",\n        \"Pytorch: An imperative style, high-performance deep learning library\",\n        \"HunCRC: annotated pathological slides to enhance deep learning applications in colorectal cancer screening\",\n        \"Kimianet: Training a deep network for histopathology using high-cellularity\",\n        \"Fine-Tuning and training of densenet for histopathology image representation using TCGA diagnostic slides\",\n        \"Self-supervised graph transformer on large-scale molecular data\",\n        \"Multi-task deep learning for colon cancer grading\",\n        \"Pathology image analysis using segmentation deep learning algorithms\",\n        \"Weakly supervised deep learning for whole slide lung cancer image analysis\",\n        \"Lymph Node Metastasis Prediction from Whole Slide Images with Transformer-guided Multi-instance Learning and Knowledge Transfer\",\n        \"Representing long-range context for graph neural networks with global attention\",\n        \"Detecting helicobacter pylori in whole slide images via weakly supervised multi-task learning\",\n        \"Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks\",\n        \"Hierarchical graph representation learning with differentiable pooling\",\n        \"Graph-bert: Only attention is needed for learning graph representations\",\n        \"An end-to-end deep learning architecture for graph classification\",\n        \"A deep learning based graph-transformer for whole slide image classification\"\n    ],\n    \"reasoning\": \"Selected these 45 references because they represent foundational work and recent advances in MulGT: Multi-Task Graph-Transformer with Task-Aware Knowledge Injection and Domain Knowledge-Driven Pooling for Whole Slide Image Analysis, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}