{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study\n\nPaper Abstract: Abstract Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were associated with a significant increase in action understanding score (0.63 ±plus-or-minus\\pm 0.04 for select augmentations, compared to 0.54 ±plus-or-minus\\pm 0.02 for the control) with p=0.002. In-depth interviews of self-identifying breast cancer patients (N=7) were also conducted via video conferencing. Augmentations, especially definitions, elicited positive responses among the seven participants, with some concerns about relying on LLMs. Augmentations were evaluated for errors by clinicians, and we found misleading errors occur, with errors more common in real donated notes than synthetic notes, illustrating the importance of carefully written clinical notes. Augmentations improve some but not all readability metrics. This work demonstrates the potential of LLMs to improve patients’ experience with clinical notes at a lower burden to clinicians. However, having a human in the loop is important to correct potential model errors.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Individuals’ Access and Use of Patient Portals and Smartphone Health Apps\",\n        \"OpenNotes After 7 Years: Patient Experiences With Ongoing Access to Their Clinicians’ Outpatient Visit Notes\",\n        \"Accuracy in Patient Understanding of Common Medical Phrases\",\n        \"Communication during radiation therapy education sessions: The role of medical jargon and emotional support in clarifying patient confusion\",\n        \"Readability of discharge summaries: with what level of information are we dismissing our patients?\",\n        \"Effect of notes’ access and complexity on OpenNotes’ utility\",\n        \"Open notes sounds great, but will a provider’s documentation change? An exploratory study of the effect of open notes on oncology documentation.\",\n        \"Your patient is now reading your note: opportunities, problems, and prospects\",\n        \"A Patient-Centered Approach to Writing Ambulatory Visit Notes in the Cures Act Era\",\n        \"Beyond readability: investigating coherence of clinical text for consumers\",\n        \"Prevalence of Burnout Among Physicians: A Systematic Review\",\n        \"A survey of automated methods for biomedical text simplification\",\n        \"Decoding radiology reports: Potential application of OpenAI ChatGPT to enhance patient understanding of diagnostic reports\",\n        \"Paniniqa: Enhancing patient education through interactive question answering\",\n        \"Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes\",\n        \"Use of artificial intelligence chatbots for cancer treatment information\",\n        \"Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation\",\n        \"Impact of open access to physician notes on radiation oncology patients: results from an exploratory survey\",\n        \"OpenNotes in oncology: oncologists’ perceptions and a baseline of the content and style of their clinician notes\",\n        \"Open notes in oncology: patient versus oncology clinician views\",\n        \"GPT-4 Technical Report\",\n        \"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts\",\n        \"A new readability yardstick\",\n        \"Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel\",\n        \"Confidence intervals from normalized data: A correction to Cousineau (2005)\",\n        \"Validation of screening questions for limited health literacy in a large VA outpatient population\",\n        \"The importance of visit notes on patient portals for engaging less educated or nonwhite patients: survey study\",\n        \"Multilingual machine translation with large language models: Empirical results and analysis\",\n        \"Presentation matters for AI-generated clinical advice\",\n        \"Leveraging large language models for generating responses to patient messages\",\n        \"Frequency and types of patient-reported errors in electronic health record ambulatory care notes\",\n        \"Write it like you see it: Detectable differences in clinical notes by race lead to differential model recommendations\",\n        \"Engaging patients through open notes: an evaluation using mixed methods\",\n        \"Patients evaluate visit notes written by their clinicians: a mixed methods investigation\",\n        \"Care partners reading patients’ visit notes via patient portals: Characteristics and perceptions\",\n        \"Open Oncology Notes: A Qualitative Study of Oncology Patients’ Experiences Reading Their Cancer Care Notes\",\n        \"Patients’ Experiences of Web-Based Access to Electronic Health Records in Finland: Cross-sectional Survey\",\n        \"Physician stress and burnout: the impact of health information technology\",\n        \"The Views and Experiences of Clinicians Sharing Medical Record Notes With Patients\",\n        \"Open notes in cancer care: coming soon to patients\",\n        \"A natural language processing system that links medical terms in electronic health record notes to lay definitions: system development using physician reviews\",\n        \"Deciphering clinical abbreviations with a privacy protecting machine learning system\",\n        \"Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing\",\n        \"Effect of expansion of abbreviations and acronyms on patient comprehension of their health records: a randomized clinical trial\",\n        \"Patient Perception of Plain-Language Medical Notes Generated Using Artificial Intelligence Software: Pilot Mixed-Methods Study\",\n        \"A large language model for electronic health records\",\n        \"Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum\",\n        \"Large language models are few-shot clinical information extractors\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}