{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 44 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning\n\nPaper Abstract: Abstract Vision transformers have recently set off a new wave in the field of medical image analysis due to their remarkable performance on various computer vision tasks. However, recent hybrid-/transformer-based approaches mainly focus on the benefits of transformers in capturing long-range dependency while ignoring the issues of their daunting computational complexity, high training costs, and redundant dependency. In this paper, we propose to employ adaptive pruning to transformers for medical image segmentation and propose a lightweight and effective hybrid network APFormer. To our best knowledge, this is the first work on transformer pruning for medical image analysis tasks. The key features of APFormer mainly are self-supervised self-attention (SSA) to improve the convergence of dependency establishment, Gaussian-prior relative position embedding (GRPE) to foster the learning of position information, and adaptive pruning to eliminate redundant computations and perception information. Specifically, SSA and GRPE consider the well-converged dependency distribution and the Gaussian heatmap distribution separately as the prior knowledge of self-attention and position embedding to ease the training of transformers and lay a solid foundation for the following pruning operation. Then, adaptive transformer pruning, both query-wise and dependency-wise, is performed by adjusting the gate control parameters for both complexity reduction and performance improvement. Extensive experiments on two widely-used datasets demonstrate the prominent segmentation performance of APFormer against the state-of-the-art methods with much fewer parameters and lower GFLOPs. More importantly, we prove, through ablation studies, that adaptive pruning can work as a plug-n-play module for performance improvement on other hybrid-/transformer-based methods. Code is available at https://github.com/xianlin7/APFormer.\n\n{retrieved_text}\n\nGenerate exactly 44 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 44\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Attention is all you need\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n        \"End-to-end object detection with transformers\",\n        \"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers\",\n        \"U-Net: Convolutional networks for biomedical image segmentation\",\n        \"Weighted ResUNet for high-quality retina vessel segmentation\",\n        \"Attention gated networks: Learning to leverage salient regions in medical images\",\n        \"TransUNet: Transformers make strong encoders for medical image segmentation\",\n        \"TransFuse: Fusing transformers and CNNs for medical image segmentation\",\n        \"Convolution-free medical image segmentation using transformers\",\n        \"TransBTS: Multimodal brain tumor segmentation using transformer\",\n        \"TransAttUNet: Multi-level attention-guided U-Net with transformer for medical image segmentation\",\n        \"CoTr: Efficiently bridging CNN and transformer for 3D medical image segmentation\",\n        \"UTNet: A hybrid transformer architecture for medical image segmentation\",\n        \"Medical transformer: Gated axial-attention for medical image segmentation\",\n        \"Boundary-aware transformers for skin lesion segmentation\",\n        \"A multi-scale transformer for medical image segmentation: architectures, model efficiency, and benchmarks\",\n        \"More than encoder: Introducing transformer decoder to upsample\",\n        \"DS-TransUNet: Dual swin transformer U-Net for medical image segmentation\",\n        \"Multi-compound transformer for accurate biomedical image segmentation\",\n        \"MISSFormer: An effective medical image segmentation transformer\",\n        \"D-Former: A U-shaped dilated transformer for 3D medical image segmentation\",\n        \"Visual transformer pruning\",\n        \"IA-RED2: Interpretability-aware redundancy reduction for vision transformers\",\n        \"DynamicViT: Efficient vision transformers with dynamic token sparsification\",\n        \"AdaViT: Adaptive tokens for efficient vision transformer\",\n        \"Multi-label co-regularization for semi-supervised facial action unit recognition\",\n        \"FAT-Net: Feature adaptive transformers for automated skin lesion segmentation\",\n        \"CPFNet: Context pyramid fusion network for medical image segmentation\",\n        \"Skin lesion segmentation via generative adversarial networks with dual discriminators\",\n        \"Cascade knowledge diffusion network for skin lesion diagnosis and segmentation\",\n        \"UNETR: Transformers for 3D medical image segmentation\",\n        \"nnFormer: Interleaved transformer for volumetric segmentation\",\n        \"Swin-UNet: UNet-like pure transformer for medical image segmentation\",\n        \"LeViT-UNet: Make faster encoders with transformer for medical image segmentation\",\n        \"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions\",\n        \"TransClaw U-Net: Claw U-Net with transformers for medical image segmentation\",\n        \"nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation\",\n        \"Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)\",\n        \"Are convolutional neural networks or transformers more like human vision\",\n        \"SpecTr: Spectral transformer for hyperspectral pathology image segmentation\",\n        \"Transformers in medical image analysis: A review\",\n        \"Contextual attention network: Transformer meets U-Net\"\n    ],\n    \"reasoning\": \"Selected these 44 references because they represent foundational work and recent advances in The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}