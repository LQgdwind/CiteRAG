{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 247 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Inductive Biases for Deep Learning of Higher-Level Cognition\n\nPaper Abstract: Abstract A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.\n\n{retrieved_text}\n\nGenerate exactly 247 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 247\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Causalworld: A robotic manipulation benchmark for causal structure and transfer learning\",\n        \"Do as i can, not as i say: Grounding language in robotic affordances\",\n        \"Learning to recombine and resample data for compositional generalization\",\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"Memory consolidation and the medial temporal lobe: a simple network model\",\n        \"Rudder: Return decomposition for delayed rewards\",\n        \"Invariant risk minimization\",\n        \"A cognitive theory of consciousness\",\n        \"In the theatre of consciousness. global workspace theory, a rigorous scientific theory of consciousness\",\n        \"Working memory\",\n        \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n        \"Systematic generalization: what is required and can it be learned?\",\n        \"Closure: Assessing systematic generalization of clevr models\",\n        \"Object permanence in five-month-old infants\",\n        \"Emergent tool use from multi-agent autocurricula\",\n        \"Discovering event structure in continuous narrative perception and memory\",\n        \"Emergent complexity via multi-agent competition\",\n        \"Automatic and conscious processing of social information\",\n        \"Simulation as an engine of physical scene understanding\",\n        \"Relational inductive biases, deep learning, and graph networks\",\n        \"Gan dissection: Visualizing and understanding generative adversarial networks\",\n        \"A model of inductive bias learning\",\n        \"Top-down and bottom-up mechanisms in biasing competition in the human brain\",\n        \"Recognition in terra incognita\",\n        \"The hippocampal film editor: sensitivity and specificity to event boundaries in continuous experience\",\n        \"Independently controllable features\",\n        \"Taking on the curse of dimensionality in joint distributions using neural networks\",\n        \"Learning long-term dependencies with gradient descent is difficult\",\n        \"Evolving culture versus local minima\",\n        \"Learning a synaptic learning rule\",\n        \"A neural probabilistic language model\",\n        \"A meta-transfer objective for learning to disentangle causal mechanisms\",\n        \"On the development of the reference prior method\",\n        \"Why am i remembering this now? predicting the occurrence of involuntary (spontaneous) episodic memories\",\n        \"Using cognitive psychology to understand gpt-3\",\n        \"Training with noise is equivalent to tikhonov regularization\",\n        \"Neural networks for pattern recognition\",\n        \"Prior knowledge modulates the neural substrates of encoding and retrieving naturalistic events at short and long delays\",\n        \"Improving language models by retrieving from trillions of tokens\",\n        \"Learning representations using causal invariance\",\n        \"Conflict monitoring and cognitive control\",\n        \"Evaluating the demand for control: Anterior cingulate cortex and crosstalk monitoring\",\n        \"Typing assumptions improve identification in causal discovery\",\n        \"Language models are few-shot learners\",\n        \"Spectral networks and locally connected networks on graphs\",\n        \"Understanding disentangling in \\\\\\\\\\\\backslashbeta-vae\",\n        \"Monet: Unsupervised scene decomposition and representation\",\n        \"Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices\",\n        \"Scaling data-driven robotics with reward sketching and batch reinforcement learning\",\n        \"Conscious attention and abstraction in concept learning\",\n        \"Multitask learning\",\n        \"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks\",\n        \"Isolating sources of disentanglement in variational autoencoders\",\n        \"A simple framework for contrastive learning of visual representations\",\n        \"Improved baselines with momentum contrastive learning\",\n        \"Babyai: First steps towards grounded language learning with a human in the loop\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Hierarchical multiscale recurrent neural networks\",\n        \"Top-down and bottom-up attention to memory: a hypothesis (atom) on the role of the posterior parietal cortex in memory retrieval\",\n        \"Cortical mechanisms of action selection: the affordance competition hypothesis\",\n        \"Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence\",\n        \"Anterior cingulate and prefrontal cortex: who’s in control?\",\n        \"Rate of apparent change of a necker cube as a function of prior stimulation\",\n        \"Theories of access consciousness\",\n        \"A unified architecture for natural language processing: Deep neural networks with multitask learning\",\n        \"An embedded-processes model of working memory\",\n        \"Markov chain monte carlo convergence diagnostics: a comparative review\",\n        \"Causal reasoning from meta-reinforcement learning\",\n        \"Convolutional neural networks on graphs with fast localized spectral filtering\",\n        \"What is consciousness, and could machines have it?\",\n        \"How We Learn: Why Brains Learn Better Than Any Machine… for Now\",\n        \"Experimental and theoretical approaches to conscious processing\",\n        \"Bayesian structure learning with generative flow networks\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"BERT: pre-training of deep bidirectional transformers for language understanding\",\n        \"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"From socrates to expert systems: The limits and dangers of calculative rationality\",\n        \"Unsupervised discovery of 3d physical objects from video\",\n        \"An empirical investigation of the challenges of real-world reinforcement learning\",\n        \"A guide to convolution arithmetic for deep learning\",\n        \"On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables\",\n        \"Genesis: Generative scene inference and sampling with object-centric latent representations\",\n        \"Why does unsupervised pre-training help deep learning?\",\n        \"Attend, infer, repeat: Fast scene understanding with generative models\",\n        \"Addressing some limitations of transformers with feedback memory\",\n        \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data\",\n        \"Connectionism and cognitive architecture: A critical analysis\",\n        \"Priors in bayesian deep learning: A review\",\n        \"Extending factor graphs so as to unify directed and undirected graphical models\",\n        \"Very loopy belief propagation for unwrapping phase images\",\n        \"Shortcut learning in deep neural networks\",\n        \"Bayesian model-building by pure thought: some principles and examples\",\n        \"The theory of affordances\",\n        \"Reasoning the fast and frugal way: models of bounded rationality\",\n        \"Markov chain Monte Carlo in practice\",\n        \"Neural message passing for quantum chemistry\",\n        \"Explaining and harnessing adversarial examples\",\n        \"What’s in an object file? evidence from priming studies\",\n        \"Recurrent independent mechanisms, 2019a\",\n        \"Recurrent independent mechanisms, 2019b\",\n        \"Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems\",\n        \"Neural production systems\",\n        \"Coordination among neural modules through a shared global workspace\",\n        \"Retrieval-augmented reinforcement learning\",\n        \"Neural turing machines\",\n        \"Tagger: Deep unsupervised perceptual grouping\",\n        \"Multi-object representation learning with iterative variational inference\",\n        \"Bootstrap your own latent-a new approach to self-supervised learning\",\n        \"Dynamic neural turing machine with soft and hard addressing schemes\",\n        \"Using imagination to understand the neural basis of episodic memory\",\n        \"Deep residual learning for image recognition\",\n        \"The many faces of robustness: A critical analysis of out-of-distribution generalization\",\n        \"Grounded language learning in a simulated 3d world\",\n        \"beta-vae: Learning basic visual concepts with a constrained variational framework\",\n        \"Hierarchical recurrent neural networks for long-term dependencies\",\n        \"Environmental drivers of systematicity and generalization in a situated agent\",\n        \"How to represent part-whole hierarchies in a neural network\",\n        \"Distributed representations\",\n        \"A fast learning algorithm for deep belief nets\",\n        \"Untersuchungen zu dynamischen neuronalen netzen [in german] diploma thesis\",\n        \"Long short-term memory\",\n        \"Densely connected convolutional networks\",\n        \"Bc-z: Zero-shot task generalization with robotic imitation learning\",\n        \"Three factors influencing minima in sgd\",\n        \"An invariant form for the prior probability in estimation problems\",\n        \"Thinking, fast and slow\",\n        \"The reviewing of object files: Object-specific integration of information\",\n        \"Sparse attentive backtracking: Temporal credit assignment through reminding\",\n        \"Learning neural causal models from unknown interventions\",\n        \"Amortized learning of neural causal representations\",\n        \"Systematic evaluation of causal discovery in visual model based reinforcement learning\",\n        \"Untangling tradeoffs between recurrence and self-attention in neural networks\",\n        \"Disentangling by factorising\",\n        \"The order of visual processing:“top-down,”“bottom-up,” or “middle-out”\",\n        \"The quest for consciousness a neurobiological approach\",\n        \"Wilds: A benchmark of in-the-wild distribution shifts\",\n        \"Learning the structure of markov logic networks\",\n        \"Mental labour\",\n        \"Sequential attend, infer, repeat: Generative modelling of moving objects\",\n        \"Towards understanding how machines can learn causal overhypotheses\",\n        \"A clockwork rnn\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Out-of-distribution generalization via risk extrapolation (rex)\",\n        \"Factor graphs and the sum-product algorithm\",\n        \"Regularization for deep learning: A taxonomy\",\n        \"Adversarial examples in the physical world\",\n        \"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\",\n        \"Efficient behavior of small-world networks\",\n        \"Learning a generative model of images by factoring appearance and shape\",\n        \"Convolutional networks for images, speech, and time series\",\n        \"Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research\",\n        \"Backpropagation and the brain\",\n        \"Discrete-valued neural communication\",\n        \"Emergent coordination through competition\",\n        \"Challenging common assumptions in the unsupervised learning of disentangled representations\",\n        \"Object-centric learning with slot attention\",\n        \"Fully convolutional networks for semantic segmentation\",\n        \"Unified-io: A unified model for vision, language, and multi-modal tasks\",\n        \"Rethinking eliminative connectionism\",\n        \"The algebraic mind: Integrating connectionism and cognitive science\",\n        \"The programmable blackboard model of reading\",\n        \"James L McClelland and David E Rumelhart\",\n        \"Parallel Distributed Processing, Volume 2: Explorations in the Microstructure of Cognition: Psychological and Biological Models, volume 2\",\n        \"Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models\",\n        \"Intuitive physics\",\n        \"Interactions of top-down and bottom-up mechanisms in human visual cortex\",\n        \"Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules\",\n        \"Playing atari with deep reinforcement learning\",\n        \"On the number of linear regions of deep neural networks\",\n        \"Parsing the stream of time: The value of event-based segmentation in a complex real-world control problem\",\n        \"A rational analysis of cognitive control in a speeded discrimination task\",\n        \"Loopy belief propagation for approximate inference: An empirical study\",\n        \"A neural global workspace model for conscious attention\",\n        \"The persistence of object file representations\",\n        \"Assessing generalization in deep reinforcement learning\",\n        \"On the number of inference regions of deep feed forward networks with piece-wise linear activations\",\n        \"Align-rudder: Learning from few demonstrations by reward redistribution\",\n        \"Causality: Models, Reasoning, and Inference\",\n        \"Elements of Causal Inference - Foundations and Learning Algorithms\",\n        \"Elements of causal inference: foundations and learning algorithms\",\n        \"Navigating the affordance landscape: feedback control as a process model of behavior and cognition\",\n        \"Efficient neural architecture search via parameter sharing\",\n        \"Discriminability-based transfer between neural networks\",\n        \"Direct transfer of learned information among neural networks\",\n        \"An analysis of the adaptation speed of causal models\",\n        \"Event boundaries in memory and cognition\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"S2rms: Spatially structured recurrent modules\",\n        \"Discovering objects and their relations from entangled scene representations\",\n        \"What is bottom-up and what is top-down in predictive coding?\",\n        \"Equivariance through parameter-sharing\",\n        \"Optimization as a model for few-shot learning\",\n        \"Goal-directed and habitual control in the basal ganglia: implications for parkinson’s disease\",\n        \"A generalist agent\",\n        \"Constructing experience: Event models from perception to action\",\n        \"Child: A first step towards continual learning\",\n        \"An overview of multi-task learning in deep neural networks\",\n        \"A benchmark for systematic generalization in grounded language understanding\",\n        \"Sequential thought processes in pdp models\",\n        \"Artificial intelligence: A modern approach by stuart\",\n        \"Multitask prompted training enables zero-shot task generalization\",\n        \"Relational recurrent neural networks\",\n        \"E (n) equivariant graph neural networks\",\n        \"Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-… hook\",\n        \"Improving robustness against common corruptions by covariate shift adaptation\",\n        \"Automatic/control processing and attention\",\n        \"Artificial intelligence: Learning to see and act\",\n        \"On causal and anticausal learning\",\n        \"Toward causal representation learning\",\n        \"Mastering atari, go, chess and shogi by planning with a learned model\",\n        \"Consciousness, emotion, and imagination\",\n        \"A cognitive architecture that combines internal simulation with a global workspace\",\n        \"Embodiment and the inner life: Cognition and Consciousness in the Space of Possible Minds\",\n        \"The brain’s connective core and its role in animal cognition\",\n        \"Applying global workspace theory to the frame problem\",\n        \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\",\n        \"Toward a rational and mechanistic account of mental effort\",\n        \"Inventing relational state and action abstractions for effective and efficient bilevel planning\",\n        \"A bayesian perspective on generalization and stochastic gradient descent\",\n        \"On the proper treatment of connectionism\",\n        \"Semantic networks\",\n        \"Origins of knowledge\",\n        \"Causation, prediction, and search\",\n        \"Dropout: a simple way to prevent neural networks from overfitting\",\n        \"Conditional routing of information to the cortex: A model of the basal ganglia’s role in cognitive coordination\",\n        \"The evolution of foresight: What is mental time travel, and is it unique to humans?\",\n        \"End-to-end memory networks\",\n        \"The bitter lesson\",\n        \"Do new caledonian crows solve physical problems through causal reasoning?\",\n        \"Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds\",\n        \"Relational neural expectation maximization: Unsupervised discovery of objects and their interactions\",\n        \"Attention is all you need\",\n        \"Learning to reinforcement learn\",\n        \"Collective dynamics of ‘small-world’networks\",\n        \"Do we still need models or just more data and compute?\",\n        \"Understanding natural language\",\n        \"No free lunch theorems for search\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Multi-scale context aggregation by dilated convolutions\",\n        \"Preparing for the unknown: Learning a universal policy with online system identification\",\n        \"Unsupervised video decomposition using spatio-temporal iterative inference\",\n        \"Event perception: a mind-brain perspective\",\n        \"mixup: Beyond empirical risk minimization\",\n        \"A consciousness-inspired planning agent for model-based reinforcement learning\"\n    ],\n    \"reasoning\": \"Selected these 247 references because they represent foundational work and recent advances in Inductive Biases for Deep Learning of Higher-Level Cognition, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}