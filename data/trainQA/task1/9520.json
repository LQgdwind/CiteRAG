{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 63 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory\n\nPaper Abstract: Abstract Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. For comparison, we also include human behavioral benchmarks. Note that all computational models were never trained with any human data or behavioral biases; yet, these models remarkably replicate some characteristics of WM in biological brains, such as primacy and recency effects. Moreover, we performed neural population analysis on these models and identified neural clusters specialized for different domains and functionalities of WM. Not all computational models exhibit a strong alignment with all human behaviors. Our experimental results also reveal several limitations in existing models to match with working memory capabilities of humans. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM’s neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at: link.\n\n{retrieved_text}\n\nGenerate exactly 63 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 63\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Limitations in working memory: Implications for language development\",\n        \"Attention-deficit/hyperactivity disorder (adhd) and working memory in adults: a meta-analytic review\",\n        \"The capacity of visual short-term memory is set both by visual information load and by number of objects\",\n        \"Serial report and item recognition of novel visual patterns\",\n        \"Working memory and language: An overview\",\n        \"Working memory and the hippocampus\",\n        \"Working memory and executive control\",\n        \"Dynamic shifts of limited working memory resources in human vision\",\n        \"The role of working memory in language comprehension\",\n        \"Learning phrase representations using rnn encoder-decoder for statistical machine translation\",\n        \"Pillow (pil fork) documentation\",\n        \"Working memory capacity and its relation to general intelligence\",\n        \"First things first: similar list length and output order effects for verbal and nonverbal stimuli\",\n        \"Attention and memory: An integrated framework\",\n        \"Persistent activity in the prefrontal cortex during working memory\",\n        \"Individual differences in working memory and reading\",\n        \"The role of working memory in visual selective attention\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"The neural basis of the central executive system of working memory\",\n        \"Adult age differences in working memory\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Memory-augmented neural machine translation\",\n        \"Working memory in schizophrenia: a meta-analysis\",\n        \"Neuron activity related to short-term memory\",\n        \"Neural turing machines\",\n        \"Impulsive decision making and working memory\",\n        \"Long short-term memory\",\n        \"The role of parietal cortex in verbal working memory\",\n        \"The effect of retention interval on serial position curves for item recognition of visual patterns and faces\",\n        \"Adam: A method for stochastic optimization\",\n        \"Age differences in short-term retention of rapidly changing information\",\n        \"Superior parietal cortex is critical for the manipulation of information in working memory\",\n        \"Ask me anything: Dynamic memory networks for natural language processing\",\n        \"Reasoning ability is (little more than) working-memory capacity?!\",\n        \"The capacity of visual working memory for features and conjunctions\",\n        \"Accessing recent events\",\n        \"Prefrontal cortex and basal ganglia control access to working memory\",\n        \"Reconfiguration of processing mode prior to task performance\",\n        \"Neural mechanisms of visual working memory in prefrontal cortex of the macaque\",\n        \"The magical number seven, plus or minus two: Some limits on our capacity for processing information\",\n        \"Die koordination kognitiver operationen. eine studie zum zusammenhang von “workingmemory”und intelligenz (the coordination of cognitive operations: a study on the relationship between working-memory and intelligence)\",\n        \"Access to information in working memory: exploring the focus of attention\",\n        \"Design for a working memory\",\n        \"A formal model of capacity limits in working memory\",\n        \"Benchmarks for models of short-term and working memory\",\n        \"Individual differences in working memory capacity and reasoning ability\",\n        \"Increased prefrontal and parietal activity after training of working memory\",\n        \"Shared mechanisms underlie the control of working memory and attention\",\n        \"Backward masking\",\n        \"The role of working memory in information acquisition and decision making: Lessons from the binary prediction task\",\n        \"Neuronal correlates of parametric working memory in the prefrontal cortex\",\n        \"Adult age differences in integrative spatial ability\",\n        \"Decomposing adult age differences in working memory\",\n        \"The separability of working memory resources for spatial thinking and language processing: an individual differences approach\",\n        \"Hippocampal–prefrontal input supports spatial encoding in working memory\",\n        \"End-to-end memory networks\",\n        \"Working-memory capacity explains reasoning ability—and a little bit more\",\n        \"Visualizing data using t-sne\",\n        \"Attention is all you need\",\n        \"Visual and spatial working memory are not that dissociated after all: a time-based resource-sharing account\",\n        \"Storage of features, conjunctions, and objects in visual working memory\",\n        \"Memory networks\",\n        \"Task representations in neural networks trained to perform many cognitive tasks\"\n    ],\n    \"reasoning\": \"Selected these 63 references because they represent foundational work and recent advances in Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}