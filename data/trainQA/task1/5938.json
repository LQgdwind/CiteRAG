{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 53 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: De-biasing “bias” measurement\n\nPaper Abstract: Abstract. When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of group-wise model performance disparities indicate statistically significant differences, when accounting for statistical bias in the estimator, the estimated between-group disparities are no longer statistically significant.\n\n{retrieved_text}\n\nGenerate exactly 53 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 53\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Fairness and robustness in invariant learning: A case study in toxicity classification\",\n        \"A reductions approach to fair classification\",\n        \"Fairsight: Visual analytics for fairness in decision making\",\n        \"Themis: Automatically testing software for discrimination\",\n        \"Machine Bias\",\n        \"Fairness On The Ground: Applying Algorithmic Fairness Approaches to Production Systems\",\n        \"More Accounts, Fewer Links: How Algorithmic Curation Impacts Media Exposure in Twitter Timelines\",\n        \"Penalizing unfairness in binary classification\",\n        \"AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias\",\n        \"Fairlearn: A toolkit for assessing and improving fairness in AI\",\n        \"Individually Fair Ranking\",\n        \"Gender shades: Intersectional accuracy disparities in commercial gender classification\",\n        \"FairVis: Visual analytics for discovering intersectional bias in machine learning\",\n        \"The combination of estimates from different experiments\",\n        \"Evaluating fairness using permutation tests\",\n        \"Is there a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing\",\n        \"Fairness through awareness\",\n        \"Decoupled classifiers for group-fair and efficient machine learning\",\n        \"An introduction to the bootstrap\",\n        \"A comparative study of fairness-enhancing interventions in machine learning\",\n        \"Fairness-aware ranking in search & recommendation systems with application to linkedin talent search\",\n        \"Characterizing Intersectional Group Fairness with Worst-Case Comparisons\",\n        \"TensorFlow Fairness Indicators\",\n        \"On the Problem of Underranking in Group-Fair Ranking\",\n        \"Maximum likelihood approaches to variance component estimation and to related problems\",\n        \"Fairness without demographics in repeated loss minimization\",\n        \"Statistical methods for meta-analysis\",\n        \"Responsibly: Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems\",\n        \"The disparate effects of strategic manipulation\",\n        \"Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition\",\n        \"An algorithm for removing sensitive information: application to race-independent recidivism prediction\",\n        \"Preventing fairness gerrymandering: Auditing and learning for subgroup fairness\",\n        \"Nonconvex optimization for regression with fairness constraints\",\n        \"A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses\",\n        \"Measuring Disparate Outcomes of Content Recommendation Algorithms with Distributional Inequality Metrics\",\n        \"Delayed impact of fair machine learning\",\n        \"To predict and serve?\",\n        \"Learning adversarially fair and transferable representations\",\n        \"Fairness-aware learning for continuous attributes and treatments\",\n        \"Income inequality in the attention economy\",\n        \"Fairness through Experimentation: Inequality in A/B testing as an approach to responsible design\",\n        \"Aequitas: A bias and fairness audit toolkit\",\n        \"FAT Forensics: A Python toolbox for algorithmic fairness, accountability and transparency\",\n        \"A unified approach to quantifying algorithmic unfairness: Measuring individual &group unfairness via inequality indices\",\n        \"Fairtest: Discovering unwarranted associations in data-driven applications\",\n        \"LiFT: A Scalable Framework for Measuring Fairness in ML Applications\",\n        \"Methods to estimate the between-study variance and its uncertainty in meta-analysis\",\n        \"Bias and efficiency of meta-analytic variance estimators in the random-effects model\",\n        \"Silva: Interactively Assessing Machine Learning Fairness Using Causality\",\n        \"Training individually fair ML models with sensitive subspace robustness\",\n        \"Fa* ir: A fair top-k ranking algorithm\",\n        \"Fair Top-k Ranking with multiple protected groups\",\n        \"Mitigating unwanted biases with adversarial learning\"\n    ],\n    \"reasoning\": \"Selected these 53 references because they represent foundational work and recent advances in De-biasing “bias” measurement, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}