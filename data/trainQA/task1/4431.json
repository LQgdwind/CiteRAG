{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 387 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Trustworthy AI: From Principles to Practices\n\nPaper Abstract: Abstract. The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.\n\n{retrieved_text}\n\nGenerate exactly 387 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 387\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep learning with differential privacy\",\n        \"Fraud detection system: A survey\",\n        \"Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda\",\n        \"Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)\",\n        \"Auditing black-box models for indirect influence\",\n        \"Learning optimal and fair decision trees for non-discriminative decision-making\",\n        \"QUOTIENT: two-party secure neural network training and prediction\",\n        \"Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment\",\n        \"Ethics Guidelines for Trustworthy AI\",\n        \"Introduction to software documentation\",\n        \"Threat of adversarial attacks on deep learning in computer vision: A survey\",\n        \"CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models\",\n        \"A literature survey and classifications on data deanonymisation\",\n        \"Artificial intelligence and blockchain for transparency in governance\",\n        \"Towards robust interpretability with self-explaining neural networks\",\n        \"Guidelines for human-AI interaction\",\n        \"Software engineering for machine learning: A case study\",\n        \"Concrete problems in AI safety\",\n        \"Towards explainable deep neural networks (xDNN)\",\n        \"Differential Privacy\",\n        \"FactSheets: Increasing trust in AI services through supplier’s declarations of conformity\",\n        \"A closer look at memorization in deep networks\",\n        \"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI\",\n        \"One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques\",\n        \"Assuring the machine learning lifecycle: Desiderata, methods, and challenges\",\n        \"The role of cooperation in responsible AI development\",\n        \"Reverse engineering the neural networks for rule extraction in classification problems\",\n        \"Witness of Things: Blockchain-based distributed decision record-keeping system for autonomous vehicles\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Trustworthy AI Implementation (TAII) Framework for AI Systems\",\n        \"Garbled neural networks are practical\",\n        \"Mitigating poisoning attacks on machine learning models: A data provenance based approach\",\n        \"The oracle problem in software testing: A survey\",\n        \"Rademacher and Gaussian complexities: Risk bounds and structural results\",\n        \"DevOps: A software architect’s perspective\",\n        \"A survey on privacy in social media: Identification, mitigation, and applications\",\n        \"Reconciling modern machine learning practice and the bias-variance trade-off\",\n        \"Reconciling modern machine-learning practice and the classical bias–variance trade-off\",\n        \"AI Fairness 360: An Extensible Toolkit for Detecting\",\n        \"Data statements for natural language processing: Toward mitigating system bias and enabling better science\",\n        \"CARSIM: Car-following model for simulation of traffic in normal and stop-and-go conditions\",\n        \"Fairness in criminal justice risk assessments: The state of the art\",\n        \"Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination\",\n        \"A survey of black-box adversarial attacks on computer vision models\",\n        \"Machine learning explainability via microaggregation and shallow decision trees\",\n        \"Benchmarking and Survey of Explanation Methods for Black Box Models\",\n        \"MP2ML: A mixed-protocol machine learning framework for private inference\",\n        \"HELP WANTED: An Examination of Hiring Algorithms, Equity, and Bias\",\n        \"Validation of the vehicle in the loop (vil); a milestone for the simulation of driver assistance systems\",\n        \"Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks\",\n        \"‘It Happened So Fast’: Inside a Fatal Tesla Autopilot Accident\",\n        \"“How to Measure the Reproducibility of System-oriented IR Experiments”\",\n        \"“Hardware-in-the-loop autonomous driving simulation without real-time constraints”\",\n        \"“Language models are few-shot learners”\",\n        \"“Software fairness”\",\n        \"“Toward trustworthy AI development: mechanisms for supporting verifiable claims”\",\n        \"“Gender shades: Intersectional accuracy disparities in commercial gender classification”\",\n        \"“Controlling attribute effect in linear regression”\",\n        \"“Three naive bayes approaches for discrimination-free classification”\",\n        \"“Optimized pre-processing for discrimination prevention”\",\n        \"“Trustworthy AI Inference Systems: An Industry Research View”\",\n        \"“Adversarial examples are not easily detected: Bypassing ten detection methods”\",\n        \"“Extracting training data from large language models”\",\n        \"“Towards evaluating the robustness of neural networks”\",\n        \"“Fairness in Machine Learning: A Survey”\",\n        \"“Classification with fairness constraints: A meta-algorithm with provable guarantees”\",\n        \"“Improved adversarial learning for fair classification”\",\n        \"“Adversarial attacks and defences: A survey”\",\n        \"“Deep learning for anomaly detection: A survey”\",\n        \"“Data sanitization against adversarial label contamination based on data complexity”\",\n        \"“The state of the art in enhancing trust in machine learning models with the use of visualizations”\",\n        \"“This looks like that: deep learning for interpretable image recognition”\",\n        \"“Why is my classifier discriminatory?”\",\n        \"“Correlated network data publication via differential privacy”\",\n        \"“Publishing set-valued data via differential privacy”\",\n        \"“Learning security classifiers with verified global robustness properties”\",\n        \"“Few-shot learning with meta metric learners”\",\n        \"“Retain: An interpretable predictive model for healthcare using reverse time attention mechanism”\",\n        \"“Fair prediction with disparate impact: A study of bias in recidivism prediction instruments”\",\n        \"“Data cleaning: Overview and emerging challenges”\",\n        \"“Parseval networks: Improving robustness to adversarial examples”\",\n        \"“Formal methods: State of the art and future directions”\",\n        \"“The measure and mismeasure of fairness: A critical review of fair machine learning”\",\n        \"“Algorithmic bias: A counterfactual perspective”\",\n        \"“Extracting tree-structured representations of trained networks”\",\n        \"“Casting out demons: Sanitizing training data for anomaly sensors”\",\n        \"“RobustBench: a standardized adversarial robustness benchmark”\",\n        \"“An Initial Analysis on How Software Transparency and Trust Influence each other.”\",\n        \"“Open Problems in Cooperative AI”\",\n        \"“Awful AI”, https://github.com/daviddao/awful-ai, 2020\",\n        \"“Deep nets don’t learn via memorization”\",\n        \"“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”\",\n        \"“ERASER: A Benchmark to Evaluate Rationalized NLP Models”\",\n        \"“Accountability in algorithmic decision making”\",\n        \"“COMPAS risk scales: Demonstrating accuracy equity and predictive parity”\",\n        \"“Blockchain analytics and artificial intelligence”\",\n        \"“Analysis of seam-carving-based anonymization of images against PRNU noise pattern-based source attribution”\",\n        \"“Explaining models: an empirical study of how explanations impact fairness judgment”\",\n        \"“Evading defenses to transferable adversarial examples by translation-invariant attacks”\",\n        \"“Accountability of AI Under the Law: The Role of Explanation”\",\n        \"“Explainable artificial intelligence: A survey”\",\n        \"“CARLA: An open urban driving simulator”\",\n        \"“Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems”\",\n        \"“The accuracy, fairness, and limits of predicting recidivism”\",\n        \"“Demographic bias in biometrics: A survey on an emerging challenge”\",\n        \"“Replicability is not reproducibility: nor is it good science”, 2009\",\n        \"“Differential privacy: A survey of results”\",\n        \"“Calibrating noise to sensitivity in private data analysis”\",\n        \"“The algorithmic foundations of differential privacy.”\",\n        \"“A systematic review of re-identification attacks on health data”\",\n        \"“Why does unsupervised pre-training help deep learning?”\",\n        \"“Rappor: Randomized aggregatable privacy-preserving ordinal response”\",\n        \"“A pragmatic introduction to secure multi-party computation”\",\n        \"“Adversarial vulnerability for any classifier”\",\n        \"“Certifying and removing disparate impact”\",\n        \"“Overview of CENTRE@ CLEF 2018: a first tale in the systematic reproducibility realm”\",\n        \"“Model-agnostic meta-learning for fast adaptation of deep networks”\",\n        \"“Decision tree classification with differential privacy: A survey”\",\n        \"“Establishing the rules for building trustworthy AI”\",\n        \"“A unified framework of five principles for AI in society”\",\n        \"“A comparative study of fairness-enhancing interventions in machine learning”\",\n        \"“The elements of statistical learning”\",\n        \"“Distilling a neural network into a soft decision tree”\",\n        \"“Monocular visual-inertial SLAM-based collision avoidance strategy for fail-safe UAV using fuzzy logic controllers”\",\n        \"“Fairness testing: testing software for discrimination”\",\n        \"“Privacy-preserving distributed linear regression on high-dimensional data”\",\n        \"“Datasheets for datasets”\",\n        \"“Interpretation of neural networks is fragile”\",\n        \"“Rich feature hierarchies for accurate object detection and semantic segmentation”\",\n        \"“Situ: Identifying and explaining suspicious behavior in networks”\",\n        \"“Machine learning basics”\",\n        \"“Explaining and harnessing adversarial examples”\",\n        \"“European Union regulations on algorithmic decision-making and a “right to explanation””\",\n        \"“Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning”\",\n        \"“Federated doubly stochastic kernel learning for vertically partitioned data”\",\n        \"“Towards deep neural network architectures robust to adversarial examples”\",\n        \"“A survey of methods for explaining black box models”\",\n        \"“Local rule-based explanations of black box decision systems”\",\n        \"“On reproducible AI: Towards reproducible research, open science, and digital scholarship in AI publications”\",\n        \"“State of the art: Reproducibility in artificial intelligence”\",\n        \"“DARPA’s explainable artificial intelligence (XAI) program”\",\n        \"“The ethics of AI ethics: An evaluation of guidelines”\",\n        \"“AI is sending people to jail-and getting it wrong” Accessed: 2021-02-20\",\n        \"“Equality of opportunity in supervised learning”\",\n        \"“Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption”\",\n        \"“Fedml: A research library and benchmark for federated machine learning”\",\n        \"“Momentum contrast for unsupervised visual representation learning”\",\n        \"“Anonymization of set-valued data via top-down, local generalization”\",\n        \"“Formal guarantees on the robustness of a classifier against adversarial manipulation”\",\n        \"“Benchmarking neural network robustness to common corruptions and perturbations”\",\n        \"“A survey on provenance: What for? What form? What from?”\",\n        \"“Metrics for explainable AI: Challenges and prospects”\",\n        \"“The dataset nutrition label: A framework to drive higher data quality standards”\",\n        \"“Interactive machine learning for health informatics: when do we need the human-in-the-loop?”\",\n        \"“Federated robustness propagation: Sharing adversarial robustness in federated learning”\",\n        \"“Collaborative search log sanitization: Toward differential privacy and boosted utility”\",\n        \"“Strategy and architecture of a safety concept for fully automatic and autonomous driving assistance systems”\",\n        \"“The ugly truth about ourselves and our robot creations: the problem of bias and social inequity”\",\n        \"“Model complexity of deep learning: A survey”\",\n        \"“Learning deep representation for imbalanced classification”\",\n        \"“A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability”\",\n        \"“Modelops: Cloud-based lifecycle management for reliable and trusted ai”\",\n        \"“Batch normalization: Accelerating deep network training by reducing internal covariate shift”\",\n        \"“Fae: A fairness-aware ensemble framework”\",\n        \"“Out-of-the-box reproducibility: A survey of machine learning platforms”\",\n        \"“Attention is not Explanation”\",\n        \"“Design and evaluation of a data anonymization pipeline to promote Open Science on COVID-19”\",\n        \"“Data governance: Organizing data for trustworthy Artificial Intelligence”\",\n        \"“Monitoring Misuse for Accountable’Artificial Intelligence as a Service”’\",\n        \"“Graph data anonymization, de-anonymization attacks, and de-anonymizability quantification: A survey”\",\n        \"“{{\\\\{SecGraph}}\\\\}: A Uniform and Open-source Evaluation System for Graph Data Anonymization and De-anonymization”\",\n        \"“Structural data de-anonymization: Quantification, practice, and implications”\",\n        \"“Differential privacy and machine learning: a survey and review”\",\n        \"“The global landscape of AI ethics guidelines”\",\n        \"“Artificial intelligence fairness in the context of accessibility research on intelligent systems for people who are deaf or hard of hearing”\",\n        \"“Advances and open problems in federated learning”\",\n        \"“On secret sharing systems”\",\n        \"“Requirements for trustworthy artificial intelligence–a review”\",\n        \"“Generalization in deep learning”\",\n        \"“Examples are not enough, learn to criticize! criticism for interpretability”\",\n        \"“Introduction to interpretable machine learning”\",\n        \"“Fairness through computationally-bounded awareness”\",\n        \"“Inherent trade-offs in the fair determination of risk scores”\",\n        \"“Crypten: Secure multi-party computation meets machine learning”\",\n        \"“The Sanction of Authority: Promoting Public Trust in AI”\",\n        \"“Siamese neural networks for one-shot image recognition”\",\n        \"“Racial disparities in automated speech recognition”\",\n        \"“Design and use paradigms for gazebo, an open-source multi-robot simulator”\",\n        \"“Understanding black-box predictions via influence functions”\",\n        \"“Universal litmus patterns: Revealing backdoor attacks in cnns”\",\n        \"“Adaptive sensitive reweighting to mitigate bias in fairness-aware classification”\",\n        \"“A simple weight decay can improve generalization”\",\n        \"“Causal inference”\",\n        \"“Tell me more? The effects of mental model soundness on personalizing an intelligent agent”\",\n        \"“Trustworthy AI in the Age of Pervasive Computing and Big Data”\",\n        \"“Cryptflow: Secure tensorflow inference”\",\n        \"“Adversarial examples in the physical world”\",\n        \"“Interpretable & explorable approximations of black box models”\",\n        \"“Technology readiness levels for machine learning systems”\",\n        \"“Certified robustness to adversarial examples with differential privacy”\",\n        \"“ConvLab: Multi-Domain End-to-End Dialog System Platform”\",\n        \"“Globally-robust neural networks”\",\n        \"“Software transparency”\",\n        \"“Fair, transparent, and accountable algorithmic decision-making processes”\",\n        \"“Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector”\",\n        \"“An Ontology for Standardising Trustworthy AI”\",\n        \"“A general retraining framework for scalable adversarial classification”\",\n        \"“Learning to optimize”\",\n        \"“Backdoor learning: A survey”\",\n        \"“Secure multiparty computation (MPC)”\",\n        \"“A proof of security of Yao’s protocol for two-party computation”\",\n        \"“The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.”\",\n        \"“Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform”\",\n        \"“Trustworthy AI: A Computational Perspective”\",\n        \"“Fine-pruning: Defending against backdooring attacks on deep neural networks”\",\n        \"“Towards identity anonymization on graphs”\",\n        \"“STGAN: A unified selective transfer network for arbitrary image attribute editing”\",\n        \"“A secure federated transfer learning framework”\",\n        \"“Federated learning for open banking”\",\n        \"“Fully convolutional networks for semantic segmentation”\",\n        \"“A Unified Approach to Interpreting Model Predictions”\",\n        \"“Deepgauge: Multi-granularity testing criteria for deep learning systems”\",\n        \"“Adversarial Machine Learning in Image Classification: A Survey Toward the Defender’s Perspective”\",\n        \"“Co-designing checklists to understand organizational challenges and opportunities around fairness in AI”\",\n        \"“Towards Deep Learning Models Resistant to Adversarial Attacks”\",\n        \"“Fail-safe motion planning of autonomous vehicles”\",\n        \"“Adversarial robustness against the union of multiple perturbation models”\",\n        \"“Anonymization techniques for privacy preserving data publishing: A comprehensive survey”\",\n        \"“Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?”\",\n        \"“Human Language Understanding & Reasoning”\",\n        \"“Software Testing for Machine Learning”\",\n        \"“Worst-case background knowledge for privacy-preserving data publishing”\",\n        \"“From softmax to sparsemax: A sparse model of attention and multi-label classification”\",\n        \"“HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection”\",\n        \"“Ciagan: Conditional identity anonymization generative adversarial networks”\",\n        \"“Intel® software guard extensions (intel® sgx) support for dynamic memory management inside an enclave”\",\n        \"“Communication-efficient learning of deep networks from decentralized data”\",\n        \"“A survey on bias and fairness in machine learning”\",\n        \"“On detecting adversarial perturbations”\",\n        \"“How to play any mental game”\",\n        \"“Explanation in artificial intelligence: Insights from the social sciences”\",\n        \"“Model cards for model reporting”\",\n        \"“Secureml: A system for scalable privacy-preserving machine learning”\",\n        \"“A human-grounded evaluation benchmark for local explanations of machine learning”\",\n        \"“A multidisciplinary survey and framework for design and evaluation of explainable AI systems”\",\n        \"“Interpretable machine learning”\",\n        \"“Are ‘pseudonymised’data always personal data? Implications of the GDPR for administrative data research in the UK”\",\n        \"“Increased autonomy for quadrocopter systems: trajectory generation, fail-safe strategies and state estimation”, 2016\",\n        \"“Hiding the presence of individuals from shared databases”\",\n        \"“OpenAI Charter” Accessed: 2021-02-20, https://openai.com/charter/, 2018\",\n        \"“A survey on transfer learning”\",\n        \"“Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training”\",\n        \"“Deep learning for anomaly detection: A review”\",\n        \"“Label sanitization against label flipping poisoning attacks”\",\n        \"“Causal inference in statistics: An overview”\",\n        \"“Deepxplore: Automated whitebox testing of deep learning systems”\",\n        \"“Argos: An advanced in-vehicle data recorder on a massively sensorized vehicle for car driver behavior experimentation”\",\n        \"“Accuracy-constrained privacy-preserving access control mechanism for relational data”\",\n        \"“Improving reproducibility in machine learning research: a report from the NeurIPS 2019 reproducibility program”\",\n        \"“Demystifying arm trustzone: A comprehensive survey”\",\n        \"“Towards evaluating and eliciting high-quality documentation for intelligent systems”\",\n        \"“On fairness and calibration”\",\n        \"“Manipulating and measuring model interpretability”\",\n        \"“Failing loudly: An empirical study of methods for detecting dataset shift”\",\n        \"“Improving language understanding by generative pre-training”, 2018\",\n        \"“Adversarial training can hurt generalization”\",\n        \"“Certified Defenses against Adversarial Examples”\",\n        \"“About ml: Annotation and benchmarking on understanding and transparency of machine learning lifecycles”\",\n        \"“Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing”\",\n        \"“Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices”\",\n        \"“Presentation attack detection methods for face recognition systems: A comprehensive survey”\",\n        \"“CrypTFlow2: Practical 2-party secure inference”\",\n        \"“Algorithmic impact assessments: A practical framework for public agency accountability”\",\n        \"“LoPub: high-dimensional crowdsourced data publication with local differential privacy”\",\n        \"“\\\" Why should i trust you?\\\" Explaining the predictions of any classifier”\",\n        \"“Anchors: High-precision model-agnostic explanations”\",\n        \"“Beyond Accuracy: Behavioral Testing of NLP Models with CheckList”\",\n        \"“The future of digital health with federated learning”\",\n        \"“Estimating the success of re-identifications in incomplete datasets using generative models”\",\n        \"“FR-Train: A mutual information-based approach to fair and robust training”\",\n        \"“Explainability in human–agent systems”\",\n        \"“Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients”\",\n        \"“Trusted execution environment: what it is, and what it is not”\",\n        \"“Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression”\",\n        \"“Auditing algorithms: Research methods for detecting discrimination on internet platforms”\",\n        \"“Meta-learning with memory-augmented neural networks”\",\n        \"“Applying Transparency in Artificial Intelligence based Personalization Systems”\",\n        \"“Automatically tracking metadata and provenance of machine learning experiments”\",\n        \"“AI Ethics in the Public, Private, and NGO Sectors: A Review of a Global Document Collection”\",\n        \"“Intervention user interfaces: a new interaction paradigm for automated systems”\",\n        \"“Hidden technical debt in machine learning systems”\",\n        \"“Grad-cam: Visual explanations from deep networks via gradient-based localization”\",\n        \"“Towards verified artificial intelligence”\",\n        \"“Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data”\",\n        \"“Secure multi-pArty computation grid LOgistic REgression (SMAC-GLORE)”\",\n        \"“Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy Human-Centered AI systems”\",\n        \"“Learning important features through propagating activation differences”\",\n        \"“Opportunities and challenges in deep learning adversarial robustness: A survey”\",\n        \"“Deep inside convolutional networks: Visualising image classification models and saliency maps”\",\n        \"“Decision provenance: Harnessing data flow for accountable systems”\",\n        \"“Fairface challenge at eccv 2020: Analyzing bias in face recognition”\",\n        \"“Prototypical networks for few-shot learning”\",\n        \"“Dropout: a simple way to prevent neural networks from overfitting”\",\n        \"“Trust and artificial intelligence”\",\n        \"“Explanations considered harmful? user interactions with machine learning systems”\",\n        \"“Is Robustness the Cost of Accuracy?–A Comprehensive Study on the Robustness of 18 Deep Image Classification Models”\",\n        \"“VL-BERT: Pre-training of Generic Visual-Linguistic Representations”\",\n        \"“Axiomatic attribution for deep networks”\",\n        \"“Auditing machine learning algorithms”\",\n        \"“Data cleaning for accurate, fair, and robust models: A big data-AI integration approach”\",\n        \"“Tree space prototypes: Another look at making tree ensembles interpretable”\",\n        \"“Night-time semantic segmentation with a large real dataset”\",\n        \"“Privacy-preserving anonymization of set-valued data”\",\n        \"“Cost-sensitive learning methods for imbalanced data”\",\n        \"“Trustworthy artificial intelligence”\",\n        \"“Scenario-based simulation environment for assistance systems”\",\n        \"“Mujoco: A physics engine for model-based control”\",\n        \"“Improving robustness of {{\\\\{ML}}\\\\} classifiers against realizable evasion attacks using conserved features”\",\n        \"“Accessibility in Chatbots: The State of the Art in Favor of Users with Visual Impairment”\",\n        \"“Ensemble Adversarial Training: Attacks and Defenses”\",\n        \"“Fairtest: Discovering unwarranted associations in data-driven applications”\",\n        \"“Stealing Machine Learning Models via Prediction {{\\\\{APIs}}\\\\}”\",\n        \"“CryptoSPN: Privacy-preserving Sum-Product Network Inference”\",\n        \"“Robustness May Be at Odds with Accuracy”\",\n        \"“Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents”\",\n        \"“The ethics of information transparency”\",\n        \"“Top 10 principles for ethical artificial intelligence”\",\n        \"“A Review of Formal Methods applied to Machine Learning”\",\n        \"“Meta-learning: A survey”\",\n        \"“The nature of statistical learning theory”\",\n        \"“Trustworthy machine learning and artificial intelligence”\",\n        \"“Fairness definitions explained”\",\n        \"“Adversarial machine learning”\",\n        \"“Counterfactual explanations without opening the black box: Automated decisions and the GDPR”\",\n        \"“Neural cleanse: Identifying and mitigating backdoor attacks in neural networks”\",\n        \"“Generalizing to Unseen Domains: A Survey on Domain Generalization”\",\n        \"“Real-time and spatio-temporal crowd-sourced social network data publishing with differential privacy”\",\n        \"“Explainable reasoning over knowledge graphs for recommendation”\",\n        \"“Generalizing from a few examples: A survey on few-shot learning”\",\n        \"“Measurement of the cognitive functional complexity of software”\",\n        \"“On the Convergence and Robustness of Adversarial Training.”\",\n        \"“Federated learning with differential privacy: Algorithms and performance analysis”\",\n        \"“A survey of transfer learning”\",\n        \"“The challenge of crafting intelligible intelligence”\",\n        \"“Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach”\",\n        \"“Trustworthy AI Development Guidelines for Human System Interaction”\",\n        \"“Attention is not not Explanation”\",\n        \"“Teach Me to Explain: A Review of Datasets for Explainable NLP”\",\n        \"“Building and auditing fair algorithms: A case study in candidate screening”\",\n        \"“Trustworthy ai”\",\n        \"“Defending against physically realizable attacks on image classification”\",\n        \"“Torcs, the open racing car simulator”\",\n        \"“Enabling realistic health data re-identification risk assessment through adversarial modeling”\",\n        \"“To be robust or to be fair: Towards fairness in adversarial training”\",\n        \"“Robustness and generalization”\",\n        \"“Show, attend and tell: Neural image caption generation with visual attention”\",\n        \"“Security of Neural Networks from Hardware Perspective: A Survey and Beyond”\",\n        \"“Federated machine learning: Concept and applications”\",\n        \"“HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering”\",\n        \"“Protocols for secure computations”\",\n        \"“Seco: Exploring sequence supervision for unsupervised representation learning”\",\n        \"“The smart black box: A value-driven high-bandwidth automotive event data recorder”\",\n        \"“On early stopping in gradient descent learning”\",\n        \"“Diverse Few-Shot Text Classification with Multiple Metrics”\",\n        \"“Protecting sensitive labels in social network data anonymization”\",\n        \"“Adversarial examples: Attacks and defenses for deep learning”\",\n        \"“Accelerating the Machine Learning Lifecycle with MLflow.”\",\n        \"“Visualizing and understanding convolutional networks”\",\n        \"“Learning fair representations”\",\n        \"“Understanding deep learning requires rethinking generalization”\",\n        \"“Theoretically principled trade-off between robustness and accuracy”\",\n        \"“Efficient neural network robustness certification with general activation functions”\",\n        \"“Towards stable and efficient training of verifiably robust neural networks”\",\n        \"“Machine learning testing: Survey, landscapes and horizons”\",\n        \"“Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems”\",\n        \"“Interpretable convolutional neural networks”\",\n        \"“Interpreting cnns via decision trees”\",\n        \"“Anonymizing query logs by differential privacy”\",\n        \"“A scalable two-phase top-down specialization approach for data anonymization using mapreduce on cloud”\",\n        \"“Secure multi-party computation: theory, practice and applications”\",\n        \"“A Tutorial on Fairness in Machine Learning”, https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb\",\n        \"“A brief survey on anonymization techniques for privacy preserving publishing of social network data”\",\n        \"“Learning deep features for discriminative localization”\",\n        \"“Domain generalization: A survey”\",\n        \"“Interpreting models via single tree approximation”\",\n        \"“Extracting symbolic rules from trained neural network ensembles”\",\n        \"“An overview of concept drift applications”\"\n    ],\n    \"reasoning\": \"Selected these 387 references because they represent foundational work and recent advances in Trustworthy AI: From Principles to Practices, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}