{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 339 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey\n\nPaper Abstract: Abstract The rapid development of Artificial Intelligence (AI) has revolutionized numerous fields, with large language models (LLMs) and computer vision (CV) systems driving advancements in natural language understanding and visual processing, respectively. The convergence of these technologies has catalyzed the rise of multimodal AI, enabling richer, cross-modal understanding that spans text, vision, audio, and video modalities. Multimodal large language models (MLLMs), in particular, have emerged as a powerful framework, demonstrating impressive capabilities in tasks like image-text generation, visual question answering, and cross-modal retrieval. Despite these advancements, the complexity and scale of MLLMs introduce significant challenges in interpretability and explainability, essential for establishing transparency, trustworthiness, and reliability in high-stakes applications. This paper provides a comprehensive survey on the interpretability and explainability of MLLMs, proposing a novel framework that categorizes existing research across three perspectives: (I) Data, (II) Model, (III) Training & Inference. We systematically analyze interpretability from token-level to embedding-level representations, assess approaches related to both architecture analysis and design, and explore training and inference strategies that enhance transparency. By comparing various methodologies, we identify their strengths and limitations and propose future research directions to address unresolved challenges in multimodal explainability. This survey offers a foundational resource for advancing interpretability and transparency in MLLMs, guiding researchers and practitioners toward developing more accountable and robust multimodal AI systems.\n\n{retrieved_text}\n\nGenerate exactly 339 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 339\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A survey of large language models\",\n        \"Deep learning for computer vision: A brief review\",\n        \"A survey on multimodal large language models for autonomous driving\",\n        \"Efficient multimodal large language models: A survey\",\n        \"The (r) evolution of multimodal large language models: A survey\",\n        \"Mm-llms: Recent advances in multimodal large language models\",\n        \"Visual prompting in multimodal large language models: A survey\",\n        \"Large multimodal agents: A survey\",\n        \"Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection\",\n        \"Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web\",\n        \"Reefknot: A comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models\",\n        \"A survey on multimodal large language models\",\n        \"Multimodal large language models: A survey\",\n        \"Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook\",\n        \"How to bridge the gap between modalities: A comprehensive survey on multimodal large language model\",\n        \"Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models\",\n        \"Mitigating modality prior-induced hallucinations in multimodal large language models via deciphering attention causality\",\n        \"Unifying multimodal transformer for bi-directional image and text generation\",\n        \"Generating images with multimodal language models\",\n        \"Instruct-imagen: Image generation with multi-modal instruction\",\n        \"Mme: A comprehensive evaluation benchmark for multimodal large language models\",\n        \"Mmbench: Is your multi-modal model an all-around player?\",\n        \"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\",\n        \"Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts\",\n        \"Unveiling the tapestry of consistency in large vision-language models\",\n        \"Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension\",\n        \"Seed-bench-2: Benchmarking multimodal large language models\",\n        \"Seed-bench: Benchmarking multimodal llms with generative comprehension\",\n        \"A comprehensive survey on cross-modal retrieval\",\n        \"Deep supervised cross-modal retrieval\",\n        \"Probabilistic embeddings for cross-modal retrieval\",\n        \"Multimodal video sentiment analysis using deep learning approaches, a survey\",\n        \"Fakeavceleb: A novel audio-video multimodal deepfake dataset\",\n        \"End-to-end generative pretraining for multimodal video captioning\",\n        \"Videoagent: A memory-augmented multimodal agent for video understanding\",\n        \"Mdmmt: Multidomain multimodal transformer for video retrieval\",\n        \"End-to-end referring video object segmentation with multimodal transformers\",\n        \"Univl: A unified video and language pre-training model for multimodal understanding and generation\",\n        \"Evolution and prospects of foundation models: From large language models to large multimodal models.\",\n        \"A comprehensive review of multimodal large language models: Performance and challenges across different tasks\",\n        \"Georeasoner: Reasoning on geospatially grounded context for natural language understanding\",\n        \"“Chatgpt for shaping the future of dentistry: the potential of multi-modal large language model\",\n        \"“mplug-docowl: Modularized multimodal large language model for document understanding\",\n        \"“A survey on deep multimodal learning for computer vision: advances\",\n        \"“A survey of multimodal hybrid deep learning for computer vision: Architectures\",\n        \"“Multimodal learning with transformers: A survey\",\n        \"“Video understanding with large language models: A survey\",\n        \"“Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving\",\n        \"“Cog-ga: A large language models-based generative agent for vision-language navigation in continuous environments\",\n        \"“Interpretable bilingual multimodal large language model for diverse biomedical tasks\",\n        \"“Mcan: multimodal causal adversarial networks for dynamic effective connectivity learning from fmri and eeg data\",\n        \"“A comprehensive survey of large language models and multimodal large language models in medicine\",\n        \"“Mmro: Are multimodal llms eligible as the brain for in-home robotics?” arXiv preprint arXiv:2406.19693\",\n        \"“Toward proactive human–robot collaborative assembly: A multimodal transfer-learning-enabled action prediction approach\",\n        \"“Progress and prospects of multimodal fusion methods in physical human–robot interaction: A review\",\n        \"“Multimodal sensors and ml-based data fusion for advanced robots\",\n        \"“Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration\",\n        \"K. Gopalakrishnan\",\n        \"“Enhanced decision-making through multimodal training\",\n        \"“Deep multimodal data fusion\",\n        \"N. Díaz-Rodríguez\",\n        \"“Opportunities and challenges in explainable artificial intelligence (xai): A survey\",\n        \"“Peeking inside the black-box: a survey on explainable artificial intelligence (xai)\",\n        \"“Towards a rigorous science of interpretable machine learning\",\n        \"“Techniques for interpretable machine learning\",\n        \"“Visualizing and understanding convolutional networks\",\n        \"“Interpretable convolutional neural networks\",\n        \"“Transformer interpretability beyond attention visualization\",\n        \"“Explainability for large language models: A survey\",\n        \"“From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai\",\n        \"“The mythos of model interpretability: In machine learning\",\n        \"“Interpreting interpretability: understanding data scientists’ use of interpretability tools for machine learning\",\n        \"“Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling\",\n        \"“Designing fuzzy inference systems from data: An interpretability-oriented review\",\n        \"“Shapley explainability on the data manifold\",\n        \"P.-J. Kindermans\",\n        \"H. Corrada Bravo\",\n        \"“Interpretability of deep learning models: A survey of results\",\n        \"“A survey on neural network interpretability\",\n        \"“On interpretability of artificial neural networks: A survey\",\n        \"V. Papastefanopoulos\",\n        \"“Interpretability and explainability: A machine learning zoo mini-tour\",\n        \"“Explainable artificial intelligence: A survey\",\n        \"“Explaining explanations: An overview of interpretability of machine learning\",\n        \"“Post-hoc interpretability for neural nlp: A survey\",\n        \"“Interpretability of machine learning models and representations: an introduction\",\n        \"“Infogan: Interpretable representation learning by information maximizing generative adversarial nets\",\n        \"A.-K. Dombrowski et al\",\n        \"“Towards tracing trustworthiness dynamics: Revisiting pre-training period of large language models\",\n        \"“Visual interpretability for deep learning: a survey\",\n        \"“Neuron-level interpretation of deep nlp models: A survey\",\n        \"“Improving neuron-level interpretability with white-box language models\",\n        \"“Dean: Deactivating the coupled neurons to mitigate fairness-privacy conflicts in large language models\",\n        \"“Identifying semantic induction heads to understand in-context learning\",\n        \"“Model-agnostic interpretability of machine learning\",\n        \"“Hybridization of model-specific and model-agnostic methods for interpretability of neural network predictions: Application to a power plant\",\n        \"“Model agnostic interpretability\",\n        \"“Model agnostic interpretability for multiple instance learning\",\n        \"“A survey on benchmarks of multimodal large language models\",\n        \"“A survey on vision transformer\",\n        \"“Explainability of vision transformers: A comprehensive review and new perspectives\",\n        \"“Multimodal explanations: Justifying decisions and pointing to the evidence\",\n        \"“Mitigating object hallucinations in large vision-language models through visual contrastive decoding\",\n        \"“Causal disentanglement of multimodal data\",\n        \"J. Teichert-Kluge\",\n        \"“High-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning\",\n        \"“A survey on evaluation of multimodal large language models\",\n        \"“Vista: A visual and textual attention dataset for interpreting multimodal models\",\n        \"“Benchlmm: Benchmarking cross-style visual capability of large multimodal models\",\n        \"“Exploring response uncertainty in mllms: An empirical evaluation under misleading scenarios\",\n        \"“Benchmarking trustworthiness of multimodal large language models: A comprehensive study\",\n        \"“Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering\",\n        \"J. Watson-Daniels\",\n        \"“What are we measuring when we evaluate large vision-language models? an analysis of latent factors and biases\",\n        \"“A study on multimodal and interactive explanations for visual question answering\",\n        \"“A deep multi-modal explanation model for zero-shot learning\",\n        \"“Learn to explain: Multimodal reasoning via thought chains for science question answering\",\n        \"“A multimodal automated interpretability agent\",\n        \"“Digital forms for all: A holistic multimodal large language model agent for health data entry\",\n        \"“Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module\",\n        \"“Multimodal explanations by predicting counterfactuality in videos\",\n        \"“Discovering the real association: Multimodal causal reasoning in video question answering\",\n        \"“Large language models are temporal and causal reasoners for video question answering\",\n        \"“Holmes-vad: Towards unbiased and explainable video anomaly detection via multi-modal llm\",\n        \"“Tv-trees: Multimodal entailment trees for neuro-symbolic video reasoning\",\n        \"“Multi-modal probabilistic prediction of interactive behavior via an interpretable model\",\n        \"“Drivegpt4: Interpretable end-to-end autonomous driving via large language model\",\n        \"“Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition\",\n        \"“Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph\",\n        \"“Musiclime: Explainable multimodal music understanding\",\n        \"“Enhancing explainability in multimodal large language models using ontological context\",\n        \"“Interpreting clip’s image representation via text-based decomposition\",\n        \"“Towards interpreting visual information processing in vision-language models\",\n        \"“From redundancy to relevance: Enhancing explainability in multimodal large language models\",\n        \"“An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models\",\n        \"“Visual explanations of image-text representations via multi-modal information bottleneck attribution\",\n        \"“Deco: Decoupling token compression from semantic abstraction in multimodal large language models\",\n        \"“Vl-match: Enhancing vision-language pretraining with token-level and instance-level matching\",\n        \"“The first to know: How token distributions reveal hidden knowledge in large vision-language models?” arXiv preprint arXiv:2403.09037\",\n        \"“Unified lexical representation for interpretable visual-language alignment\",\n        \"“Plausible may not be faithful: Probing object hallucination in vision-language pre-training\",\n        \"“Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation\",\n        \"S. Radhakrishnan\",\n        \"“Probing image-language transformers for verb understanding\",\n        \"“Text-to-concept (and back) via cross-model alignment\",\n        \"“Using sparse semantic embeddings learned from multimodal text and image data to model human conceptual knowledge\",\n        \"“Stair: learning sparse text and image representation in grounded tokens\",\n        \"“Sharcs: Shared concept space for explainable multimodal learning\",\n        \"“Interpreting clip with sparse linear concept embeddings (splice)\",\n        \"“Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers\",\n        \"“Freebind: Free lunch in unified multimodal space via knowledge fusion\",\n        \"“A concept-based explainability framework for large multimodal models\",\n        \"“From text to pixels: Enhancing user understanding through text-to-image model explanations\",\n        \"“Are vision-language transformers learning multimodal representations? a probing perspective\",\n        \"“Investigation of explainability techniques for multimodal transformers\",\n        \"“Multimodal neurons in artificial neural networks\",\n        \"“Interpreting the second-order effects of neurons in clip\",\n        \"“Finding and editing multi-modal neurons in pre-trained transformer\",\n        \"“Mmneuron: Discovering neuron-level domain-specific interpretation in multimodal large language model\",\n        \"“Miner: Mining the underlying pattern of modality-specific neurons in multimodal large language models\",\n        \"“Behind the scene: Revealing the secrets of pre-trained vision-and-language models\",\n        \"“How and where does clip process negation?” arXiv preprint arXiv:2407.10488\",\n        \"“Bridging vision and language encoders: Parameter-efficient tuning for referring image segmentation\",\n        \"“Probing multimodal large language models for global and local semantic representation\",\n        \"“Multi-task learning of hierarchical vision-language representation\",\n        \"R. Salakhutdinov\",\n        \"“Towards transparent ai systems: Interpreting visual question answering models\",\n        \"“Faithful multimodal explanation for visual question answering\",\n        \"“Vale: A multimodal visual and language explanation framework for image classifiers using explainable ai and language models\",\n        \"A. Bhiwandiwalla\",\n        \"“Diffusion explainer: Visual explanation for text-to-image stable diffusion\",\n        \"“Law of vision representation in mllms\",\n        \"“Nbdt: Neural-backed decision trees\",\n        \"C. Callison-Burch\",\n        \"“Trace: Temporal grounding video llm via causal event modeling\",\n        \"“Multimodal graph causal embedding for multimedia-based recommendation\",\n        \"interpretable modular networks\",\n        \"“Scaling vision-language models with sparse mixture of experts\",\n        \"“Imkga-sm: Interpretable multimodal knowledge graph answer prediction via sequence modeling\",\n        \"“Pre-trained vision-language models learn discoverable visual concepts\",\n        \"“Disttrain: Addressing model and data heterogeneity with disaggregated training for multimodal large language models\",\n        \"“Aligning large multimodal models with factually augmented rlhf\",\n        \"“Vigor: Improving visual grounding of large vision language models with fine-grained reward modeling\",\n        \"“Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback\",\n        \"J. Benois-Pineau\",\n        \"“Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization\",\n        \"T.-S. Chua et al\",\n        \"“Aligning modalities in vision large language models via preference fine-tuning\",\n        \"“A survey on hallucination in large vision-language models\",\n        \"“Hallucination of multimodal large language models: A survey\",\n        \"“Multimodal chain-of-thought reasoning in language models\",\n        \"“Chain of thought prompt tuning in vision language models\",\n        \"“Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals\",\n        \"“Visualizing deep neural network decisions: Prediction difference analysis\",\n        \"“Intriguing properties of neural networks\",\n        \"“Rise: Randomized input sampling for explanation of black-box models\",\n        \"“Understanding deep networks via extremal perturbations and smooth masks\",\n        \"“Learning to explain with complemental examples\",\n        \"“A holistic approach to unifying automatic concept extraction and concept importance estimation\",\n        \"“Connectivity-contrastive learning: Combining causal discovery and representation learning for multimodal data\",\n        \"“Diffusion-ts: Interpretable diffusion for general time series generation\",\n        \"“Understanding diffusion models: A unified perspective\",\n        \"“Diffusion models: A comprehensive survey of methods and applications\",\n        \"“Diffusion models for counterfactual explanations\",\n        \"“What the daam: Interpreting stable diffusion using cross attention\",\n        \"“Interpretable diffusion via information decomposition\",\n        \"“Coco-o: A benchmark for object detectors under natural distribution shifts\",\n        \"“Defoiling foiled image captions\",\n        \"“Designing and interpreting probes with control tasks\",\n        \"“Find: A function description benchmark for evaluating interpretability methods\",\n        \"“Explainable multi-task learning for multi-modality biological data analysis\",\n        \"“An integrated multimodal model of alcohol use disorder generated by data-driven causal discovery analysis\",\n        \"“Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models\",\n        \"“Multimodal attention merging for improved speech recognition and audio event classification\",\n        \"“Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition\",\n        \"“Toward interpretable music tagging with self-attention\",\n        \"“Perceptual musical features for interpretable audio tagging\",\n        \"“Leveraging pre-trained autoencoders for interpretable prototype learning of music audio\",\n        \"“Concept-based techniques for” musicologist-friendly” explanations in a deep music classifier\",\n        \"“Dopra: Decoding over-accumulation penalization and re-allocation in specific weighting layer\",\n        \"“Explaining generative diffusion models via visual analysis for interpretable decision-making process\",\n        \"“The tree of diffusion life: Evolutionary embeddings to understand the generation process of diffusion models\",\n        \"“Contrastive visual semantic pretraining magnifies the semantics of natural language representations\",\n        \"“Probing multimodal embeddings for linguistic properties: the visual-semantic case\",\n        \"“Robust multimodal models have outlier features and encode more concepts\",\n        \"“Diffusion models already have a semantic latent space\",\n        \"“Interpretable basis decomposition for visual explanation\",\n        \"“Visualizing and understanding patch interactions in vision transformer\",\n        \"“Vit-net: Interpretable vision transformers with neural tree decoder\",\n        \"“Interpretability-aware vision transformer\",\n        \"“Dynamicvit: Efficient vision transformers with dynamic token sparsification\",\n        \"“Understanding intermediate layers using linear classifier probes\",\n        \"F. Viegas et al\",\n        \"“The better angels of machine personality: How personality relates to llm safety\",\n        \"“Reef: Representation encoding fingerprints for large language models\",\n        \"“A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity\",\n        \"“Axiomatic attribution for deep networks\",\n        \"“Network dissection: Quantifying interpretability of deep visual representations\",\n        \"“Interpreting deep visual representations via network dissection\",\n        \"“Where and what? examining interpretable disentangled representations\",\n        \"“Intrinsic probing through dimension selection\",\n        \"“Towards interpretable sparse graph representation learning with laplacian pooling\",\n        \"“Gan dissection: Visualizing and understanding generative adversarial networks\",\n        \"“Understanding the role of individual units in a deep neural network\",\n        \"“Natural language descriptions of deep visual features\",\n        \"“Knowledge neurons in pretrained transformers\",\n        \"“Clip-dissect: Automatic description of neuron representations in deep vision networks\",\n        \"“Locating and editing factual associations in gpt\",\n        \"“Hint: Hierarchical neuron concept explainer\",\n        \"“Rosetta neurons: Mining the common units in a model zoo\",\n        \"“Language models can explain neurons in language models\",\n        \"“Cones: Concept neurons in diffusion models for customized generation\",\n        \"“Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons\",\n        \"“Scaling and evaluating sparse autoencoders\",\n        \"“Finding nemo: Localizing neurons responsible for memorization in diffusion models\",\n        \"“High-low frequency detectors\",\n        \"“Curve detectors\",\n        \"“Zoom in: An introduction to circuits\",\n        \"“Causal analysis of syntactic agreement neurons in multilingual language models\",\n        \"“Multimodal neurons in pretrained text-only transformers\",\n        \"“Language-specific neurons: The key to multilingual capabilities in large language models\",\n        \"“On the multilingual ability of decoder-based pre-trained language models: Finding and controlling language-specific neurons\",\n        \"“Universal neurons in gpt2 language models\",\n        \"“Measuring and interpreting neuronal correlations\",\n        \"“Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"“Understanding deep image representations by inverting them\",\n        \"“Inverting convolutional networks with convolutional networks\",\n        \"“On the relationship between self-attention and convolutional layers\",\n        \"“Augmenting self-attention with persistent memory\",\n        \"“Transformer feed-forward layers are key-value memories\",\n        \"“Are sixteen heads really better than one?” Advances in neural information processing systems\",\n        \"“Analyzing multi-head self-attention: Specialized heads do the heavy lifting\",\n        \"“What does bert look at? an analysis of bert’s attention\",\n        \"“Revealing the dark secrets of bert\",\n        \"“Do attention heads in bert track syntactic dependencies?” arXiv preprint arXiv:1911.12246\",\n        \"“Freeu: Free lunch in diffusion u-net\",\n        \"“Visual concept connectome (vcc): Open world concept discovery and their interlayer connections in deep models\",\n        \"“How does bert answer questions? a layer-wise analysis of transformer representations\",\n        \"“Bert rediscovers the classical nlp pipeline\",\n        \"“Towards vision-language mechanistic interpretability: A causal tracing tool for blip\",\n        \"“Unraveling the temporal dynamics of the unet in diffusion models\",\n        \"“” why should i trust you?” explaining the predictions of any classifier\",\n        \"“Learning important features through propagating activation differences\",\n        \"“A unified approach to interpreting model predictions\",\n        \"“Learning deep features for discriminative localization\",\n        \"“Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"“U-cam: Visual explanation using uncertainty based class activation maps\",\n        \"“Score-cam: Score-weighted visual explanations for convolutional neural networks\",\n        \"“gscorecam: What objects is clip looking at?” in Proceedings of the Asian Conference on Computer Vision\",\n        \"“Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n        \"“Layer-wise relevance propagation for neural networks with local renormalization layers\",\n        \"“Explaining nonlinear classification decisions with deep taylor decomposition\",\n        \"“Visualizing deep networks by optimizing with integrated gradients.” in CVPR workshops\",\n        \"“Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers\",\n        \"“The building blocks of interpretability\",\n        \"“On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n        \"“The shapley taylor interaction index\",\n        \"“Explaining explanations: Axiomatic feature interactions for deep networks\",\n        \"“Faith-shap: The faithful shapley interaction index\",\n        \"“Defining and quantifying the emergence of sparse concepts in dnns\",\n        \"“Does a neural network really encode symbolic concepts?” in International conference on machine learning. PMLR\",\n        \"“Where we have arrived in proving the emergence of sparse interaction primitives in dnns\",\n        \"“Can we faithfully represent masked states to compute shapley values on a dnn?” arXiv preprint arXiv:2105.10719\",\n        \"“Defining and extracting generalizable interaction primitives from dnns\",\n        \"“Layerwise change of knowledge in neural networks\",\n        \"“Ganspace: Discovering interpretable gan controls\",\n        \"“Where we have arrived in proving the emergence of sparse symbolic concepts in ai models\",\n        \"“A unified approach to interpreting and boosting adversarial transferability\",\n        \"“Towards a unified game-theoretic view of adversarial perturbations and robustness\",\n        \"“Explaining generalization power of a dnn using interactive concepts\",\n        \"“Towards the difficulty for a deep neural network to learn concepts of different complexities\",\n        \"“Discovering and explaining the representation bottleneck of dnns\",\n        \"“Bayesian neural networks avoid encoding complex and perturbation-sensitive concepts\",\n        \"“Towards the dynamics of a dnn learning symbolic interactions\",\n        \"“Two-phase dynamics of interactions explains the starting point of a dnn learning over-fitted features\",\n        \"“Unifying fourteen post-hoc attribution methods with taylor interactions\",\n        \"“The importance of being recurrent for modeling hierarchical structure\",\n        \"“Interpretable visual understanding with cognitive attention network\",\n        \"“Improving the interpretability of deep neural networks with knowledge distillation\",\n        \"“Leveraging sparse linear layers for debuggable deep networks\",\n        \"“Concept bottleneck models\",\n        \"“Post-hoc concept bottleneck models\",\n        \"“Llcp: Learning latent causal processes for reasoning-based video question answer\",\n        \"“Steering llms towards unbiased responses: A causality-guided debiasing framework\",\n        \"“Revealing multimodal contrastive representation learning through latent partial causal models\",\n        \"“Do vision-language pretrained models learn composable primitive concepts?” arXiv preprint arXiv:2203.17271\",\n        \"“Lima: Less is more for alignment\",\n        \"“Silkie: Preference distillation for large visual language models\",\n        \"“Smoothgrad: removing noise by adding noise\",\n        \"“A survey on hallucination in large language models: Principles\",\n        \"“A survey of hallucination in large foundation models\",\n        \"“Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models\",\n        \"“Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models\",\n        \"“Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning\",\n        \"“M3cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought\",\n        \"“Visual chain of thought: bridging logical gaps with multimodal infillings\",\n        \"“3vl: using trees to teach vision & language models compositional concepts\",\n        \"“A survey on in-context learning\",\n        \"“Multimodal contrastive in-context learning\"\n    ],\n    \"reasoning\": \"Selected these 339 references because they represent foundational work and recent advances in Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}