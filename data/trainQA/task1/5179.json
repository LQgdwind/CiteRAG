{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Representation Learning on Heterostructures via Heterogeneous Anonymous Walks\n\nPaper Abstract: Abstract Capturing structural similarity has been a hot topic in the field of network embedding recently due to its great help in understanding the node functions and behaviors. However, existing works have paid very much attention to learning structures on homogeneous networks while the related study on heterogeneous networks is still a void. In this paper, we try to take the first step for representation learning on heterostructures, which is very challenging due to their highly diverse combinations of node types and underlying structures. To effectively distinguish diverse heterostructures, we firstly propose a theoretically guaranteed technique called heterogeneous anonymous walk (HAW) and its variant coarse HAW (CHAW). Then, we devise the heterogeneous anonymous walk embedding (HAWE) and its variant coarse HAWE in a data-driven manner to circumvent using an extremely large number of possible walks and train embeddings by predicting occurring walks in the neighborhood of each node. Finally, we design and apply extensive and illustrative experiments on synthetic and real-world networks to build a benchmark on heterostructure learning and evaluate the effectiveness of our methods. The results demonstrate our methods achieve outstanding performance compared with both homogeneous and heterogeneous classic methods, and can be applied on large-scale networks.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Network representation learning: A survey\",\n        \"Network geometry\",\n        \"struc2vec: Learning node representations from structural identity\",\n        \"Temporal network embedding for link prediction via vae joint attention mechanism\",\n        \"Iteratively learning embeddings and rules for knowledge graph reasoning\",\n        \"Deepwalk: Online learning of social representations\",\n        \"On proximity and structural role-based embeddings in networks: Misconceptions, techniques, and applications\",\n        \"A survey on role-oriented network embedding\",\n        \"Role discovery in networks\",\n        \"Role-oriented graph auto-encoder guided by structural information\",\n        \"A structural graph representation learning framework\",\n        \"Riwalk: Fast structural node embedding via role identification\",\n        \"Learning structural node representations using graph kernels\",\n        \"Gralsp: Graph neural networks with local structural patterns\",\n        \"Graph structural-topic neural network\",\n        \"Theoretically improving graph neural networks via anonymous walk graph kernels\",\n        \"Reconstructing markov processes from independent and anonymous experiments\",\n        \"Heterogeneous network representation learning: A unified framework with survey and benchmark\",\n        \"metapath2vec: Scalable representation learning for heterogeneous networks\",\n        \"Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning\",\n        \"Modeling relational data with graph convolutional networks\",\n        \"An attention-based graph neural network for heterogeneous structural learning\",\n        \"Heterogeneous graph transformer\",\n        \"node2bits: Compact time-and attribute-aware node representations\",\n        \"Network motifs: simple building blocks of complex networks\",\n        \"Bells-versatile numbers that can count partitions of a set, primes and even rhymes\",\n        \"Heterogeneous graphlets\",\n        \"Anonymous walk embeddings\",\n        \"Distributed representations of sentences and documents\",\n        \"How powerful are graph neural networks?\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Line: Large-scale information network embedding\",\n        \"Rolx: structural role extraction & mining in large graphs\",\n        \"Itâ€™s who you know: graph mining using recursive structural features\",\n        \"Learning structural node embeddings via diffusion wavelets\",\n        \"Role-based graph embeddings\",\n        \"Translating embeddings for modeling multi-relational data\",\n        \"Heterogeneous deep graph infomax\",\n        \"Heterogeneous graph attention network\",\n        \"Random graphs\",\n        \"Emergence of scaling in random networks\",\n        \"Toward understanding and evaluating structural node embeddings\",\n        \"Guided learning for role discovery (glrd) framework, algorithms, and applications\",\n        \"Deep recursive network embedding with regular equivalence\",\n        \"Long short-term memory\",\n        \"Learning stochastic equivalence based on discrete ricci curvature\",\n        \"Auto-encoding variational bayes\",\n        \"Pte: Predictive text embedding through large-scale heterogeneous text networks\",\n        \"Aspem: Embedding learning by aspects in heterogeneous information networks\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Deep graph infomax.\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Representation Learning on Heterostructures via Heterogeneous Anonymous Walks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}