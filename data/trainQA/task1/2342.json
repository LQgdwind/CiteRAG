{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 445 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Continual Reinforcement Learning: A Review and Perspectives\n\nPaper Abstract: Abstract In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.111This work has been accepted by the Journal of Artificial Intelligence Research (JAIR).\n\n{retrieved_text}\n\nGenerate exactly 445 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 445\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Interference and generalization in temporal difference learning\",\n        \"Child: A first step towards continual learning\",\n        \"Continual lifelong learning with neural networks: A review\",\n        \"Continual learning: A comparative study on how to defy forgetting in classification tasks\",\n        \"A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning\",\n        \"Embracing change: Continual learning in deep neural networks\",\n        \"Markov decision processes. 1994.\",\n        \"Introduction to Reinforcement Learning\",\n        \"Learning to achieve goals\",\n        \"Deep reinforcement learning amidst continual structured non-stationarity\",\n        \"Continual learning in environments with polynomial mixing times\",\n        \"Beyond domain randomization\",\n        \"Transfer learning for reinforcement learning domains: A survey\",\n        \"Multi-task reinforcement learning: a hierarchical bayesian approach\",\n        \"Online multi-task learning for policy gradient methods\",\n        \"Offline reinforcement learning: Tutorial, review, and perspectives on open problems\",\n        \"Generalization and regularization in dqn\",\n        \"Gotta learn fast: A new benchmark for generalization in rl\",\n        \"Assessing generalization in deep reinforcement learning\",\n        \"Illuminating generalization in deep reinforcement learning through procedural level generation\",\n        \"A study on overfitting in deep reinforcement learning\",\n        \"Quantifying generalization in reinforcement learning\",\n        \"Is a good representation sufficient for sample efficient reinforcement learning?\",\n        \"A massively parallel architecture for a self-organizing neural pattern recognition machine\",\n        \"Catastrophic interference in connectionist networks: The sequential learning problem\",\n        \"Overcoming catastrophic forgetting in neural networks\",\n        \"Gradient episodic memory for continuum learning\",\n        \"Learning to learn without forgetting by maximizing transfer and minimizing interference\",\n        \"Efficient lifelong learning with a-gem\",\n        \"Reinforcement learning and its relationship to supervised learning\",\n        \"Near-optimal model-free reinforcement learning in non-stationary episodic mdps\",\n        \"Markov games as a framework for multi-agent reinforcement learning\",\n        \"Influencing long-term behavior in multiagent reinforcement learning\",\n        \"Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations\",\n        \"Hidden-mode markov decision processes for nonstationary sequential decision making\",\n        \"Planning under uncertainty for robotic tasks with mixed observability\",\n        \"Near-optimal regret bounds for reinforcement learning\",\n        \"Minimax regret bounds for reinforcement learning\",\n        \"Provably efficient rl with rich observations via latent state decoding\",\n        \"Sparsely changing latent states for prediction and planning in partially observable domains\",\n        \"Efficient reinforcement learning in factored mdps\",\n        \"Stochastic dynamic programming with factored representations\",\n        \"Sample-efficient reinforcement learning for pomdps with linear function approximations\",\n        \"Flambe: Structural complexity and representation learning of low rank mdps\",\n        \"Representation learning for online and offline rl in low-rank mdps\",\n        \"Provably efficient representation learning in low-rank markov decision processes\",\n        \"Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms\",\n        \"Adarl: What, where, and how to adapt in transfer reinforcement learning\",\n        \"Provable rich observation reinforcement learning with combinatorial latent states\",\n        \"Bayesian reinforcement learning in factored pomdps\",\n        \"Block contextual mdps for continual learning\",\n        \"Learning domain invariant representations in goal-conditioned block mdps\",\n        \"Generalizing plans to new environments in relational mdps\",\n        \"Symbolic dynamic programming for first-order mdps\",\n        \"An object-oriented representation for efficient reinforcement learning\",\n        \"Ltl2action: Generalizing ltl instructions for multi-task rl\",\n        \"Temporal-logic-based reward shaping for continuing reinforcement learning tasks\",\n        \"Learning action representations for reinforcement learning\",\n        \"How rl agents behave when their actions are modified\",\n        \"Know your action set: Learning action relations for reinforcement learning\",\n        \"Anymorph: Learning transferable polices by inferring agent morphology\",\n        \"Invariant causal prediction for block mdps\",\n        \"Multitask learning\",\n        \"Multi-criteria reinforcement learning\",\n        \"Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks\",\n        \"Gradient surgery for multi-task learning\",\n        \"Multi-task reinforcement learning in partially observable stochastic environments\",\n        \"Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-… hook\",\n        \"On the optimization of a synaptic learning rule\",\n        \"Learning to reinforcement learn\",\n        \"Rl2: Fast reinforcement learning via slow reinforcement learning\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"A simple neural attentive meta-learner\",\n        \"Reptile: a scalable metalearning algorithm\",\n        \"A perspective view and survey of meta-learning\",\n        \"Meta-learning in neural networks: A survey\",\n        \"Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning\",\n        \"Solving hidden-semi-markov-mode markov decision problems\",\n        \"Non-stationary markov decision processes, a worst-case approach using model-based reinforcement learning\",\n        \"Dealing with non-stationarity in marl via trust-region decomposition\",\n        \"Reinforcement learning in non-stationary environments\",\n        \"Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism\",\n        \"A kernel-based approach to non-stationary reinforcement learning in metric spaces\",\n        \"Efficient learning in non-stationary linear markov decision processes, 2020\",\n        \"Curious model-building control systems\",\n        \"Intrinsically motivated reinforcement learning\",\n        \"Where do rewards come from\",\n        \"Intrinsic motivation and reinforcement learning\",\n        \"Surprise-based intrinsic motivation for deep reinforcement learning\",\n        \"Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem\",\n        \"Automated curriculum learning by rewarding temporally rare events\",\n        \"Automated curriculum generation for policy gradients from demonstrations\",\n        \"Automatic curriculum learning for deep rl: A short survey\",\n        \"Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions\",\n        \"Automatic data augmentation for generalization in reinforcement learning\",\n        \"Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems\",\n        \"Improving generalization in meta-rl with imaginary tasks from latent dynamics mixture\",\n        \"Automatic goal generation for reinforcement learning agents\",\n        \"Automated curricula through setter-solver interactions\",\n        \"Autonomous reinforcement learning via subgoal curricula\",\n        \"Boosted curriculum reinforcement learning\",\n        \"Replay-guided adversarial environment design\",\n        \"Intrinsic motivation and automatic curricula via asymmetric self-play\",\n        \"Dream architecture: a developmental approach to open-ended learning in robotics\",\n        \"Learning with opponent-learning awareness\",\n        \"Dice: The infinitely differentiable monte-carlo estimator\",\n        \"A policy gradient algorithm for learning to learn in multiagent reinforcement learning\",\n        \"Learning shared representations in multi-task reinforcement learning\",\n        \"Sparse multi-task reinforcement learning\",\n        \"Progressive neural networks\",\n        \"The Beneﬁt of Multitask Representation Learning\",\n        \"Sharing knowledge in multi-task deep reinforcement learning\",\n        \"Meta-adaptive nonlinear control: Theory and algorithms\",\n        \"Conflict-averse gradient descent for multi-task learning\",\n        \"Superposition of many models into one\",\n        \"Supermasks in superposition\",\n        \"Model compression\",\n        \"Distilling the knowledge in a neural network\",\n        \"Policy distillation\",\n        \"Learning without forgetting\",\n        \"Representation stability as a regularizer for improved text analytics transfer learning\",\n        \"Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures\",\n        \"Progress & compress: A scalable framework for continual learning\",\n        \"Progressive reinforcement learning with distillation for multi-skilled motion control\",\n        \"Policy consolidation for continual reinforcement learning\",\n        \"Discorl: Continual reinforcement learning via policy distillation\",\n        \"Exploiting hierarchy for learning and transfer in kl-regularized rl\",\n        \"Transient non-stationarity and generalisation in deep reinforcement learning\",\n        \"Memory-efficient reinforcement learning with knowledge consolidation\",\n        \"Catastrophic interference in reinforcement learning: A solution based on context division and knowledge distillation\",\n        \"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\n        \"Organizing experience: a deeper look at replay mechanisms for sample-based planning in continuous state domains\",\n        \"Dyna, an integrated architecture for learning, planning, and reacting\",\n        \"Selective experience replay for lifelong learning\",\n        \"Experience replay for continual learning\",\n        \"Model-augmented prioritized experience replay\",\n        \"Posterior meta-replay for continual learning\",\n        \"Regret minimization experience replay in off-policy reinforcement learning\",\n        \"Generalized proximal policy optimization with sample reuse\",\n        \"Universal off-policy evaluation\",\n        \"Towards mental time travel: a hierarchical memory for reinforcement learning agents\",\n        \"Policy gradients incorporating the future\",\n        \"Lifelong hyper-policy optimization with multiple importance sampling regularization\",\n        \"Catastrophic forgetting, rehearsal and pseudorehearsal\",\n        \"Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting\",\n        \"Model-free generative replay for lifelong reinforcement learning: Application to starcraft-2\",\n        \"Scalable recollections for continual lifelong learning\",\n        \"Online learned continual compression with stacked quantization module\",\n        \"Continual reinforcement learning with complex synapses\",\n        \"Discovering structure in multiple learning tasks: The tc algorithm\",\n        \"Doing more with less: meta-reasoning and meta-learning in humans and machines\",\n        \"Reinforcement learning with hierarchies of machines\",\n        \"Finding structure in reinforcement learning\",\n        \"Transfer of learning by composing solutions of elemental sequential tasks\",\n        \"Multiple model-based reinforcement learning\",\n        \"Learning modular neural network policies for multi-task and multi-robot transfer\",\n        \"Meta learning shared hierarchies\",\n        \"Pathnet: Evolution channels gradient descent in super neural networks\",\n        \"Routing networks: Adaptive selection of non-linear functions for multi-task learning\",\n        \"Beyond shared hierarchies: Deep multitask learning through soft layer ordering\",\n        \"Modular networks: learning to decompose neural computation\",\n        \"Diversity and depth in per-example routing models\",\n        \"Automatically composing representation transformations as a means for generalization\",\n        \"Evolutionary architecture search for deep multitask networks\",\n        \"Modular meta-learning\",\n        \"Recursive routing networks: Learning to compose modules for language understanding\",\n        \"Multi-task reinforcement learning with soft modularization\",\n        \"Sharing less is more: Lifelong learning in deep networks with selective layer transfer\",\n        \"Toward robust long range policy transfer\",\n        \"How to reuse and compose knowledge for a lifetime of tasks: A survey on continual learning and functional composition\",\n        \"Modular lifelong reinforcement learning via neural composition\",\n        \"Building a subspace of policies for scalable continual learning\",\n        \"Neural architecture search with reinforcement learning\",\n        \"Smash: one-shot model architecture search through hypernetworks\",\n        \"Continual and multi-task architecture search\",\n        \"Reinforced continual learning\",\n        \"Routing networks and the challenges of modular and compositional computation\",\n        \"Towards a unified theory of state abstraction for mdps\",\n        \"State abstractions for lifelong reinforcement learning\",\n        \"Decoupling dynamics and reward for transfer learning\",\n        \"Combined reinforcement learning via abstract representations\",\n        \"Learning causal state representations of partially observable environments\",\n        \"Bisimulation makes analogies in goal-conditioned reinforcement learning\",\n        \"Structural similarity for improved transfer in reinforcement learning\",\n        \"Learning markov state abstractions for deep reinforcement learning\",\n        \"The value equivalence principle for model-based reinforcement learning\",\n        \"Monte carlo tree search with iteratively refining state abstractions\",\n        \"Control-aware representations for model-based reinforcement learning\",\n        \"Discovering state and action abstractions for generalized task and motion planning\",\n        \"Context-specific representation abstraction for deep option learning\",\n        \"Hierarchical solution of markov decision processes using macro-actions\",\n        \"The option-critic architecture\",\n        \"A laplacian framework for option discovery in reinforcement learning\",\n        \"Eigenoption discovery through the deep successor representation\",\n        \"When waiting is not an option: Learning options with a deliberation cost\",\n        \"Options of interest: Temporal abstraction with interest functions\",\n        \"On the role of weight sharing during deep option learning\",\n        \"Flexible option learning\",\n        \"Pac-inspired option discovery in lifelong reinforcement learning\",\n        \"Variational option discovery algorithms\",\n        \"Learning abstract options\",\n        \"Multitask soft option learning\",\n        \"Adaptive skills adaptive partitions (asap)\",\n        \"Diversity is all you need: Learning skills without a reward function\",\n        \"Explore, discover and learn: Unsupervised discovery of state-covering skills\",\n        \"A deep hierarchical approach to lifelong learning in minecraft\",\n        \"Learning to compose skills\",\n        \"The option keyboard: Combining skills in reinforcement learning\",\n        \"Reset-free lifelong learning with skill-space planning\",\n        \"Toward good abstractions for lifelong learning\",\n        \"Value preserving state-action abstractions\",\n        \"Reward-free exploration for reinforcement learning\",\n        \"On reward-free reinforcement learning with linear function approximation\",\n        \"Reward-free model-based reinforcement learning with linear function approximation\",\n        \"Variational intrinsic control\",\n        \"Learning an embedding space for transferable robot skills\",\n        \"Learning one representation to optimize all rewards\",\n        \"Many-goals reinforcement learning\",\n        \"Modular multitask reinforcement learning with policy sketches\",\n        \"Zero-shot task generalization with multi-task deep reinforcement learning\",\n        \"Hierarchical and interpretable skill acquisition in multi-task reinforcement learning\",\n        \"Dynamic dialogue policy transformer for continual reinforcement learning\",\n        \"Learning effective subgoals with multi-task hierarchical reinforcement learning\",\n        \"Universal value function approximators\",\n        \"Unicorn: Continual learning with a universal, off-policy agent\",\n        \"Continual reinforcement learning with diversity exploration and adversarial self-correction\",\n        \"Hindsight experience replay\",\n        \"Hindsight policy gradients\",\n        \"Generalized hindsight for reinforcement learning\",\n        \"Goal-aware cross-entropy for multi-target reinforcement learning\",\n        \"Goal-directed planning via hindsight experience replay\",\n        \"Learning multi-level hierarchies with hindsight\",\n        \"Curious: intrinsically motivated modular multi-goal reinforcement learning\",\n        \"Recurrent reinforcement learning: a hybrid approach\",\n        \"Playing fps games with deep reinforcement learning\",\n        \"Learning to navigate in complex environments\",\n        \"Reinforcement learning with unsupervised auxiliary tasks\",\n        \"Multi-task deep reinforcement learning with popart\",\n        \"Loss is its own reward: Self-supervision for reinforcement learning\",\n        \"Bootstrap latent-predictive representations for multitask reinforcement learning\",\n        \"Discovery of useful questions as auxiliary tasks\",\n        \"Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction\",\n        \"A geometric perspective on optimal representations for reinforcement learning\",\n        \"Fast context adaptation via meta-learning\",\n        \"Efficient off-policy meta-reinforcement learning via probabilistic context variables\",\n        \"Varibad: A very good method for bayes-adaptive deep rl via meta-learning\",\n        \"Meta reinforcement learning as task inference\",\n        \"Pratik Chaudhari\",\n        \"Meta-learning of sequential strategies\",\n        \"Generalized hidden parameter mdps transferable model-based rl in a handful of trials\",\n        \"Offline meta reinforcement learning–identifiability challenges and effective data collection strategies\",\n        \"Risk-averse bayes-adaptive reinforcement learning\",\n        \"Multi-task reinforcement learning with context-based representations\",\n        \"Towards effective context for meta-reinforcement learning: an approach based on contrastive learning\",\n        \"Dealing with non-stationary environments using context detection\",\n        \"Scalable bayesian reinforcement learning for multiagent pomdps\",\n        \"Prediction-based multi-agent reinforcement learning in inherently non-stationary environments\",\n        \"Options as responses: Grounding behavioural hierarchies in multi-agent rl\",\n        \"Policy evaluation networks\",\n        \"Fast adaptation via policy-dynamics value functions\",\n        \"A multitask representation using reusable local policy templates\",\n        \"Sequential decision-making under non-stationary environments via sequential change-point detection\",\n        \"Context-aware policy reuse\",\n        \"Same state, different task: Continual reinforcement learning without interference\",\n        \"Adapt to environment sudden changes by learning a context sensitive policy\",\n        \"An exploration strategy for non-stationary opponents\",\n        \"On q-learning convergence for non-markov decision processes\",\n        \"Bayesian decision problems and Markov chains\",\n        \"Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes\",\n        \"Reinforcement learning with self-modifying policies\",\n        \"Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement\",\n        \"A general method for incremental self-improvement and multi-agent learning\",\n        \"Learning a synaptic learning rule\",\n        \"Learning to control fast-weight memories: An alternative to dynamic recurrent networks\",\n        \"Meta-learning representations for continual learning\",\n        \"Meta-learnt priors slow down catastrophic forgetting in neural networks\",\n        \"Learning to continually learn\",\n        \"Online fast adaptation and knowledge accumulation: a new approach to continual learning\",\n        \"Accelerating online reinforcement learning via model-based meta-learning\",\n        \"Bayesian model-agnostic meta-learning\",\n        \"Promp: Proximal meta-policy search\",\n        \"Transferring knowledge across learning processes\",\n        \"Deep online learning via meta-learning: Continual adaptation for model-based rl\",\n        \"Guided meta-policy search\",\n        \"Online meta-learning\",\n        \"Model-based adversarial meta-reinforcement learning\",\n        \"Comps: Continual meta policy search\",\n        \"Evolving reinforcement learning algorithms\",\n        \"Introducing symmetries to black box meta reinforcement learning\",\n        \"Hindsight foresight relabeling for meta-reinforcement learning\",\n        \"Transformers are meta-reinforcement learners\",\n        \"Skill-based meta-reinforcement learning\",\n        \"Continuous adaptation via meta-learning in nonstationary and competitive environments\",\n        \"Learning to learn: Meta-critic networks for sample efficient learning\",\n        \"Learning to learn: Hierarchical meta-critic networks\",\n        \"Online meta-critic learning for off-policy actor-critic methods\",\n        \"Bootstrapped meta-learning\",\n        \"Optimizing for the future in non-stationary mdps\",\n        \"Adapting bias by gradient descent: an incremental version of the delta-bar-delta\",\n        \"Meta-gradient reinforcement learning\",\n        \"Self-tuning deep reinforcement learning\",\n        \"Meta-gradient reinforcement learning with an objective discovered online\",\n        \"R-iac: Robust intrinsically motivated exploration and active learning\",\n        \"On learning intrinsic rewards for policy gradient methods\",\n        \"Some considerations on learning to explore via meta-reinforcement learning\",\n        \"Learning to explore with meta-policy gradient\",\n        \"Evolved policy gradients\",\n        \"Norml: No-reward meta learning\",\n        \"Reward shaping via meta-learning\",\n        \"What can learned intrinsic rewards capture?\",\n        \"Making the world differentiable: On using self-supervised fully recurrent n eu al networks for dynamic reinforcement learning and planning in non-stationary environm nts\",\n        \"Empowerment: A universal agent-centric measure of control\",\n        \"Curiosity-driven development\",\n        \"Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes\",\n        \"Curiosity driven reinforcement learning for motion planning on humanoids\",\n        \"Variational information maximisation for intrinsically motivated reinforcement learning\",\n        \"Unifying count-based exploration and intrinsic motivation\",\n        \"Vime: Variational information maximizing exploration\",\n        \"Curiosity-driven exploration by self-supervised prediction\",\n        \"Unsupervised real-time control through variational empowerment\",\n        \"Large-scale study of curiosity-driven learning\",\n        \"Model-based active exploration\",\n        \"Competitive experience replay\",\n        \"Go-explore: a new approach for hard-exploration problems\",\n        \"Planning to explore via self-supervised world models\",\n        \"Reactive exploration to cope with non-stationarity in lifelong reinforcement learning\",\n        \"Smirl: Surprise minimizing reinforcement learning in unstable environments\",\n        \"Learning to multi-task by active sampling\",\n        \"Interactive agent modeling by learning to probe\",\n        \"The arcade learning environment: An evaluation platform for general agents\",\n        \"Openai gym\",\n        \"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play\",\n        \"The barbados 2018 list of open issues in continual learning\",\n        \"Hierarchical reinforcement learning with the maxq value function decomposition\",\n        \"Deepmind lab\",\n        \"Minecraft, beyond construction and survival\",\n        \"Vizdoom: A doom-based ai research platform for visual reinforcement learning\",\n        \"Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents\",\n        \"Benchmark environments for multitask learning in continuous domains\",\n        \"Leveraging procedural generation to benchmark reinforcement learning\",\n        \"Behaviour suite for reinforcement learning\",\n        \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning\",\n        \"Causalworld: A robotic manipulation benchmark for causal structure and transfer learning\",\n        \"Embodiment theory and education: The foundations of cognition in perception and action\",\n        \"Virtual embodiment: A scalable long-term strategy for artificial intelligence research\",\n        \"Ingredients of intelligence: From classic debates to an engineering roadmap\",\n        \"Experience grounds language\",\n        \"Environments for lifelong reinforcement learning\",\n        \"Jelly bean world: A testbed for never-ending learning\",\n        \"Continuous coordination as a realistic scenario for lifelong learning\",\n        \"Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents\",\n        \"L2explorer: A lifelong reinforcement learning assessment environment\",\n        \"Novelgridworlds: A benchmark environment for detecting and adapting to novelties in open worlds\",\n        \"Towards safe policy improvement for non-stationary mdps, 2020b\",\n        \"Deep reinforcement learning that matters\",\n        \"Re-evaluate: Reproducibility in evaluating reinforcement learning algorithms\",\n        \"A note on measurement of utility\",\n        \"Neuroscience-inspired artificial intelligence\",\n        \"Reinforcement learning in the brain\",\n        \"Optimizing agent behavior over long time scales by transporting value\",\n        \"Moderate levels of activation lead to forgetting in the think/no-think paradigm\",\n        \"Sebastian Musslick\",\n        \"Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory\",\n        \"A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits\",\n        \"Continual backprop: Stochastic gradient descent with persistent randomness\",\n        \"Behavioral considerations suggest an average reward td model of the dopamine system\",\n        \"On the value of information and other rewards\",\n        \"Computational models of reinforcement learning: the role of dopamine as a reward signal\",\n        \"Dopamine reward prediction errors reflect hidden-state inference across time\",\n        \"Prefrontal cortex as a meta-reinforcement learning system\",\n        \"Acute stress selectively reduces reward sensitivity\",\n        \"The role of executive function in shaping reinforcement learning\",\n        \"How much of reinforcement learning is working memory, not reinforcement learning? a behavioral, computational, and neurogenetic analysis\",\n        \"Prioritized memory access explains planning and hippocampal replay\",\n        \"Offline replay supports planning in human reinforcement learning\",\n        \"Hippocampal contributions to model-based planning and spatial memory\",\n        \"Learning structures: Predictive representations, replay, and generalization\",\n        \"Search on the replay buffer: Bridging planning and reinforcement learning\",\n        \"Consolidation in neural networks and in the sleeping brain\",\n        \"Catastrophic forgetting in simple networks: an analysis of the pseudorehearsal solution\",\n        \"Sequential replay of nonspatial task states in the human hippocampus\",\n        \"Generalization of value in reinforcement learning by humans\",\n        \"Reinforcement learning and causal models\",\n        \"Positive reward prediction errors during decision-making strengthen memory encoding\",\n        \"Prioritized experience replay\",\n        \"States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning\",\n        \"Model-based influences on humans’ choices and striatal prediction errors\",\n        \"The ubiquity of model-based reinforcement learning\",\n        \"Model-based predictions for dopamine\",\n        \"Environmental statistics and the trade-off between model-based and td learning in humans\",\n        \"Model-based learning protects against forming habits\",\n        \"Improving generalization for temporal difference learning: The successor representation\",\n        \"The successor representation in human reinforcement learning\",\n        \"Internal models in the cerebellum\",\n        \"Separated modules for visuomotor control and learning in the cerebellum: a functional mri study\",\n        \"Human cerebellar activity reflecting an acquired internal model of a new tool\",\n        \"Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control\",\n        \"Dopamine, uncertainty and td learning\",\n        \"Self-evaluation of decision-making: A general bayesian framework for metacognitive computation\",\n        \"Belief state representation in the dopamine system\",\n        \"Distributional reinforcement learning in the brain\",\n        \"Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective\",\n        \"Human reinforcement learning subdivides structured action spaces by learning effector-specific values\",\n        \"Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis\",\n        \"Mechanisms of hierarchical reinforcement learning in cortico–striatal circuits 2: Evidence from fmri\",\n        \"Divide and conquer: hierarchical reinforcement learning and task decomposition in humans\",\n        \"Model-based hierarchical reinforcement learning and human action control\",\n        \"Optimal behavioral hierarchy\",\n        \"Subgoal-and goal-related reward prediction errors in medial prefrontal cortex\",\n        \"Reinforcement learning in multidimensional environments relies on attention mechanisms\",\n        \"Learning task-state representations\",\n        \"Discovering latent causes in reinforcement learning\",\n        \"The bitter lesson\",\n        \"Improving generalization in meta reinforcement learning using learned objectives\",\n        \"Discovering reinforcement learning algorithms\",\n        \"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions\",\n        \"On value functions and the agent-environment boundary\",\n        \"What is an agent?\",\n        \"What can i do here? a theory of affordances in reinforcement learning\",\n        \"Navigating the affordance landscape: feedback control as a process model of behavior and cognition\",\n        \"The theory of affordances\",\n        \"Affordances and the body: An intentional analysis of gibson’s ecological approach to visual perception\",\n        \"An outline of a theory of affordances\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Imagenet large scale visual recognition challenge\",\n        \"Visualizing and understanding convolutional networks\",\n        \"Intriguing properties of neural networks\",\n        \"The hardware lottery\",\n        \"Language models are few-shot learners\",\n        \"Scaling laws for neural language models\",\n        \"A neural scaling law from the dimension of the data manifold\",\n        \"Scaling laws for autoregressive generative modeling\",\n        \"A survey and critique of multiagent deep reinforcement learning\",\n        \"Friend-or-foe q-learning in general-sum games\",\n        \"Reinforcement learning to play an optimal nash equilibrium in team markov games\",\n        \"Convergence and no-regret in multiagent learning\",\n        \"Correlated-Q learning\",\n        \"Cyclic equilibria in markov games\",\n        \"Stable opponent shaping in differentiable games\",\n        \"Teaching on a budget: Agents advising agents in reinforcement learning\",\n        \"Learning to teach in cooperative multiagent reinforcement learning\",\n        \"Learning hierarchical teaching policies for cooperative agents\",\n        \"Multi-agent learning with policy prediction\",\n        \"Toward an architecture for never-ending language learning\",\n        \"Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage\"\n    ],\n    \"reasoning\": \"Selected these 445 references because they represent foundational work and recent advances in Towards Continual Reinforcement Learning: A Review and Perspectives, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}