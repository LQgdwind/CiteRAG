{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 168 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation\n\nPaper Abstract: Abstract As its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator, called Sprint 111Sprint: SParse attention acceleration with appRoximate IN-memory Token pruning, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner. Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling Sprint to fetch only a small subset of relevant data to on-chip memory. To mitigate potential negative repercussions for model accuracy, Sprint re-computes the attention scores for the few fetched data in digital. The combined in-memory pruning and on-chip recompute of the relevant attention scores enables Sprint to transform quadratic complexity to a merely linear one. In addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches. We evaluate our proposed technique on a wide range of state-of-the-art transformer models. On average, Sprint yields 7.5×\\times speedup and 19.6×\\times energy reduction when total 16KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36%percent\\% degradation).\n\n{retrieved_text}\n\nGenerate exactly 168 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 168\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Compute Caches\",\n        \"Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays\",\n        \"A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing\",\n        \"PIM-enabled Instructions: A Low-Overhead, Locality-Aware Processing-in-Memory Architecture\",\n        \"An Introduction to SDRAM and Memory Controllers\",\n        \"Predator: A Predictable SDRAM Memory Controller\",\n        \"SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks\",\n        \"Bit-Pragmatic Deep Neural Network Computing\",\n        \"Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing\",\n        \"ReDRAM: A Reconfigurable Processing-In-DRAM Platform for Accelerating Bulk Bit-Wise Operations\",\n        \"DIMA: A Depthwise CNN In-Memory Accelerator\",\n        \"ParaPIM: A Parallel Processing-in-memory Accelerator for Binary-Weight Deep Neural Networks\",\n        \"Artisan Memory Compilers\",\n        \"Fafnir: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction\",\n        \"Multi-Level Control of Resistive RAM (RRAM) Using a Write Termination to Achieve 4 Bits/Cell in High Resistance State\",\n        \"Longformer: The Long-Document Transformer\",\n        \"ReVAMP: ReRAM based VLIW Architecture for In-memory Computing\",\n        \"PARDIS: A Programmable Memory Controller for the DDRx Interfacing Standards\",\n        \"Genus Synthesis Solution\",\n        \"Innovus Implementation System\",\n        \"A Fully Integrated Reprogrammable Memristor–CMOS System for Efficient Multiply–Accumulate Operations\",\n        \"Impulse: Building a Smarter Memory Controller\",\n        \"Circuit Design Challenges in Embedded Memory and Resistive RAM (RRAM) for Mobile SoC and 3D-IC\",\n        \"Chasing Sparsity in Vision Transformers: An End-to-End Exploration\",\n        \"DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning\",\n        \"Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks\",\n        \"Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices\",\n        \"DaDianNao: A Machine-Learning Supercomputer\",\n        \"Prime: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-based Main Memory\",\n        \"Generating Long Sequences with Sparse Transformers\",\n        \"Multi-Stream Write SSD\",\n        \"Rethinking Attention with Performers\",\n        \"Adaptively Sparse Transformers\",\n        \"Fine-Tune BERT with Sparse Self-Attention Mechanism\",\n        \"Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks\",\n        \"DrAcc: A DRAM based Accelerator for Accurate CNN Inference\",\n        \"Design of Ion-Implanted MOSFET’s with Very Small Physical Dimensions\",\n        \"NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory\",\n        \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n        \"Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks\",\n        \"“NDA: Near-DRAM Acceleration Architecture Leveraging Commodity DRAM Devices and Standard Memory Modules\",\n        \"“A Configurable Cloud-Scale DNN Processor for Real-Time AI\",\n        \"“Parallel Computing of Graph-based Functions in ReRAM\",\n        \"G. Tziantzioulis\",\n        \"“TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory\",\n        \"“Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators\",\n        \"“DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs\",\n        \"“Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks\",\n        \"“Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic\",\n        \"“Bit-Parallel Vector Composability for Neural Acceleration\",\n        \"“Resistive RAM Endurance: Array-Level Characterization and Correction Techniques Targeting Deep Learning Applications\",\n        \"“AC-DIMM: Associative Computing with STT-MRAM\",\n        \"“ReRAM-based In-memory Computing for Search Engine and Neural Network Applications\",\n        \"“A3̂: Accelerating Attention Mechanisms in Neural Networks with Approximation\",\n        \"“ELSA: Hardware-Software Co-design for Efficient\",\n        \"“EIE: Efficient Inference Engine on Compressed Deep Neural Network\",\n        \"“Simulating DRAM Controllers for Future System Architecture Exploration\",\n        \"“Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning\",\n        \"“Noise Injection Adaption: End-to-end ReRAM Crossbar Mon-Ideal Effect Adaption for Meural Network Mapping\",\n        \"“Dot-product Engine for Neuromorphic Computing: Programming 1T1M Crossbar to Accelerate Matrix-Vector Multiplication\",\n        \"“FastWave: Accelerating Autoregressive Convolutional Neural Networks on FPGA\",\n        \"“FloatPIM: In-Memory Acceleration of Deep Neural Network Training with High Precision\",\n        \"“XNOR-POP: A Processing-In-Memory Architecture for Binary Convolutional Neural Networks in Wide-IO2 DRAMs\",\n        \"“AccuReD: High Accuracy Training of CNNs on ReRAM/GPU Heterogeneous 3-D Architecture\",\n        \"R. Narayanaswami\",\n        \"“Stripes: Bit-serial Deep Neural Network Computing\",\n        \"“A Multi-functional In-memory Inference Processor using a Standard 6T SRAM Array\",\n        \"“An Energy-Efficient VLSI Architecture for Pattern Recognition via Deep Embedding of Computation in SRAM\",\n        \"“FlexRAM: Toward an Advanced Intelligent Memory System\",\n        \"“FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks\",\n        \"“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n        \"“MViD: Sparse Matrix-Vector Multiplication in Mobile DRAM for Accelerating Recurrent Neural Networks\",\n        \"“Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory\",\n        \"“GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent\",\n        \"“Learned Token Pruning for Transformers\",\n        \"“Reformer: The Efficient Transformer\",\n        \"“EXECUBE-A New Architecture for Scaleable MPPs\",\n        \"“Learning Multiple Layers of Features from Tiny Images\",\n        \"“Understanding Reuse\",\n        \"“MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects\",\n        \"“TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning\",\n        \"“ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\",\n        \"“ComPEND: Computation Pruning through Early Negative Detection for ReLU in a deep neural network accelerator\",\n        \"“Buffered Compares: Excavating the Hidden Parallelism Inside DRAM Architectures with Lightweight Logic\",\n        \"“UNPU: A 50.6 TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision\",\n        \"“Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology: Industrial Product\",\n        \"“DRISA: A DRAM-based Reconfigurable In-Situ Accelerator\",\n        \"“Pinatubo: A Processing-in-Memory Architecture for Bulk Bitwise Operations in Emerging Non-Volatile Memories\",\n        \"“TIMELY: Pushing Data Movements and Interfaces in PIM Accelerators towards Local and in Time Domain\",\n        \"“Accelerating Attention through Gradient-Based Learned Runtime Pruning\",\n        \"“PredictiveNet: An Energy-Efficient Convolutional Neural Network via Zero Prediction\",\n        \"“Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach\",\n        \"“Generating Wikipedia by Summarizing Long Sequences\",\n        \"“Cambricon: An Instruction Set Architecture for Neural Networks\",\n        \"“3DICT: A Reliable and QoS Capable Mobile Process-in-Memory Architecture for Lookup-Based CNNs in 3D XPoint ReRAMs\",\n        \"“Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture\",\n        \"“Smart Memories: A Modular Reconfigurable Architecture\",\n        \"“LerGAN: A Zero-free\",\n        \"“The WikiText Long Term Dependency Language Modeling Dataset\",\n        \"“Are Sixteen Heads Really Better than One?” arXiv preprint arXiv:1905.10650\",\n        \"“A Survey of ReRAM-based Architectures for Processing-In-Memory and Neural Networks\",\n        \"N. Muralimanohar\",\n        \"“Missing the Memory Wall: The Case for Processor/Memory Integration\",\n        \"“SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks\",\n        \"“OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator\",\n        \"“PyTorch: An Imperative Style\",\n        \"“A Case for Intelligent RAM\",\n        \"R. Balasubramonian\",\n        \"“SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training\",\n        \"“Blockwise Self-Attention for Long Document Understanding\",\n        \"“Language Models are Unsupervised Multitask Learners\",\n        \"“Compressive transformers for long-range sequence modelling\",\n        \"000+ Questions for Machine Comprehension of Text\",\n        \"“Exploring the Impact of Memory Block Permutation on Performance of a Crossbar ReRAM Main Memory\",\n        \"“Minerva: Enabling Low-Power\",\n        \"“DeepFense: Online Accelerated Defense against Adversarial Deep Learning\",\n        \"“Efficient Content-based Sparse Attention with Routing Transformers\",\n        \"“BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation\",\n        \"“EncoDeep: Realizing Bit-Flexible Encoding for Deep Neural Networks\",\n        \"R. Ausavarungnirun\",\n        \"“Ambit: In-Memory Accelerator for Bulk Bitwise Operations using Commodity DRAM Technology\",\n        \"“Gather-Scatter DRAM: In-DRAM Address Translation to Improve the Spatial Locality of Non-unit Strided Accesses\",\n        \"“Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture\",\n        \"“Laconic Deep Learning Inference Acceleration\",\n        \"“Loom: Exploiting Weight and Activation Precisions to Accelerate Convolutional Neural Networks\",\n        \"“From High-Level Deep Neural Models to FPGAs\",\n        \"“Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks\",\n        \"“McDRAM: Low latency and energy-efficient matrix computations in DRAM\",\n        \"“Thanks for Nothing: Predicting Zero-valued Activations with Lightweight Convolutional Neural Networks\",\n        \"“PipeLayer: A Pipelined ReRAM-based Accelerator for Deep Learning\",\n        \"“GraphR: Accelerating Graph Processing using ReRAM\",\n        \"“General-Purpose Code Acceleration with Limited-Precision Analog Computation\",\n        \"“Memristor Crossbar Architecture for Synchronous Neural Networks\",\n        \"“Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers\",\n        \"“Flash ADC architecture\",\n        \"D. Brooks et al\",\n        \"“Long Range Arena: A Benchmark for Efficient Transformers\",\n        \"“ADC performance survey 1997-2016\",\n        \"“Attention is All You Need\",\n        \"“A 74 TMACS/W CMOS-RRAM Neurosynaptic Core with Dynamically Reconfigurable Dataflow and In-situ Transposable Weights for Probabilistic Graphical Models\",\n        \"“Edge AI without Compromise: Efficient\",\n        \"“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\",\n        \"“SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning\",\n        \"“Rerec: In-ReRAM Acceleration with Access-Aware Mapping for Personalized Recommendation\",\n        \"“Structured Pruning of Large Language Models\",\n        \"“Learning Intrinsic Sparse Structures within Long Short-Term Memory\",\n        \"M. Funtowicz et al\",\n        \"“A Study on Low-Power\",\n        \"“PIMGCN: A ReRAM-Based PIM Design for Graph Convolutional Network Acceleration\",\n        \"“Face Classification Using Electronic Synapses\",\n        \"“FlexiGAN: An End-to-End Solution for FPGA Acceleration of Generative Adversarial Networks\",\n        \"“GANAX: A Unified SIMD-MIMD Acceleration for Generative Adversarial Network\",\n        \"“Neural Acceleration for GPU Throughput Processors\",\n        \"“An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks\",\n        \"“In-DRAM Near-Data Approximate Acceleration for GPUs\",\n        \"“TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference\",\n        \"“BP-Transformer: Modelling Long-Range Context via Binary Partitioning\",\n        \"“XNOR-SRAM: In-Memory Computing SRAM Macro for Binary/Ternary Deep Neural Networks\",\n        \"“Investigating the Switching Dynamics and Multilevel Capability of Bipolar Metal Oxide Resistive Switching Memory\",\n        \"“FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator\",\n        \"“GOBO: Quantizing Attention-based NLP Models for Low Latency and Energy Efficient Inference\",\n        \"“Big Bird: Transformers for Longer Sequences\",\n        \"“TOP-PIM: Throughput-Oriented Programmable Processing in Memory\",\n        \"“SpongeDirectory: Flexible Sparse Directories Utilizing Multi-Level Memristors\",\n        \"“GraphP: Reducing Communication for PIM-based Graph Processing with Efficient Data Partition\",\n        \"“Cambricon-X: An Accelerator for Sparse Neural Networks\",\n        \"“Explicit Sparse Transformer: Concentrated Attention through Explicit Selection\",\n        \"“GraphQ: Scalable PIM-based Graph Processing\"\n    ],\n    \"reasoning\": \"Selected these 168 references because they represent foundational work and recent advances in Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}