{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Cognitive Accident Prediction in Driving Scenes: A Multimodality Benchmark\n\nPaper Abstract: Abstract Traffic accident prediction in driving videos aims to provide an early warning of accident occurrence, and supports the decision-making of safe driving systems. Previous works usually concentrate on the spatial-temporal correlation of object-level context, while they do not fit the inherent long-tailed data distribution well and are vulnerable to severe environmental change. In this work, we propose a Cognitive Accident Prediction (CAP) method that explicitly leverages human-inspired cognition of text description on the visual observation and the driver attention to facilitate model training. In particular, the text description provides a dense semantic description guidance for the primary context of the traffic scene, while the driver attention provides traction to focus on the critical region closely correlating with safe driving. CAP is formulated by an attentive text-to-vision shift fusion module, an attentive scene context transfer module, and the driver attention-guided accident prediction module. We leverage the attention mechanism in these modules to explore the core semantic cues for accident prediction. In order to train CAP, we extend an existing self-collected DADA-2000 dataset (with annotated driver attention for each frame) with further factual text descriptions for the visual observations before the accidents. Besides, we construct a new large-scale benchmark consisting of 11,727 in-the-wild accident videos with over 2.19 million frames (named as CAP-DATA) together with labeled fact-effect-reason-introspection description and temporal accident frame label. Based on extensive experiments, the superiority of CAP is validated compared with state-of-the-art approaches.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Global health observatory: Number of road traffic deaths\",\n        \"Graph-based spatial-temporal convolutional network for vehicle trajectory prediction in autonomous driving\",\n        \"Trajectory forecasting based on prior-aware directed graph convolutional neural network\",\n        \"Maneuver-based trajectory prediction for self-driving cars using spatio-temporal convolutional networks\",\n        \"Heterogeneous trajectory forecasting via risk and scene graph learning\",\n        \"Vehiclenet: Learning robust visual representation for vehicle re-identification\",\n        \"Spatial-temporal relation reasoning for action prediction in videos\",\n        \"Coupling intent and action for pedestrian crossing behavior prediction\",\n        \"Recurrent semantic preserving generation for action prediction\",\n        \"Behavioral intention prediction in driving scenes: A survey\",\n        \"DRAMA: joint risk localization and captioning in driving\",\n        \"DADA: driver attention prediction in driving accident scenarios\",\n        \"Anticipating accidents in dashcam videos\",\n        \"Anticipating traffic accidents with adaptive loss and large-scale incident DB\",\n        \"An attention-guided multistream feature fusion network for localization of risky objects in driving videos\",\n        \"A dynamic spatial-temporal attention network for early anticipation of traffic accidents\",\n        \"Uncertainty-based traffic accident anticipation with spatio-temporal relational learning\",\n        \"Pipa: Pixel-and patch-wise self-supervised learning for domain adaptative semantic segmentation\",\n        \"DRIVE: deep reinforced accident anticipation with visual explanation\",\n        \"HERO: hierarchical spatio-temporal reasoning with contrastive action correspondence for end-to-end video object grounding\",\n        \"Invariant grounding for video question answering\",\n        \"Equivariant and invariant grounding for video question answering\",\n        \"Video as conditional graph hierarchy for multi-granular question answering\",\n        \"Who is better at anticipating traffic crashes, human or artificial intelligence? A gaze data-based exploratory study\",\n        \"CADP: A novel dataset for cctv traffic camera based accident analysis\",\n        \"Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events\",\n        \"Unsupervised traffic accident detection in first-person videos\",\n        \"MEDIRL: predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning\",\n        \"DoTA: unsupervised detection of traffic anomaly in driving videos\",\n        \"Human action recognition and prediction: A survey\",\n        \"Adaptive graph convolutional network with adversarial learning for skeleton-based action prediction\",\n        \"Parameter-efficient person re-identification in the 3d space\",\n        \"Explainability of deep vision-based autonomous driving systems: Review and challenges\",\n        \"Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents\",\n        \"Grad-cam: Visual explanations from deep networks via gradient-based localization\",\n        \"DADA-2000: can driving accident be predicted by driver attention? analyzed by A benchmark\",\n        \"Traffic accident benchmark for causality recognition\",\n        \"Spatiotemporal scene-graph embedding for autonomous vehicle collision prediction\",\n        \"Learning to predict collision risk from simulated video data\",\n        \"Traffic accident detection via self-supervised consistency learning in driving scenarios\",\n        \"A robustly optimized bert pre-training approach with post-training\",\n        \"Attention is all you need\",\n        \"Multimodal learning with transformers: A survey\",\n        \"Predicting driver attention in critical situations\",\n        \"Predicting the driver's focus of attention: The dr(eye)ve project\",\n        \"Revisiting video saliency prediction in the deep learning era\",\n        \"A deep multi-level network for saliency prediction\",\n        \"Impairing the useful field of view in natural scenes: Tunnel vision versus general interference\",\n        \"Beyond gaze fixation: Modeling peripheral vision in relation to speed, tesla autopilot, cognitive load, and age in highway driving\",\n        \"University-1652: A multi-view multi-source benchmark for drone-based geo-localization\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Cognitive Accident Prediction in Driving Scenes: A Multimodality Benchmark, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}