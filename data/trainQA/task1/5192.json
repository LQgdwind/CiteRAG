{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 306 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Temporal Sentence Grounding in Videos: A Survey and Future Directions\n\nPaper Abstract: Abstract Temporal sentence grounding in videos (TSGV), a.k.a., natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. Lastly, we discuss issues with the current TSGV research and share our insights about promising research directions.\n\n{retrieved_text}\n\nGenerate exactly 306 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 306\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Learning spatiotemporal features with 3d convolutional networks\",\n        \"Convolutional two-stream network fusion for video action recognition\",\n        \"Quo vadis, action recognition? a new model and the kinetics dataset\",\n        \"G-tad: Sub-graph localization for temporal action detection\",\n        \"Gated self-matching networks for reading comprehension and question answering\",\n        \"Bidirectional attention flow for machine comprehension\",\n        \"Fast and accurate reading comprehension by combining self-attention and convolution\",\n        \"Fusionnet: Fusing via fully-aware attention with application to machine comprehension\",\n        \"Tall: Temporal activity localization via language query\",\n        \"Localizing moments in video with natural language\",\n        \"Cross-modal moment localization in videos\",\n        \"Attentive moment retrieval in videos\",\n        \"Mac: Mining activity concepts for language-based temporal localization\",\n        \"Text-to-clip video retrieval with early fusion and re-captioning\",\n        \"Temporally grounding natural sentence in video\",\n        \"Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment\",\n        \"Semantic conditioned dynamic modulation for temporal sentence grounding in videos\",\n        \"Learning 2d temporal adjacent networks formoment localization with natural language\",\n        \"To find where you talk: Temporal sentence localization in video with attention based location regression\",\n        \"ExCL: Extractive Clip Localization Using Natural Language Descriptions\",\n        \"Localizing natural language in videos\",\n        \"DEBUG: A dense bottom-up grounding approach for natural language video localization\",\n        \"Span-based localizing network for natural language video localization\",\n        \"Read, watch, and move: Reinforcement learning for temporally grounding natural language descriptions in videos\",\n        \"Language-driven temporal activity localization: A semantic matching reinforcement learning model\",\n        \"Tree-structured policy based progressive reinforcement learning for temporally language grounding in video\",\n        \"Weakly supervised video moment retrieval from text queries\",\n        \"WSLLN:weakly supervised natural language localization networks\",\n        \"Weakly supervised dense event captioning in videos\",\n        \"Weakly-supervised video moment retrieval via semantic completion network\",\n        \"Towards bridging event captioner and sentence localizer for weakly supervised dense event captioning\",\n        \"A survey of temporal activity localization via language in untrimmed videos\",\n        \"A survey on natural language video localization\",\n        \"A survey on temporal sentence grounding in videos\",\n        \"A survey on video moment localization\",\n        \"Efficient estimation of word representations in vector space\",\n        \"GloVe: Global vectors for word representation\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Skip-thought vectors\",\n        \"Supervised learning of universal sentence representations from natural language inference data\",\n        \"Sentence-BERT: Sentence embeddings using Siamese BERT-networks\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Deep residual learning for image recognition\",\n        \"Localizing moments in video with temporal language\",\n        \"Multi-modal circulant fusion for video-to-language and backward\",\n        \"Exploiting temporal relationships in video moment localization with natural language\",\n        \"Cross-modal video moment retrieval with spatial and language-temporal attention\",\n        \"An attentive sequence to sequence translator for localizing video clips by natural language\",\n        \"Multi-modal relational graph for cross-modal video moment retrieval\",\n        \"“Interaction-integrated network for natural language moment localization\",\n        \"“R-c3d: Region convolutional 3d network for temporal activity detection\",\n        \"“Multilevel language and vision integration for text-to-clip retrieval\",\n        \"“Semantic proposal for activity localization in videos via sentence query\",\n        \"“Boundary proposal network for two-stage natural language video localization\",\n        \"“Adaptive proposal generation network for temporal sentence localization in videos\",\n        \"“Natural language video localization with learnable moment proposals\",\n        \"“Video moment localization via deep cross-modal hashing\",\n        \"“Cross-modal interaction networks for query-based moment retrieval in videos\",\n        \"“Semantic conditioned dynamic modulation for temporal sentence grounding in videos\",\n        \"“Moment retrieval via cross-modal interaction networks with query reconstruction\",\n        \"“Temporally grounding language queries in videos by contextual boundary-aware prediction\",\n        \"“Fine-grained iterative attention network for temporal language localization in videos\",\n        \"“Jointly cross- and self-modal graph attention network for query-based moment localization\",\n        \"“Reasoning step-by-step: Temporal sentence localization in videos via deep rectification-modulation network\",\n        \"“Hierarchical deep residual reasoning for temporal moment localization\",\n        \"“Multi-modal interaction graph convolutional network for temporal language localization in videos\",\n        \"“Progressively guide to attend: An iterative alignment framework for temporal sentence grounding\",\n        \"“Dct-net: A deep co-interactive transformer network for video temporal grounding\",\n        \"“Temporal modular networks for retrieving complex compositional activities in videos\",\n        \"“Multi-scale 2d temporal adjacency networks for moment localization with natural language\",\n        \"“Progressive localization networks for language-based moment localization\",\n        \"“Structured multi-level interaction network for video moment localization via language query\",\n        \"“Coarse-to-fine semantic alignment for cross-modal moment localization\",\n        \"“Vlg-net: Video-language graph matching network for video grounding\",\n        \"“Relation-aware video reading comprehension for temporal language grounding\",\n        \"“Multi-stage aggregated transformer network for temporal language localization in videos\",\n        \"“Aligned dual channel graph convolutional network for visual question answering\",\n        \"“Fast video moment retrieval\",\n        \"“Diving into the relations: Leveraging semantic and visual structures for video moment retrieval\",\n        \"“Negative sample matters: A renaissance of metric learning for temporal grounding\",\n        \"“Stcm-net: A symmetrical one-stage network for temporal language localization in videos\",\n        \"“Find and focus: Retrieve and localize video events with natural language queries\",\n        \"“Context-aware biaffine localizing network for temporal sentence grounding\",\n        \"“Dual path interaction network for video moment localization\",\n        \"“Dense events grounding in video\",\n        \"“Support-set based cross-supervision for video grounding\",\n        \"“A simple yet effective method for video temporal grounding with cross-modality attention\",\n        \"“Dense regression network for video grounding\",\n        \"“Local-global video-text interactions for temporal grounding\",\n        \"“Proposal-free video grounding with contextual pyramid network\",\n        \"“Embracing uncertainty: Decoupling and de-bias for robust temporal grounding\",\n        \"“End-to-end multi-modal video temporal grounding\",\n        \"“On pursuit of designing multi-modal transformer for video grounding\",\n        \"“Rethinking the bottom-up framework for query-based video localization\",\n        \"“Single-shot semantic matching network for moment localization in videos\",\n        \"“Boundary-sensitive pre-training for temporal localization in videos\",\n        \"“Hierarchical visual-textual graph for temporal activity localization via language\",\n        \"“Learning modality interaction for temporal sentence localization and event captioning in videos\",\n        \"“Natural language video localization: A revisit in span-based question answering framework\",\n        \"“Parallel attention network with sequence matching for video grounding\",\n        \"“Interventional video grounding with dual contrastive learning\",\n        \"“Cross interaction network for natural language guided video moment retrieval\",\n        \"“Multi-level query interaction for temporal language grounding\",\n        \"“Frame-wise cross-modal matching for video moment retrieval\",\n        \"“Temporal textual localization in video via adversarial bi-directional interaction networks\",\n        \"“Collaborative spatial-temporal interaction for language-based moment retrieval\",\n        \"“Natural language video moment localization through query-controlled temporal convolution\",\n        \"“Sneak: Synonymous sentences-aware adversarial attack on natural language video localization\",\n        \"E. Marrese-Taylor\",\n        \"“Cascaded prediction network via segment tree for temporal video grounding\",\n        \"“Local-enhanced interaction for temporal moment localization\",\n        \"“Yfcc100m: The new data in multimedia research\",\n        \"“Hollywood in homes: Crowdsourcing data collection for activity understanding\",\n        \"“The Stanford CoreNLP natural language processing toolkit\",\n        \"“Dense-captioning events in videos\",\n        \"“Activitynet: A large-scale video benchmark for human activity understanding\",\n        \"“Grounding action descriptions in videos\",\n        \"“Script data for attribute-based recognition of composite activities\",\n        \"“Mad: A scalable dataset for language grounding in videos from movie audio descriptions\",\n        \"“Rich feature hierarchies for accurate object detection and semantic segmentation\",\n        \"“Faster r-cnn: Towards real-time object detection with region proposal networks\",\n        \"“Focal loss for dense object detection\",\n        \"“Natural language object retrieval\",\n        \"“A closer look at temporal sentence grounding in videos: Dataset and metric\",\n        \"“Camg: Context-aware moment graph network for multimodal temporal activity localization via language\",\n        \"“Efficient video grounding with which-where reading comprehension\",\n        \"then perusing: A human-like framework for natural language video localization\",\n        \"“Eccl: Explicit correlation-based convolution boundary locator for moment localization\",\n        \"“Exploring optical-flow-guided motion and detection-based appearance for temporal sentence grounding\",\n        \"“Simple bert models for relation extraction and semantic role labeling\",\n        \"“Learning commonsense-aware moment-text alignment for fast video temporal grounding\",\n        \"“A hybird alignment loss for temporal moment localization with natural language\",\n        \"“Fast and robust earth mover’s distances\",\n        \"“Dual-channel localization networks for moment retrieval with natural language\",\n        \"“Learning to combine the modalities of language and video for temporal moment localization\",\n        \"“Contrastive language-action pre-training for temporal localization\",\n        \"“Learning sample importance for cross-scenario video temporal grounding\",\n        \"“Team pku-wict-mipl pic makeup temporal video grounding challenge 2022 technical report\",\n        \"“Exploring language hierarchy for video grounding\",\n        \"“Cross-modal dynamic networks for video moment retrieval with text query\",\n        \"“Language-enhanced object reasoning networks for video moment retrieval with text query\",\n        \"“You need to read again: Multi-granularity perception network for moment retrieval in videos\",\n        \"“Compositional temporal grounding with structured variational cross-graph correspondence learning\",\n        \"“Hierarchical local-global transformer for temporal sentence grounding\",\n        \"“Hisa: Hierarchically semantic associating for video temporal grounding\",\n        \"“Memory-guided semantic learning network for temporal sentence grounding\",\n        \"“Phrase-level prediction for video temporal localization\",\n        \"“Taohighlight: Commodity-aware multi-modal video highlight detection in e-commerce\",\n        \"“Attention is all you need\",\n        \"“Exploring motion and appearance information for temporal sentence grounding\",\n        \"“Soundnet: Learning sound representations from unlabeled video\",\n        \"“Simple and effective multi-paragraph reading comprehension\",\n        \"“Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding\",\n        \"“Coarse-to-fine spatial-temporal relationship inference for temporal sentence grounding\",\n        \"“Entity-aware and motion-aware transformers for language-driven action localization in videos\",\n        \"“Joint modality synergy and spatio-temporal cue purification for moment localization\",\n        \"“Multiple cross-attention for video-subtitle moment retrieval\",\n        \"“Point prompt tuning for temporally language grounding\",\n        \"“Query-aware video encoder for video moment retrieval\",\n        \"“Reducing the vision and language bias for temporal sentence grounding\",\n        \"“Stdnet: Spatio-temporal decomposed network for video grounding\",\n        \"“Towards visual-prompt temporal answering grounding in medical instructional video\",\n        \"“Video activity localisation with uncertainties in temporal boundary\",\n        \"“End-to-end sequence labeling via bi-directional lstm-cnns-crf\",\n        \"“Dual adversarial neural transfer for low-resource named entity recognition\",\n        \"“Named entity recognition as dependency parsing\",\n        \"Causal inference in statistics: A primer. John Wiley & Sons\",\n        \"Reinforcement learning: An introduction. MIT press\",\n        \"“Monte carlo sampling methods\",\n        \"“A2-rl: Aesthetics aware reinforcement learning for image cropping\",\n        \"“Adversarial video moment retrieval by jointly modeling ranking and localization\",\n        \"“Moment is important: Language-based video moment retrieval via adversarial learning\",\n        \"“Strong: Spatio-temporal reinforcement learning for cross-modal video moment localization\",\n        \"“Tripping through time: Efficient localization of activities in videos\",\n        \"“Maban: Multi-agent boundary-aware network for natural language moment retrieval\",\n        \"“Temporal action detection with structured segment networks\",\n        \"“Joint video summarization and moment localization by cross-task sample transfer\",\n        \"“Support-set bottlenecks for video-text representation learning\",\n        \"“End-to-end dense video grounding via parallel regression\",\n        \"“Gtlr: Graph-based transformer with language reconstruction for video paragraph grounding\",\n        \"“Semi-supervised video paragraph grounding with contrastive encoder\",\n        \"“Video moment retrieval with cross-modal neural architecture search\",\n        \"“Locvtp: Video-text pre-training for temporal localization\",\n        \"“Temporal moment localization via natural language by utilizing video question answers as a special variant and bypassing nlp for corpora\",\n        \"“A multi-level alignment training scheme for video-and-language grounding\",\n        \"“End-to-end object detection with transformers\",\n        \"“Explore-and-match: Bridging proposal-based and proposal-free with transformer for sentence grounding in videos\",\n        \"“Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection\",\n        \"“Look closer to ground better: Weakly-supervised temporal grounding of sentence in video\",\n        \"“Vlanet: Video-language alignment network for weakly-supervised video moment retrieval\",\n        \"“Reinforcement learning for weakly supervised temporal grounding of natural language in untrimmed videos\",\n        \"“Counterfactual contrastive learning for weakly-supervised vision-language grounding\",\n        \"“Asynce: Disentangling false-positives for weakly-supervised video grounding\",\n        \"“Visual co-occurrence alignment learning for weakly-supervised video moment retrieval\",\n        \"“Fine-grained semantic alignment network for weakly supervised temporal language grounding\",\n        \"“Cross-sentence temporal and semantic relations in video activity localisation\",\n        \"“Local correspondence network for weakly supervised temporal sentence grounding\",\n        \"“Regularized two granularity loss function for weakly supervised video moment retrieval\",\n        \"“Weakly supervised temporal adjacent network for language grounding\",\n        \"“Logan: Latent graph co-attention network for weakly-supervised video moment retrieval\",\n        \"“Explore inter-contrast between videos via composition for weakly supervised temporal sentence grounding\",\n        \"“Multi-scale self-contrastive learning with hard negative mining for weakly-supervised query-based video grounding\",\n        \"“Siamese alignment network for weakly supervised video moment retrieval\",\n        \"“Weakly supervised moment localization with natural language based on semantic reconstruction\",\n        \"“Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos\",\n        \"“Bmn: Boundary-matching network for temporal action proposal generation\",\n        \"“Towards bridging video and language by caption generation and sentence localization\",\n        \"“Weakly supervised temporal sentence grounding with gaussian-based contrastive proposal learning\",\n        \"“Weakly supervised video moment localization with contrastive negative sample mining\",\n        \"“Regularized two-branch proposal networks for weakly-supervised moment retrieval in videos\",\n        \"“Self-supervised learning for semi-supervised temporal language grounding\",\n        \"“Zero-shot natural language video localization\",\n        \"“Learning video moment retrieval without a single annotated video\",\n        \"“Unsupervised temporal video grounding with deep semantic clustering\",\n        \"“Text-based temporal localization of novel events\",\n        \"“Point-supervised video temporal grounding\",\n        \"“Video moment retrieval from text queries via single frame annotation\",\n        \"“Multi-scale 2d representation learning for weakly-supervised moment retrieval\",\n        \"inception-resnet and the impact of residual connections on learning\",\n        \"“Uncovering hidden challenges in query-based video moment retrieval\",\n        \"“Thinking inside uncertainty: Interest moment perception for diverse temporal grounding\",\n        \"“Deconfounded video moment retrieval with causal intervention\",\n        \"“Towards debiasing temporal sentence grounding in video\",\n        \"“Qvhighlights: Detecting moments and highlights in videos via natural language queries\",\n        \"“An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"“Vl-bert: Pre-training of generic visual-linguistic representations\",\n        \"“Learning transferable visual models from natural language supervision\",\n        \"“Actbert: Learning global-local video-text representations\",\n        \"“Bevt: Bert pretraining of video transformers\",\n        \"“Less is more: Clipbert for video-and-language learningvia sparse sampling\",\n        \"“Videoclip: Contrastive pre-training for zero-shot video-text understanding\",\n        \"C. Feichtenhofer\",\n        \"“Merlot: Multimodal neural script knowledge models\",\n        \"“Finding ”it”: Weakly-supervised reference-aware visual grounding in instructional videos\",\n        \"“Not all frames are equal: Weakly-supervised video grounding with contextual similarity and visual clustering losses\",\n        \"“Weakly-supervised spatio-temporally grounding natural sentence in video\",\n        \"“Activity-driven weakly-supervised spatio-temporal grounding from untrimmed videos\",\n        \"“Video object grounding using semantic roles in language description\",\n        \"“Where does it exist: Spatio-temporal video grounding for multi-form sentences\",\n        \"“Object-aware multi-branch relation networks for spatio-temporal video grounding\",\n        \"“Hierarchical attention based spatial-temporal graph-to-sequence learning for grounded video description\",\n        \"“Decoupled spatial temporal graphs for generic visual grounding\",\n        \"“Human-centric spatio-temporal video grounding with visual transformers\",\n        \"“Look at what i’m doing: Self-supervised spatial grounding of narrations in instructional videos\",\n        \"“Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding\",\n        \"“Correspondence matters for video referring expression comprehension\",\n        \"“Cross-modal target retrieval for tracking by natural language\",\n        \"“End-to-end modeling via information tree for one-shot natural language spatial video grounding\",\n        \"“Gaussian kernel-based cross modal network for spatio-temporal video grounding\",\n        \"“Stvgformer: Spatio-temporal video grounding with static-dynamic cross-modal understanding\",\n        \"“Tubedetr: Spatio-temporal video grounding with transformers\",\n        \"“Audio-visual event localization in unconstrained videos\",\n        \"“Dual attention matching for audio-visual event localization\",\n        \"“Cross-modal relation-aware networks for audio-visual event localization\",\n        \"“Cross-modal attention network for temporal inconsistent audio-visual event localization\",\n        \"“Audio-visual event localization via recursive fusion by joint co-attention\",\n        \"“Discriminative cross-modality attention network for temporal inconsistent audio-visual event localization\",\n        \"“Audio-visual event localization by learning spatial and semantic co-attention\",\n        \"“Video-guided curriculum learning for spoken video grounding\",\n        \"“Asymmetric spatio-temporal embeddings for large-scale image-to-video retrieval\",\n        \"“Localizing unseen activities in video via image query\",\n        \"“A proposal-based approach for activity image-to-video retrieval\",\n        \"“Activity image-to-video retrieval by disentangling appearance and motion\",\n        \"“Video re-localization\",\n        \"“Spatio-temporal video re-localization by warp lstm\",\n        \"“Weakly-supervised video re-localization with multiscale attention model\",\n        \"“Learning segment similarity and alignment in large-scale content based video retrieval\",\n        \"“Audio set: An ontology and human-labeled dataset for audio events\",\n        \"“Deep audio-visual speech recognition\",\n        \"“Racial disparities in automated speech recognition\",\n        \"“Deep graph random process for relational-thinking-based speech recognition\",\n        \"“Temporal localization of moments in video collections with natural language\",\n        \"“Tvr: A large-scale dataset for video-subtitle moment retrieval\",\n        \"“mTVR: Multilingual moment retrieval in videos\",\n        \"“HERO: Hierarchical encoder for Video+Language omni-representation pre-training\",\n        \"“A hierarchical multi-modal encoder for moment localization in video corpus\",\n        \"“Video corpus moment retrieval with contrastive learning\",\n        \"“Video moment retrieval with text query considering many-to-many correspondence using potentially relevant pair\",\n        \"“Text-based localization of moments in a video corpus\",\n        \"“Conquer: Contextual query-aware ranking for video corpus moment retrieval\",\n        \"“Coarse to fine: Video retrieval before moment localization\",\n        \"“Cascaded mpn: Cascaded moment proposal network for video corpus moment retrieval\",\n        \"“Cross-lingual cross-modal consolidation for effective multilingual video corpus moment retrieval\",\n        \"“Semantic association network for video corpus moment retrieval\",\n        \"“Vsrnet: End-to-end video segment retrieval with text query\",\n        \"“Caffe: Convolutional architecture for fast feature embedding\",\n        \"“Visual grounding via accumulated attention\",\n        \"“Dynamic graph attention for referring expression comprehension\",\n        \"“Transvg: End-to-end visual grounding with transformers\",\n        \"“Jointly modeling embedding and translation to bridge video and language\",\n        \"“Sea: Sentence encoder assembly for video retrieval by textual queries\",\n        \"“Multi-query video retrieval\",\n        \"“Align and tell: Boosting text-video retrieval with local alignment and fine-grained supervision\",\n        \"“TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\n        \"“TVQA: Localized\",\n        \"“Motion-appearance co-memory networks for video question answering\",\n        \"“Focal visual-text attention for memex question answering\",\n        \"“Activitynet-qa: A dataset for understanding complex web videos via question answering\",\n        \"“Multimodal transformer networks for end-to-end video-grounded dialogue systems\",\n        \"“TVQA+: Spatio-temporal grounding for video question answering\",\n        \"“Modality shifting attention network for multi-modal video question answering\",\n        \"“Self-supervised pre-training and contrastive representation learning for multiple-choice video qa\",\n        \"“Game-based video-context dialogue\",\n        \"“Dstc7-avsd: Scene-aware video-dialogue systems with dual attention\",\n        \"“Vgnmn: Video-grounded neural module networks for video-grounded dialogue systems\"\n    ],\n    \"reasoning\": \"Selected these 306 references because they represent foundational work and recent advances in Temporal Sentence Grounding in Videos: A Survey and Future Directions, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}