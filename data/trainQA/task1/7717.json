{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 119 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: The State of Human-centered NLP Technology for Fact-checking\n\nPaper Abstract: Abstract Misinformation threatens modern society by promoting distrust in science, changing narratives in public health, heightening social polarization, and disrupting democratic elections and financial markets, among a myriad of other societal harms. To address this, a growing cadre of professional fact-checkers and journalists provide high-quality investigations into purported facts. However, these largely manual efforts have struggled to match the enormous scale of the problem. In response, a growing body of Natural Language Processing (NLP) technologies have been proposed for more scalable fact-checking. Despite tremendous growth in such research, however, practical adoption of NLP technologies for fact-checking still remains in its infancy today. In this work, we review the capabilities and limitations of the current NLP technologies for fact-checking. Our particular focus is to further chart the design space for how these technologies can be harnessed and refined in order to better meet the needs of human fact-checkers. To do so, we review key aspects of NLP-based fact-checking: task formulation, dataset construction, modeling, and human-centered strategies, such as explainable models and human-in-the-loop approaches. Next, we review the efficacy of applying NLP-based fact-checking tools to assist human fact-checkers. We recommend that future research include collaboration with fact-checker stakeholders early on in NLP research, as well as incorporation of human-centered design practices in model development, in order to further guide technology development for human use and practical adoption. Finally, we advocate for more research on benchmark development supporting extrinsic evaluation of human-centered fact-checking technologies.\n\n{retrieved_text}\n\nGenerate exactly 119 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 119\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Squash report card: Improvements during State of the Union … and how humans will make our AI smarter\",\n        \"A lesson in automated journalism: Bring back the humans\",\n        \"Explainable fact checking with probabilistic answer set programming\",\n        \"Rulehub: A public corpus of rules for knowledge graphs\",\n        \"Detection of context-varying rumors on twitter through deep learning\",\n        \"Simple open stance classification for rumour analysis\",\n        \"Fighting the covid-19 infodemic in social media: A holistic perspective and a call to arms\",\n        \"Fighting the COVID-19 infodemic: Modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society\",\n        \"Where is your evidence: Improving fact-checking by justification modeling\",\n        \"Scaling up fact-checking using the wisdom of crowds\",\n        \"An ontological analysis of misinformation in online social networks\",\n        \"Feverous: Fact extraction and verification over unstructured and structured information\",\n        \"Power to the people: The role of humans in interactive machine learning\",\n        \"The challenges of online fact checking: how technology can (and can’t) help - full fact\",\n        \"Overview of the clef-2018 checkthat! lab on automatic identification and verification of political claims. task 1: Check-worthiness\",\n        \"Overview of the clef-2019 checkthat! lab: Automatic identification and verification of claims. task 1: Check-worthiness\",\n        \"Automatic fact-checking using context and discourse information\",\n        \"Generating fact checking explanations\",\n        \"Generating label cohesive and well-formed adversarial claims\",\n        \"Multifc: A real-world multi-domain dataset for evidence-based fact checking of claims\",\n        \"Investigating differences in crowdsourced news credibility assessment: Raters, tasks, and expert criteria\",\n        \"The Chicago guide to fact-checking\",\n        \"A neural model to jointly predict and explain truthfulness of statements\",\n        \"hello ai\\\": uncovering the onboarding needs of medical practitioners for human-ai collaborative decision-making\",\n        \"Improving twitter search with real-time human computation\",\n        \"Tabfact: A large-scale dataset for table-based fact verification, in: International Conference on Learning Representations\",\n        \"Dynamics of online hate and misinformation\",\n        \"Online hate: Behavioural dynamics and relationship with misinformation\",\n        \"Overview of the trec 2020 health misinformation track, in: TREC\",\n        \"A survey on computational propaganda detection, in: IJCAI\",\n        \"Recognizing textual entailment: Rational, evaluation and approaches--erratum\",\n        \"Hybrid human--machine information systems: Challenges and opportunities\",\n        \"Human-in-the-loop artificial intelligence for fighting online misinformation: Challenges and opportunities\",\n        \"Nl-augmenter: A framework for task-sensitive natural language augmentation\",\n        \"Climate-fever: A dataset for verification of real-world climate claims\",\n        \"An interactive web-based dashboard to track covid-19 in real time\",\n        \"The bootstrap method for assessing statistical accuracy\",\n        \"Fairness in information access systems\",\n        \"Emergent: a novel data-set for stance classification, in: NAACL\",\n        \"Understanding the promise and limits of automated fact-checking\",\n        \"Anatomy of a fact check: Objective practice and the contested epistemology of fact checking\",\n        \"Boundaries not drawn: Mapping the institutional roots of the global fact-checking movement\",\n        \"Fact-checking as idea and practice in journalism, in: Oxford Research Encyclopedia of Communication\",\n        \"Nela-gt-2020: A large multi-labelled news dataset for the study of misinformation in news articles\",\n        \"A Survey on Automated Fact-Checking\",\n        \"X-fact: A new benchmark dataset for multilingual fact checking, in: ACL/IJCNLP\",\n        \"A retrospective analysis of the fake news challenge stance-detection task, in: COLING\",\n        \"A survey on stance detection for mis- and disinformation identification\",\n        \"Studying effectiveness of Web search for fact checking\",\n        \"Evaluating explainable ai: Which algorithmic explanations help users predict model behavior?, in: ACL\",\n        \"Claimbuster: The first-ever end-to-end fact-checking system\",\n        \"Sampling the news producers: A large news and feature data set for the study of the complex media landscape, in: Twelfth International AAAI Conference on Web and Social Media\",\n        \"Aligning Faithful Interpretations with their Social Attribution\",\n        \"Search engines that learn from implicit feedback\",\n        \"The gulf information war| propaganda, fake news, and fake trends: The weaponization of twitter bots in the gulf crisis\",\n        \"Human and technological infrastructures of fact-checking\",\n        \"Scrutinizer: A mixed-initiative approach to large-scale, data-driven claim verification\",\n        \"Who’s in the crowd matters: Cognitive factors and beliefs predict misinformation assessment accuracy\",\n        \"Claim matching beyond english to scale global fact-checking\",\n        \"Tiplines to combat misinformation on encrypted platforms: A case study of the 2019 indian election on whatsapp\",\n        \"Toward automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection\",\n        \"Explainable automated fact-checking: A survey, in: COLING\",\n        \"Explainable automated fact-checking for public health claims, in: EMNLP\",\n        \"Annotator rationales for labeling tasks in crowdsourcing\",\n        \"Crowdsourcing truthfulness: The impact of judgment scale and assessor bias\",\n        \"Towards a science of human-ai decision making: A survey of empirical studies\",\n        \"Argument mining: A survey\",\n        \"Designing human-ai partnerships to combat misinfomation\",\n        \"Entitled to the facts: A fact-checking role for librarians\",\n        \"Trust, control strategies and allocation of function in human-machine systems\",\n        \"Trust in automation: designing for appropriate reliance\",\n        \"Misinformation and its correction: Continued influence and successful debiasing\",\n        \"AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries\",\n        \"A survey on truth discovery\",\n        \"Wanli: Worker and ai collaboration for natural language inference dataset creation\",\n        \"Detecting rumors from microblogs with recurrent neural networks, in: IJCAI\",\n        \"Mesoamerican writing systems: Propaganda, myth, and history in four ancient civilizations\",\n        \"True or False: Studying the Work Practices of Professional Fact-Checkers\",\n        \"Fact checking in community forums, in: Thirty-Second AAAI Conference on Artificial Intelligence\",\n        \"Interpretable machine learning\",\n        \"Fake news detection on social media using geometric deep learning\",\n        \"Automated fact-checking for assisting human fact-checkers, in: IJCAI\",\n        \"Checkthat! lab on fighting the covid-19 infodemic and fake news detection (proposal for a clef-2022 lab)\",\n        \"The clef-2021 checkthat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news, in: ECIR (2)\",\n        \"Focus on the facts: A news and information literacy instructional program\",\n        \"A survey on natural language processing for fake news detection, in: LREC\",\n        \"A stylometric inquiryintohyperpartisanandfakenews\",\n        \"Combining human and machine confidence in truthfulness assessment\",\n        \"Human-in-the-loop systems for truthfulness: A study of human and machine confidence\",\n        \"Language models are unsupervised multitask learners\",\n        \"The limitations of stylometry for detecting machine-generated fake news\",\n        \"Active learning literature survey\",\n        \"Assisting the human fact-checkers: Detecting all previously fact-checked claims in a document\",\n        \"Overview of the clef-2021 checkthat! lab task 1 on check-worthiness estimation in tweets and political debates, in: CLEF\",\n        \"That is a known lie: Detecting previously fact-checked claims, in: ACL\",\n        \"Sams: Human-in-the-loop approach to combat the sharing of digital misinformation, in: CEUR Workshop Proc\",\n        \"Discriminative predicate path mining for fact checking in knowledge graphs\",\n        \"Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media\",\n        \"Fake news detection on social media: A data mining perspective\",\n        \"The case for claim difficulty assessment in automatic fact checking\",\n        \"Sciclops: Detecting and contextualizing scientific claims for assisting manual fact-checking, in: Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM)\",\n        \"The many dimensions of truthfulness: Crowdsourcing misinformation assessments on a multidimensional scale\",\n        \"Global Fact 8 Pre-recorded Segment 4\",\n        \"Adversarial attacks against fact extraction and verification\",\n        \"A digital nudge to counter confirmation bias\",\n        \"The epistemology of fact checking (is still naìve): Rejoinder to amazeen\",\n        \"Crowdsourcing the research process\",\n        \"A human-centered agenda for intelligible machine learning\",\n        \"Polarization and fake news: Early warning of potential misinformation targets\",\n        \"liar, liar pants on fire\\\": A new benchmark dataset for fake news detection, in: ACL\",\n        \"Accenture at checkthat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models\",\n        \"Human-in-the-loop artificial intelligence\",\n        \"Defending against neural fake news\",\n        \"Automated fact-checking: A survey\",\n        \"Active discriminative text representation learning, in: Proceedings of the AAAI Conference on Artificial Intelligence\",\n        \"Fake news early detection: A theory-driven model\",\n        \"A survey of fake news: Fundamental theories, detection methods, and opportunities\",\n        \"Detection and resolution of rumours in social media: A survey\",\n        \"Analysing how people orient to and spread rumours in social media by looking at conversational threads\"\n    ],\n    \"reasoning\": \"Selected these 119 references because they represent foundational work and recent advances in The State of Human-centered NLP Technology for Fact-checking, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}