{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Computational Framework of Cortical Microcircuits Approximates Sign-concordant Random Backpropagation\n\nPaper Abstract: Abstract Several recent studies attempt to address the biological implausibility of the well-known backpropagation (BP) method. While promising methods such as feedback alignment, direct feedback alignment, and their variants like sign-concordant feedback alignment tackle BP’s weight transport problem, their validity remains controversial owing to a set of other unsolved issues. In this work, we answer the question of whether it is possible to realize random backpropagation solely based on mechanisms observed in neuroscience. We propose a hypothetical framework consisting of a new microcircuit architecture and its supporting Hebbian learning rules. Comprising three types of cells and two types of synaptic connectivity, the proposed microcircuit architecture computes and propagates error signals through local feedback connections and supports the training of multi-layered spiking neural networks with a globally defined spiking error function. We employ the Hebbian rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, we interpret the proposed framework from an optimization point of view and show its equivalence to sign-concordant feedback alignment. The proposed framework is benchmarked on several datasets including MNIST and CIFAR10, demonstrating promising BP-comparable accuracy.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A fast learning algorithm for deep belief nets\",\n        \"Scaling learning algorithms towards AI\",\n        \"Deep learning in neural networks: An overview\",\n        \"Deep learning, volume 1\",\n        \"Learning representations by back-propagating errors\",\n        \"Is backpropagation biologically plausible\",\n        \"Adaptive optimal control without weight transport\",\n        \"Random synaptic feedback weights support error backpropagation for deep learning\",\n        \"Direct feedback alignment provides learning in deep neural networks\",\n        \"Biologically plausible deep learning—but how far can we go with shallow networks?\",\n        \"Competitive learning: From interactive activation to adaptive resonance\",\n        \"Learning to adapt by minimizing discrepancy\",\n        \"How important is weight symmetry in backpropagation?\",\n        \"Feedback alignment in deep convolutional networks\",\n        \"Deep learning without weight transport\",\n        \"Dendritic cortical microcircuits approximate the backpropagation algorithm\",\n        \"Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits\",\n        \"Supervised and unsupervised learning with two sites of synaptic integration\",\n        \"Cortical region interactions and the functional role of apical dendrites\",\n        \"A feedback model of perceptual learning and categorization\",\n        \"Pyramidal neurons: dendritic structure and synaptic integration\",\n        \"Towards deep learning with segregated dendrites\",\n        \"Event-driven random back-propagation: Enabling neuromorphic deep learning machines\",\n        \"A solution to the learning dilemma for recurrent networks of spiking neurons\",\n        \"Pharmacology and nerve-endings\",\n        \"Cholinergic and inhibitory synapses in a pathway from motor-axon collaterals to motoneurones\",\n        \"Superspike: Supervised learning in multilayer spiking neural networks\",\n        \"Learning to live with dale’s principle: {ANN}s with separate excitatory and inhibitory units\",\n        \"Assessing the scalability of biologically-motivated deep learning algorithms and architectures\",\n        \"Philosophy of the spike: rate-based vs. spike-based theories of the brain\",\n        \"Backpropagated neighborhood aggregation for accurate training of spiking neural networks\",\n        \"Spiking neuron models: Single neurons, populations, plasticity\",\n        \"Canonical microcircuits for predictive coding\",\n        \"An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity\",\n        \"SLAYER: spike layer error reassignment in time\",\n        \"Temporal spike sequence learning via backpropagation for deep spiking neural networks\",\n        \"Rectified linear units improve restricted boltzmann machines\",\n        \"Target-specific effects of somatostatin-expressing interneurons on neocortical visual processing\",\n        \"Parvalbumin-immunoreactive neurons in the entorhinal cortex of the rat: localization, morphology, connectivity and ultrastructure\",\n        \"Entorhinal fibers form synaptic contacts on parvalbumin-immunoreactive neurons in the rat fascia dentata\",\n        \"Dendritic spikes induce ripples in parvalbumin interneurons during hippocampal sharp waves\",\n        \"Somatostatin-expressing neurons in cortical networks\",\n        \"Spatio-temporal backpropagation for training high-performance spiking neural networks\",\n        \"Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons\",\n        \"Subtype-specific plasticity of inhibitory circuits in motor cortex during motor learning\",\n        \"Parvalbumin positive dendrites co-localize with apical dendritic bundles in rat retrosplenial cortex\",\n        \"Polysynaptic subcircuits in the neocortex: spatial and temporal diversity\",\n        \"Gabaergic interneurons in the neocortex: from cellular properties to circuits\",\n        \"The organization of behavior: A neuropsychological theory\",\n        \"Unsupervised feature learning with winner-takes-all based stdp\",\n        \"Unsupervised learning of digit recognition using spike-timing-dependent plasticity\",\n        \"Stdp-based spiking deep convolutional neural networks for object recognition\",\n        \"Autapses enhance bursting and coincidence detection in neocortical pyramidal cells\",\n        \"Interneuron-specific plasticity at parvalbumin and somatostatin inhibitory synapses onto ca1 pyramidal neurons shapes hippocampal output\",\n        \"The mnist database of handwritten digits\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Decoupled weight decay regularization\",\n        \"Bioleaf: A bio-plausible learning framework for training of spiking neural networks\",\n        \"A survey on image data augmentation for deep learning\",\n        \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\",\n        \"Widespread and highly correlated somato-dendritic activity in cortical layer 5 neurons\",\n        \"High and asymmetric somato-dendritic coupling of v1 layer 5 neurons independent of visual stimulation and locomotion\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in A Computational Framework of Cortical Microcircuits Approximates Sign-concordant Random Backpropagation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}