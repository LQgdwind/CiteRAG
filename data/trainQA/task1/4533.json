{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 226 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Ego4D: Around the World in 3,000 Hours of Egocentric Video\n\nPaper Abstract: Abstract We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/\n\n{retrieved_text}\n\nGenerate exactly 226 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 226\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Github repository of the ESPNet model zoo.\",\n        \"Kaldi English GLM file.\",\n        \"NIST SRE 2000 Evaluation Plan.\",\n        \"When will you do what?-anticipating temporal occurrences of activities.\",\n        \"Deep audio-visual speech recognition.\",\n        \"The conversation: Deep audio-visual speech enhancement.\",\n        \"Self-supervised Learning of Audio-Visual Objects from Video.\",\n        \"Joint discovery of object states and manipulation actions.\",\n        \"Diagnosing error in temporal action detectors.\",\n        \"Speaker diarization: A review of recent research.\",\n        \"Robust speaker diarization for meetings.\",\n        \"VQA: Visual Question Answering.\",\n        \"Multi-modal egocentric activity recognition using audio-visual features.\",\n        \"Objects that sound.\",\n        \"wav2vec 2.0: A framework for self-supervised learning of speech representations.\",\n        \"Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions.\",\n        \"The cocktail party problem: what is it? how can it be solved? and why should animal behaviorists study it?\",\n        \"Multiple object tracking performance metrics and evaluation in a smart room environment.\",\n        \"Evaluating multiple object tracking performance: the clear mot metrics.\",\n        \"First-person action-object detection with egonet.\",\n        \"Prediction of the leadership style of an emergent leader using audio and visual nonverbal features.\",\n        \"Know Your Surroundings: Exploiting Scene Information for Object Tracking.\",\n        \"Learning 6d object pose estimation using 3d object coordinates.\",\n        \"Language models are few-shot learners\",\n        \"The yale human grasping dataset: Grasp, object, and task data in household and machine shop environments.\",\n        \"Understanding hand-object manipulation with grasp types and object attributes.\",\n        \"End-to-end object detection with transformers.\",\n        \"The AMI meeting corpus: A pre-announcement.\",\n        \"Quo vadis, action recognition? a new model and the kinetics dataset.\",\n        \"Procedure planning in instructional videos.\",\n        \"Ava-speech: A densely labeled dataset of speech activity in movies.\",\n        \"Soundspaces: Audio-visual navigation in 3d environments.\",\n        \"Audio-visual embodied navigation.\",\n        \"Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio.\",\n        \"Microsoft coco captions: Data collection and evaluation server.\",\n        \"Detection of eye contact with deep neural networks is as accurate as human experts.\",\n        \"Detecting Attended Visual Targets in Video.\",\n        \"In defence of metric learning for speaker recognition.\",\n        \"Spot the conversation: speaker diarisation in the wild.\",\n        \"VoxCeleb2: Deep Speaker Recognition.\",\n        \"Word association norms, mutual information, and lexicography.\",\n        \"The epic-kitchens dataset: Collection, challenges and baselines.\",\n        \"Rescaling egocentric vision.\",\n        \"Scaling egocentric vision: The epic-kitchens dataset.\",\n        \"You-Do, I-Learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video.\",\n        \"You-do, i-learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance.\",\n        \"A technique for computer detection and correction of spelling errors.\",\n        \"Summarization of egocentric videos: A comprehensive survey.\",\n        \"ImageNet: A large-scale hierarchical image database.\",\n        \"Superpoint: Self-supervised interest point detection and description.\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding.\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Easycom: An augmented reality dataset to support algorithms for easy communication in noisy environments\",\n        \"Boosting image-based mutual gaze detection using pseudo 3d gaze\",\n        \"Action modifiers: Learning from adverbs in instructional videos\",\n        \"Is first person vision challenging for object tracking?\",\n        \"Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation\",\n        \"Oops! predicting unintentional action in video\",\n        \"The Second DIHARD Diarization Challenge: Dataset, task, and baselines\",\n        \"The pascal visual object classes (voc) challenge\",\n        \"Activitynet: A large-scale video benchmark for human activity understanding\",\n        \"LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking\",\n        \"Multiscale vision transformers\",\n        \"CN-CELEB: a challenging Chinese speaker recognition dataset\",\n        \"Dual Attention Guided Gaze Target Detection in the Wild\",\n        \"Social interactions: A first-person perspective\",\n        \"Modeling actions through state changes\",\n        \"Slowfast networks for video recognition\",\n        \"NIST sclite sscoring toolkit\",\n        \"Datasets for face and object detection in fisheye images\",\n        \"Next-active-object prediction from egocentric videos\",\n        \"Rolling-unrolling lstms for action anticipation from first-person video\",\n        \"What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention\",\n        \"Red: Reinforced encoder-decoder networks for action anticipation\",\n        \"Learning to separate object sounds by watching unlabeled video\",\n        \"Ruohan Gao and Kristen Grauman\",\n        \"Co-separating sounds of visual objects\",\n        \"VisualVoice: Audio-visual speech separation with cross-modal consistency\",\n        \"Audio-visual speaker diarization based on spatiotemporal bayesian fusion\",\n        \"Multiview rgb-d dataset for object instance detection\",\n        \"Anticipative video transformer\",\n        \"Fast r-cnn\",\n        \"Finding action tubes\",\n        \"Action phases and mind-sets, Handbook of motivation and cognition: Foundations of social behavior\",\n        \"The” something something” video database for learning and evaluating visual common sense\",\n        \"Bidirectional lstm networks for improved phoneme classification and recognition\",\n        \"Ava: A video dataset of spatio-temporally localized atomic visual actions\",\n        \"Conformer: Convolution-augmented transformer for speech recognition\",\n        \"Georgia Gkioxari\",\n        \"Deep residual learning for image recognition\",\n        \"Detecting the moment of completion: Temporal models for localising action completion\",\n        \"spaCy: Industrial-strength Natural Language Processing in Python, 2020\",\n        \"GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild\",\n        \"Timeception for complex action recognition\",\n        \"Seeing through sounds: Predicting visual semantic segmentation results from multichannel audio signals\",\n        \"Discovering states and transformations in image collections\",\n        \"Audio-visual speech recognition using lip information extracted from side-face images\",\n        \"Perceiver: General perception with iterative attention\",\n        \"A multi-view dataset for learning multi-agent multi-task activities\",\n        \"Seeing invisible poses: Estimating 3d body pose from egocentric video\",\n        \"The kinetics human action video dataset\",\n        \"Epic-fusion: Audio-visual temporal binding for egocentric action recognition\",\n        \"Gaze360: Physically Unconstrained Gaze Estimation in the Wild\",\n        \"Joint ctc-attention based end-to-end speech recognition using multi-task learning\",\n        \"Activity forecasting\",\n        \"Testing the correlation of word error rate and perplexity\",\n        \"Nonverbal Communication in Human Interaction\",\n        \"Ikeabot: An autonomous multi-robot coordinated furniture assembly system\",\n        \"Auditory distance perception in humans: a review of cues, development, neuronal bases, and effects of sensory loss\",\n        \"Anticipating human activities using object affordances for reactive robotic response\",\n        \"Dense-captioning events in videos\",\n        \"Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks\",\n        \"The eighth visual object tracking VOT2020 challenge results, 2020\",\n        \"Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing\",\n        \"The open images dataset v4\",\n        \"Guide to the carnegie mellon university multimodal activity (cmu-mmac) database\",\n        \"Audio/video fusion for objects recognition\",\n        \"Unsupervised feature learning for 3d scene labeling\",\n        \"A hierarchical representation for future action prediction\",\n        \"Bayesian hmm clustering of x-vector sequences (vbx) in speaker diarization: theory, implementation and analysis on standard tasks\",\n        \"Discovering important people and objects for egocentric video summarization\",\n        \"Predicting important objects for egocentric video summarization\",\n        \"Connecting meeting behavior with extraversion-a systematic study\",\n        \"Binary codes capable of correcting deletions, insertions, and reversals\",\n        \"Model recommendation with virtual probes for ego-centric hand detection\",\n        \"Learning to predict gaze in egocentric video\",\n        \"Ego-exo: Transferring visual representations from third-person to first-person videos\",\n        \"Bmn: Boundary-matching network for temporal action proposal generation\",\n        \"Bsn: Boundary sensitive network for temporal action proposal generation\",\n        \"Feature Pyramid Networks for Object Detection\",\n        \"Microsoft COCO: Common objects in context\",\n        \"Forecasting human-object interaction: joint prediction of motor attention and actions in first person video\",\n        \"Future frame prediction for anomaly detection–a new baseline\",\n        \"Deep predictive coding networks for video prediction and unsupervised learning\",\n        \"Personal object discovery in first-person videos\",\n        \"Story-driven summarization for egocentric video\",\n        \"Joint prediction of activity labels and starting times in untrimmed videos\",\n        \"Laeo-net: revisiting people looking at each other in videos\",\n        \"Detecting people looking at each other in videos\",\n        \"Here’s looking at you, kid\",\n        \"Deep multi-scale video prediction beyond mean square error\",\n        \"The AMI meeting corpus\",\n        \"Deep template-based object instance detection\",\n        \"An evaluation of psychophysical models of auditory change perception\",\n        \"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\n        \"From red wine to red tomato: Composition with context\",\n        \"Deep eye contact detector: Robust eye contact bid detection using convolutional neural network\",\n        \"Trespassing the boundaries: Labelling temporal bounds for object interactions in egocentric video\",\n        \"Self-supervised generation of spatial audio for 360∘ video\",\n        \"TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild\",\n        \"Grounded human-object interaction hotspots from video\",\n        \"Attributes as operators: factorizing unseen attribute-object compositions\",\n        \"VoxCeleb: a large-scale speaker identification dataset\",\n        \"Jointly learning energy expenditures and activities using egocentric multimodal signals\",\n        \"Future event prediction: If and when\",\n        \"You2me: Inferring body pose in egocentric video via first and second person interactions\",\n        \"Direction of arrival based spatial covariance model for blind sound source separation\",\n        \"Egocom: A multi-person multi-modal egocentric communications dataset\",\n        \"Audio-visual scene analysis with self-supervised multisensory features\",\n        \"Automatic mutual gaze detection in face-to-face dyadic interaction videos\",\n        \"Specaugment: A simple data augmentation method for automatic speech recognition\",\n        \"Egocentric future localization\",\n        \"3D social saliency from head-mounted cameras\",\n        \"A review of speaker diarization: Recent advances with deep learning\",\n        \"Minimum audible angle thresholds for sources varying in both elevation and azimuth\",\n        \"Detecting activities of daily living in first-person camera views\",\n        \"The Kaldi speech recognition toolkit\",\n        \"Task-driven modular networks for zero-shot compositional learning\",\n        \"Egocentric visitors localization in cultural sites\",\n        \"The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain\",\n        \"Vision transformers for dense prediction\",\n        \"Where are they looking?\",\n        \"Yolov3: An incremental improvement\",\n        \"Behavioral Imaging and Autism\",\n        \"Faster r-cnn: Towards real-time object detection with region proposal networks\",\n        \"Predicting the future from first person (egocentric) vision: A survey\",\n        \"Ava-activespeaker: An audio-visual dataset for active speaker detection\",\n        \"Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection\",\n        \"First-person activity recognition: What are they doing to me?\",\n        \"Superglue: Learning feature matching with graph neural networks\",\n        \"Structure-from-motion revisited\",\n        \"Learning to localize sound sources in visual scenes: Analysis and applications\",\n        \"Understanding human hands in contact at internet scale\",\n        \"Learning semantic embedding spaces for slicing vegetables\",\n        \"Charades-ego: A large-scale dataset of paired third and first person videos\",\n        \"Indoor segmentation and support inference from rgbd images\",\n        \"Silero vad: Pre-trained enterprise-grade voice activity detector (VAD), number detector and language classifier\",\n        \"A weighted sparse sampling and smoothing frame transition approach for semantic fast-forward first-person videos\",\n        \"X-vectors: Robust DNN embeddings for speaker recognition\",\n        \"Ucf101: A dataset of 101 human action classes from videos in the wild\",\n        \"Egocentric shopping cart localization\",\n        \"The replica dataset: A digital replica of indoor spaces\",\n        \"Detecting engagement in egocentric video\",\n        \"Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection\",\n        \"Audio-visual event localization in unconstrained videos\",\n        \"Episodic and semantic memory\",\n        \"The 20BN-jester Dataset V1\",\n        \"Transformation-based models of video sequences\",\n        \"Attention is all you need\",\n        \"Decomposing motion and content for natural video sequence prediction\",\n        \"Anticipating visual representations from unlabeled video\",\n        \"Learning a generative model for multi-step human-object interactions from videos\",\n        \"Temporal segment networks: Towards good practices for deep action recognition\",\n        \"Fast online object tracking and segmentation: A unifying approach, 2019\",\n        \"Actions~ transformations\",\n        \"Non-local neural networks\",\n        \"Alexander Kirillov\",\n        \"Audiovisual slowfast networks for video recognition\",\n        \"Convolutional lstm network: A machine learning approach for precipitation nowcasting\",\n        \"Msr-vtt: A large video description dataset for bridging video and language\",\n        \"G-tad: Sub-graph localization for temporal action detection\",\n        \"Future person localization in first-person videos\",\n        \"Recognizing micro-actions and reactions from paired egocentric videos\",\n        \"Visual motif discovery via first-person vision\",\n        \"Deep layer aggregation\",\n        \"Audio visual attribute discovery for fine-grained object recognition\",\n        \"Span-based localizing network for natural language video localization\",\n        \"Learning 2d temporal adjacent networks formoment localization with natural language\",\n        \"Video self-stitching graph network for temporal action localization\",\n        \"The sound of pixels\",\n        \"Temporal relational reasoning in videos\",\n        \"Embracing uncertainty: Decoupling and de-bias for robust temporal grounding\",\n        \"Objects as points\",\n        \"Learning temporal transformations from time-lapse videos\",\n        \"Temporal perception and prediction in ego-centric video\",\n        \"Deep audio-visual learning: A survey\"\n    ],\n    \"reasoning\": \"Selected these 226 references because they represent foundational work and recent advances in Ego4D: Around the World in 3,000 Hours of Egocentric Video, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}