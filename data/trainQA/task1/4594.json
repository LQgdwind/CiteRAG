{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 60 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Projection-Free Stochastic Bi-level Optimization\n\nPaper Abstract: Abstract Bi-level optimization, where the objective function depends on the solution of an inner optimization problem, provides a flexible framework for solving a rich class of problems such as hyper-parameter optimization and model-agnostic meta learning. This work puts forth the first Stochastic Bi-level Frank-Wolfe (SBFW) algorithm to solve the stochastic bi-level optimization problems in a projection-free manner. We utilize a momentum-based gradient tracker that results in a sample complexity of ùí™‚Äã(œµ‚àí3)ùí™superscriptitalic-œµ3\\mathcal{O}(\\epsilon^{-3}) for convex objectives and ùí™‚Äã(œµ‚àí4)ùí™superscriptitalic-œµ4\\mathcal{O}(\\epsilon^{-4}) for non-convex objectives. The stochastic compositional optimization problems, which are a special case of bi-level optimization problems entailing the minimization of a composition of two expected-value functions, are also considered within the same rubric. The proposed Stochastic Compositional Frank-Wolfe (SCFW) algorithm is shown to achieve a sample complexity of ùí™‚Äã(œµ‚àí2)ùí™superscriptitalic-œµ2\\mathcal{O}(\\epsilon^{-2}) for convex objectives and ùí™‚Äã(œµ‚àí3)ùí™superscriptitalic-œµ3\\mathcal{O}(\\epsilon^{-3}) for non-convex objectives, at par with the state-of-the-art rates of projection-free algorithms for single-level problems. The usefulness and flexibility of SBFW and SCFW algorithms is demonstrated via extensive numerical tests. We show that SBFW outperforms the state-of-the-art methods for the problem of matrix completion with denoising, and achieve improvements of up to 82%percent8282\\% in terms of the wall-clock time required to achieve a particular level of accuracy. Furthermore, we demonstrate the improved performance of SCFW over competing projection-free variants on the policy evaluation problem in reinforcement learning.\n\n{retrieved_text}\n\nGenerate exactly 60 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 60\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Meta-learning with implicit gradients\",\n        \"Coresets via bilevel optimization for continual learning and streaming\",\n        \"Bi-level actor-critic for multi-agent coordination\",\n        \"On the iteration complexity of hypergradient computation\",\n        \"Bilevel programming for hyperparameter optimization and meta-learning\",\n        \"Bilevel Optimization\",\n        \"Approximation methods for bilevel programming\",\n        \"Provably faster algorithms for bilevel optimization\",\n        \"A single-timescale stochastic bilevel optimization method\",\n        \"Stochastic conditional gradient methods: From convex minimization to submodular maximization\",\n        \"Efficient projection-free online methods with stochastic recursive gradient.\",\n        \"One sample stochastic frank-wolfe\",\n        \"Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions\",\n        \"Accelerating stochastic composition optimization\",\n        \"A single timescale stochastic approximation method for nested stochastic optimization\",\n        \"Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization\",\n        \"Bilevel optimization: Nonasymptotic analysis and faster algorithms\",\n        \"A momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization\",\n        \"A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic\",\n        \"A near-optimal algorithm for stochastic bilevel optimization via double-momentum\",\n        \"Tighter analysis of alternating stochastic gradient method for stochastic nested problems\",\n        \"Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach\",\n        \"Revisiting frank-wolfe: Projection-free sparse convex optimization\",\n        \"An algorithm for quadratic programming\",\n        \"Zeroth and first order stochastic frank-wolfe algorithms for constrained optimization\",\n        \"Enhanced bilevel optimization via bregman distance\",\n        \"Sparse model-agnostic meta-learning algorithm for few-shot learning\",\n        \"Bilevel methods for image reconstruction\",\n        \"Low-rank matrix completion using alternating minimization\",\n        \"Low-rank quaternion approximation for color image processing\",\n        \"Multi-task feature learning\",\n        \"Transfer learning for collaborative filtering via a rating-matrix generative model\",\n        \"Simultaneous visual data completion and denoising based on tensor rank and total variation minimization and its primal-dual splitting algorithm\",\n        \"Low-rank matrix completion and denoising under poisson noise\",\n        \"Robust video denoising using low rank matrix completion\",\n        \"Matrix completion from noisy entries\",\n        \"Supervised learning of sparsity-promoting regularizers for denoising\",\n        \"Motivating bilevel approaches to filter learning: A case study\",\n        \"The theory of the market economy\",\n        \"Bilevel optimization and machine learning\",\n        \"Biadam: Fast adaptive bilevel optimization methods\",\n        \"Momentum-based variance reduction in non-convex sgd\",\n        \"Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization\",\n        \"Stochastic recursive momentum method for non-convex compositional optimization\",\n        \"Stochastic compositional gradient descent under compositional constraints\",\n        \"Variance reduced methods for non-convex composition optimization\",\n        \"Accelerated method for stochastic composition optimization with nonsmooth regularization\",\n        \"Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent\",\n        \"Robust stochastic approximation approach to stochastic programming\",\n        \"Stochastic proximal gradient descent with acceleration techniques\",\n        \"An optimal method for stochastic composite optimization\",\n        \"Variance-reduced and projection-free stochastic optimization\",\n        \"Stochastic frank-wolfe methods for nonconvex optimization\",\n        \"Conservative stochastic optimization with expectation constraints\",\n        \"Stochastic conditional gradient++:(non) convex minimization and continuous submodular maximization\",\n        \"A stochastic compositional gradient method using markov samples\",\n        \"First-order methods in optimization\",\n        \"Convergence rate of frank-wolfe for non-convex objectives\",\n        \"A stochastic composite gradient method with incremental variance reduction\",\n        \"Investigating practical linear temporal difference learning\"\n    ],\n    \"reasoning\": \"Selected these 60 references because they represent foundational work and recent advances in Projection-Free Stochastic Bi-level Optimization, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}