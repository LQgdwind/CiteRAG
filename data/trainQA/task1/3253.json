{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: One-way communication complexity and non-adaptive decision trees1footnote 11footnote 1This paper includes and improves upon results from an earlier unpublished manuscript of one of the authors [San17].\n\nPaper Abstract: Abstract We study the relationship between various one-way communication complexity measures of a composed function with the analogous decision tree complexity of the outer function. We consider two gadgets: the AND function on 2 inputs, and the Inner Product on a constant number of inputs. More generally, we show the following when the gadget is Inner Product on 2â€‹b2ğ‘2b input bits for all bâ‰¥2ğ‘2b\\geq 2, denoted ğ–¨ğ–¯ğ–¨ğ–¯\\mathsf{IP}. â€¢ If fğ‘“f is a total Boolean function that depends on all of its nğ‘›n input bits, then the bounded-error one-way quantum communication complexity of fâˆ˜ğ–¨ğ–¯ğ‘“ğ–¨ğ–¯f\\circ\\mathsf{IP} equals Î©â€‹(nâ€‹(bâˆ’1))Î©ğ‘›ğ‘1\\Omega(n(b-1)). â€¢ If fğ‘“f is a partial Boolean function, then the deterministic one-way communication complexity of fâˆ˜ğ–¨ğ–¯ğ‘“ğ–¨ğ–¯f\\circ\\mathsf{IP} is at least Î©â€‹(bâ‹…ğ–£dtâ†’â€‹(f))Î©â‹…ğ‘superscriptsubscriptğ–£dtâ†’ğ‘“\\Omega(b\\cdot\\mathsf{D}_{\\mathrm{dt}}^{\\rightarrow}(f)), where ğ–£dtâ†’â€‹(f)superscriptsubscriptğ–£dtâ†’ğ‘“\\mathsf{D}_{\\mathrm{dt}}^{\\rightarrow}(f) denotes non-adaptive decision tree complexity of fğ‘“f. To prove our quantum lower bound, we first show a lower bound on the VC-dimension of fâˆ˜ğ–¨ğ–¯ğ‘“ğ–¨ğ–¯f\\circ\\mathsf{IP}. We then appeal to a result of Klauck [STOCâ€™00], which immediately yields our quantum lower bound. Our deterministic lower bound relies on a combinatorial result independently proven by Ahlswede and Khachatrian [Adv. Appl. Math.â€™98], and Frankl and Tokushige [Comb.â€™99]. It is known due to a result of Montanaro and Osborne [arXivâ€™09] that the deterministic one-way communication complexity of fâˆ˜ğ–·ğ–®ğ–±ğ‘“ğ–·ğ–®ğ–±f\\circ\\mathsf{XOR} equals the non-adaptive parity decision tree complexity of fğ‘“f. In contrast, we show the following when the inner gadget is the AND function on 2 input bits. â€¢ There exists a function for which even the quantum non-adaptive AND decision tree complexity of fğ‘“f is exponentially large in the deterministic one-way communication complexity of fâˆ˜ğ– ğ–­ğ–£ğ‘“ğ– ğ–­ğ–£f\\circ\\mathsf{AND}. â€¢ However, for symmetric functions fğ‘“f, the non-adaptive AND decision tree complexity of fğ‘“f is at most quadratic in the (even two-way) communication complexity of fâˆ˜ğ– ğ–­ğ–£ğ‘“ğ– ğ–­ğ–£f\\circ\\mathsf{AND}. In view of the first bullet, a lower bound on non-adaptive AND decision tree complexity of fğ‘“f does not lift to a lower bound on one-way communication complexity of fâˆ˜ğ– ğ–­ğ–£ğ‘“ğ– ğ–­ğ–£f\\circ\\mathsf{AND}. The proof of the first bullet above uses the well-studied Odd-Max-Bit function. For the second bullet, we first observe a connection between the one-way communication complexity of fğ‘“f and the MÃ¶bius sparsity of fğ‘“f, and then give a lower bound on the MÃ¶bius sparsity of symmetric functions. An upper bound on the non-adaptive AND decision tree complexity of symmetric functions follows implicitly from prior work on combinatorial group testing; for the sake of completeness, we include a proof of this result. It is well known that the rank of the communication matrix of a function Fğ¹F is an upper bound on its deterministic one-way communication complexity. This bound is known to be tight for some Fğ¹F. However, in our final result we show that this is not the case when F=fâˆ˜ğ– ğ–­ğ–£ğ¹ğ‘“ğ– ğ–­ğ–£F=f\\circ\\mathsf{AND}. More precisely we show that for all fğ‘“f, the deterministic one-way communication complexity of F=fâˆ˜ğ– ğ–­ğ–£ğ¹ğ‘“ğ– ğ–­ğ–£F=f\\circ\\mathsf{AND} is at most (rankâ€‹(MF))â€‹(1âˆ’Î©â€‹(1))ranksubscriptğ‘€ğ¹1Î©1(\\mathrm{rank}(M_{F}))(1-\\Omega(1)), where MFsubscriptğ‘€ğ¹M_{F} denotes the communication matrix of Fğ¹F.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A composition theorem for randomized query complexity\",\n        \"Power of decision trees with monotone queries\",\n        \"The diametric theorem in hamming spacesâ€”optimal anticodes\",\n        \"A tight composition theorem for the randomized query complexity of partial functions: Extended abstract\",\n        \"Communication complexity lower bounds by polynomials\",\n        \"The polynomial method in circuit complexity\",\n        \"Randomized query complexity of sabotaged and composed functions\",\n        \"Dual lower bounds for approximate degree and markov-bernstein inequalities\",\n        \"Quantum complexity theory\",\n        \"On computation and communication with small bias\",\n        \"Query-to-communication lifting using low-discrepancy gadgets\",\n        \"A survey on nonadaptive group testing algorithms through the angle of decoding\",\n        \"Simulation theorems via pseudo-random properties\",\n        \"A survey of superimposed code theory\",\n        \"How limited interaction hinders real communication (and what it means for proof and circuit complexity)\",\n        \"The Erdos-Ko-Rado theorem for integer sequences\",\n        \"Monotone circuit lower bounds from resolution\",\n        \"A composition theorem for conical juntas\",\n        \"Rectangles are nonnegative juntas\",\n        \"A composition theorem for randomized query complexity via max-conflict complexity\",\n        \"Toward better formula lower bounds: The composition of a function and a universal relation\",\n        \"Deterministic communication vs. partition number\",\n        \"Query-to-communication lifting for BPP\",\n        \"Structure of protocols for XOR functions\",\n        \"Negative weights make adversaries stronger\",\n        \"Optimality of linear sketching under modular updates\",\n        \"On quantum and probabilistic communication: Las vegas and one-way protocols\",\n        \"Log-rank and lifting for and-functions\",\n        \"Linear sketching over f_2\",\n        \"Communication complexity\",\n        \"One-way quantum communication complexity with inner product gadget\",\n        \"Lifting theorems for equality\",\n        \"Communication is bounded by root of rank\",\n        \"Lattices, mÃ¶bius functions and communication complexity\",\n        \"On the communication complexity of XOR functions\",\n        \"Nonadaptive quantum query complexity\",\n        \"A composition theorem for decision tree complexity\",\n        \"Quantum Computation and Quantum Information\",\n        \"Reflections for quantum query algorithms\",\n        \"Separation of the monotone NC hierarchy\",\n        \"One-way communication and non-adaptive decision tree\",\n        \"Fourier sparsity and dimension\",\n        \"Making polynomials robust to noise\",\n        \"Approximating the AND-OR tree\",\n        \"Properties and applications of boolean function composition\",\n        \"On the uniform convergence of relative frequencies of events to their probabilities\",\n        \"Quantum communication and complexity\",\n        \"On the communication complexity of AND functions\",\n        \"Raz-mckenzie simulation with the inner product gadget\",\n        \"Some complexity questions related to distributive computing (preliminary report)\",\n        \"Pooled testing with replication as a mass testing strategy for the covid-19 pandemics\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in One-way communication complexity and non-adaptive decision trees1footnote 11footnote 1This paper includes and improves upon results from an earlier unpublished manuscript of one of the authors [San17]., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}