{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 55 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Quantum statistical query learning\n\nPaper Abstract: Abstract We propose a learning model called the quantum statistical learning (ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ}) model, which extends the ğ–²ğ–°ğ–²ğ–°\\mathsf{SQ} learning model introduced by Kearns [Kea98] to the quantum setting. Our model can be also seen as a restriction of the quantum PAC learning model: here, the learner does not have direct access to quantum examples, but can only obtain estimates of measurement statistics on them. Theoretically, this model provides a simple yet expressive setting to explore the power of quantum examples in machine learning. From a practical perspective, since simpler operations are required, learning algorithms in the ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ} model are more feasible for implementation on near-term quantum devices. We prove a number of results about the ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ} learning model. We first show that parity functions, Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n)-juntas and polynomial-sized DNF formulas are efficiently learnable in the ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ} model, in contrast to the classical setting where these problems are provably hard. This implies that many of the advantages of quantum PAC learning can be realized even in the more restricted quantum ğ–²ğ–°ğ–²ğ–°\\mathsf{SQ} learning model. It is well-known that weak statistical query dimension, denoted by ğ–¶ğ–¾ğ–ºğ—„ğ–²ğ–°ğ–£ğ–¨ğ–¬â€‹(C)ğ–¶ğ–¾ğ–ºğ—„ğ–²ğ–°ğ–£ğ–¨ğ–¬C\\mathsf{WeakSQDIM}({\\pazocal C}), characterizes the complexity of learning a concept class CC{\\pazocal C} in the classical ğ–²ğ–°ğ–²ğ–°\\mathsf{SQ} model. We show that logâ¡(ğ–¶ğ–¾ğ–ºğ—„ğ–²ğ–°ğ–£ğ–¨ğ–¬â€‹(C))ğ–¶ğ–¾ğ–ºğ—„ğ–²ğ–°ğ–£ğ–¨ğ–¬C\\log(\\mathsf{WeakSQDIM}({\\pazocal C})) is a lower bound on the complexity of ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ} learning, and furthermore it is tight for certain concept classes CC{\\pazocal C}. Additionally, we show that this quantity provides strong lower bounds for the small-bias quantum communication model under product distributions. Finally, we introduce the notion of private quantum PAC learning, in which a quantum PAC learner is required to be differentially private. We show that learnability in the ğ–°ğ–²ğ–°ğ–°ğ–²ğ–°\\mathsf{QSQ} model implies learnability in the quantum private PAC model. Additionally, we show that in the private PAC learning setting, the classical and quantum sample complexities are equal, up to constant factors.\n\n{retrieved_text}\n\nGenerate exactly 55 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 55\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Sample-efficient learning of quantum many-body systems\",\n        \"The learnability of quantum states\",\n        \"Online learning of quantum states\",\n        \"Two new results about quantum exact learning\",\n        \"General bounds on statistical query learning and PAC learning with noise via hypothesis boosting\",\n        \"Dense quantum coding and a lower bound for 1-way quantum automata\",\n        \"Gentle measurement of quantum states and differential privacy\",\n        \"Improved bounds on quantum learning algorithms\",\n        \"Adiabatic quantum state generation\",\n        \"Guest column: A survey of quantum learning theory\",\n        \"Optimal quantum sample complexity of learning algorithms\",\n        \"Bounds on the sample complexity for private learning and private data release\",\n        \"Entanglement is necessary for optimal quantum property testing\",\n        \"Practical privacy: the SuLQ framework\",\n        \"Learnability and the Vapnik-Chervonenkis dimension\",\n        \"Weakly learning DNF and characterizing statistical query learning using Fourier analysis\",\n        \"Learning DNF over the uniform distribution using a quantum example oracle\",\n        \"Quantum SDP solvers: Large speed-ups, optimality, and applications to quantum learning\",\n        \"Characterizing the sample complexity of private learners\",\n        \"Order-revealing encryption and the hardness of private learning\",\n        \"Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning\",\n        \"Calibrating noise to sensitivity in private data analysis\",\n        \"Privacy-preserving datamining on vertically partitioned databases\",\n        \"The algorithmic foundations of differential privacy\",\n        \"Differential privacy\",\n        \"Learning DNF expressions from Fourier spectrum\",\n        \"Statistical query learning\",\n        \"A general characterization of the statistical query complexity\",\n        \"Statistical algorithms and a lower bound for detecting planted cliques\",\n        \"Sample complexity bounds on differentially private learning via communication complexity\",\n        \"Learning with Errors is easy with quantum samples\",\n        \"A hard-core predicate for all one-way functions\",\n        \"The optimal sample complexity of PAC learning\",\n        \"New bounds on classical and quantum one-way communication complexity\",\n        \"Efficient noise-tolerant learning from statistical queries\",\n        \"Lower bounds for quantum communication complexity\",\n        \"What can we learn privately?\",\n        \"Learning decision trees using the Fourier spectrum\",\n        \"Sample efficient algorithms for learning quantum channels in pac model and the approximate state discrimination problem\",\n        \"On randomized one-round communication complexity\",\n        \"Quantum recommendation systems\",\n        \"Quantum algorithms for supervised and unsupervised machine learning\",\n        \"Analysis of Boolean Functions\",\n        \"Cryptography from learning parity with noise\",\n        \"On lattices, learning with errors, random linear codes, and cryptography\",\n        \"Statistical queries and statistical algorithms: Foundations and applications\",\n        \"Quantum support vector machine for big data classification\",\n        \"Stabiliser states are efficiently pac-learnable\",\n        \"Halfspace matrices\",\n        \"Characterizing statistical query learning: simplified notions and proofs\",\n        \"A quantum-inspired classical algorithm for recommendation systems\",\n        \"The complexity of differential privacy\",\n        \"A theory of the learnable\",\n        \"On the uniform convergence of relative frequencies of events to their probabilities\",\n        \"On learning correlated boolean functions using statistical queries\"\n    ],\n    \"reasoning\": \"Selected these 55 references because they represent foundational work and recent advances in Quantum statistical query learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}