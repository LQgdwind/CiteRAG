{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 216 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks\n\nPaper Abstract: Abstract A longstanding goal of artificial intelligence is to create artificial agents capable of learning to perform tasks that require sequential decision making. Importantly, while it is the artificial agent that learns and acts, it is still up to humans to specify the particular task to be performed. Classical task-specification approaches typically involve humans providing stationary reward functions or explicit demonstrations of the desired tasks. However, there has recently been a great deal of research energy invested in exploring alternative ways in which humans may guide learning agents that may, e.g., be more suitable for certain tasks or require less human effort. This survey provides a high-level overview of five recent machine learning frameworks that primarily rely on human guidance apart from pre-specified reward functions or conventional, step-by-step action demonstrations. We review the motivation, assumptions, and implementation of each framework, and we discuss possible future research directions.\n\n{retrieved_text}\n\nGenerate exactly 216 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 216\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Apprenticeship learning via inverse reinforcement learning\",\n        \"Autonomous helicopter aerobatics through apprenticeship learning\",\n        \"Agent-agnostic human-in-the-loop reinforcement learning\",\n        \"Trajectory-tracking and path-following of underactuated autonomous vehicles with parametric modeling uncertainty\",\n        \"Accelerated robot learning via human brain signals\",\n        \"Programming by feedback\",\n        \"Interactive teaching strategies for agent training\",\n        \"Modular multitask reinforcement learning with policy sketches\",\n        \"Dqn-tamer: Human-in-the-loop reinforcement learning with intractable feedback\",\n        \"A survey of robot learning from demonstration\",\n        \"A survey of inverse reinforcement learning: Challenges, methods and progress\",\n        \"Deep reinforcement learning from policy-dependent human feedback\",\n        \"Playing hard exploration games by watching youtube\",\n        \"The option-critic architecture\",\n        \"A framework for behavioural cloning\",\n        \"Transmission of aggression through imitation of aggressive models\",\n        \"Recent advances in hierarchical reinforcement learning\",\n        \"The arcade learning environment: An evaluation platform for general agents\",\n        \"Humanoid robot learning and game playing using pc-based vision\",\n        \"Learning human ergonomic preferences for handovers\",\n        \"Preference learning along multiple criteria: A game-theoretic perspective\",\n        \"Batch active preference-based learning of reward functions\",\n        \"The green choice: Learning and influencing human decisions on shared roads\",\n        \"Active preference-based gaussian process regression for reward learning\",\n        \"Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences\",\n        \"Infinite time horizon maximum causal entropy inverse reinforcement learning\",\n        \"End to end learning for self-driving cars\",\n        \"Salient object detection: A benchmark\",\n        \"Emotion and reinforcement: affective facial expressions facilitate robot learning\",\n        \"Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations\",\n        \"Preference-based evolutionary direct policy search\",\n        \"Intrinsic and extrinsic effects on image memorability\",\n        \"Mit saliency benchmark\",\n        \"Where should saliency models look next?\",\n        \"What do different evaluation metrics tell us about saliency models?\",\n        \"Learning by imitation: A hierarchical approach\",\n        \"Incremental learning of gestures by imitation in a humanoid robot\",\n        \"Realtime multi-person 2d pose estimation using part affinity fields\",\n        \"Trajectory tracking control of a four-wheel differentially driven mobile robot\",\n        \"Policy shaping with human teachers\",\n        \"Injective state-image mapping facilitates visual adversarial imitation learning\",\n        \"Gaze training by modulated dropout improves imitation learning\",\n        \"Robot navigation in crowds by graph convolutional networks with attention learned from human gaze\",\n        \"Deep reinforcement learning from human preferences\",\n        \"End-to-end driving via conditional imitation learning\",\n        \"Active reward learning from critiques\",\n        \"The empathic framework for task learning from implicit human feedback\",\n        \"How do drivers allocate their potential attention? driving fixation prediction via convolutional neural networks\",\n        \"Hierarchical reinforcement learning with the maxq value function decomposition\",\n        \"Learning actionable representations from visual observations\",\n        \"Imitating latent policies from observation\",\n        \"Survey of imitation learning for robotic manipulation\",\n        \"Pan Z (2009) Motion capture in robotics review. In: 2009 IEEE International Conference on Control and Automation\",\n        \"Abbeel P (2016) Guided cost learning: Deep inverse optimal control via policy optimization. In: International Conference on Machine Learning\",\n        \"Stoica I (2018) Parametrized hierarchical procedures for neural programming. International Conference on Learning Representations 2018\",\n        \"Goldberg K (2019) Multi-task hierarchical imitation learning for home automation\",\n        \"Rao RP (2010) Imitation learning with hierarchical actions. In: 2010 IEEE 9th International Conference on Development and Learning\",\n        \"Levine S (2018) Learning robust rewards with adverserial inverse reinforcement learning. In: International Conference on Learning Representations\",\n        \"Park SH (2012) Preference-based reinforcement learning: a formal framework and a policy iteration algorithm. Machine learning 89(1-2):123–156\",\n        \"Makar R (2006) Hierarchical multi-agent reinforcement learning. Autonomous Agents and Multi-Agent Systems 13(2):197–229\",\n        \"et al. (2016) A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters 1(2):661–667\",\n        \"Waytowich NR (2019) Efficiently combining human demonstrations and interventions for safe training of autonomous systems in real-time\",\n        \"Niekum S (2019) One-shot learning of multi-step tasks from observation via activity localization in auxiliary video\",\n        \"Bengio Y (2014) Generative adversarial nets. In: Advances in neural information processing systems\",\n        \"Goswami U (2008) Cognitive development: The learning brain\",\n        \"Thomaz AL (2013) Policy shaping: Integrating human feedback with reinforcement learning. In: Advances in neural information processing systems\",\n        \"Lopes M (2014) Interactive learning from unlabeled instructions\",\n        \"Babuska R (2012) A survey of actor-critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems\",\n        \"Campbell M (2019) Hybrid reinforcement learning with expert state sequences. In: Proceedings of the AAAI Conference on Artificial Intelligence\",\n        \"Levine S (2018) Learning invariant feature spaces to transfer skills with reinforcement learning\",\n        \"Hausman K (2020) Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In: Conference on Robot Learning\",\n        \"Russell S (2016) Cooperative inverse reinforcement learning\",\n        \"Stone P (2017) Grounded action transformation for robot learning in simulation\",\n        \"Lim JJ (2017) Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets\",\n        \"Pugeault N (2019) Understanding and visualizing deep visual saliency models\",\n        \"Precup D (2018a) Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning\",\n        \"Meger D (2018b) Deep reinforcement learning that matters. In: Proceedings of the AAAI Conference on Artificial Intelligence\",\n        \"Taylor ME (2019) A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems 33(6):750–797\",\n        \"Ermon S (2016) Generative adversarial imitation learning. In: Advances in Neural Information Processing Systems\",\n        \"Austerweil JL (2016) Showing versus doing: Teaching by demonstration. In: Advances in neural information processing systems\",\n        \"Komura T (2016) A deep learning framework for character motion synthesis and editing. ACM Transactions on Graphics (TOG) 35(4):138\",\n        \"Sabharwal A (2017) Tabletgaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets\",\n        \"Sato Y (2020) Mutual context network for jointly estimating egocentric gaze and action. IEEE Transactions on Image Processing 29:7795–7806\",\n        \"Jayne C (2017) Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR) 50(2):21\",\n        \"Amodei D (2018) Reward learning from human preferences and demonstrations in atari. In: Advances in Neural Information Processing Systems\",\n        \"Schaal S (2002) Movement imitation with nonlinear dynamical systems in humanoid robots\",\n        \"Schaal S (2011) Trajectory formation for imitation with nonlinear dynamical systems\",\n        \"Stone P (2001) A social reinforcement learning agent. In: Proceedings of the fifth international conference on Autonomous agents\",\n        \"Niebur E (1998) A model of saliency-based visual attention for rapid scene analysis\",\n        \"et al. (2019) Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science 364(6443):859–865\",\n        \"Yu Y (2020) Offline imitation learning with a misspecified simulator. Advances in Neural Information Processing Systems 33\",\n        \"Gay G (2017) Accurately interpreting clickthrough data as implicit feedback. In: ACM SIGIR Forum\",\n        \"International Foundation for Autonomous Agents and Multiagent Systems\",\n        \"Dasgupta S (2018) Internal model from observations for reward shaping. arXiv preprint arXiv:180601267\",\n        \"Sanchez-Gonzalez A\",\n        \"Stone P (2009) Interactively shaping agents via human reinforcement: The tamer framework\",\n        \"Peters J (2013) Reinforcement learning in robotics: A survey. The International Journal of Robotics Research 32(11):1238–1274\",\n        \"Barto A (2012) Robot learning from demonstration by constructing skill trees. The International Journal of Robotics Research 31(3):360–375\",\n        \"URL https://openreview.net/forum?id=Hk4fpoA5Km\",\n        \"Torralba A (2016) Eye tracking for everyone. In: Proceedings of the IEEE conference on computer vision and pattern recognition\",\n        \"Goldberg K (2017) Ddco: Discovery of deep continuous options for robot learning from demonstrations. In: Conference on Robot Learning\",\n        \"Peters J (2015) Towards learning hierarchical skills for multi-phase manipulation tasks\",\n        \"Tenenbaum J (2016) Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation\",\n        \"Goldberg K (2016) Robot grasping in clutter: Using a hierarchy of supervisors for learning from demonstrations\",\n        \"Daumé H (2018) Hierarchical imitation and reinforcement learning. In: International Conference on Machine Learning\",\n        \"Hinton G (2015) Deep learning. nature 521(7553):436\",\n        \"Abbeel P (2016) End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research 17(1):1334–1373\",\n        \"Kushman N (2016a) Neural program lattices. International Conference on Learning Representations 2018\",\n        \"Hung H (2016b) Using informative behavior to increase engagement while learning from human reward\",\n        \"Yuille AL (2014) The secrets of salient object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\",\n        \"Rehg JM (2018) In the eye of beholder: Joint learning of gaze and actions in first person video\",\n        \"Shi BE (2019) A gaze model improves autonomous driving. In: Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications\",\n        \"Liu Y (2020) Hilonet: Hierarchical imitation learning from non-aligned observations. arXiv preprint arXiv:201102671\",\n        \"Levine S (2018) Imitation from observation: Learning to imitate behaviors from raw video via context translation\",\n        \"Roberts DL (2014) Learning something from nothing: Leveraging implicit human feedback strategies\",\n        \"Roberts DL (2016) Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning\",\n        \"Littman ML (2017) Interactive learning from policy-dependent human feedback\",\n        \"Bowling M (2018) Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents\",\n        \"Sminchisescu C (2014) Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition\",\n        \"Heess N (2017) Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:170702201\",\n        \"Hebert M (2016) Shuffle and learn: unsupervised learning using temporal order verification. In: European conference on computer vision\",\n        \"et al. (2015) Human-level control through deep reinforcement learning. Nature 518(7540):529–533\",\n        \"Miller D (2015) Interactive hierarchical task learning from a single demonstration\",\n        \"Levine S (2018) Data-efficient hierarchical reinforcement learning. In: Advances in neural information processing systems\",\n        \"Levine S (2017) Combining self-supervised learning and imitation for vision-based rope manipulation\",\n        \"Chetouani M (2020) Interactively shaping robot behaviour with unlabeled human instructions. Auton Agents Multi Agent Syst 34(2):35\",\n        \"Kawato M (2004) Learning from demonstration and adaptation of biped locomotion. Robotics and autonomous systems 47(2-3):79–91\",\n        \"Barto AG (2015) Learning grounded finite-state representations from unstructured demonstrations\",\n        \"et al. (2018) An algorithmic perspective on imitation learning. Foundations and Trends® in Robotics 7(1-2):1–179\",\n        \"Sadigh D (2019) Learning reward functions by integrating human demonstrations and preferences. In: Proceedings of Robotics: Science and Systems (RSS)\",\n        \"(2018) Predicting the driver’s focus of attention: the dr (eye) ve project\",\n        \"Ferrari V (2014) Training object class detectors from eye tracking data. In: European conference on computer vision\",\n        \"In: Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence\",\n        \"Hilliges O (2018) Deep pictorial gaze estimation. In: Proceedings of the European Conference on Computer Vision (ECCV)\",\n        \"Darrell T (2018) Zero-shot visual imitation. In: International Conference on Learning Representations\",\n        \"Stone P (2020) Ridm: Reinforced inverse dynamics modeling for learning from a single observed demonstration\",\n        \"van de Panne M (2018a) Deepmimic: Example-guided deep reinforcement learning of physics-based character skills\",\n        \"Levine S (2018b) Sfv: Reinforcement learning of physical skills from videos. In: SIGGRAPH Asia 2018 Technical Papers\",\n        \"Sutton RS (2011) Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning\",\n        \"Neumann G (2018) Sample and feedback efficient hierarchical reinforcement learning from human preferences\",\n        \"Pomerleau DA (1989) Alvinn: An autonomous land vehicle in a neural network\",\n        \"Yip MC (2019) Adversarial imitation via variational inverse reinforcement learning. In: International Conference on Learning Representations\",\n        \"Malik J (2020) State-only imitation learning for dexterous manipulation. arXiv preprint arXiv:200404650\",\n        \"Dani A (2018) Gaze and motion information fusion for human intention inference\",\n        \"De Freitas N (2015) Neural programmer-interpreters. arXiv preprint arXiv:151106279\",\n        \"Walter MR (2020) Concurrent training improves the performance of behavioral cloning from observation. arXiv preprint arXiv:200801205\",\n        \"Bagnell D (2010) Efficient reductions for imitation learning\",\n        \"Bagnell D (2011) A reduction of imitation learning and structured prediction to no-regret online learning\",\n        \"Seshia SA (2017) Active preference-based learning of reward functions. In: Robotics: Science and Systems\",\n        \"Niekum S (2020) Efficiently guiding imitation learning algorithms with human gaze. arXiv preprint arXiv:200212500\",\n        \"Kawaguchi A (2019) Sample efficient imitation learning for continuous control. In: International Conference on Learning Representations\",\n        \"Schaal S (1999) Is imitation learning the route to humanoid robots? Trends in cognitive sciences 3(6):233–242\",\n        \"Finn C (2020) Reinforcement learning with videos: Combining offline observations with interaction. arXiv preprint arXiv:201106507\",\n        \"Moritz P (2015) Trust region policy optimization. In: International Conference on Machine Learning\",\n        \"Brain G (2018) Time-contrastive networks: Self-supervised learning from video\",\n        \"Kitani KM (2018) Directed-info gail: Learning hierarchical policies from unsegmented demonstrations using directed information\",\n        \"Zhuang N (2018) Egocentric activity prediction via event modulated attention. In: Proceedings of the European Conference on Computer Vision (ECCV)\",\n        \"Webb R (2017) Learning from simulated and unsupervised images through adversarial training\",\n        \"Stentz A (2010) Learning from demonstration for autonomous navigation in complex unstructured terrain\",\n        \"Van Den Driessche G\",\n        \"et al. (2017) Mastering the game of go without human knowledge. Nature 550(7676):354\",\n        \"et al. (2018) A general reinforcement learning algorithm that masters chess\",\n        \"Skinner BF (1938) The behavior of organisms: An experimental analysis\",\n        \"Sutskever I (2017) Third-person imitation learning. International Conference on Learning Representations\",\n        \"Lanz O (2019) Lsta: Long short-term attention for egocentric action recognition\",\n        \"Bagnell D (2019) Provably efficient imitation learning from observation alone. In: International Conference on Machine Learning\",\n        \"Barto AG (1998) Reinforcement learning: An introduction\",\n        \"Barto AG (2018) Reinforcement learning: An introduction. MIT press\",\n        \"Singh S (1999) Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence 112(1-2):181–211\",\n        \"Mansour Y (2000) Policy gradient methods for reinforcement learning with function approximation\",\n        \"Borji A (2019) Digging deeper into egocentric gaze prediction. In: 2019 IEEE Winter Conference on Applications of Computer Vision\",\n        \"Villaseñor-Pineda L (2010) Dynamic reward shaping: training a robot by voice. In: Ibero-American conference on artificial intelligence\",\n        \"Breazeal C (2008) Teachable robots: Understanding human teaching behavior to build more effective robot learners\",\n        \"Tassa Y (2012) Mujoco: A physics engine for model-based control. In: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\n        \"Stone P (2018a) Behavioral cloning from observation. In: Proceedings of the 27th International Joint Conference on Artificial Intelligence\",\n        \"Stone P (2018b) Generative adversarial imitation from observation. arXiv preprint arXiv:180706158\",\n        \"Stone P (2019a) Sample-efficient adversarial imitation learning from observation. arXiv preprint arXiv:190607374\",\n        \"Stone P (2019c) Imitation learning from video by leveraging proprioception\",\n        \"Stone P (2019d) Recent advances in imitation learning from observation\",\n        \"Kavukcuoglu K (2017) Feudal networks for hierarchical reinforcement learning\",\n        \"et al. (2019) Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature 575(7782):350–354\",\n        \"Wei P (2019) 3d human pose machines with self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 42(5):1069–1082\",\n        \"Deng W (2018) Deep visual domain adaptation: A survey. Neurocomputing 312:135–153\",\n        \"Borji A (2018) Revisiting video saliency: A large-scale benchmark and a new model\",\n        \"Freitas N (2016) Dueling network architectures for deep reinforcement learning. In: International Conference on Machine Learning\",\n        \"Stone P (2018) Deep tamer: Interactive agent shaping in high-dimensional state spaces\",\n        \"Dayan P (1992) Q-learning. Machine learning 8(3-4):279–292\",\n        \"Lawhern VJ (2018) Cycle-of-learning for autonomous systems from human interaction. arXiv preprint arXiv:180809572\",\n        \"Tadepalli P (2012) A bayesian approach for policy learning from trajectory preference queries. In: Advances in neural information processing systems\",\n        \"Neumann G (2016) Model-free preference-based reinforcement learning. In: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence\",\n        \"Fürnkranz J (2017) A survey of preference-based reinforcement learning methods. The Journal of Machine Learning Research 18(1):4945–4990\",\n        \"Bulling A (2015) Rendering of eyes for eye-shape registration and gaze estimation\",\n        \"Hausman K (2020) Learning to interactively learn and assist. In: Proceedings of the AAAI Conference on Artificial Intelligence\",\n        \"Ryoo MS (2020) Model-based behavioral cloning with future image similarity learning. In: Conference on Robot Learning\",\n        \"Whitney D (2018) Predicting driver attention in critical situations. In: Asian conference on computer vision\",\n        \"Whitney D (2020) Periphery-fovea multi-resolution driving model guided by human attention\",\n        \"Savarese S (2018a) Neural task programming: Learning to generalize across hierarchical tasks\",\n        \"Sivakumar R (2020) Playing games with implicit human feedback. In: Workshop on Reinforcement Learning in Games\",\n        \"Zhao Q (2014) Predicting human gaze beyond pixels. Journal of vision 14(1):28–28\",\n        \"Wu S (2018b) Shared multi-task imitation learning for indoor self-navigation. In: 2018 IEEE Global Communications Conference (GLOBECOM)\",\n        \"Gao S (2018c) Gaze prediction in dynamic 360 immersive videos. In: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\",\n        \"Gan C (2019) Imitation learning from observations by minimizing inverse dynamics disagreement. In: Advances in Neural Information Processing Systems\",\n        \"Kim JH (1999) Sliding mode control for trajectory tracking of nonholonomic wheeled mobile robots\",\n        \"Darrell T (2018) Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:180504687\",\n        \"Ballard DH (2018a) Agil: Learning attention from human for visuomotor tasks. In: Proceedings of the European Conference on Computer Vision (ECCV)\",\n        \"Stone P (2019) Leveraging human guidance for deep reinforcement learning tasks\",\n        \"Hayhoe M (2020a) Human gaze assisted artificial intelligence: A review. In: IJCAI: proceedings of the conference\",\n        \"Ballard D (2020b) Atari-head: Atari human eye-tracking and demonstration dataset. In: Proceedings of the AAAI Conference on Artificial Intelligence\",\n        \"Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition\",\n        \"Bulling A (2017) Mpiigaze: Real-world dataset and deep appearance-based gaze estimation\",\n        \"Gao S (2018b) Saliency detection in 360 videos. In: Proceedings of the European Conference on Computer Vision (ECCV)\",\n        \"Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks\",\n        \"Dey AK (2008) Maximum entropy inverse reinforcement learning. In: AAAI\",\n        \"Dey AK (2010) Modeling interaction via the principle of maximum causal entropy. In: ICML\",\n        \"Pinheiro PO (2018) Reinforced imitation learning from observations. NeurIPS 2018 Workshop\",\n        \"Qu Y (2018) Gaze-informed egocentric action recognition for memory aid systems. IEEE Access 6:12894–12904\"\n    ],\n    \"reasoning\": \"Selected these 216 references because they represent foundational work and recent advances in Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}