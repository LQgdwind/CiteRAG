{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 45 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Polynomial-Time Power-Sum Decomposition of Polynomials\n\nPaper Abstract: Abstract We give efficient algorithms for finding power-sum decomposition of an input polynomial Pâ€‹(x)=âˆ‘iâ©½mpiâ€‹(x)dğ‘ƒğ‘¥subscriptğ‘–ğ‘šsubscriptğ‘ğ‘–superscriptğ‘¥ğ‘‘P(x)=\\sum_{i\\leqslant m}p_{i}(x)^{d} with component pisubscriptğ‘ğ‘–p_{i}s. The case of linear pisubscriptğ‘ğ‘–p_{i}s is equivalent to the well-studied tensor decomposition problem while the quadratic case occurs naturally in studying identifiability of non-spherical Gaussian mixtures from low-order moments. Unlike tensor decomposition, both the unique identifiability and algorithms for this problem are not well-understood. For the simplest setting of quadratic pisubscriptğ‘ğ‘–p_{i}s and d=3ğ‘‘3d=3, prior work of [GHK15] yields an algorithm only when mâ©½O~â€‹(n)ğ‘š~ğ‘‚ğ‘›m\\leqslant\\widetilde{O}(\\sqrt{n}). On the other hand, the more general recent result of [GKS20] builds an algebraic approach to handle any m=nOâ€‹(1)ğ‘šsuperscriptğ‘›ğ‘‚1m=n^{O(1)} components but only when dğ‘‘d is large enough (while yielding no bounds for d=3ğ‘‘3d=3 or even d=100ğ‘‘100d=100) and only handles an inverse exponential noise. Our results obtain a substantial quantitative improvement on both the prior works above even in the base case of d=3ğ‘‘3d=3 and quadratic pisubscriptğ‘ğ‘–p_{i}s. Specifically, our algorithm succeeds in decomposing a sum of mâˆ¼O~â€‹(n)similar-toğ‘š~ğ‘‚ğ‘›m\\sim\\widetilde{O}(n) generic quadratic pisubscriptğ‘ğ‘–p_{i}s for d=3ğ‘‘3d=3 and more generally the dğ‘‘dth power-sum of mâˆ¼n2â€‹d/15similar-toğ‘šsuperscriptğ‘›2ğ‘‘15m\\sim n^{2d/15} generic degree-Kğ¾K polynomials for any Kâ©¾2ğ¾2K\\geqslant 2. Our algorithm relies only on basic numerical linear algebraic primitives, is exact (i.e., obtain arbitrarily tiny error up to numerical precision), and handles an inverse polynomial noise when the pisubscriptğ‘ğ‘–p_{i}s have random Gaussian coefficients. Our main tool is a new method for extracting the linear span of pisubscriptğ‘ğ‘–p_{i}s by studying the linear subspace of low-order partial derivatives of the input Pğ‘ƒP. For establishing polynomial stability of our algorithm in average-case, we prove inverse polynomial bounds on the smallest singular value of certain correlated random matrices with low-degree polynomial entries that arise in our analyses. Since previous techniques only yield significantly weaker bounds, we analyze the smallest singular value of matrices by studying the largest singular value of certain deviation matrices via graph matrix decomposition and the trace moment method.\n\n{retrieved_text}\n\nGenerate exactly 45 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 45\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A spectral algorithm for latent dirichlet allocation\",\n        \"Tensor decompositions for learning latent variable models (A survey for ALT)\",\n        \"Graph matrices: Norm bounds and applications\",\n        \"Smoothed analysis of tensor decompositions\",\n        \"On zeros of a polynomial in a finite grid\",\n        \"A nearly tight sum-of-squares lower bound for the planted clique problem\",\n        \"Dictionary learning and tensor decomposition via the sum-of-squares method\",\n        \"Sum-of-squares proofs and the quest toward optimal algorithms\",\n        \"Partial derivatives in arithmetic complexity and beyond\",\n        \"Semialgebraic proofs and efficient algorithm design\",\n        \"Learning mixtures of gaussians in high dimensions\",\n        \"Sum-of-squares lower bounds for sherrington-kirkpatrick via planted affine planes\",\n        \"Learning sums of powers of low-degree polynomials in the non-degenerate case\",\n        \"Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares Algorithms\",\n        \"Foundations of the parafac procedure: Models and conditions for anâ€ explanatoryâ€ multi-modal factor analysis\",\n        \"Tensor rank is NP-complete\",\n        \"Learning mixtures of spherical gaussians: moment methods and spectral decompositions\",\n        \"Algorithmic thresholds for refuting random polynomial systems\",\n        \"SoS and planted clique: Tight analysis of MPW moments at all degrees and an optimal lower bound at degree four\",\n        \"Tensor principal component analysis via sum-of-square proofs\",\n        \"Speeding up sum-of-squares for tensor decomposition and planted sparse vectors\",\n        \"Sum-of-squares lower bounds for sparse independent set\",\n        \"Exact nuclear norm, completion and decomposition for random overcomplete tensors via degree-4 sos\",\n        \"Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics\",\n        \"Reconstruction of non-degenerate homogeneous depth three circuits\",\n        \"A super-polynomial lower bound for regular arithmetic formulas\",\n        \"Global optimization with polynomials and the problem of moments\",\n        \"Fourth-order cumulant-based blind identification of underdetermined mixtures\",\n        \"A decomposition for three-way arrays\",\n        \"Tensor methods in statistics\",\n        \"Learning nonsingular phylogenies and hidden Markov models\",\n        \"Lifting sum-of-squares lower bounds: degree-2 to degree-4\",\n        \"Polynomial-time tensor decompositions with sum-of-squares\",\n        \"Settling the polynomial learnability of mixtures of Gaussians\",\n        \"Spectral methods from tensor networks\",\n        \"Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization\",\n        \"Machinery for proving sum-of-squares lower bounds on certification problems\",\n        \"The sample complexity of multi-reference alignment\",\n        \"The Littlewood-Offord Problem and invertibility of random matrices\",\n        \"A survey of lower bounds in arithmetic complexity\",\n        \"Matrix Perturbation Theory\",\n        \"Concentration and moment inequalities for polynomials of independent random variables\",\n        \"Arithmetic circuits: A survey of recent results and open questions\",\n        \"Inverse Littlewood-Offord theorems and the condition number of random discrete matrices\",\n        \"Random matrices: The distribution of the smallest singular values\"\n    ],\n    \"reasoning\": \"Selected these 45 references because they represent foundational work and recent advances in Polynomial-Time Power-Sum Decomposition of Polynomials, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}