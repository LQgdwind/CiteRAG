{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 67 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Nearly-Linear Time Seeded Extractors with Short Seeds\n\nPaper Abstract: Abstract Seeded extractors are fundamental objects in pseudorandomness and cryptography, and a deep line of work has designed polynomial-time seeded extractors with nearly-optimal parameters. However, existing constructions of seeded extractors with short seed length and large output length run in time Ω​(n​log⁡(1/ε))Ω𝑛1𝜀\\Omega(n\\log(1/\\varepsilon)) and often slower, where n𝑛n is the input source length and ε𝜀\\varepsilon is the error of the extractor. Since cryptographic applications of extractors require ε𝜀\\varepsilon to be small, the resulting runtime makes these extractors unusable in practice. Motivated by this, we explore constructions of strong seeded extractors with short seeds computable in nearly-linear time O​(n​logc⁡n)𝑂𝑛superscript𝑐𝑛O(n\\log^{c}n), for any error ε𝜀\\varepsilon. We show that an appropriate combination of modern condensers and classical approaches for constructing seeded extractors for high min-entropy sources yields strong extractors for n𝑛n-bit sources with any min-entropy k𝑘k and any target error ε𝜀\\varepsilon with seed length d=O​(log⁡(n/ε))𝑑𝑂𝑛𝜀d=O(\\log(n/\\varepsilon)) and output length m=(1−η)​k𝑚1𝜂𝑘m=(1-\\eta)k for an arbitrarily small constant η>0𝜂0\\eta>0, running in nearly-linear time, after a reasonable one-time preprocessing step (finding a primitive element of 𝔽qsubscript𝔽𝑞\\mathds{F}_{q} with q=poly⁡(n/ε)𝑞poly𝑛𝜀q=\\operatorname{poly}(n/\\varepsilon) a power of 222) that is only required when k<2C​log∗⁡n⋅log2⁡(n/ε)𝑘⋅superscript2𝐶superscript𝑛superscript2𝑛𝜀k<2^{C\\log^{*}\\!n}\\cdot\\log^{2}(n/\\varepsilon), for a constant C>0𝐶0C>0 and log∗superscript\\log^{*}\\! the iterated logarithm, and which can be implemented in time polylog⁡(n/ε)polylog𝑛𝜀\\operatorname{polylog}(n/\\varepsilon) under mild conditions on q𝑞q. As a second contribution, we give an instantiation of Trevisan’s extractor that can be evaluated in truly linear time in the RAM model, as long as the number of output bits is at most nlog⁡(1/ε)​polylog⁡(n)𝑛1𝜀polylog𝑛\\frac{n}{\\log(1/\\varepsilon)\\operatorname{polylog}(n)}. Previous fast implementations of Trevisan’s extractor ran in O~​(n)~𝑂𝑛\\widetilde{O}(n) time in this setting. In particular, these extractors directly yield privacy amplification protocols with the same time complexity and output length, and communication complexity equal to their seed length.\n\n{retrieved_text}\n\nGenerate exactly 67 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 67\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Low-degree polynomials extract from local sources\",\n        \"Simple constructions of almost k𝑘k-wise independent random variables\",\n        \"Low-degree polynomials are good extractors\",\n        \"Explicit expanders of every degree and size\",\n        \"Generalized privacy amplification\",\n        \"Sparse extractor families for all the entropy\",\n        \"Fast modular transforms\",\n        \"Topics in (and out) the theory of computing: Lecture notes\",\n        \"Streaming computation of combinatorial objects\",\n        \"Unbiased bits from sources of weak randomness and probabilistic communication complexity\",\n        \"Randomness extraction in AC0 and with small locality\",\n        \"Balls and bins: Smaller hash families and faster evaluation\",\n        \"An algorithm for the machine calculation of complex Fourier series\",\n        \"Randomness extractors in AC0 and NC1: Optimal up to constant factors\",\n        \"Efficient polynomial time algorithms computing industrial-strength primitive roots\",\n        \"Extensions to the method of multiplicities, with applications to Kakeya sets and mergers\",\n        \"Nearly optimal pseudorandomness from hardness\",\n        \"Fuzzy extractors: How to generate strong keys from biometrics and other noisy data\",\n        \"Trevisan’s extractor in the presence of quantum side information\",\n        \"Derandomization with minimal memory footprint\",\n        \"Faster integer multiplication\",\n        \"Practical randomness amplification and privatisation with implementations on quantum computers\",\n        \"Cryptomite: A versatile and user-friendly library of randomness extractors\",\n        \"Hilbert functions and low-degree randomness extractors\",\n        \"A Chernoff bound for random walks on expander graphs\",\n        \"Unbalanced expanders and randomness extractors from Parvaresh–Vardy codes\",\n        \"On randomness extraction in AC0\",\n        \"Randomness-efficient sampling within NC1superscriptNC1\\\\textnormal{NC}^{1}\",\n        \"Upper tail estimates with combinatorial proofs\",\n        \"Affine extractors and AC0-parity\",\n        \"More efficient privacy amplification with less random seeds via dual universal hash function\",\n        \"Constant-depth circuits for arithmetic in finite fields of characteristic two\",\n        \"Faster polynomial multiplication over finite fields using cyclotomic coefficient rings\",\n        \"Integer multiplication in time O​(n​log​n)𝑂𝑛log𝑛O(n\\\\mathrm{log}\\\\,n)\",\n        \"Polynomial multiplication over finite fields in time O​(n​log​n)𝑂𝑛log𝑛O(n\\\\mathrm{log}\\\\,n)\",\n        \"Class of constructive asymptotically good algebraic codes\",\n        \"Unbalanced expanders from multiplicity codes\",\n        \"Non-malleable condensers for arbitrary min-entropy, and almost optimal protocols for privacy amplification\",\n        \"Extractors: Optimal up to constant factors\",\n        \"Hyper-encryption against space-bounded adversaries from on-line strong extractors\",\n        \"A modular framework for randomness extraction based on Trevisan’s construction\",\n        \"Fast pseudorandomness for independence and load balancing\",\n        \"Privacy amplification secure against active adversaries\",\n        \"Small-bias probability spaces: Efficient constructions and applications\",\n        \"Extracting randomness: A survey and new constructions\",\n        \"Randomness is linear in space\",\n        \"An exposition of Bourgain’s 2-source extractor\",\n        \"Extracting all the randomness and reducing the error in Trevisan’s extractors\",\n        \"Extracting randomness via repeated condensing\",\n        \"Bounds for dispersers, extractors, and depth-two superconcentrators\",\n        \"Schnelle multiplikation von polynomen über körpern der charakteristik 2\",\n        \"Searching for primitive roots in finite fields\",\n        \"On primitive elements in finite fields and on elliptic curves\",\n        \"Linear-time encodable and decodable error-correcting codes\",\n        \"Simple extractors for all min-entropies and a new pseudorandom generator\",\n        \"Computing with very weak random sources\",\n        \"Explicit, almost optimal, epsilon-balanced codes\",\n        \"Extractors and pseudorandom generators\",\n        \"Leftover hashing against quantum side information\",\n        \"Better condensers and new extractors from Parvaresh-Vardy codes\",\n        \"Extractors from Reed–Muller codes\",\n        \"Constructing locally computable extractors and cryptosystems in the bounded-storage model\",\n        \"Pseudorandomness\",\n        \"Modern Computer Algebra\",\n        \"Expanders that beat the eigenvalue bound: Explicit construction and applications\",\n        \"Near-optimal averaging samplers\",\n        \"Randomness-optimal oblivious sampling\"\n    ],\n    \"reasoning\": \"Selected these 67 references because they represent foundational work and recent advances in Nearly-Linear Time Seeded Extractors with Short Seeds, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}