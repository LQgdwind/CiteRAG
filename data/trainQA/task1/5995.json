{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 266 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis\n\nPaper Abstract: Abstract Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in order to understand how to effectively apply techniques such as vectorization. We also formally analyze GNN pipelining, and we generalize the established Message-Passing class of GNN models to cover arbitrary pipeline depths, facilitating future optimizations. Finally, we investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines. The outcomes of our analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and opportunities for further research into efficient GNN computations. Our work will help to advance the design of future GNNs.\n\n{retrieved_text}\n\nGenerate exactly 266 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 266\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Computing graph neural networks: A survey from algorithms to accelerators\",\n        \"Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing\",\n        \"Graph convolutional neural network for action recognition: A comprehensive survey\",\n        \"A scalable processing-in-memory accelerator for parallel graph processing\",\n        \"A survey on graph neural networks for knowledge graph completion\",\n        \"Diffusion-convolutional neural networks\",\n        \"Hardware acceleration of graph neural networks\",\n        \"Approximating betweenness centrality\",\n        \"Graph partitioning and graph clustering, volume 588\",\n        \"Efficient data loader for fast sampling-based gnn training on large graphs\",\n        \"Bridging the gap between spectral and spatial domains in graph neural networks\",\n        \"Mg-gcn: Scalable multi-gpu gcn training framework\",\n        \"Gnnmark: A benchmark suite to characterize graph neural network training on gpus\",\n        \"Relational inductive biases, deep learning, and graph networks\",\n        \"A modular benchmarking infrastructure for high-performance and reproducible deep learning\",\n        \"Demystifying parallel and distributed deep learning: An in-depth concurrency analysis\",\n        \"11 label propagation and quadratic criterion\",\n        \"Enabling High-Performance Large-Scale Irregular Computations\",\n        \"High-performance parallel graph coloring with strong guarantees on work, depth, and quality\",\n        \"High-performance routing with multipathing and path diversity in ethernet and hpc networks\",\n        \"Slim graph: Practical lossy graph compression for approximate graph processing, storage, and analytics\",\n        \"Communication-efficient jaccard similarity for high-performance distributed genome comparisons\",\n        \"Graphminesuite: Enabling high-performance and programmable graph mining algorithms with set algebra\",\n        \"Sisa: Set-centric instruction set architecture for graph mining on processing-in-memory systems\",\n        \"Practice of streaming processing of dynamic graphs: Concepts, models, and systems\",\n        \"Substream-centric maximum matchings on fpga\",\n        \"Motif prediction with graph neural networks\",\n        \"Slim noc: A low-diameter on-chip network topology for high energy efficiency and scalability\",\n        \"Fault tolerance for remote memory access programming models\",\n        \"Slim fly: A cost effective low-diameter network topology\",\n        \"Accelerating irregular computations with hardware transactional memory and active messages\",\n        \"Active access: A mechanism for high-performance distributed data-centric computations\",\n        \"Slimsell: A vectorizable graph representation for breadth-first search\",\n        \"Demystifying graph databases: Analysis and taxonomy of data organization, system designs, and graph queries\",\n        \"To push or to pull: On reducing communication and synchronization in graph computations\",\n        \"Fatpaths: Routing in supercomputers and data centers when shortest paths fall short\",\n        \"Graph processing on fpgas: Taxonomy, survey, challenges\",\n        \"Graph neural networks with convolutional arma filters\",\n        \"Models of Computation, Theoretical, pages 1150â€“1158\",\n        \"Pre x sums and their applications\",\n        \"Programming parallel algorithms\",\n        \"Parallel Algorithms\",\n        \"Scaling graph neural networks with approximate pagerank\",\n        \"Hyperanf: Approximating the neighbourhood function of very large graphs on a budget\",\n        \"Kadabra is an adaptive algorithm for betweenness via random approximation\",\n        \"Graph kernels\",\n        \"Residual gated graph convnets\",\n        \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges\",\n        \"Geometric deep learning: going beyond euclidean data\",\n        \"Towards scalable parallel hypergraph partitioning\",\n        \"Recent advances in graph partitioning\",\n        \"Graph partitioning for scalable distributed graph computations\",\n        \"A comprehensive survey of graph embedding: Problems, techniques, and applications\",\n        \"Dgcl: an efficient communication library for distributed gnn training\",\n        \"A fine-grain hypergraph model for 2d decomposition of sparse matrices\",\n        \"Efficient scaling of dynamic graph neural networks\",\n        \"Graph mining: Laws, generators, and algorithms\",\n        \"Machine learning on graphs: A model and comprehensive taxonomy\",\n        \"Approximating the minimum spanning tree weight in sublinear time\",\n        \"Stochastic training of graph convolutional networks with variance reduction\",\n        \"Fastgcn: fast learning with graph convolutional networks via importance sampling\",\n        \"Rubik: A hierarchical architecture for efficient graph neural network training\",\n        \"Bridging the gap between spatial and spectral domains: A unified framework for graph neural networks\",\n        \"fusegnn: accelerating graph convolutional neural network training on gpgpu\",\n        \"Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks\",\n        \"Mining graph data\",\n        \"Sebs: A serverless benchmark suite for function-as-a-service computing\",\n        \"A survey on network embedding\",\n        \"A survey on knowledge graph embedding: Approaches, applications and benchmarks\",\n        \"Advancing mathematics by guiding human intuition with ai\",\n        \"Transformations of high-level synthesis codes for high-performance computing\",\n        \"Convolutional neural networks on graphs with fast localized spectral filtering\",\n        \"Parallel hypergraph partitioning for scientific computing\",\n        \"Network-accelerated non-contiguous memory transfers\",\n        \"A min-max cut algorithm for graph partitioning and data clustering\",\n        \"Approximate evaluation of label-constrained reachability queries\",\n        \"Benchmarking graph neural networks\",\n        \"Lasas: an aggregated search based graph matching approach\",\n        \"Fast graph representation learning with pytorch geometric\",\n        \"Sign: Scalable inception graph neural networks\",\n        \"P3: Distributed deep graph learning at scale\",\n        \"Understanding the design space of sparse/dense multiphase dataflows for mapping graph neural networks on spatial accelerators\",\n        \"Gemnet: Universal directional graph neural networks for molecules\",\n        \"Better approximation of betweenness centrality\",\n        \"Parallel minimum cuts in near-linear work and low depth\",\n        \"I-gcn: A graph convolutional network accelerator with runtime locality enhancement through islandization\",\n        \"Awb-gcn: A graph convolutional network accelerator with runtime workload rebalancing\",\n        \"Communication Avoiding and Overlapping for Numerical Linear Algebra\",\n        \"Enabling highly-scalable remote memory access programming with mpi-3 one sided\",\n        \"The processing-in-memory paradigm: Mechanisms to enable adoption\",\n        \"Learning combinatorial node labeling algorithms\",\n        \"Communication-avoiding parallel minimum cuts and connected components\",\n        \"Neural message passing for quantum chemistry\",\n        \"I/o-optimal cache-oblivious sparse matrix-sparse matrix multiplication\",\n        \"Benchmarking a new paradigm: An experimental analysis of a real processing-in-memory architecture\",\n        \"Powergraph: distributed graph-parallel computation on natural graphs\",\n        \"Deep learning\",\n        \"The parallel bgl: A generic library for distributed graph computations\",\n        \"node2vec: Scalable feature learning for networks\",\n        \"Simdram: a framework for bit-serial simd processing using dram\",\n        \"Graph representation learning\",\n        \"Representation learning on graphs: Methods and applications\",\n        \"Inductive representation learning on large graphs\",\n        \"Spreadgnn: Serverless multi-task federated learning for graph neural networks\",\n        \"Tare: Task-adaptive in-situ reram computing for graph learning\",\n        \"An improved spectral graph partitioning algorithm for mapping parallel computations\",\n        \"Efficient distribution for deep learning on large graphs, 2021\",\n        \"spin: High-performance streaming processing in the network\",\n        \"Energy, memory, and runtime tradeoffs for implementing collective communication operations\",\n        \"Cyclic pattern kernels for predictive graph mining\",\n        \"Graph classification by mixture of diverse experts\",\n        \"Open graph benchmark: Datasets for machine learning on graphs\",\n        \"Featgraph: A flexible and efficient backend for graph neural network systems\",\n        \"Heterogeneous graph transformer\",\n        \"Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks\",\n        \"Understanding and bridging the gaps in current gnn performance optimizations\",\n        \"Text level graph neural network for text classification\",\n        \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\",\n        \"Improving the accuracy, scalability, and performance of graph neural networks with roc\",\n        \"A survey of frequent subgraph mining algorithms\",\n        \"Graph neural network for traffic forecasting: A survey\",\n        \"Highly accurate protein structure prediction with alphafold\",\n        \"Global min-cuts in rnc, and other ramifications of a simple min-cut algorithm\",\n        \"Analysis of multilevel graph partitioning\",\n        \"Mathematical foundations of the graphblas\",\n        \"Sbv-cut: Vertex-cut based graph partitioning using structural balance vertices\",\n        \"Accelerating gnn training with locality-aware partial execution\",\n        \"Greta: Hardware optimized graph processing for gnns\",\n        \"Grip: a graph neural network accelerator architecture\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Predict then propagate: Graph neural networks meet personalized pagerank\",\n        \"Diffusion improves graph learning\",\n        \"A survey on graph kernels\",\n        \"One weird trick for parallelizing convolutional neural networks\",\n        \"Optimistic parallelism requires abstractions\",\n        \"Pebbles, graphs, and a pinch of combinatorics: Towards tight i/o lower bounds for statically analyzable programs\",\n        \"On the parallel i/o optimality of linear algebra kernels: near-optimal lu factorization\",\n        \"Red-blue pebbling revisited: near optimal parallel matrix-matrix multiplication\",\n        \"Graph neural networks meet neural-symbolic computing: A survey and perspective\",\n        \"Training graph neural networks with 1000 layers\",\n        \"Hardware acceleration for gcns via bidirectional fusion\",\n        \"Gcnax: A flexible and energy-efficient accelerator for graph convolutional neural networks\",\n        \"Semi-supervised graph classification: A hierarchical graph perspective\",\n        \"Taming unbalanced training workloads in deep learning with partial collective operations\",\n        \"Chimera: efficiently training large-scale neural networks with bidirectional pipelines\",\n        \"Engn: A high-throughput and energy-efficient accelerator for large graph neural networks\",\n        \"Accelerating spmm kernel with cache-first edge sampling for gnn inference\",\n        \"Shentu: processing multi-trillion edge graphs on millions of cores in seconds\",\n        \"Learning entity and relation embeddings for knowledge graph completion\",\n        \"Pagraph: Scaling gnn training on large graphs via computation-aware caching\",\n        \"G3: when graph neural networks meet parallel graph processing systems on gpus\",\n        \"Bgl: Gpu-efficient gnn training by optimizing graph data i/o and preprocessing\",\n        \"Sampling methods for efficient training of graph convolutional networks: A survey\",\n        \"Challenges in parallel graph processing\",\n        \"Neugraph: parallel deep neural network computation on large graphs\",\n        \"Mixture of experts: a literature survey\",\n        \"Distgnn: Scalable distributed training for large-scale graph neural networks\",\n        \"Large graph convolutional network training with gpu-oriented data communication architecture\",\n        \"A graph placement methodology for fast chip design\",\n        \"Gnnie: Gnn inference engine with load-balancing and graph-specific caching\",\n        \"Geometric deep learning on graphs and manifolds using mixture model cnns\",\n        \"Sequential aggregation and rematerialization: Distributed full-batch training of graph neural networks on large graphs\",\n        \"A fast parallel algorithm to compute the rank of a matrix over an arbitrary field\",\n        \"Processing Data Where It Makes Sense: Enabling In-Memory Computation\",\n        \"A modern primer on processing in memory\",\n        \"Pipedream: generalized pipeline parallelism for dnn training\",\n        \"Learning convolutional neural networks for graphs\",\n        \"Damov: A new methodology and benchmark suite for evaluating data movement bottlenecks\",\n        \"Accelerating deep learning frameworks with micro-batches\",\n        \"The pagerank citation ranking: Bringing order to the web\",\n        \"Deepwalk: Online learning of social representations\",\n        \"Theoretical foundations of graph neural networks\",\n        \"Learning mesh-based simulation with graph networks\",\n        \"Optimizing sparse matrix multiplications for graph neural networks\",\n        \"Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks\",\n        \"Fast approximation of betweenness centrality through sampling\",\n        \"Abra: Approximating betweenness centrality in static and dynamic graphs with rademacher averages\",\n        \"Fast approximation algorithms for the diameter and radius of sparse graphs\",\n        \"An interactive data repository with visual analytics\",\n        \"The future is big graphs: a community view on graph processing systems\",\n        \"Learning to simulate complex physics with graph networks\",\n        \"A survey on the expressive power of graph neural networks\",\n        \"High-performance distributed rma locks\",\n        \"Evaluating the cost of atomic operations on modern architectures\",\n        \"Scalable graph neural network training: The case for sampling\",\n        \"Rowclone: fast and energy-efficient in-dram bulk data copy and initialization\",\n        \"Ambit: In-memory accelerator for bulk bitwise operations using commodity dram technology\",\n        \"A survey of heterogeneous information network analysis\",\n        \"Graph processing on gpus: A survey\",\n        \"An o (logn) parallel connectivity algorithm\",\n        \"Complex network analysis using parallel approximate motif counting\",\n        \"Scaling betweenness centrality using communication-efficient sparse matrix multiplication\",\n        \"Tradeoffs between synchronization, communication, and work in parallel linear algebra computations\",\n        \"Sparse Tensor Algebra as a Parallel Programming Model\",\n        \"Local sgd converges fast and communicates little\",\n        \"Asynchronous distributed-memory triangle counting and lcc with rma caching\",\n        \"Adaptive load balancing for parallel gnn training\",\n        \"Learning multiagent communication with backpropagation\",\n        \"Towards efficient point cloud graph neural networks through architectural simplification\",\n        \"Programming abstractions for data locality\",\n        \"A survey on modern deep neural network for traffic prediction: Trends, methods and challenges\",\n        \"Attention-based graph neural network for semi-supervised learning\",\n        \"Dorylus: Affordable, scalable, and accurate gnn training over billion-edge graphs\",\n        \"Pcgcn: Partition-centric processing for accelerating graph convolutional network\",\n        \"Reducing communication in graph neural network training\",\n        \"A bridging model for parallel computation\",\n        \"Attention is all you need\",\n        \"Graph attention networks\",\n        \"Marius++: Large-scale training of graph neural networks on a single machine\",\n        \"Bns-gcn: Efficient full-graph training of graph convolutional networks with partition-parallelism and random boundary node sampling sampling\",\n        \"Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communication\",\n        \"Structural deep network embedding\",\n        \"Flexgraph: a flexible and efficient distributed framework for gnn training\",\n        \"Deep graph library: A graph-centric, highly-performant package for graph neural networks\",\n        \"Knowledge graph embedding: A survey of approaches and applications\",\n        \"A survey on heterogeneous graph embedding: methods, techniques, applications and sources\",\n        \"Heterogeneous graph attention network\",\n        \"Gunrock: A high-performance graph processing library on the GPU\",\n        \"Gnnadvisor: An efficient runtime system for gnn acceleration on gpus\",\n        \"Qgtc: Accelerating quantized gnn via gpu tensor core\",\n        \"Dynamic graph cnn for learning on point clouds\",\n        \"Molecular contrastive learning of representations via graph neural networks\",\n        \"Gnn-pim: A processing-in-memory architecture for graph neural networks\",\n        \"Empirical analysis of performance bottlenecks in graph neural network training and inference with gpus\",\n        \"Simplifying graph convolutional networks\",\n        \"Graph neural networks in recommender systems: a survey\",\n        \"Learning with partially absorbing random walks\",\n        \"Seastar: vertex-centric programming for graph neural networks\",\n        \"A comprehensive survey on graph neural networks\",\n        \"A survey on heterogeneous network representation learning\",\n        \"Capsule graph neural network\",\n        \"How powerful are graph neural networks?\",\n        \"Characterizing and understanding gcns on gpu\",\n        \"Hygcn: A gcn accelerator with hybrid architecture\",\n        \"Heterogeneous network representation learning: A unified framework with survey and benchmark\",\n        \"Graph convolutional neural networks for web-scale recommender systems\",\n        \"Hierarchical graph representation learning with differentiable pooling\",\n        \"Gcod: Graph convolutional network acceleration via dedicated algorithm and accelerator co-design\",\n        \"Explainability in graph neural networks: A taxonomic survey\",\n        \"Decoupling the depth and scope of graph neural networks\",\n        \"Graphact: Accelerating gcn training on cpu-fpga heterogeneous platforms\",\n        \"Graphsaint: Graph sampling based inductive learning method\",\n        \"Hardware acceleration of large scale gcn inference\",\n        \"Heterogeneous graph neural network\",\n        \"Agl: a scalable system for industrial-purpose graph machine learning\",\n        \"Understanding gnn computational graph: A coordinated computation, io, and memory perspective\",\n        \"Gaan: Gated attention networks for learning on large and spatiotemporal graphs\",\n        \"Point-x: A spatial-locality-aware architecture for energy-efficient graph-based point-cloud deep learning\",\n        \"2pgraph: Accelerating gnn training over large graphs on gpu clusters\",\n        \"Graph convolutional networks: a comprehensive review\",\n        \"Gmlp: Building scalable and flexible graph neural networks with feature-message passing\",\n        \"Pasca: A graph neural architecture search system under the scalable paradigm\",\n        \"Deep learning on graphs: A survey\",\n        \"Zipper: Exploiting tile-and operator-level parallelism for general and scalable graph neural network acceleration\",\n        \"Architectural implications of graph neural networks\",\n        \"Distdgl: distributed graph neural network training for billion-scale graphs\",\n        \"Distributed hybrid cpu and gpu training for graph neural networks on billion-scale graphs\",\n        \"Optimizing memory efficiency of graph neural networks on edge computing platforms\",\n        \"Learning with local and global consistency\",\n        \"Graph neural networks: A review of methods and applications\",\n        \"Explore mixture of experts in graph neural networks, 2019\",\n        \"Gcnear: A hybrid architecture for efficient gcn training with near-memory processing\",\n        \"Blockgnn: Towards efficient gnn acceleration using block-circulant weight matrices\",\n        \"Aligraph: A comprehensive graph neural network platform\",\n        \"Semi-supervised learning using gaussian fields and harmonic functions\",\n        \"Layer-dependent importance sampling for training deep and large graph convolutional networks\"\n    ],\n    \"reasoning\": \"Selected these 266 references because they represent foundational work and recent advances in Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}