{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 175 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Automated Speaker Independent Visual Speech Recognition: A Comprehensive Survey\n\nPaper Abstract: Abstract Speaker-independent visual speech recognition (VSR) is a complex task that involves identifying spoken words or phrases from video recordings of a speaker’s facial movements. Decoding the intricate visual dynamics of a speaker’s mouth in a high-dimensional space is a significant challenge in this field. To address this challenge, researchers have employed advanced techniques that enable machines to recognize human speech through visual cues automatically. Over the years, there has been a considerable amount of research in the field of VSR involving different algorithms and datasets to evaluate system performance. These efforts have resulted in significant progress in developing effective VSR models, creating new opportunities for further research in this area. This survey provides a detailed examination of the progression of VSR over the past three decades, with a particular emphasis on the transition from speaker-dependent to speaker-independent systems. We also provide a comprehensive overview of the various datasets used in VSR research and the preprocessing techniques employed to achieve speaker independence. The survey covers the works published from 1990 to 2023, thoroughly analyzing each work and comparing them on various parameters. This survey provides an in-depth analysis of speaker-independent VSR systems evolution from 1990 to 2023. It outlines the development of VSR systems over time and highlights the need to develop end-to-end pipelines for speaker-independent VSR. The pictorial representation offers a clear and concise overview of the techniques used in speaker-independent VSR, thereby aiding in the comprehension and analysis of the various methodologies. The survey also highlights the strengths and limitations of each technique and provides insights into developing novel approaches for analyzing visual speech cues. Overall, This comprehensive review provides insights into the current state-of-the-art speaker-independent VSR and highlights potential areas for future research.\n\n{retrieved_text}\n\nGenerate exactly 175 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 175\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A review on speech recognition technique\",\n        \"Speech recognition by machine: A review\",\n        \"Discrete cosine transform\",\n        \"Principal component analysis\",\n        \"Discrete wavelet transforms: Theory and implementation\",\n        \"Robust biometric person identification using automatic classifier fusion of speech, mouth, and face experts\",\n        \"Lip as biometric and beyond: a survey\",\n        \"The lip as a biometric\",\n        \"Deep learning based assistive technology on audio visual speech recognition for hearing impaired\",\n        \"Multimedia content analysis-using both audio and visual clues\",\n        \"“Some normative data on lip-reading skills (l)\",\n        \"“Viseme definitions comparison for visual-only speech recognition\",\n        \"“Decoding visemes: Improving machine lip-reading\",\n        \"“The syllabic phonemes of english\",\n        \"“Frequency of occurrence of phonemes in conversational english\",\n        \"“Facial action recognition for facial expression analysis from static face images\",\n        \"“On the relationship between face movements\",\n        \"“Is lip region-of-interest sufficient for lipreading?” in Proceedings of the 2022 International Conference on Multimodal Interaction\",\n        \"D. Bhattacharjee\",\n        \"“Roi processing for visual features extraction in lip-reading\",\n        \"“Feature extraction and normalization algorithms for high-density oligonucleotide gene expression array data\",\n        \"“A multi-scale feature extraction-based normalized attention neural network for image denoising\",\n        \"“A novel automatic method on pseudo-invariant features extraction for enhancing the relative radiometric normalization of high-resolution images\",\n        \"“Channel separability in the audio-visual integration of speech: A bayesian approach\",\n        \"“Combining evidence in personal identity verification systems\",\n        \"“Extraction of visual features for lipreading\",\n        \"“The challenge of multispeaker lip-reading\",\n        \"“Lipreading with local spatiotemporal descriptors\",\n        \"“Lip reading sentences in the wild\",\n        \"“LRS3-TED: a large-scale dataset for visual speech recognition\",\n        \"“Temporal multimodal learning in audiovisual speech recognition\",\n        \"“A new visual speech recognition approach for rgb-d cameras\",\n        \"“Lip reading in the wild\",\n        \"“Lipreading model based on whole-part collaborative learning\",\n        \"“A cascade sequence-to-sequence model for chinese mandarin lip reading\",\n        \"“A multimodal german dataset for automatic lip reading systems and transfer learning\",\n        \"“Cn-cvs: A mandarin audio-visual dataset for large vocabulary continuous visual to speech synthesis.”\",\n        \"“RUSAVIC corpus: Russian audio-visual speech in cars\",\n        \"“Olkavs: An open large-scale korean audio-visual speech dataset\",\n        \"“A survey of fuzzy clustering\",\n        \"Algorithms for fuzzy clustering. Springer\",\n        \"“A validity measure for fuzzy clustering\",\n        \"B. Yegnanarayana\",\n        \"“An active contour model without edges\",\n        \"“Lip contour extraction of rgb-based improved region growing algorithm\",\n        \"“An improved seeded region growing algorithm\",\n        \"“Automatic lip contour extraction using pixel-based segmentation and piece-wise polynomial fitting\",\n        \"“A lip contour extraction method using localized active contour model with automatic parameter selection\",\n        \"“Lips contour detection and tracking using watershed region-based active contour model and modified h∞subscriptℎh_{\\\\infty}\",\n        \"“Segmentation of color lip images by spatial fuzzy clustering\",\n        \"“A new spatial fuzzy c-means for spatial clustering\",\n        \"“Lip image segmentation using fuzzy clustering incorporating an elliptic shape function\",\n        \"“A real-time lip localization and tacking for lip reading\",\n        \"“Explaining adaboost\",\n        \"“Multi-class adaboost\",\n        \"“Haar features for facs au recognition\",\n        \"“Facial feature detection using haar classifiers\",\n        \"“Lip segmentation using geometrical model of color distribution\",\n        \"“Lip feature extraction based on improved jumping-snake model\",\n        \"“The use of dynamic deformable templates for lip tracking in an audio-visual corpus with large variations in head pose\",\n        \"“Lip tracking using adaptive fuzzy particle filter in the context of car driving simulator under low contrast near-infrared illumination\",\n        \"“An efficient algorithm for lip detection in color face images\",\n        \"“Training strategies for improved lip-reading\",\n        \"“Human face detection and tracking using retinaface network for surveillance systems\",\n        \"“How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230\",\n        \"“Accurate and resource-efficient lipreading with efficientnetv2 and transformers\",\n        \"“Visual speech recognition in a driver assistance system\",\n        \"“Audio-visual speech and gesture recognition by sensors of mobile devices\",\n        \"“Facial detection and normalization in color image\",\n        \"“Real-time normalization and feature extraction of 3d face data using curvature characteristics\",\n        \"“High-fidelity illumination normalization for face recognition based on auto-encoder\",\n        \"“Illumination normalization of face image based-on multi-stage feature maps\",\n        \"“Recent progress on generative adversarial networks (gans): A survey\",\n        \"“Generative adversarial networks: introduction and outlook\",\n        \"“Generative adversarial networks (gans) challenges\",\n        \"M. Kavakli-Thorne\",\n        \"“Deep learning based holistic speaker independent visual speech recognition\",\n        \"“Face recognition and lip-reading in autism\",\n        \"“Benefit from visual cues in auditory-visual speech recognition by middle-aged and elderly persons\",\n        \"“Audio-visual speech recognition for a vowel discrimination task\",\n        \"“Visual speech recognition with stochastic networks\",\n        \"“What hmms can do\",\n        \"“Discriminative training of hmms for automatic speech recognition: A survey\",\n        \"“Hmms and related speech recognition technologies\",\n        \"“Visual word recognition in two facial motion conditions: Full-face versus lips-plus-mandible\",\n        \"“Nonlinear manifold learning for visual speech recognition\",\n        \"“Statistical lip modelling for visual speech recognition\",\n        \"“Active appearance models\",\n        \"“Visual speech recognition using active shape models and hidden markov models\",\n        \"“Lipreading from color video\",\n        \"“Lipreading using fourier transform over time\",\n        \"“Towards speaker independent continuous speechreading\",\n        \"“Visual speech recognition by recurrent neural networks\",\n        \"“Intensity-and location-normalized training for hmm-based visual speech recognition\",\n        \"“A diffusion network approach to visual speech recognition\",\n        \"“Automatic computer lip-reading using fuzzy set theory\",\n        \"Recurrent neural networks for prediction: learning algorithms\",\n        \"“A critical review of recurrent neural networks for sequence learning\",\n        \"“Lip location normalized training for visual speech recognition\",\n        \"“Normalized training for hmm-based visual speech recognition\",\n        \"“Support vector machines\",\n        \"“A geometric approach to support vector machine (svm) classification\",\n        \"“A support vector machine-based dynamic network for visual speech recognition applications\",\n        \"“A temporal network of support vector machine classifiers for the recognition of visual speech\",\n        \"“Visual speech recognition using support vector machines\",\n        \"“A boosted multi-hmm classifier for recognition of visual speech elements\",\n        \"“Visual features extracting & selecting for lipreading\",\n        \"“Learning fuzzy rules for visual speech recognition\",\n        \"“Recognition of visual speech elements using adaptively boosted hidden markov models\",\n        \"“Visual speech recognition with loosely synchronized feature streams\",\n        \"“Hyper column model vs. fast dct for feature extraction in visual arabic speech recognition\",\n        \"“Training hidden markov models by hybrid simulated annealing for visual speech recognition\",\n        \"“Visual speech recognition using image moments and multiresolution wavelet images\",\n        \"“Multilayer perceptron (mlp)\",\n        \"“The multilayer perceptron (mlp)\",\n        \"“Automatic lipreading with limited training data\",\n        \"“A new manifold representation for visual speech recognition\",\n        \"“Modular bdpca based visual feature representation for lip-reading\",\n        \"“Speech recognition system based on visual feature for the hearing impaired\",\n        \"“Static and dynamic features for improved hmm based visual speech recognition\",\n        \"“Inter-frame contextual modelling for visual speech recognition\",\n        \"“Visual speech recognition using convolutional vef snake and canonical correlations\",\n        \"“Genetic algorithm-a literature review\",\n        \"“Genetic algorithm\",\n        \"“Automatic lip reading in the dutch language using active appearance models on high speed recordings\",\n        \"“Visual speech recognition of modern classic arabic language\",\n        \"“Visual speech recognition using optical flow and support vector machines\",\n        \"“Lip feature extraction for visual speech recognition using hidden markov model\",\n        \"“Lip localization and viseme classification for visual speech recognition\",\n        \"“Resolution limits on visual speech recognition\",\n        \"“Lip reading using cnn and lstm\",\n        \"“Very deep convolutional neural network based image classification using small training sample size\",\n        \"“Going deeper in spiking neural networks: Vgg and residual architectures\",\n        \"“Transfer learning using vgg-16 with deep convolutional neural network for classifying images\",\n        \"” Convolutional Neural Networks with Swift for Tensorflow: Image Recognition and Dataset Categorization\",\n        \"“Hybrid speech recognition with deep bidirectional lstm\",\n        \"“Lipnet: Sentence-level lipreading\",\n        \"“Stcnn: A spatio-temporal convolutional neural network for long-term traffic prediction\",\n        \"“Four-dimensional modeling of fmri data via spatio–temporal convolutional neural networks (st-cnns)\",\n        \"“Spatiotemporal cnn for video object segmentation\",\n        \"“Empirical evaluation of gated recurrent neural networks on sequence modeling\",\n        \"“Deep complementary bottleneck features for visual speech recognition\",\n        \"“Auto-encoder bottleneck features using deep belief networks\",\n        \"“Improved bottleneck features using pretrained deep neural networks\",\n        \"“Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification\",\n        \"“Challenge based visual speech recognition using deep learning\",\n        \"“A normalized levenshtein distance metric\",\n        \"“Feature extraction using multimodal convolutional neural networks for visual speech recognition\",\n        \"“Deep learning transfer with alexnet for chest x-ray covid-19 recognition\",\n        \"“Imagenet classification with deep convolutional neural networks\",\n        \"“Inception-v3 for flower classification\",\n        \"“Pulmonary image classification based on inception-v3 transfer learning model\",\n        \"“Face mask detection using transfer learning of inceptionv3\",\n        \"“Convolutional neural networks for predicting words: A lip-reading system\",\n        \"“A lip reading model using cnn with batch normalization\",\n        \"“Deep lip reading-a deep learning based lip-reading software for the hearing impaired\",\n        \"“Lip reading using convolutional auto encoders as feature extractor\",\n        \"“Automatic lip-reading system based on deep convolutional neural network and attention-based long short-term memory\",\n        \"“Lip image segmentation based on a fuzzy convolutional neural network\",\n        \"“Visual speech recognition: A deep learning approach\",\n        \"“Speaker-independent visual speech recognition with the inception v3 model\",\n        \"“Speaker-independent lipreading with limited data\",\n        \"“Speaker-independent lipreading by disentangled representation learning\",\n        \"“Lipformer: Learning to lipread unseen speakers based on visual-landmark transformers\",\n        \"“Visual speech recognition for multiple languages in the wild\",\n        \"“Prompt tuning of deep neural networks for speaker-adaptive visual speech recognition\",\n        \"“Visual speech recognition using convolutional neural network\",\n        \"“Action recognition via local descriptors and holistic features\",\n        \"“A computationally efficient approach to 3d ear recognition employing local and holistic features\",\n        \"“The holistic representation of faces\",\n        \"“Auditory-visual interaction in voice localization and in bimodal speech recognition: The effects of desynchronization\",\n        \"“Tinyml: Enabling of inference deep learning models on ultra-low-power iot edge devices for ai applications\",\n        \"“An overview on edge computing research\",\n        \"“Challenges and opportunities in edge computing\",\n        \"“Edge computing: Vision and challenges\"\n    ],\n    \"reasoning\": \"Selected these 175 references because they represent foundational work and recent advances in Automated Speaker Independent Visual Speech Recognition: A Comprehensive Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}