{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 29 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models\n\nPaper Abstract: Abstract Transformer neural networks, driven by self-attention mechanisms, are core components of foundational and Large Language Models. In generative transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks for long sequences. In this work, we propose a fast and energy-efficient hardware implementation of self-attention using analog in-memory computing based on gain cell memories. Volatile gain cell memories can be efficiently written to store new tokens during sequence generation, while performing analog signed weight multiplications to compute the dot-products required for self-attention. We implement Sliding Window Attention, which keeps memory of a finite set of past steps. A charge-to-pulse converter for array readout eliminates the need for analog-to-digital conversion between self-attention stages. Using a co-designed initialization algorithm to adapt pre-trained weights to gain cell non-idealities, we achieve NLP performance comparable to ChatGPT-2 with minimal training iterations, despite hardware constraints. Our end-to-end hardware design includes digital controls, estimating area, latency, and energy. The system reduces attention latency by up to two orders of magnitude and energy consumption by up to five orders compared to GPUs, marking a significant step toward ultra-fast, low-power sequence generation in Large Language Models.\n\n{retrieved_text}\n\nGenerate exactly 29 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 29\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Attention is all you need\",\n        \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n        \"Mistral 7B\",\n        \"Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product\",\n        \"Challenges in deploying long-context transformers: A theoretical peak performance analysis\",\n        \"Retransformer: Reram-based processing-in-memory architecture for transformer acceleration\",\n        \"Attention-in-Memory for Few-Shot Learning with Configurable Ferroelectric FET Arrays\",\n        \"Longformer: The Long-Document Transformer\",\n        \"25.4 a 20nm 6gb function-in-memory dram, based on hbm2 with a 1.2tflops programmable computing unit using bank-level parallelism, for machine learning applications\",\n        \"Counteractive coupling igzo/cnt hybrid 2t0c dram accelerating rram-based computing-in-memory via monolithic 3d integration for edge ai\",\n        \"Lowest ioff << 3×10-21 a/µm in capacitorless dram achieved by reactive ion etch of igzo-tft\",\n        \"15.3 a 65nm 3t dynamic analog RAM-based computing-in-memory macro and CNN accelerator with retention enhancement, adaptive analog sparsity and 44tops/w system energy efficiency\",\n        \"Design of an energy efficient voltage-to-time converter with rectified linear unit characteristics for artificial neural networks\",\n        \"Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks\",\n        \"Pim-gpt: A hybrid process-in-memory accelerator for autoregressive transformers\",\n        \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\",\n        \"Mega: Moving Average Equipped Gated Attention\",\n        \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\",\n        \"State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\",\n        \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\",\n        \"All you need is a good init\",\n        \"15.3 A 351TOPS/W and 372.4GOPS Compute-in-Memory SRAM Macro in 7nm FinFET CMOS for Machine-Learning Applications\",\n        \"Openwebtext corpus\",\n        \"Decoupled Weight Decay Regularization\",\n        \"Mamba: Linear-time sequence modeling with selective state spaces\",\n        \"xlstm: Extended long short-term memory\",\n        \"Hellaswag: Can a machine really finish your sentence?\",\n        \"The lambada dataset: Word prediction requiring a broad discourse context\",\n        \"Piqa: Reasoning about physical commonsense in natural language\"\n    ],\n    \"reasoning\": \"Selected these 29 references because they represent foundational work and recent advances in Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}