{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: The Impact of Token Granularity on the Predictive Power of Language Model Surprisal\n\nPaper Abstract: Abstract Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power. One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned. This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions. Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive. In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting their increased sensitivity to syntax. Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Syntactic surprisal from neural models predicts, but underestimates, human processing difficulty from syntactic ambiguities\",\n        \"Why do language models perform worse for morphologically complex languages?\",\n        \"Random effects structure for confirmatory hypothesis testing: Keep it maximal\",\n        \"The word-length effect in reading: A review\",\n        \"Fitting linear mixed-effects models using lme4\",\n        \"Pythia: A suite for analyzing large language models across training and scaling\",\n        \"Byte pair encoding is suboptimal for language model pretraining\",\n        \"The Mysterious Affair at Styles\",\n        \"Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading\",\n        \"Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality\",\n        \"The Natural Stories corpus: A reading-time corpus of English texts containing rare syntactic constructions\",\n        \"On the proper treatment of tokenization in psycholinguistics\",\n        \"OpenWebText Corpus\",\n        \"Wiki-40B: Multilingual language model dataset\",\n        \"A probabilistic Earley parser as a psycholinguistic model\",\n        \"Scalable modified Kneser-Ney language model estimation\",\n        \"Large-scale benchmark yields no evidence that language model surprisal explains syntactic disambiguation difficulty\",\n        \"A theory of reading: From eye fixations to comprehension\",\n        \"The Dundee Corpus\",\n        \"Subword regularization: Improving neural network translation models with multiple subword candidates\",\n        \"Computational analysis of present-day American English\",\n        \"Fishing for Magikarp: Automatically detecting under-trained tokens in large language models\",\n        \"Expectation-based syntactic comprehension\",\n        \"Decoupled weight decay regularization\",\n        \"The Provo Corpus: A large eye-tracking corpus with predictability norms\",\n        \"Human sentence processing: Recurrence or attention?\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Words, subwords, and morphemes: What really matters in the surprisal-reading time relationship?\",\n        \"Surprisal estimators for human reading times need character models\",\n        \"Transformer-based language model surprisal predicts human reading times best with about two billion training tokens\",\n        \"Why does surprisal from larger Transformer-based language models provide a poorer fit to human reading times?\",\n        \"Leading whitespaces of language modelsâ€™ subword vocabulary pose a confound for calculating word probabilities\",\n        \"How to compute the probability of a word\",\n        \"Language models are unsupervised multitask learners\",\n        \"The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences\",\n        \"Tokenization is more than compression\",\n        \"Neural machine translation of rare words with subword units\",\n        \"Word frequency and predictability dissociate in naturalistic reading\",\n        \"Large-scale evidence for logarithmic effects of word predictability on reading time\",\n        \"The effect of word predictability on reading time is logarithmic\",\n        \"Single-stage prediction models do not explain the magnitude of syntactic disambiguation difficulty\",\n        \"Attention is all you need\",\n        \"From language models over tokens to language models over characters\",\n        \"On the predictive power of neural language models for human real-time comprehension behavior\",\n        \"Language model quality correlates with psychometric predictive power in multiple languages\",\n        \"Testing the predictions of surprisal theory in 11 languages\",\n        \"The linearity of the effect of surprisal on reading times across languages\",\n        \"Tokenization and the noiseless channel\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in The Impact of Token Granularity on the Predictive Power of Language Model Surprisal, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}