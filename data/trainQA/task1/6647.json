{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 183 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Mixed-Precision Neural Networks: A Survey\n\nPaper Abstract: Abstract Mixed-precision Deep Neural Networks achieve the energy efficiency and throughput needed for hardware deployment, particularly when the resources are limited, without sacrificing accuracy. However, the optimal per-layer bit precision that preserves accuracy is not easily found, especially with the abundance of models, datasets, and quantization techniques that creates an enormous search space. In order to tackle this difficulty, a body of literature has emerged recently, and several frameworks that achieved promising accuracy results have been proposed. In this paper, we start by summarizing the quantization techniques used generally in literature. Then, we present a thorough survey of the mixed-precision frameworks, categorized according to their optimization techniques such as reinforcement learning and quantization techniques like deterministic rounding. Furthermore, the advantages and shortcomings of each framework are discussed, where we present a juxtaposition. We finally give guidelines for future mixed-precision frameworks.\n\n{retrieved_text}\n\nGenerate exactly 183 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 183\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep neural networks for object detection\",\n        \"Deeptest: Automated testing of deep-neural-network-driven autonomous cars\",\n        \"Toward collaborative inferencing of deep neural networks on internet-of-things devices\",\n        \"Cadd-spliceâ€”improving genome-wide variant effect prediction using deep learning-derived splice scores\",\n        \"Enabling cognitive smart cities using big data and machine learning: Approaches and challenges\",\n        \"Importance estimation for neural network pruning\",\n        \"Pruning filters for efficient convnets\",\n        \"Learning both weights and connections for efficient neural network\",\n        \"Optimal brain damage\",\n        \"Designing energy-efficient convolutional neural networks using energy-aware pruning\",\n        \"Exploring the regularity of sparse structure in convolutional neural networks\",\n        \"Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size\",\n        \"Mobilenetv2: Inverted residuals and linear bottlenecks\",\n        \"Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"Squeezenext: Hardware-aware neural network design\",\n        \"Efficient methods and hardware for deep learning\",\n        \"Searching for mobilenetv3\",\n        \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\",\n        \"Distilling the knowledge in a neural network\",\n        \"Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy\",\n        \"Model compression via distillation and quantization\",\n        \"Dreaming to distill: Data-free knowledge transfer via deepinversion\",\n        \"Experimental determination of precision requirements for back-propagation training of artificial neural networks\",\n        \"Lsq+: Improving low-bit quantization through learnable offsets and better initialization\",\n        \"One weight bitwidth to rule them all\",\n        \"Binarized neural networks\",\n        \"Quantization and training of neural networks for efficient integer-arithmetic-only inference\",\n        \"Xnor-net: Imagenet classification using binary convolutional neural networks\",\n        \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks\",\n        \"Incremental network quantization: Towards lossless cnns with low-precision weights\",\n        \"Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network\",\n        \"Hawq: Hessian aware quantization of neural networks with mixed-precision\",\n        \"Bit-mixer: Mixed-precision networks with runtime bit-width selection\",\n        \"Weight discretization paradigm for optical neural networks\",\n        \"Zeroq: A novel zero shot quantization framework\",\n        \"1.1 computingâ€™s energy problem (and what we can do about it)\",\n        \"Rethinking differentiable search for mixed-precision neural networks\",\n        \"Binary neural networks: A survey\",\n        \"Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients\",\n        \"Pact: Parameterized clipping activation for quantized neural networks\",\n        \"Low-bit quantization of neural networks for efficient inference\",\n        \"Post-training 4-bit quantization of convolution networks for rapid-deployment\",\n        \"Quantizing deep convolutional networks for efficient inference: A whitepaper\",\n        \"Towards efficient training for neural network quantization\",\n        \"Ternary neural networks with fine-grained quantization\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"Value-aware quantization for training and inference of neural networks\",\n        \"Mxqn: mixed quantization for reducing bit-width of weights and activations in deep convolutional neural networks\",\n        \"Releq: A reinforcement learning approach for deep quantization of neural networks\",\n        \"Learning and generalization in overparameterized neural networks, going beyond two layers\",\n        \"M. Mahoney et al\",\n        \"â€œA survey on methods and theories of quantized neural networks\",\n        \"â€œA survey of quantization methods for efficient neural network inference\",\n        \"â€œBinaryconnect: Training deep neural networks with binary weights during propagations\",\n        \"K. Gopalakrishnan\",\n        \"â€œTraining and inference with integers in deep neural networks\",\n        \"â€œTowards the limit of network quantization\",\n        \"â€œQuantized convolutional neural networks for mobile devices\",\n        \"â€œWeighted-entropy-based quantization for deep neural networks\",\n        \"â€œCompressing deep convolutional networks using vector quantization\",\n        \"â€œTernary weight networks\",\n        \"â€œExtremely low bit neural network: Squeeze the last bit out with admm\",\n        \"â€œAdaptive quantization of neural networks\",\n        \"â€œLoss-aware binarization of deep networks\",\n        \"â€œFixed-point feedforward deep neural network design using weights+ 1\",\n        \"â€œTrained ternary quantization\",\n        \"â€œMultilayer feedforward neural networks with single powers-of-two weights\",\n        \"â€œVariational network quantization\",\n        \"â€œRounding methods for neural networks with low resolution synaptic weights\",\n        \"â€œNeural networks with few multiplications\",\n        \"â€œExpectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights.â€ in NIPS\",\n        \"â€œLearning discrete weights using the local reparameterization trick\",\n        \"â€œAn introduction to variational methods for graphical models\",\n        \"â€œMixed precision quantization of convnets via differentiable neural architecture search\",\n        \"â€œJoint training of low-precision neural network with quantization interval parameters\",\n        \"â€œTraining compact neural networks with binary weights and low precision activations\",\n        \"â€œ1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns\",\n        \"â€œScalable distributed dnn training using commodity gpu cloud computing\",\n        \"â€œQsgd: Communication-efficient sgd via gradient quantization and encoding\",\n        \"â€œCommunication quantization for data-parallel training of deep neural networks\",\n        \"â€œEvoq: Mixed precision quantization of dnns via sensitivity guided evolutionary search\",\n        \"â€œDeep learning with low precision by half-wave gaussian quantization\",\n        \"â€œExplicit loss-error-aware quantization for low-bit deep neural networks\",\n        \"â€œDiscovering low-precision networks close to full-precision networks for efficient embedded inference\",\n        \"â€œNvidia 8-bit inference with tensorrt\",\n        \"â€œInteger quantization for deep learning inference: Principles and empirical evaluation\",\n        \"â€œHmq: Hardware friendly mixed precision quantization block for cnns\",\n        \"â€œSearch what you want: Barrier panelty nas for mixed precision quantization\",\n        \"â€œApq: Joint search for network architecture\",\n        \"â€œJoint neural architecture search and quantization\",\n        \"â€œDifferentiable soft quantization: Bridging full-precision and low-bit neural networks\",\n        \"â€œTowards mixed-precision quantization of neural networks via constrained optimization\",\n        \"â€œOpq: Compressing deep neural networks with one-shot pruning-quantization\",\n        \"â€œConstructing energy-efficient mixed-precision neural networks through principal component analysis for edge intelligence\",\n        \"S.-M. Moosavi-Dezfooli\",\n        \"â€œMixed-precision quantized neural network with progressively decreasing bitwidth for image classification and object detection\",\n        \"â€œSimple augmentation goes a long way: Adrl for dnn quantization\",\n        \"â€œHaq: Hardware-aware automated quantization with mixed precision\",\n        \"â€œBayesian bits: Unifying quantization and pruning\",\n        \"â€œDifferentiable quantization of deep neural networks\",\n        \"â€œStochastic layer-wise precision in deep neural networks\",\n        \"V. Chandrasekhar\",\n        \"â€œEstimating or propagating gradients through stochastic neurons for conditional computation\",\n        \"â€œDropout: a simple way to prevent neural networks from overfitting\",\n        \"â€œCategorical reparameterization with gumbel-softmax\",\n        \"â€œGradient-based learning applied to document recognition\",\n        \"â€œImagenet classification with deep convolutional neural networks\",\n        \"Constrained optimization and Lagrange multiplier methods. Academic press\",\n        \"â€œClip-q: Deep network compression learning by in-parallel pruning-quantization\",\n        \"â€œSyq: Learning symmetric quantization for efficient deep neural networks\",\n        \"â€œLearning sparse neural networks through lâ€‹_â€‹0ð‘™_0l\\\\_0 regularization\",\n        \"â€œAuto-encoding variational bayes\",\n        \"â€œStochastic backpropagation and approximate inference in deep generative models\",\n        \"â€œRelaxed quantization for discretized neural networks\",\n        \"â€œLearned step size quantization\",\n        \"â€œUp or down? adaptive rounding for post-training quantization\",\n        \"â€œTrained uniform quantization for accurate and efficient neural network inference on fixed-point hardware\",\n        \"â€œMixed precision dnns: All you need is a good parametrization\",\n        \"â€œFixed point quantization of deep convolutional networks\",\n        \"â€œBi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm\",\n        \"â€œBinarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1\",\n        \"â€œHawq-v2: Hessian aware trace-weighted quantization of neural networks\",\n        \"â€œRandomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix\",\n        \"â€œFully quantized network for object detection\",\n        \"â€œFocal loss for dense object detection\",\n        \"â€œMicrosoft coco: Common objects in context\",\n        \"â€œA low effort approach to structured cnn design using pca\",\n        \"â€œHybrid binary networks: optimizing for accuracy\",\n        \"â€œLow-bit quantization of neural networks for efficient inference.â€ in ICCV Workshops\",\n        \"â€œImproving neural network quantization without retraining using outlier channel splitting\",\n        \"â€œData-free quantization through weight equalization and bias correction\",\n        \"â€œThe knowledge within: Methods for data-free model compression\",\n        \"â€œTvm: end-to-end optimization stack for deep learning\",\n        \"â€œPulp: a linear programming toolkit for python\",\n        \"â€ https://github.com/NVIDIA/cutlass\",\n        \"â€œDeep residual learning for image recognition\",\n        \"â€œImproving post training neural quantization: Layer-wise calibration and integer programming\",\n        \"â€œLearning compression from limited unlabeled data\",\n        \"â€œDeep neural network compression by in-parallel pruning-quantization\",\n        \"â€œAutomatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach\",\n        \"â€œTowards accurate binary convolutional neural network\",\n        \"â€œAutoq: Automated kernel-wise neural network quantization\",\n        \"G. Hinton et al\",\n        \"â€œIdentity mappings in deep residual networks\",\n        \"â€œImagenet: A large-scale hierarchical image database\",\n        \"â€œLearning transferable architectures for scalable image recognition\",\n        \"â€œA comparative analysis of selection schemes used in genetic algorithms\",\n        \"â€œEfficient neural architecture search via parameters sharing\",\n        \"L. Van Der Maaten\",\n        \"â€œProgressive neural architecture search\",\n        \"â€œRegularized evolution for image classifier architecture search\",\n        \"â€œDarts: Differentiable architecture search\",\n        \"â€œMixed precision neural architecture search for energy efficient deep learning\",\n        \"â€œSimple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"â€œVery deep convolutional networks for large-scale image recognition\",\n        \"â€œmixup: Beyond empirical risk minimization\",\n        \"â€œNeural architecture optimization\",\n        \"â€œDesign automation for efficient deep learning computing\",\n        \"â€œAmc: Automl for model compression and acceleration on mobile devices\",\n        \"â€œOnce-for-all: Train one network and specialize it for efficient deployment\",\n        \"â€œSingle path one-shot neural architecture search with uniform sampling\",\n        \"â€œProxylessnas: Direct neural architecture search on target task and hardware\",\n        \"â€œInterior point methods in semidefinite programming with applications to combinatorial optimization\",\n        \"E. Zheltonozhskii\",\n        \"â€œSsd: Single shot multibox detector\",\n        \"â€œHigh-capacity expert binary networks\",\n        \"â€œLearning to quantize deep networks by optimizing quantization intervals with task loss\",\n        \"â€œAdditive powers-of-two quantization: An efficient non-uniform discretization for neural networks\",\n        \"â€œWrpn: Wide reduced-precision networks\",\n        \"â€œProximal policy optimization algorithms\",\n        \"â€œStripes: Bit-serial deep neural network computing\",\n        \"â€œA unified framework of dnn weight pruning and weight clustering/quantization using admm\",\n        \"â€œContinuous control with deep reinforcement learning\",\n        \"â€œAdam: A method for stochastic optimization\",\n        \"â€œBismo: A scalable bit-serial matrix multiplication overlay for reconfigurable computing\",\n        \"â€œMobilenets: Efficient convolutional neural networks for mobile vision applications\",\n        \"â€œData-efficient hierarchical reinforcement learning\",\n        \"â€œOn learning-based methods for design-space exploration with high-level synthesis\",\n        \"â€œPrimal: Power inference using machine learning\",\n        \"â€œTwo-step quantization for low-bit neural networks\",\n        \"â€œCirculant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation\",\n        \"â€œBalanced binary neural networks with gated residual\",\n        \"â€œPruning algorithms-a survey\"\n    ],\n    \"reasoning\": \"Selected these 183 references because they represent foundational work and recent advances in Mixed-Precision Neural Networks: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}