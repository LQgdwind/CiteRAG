{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 60 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel\n\nPaper Abstract: Abstract It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nyström approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demonstrate that ICK has superior performance and flexibility on both synthetic and real-world data sets. The ICK framework can be used to include prior information into neural networks in many applications.\n\n{retrieved_text}\n\nGenerate exactly 60 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 60\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep neural network approach for predicting the productivity of garment employees\",\n        \"Pattern recognition and machine learning, volume 4\",\n        \"Optimization methods for large-scale machine learning\",\n        \"Gradient-enhanced kriging for high-dimensional problems\",\n        \"Improving kriging surrogates of high-dimensional design models by partial least squares dimension reduction\",\n        \"Nonparametric binary regression using a gaussian process prior\",\n        \"Hierarchical nearest-neighbor gaussian process models for large geostatistical datasets\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"On the nyström method for approximating a gram matrix for improved kernel-based learning\",\n        \"Automatic model construction with Gaussian processes\",\n        \"Geometric data analysis, beyond convolutions\",\n        \"Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration\",\n        \"Neural processes\",\n        \"Deep convolutional networks as shallow gaussian processes\",\n        \"Spatial statistics and gaussian processes: A beautiful marriage\",\n        \"Variational bayesian multinomial probit regression with gaussian process priors\",\n        \"Deep learning\",\n        \"Bayesian deep ensembles via the neural tangent kernel\",\n        \"Masked autoencoders are scalable vision learners\",\n        \"Gaussian processes for big data\",\n        \"Neural tangent kernel: Convergence and generalization in neural networks\",\n        \"Spatial transformer networks\",\n        \"Improving spatial variation of ground-level pm2. 5 prediction with contrastive learning from satellite imagery\",\n        \"Mining geostatistics\",\n        \"Analyzing nonstationary spatial data using piecewise gaussian processes\",\n        \"Adam: A method for stochastic optimization\",\n        \"A statistical approach to some basic mine valuation problems on the witwatersrand\",\n        \"Artificial neural networks for solving ordinary and partial differential equations\",\n        \"Deep neural networks as gaussian processes\",\n        \"Wide neural networks of any depth evolve as linear models under gradient descent\",\n        \"The concrete distribution: A continuous relaxation of discrete random variables\",\n        \"Deep learning: A critical appraisal\",\n        \"The ridgelet prior: A covariance function approach to prior specification for bayesian neural networks\",\n        \"Sample-then-optimize posterior sampling for bayesian linear models\",\n        \"Gaussian process behaviour in wide deep neural networks\",\n        \"Learning multi-modal similarity\",\n        \"Solving the wave equation with physics-informed deep learning\",\n        \"Priors for infinite networks\",\n        \"Neural tangents: Fast and easy infinite neural networks in python\",\n        \"Expressive priors in bayesian neural networks: Kernel combinations and periodic functions\",\n        \"Evolving gaussian process models for prediction of ozone concentration in the air\",\n        \"Random features for large-scale kernel machines\",\n        \"Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning\",\n        \"Unsupervised learning with random forest predictors\",\n        \"Sparse gaussian processes using pseudo-inputs\",\n        \"Convolutional gaussian processes\",\n        \"Attention is all you need\",\n        \"Continuously indexed domain adaptation\",\n        \"Exact gaussian processes on a million data points\",\n        \"Using the nyström method to speed up kernel machines\",\n        \"Gaussian processes for machine learning, volume 2\",\n        \"Bayesian classification with gaussian processes\",\n        \"Gaussian process kernels for pattern discovery and extrapolation\",\n        \"Deep kernel learning\",\n        \"Gaussian process regression networks\",\n        \"Nyström method vs random fourier features: A theoretical and empirical comparison\",\n        \"Multimodal classification of alzheimer’s disease and mild cognitive impairment\",\n        \"Local pm2. 5 hotspot detector at 300 m resolution: A random forest–convolutional neural network joint model jointly trained on satellite images and meteorology\",\n        \"Estimating ground-level pm2. 5 using micro-satellite images by a convolutional neural network and random forest approach\",\n        \"Deepvit: Towards deeper vision transformer\"\n    ],\n    \"reasoning\": \"Selected these 60 references because they represent foundational work and recent advances in Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}