{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 59 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Structured Pruning of Neural Networks for Constraints Learning\n\nPaper Abstract: Abstract In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly impacts computational efforts during training and necessitates significant memory resources for storage. In this paper, we showcase the effectiveness of pruning, one of these techniques, when applied to ANNs prior to their integration into MIPs. By pruning the ANN, we achieve significant improvements in the speed of the solution process. We discuss why pruning is more suitable in this context compared to other ML compression techniques, and we identify the most appropriate pruning strategies. To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.\n\n{retrieved_text}\n\nGenerate exactly 59 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 59\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Bounding and counting linear regions of deep neural networks\",\n        \"Principled deep neural network training through linear programming\",\n        \"Strong mixed-integer programming formulations for trained neural networks\",\n        \"Entmoot: A framework for optimization over ensemble tree models\",\n        \"Janos: An integrated predictive and prescriptive modeling framework\",\n        \"Mixed-integer optimization with constraint learning\",\n        \"Omlt: Optimization & machine learning toolkit\",\n        \"Analytics for an online retailer: Demand forecasting and price optimization\",\n        \"A learning-based mathematical programming formulation for the automatic configuration of optimization solvers\",\n        \"Learning to configure mathematical programming solvers by mathematical programming\",\n        \"Deep neural networks and mixed integer linear optimization\",\n        \"An analytics approach to designing combination chemotherapy regimens for cancer\",\n        \"Process synthesis of biodiesel production plant using artificial neural networks as the surrogate models\",\n        \"Optimization of fischer-tropsch synthesis using neural networks\",\n        \"Relu networks as surrogate models in mixed-integer linear programs\",\n        \"When deep learning meets polyhedral theory: A survey\",\n        \"Efficient methods and hardware for deep learning\",\n        \"Variational dropout sparsifies deep neural networks\",\n        \"Deep rewiring: Training very sparse deep networks\",\n        \"Bayesian compression for deep learning\",\n        \"Accelerate cnn via recursive bayesian pruning\",\n        \"Compression-aware training of deep networks\",\n        \"A unified approximation framework for compressing and accelerating deep neural networks\",\n        \"Optimal brain compression: A framework for accurate post-training quantization and pruning\",\n        \"Sensitivity-informed provable pruning of neural networks\",\n        \"Data-efficient structured pruning via submodular optimization\",\n        \"Model preserving compression for neural networks\",\n        \"The combinatorial brain surgeon: Pruning weights that cancel one another in neural networks\",\n        \"The adaptive berhu penalty in robust regression\",\n        \"Only train once: A one-shot neural network training and pruning framework\",\n        \"Structural pruning via latency-saliency knapsack\",\n        \"Topology-aware network pruning using multi-stage graph embedding and reinforcement learning\",\n        \"Winning the lottery ahead of time: Efficient early network pruning\",\n        \"SPDY: Accurate pruning with speedup guarantees\",\n        \"Dhp: Differentiable meta pruning via hypernetworks\",\n        \"Towards efficient model compression via learned global ranking\",\n        \"Advancing model pruning via bi-level optimization\",\n        \"Campfire: Compressible, regularization-free, structured sparse training for hardware accelerators\",\n        \"A systematic dnn weight pruning framework using alternating direction method of multipliers\",\n        \"”learning-compression” algorithms for neural net pruning\",\n        \"Learning sparse neural networks through l_0 regularization\",\n        \"Deep neural networks pruning via the structured perspective regularization\",\n        \"Strengthening the sequential convex minlp technique by perspective reformulations\",\n        \"Projected perspective reformulations with applications in design problems\",\n        \"Approximated perspective relaxations: a project&lift approach\",\n        \"Improving the approximated projected perspective reformulation by dual information\",\n        \"Binaryconnect: Training deep neural networks with binary weights during propagations\",\n        \"Integer quantization for deep learning inference: Principles and empirical evaluation\",\n        \"On the convergence of stochastic gradient descent in low-precision number formats\",\n        \"Quantization and training of neural networks for efficient integer-arithmetic-only inference\",\n        \"Learning compact recurrent neural networks with block-term tensor decomposition\",\n        \"Speeding up convolutional neural networks with low rank expansions\",\n        \"Understanding deep architectures using a recursive convolutional network\",\n        \"Recurrent stacking of layers for compact neural machine translation models\",\n        \"Efficient neural network verification via layer-based semidefinite relaxations and linear cuts.\",\n        \"Smart predict–and–optimize for hard combinatorial optimization problems\",\n        \"Decision-focused learning: through the lens of learning to rank\",\n        \"Adversarial network pruning by filter robustness estimation\",\n        \"On the robustness and anomaly detection of sparse neural networks\"\n    ],\n    \"reasoning\": \"Selected these 59 references because they represent foundational work and recent advances in Structured Pruning of Neural Networks for Constraints Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}