{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Evolving Neural Networks through a Reverse Encoding Tree\n\nPaper Abstract: Abstract NeuroEvolution is one of the most competitive evolutionary learning strategies for designing novel neural networks for use in specific tasks, such as logic circuit design and digital gaming. However, the application of benchmark methods such as the NeuroEvolution of Augmenting Topologies (NEAT) remains a challenge, in terms of their computational cost and search time inefficiency. This paper advances a method which incorporates a type of topological edge coding, named Reverse Encoding Tree (RET), for evolving scalable neural networks efficiently. Using RET, two types of approaches – NEAT with Binary search encoding (Bi-NEAT) and NEAT with Golden-Section search encoding (GS-NEAT) – have been designed to solve problems in benchmark continuous learning environments such as logic gates, Cartpole, and Lunar Lander, and tested against classical NEAT and FS-NEAT as baselines. Additionally, we conduct a robustness test to evaluate the resilience of the proposed NEAT approaches. The results show that the two proposed approaches deliver an improved performance, characterized by (1) a higher accumulated reward within a finite number of time steps; (2) using fewer episodes to solve problems in targeted environments, and (3) maintaining adaptive robustness under noisy perturbations, which outperform the baselines in all tested cases. Our analysis also demonstrates that RET expends potential future research directions in dynamic environments. Code is available from https://github.com/HaolingZHANG/ReverseEncodingTree.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Designing neural networks through neuroevolution\",\n        \"A critique of pure learning and what artificial neural networks can learn from animal brains\",\n        \"Neuroevolution\",\n        \"Evolving neural networks through augmenting topologies\",\n        \"Automatic feature selection in neuroevolution\",\n        \"Safe mutations for deep and recurrent neural networks through output gradients\",\n        \"Natural evolution speciation for neat\",\n        \"Toward an estimation of distribution algorithm for the evolution of artificial neural networks\",\n        \"Completely derandomized self-adaptation in evolution strategies\",\n        \"Neuroevolution for reinforcement learning using evolution strategies\",\n        \"Vine: an open source interactive data visualization tool for neuroevolution\",\n        \"Classification and regression by randomforest\",\n        \"Generalized binary search for split-neighborly problems\",\n        \"N-dimension golden section search: Its variants and limitations\",\n        \"A new optimization algorithm based on chaotic maps and golden section search method\",\n        \"Search for global maxima in multimodal functions by applying numerical optimization algorithms: a comparison between golden section and simulated annealing\",\n        \"Position-based sequence weights\",\n        \"Sequential minimax search for a maximum\",\n        \"Causation and pearson’s correlation coefficient\",\n        \"A review of clustering techniques and developments\",\n        \"Openai gym\",\n        \"Evolving reusable neural modules\",\n        \"Blocky net: A new neuroevolution method\",\n        \"Multidimensional binary search trees used for associative searching\",\n        \"Complex networks from simple rewrite systems\",\n        \"A novel optimization method for solving constrained and unconstrained problems: modified golden sine algorithm\",\n        \"An improved local search involving bee colony optimization using lambda iteration combined with a golden section search method to solve an economic dispatch problem\",\n        \"Real-parameter evolutionary multimodal optimization—a survey of the state-of-the-art\",\n        \"Fibonacci multi-modal optimization algorithm in noisy environment\",\n        \"Evolving complete robots with cppn-neat: the utility of recurrent connections\",\n        \"Does aligning phenotypic and genotypic modularity improve the evolution of neural networks?\",\n        \"Compositional pattern producing networks: A novel abstraction of development\",\n        \"Genetic synthesis of boolean neural networks with a cell rewriting developmental process\",\n        \"Evolving graphs and networks with edge encoding: Preliminary report\",\n        \"k 2-trees for compact web graph representation\",\n        \"Reducing fitness evaluations using clustering techniques and neural network ensembles\",\n        \"Genetic evolution of the topology and weight distribution of neural networks\",\n        \"Stochastic local search: Foundations and applications\",\n        \"Elementary Linear Algebra, Binder Ready Version: Applications Version\",\n        \"Approximate k-means++ in sublinear time\",\n        \"Fast approximate spectral clustering\",\n        \"Birch: an efficient data clustering method for very large databases\",\n        \"Enhanced adversarial strategically-timed attacks against deep reinforcement learning\",\n        \"Neuronlike adaptive elements that can solve difficult learning control problems\",\n        \"Behaviour suite for reinforcement learning\",\n        \"Anti-jamming communications using spectrum waterfall: A deep reinforcement learning approach\",\n        \"Learning functions in large networks requires modularity and produces multi-agent dynamics\",\n        \"Controllability, multiplexing, and transfer learning in networks using evolutionary learning\",\n        \"neat-python\",\n        \"Genetic algorithms and evolution strategies: Similarities and differences\",\n        \"High-resolution 3d modelling and visualization of mount everest\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in Evolving Neural Networks through a Reverse Encoding Tree, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}