{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 218 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey\n\nPaper Abstract: Abstract Deep learning technologies have demonstrated remarkable effectiveness in a wide range of tasks, and deep learning holds the potential to advance a multitude of applications, including in edge computing, where deep models are deployed on edge devices to enable instant data processing and response. A key challenge is that while the application of deep models often incurs substantial memory and computational costs, edge devices typically offer only very limited storage and computational capabilities that may vary substantially across devices. These characteristics make it difficult to build deep learning solutions that unleash the potential of edge devices while complying with their constraints. A promising approach to addressing this challenge is to automate the design of effective deep learning models that are lightweight, require only a little storage, and incur only low computational overheads. This survey offers comprehensive coverage of studies of design automation techniques for deep learning models targeting edge computing. It offers an overview and comparison of key metrics that are used commonly to quantify the proficiency of models in terms of effectiveness, lightness, and computational costs. The survey then proceeds to cover three categories of the state-of-the-art of deep model design automation techniques: automated neural architecture search, automated model compression, and joint automated design and compression. Finally, the survey covers open issues and directions for future research.\n\n{retrieved_text}\n\nGenerate exactly 218 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 218\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep learning based recommender system: A survey and new perspectives\",\n        \"BERT: pre-training of deep bidirectional transformers for language understanding\",\n        \"Learning attentional temporal cues of brainwaves with spatial embedding for motion intent detection\",\n        \"Data security and privacy protection issues in cloud computing\",\n        \"Energy and policy considerations for deep learning in NLP\",\n        \"Green ai\",\n        \"Predicting parameters in deep learning\",\n        \"Optimal brain damage\",\n        \"Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n        \"Mobilenetv2: Inverted residuals and linear bottlenecks\",\n        \"Shufflenet: An extremely efficient convolutional neural network for mobile devices\",\n        \"Shufflenet v2: Practical guidelines for efficient cnn architecture design\",\n        \"Channel pruning for accelerating very deep neural networks\",\n        \"Thinet: A filter level pruning method for deep neural network compression\",\n        \"Training deep neural networks with low precision multiplications\",\n        \"A logical calculus of the ideas immanent in nervous activity\",\n        \"Hippocampal spine head sizes are highly precise\",\n        \"Think Tank: Forty Neuroscientists Explore the Biological Roots of Human Experience\",\n        \"Fixed point quantization of deep convolutional networks\",\n        \"Deep learning with limited numerical precision\",\n        \"Distilling the knowledge in a neural network\",\n        \"Learning compact recurrent neural networks with block-term tensor decomposition\",\n        \"Proxylessnas: Direct neural architecture search on target task and hardware\",\n        \"Autoprune: Automatic network pruning by regularizing auxiliary parameters\",\n        \"HAQ: hardware-aware automated quantization with mixed precision\",\n        \"AMC: automl for model compression and acceleration on mobile devices\",\n        \"Beyond floating-point ops: CNN performance prediction with critical datapath length\",\n        \"Fastdeepiot: Towards understanding and optimizing neural network execution time on mobile and embedded devices\",\n        \"Condensenet v2: Sparse feature reactivation for deep networks\",\n        \"Survey of precision-scalable multiply-accumulate units for neural-network processing\",\n        \"Discovering multi-hardware mobile models via architecture search\",\n        \"Benchmark analysis of representative deep neural network architectures\",\n        \"Research and design of activation function hardware implementation methods\",\n        \"Improving strong-scaling of cnn training by exploiting finer-grained parallelism\",\n        \"Rt3d: Achieving real-time execution of 3d convolutional neural networks on mobile devices\",\n        \"cudnn: Efficient primitives for deep learning\",\n        \"Neural networks on microcontrollers: saving memory at inference via operator reordering\",\n        \"Machine learning on arm cortex-m microcontrollers\",\n        \"Memory requirements for convolutional neural network hardware accelerators\",\n        \"Saving energy on the edge: In-memory caching for multi-tier heterogeneous networks\",\n        \"Efficient neural network deployment for microcontroller\",\n        \"Deep roots: Improving cnn efficiency with hierarchical filter groups\",\n        \"A note on latency variability of deep neural networks for mobile inference\",\n        \"Generalized latency performance estimation for once-for-all neural architecture search\",\n        \"Gpgpu performance estimation with core and memory frequency scaling\",\n        \"Mcunet: Tiny deep learning on iot devices\",\n        \"Deep residual learning for image recognition\",\n        \"Mnasnet: Platform-aware neural architecture search for mobile\",\n        \"Learning transferable architectures for scalable image recognition\",\n        \"Automl: A survey of the state-of-the-art\",\n        \"A survey on evolutionary neural architecture search\",\n        \"“Neural architecture search with reinforcement learning\",\n        \"“Searching for a robust neural architecture in four GPU hours\",\n        \"“Darts: Differentiable architecture search\",\n        \"“Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation\",\n        \"“Miniseg: An extremely minimum network for efficient covid-19 segmentation\",\n        \"“Neural architecture search: A survey\",\n        \"“Hierarchical representations for efficient architecture search\",\n        \"“Very deep convolutional networks for large-scale image recognition\",\n        \"L. van der Maaten\",\n        \"“Nsga-net: neural architecture search using multi-objective genetic algorithm\",\n        \"“Constrained deep neural network architecture search for iot devices accounting for hardware calibration\",\n        \"“On neural architecture search for resource-constrained hardware platforms\",\n        \"“Efficient multi-objective neural architecture search via lamarckian evolution\",\n        \"“Resource-efficient neural architect\",\n        \"D. Lymberopoulos\",\n        \"“Densely connected search space for more flexible neural architecture search\",\n        \"“Efficientnet: Rethinking model scaling for convolutional neural networks\",\n        \"“Mobiledets: Searching for object detection architectures for mobile accelerators\",\n        \"“Searching for fast model families on datacenter accelerators\",\n        \"“Squeeze-and-excitation networks\",\n        \"“Searching for mobilenetv3\",\n        \"“Semantic segmentation of satellite images using a modified CNN with hard-swish activation function\",\n        \"“Binaryconnect: Training deep neural networks with binary weights during propagations\",\n        \"“Bignas: Scaling up neural architecture search with big single-stage models\",\n        \"“Moga: Searching beyond mobilenetv3\",\n        \"“Once-for-all: Train one network and specialize it for efficient deployment\",\n        \"“Fbnetv3: Joint architecture-recipe search using predictor pretraining\",\n        \"“Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions\",\n        \"“Mnasfpn: Learning latency-aware pyramid architecture for object detection on mobile devices\",\n        \"“Lighttrack: Finding lightweight neural networks for object tracking via one-shot architecture search\",\n        \"“Aggregated residual transformations for deep neural networks\",\n        \"“Imagenet classification with deep convolutional neural networks\",\n        \"“Vipnas: Efficient video pose estimation via neural architecture search\",\n        \"“Dpp-net: Device-aware progressive search for pareto-optimal neural architectures\",\n        \"“Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\",\n        \"“Resource constrained neural network architecture search: Will a submodularity assumption help?” in ICCV\",\n        \"“Monas: Multi-objective neural architecture search\",\n        \"“Fasterseg: Searching for faster real-time semantic segmentation\",\n        \"“Searching efficient 3d architectures with sparse point-voxel convolution\",\n        \"“HR-NAS: searching efficient high-resolution neural architectures with lightweight transformers\",\n        \"“Attention is all you need\",\n        \"“The evolved transformer\",\n        \"“End-to-end object detection with transformers\",\n        \"“An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"“Modulenet: Knowledge-inherited neural architecture search\",\n        \"“Going deeper with convolutions\",\n        \"“Rethinking the inception architecture for computer vision\",\n        \"“Progressive neural architecture search\",\n        \"“Deep pyramidal residual networks\",\n        \"“Path-level network transformation for efficient architecture search\",\n        \"“Exploring randomly wired neural networks for image recognition\",\n        \"“Random search for hyper-parameter optimization\",\n        \"“Single path one-shot neural architecture search with uniform sampling\",\n        \"“Can weight sharing outperform random architecture search? an investigation with tunas\",\n        \"“Lens: Layer distribution enabled neural architecture search in edge-cloud hierarchies\",\n        \"“Efficient resource-aware convolutional neural architecture search for edge computing with pareto-bayesian optimization\",\n        \"“Bayesian multi-objective hyperparameter optimization for accurate\",\n        \"G. Venkatesh et al\",\n        \"“High-dimensional bayesian multi-objective optimization\",\n        \"“Multi-objective bayesian optimization over high-dimensional search spaces\",\n        \"“Distilling optimal neural networks: Rapid search in diverse spaces\",\n        \"“Hsconas: Hardware-software co-design of efficient dnns via neural architecture search\",\n        \"“Chamnet: Towards efficient network design through platform-aware model adaptation\",\n        \"“Adaptive probabilities of crossover and mutation in genetic algorithms\",\n        \"“Regularized evolution for image classifier architecture search\",\n        \"“Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search\",\n        \"“A fast and elitist multiobjective genetic algorithm: NSGA-II\",\n        \"“Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\n        \"“Proximal policy optimization algorithms\",\n        \"“Efficient neural architecture search via parameters sharing\",\n        \"“Categorical reparameterization with gumbel-softmax\",\n        \"“Efficient neural architecture search via parameter sharing\",\n        \"“Understanding and simplifying one-shot architecture search\",\n        \"“Universally slimmable networks and improved training techniques\",\n        \"“Model compression and hardware acceleration for neural networks: A comprehensive survey\",\n        \"“A comprehensive survey on model compression and acceleration\",\n        \"“Some mathematical notes on three-mode factor analysis\",\n        \"“Compression of deep convolutional neural networks for fast and low power mobile applications\",\n        \"“Tensor contraction layers for parsimonious deep nets\",\n        \"“Analysis of individual differences in multidimensional scaling via an n-way generalization of “eckart-young” decomposition\",\n        \"“Cp-decomposition with tensor power method for convolutional neural networks compression\",\n        \"“Speeding-up convolutional neural networks using fine-tuned cp-decomposition\",\n        \"“Global analytic solution of fully-observed variational bayesian matrix factorization\",\n        \"M. Kholyavchenko\",\n        \"“Towards compact neural networks via end-to-end training: A bayesian tensor approach with automatic rank determination\",\n        \"“Bayesian tensorized neural networks with automatic rank selection\",\n        \"“Mars: Masked automatic ranks selection in tensor decompositions\",\n        \"“Autorank: Automated rank selection for effective neural network customization\",\n        \"“A bayesian optimization framework for neural network compression\",\n        \"“Model compression\",\n        \"“Do deep nets really need to be deep?” in NIPS\",\n        \"“Search to distill: Pearls are everywhere but not the eyes\",\n        \"“Rosearch: Search for robust student architectures when distilling pre-trained language models\",\n        \"“Kdas-reid: Architecture search for person re-identification via distilled knowledge with dynamic temperature\",\n        \"“Autokd: Automatic knowledge distillation into a student architecture family\",\n        \"“Autodistill: an end-to-end framework to explore and distill hardware-efficient language models\",\n        \"“Scene-adaptive knowledge distillation for sequential recommendation via differentiable architecture search\",\n        \"“Automatic student network search for knowledge distillation\",\n        \"“Gan compression: Efficient architectures for interactive conditional gans\",\n        \"“Mfagan: A compression framework for memory-efficient on-device super-resolution gan\",\n        \"“PPCD-GAN: progressive pruning and class-aware distillation for large-scale conditional gans compression\",\n        \"“Towards oracle knowledge distillation with neural architecture search\",\n        \"“Channel planting for deep neural networks using knowledge distillation\",\n        \"“NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search\",\n        \"“Block-wisely supervised neural architecture search with knowledge distillation\",\n        \"“Lookahead: A far-sighted alternative of magnitude-based pruning\",\n        \"“A systematic weight pruning framework of dnns using alternating direction method of multipliers\",\n        \"“Network automatic pruning: Start nap and take a nap\",\n        \"“Ss-auto: A single-shot\",\n        \"“An information theory-inspired strategy for automatic network pruning\",\n        \"“Autocompress: An automatic DNN structured pruning framework for ultra-high compression rates\",\n        \"“Auto-prune: automated dnn pruning and mapping for reram-based accelerator\",\n        \"“Automatic channel pruning with hyper-parameter search and dynamic masking\",\n        \"“Fine-pruning: Joint fine-tuning and compression of a convolutional network with bayesian optimization\",\n        \"“Constraint-aware deep neural network compression\",\n        \"“High-dimensional bayesian optimization for cnn auto pruning with clustering and rollback\",\n        \"“Abcp: Automatic block-wise and channel-wise network pruning via joint search\",\n        \"“Automatic pruning for quantized neural networks\",\n        \"“Netadapt: Platform-aware neural network adaptation for mobile applications\",\n        \"“Netadaptv2: Efficient neural architecture search with fast super-network training and architecture optimization\",\n        \"“Dais: Automatic channel pruning via differentiable annealing indicator search\",\n        \"“Learning efficient convolutional networks through network slimming\",\n        \"“DHP: differentiable meta pruning via hypernetworks\",\n        \"“ASBP: automatic structured bit-pruning for rram-based NN accelerator\",\n        \"“Network pruning via transformable architecture search\",\n        \"“Network compression via cooperative architecture search and distillation\",\n        \"“Joint-detnas: Upgrade your detector with nas\",\n        \"“Search for better students to learn distilled knowledge\",\n        \"“1.1 computing’s energy problem (and what we can do about it)\",\n        \"“Batchquant: Quantized-for-all architecture search with robust quantizer\",\n        \"“Apple describes 7nm a12 bionic chips\",\n        \"“Nvidia tensor cores\",\n        \"“Hardware-centric automl for mixed-precision quantization\",\n        \"“Stochastic layer-wise precision in deep neural networks\",\n        \"“Search what you want: Barrier panelty NAS for mixed precision quantization\",\n        \"“Mixed precision quantization of transformer language models for speech recognition\",\n        \"“Effective and fast: A novel sequential single path search for mixed-precision quantization\",\n        \"“Mixed-precision quantization for cnn-based remote sensing scene classification\",\n        \"“Mixed precision quantization of convnets via differentiable neural architecture search\",\n        \"“A white paper on neural network quantization\",\n        \"“Differentiable feature aggregation search for knowledge distillation\",\n        \"“Rethinking the value of network pruning\",\n        \"“Metapruning: Meta learning for automatic neural network channel pruning\",\n        \"“Latency-aware automatic cnn channel pruning with gpu runtime analysis\",\n        \"“Channel pruning via automatic structure search\",\n        \"“Acp: Automatic channel pruning via clustering and swarm intelligence optimization for cnn\",\n        \"“Superpruner: Automatic neural network pruning via super network\",\n        \"“Aacp: Model compression by accurate and automatic channel pruning\",\n        \"“Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers\",\n        \"“On-device image classification with proxyless neural architecture search and quantization-aware fine-tuning\",\n        \"on-device and explainable neural architecture search\",\n        \"“Resource-efficient dnns for keyword spotting using neural architecture search and quantization\",\n        \"“BATS: binary architecture search\",\n        \"“Binarizing mobilenet via evolution-based searching\",\n        \"“Learning architectures for binary networks\",\n        \"“Searching for accurate binary neural architectures\",\n        \"“Frostnet: Towards quantization-awareof network architecture search\",\n        \"“Layer-wise searching for 1-bit detectors\",\n        \"“Joint neural architecture search and quantization\",\n        \"“Mixed precision neural architecture search for energy efficient deep learning\",\n        \"“APQ: joint search for network architecture\",\n        \"“Once quantization-aware training: High performance extremely low-bit architecture search\",\n        \"“Zero-cost proxies for lightweight NAS\",\n        \"“Zen-nas: A zero-shot NAS for high-performance image recognition\",\n        \"“Squeezenext: Hardware-aware neural network design\",\n        \"“HSIM-DNN: hardware simulator for computation-\",\n        \"“Hw-nas-bench: Hardware-aware neural architecture search benchmark\"\n    ],\n    \"reasoning\": \"Selected these 218 references because they represent foundational work and recent advances in Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}