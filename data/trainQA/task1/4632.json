{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Efficient Learning and Decoding of the Continuous-Time Hidden Markov Model for Disease Progression Modeling\n\nPaper Abstract: Abstract The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models, as well as the first solution to decoding the optimal state transition sequence and the corresponding state dwelling time. We show that EM-based learning consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem as an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three distinct approaches from the continuous time Markov chain (CTMC) literature to the CT-HMM domain. Additionally, we further improve the efficiency of the most efficient method by a factor of the number of states. Then, for decoding, we incorporate a state-of-the-art method from the (CTMC) literature, and extend the end-state conditioned optimal state sequence decoding to the CT-HMM case with the computation of the expected state dwelling time. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer’s disease dataset, and to decode and visualize the most probable state transition trajectory for individuals on the glaucoma dataset, which helps to identify progressing phenotypes in a comprehensive way. Finally, we apply the CT-HMM modeling and decoding strategy to investigate the progression of language acquisition and development.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Computing the fréchet derivative of the matrix exponential, with an application to condition number estimation\",\n        \"Computing the action of the matrix exponential, with an application to exponential integrators\",\n        \"Patterns of growth in verbal abilities among children with autism spectrum disorder\",\n        \"Progression of liver cirrhosis to HCC: an application of hidden markov model\",\n        \"Norms and exclusion theorems\",\n        \"Principles of scientific computing\",\n        \"Statistical inference for discretely observed markov jump processes\",\n        \"The Theory of Stochastic Processes\",\n        \"isurvive: An interpretable, event-time prediction model for mhealth\",\n        \"Decreased CSF A beta 42 correlates with brain atrophy in cognitively normal elderly\",\n        \"Matrix computations, volume 3\",\n        \"State sequence analysis in hidden markov models\",\n        \"Efficient continuous-time markov chain estimation\",\n        \"Functions of Matrices: Theory and Computation\",\n        \"Statistical inference in evolutionary models of DNA sequences via the EM algorithm\",\n        \"Summary statistics for endpoint-conditioned continuous-time markov chains\",\n        \"MacArthur-Bates Communicative Development Inventories, Second Edition, pages 1773–1779\",\n        \"Multi-state models for panel data: the msm package for R\",\n        \"Markoff chains as an aid in the study of markoff processes\",\n        \"Lightgbm: A highly efficient gradient boosting decision tree\",\n        \"Glaucoma is second leading cause of blindness globally\",\n        \"Modeling disease progression trajectories from longitudinal observational data\",\n        \"Fitting and interpreting continuous-time latent markov models for panel data\",\n        \"Visualization and prediction of disease interactions with continuous-time hidden markov models\",\n        \"What do molecules do when we are not looking? state sequence analysis for stochastic chemical systems\",\n        \"Longitudinal modeling of glaucoma progression using 2-dimensional continuous-time hidden markov model\",\n        \"Efficient learning of continuous-time hidden markov models for disease progression\",\n        \"Learning continuous-time hidden markov models for event data\",\n        \"Vocabulary development\",\n        \"Age of first words predicts cognitive ability and adaptive skills in children with asd\",\n        \"Efficient maximum likelihood parameterization of continuous-time markov processes\",\n        \"Combining structural and functional measurements to improve estimates of rates of glaucomatous progression\",\n        \"Generator estimation of markov jump processes based on incomplete observations nonequidistant in time\",\n        \"Generator estimation of markov jump processes\",\n        \"Nineteen dubious ways to compute the exponential of a matrix\",\n        \"Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later\",\n        \"Communication and Symbolic Behavior Scale, pages 722–726\",\n        \"A spatiotemporal approach to predicting glaucoma progression using a ct-hmm\",\n        \"Expectation maximization and complex duration distributions for continuous time bayesian networks\",\n        \"On pre-conditioning of matrices\",\n        \"Maximum likelihood trajectories for continuous-time markov chains\",\n        \"A tutorial on hidden markov models and selected applications in speech recognition\",\n        \"Stochastic Processes\",\n        \"Comparison of methods for calculating conditional expectations of sufficient statistics for continuous time markov chains\",\n        \"http://adni.loni.usc.edu.\",\n        \"Computing integrals involving the matrix exponential\",\n        \"Unsupervised learning of disease progression models\",\n        \"Retinal nerve fibre layer and visual function loss in glaucoma: the tipping point\",\n        \"Value-added predictors of expressive and receptive language growth in initially nonverbal preschoolers with autism spectrum disorders\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Efficient Learning and Decoding of the Continuous-Time Hidden Markov Model for Disease Progression Modeling, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}