{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 164 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?\n\nPaper Abstract: Abstract Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity.\n\n{retrieved_text}\n\nGenerate exactly 164 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 164\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Contributions to the study of sms spam filtering: New collection and results\",\n        \"MathQA: Towards interpretable math word problem solving with operation-based formalisms\",\n        \"How to train your MAML\",\n        \"Guiding extractive summarization with question-answering rewards\",\n        \"TweetEval: Unified benchmark and comparative evaluation for tweet classification\",\n        \"Beat the AI: Investigating adversarial human annotation for reading comprehension\",\n        \"Semantic parsing on Freebase from question-answer pairs\",\n        \"Abductive commonsense reasoning\",\n        \"PIQA: reasoning about physical commonsense in natural language\",\n        \"ProtoQA: A question answering dataset for prototypical common-sense reasoning\",\n        \"Learning to split and rephrase from Wikipedia edit history\",\n        \"Language models are few-shot learners\",\n        \"Acquiring a single new word\",\n        \"SemEval-2019 task 3: EmoContext contextual emotion detection in text\",\n        \"CODAH: An adversarially-authored question answering dataset for common sense\",\n        \"Tabfact: A large-scale dataset for table-based fact verification\",\n        \"Low-resource domain adaptation for compositional task-oriented semantic parsing\",\n        \"Meta-learning via language model in-context tuning\",\n        \"BoolQ: Exploring the surprising difficulty of natural yes/no questions\",\n        \"Think you have solved question answering? try arc, the ai2 reasoning challenge\",\n        \"Structural scaffolds for citation intent classification in scientific publications\",\n        \"The pascal recognising textual entailment challenge\",\n        \"Quoref: A reading comprehension dataset with questions requiring coreferential reasoning\",\n        \"Automated hate speech detection and the problem of offensive language\",\n        \"Hate speech dataset from a white supremacy forum\",\n        \"The commitmentbank: Investigating projection in naturally occurring discourse\",\n        \"Climate-fever: A dataset for verification of real-world climate claims\",\n        \"Wizard of wikipedia: Knowledge-powered conversational agents\",\n        \"Is GPT-3 a good data annotator?\",\n        \"Automatically constructing a corpus of sentential paraphrases\",\n        \"Investigating meta-learning algorithms for low-resource natural language understanding tasks\",\n        \"Searchqa: A new q&a dataset augmented with context from a search engine\",\n        \"Semantic noise matters for neural natural language generation\",\n        \"Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge\",\n        \"T-REx: A large scale alignment of natural language with knowledge base triples\",\n        \"Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model\",\n        \"ELI5: Long form question answering\",\n        \"Identifying well-formed natural language questions\",\n        \"Deep multi-task and meta learning\",\n        \"Model-agnostic meta-learning for fast adaptation of deep networks\",\n        \"Making pre-trained language models better few-shot learners\",\n        \"Neural snowball for few-shot relation learning\",\n        \"Conditional neural processes\",\n        \"SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization\",\n        \"SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning\",\n        \"Generalized inner loop meta-learning\",\n        \"Meta-learning for low-resource neural machine translation\",\n        \"PPT: Pre-trained prompt tuning for few-shot learning\",\n        \"Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports\",\n        \"FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation\",\n        \"Question-answer driven semantic role labeling: Using natural language to annotate natural language\",\n        \"Robust disambiguation of named entities in text\",\n        \"Toward semantics-based answer pinpointing\",\n        \"Few-shot charge prediction with discriminative legal attributes\",\n        \"Cosmos QA: Machine reading comprehension with contextual commonsense reasoning\",\n        \"Learning a better initialization for soft prompts via meta-learning\",\n        \"FreebaseQA: A new factoid QA data set matching trivia-style question-answer pairs with Freebase\",\n        \"Beyond reptile: Meta-learned dot-product maximization between gradients for improved single-task regularization\",\n        \"Looking beyond the surface: A challenge set for reading comprehension over multiple sentences\",\n        \"Qasc: A dataset for question answering via sentence composition\",\n        \"Scitail: A textual entailment dataset from science question answering\",\n        \"Abstractive summarization of Reddit posts with multi-level memory networks\",\n        \"Siamese neural networks for one-shot image recognition\",\n        \"Explainable automated fact-checking for public health claims\",\n        \"Natural questions: A benchmark for question answering research\",\n        \"RACE: Large-scale ReAding comprehension dataset from examinations\",\n        \"Building machines that learn and think like people\",\n        \"How many data points is a prompt worth?\",\n        \"Neural text generation from structured data with application to the biography domain\",\n        \"Meta learning for natural language processing: A survey\",\n        \"Dbpedia - a large-scale, multilingual knowledge base extracted from wikipedia\",\n        \"The power of scale for parameter-efficient prompt tuning\",\n        \"The winograd schema challenge\",\n        \"Zero-shot relation extraction via reading comprehension\",\n        \"Learning to transfer prompts for text generation\",\n        \"Prefix-tuning: Optimizing continuous prompts for generation\",\n        \"Learning question classifiers\",\n        \"Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models\",\n        \"CommonGen: A constrained text generation challenge for generative commonsense reasoning\",\n        \"Reasoning over paragraph effects in situations\",\n        \"TRGP: Trust region gradient projection for continual learning\",\n        \"Program induction by rationale generation: Learning to solve and explain algebraic word problems\",\n        \"How can we accelerate progress towards human-like linguistic generalization?\",\n        \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n        \"Gpt understands, too\",\n        \"“I’d rather just go to bed”: Understanding indirect answers\",\n        \"Learning word vectors for sentiment analysis\",\n        \"Good debt or bad debt: Detecting semantic orientations in economic texts\",\n        \"LiMiT: The literal motion in text dataset\",\n        \"A SICK cure for the evaluation of compositional distributional semantic models\",\n        \"Hatexplain: A benchmark dataset for explainable hate speech detection\",\n        \"Hidden factors and hidden topics: understanding rating dimensions with review text\",\n        \"Effective transfer learning for identifying similar questions: Matching user questions to COVID-19 faqs\",\n        \"Can a suit of armor conduct electricity? a new dataset for open book question answering\",\n        \"MetaICL: Learning to learn in context\",\n        \"A simple neural attentive meta-learner\",\n        \"Ethos: an online hate speech detection dataset\",\n        \"CrowS-pairs: A challenge dataset for measuring social biases in masked language models\",\n        \"Annotated Gigaword\",\n        \"Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization\",\n        \"On first-order meta-learning algorithms\",\n        \"Adversarial NLI: A new benchmark for natural language understanding\",\n        \"English-asl gloss parallel corpus 2012: Aslg-pc12\",\n        \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales\",\n        \"BioMRC: A dataset for biomedical machine reading comprehension\",\n        \"How context affects language models’ factual predictions\",\n        \"Language models as knowledge bases?\",\n        \"WiC: the word-in-context dataset for evaluating context-sensitive meaning representations\",\n        \"What does this acronym mean? introducing a new dataset for acronym identification and disambiguation\",\n        \"Continual few-shot relation learning via embedding space regularization and data augmentation\",\n        \"LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5\",\n        \"Is chatgpt a general-purpose natural language processing task solver?\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Explain yourself! leveraging language models for commonsense reasoning\",\n        \"SQuAD: 100,000+ questions for machine comprehension of text\",\n        \"Getting closer to ai complete question answering: A set of prerequisite real tasks\",\n        \"DuoRC: Towards complex language understanding with paraphrased reading comprehension\",\n        \"Gradient projection memory for continual learning\",\n        \"Winogrande: An adversarial winograd schema challenge at scale\",\n        \"Meta-learning with memory-augmented neural networks\",\n        \"Social IQa: Commonsense reasoning about social interactions\",\n        \"CARER: Contextualized affect representations for emotion recognition\",\n        \"Exploiting cloze-questions for few-shot text classification and natural language inference\",\n        \"Investigating societal biases in a poetry composition system\",\n        \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\",\n        \"Mining discourse markers for unsupervised sentence representation learning\",\n        \"Prototypical networks for few-shot learning\",\n        \"Recursive deep models for semantic compositionality over a sentiment treebank\",\n        \"DREAM: A challenge data set and models for dialogue-based reading comprehension\",\n        \"Quarel: A dataset and models for answering questions about qualitative relationships\",\n        \"QuaRTz: An open-domain dataset of qualitative relationship questions\",\n        \"CommonsenseQA: A question answering challenge targeting commonsense knowledge\",\n        \"WIQA: A dataset for “what if…” reasoning over procedural text\",\n        \"FEVER: a large-scale dataset for fact extraction and VERification\",\n        \"Few-shot learning through an information retrieval lens\",\n        \"OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification\",\n        \"Matching networks for one shot learning\",\n        \"SPoT: Better frozen model adaptation through soft prompt transfer\",\n        \"“liar, liar pants on fire”: A new benchmark dataset for fake news detection\",\n        \"BLiMP: The benchmark of linguistic minimal pairs for English\",\n        \"Neural network acceptability judgments\",\n        \"Crowdsourcing multiple choice science questions\",\n        \"A broad-coverage challenge corpus for sentence understanding through inference\",\n        \"Transformers: State-of-the-art natural language processing\",\n        \"Break it down: A question understanding benchmark\",\n        \"TWEETQA: A social media focused question answering dataset\",\n        \"WikiQA: A challenge dataset for open-domain question answering\",\n        \"HotpotQA: A dataset for diverse, explainable multi-hop question answering\",\n        \"CrossFit: A few-shot learning challenge for cross-task generalization in NLP\",\n        \"Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task\",\n        \"SWAG: A large-scale adversarial dataset for grounded commonsense inference\",\n        \"HellaSwag: Can a machine really finish your sentence?\",\n        \"Semi-supervised URL segmentation with recurrent neural networks pre-trained on knowledge graph entities\",\n        \"This email could save your life: Introducing the task of email subject line generation\",\n        \"Record: Bridging the gap between human and machine commonsense reading comprehension\",\n        \"Character-level convolutional networks for text classification\",\n        \"PAWS: Paraphrase adversaries from word scrambling\",\n        \"Verify-and-edit: A knowledge-enhanced chain-of-thought framework\",\n        \"Calibrate before use: Improving few-shot performance of language models\",\n        \"Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections\",\n        \"Seq2sql: Generating structured queries from natural language usin\",\n        \"“going on a vacation” takes longer than “going for a walk”: A study of temporal commonsense understanding\",\n        \"Efficiently tuned parameters are task embeddings\",\n        \"Prompt-aligned gradient for prompt tuning\"\n    ],\n    \"reasoning\": \"Selected these 164 references because they represent foundational work and recent advances in Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}