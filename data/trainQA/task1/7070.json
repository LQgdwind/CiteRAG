{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Threshold Treewidth and Hypertree Width\n\nPaper Abstract: Abstract Treewidth and hypertree width have proven to be highly successful structural parameters in the context of the Constraint Satisfaction Problem (CSP). When either of these parameters is bounded by a constant, then CSP becomes solvable in polynomial time. However, here the order of the polynomial in the running time depends on the width, and this is known to be unavoidable; therefore, the problem is not fixed-parameter tractable parameterized by either of these width measures. Here we introduce an enhancement of tree and hypertree width through a novel notion of thresholds, allowing the associated decompositions to take into account information about the computational costs associated with solving the given CSP instance. Aside from introducing these notions, we obtain efficient theoretical as well as empirical algorithms for computing threshold treewidth and hypertree width and show that these parameters give rise to fixed-parameter algorithms for CSP as well as other, more general problems. We complement our theoretical results with experimental evaluations in terms of heuristics as well as exact methods based on SAT/SMT encodings. ‚Ä†‚Ä†Preliminary and shortened versions of the results presented in this submission appeared in the proceedings of IJCAI 2020 [27]. This article expands the exposition of that version by providing full proofs, detailed explanations especially including a more in-depth discussion of the applications of threshold treewidth, and an expanded experimental section. This article appeared in the Journal of Artificial Intelligence Research [29].\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Improving the efficiency of dynamic programming on tree decompositions via machine learning\",\n        \"Weighted treewidth ‚Äî algorithmic techniques and results\",\n        \"Nonserial Dynamic Programming\",\n        \"Datalog and constraint satisfaction with infinite templates\",\n        \"A linear-time algorithm for finding tree-decompositions of small treewidth\",\n        \"Treewidth computations I. Upper bounds\",\n        \"Treewidth computations II. Lower bounds\",\n        \"A ck‚Äãnsuperscriptùëêùëòùëõ{c}^{k}n 5-approximation algorithm for treewidth\",\n        \"Understanding model counting for beta-acyclic CNF-formulas\",\n        \"Tractability in constraint satisfaction problems: a survey\",\n        \"The complexity of general-valued CSPs seen from the other side\",\n        \"A greedy heuristic for the set-covering problem\",\n        \"Parameterized Algorithms\",\n        \"Solving infinite-domain CSPs using the patchwork property\",\n        \"Bucket elimination: a unifying framework for reasoning\",\n        \"Heuristic methods for hypertree decomposition\",\n        \"Graph Theory, 4th Edition, volume 173 of Graduate texts in mathematics\",\n        \"Fundamentals of Parameterized Complexity\",\n        \"An extensible SAT-solver\",\n        \"Solving Integer Quadratic Programming via explicit and structural restrictions\",\n        \"An SMT approach to fractional hypertree width\",\n        \"General and fractional hypertree decompositions: Hard and easy cases\",\n        \"Hyperbench: A benchmark and tool for hypergraphs and empirical findings\",\n        \"Parameterized Complexity Theory, volume XIV of Texts in Theoretical Computer Science. An EATCS Series\",\n        \"A sufficient condition for backtrack-bounded search\",\n        \"Sum-of-products with default values: Algorithms and complexity results\",\n        \"Threshold treewidth and hypertree width\",\n        \"On tractable queries and constraints\",\n        \"Hypertree decompositions and tractable queries\",\n        \"Fixed-parameter complexity in AI and nonmonotonic reasoning\",\n        \"Hypertree Decompositions: Structure, Algorithms, and Applications\",\n        \"Bounded treewidth as a key to tractability of knowledge representation and reasoning\",\n        \"Treewidth and hypertree width\",\n        \"Decomposition and tractability in qualitative spatial and temporal reasoning\",\n        \"Combining restarts, nogoods and bag-connected decompositions for solving CSPs\",\n        \"Pushing the power of stochastic greedy ordering schemes for inference in graphical models\",\n        \"Treewidth, Computations and Approximations, volume 842 of Lecture Notes in Computer Science\",\n        \"On the complexity of nonlinear mixed-integer optimization\",\n        \"Integer programming with a fixed number of variables\",\n        \"Invitation to Fixed-Parameter Algorithms\",\n        \"Constraint satisfaction with bounded treewidth revisited\",\n        \"Encoding treewidth into SAT\",\n        \"Weighted hypertree decompositions and optimal query plans\",\n        \"Computing optimal hypertree decompositions\",\n        \"Computing optimal hypertree decompositions with SAT\",\n        \"Valued constraint satisfaction problems: Hard and easy problems\",\n        \"Theory of linear and integer programming\",\n        \"OptiMathSAT: A tool for optimization modulo theories\",\n        \"The Design of Approximation Algorithms\",\n        \"The Complexity of Valued Constraint Satisfaction Problems\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Threshold Treewidth and Hypertree Width, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}