{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 302 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: On Efficient Training of Large-Scale Deep Learning Models: A Literature Review\n\nPaper Abstract: Abstract The field of deep learning has witnessed significant progress in recent times, particularly in areas such as computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. However, it extremely suffers from the unstable training process and stringent requirements of computational resources. With the increasing demands on the adaption of computational capacity, though numerous studies have explored the efficient training field to a certain extent, a comprehensive summarization/guideline on those general acceleration techniques of training large-scale deep learning models is still much anticipated. In this survey, we present a detailed review of the general techniques for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) “data-centric”: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) “model-centric”, including acceleration of basic modules, compression training, model initialization and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters and providing better initialization; (3) “optimization-centric”, including the selection of learning rate, the employment of large batchsize, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models; (4) “budgeted training”, including some distinctive acceleration methods on source-constrained situations, e.g. for limitation on the total iterations; (5) “system-centric”, including some efficient distributed frameworks and open-source libraries which provide adequate hardware support for the implementation of above mentioned acceleration algorithms. By presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction. Meanwhile, we further provide a detailed analysis and discussion of future works on the development of general acceleration techniques, which could inspire us to re-think and design novel efficient paradigms. Overall, we hope that this survey will serve as a valuable guideline for general efficient training.\n\n{retrieved_text}\n\nGenerate exactly 302 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 302\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Iterative procedures for nonlinear integral equations\",\n        \"GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch\",\n        \"Towards understanding sharpness-aware minimization\",\n        \"How important is importance sampling for deep budgeted training?\",\n        \"Adaptive input representations for neural language modeling\",\n        \"Efficient data loader for fast sampling-based gnn training on large graphs\",\n        \"Beit: Bert pre-training of image transformers\",\n        \"Greedy layer-wise training of deep networks\",\n        \"Curriculum learning\",\n        \"Quasi-newton methods for machine learning: forget the past, just sample\",\n        \"Colossal-ai: A unified deep learning system for large-scale parallel training\",\n        \"Maximizing parallelism in distributed training for huge neural networks\",\n        \"GPT-NeoX-20B: An open-source autoregressive language model\",\n        \"Token merging: Your vit but faster\",\n        \"On the opportunities and risks of foundation models\",\n        \"JAX: composable transformations of Python+NumPy programs\",\n        \"Freezeout: Accelerate training by progressively freezing layers\",\n        \"Language models are few-shot learners\",\n        \"Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition\",\n        \"Improving deep learning by inverse square root linear units (isrlus)\",\n        \"Avalanche: A pytorch library for deep continual learning\",\n        \"Scatterbrain: Unifying sparse and low-rank attention\",\n        \"Transmix: Attend to mix for vision transformers\",\n        \"Rex: Revisiting budgeted training with an improved schedule\",\n        \"Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective\",\n        \"Training deep nets with sublinear memory cost\",\n        \"A simple framework for contrastive learning of visual representations\",\n        \"Symbolic discovery of optimization algorithms\",\n        \"Mask-guided vision transformer (mg-vit) for few-shot learning\",\n        \"Masked-attention mask transformer for universal image segmentation\",\n        \"Fast 2d convolution algorithms for convolutional neural networks\",\n        \"Rethinking attention with performers\",\n        \"Palm: Scaling language modeling with pathways\",\n        \"Fast and accurate deep network learning by exponential linear units (elus)\",\n        \"Turing-NLG: A 17-billion-parameter language model by Microsoft\",\n        \"Autoaugment: Learning augmentation policies from data\",\n        \"Randaugment: Practical automated data augmentation with a reduced search space\",\n        \"Supermix: Supervising the mixing data augmentation\",\n        \"Chataug: Leveraging chatgpt for text data augmentation\",\n        \"Flashattention: Fast and memory-efficient exact attention with io-awareness\",\n        \"A scalable pipeline for gigapixel whole slide imaging analysis on leadership class hpc systems\",\n        \"Metainit: Initializing learning by learning to initialize\",\n        \"Scenic: A jax library for computer vision research and beyond\",\n        \"Scaling vision transformers to 22 billion parameters\",\n        \"Imagenet: A large-scale hierarchical image database\",\n        \"Quasi-newton methods, motivation and theory\",\n        \"8-bit optimizers via block-wise quantization\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Improved regularization of convolutional neural networks with cutout\",\n        \"Sequential normalization: Embracing smaller sample sizes for normalization\",\n        \"Progressive multi-granularity training for non-autoregressive translation\",\n        \"Re-parameterizing your optimizers rather than architectures\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Adaptive subgradient methods for online learning and stochastic optimization\",\n        \"Resist: Layer-wise decomposition of resnets for distributed training\",\n        \"Understanding back-translation at scale\",\n        \"Learning and development in neural networks: The importance of starting small\",\n        \"Fairscale: A general purpose modular pytorch library for high performance and large scale training\",\n        \"Adaptable butterfly accelerator for attention-based nns via hardware and algorithm co-design\",\n        \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity\",\n        \"(beta) channels last memory format in pytorch, 2022\",\n        \"A survey of data augmentation approaches for nlp\",\n        \"Sharpness-aware minimization for efficiently improving generalization\",\n        \"The lottery ticket hypothesis: Finding sparse, trainable neural networks\",\n        \"M-fac: Efficient matrix-free approximations of second-order information\",\n        \"Bagua: Scaling up distributed learning with system relaxations, 2021\",\n        \"The pile: An 800gb dataset of diverse text for language modeling\",\n        \"Cramming: Training a language model on a single gpu in one day\",\n        \"A survey on efficient convolutional neural networks and hardware acceleration\",\n        \"Resprop: Reuse sparsified backpropagation\",\n        \"On the computational inefficiency of large batch sizes for stochastic gradient descent\",\n        \"Efficient training of bert by progressively stacking\",\n        \"Accurate, large minibatch sgd: Training imagenet in 1 hour\",\n        \"Self-supervised pretraining of visual features in the wild\",\n        \"On the transformer growth for progressive bert training\",\n        \"Sequence-level mixed sample data augmentation\",\n        \"Shampoo: Preconditioned stochastic tensor optimization\",\n        \"Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit\",\n        \"Turbo training with token dropout\",\n        \"Pre-trained models: Past, present and future\",\n        \"Accelerating parallel stochastic gradient descent via non-blocking mini-batches\",\n        \"Deep residual learning for image recognition\",\n        \"Masked autoencoders are scalable vision learners\",\n        \"Bag of tricks for image classification with convolutional neural networks\",\n        \"Large-scale deep learning optimizations: A comprehensive survey\",\n        \"Flax: A neural network library and ecosystem for JAX, 2023\",\n        \"Gaussian error linear units (gelus)\",\n        \"Using pre-training can improve model robustness and uncertainty\",\n        \"Augmix: A simple data processing method to improve robustness and uncertainty\",\n        \"Haiku: Sonnet for JAX, 2020\",\n        \"Population based augmentation: Efficient learning of augmentation policy schedules\",\n        \"Train longer, generalize better: closing the generalization gap in large batch training of neural networks\",\n        \"Training compute-optimal large language models\",\n        \"Stylemix: Separating content and style for enhanced data augmentation\",\n        \"Fastai - progressive resizing\",\n        \"Fastai: A layered api for deep learning\",\n        \"Transformer quality in linear time\",\n        \"Hardware-aware quantization/mapping strategies for compute-in-memory accelerators\",\n        \"Improving transformer optimization through better initialization\",\n        \"Gpipe: Efficient training of giant neural networks using pipeline parallelism\",\n        \"Decoupled parallel backpropagation with convergence guarantee\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"Averaging weights leads to wider optima and better generalization\",\n        \"How to train bert with an academic budget\",\n        \"Sparse is enough in scaling transformers\",\n        \"A data-loader tunable knob to shorten gpu idleness for distributed deep learning\",\n        \"Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes\",\n        \"Accelerating deep learning by focusing on the biggest losers\",\n        \"Stop wasting my time! saving days of imagenet and BERT training with latest weight averaging\",\n        \"Scaling laws for neural language models\",\n        \"Progressive growing of gans for improved quality, stability, and variation\",\n        \"Biased importance sampling for deep neural network training\",\n        \"Not all samples are created equal: Deep learning with importance sampling\",\n        \"Ordered sgd: A new stochastic optimization framework for empirical risk minimization\",\n        \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n        \"Supervised contrastive learning\",\n        \"Co-mixup: Saliency guided joint mixup with supermodular diversity\",\n        \"Adam: A method for stochastic optimization\",\n        \"Reformer: The efficient transformer\",\n        \"Self-normalizing neural networks\",\n        \"Accelerating self-supervised learning via efficient training strategies\",\n        \"Peeling the onion: Hierarchical reduction of data redundancy for efficient vision transformer training\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"On weight initialization in deep neural networks\",\n        \"Christopher Akiki\",\n        \"Ffcv: Accelerating training by removing data bottlenecks\",\n        \"Deduplicating training data makes language models better\",\n        \"xformers: A modular and hackable transformer modelling library\",\n        \"Shallow-to-deep training for neural machine translation\",\n        \"Automated progressive learning for efficient training of vision transformers\",\n        \"Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training\",\n        \"Deepspeed data efficiency: Improving deep learning model quality and training efficiency via efficient data sampling and routing\",\n        \"The stability-efficiency dilemma: Investigating sequence length warmup for training gpt models\",\n        \"Budgeted training: Rethinking deep neural network training under resource constraints\",\n        \"Sequence parallelism: Making 4d parallelism possible\",\n        \"Efficient quantized sparse matrix operations on tensor cores\",\n        \"Trainable weight averaging for fast convergence and better generalization\",\n        \"Mhformer: Multi-hypothesis transformer for 3d human pose estimation\",\n        \"Localvit: Bringing locality to vision transformers\",\n        \"Butterfly factorization\",\n        \"Accelerating attention through gradient-based learned runtime pruning\",\n        \"Train big, then compress: Rethinking model size for efficient training and inference of transformers\",\n        \"Fast autoaugment\",\n        \"A survey of transformers\",\n        \"Tokenmix: Rethinking image mixing for data augmentation in vision transformers\",\n        \"Ecoformer: Energy-saving attention with linear complexity\",\n        \"Communication-efficient distributed learning for large batch optimization\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n        \"Rsc: Accelerating graph neural networks training via randomized sparse computations\",\n        \"Fully convolutional networks for semantic segmentation\",\n        \"Sgdr: Stochastic gradient descent with warm restarts\",\n        \"Decoupled weight decay regularization\",\n        \"Maximizing communication efficiency for large-scale training via 0/1 adam\",\n        \"Prunetrain: fast neural network training by dynamic sparse model reconfiguration\",\n        \"HRBP: Hardware-friendly regrouping towards block-wise pruning for sparse training, 2023\",\n        \"TorchScale: Transformers at scale\",\n        \"Luna: Linear unified nested attention\",\n        \"Paddlepaddle: An open-source deep learning platform from industrial practice\",\n        \"Efficient winograd or cook-toom convolution kernel implementation on widely used mobile cpus\",\n        \"Project cgx: Scalable deep learning on commodity gpus\",\n        \"Optimizing neural networks with kronecker-factored approximate curvature\",\n        \"An empirical model of large-batch training\",\n        \"Butterflyflow: Building invertible layers with butterfly matrices\",\n        \"Mixed precision training\",\n        \"Prioritized training on points that are learnable, worth learning, and not yet learnt\",\n        \"Mindspore, 2021\",\n        \"All you need is a good init\",\n        \"Accelerating sparse deep neural networks\",\n        \"When does label smoothing help?\",\n        \"Pre-training a bert with curriculum learning by increasing block-size of input text\",\n        \"Pipedream: Generalized pipeline parallelism for dnn training\",\n        \"Efficient large-scale language model training on gpu clusters using megatron-lm\",\n        \"Improving transformers with probabilistic attention keys\",\n        \"K-sam: Sharpness-aware minimization at the speed of sgd\",\n        \"Deep model assembling\",\n        \"Machine Learning Systems: Design and Implementation\",\n        \"Bmtrain, 2021\",\n        \"fairseq: A fast, extensible toolkit for sequence modeling\",\n        \"Ia-red: Interpretability-aware redundancy reduction for vision transformers\",\n        \"Eigencurve: Optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums\",\n        \"Extremebert: A toolkit for accelerating pretraining of customized bert\",\n        \"Budgeted training for vision transformer\",\n        \"Fast vision transformers with hilo attention\",\n        \"Simpletrack: Understanding and rethinking 3d multi-object tracking\",\n        \"A survey on textual entailment based question answering\",\n        \"Kaisa: an adaptive second-order optimizer framework for deep neural networks\",\n        \"Random feature attention\",\n        \"Shortformer: Better language modeling using shorter inputs\",\n        \"Speechain: A speech toolkit for large-scale machine speech chain\",\n        \"The devil in linear transformer\",\n        \"Pre-trained models for natural language processing: A survey\",\n        \"Self-attention does not need o​(n2)𝑜superscript𝑛2o(n^{2}) memory\",\n        \"Improving language understanding by generative pre-training\",\n        \"Language models are unsupervised multitask learners\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Scaling language models: Methods, analysis & insights from training gopher\",\n        \"Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability\",\n        \"Zero: Memory optimizations toward training trillion parameter models\",\n        \"Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning\",\n        \"Zero-shot text-to-image generation\",\n        \"Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters\",\n        \"Ray: A distributed framework for emerging ai applications\",\n        \"You only look once: Unified, real-time object detection\",\n        \"Combiner: Full attention transformer with sparse computation cost\",\n        \"Zero-offload: Democratizing billion-scale model training\",\n        \"A stochastic approximation method\",\n        \"Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension\",\n        \"Imagenet large scale visual recognition challenge\",\n        \"Bitblade: Energy-efficient variable bit-precision hardware accelerator for quantized neural networks\",\n        \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\",\n        \"What language model to train if you have one million gpu hours?\",\n        \"Regularized nonlinear acceleration\",\n        \"Nonlinear acceleration of stochastic algorithms\",\n        \"1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns\",\n        \"Improving neural machine translation models with monolingual data\",\n        \"Horovod: fast and easy distributed deep learning in TensorFlow\",\n        \"Glu variants improve transformer\",\n        \"Megatron-lm: Training multi-billion parameter language models using model parallelism\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Feature wise normalization: An effective way of normalizing data\",\n        \"Cyclical learning rates for training neural networks\",\n        \"Super-convergence: Very fast training of neural networks using large learning rates\",\n        \"Gradual dropin of layers to train very deep neural networks\",\n        \"Don’t decay the learning rate, increase the batch size\",\n        \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model\",\n        \"Dnn training acceleration via exploring gpgpu friendly sparsity\",\n        \"How to train your vit? data, augmentation, and regularization in vision transformers\",\n        \"A monolingual approach to contextualized word embeddings for mid-resource languages\",\n        \"Prioritizing samples in reinforcement learning with reducible loss\",\n        \"Revisiting unreasonable effectiveness of data in deep learning era\",\n        \"Ernie: Enhanced representation through knowledge integration\",\n        \"Random walk initialization for training very deep feedforward networks\",\n        \"On the importance of initialization and momentum in deep learning\",\n        \"Profiling and improving the pytorch dataloader for high-latency storage: A technical report\",\n        \"Going deeper with convolutions\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"Inception-v4, inception-resnet and the impact of residual connections on learning\",\n        \"Convolutional neural networks with low-rank regularization\",\n        \"Adversarial attack and defense strategies of speaker recognition systems: A survey\",\n        \"Efficientnetv2: Smaller models and faster training\",\n        \"1-bit adam: Communication efficient large-scale training with adam’s convergence speed\",\n        \"Efficient transformers: A survey\",\n        \"What’s in my ai? a comprehensive analysis of datasets used to train gpt-1, gpt-2, gpt-3, gpt-neox-20b, megatron-11b, mt-nlg, and gopher, 2022\",\n        \"Mlp-mixer: An all-mlp architecture for vision\",\n        \"Fixing the train-test resolution discrepancy\",\n        \"Training data-efficient image transformers & distillation through attention\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Efficient methods for natural language processing: a survey\",\n        \"Saliencymix: A saliency guided data augmentation strategy for better regularization\",\n        \"Attention is all you need\",\n        \"Manifold mixup: Better representations by interpolating hidden states\",\n        \"Anderson acceleration for fixed-point iterations\",\n        \"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX\",\n        \"Deep growing learning\",\n        \"Haq: Hardware-aware automated quantization with mixed precision\",\n        \"Bfloat16: The secret to high performance on cloud tpus — google cloud blog, 2020\",\n        \"Linformer: Self-attention with linear complexity\",\n        \"Image as a foreign language: Beit pretraining for all vision and vision-language tasks\",\n        \"That’s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets\",\n        \"Lightseq2: Accelerated training for transformer-based models on gpus\",\n        \"LightSeq: A high performance inference library for transformers\",\n        \"Efficienttrain: Exploring generalized curriculum learning for training visual backbones\",\n        \"One-dimensional deep low-rank and sparse network for accelerated mri\",\n        \"Eda: Easy data augmentation techniques for boosting performance on text classification tasks\",\n        \"Emergent abilities of large language models\",\n        \"Fastformer: Additive attention can be all you need\",\n        \"When do curricula work?\",\n        \"Cross-domain collaborative normalization via structural knowledge\",\n        \"Fast deep learning training through intelligently freezing layers\",\n        \"A deep learning dataloader with shared data preparation\",\n        \"Data selection for language models via importance resampling\",\n        \"Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models, 2022b\",\n        \"Nyströmformer: A nyström-based algorithm for approximating self-attention\",\n        \"An efficient 2d method for training super-large deep learning models\",\n        \"Vitae: Vision transformer advanced by exploring intrinsic inductive bias\",\n        \"mt5: A massively multilingual pre-trained text-to-text transformer\",\n        \"Large-batch optimization for dense visual predictions\",\n        \"Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers\",\n        \"Large batch training of convolutional networks\",\n        \"Imagenet training in minutes\",\n        \"Decentralized training of foundation models in heterogeneous environments\",\n        \"Cutmix: Regularization strategy to train strong classifiers with localizable features\",\n        \"Adadelta: an adaptive learning rate method\",\n        \"Scaling vision transformers\",\n        \"Root mean square layer normalization\",\n        \"mixup: Beyond empirical risk minimization\",\n        \"Residual learning without normalization via better initialization\",\n        \"Autoassist: A framework to accelerate training of deep neural networks\",\n        \"Accelerating training of transformer-based language models with progressive layer dropping\",\n        \"Character-level convolutional networks for text classification\",\n        \"Adversarial autoaugment\",\n        \"Mixhead: Breaking the low-rank bottleneck in multi-head attention language models\",\n        \"Zero initialization: Initializing neural networks with only zeros and ones\",\n        \"Differentiable augmentation for data-efficient gan training\",\n        \"A faster algorithm for reducing the computational complexity of convolutional neural networks\",\n        \"Random erasing data augmentation\",\n        \"A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\",\n        \"Gradinit: Learning to initialize neural networks for stable and efficient training\",\n        \"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books\",\n        \"A survey on efficient training of transformers\"\n    ],\n    \"reasoning\": \"Selected these 302 references because they represent foundational work and recent advances in On Efficient Training of Large-Scale Deep Learning Models: A Literature Review, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}