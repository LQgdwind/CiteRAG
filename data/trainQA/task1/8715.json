{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: CLIP-Count: Towards Text-Guided Zero-Shot Object Counting\n\nPaper Abstract: Abstract. Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to downstream tasks such as object detection and segmentation. Adapting these models for object counting, however, remains a formidable challenge. In this study, we first investigate transferring vision-language models (VLMs) for class-agnostic object counting. Specifically, we propose CLIP-Count, the first end-to-end pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner. To align the text embedding with dense visual features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level visual representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module to propagate semantic information across different resolution levels of visual features. Benefiting from the full exploitation of the rich image-text alignment knowledge of pretrained VLMs, our method effectively generates high-quality density maps for objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech crowd counting datasets demonstrate state-of-the-art accuracy and generalizability of the proposed method. Code is available: https://github.com/songrise/CLIP-Count.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Emerging properties in self-supervised vision transformers\",\n        \"The number sense: How the mind creates mathematics\",\n        \"A Low-Shot Object Counting Network With Iterative Prototype Adaptation\",\n        \"An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"Dr. vic: Decomposition and reasoning for video individual counting\",\n        \"Learning to Count Anything: Reference-less Class-agnostic Counting with Weak Supervision\",\n        \"AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars\",\n        \"Drone-based object counting by spatially regularized regional proposal network\",\n        \"Scaling up visual and vision-language representation learning with noisy text supervision\",\n        \"Visual prompt tuning\",\n        \"Video Crowd Localization With Multifocus Gaussian Neighborhood Attention and a Large-Scale Benchmark\",\n        \"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation\",\n        \"Exploring plain vision transformer backbones for object detection\",\n        \"Clip surgery for better explainability with enhancement in open-vocabulary tasks\",\n        \"CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model\",\n        \"Object Counting: You Only Need to Look at One\",\n        \"GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning\",\n        \"Scale-Prior Deformable Convolution for Exemplar-Guided Class-Agnostic Counting\",\n        \"Countr: Transformer-based generalised visual counting\",\n        \"Efficient crowd counting via structured knowledge transfer\",\n        \"Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting\",\n        \"Crowd counting with deep structured scale integration network\",\n        \"Crowd counting using deep recurrent spatial-aware network\",\n        \"Decoupled weight decay regularization\",\n        \"Class-agnostic counting\",\n        \"Representation learning with contrastive predictive coding\",\n        \"Teaching CLIP to Count to Ten\",\n        \"Styleclip: Text-driven manipulation of stylegan imagery\",\n        \"Crowd counting via multi-view scale aggregation networks\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Exemplar free class agnostic counting\",\n        \"Learning to count everything\",\n        \"Denseclip: Language-guided dense prediction with context-aware prompting\",\n        \"Clip-forge: Towards zero-shot text-to-shape generation\",\n        \"Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting\",\n        \"Generating high-quality crowd density maps using contextual pyramid cnns\",\n        \"NeRF-Art: Text-Driven Neural Radiance Fields Stylization\",\n        \"GCNet: Probing Self-Similarity Learning for Generalized Counting Network\",\n        \"Cris: Clip-driven referring image segmentation\",\n        \"Multimodal Crowd Counting with Mutual Attention Transformers\",\n        \"Zero-shot Object Counting\",\n        \"Class-agnostic few-shot object counting\",\n        \"Few-shot object counting with similarity-aware feature enhancement\",\n        \"Crowd counting via scale-communicative aggregation networks\",\n        \"Pointclip: Point cloud understanding by clip\",\n        \"Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras\",\n        \"Single-image crowd counting via multi-column convolutional neural network\",\n        \"Regionclip: Region-based language-image pretraining\",\n        \"Conditional prompt learning for vision-language models\",\n        \"Learning to prompt for vision-language models\",\n        \"ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in CLIP-Count: Towards Text-Guided Zero-Shot Object Counting, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}