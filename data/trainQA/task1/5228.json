{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 62 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators’ ExpertiseThe author ordering is alphabetical due to coincidence and not by policy [1].\n\nPaper Abstract: Abstract Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientific papers, and peer assessment of employees in organizations. Since an individual’s own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators’ expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators’ expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.\n\n{retrieved_text}\n\nGenerate exactly 62 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 62\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"There’s lots in a name (whereas there shouldn’t be)\",\n        \"A case for ordinal peer-evaluation in MOOCs\",\n        \"Peer assessment in MOOCs using preference learning via matrix factorization\",\n        \"Tuned models of peer assessment in MOOCs\",\n        \"Design and analysis of the NIPS 2016 review process\",\n        \"Reviewer bias in single-versus double-blind peer review\",\n        \"An overview of challenges, experiments, and computational solutions in peer review\",\n        \"Performance appraisal: An update\",\n        \"Are peer reviews the future of performance evaluations?\",\n        \"Peer review and competition in the art exhibition game\",\n        \"Peer-review in a world with rational scientists: Toward selection of the average\",\n        \"Telescope time without tears: A distributed approach to peer review\",\n        \"Incentives, quality, and risks: A look into the NSF proposal review pilot\",\n        \"Sum of us: Strategyproof selection from the selectors\",\n        \"Impartial nominations for a prize\",\n        \"A near-optimal mechanism for impartial selection\",\n        \"Optimal impartial selection\",\n        \"Strategyproof peer selection: Mechanisms, analyses, and experiments\",\n        \"Strategyproof peer selection using randomization, partitioning, and apportionment\",\n        \"PeerNomination: Relaxing exactness for increased accuracy in peer selection\",\n        \"Ranking wily people who rank each other\",\n        \"On strategyproof conference peer review\",\n        \"Approximate mechanism design without money\",\n        \"Truthful assignment without money\",\n        \"Scheduling without payments\",\n        \"Mix and match: A strategyproof mechanism for multi-hospital kidney exchange\",\n        \"Impartial peer review\",\n        \"The Toronto Paper Matching System: An automated paper-reviewer assignment system\",\n        \"Expertise modeling for matching papers with reviewers\",\n        \"A SUPER* algorithm to optimize paper bidding in peer review\",\n        \"A market-inspired bidding scheme for peer review paper assignment\",\n        \"The AI conference paper assignment problem\",\n        \"On the optimal assignment of conference papers to reviewers\",\n        \"Expertise matching via constraint-based optimization\",\n        \"A framework for optimizing paper matching\",\n        \"The new automated IEEE INFOCOM review assignment system\",\n        \"Assigning papers to referees\",\n        \"PeerReview4All: Fair and accurate reviewer assignment in peer review\",\n        \"Minimum cost flows, MDPs, and ℓ1subscriptℓ1\\\\ell_{1}-regression in nearly linear time for dense instances\",\n        \"Proof of a conjecture of P. Erdos\",\n        \"A fast algorithm for equitable coloring\",\n        \"Node-and edge-deletion NP-complete problems\",\n        \"Paper matching with local fairness constraints\",\n        \"OpenReview explorer\",\n        \"Near-optimal reviewer splitting in two-phase paper reviewing and conference experiment design\",\n        \"Catch me if i can: Detecting strategic behaviour in peer assessment\",\n        \"China’s publication bazaar\",\n        \"Publishing: The peer-review scam\",\n        \"How many scientists fabricate and falsify research? a systematic review and meta-analysis of survey data\",\n        \"Perceptions of ethical problems with scientific journal peer review: An exploratory study\",\n        \"Potential organized fraud in ACM/IEEE computer architecture conferences\",\n        \"Collusion rings threaten the integrity of computer science research\",\n        \"Mitigating manipulation in peer review via randomized reviewer assignments\",\n        \"Making paper reviewing robust to bid manipulation attacks\",\n        \"Commensuration bias in peer review\",\n        \"Loss functions, axioms, and peer review\",\n        \"How to calibrate the scores of biased reviewers by quadratic programming\",\n        \"Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings\",\n        \"On testing for biases in peer review\",\n        \"Uncovering latent biases in text: Method and application to peer review\",\n        \"Prior and prejudice: The novice reviewers’ bias against resubmissions in conference peer review\",\n        \"Debiasing evaluations that are biased by evaluations\"\n    ],\n    \"reasoning\": \"Selected these 62 references because they represent foundational work and recent advances in Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators’ ExpertiseThe author ordering is alphabetical due to coincidence and not by policy [1]., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}