{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 51 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: From Understanding Genetic Drift to a Smart-Restart Parameter-less Compact Genetic AlgorithmExtended version of a paper [DZ20a] that appeared in the proceedings of GECCO 2020. This version also discusses how the algorithms considered behave in the presence of additive posterior noise. Also, this version contains all mathematical proofs. Authors are given in alphabetic order. Both authors contributed equally to this work and both act as corresponding authors.\n\nPaper Abstract: Abstract One of the key difficulties in using estimation-of-distribution algorithms is choosing the population size(s) appropriately: Too small values lead to genetic drift, which can cause enormous difficulties. In the regime with no genetic drift, however, often the runtime is roughly proportional to the population size, which renders large population sizes inefficient. Based on a recent quantitative analysis which population sizes lead to genetic drift, we propose a parameter-less version of the compact genetic algorithm that automatically finds a suitable population size without spending too much time in situations unfavorable due to genetic drift. We prove a mathematical runtime guarantee for this algorithm and conduct an extensive experimental analysis on four classic benchmark problems both without and with additive centered Gaussian posterior noise. The former shows that under a natural assumption, our algorithm has a performance very similar to the one obtainable from the best problem-specific population size. The latter confirms that missing the right population size in the original cGA can be detrimental and that previous theory-based suggestions for the population size can be far away from the right values; it also shows that our algorithm as well as a previously proposed parameter-less variant of the cGA based on parallel runs avoid such pitfalls. Comparing the two parameter-less approaches, ours profits from its ability to abort runs which are likely to be stuck in a genetic drift situation.\n\n{retrieved_text}\n\nGenerate exactly 51 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 51\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Runtime analysis for the (μ+λ) EA optimizing OneMax\",\n        \"The (1+(λ,λ)) GA is even faster on multimodal problems\",\n        \"Optimal fixed and adaptive mutation rates for the LeadingOnes problem\",\n        \"A sharp estimate of the binomial mean absolute deviation with applications\",\n        \"Theory of parameter control for discrete black-box optimization: provable performance gains through dynamic parameter choices\",\n        \"Self-adjusting mutation rates with provably optimal success rules\",\n        \"Escaping local optima with diversity mechanisms and crossover\",\n        \"Ants easily solve stochastic shortest path problems\",\n        \"On the analysis of the (1+1) evolutionary algorithm\",\n        \"Optimizing linear functions with the (1+λ) evolutionary algorithm—different asymptotic runtimes for different instances\",\n        \"Bivariate estimation-of-distribution algorithms can find an exponential number of optima\",\n        \"Significance-based estimation-of-distribution algorithms\",\n        \"The univariate marginal distribution algorithm copes well with deception and epistasis\",\n        \"A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes\",\n        \"Simplified runtime analysis of estimation of distribution algorithms\",\n        \"Fast genetic algorithms\",\n        \"Analyzing randomized search heuristics via stochastic domination\",\n        \"Does comma selection help to cope with local optima?\",\n        \"The runtime of the compact genetic algorithm on jump functions\",\n        \"A rigorous analysis of the compact genetic algorithm for linear functions\",\n        \"A parameter-less compact genetic algorithm\",\n        \"Sharp bounds for genetic drift in estimation-of-distribution algorithms\",\n        \"EDAs cannot be balanced and stable\",\n        \"The compact genetic algorithm is efficient under extreme Gaussian noise\",\n        \"Robustness of populations in stochastic environments\",\n        \"Rigorous hitting times for binary mutations\",\n        \"Parameter-less population pyramid\",\n        \"A parameter-less genetic algorithm\",\n        \"The compact genetic algorithm\",\n        \"On the runtime dynamics of the compact genetic algorithm on jump functions\",\n        \"On the choice of the offspring population size in evolutionary algorithms\",\n        \"Theory of estimation-of-distribution algorithms\",\n        \"Lower bounds on the run time of the Univariate Marginal Distribution Algorithm on OneMax\",\n        \"Estimation of Distribution Algorithms\",\n        \"Parameter-less optimization with the extended compact genetic algorithm and iterated local search\",\n        \"On the limitations of the univariate marginal distribution algorithm to deception and where bivariate EDAs might help\",\n        \"Simple hyper-heuristics control the neighbourhood size of randomised local search optimally for LeadingOnes\",\n        \"Medium step sizes are harmful for the compact genetic algorithm\",\n        \"From recombination of genes to the estimation of distributions I. Binary parameters\",\n        \"How genetic algorithms really work: mutation and hillclimbing\",\n        \"Parameter-less hierarchical BOA\",\n        \"The benefits and limitations of voting mechanisms in evolutionary optimisation\",\n        \"The choice of the offspring population size in the (1, λ) evolutionary algorithm\",\n        \"Self-adjusting evolutionary algorithms for multimodal optimization\",\n        \"A simple ant colony optimizer for stochastic shortest path problems\",\n        \"A new method for lower bounds on the running time of evolutionary algorithms\",\n        \"On the choice of the update strength in estimation-of-distribution algorithms and ant colony optimization\",\n        \"Runtime analysis of the (μ + 1) EA on simple pseudo-Boolean functions\",\n        \"Exploration and exploitation without mutation: solving the jump function in Θ​(n) time\",\n        \"Choosing the right algorithm with hints from complexity theory\",\n        \"On the convergence of a class of estimation of distribution algorithms\"\n    ],\n    \"reasoning\": \"Selected these 51 references because they represent foundational work and recent advances in From Understanding Genetic Drift to a Smart-Restart Parameter-less Compact Genetic AlgorithmExtended version of a paper [DZ20a] that appeared in the proceedings of GECCO 2020. This version also discusses how the algorithms considered behave in the presence of additive posterior noise. Also, this version contains all mathematical proofs. Authors are given in alphabetic order. Both authors contributed equally to this work and both act as corresponding authors., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}