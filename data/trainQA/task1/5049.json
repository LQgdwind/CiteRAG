{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 259 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey on Interpretable Reinforcement Learning\n\nPaper Abstract: Abstract Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.\n\n{retrieved_text}\n\nGenerate exactly 259 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 259\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Reinforcement learning: An introduction\",\n        \"Mastering the game of Go without human knowledge\",\n        \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\",\n        \"Solving Rubik’s Cube with a Robot Hand\",\n        \"Graying the black box: Understanding DQNs\",\n        \"Deep reinforcement learning that matters\",\n        \"A Study on Overfitting in Deep Reinforcement Learning\",\n        \"Adversarial Attacks on Neural Network Policies\",\n        \"AI Now Report\",\n        \"Building ethics into artificial intelligence\",\n        \"Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector\",\n        \"From what to how: An initial review of publicly available AI ethics tools, methods and research to translate principles into practices\",\n        \"Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward\",\n        \"Fairness through awareness\",\n        \"The (Im)possibility of fairness: Different value systems require different mechanisms for fair decision making\",\n        \"A Survey on Bias and Fairness in Machine Learning\",\n        \"Concrete Problems in AI Safety\",\n        \"Accountability of AI Under the Law: The Role of Explanation\",\n        \"Ethics guidelines for trustworthy AI\",\n        \"Data, privacy, and the greater good\",\n        \"The Trolley, The Bull Bar, and Why Engineers Should Care About The Ethics of Autonomous Cars [point of view]\",\n        \"A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems\",\n        \"The Societal Implications of Deep Reinforcement Learning\",\n        \"Explainable reinforcement learning: A survey\",\n        \"Reinforcement Learning Interpretation Methods: A Survey\",\n        \"Explainability in deep reinforcement learning\",\n        \"Markov decision processes: discrete stochastic dynamic programming\",\n        \"“Human-level control through deep reinforcement learning\",\n        \"“Proximal Policy Optimization Algorithms\",\n        \"“Addressing Function Approximation Error in Actor-Critic Methods\",\n        \"“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n        \"“Combined reinforcement learning via abstract representations\",\n        \"“Entity abstraction in visual model-based reinforcement learning\",\n        \"“A Physics-Based Model Prior for Object-Oriented MDPs\",\n        \"“Recent Advances in Hierarchical Reinforcement Learning\",\n        \"“Model-Agnostic Interpretability of Machine Learning\",\n        \"“Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n        \"“Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\",\n        \"“The Mythos of Model Interpretability\",\n        \"N. Díaz-Rodríguez\",\n        \"“Directions for Explainable Knowledge-Enabled Systems\",\n        \"“Explaining Explanations: An Overview of Interpretability of Machine Learning\",\n        \"“The Pragmatic Turn in Explainable Artificial Intelligence (XAI)\",\n        \"“Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n        \"“Learning Programmatically Structured Representations with Perceptor Gradients\",\n        \"“Learning the structure of factored Markov decision processes in reinforcement learning problems\",\n        \"“Relational reinforcement learning\",\n        \"“An introduction to first-order logic\",\n        \"“Knowledge Representation\",\n        \"“Blocks World Revisited\",\n        \"“Generalizing plans to new environments in relational MDPs\",\n        \"“Learning Digger using Hierarchical Reinforcement Learning for Concurrent Goals.” in EWRL\",\n        \"“Symbolic Learning for Adaptive Agents\",\n        \"“Relational Reinforcement Learning via Sampling the Space of First-Order Conjunctive Features\",\n        \"“Simultaneous Learning of Structure and Value in Relational Reinforcement Learning\",\n        \"“Graph Kernels and Gaussian Processes for Relational Reinforcement Learning\",\n        \"“Symbolic Network: Generalized Neural Policies for Relational MDPs\",\n        \"“Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks\",\n        \"“Stochastic dynamic programming with factored representations\",\n        \"“Probabilistic relational models\",\n        \"“A model for reasoning about persistence and causation\",\n        \"“Relational Dynamic Influence Diagram Language (RDDL): Language Description\",\n        \"“PPDDL1.0: The language for the probabilistic part of IPC-4\",\n        \"“An object-oriented representation for efficient reinforcement learning\",\n        \"“Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning\",\n        \"“A survey of reinforcement learning in relational domains\",\n        \"J. Schrittwieser\",\n        \"The Logic of Adaptive Behavior Knowledge Representation and Algorithms for Adaptive Sequential Decision Making under Uncertainty in First Order and Relational Domains\",\n        \"“Solving relational and first order logical markov decision processes: A survey\",\n        \"“Relational reinforcement learning with guided demonstrations\",\n        \"“The symbol grounding problem\",\n        \"“Constructing Symbolic Representations for High-Level Planning\",\n        \"“Symbol Acquisition for Probabilistic High-Level Planning\",\n        \"“Active Exploration for Learning Symbolic Representations\",\n        \"“From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning\",\n        \"“Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\",\n        \"“Active Learning for Teaching a Robot Grounded Relational Symbols.” in IJCAI\",\n        \"“Program Guided Agent\",\n        \"“Object-sensitive Deep Reinforcement Learning\",\n        \"“Towards Deep Symbolic Reinforcement Learning\",\n        \"“Unsupervised video object segmentation for deep reinforcement learning\",\n        \"“Symbolic Relation Networks for Reinforcement Learning\",\n        \"“Deep reinforcement learning with relational inductive biases\",\n        \"Template Matching Techniques in Computer Vision: Theory and Practice. Wiley Publishing\",\n        \"“Robust real-time object detection\",\n        \"“Transparency and Explanation in Deep Reinforcement Learning Neural Networks\",\n        \"“Dimensions of neural-symbolic integration — a structured survey\",\n        \"“A Comparison between deep Q-networks and deep symbolic reinforcement learning\",\n        \"“Towards Symbolic Reinforcement Learning with Common Sense\",\n        \"“Relational inductive biases\",\n        \"“The Graph Neural Network Model\",\n        \"“Neural message passing for quantum chemistry\",\n        \"A. Sanchez Gonzalez\",\n        \"“Graph networks as learnable physics engines for inference and control\",\n        \"“Gated Graph Sequence Neural Networks\",\n        \"“Attention is all you need\",\n        \"“NerveNet: Learning Structured Policy with Graph Neural Networks\",\n        \"“A simple neural network module for relational reasoning\",\n        \"“A Compositional Object-Based Approach to Learning Physical Dynamics\",\n        \"“Object Detection and Tracking Algorithms for Vehicle Counting: A Comparative Analysis\",\n        \"“Learning physical graph representations from visual scenes\",\n        \"“Logic tensor networks for semantic image interpretation\",\n        \"“Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge\",\n        \"“Towards a Definition of Disentangled Representations\",\n        \"“Hierarchical reinforcement learning\",\n        \"“Dot-to-dot: Explainable hierarchical reinforcement learning for robotic manipulation\",\n        \"“REBA: A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics\",\n        \"“Modular multitask reinforcement learning with policy sketches\",\n        \"“Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning\",\n        \"“Model primitive hierarchical lifelong reinforcement learning\",\n        \"“A synthesis of automated planning and reinforcement learning for efficient\",\n        \"“PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making\",\n        \"“Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots\",\n        \"“SDRL: Interpretable and Data-Efficient Deep Reinforcement Learning Leveraging Symbolic Planning\",\n        \"“Induction and Exploitation of Subgoal Automata for Reinforcement Learning\",\n        \"“A formal methods approach to interpretable reinforcement learning for robotic planning\",\n        \"“Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning\",\n        \"“Robot Representation and Reasoning with Knowledge from Reinforcement Learning\",\n        \"“LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning\",\n        \"“Plan-based reward shaping for reinforcement learning\",\n        \"“Automated planning\",\n        \"“Symbolic Plans as High-Level Instructions for Reinforcement Learning\",\n        \"“A Survey of Knowledge-based Sequential Decision Making under Uncertainty\",\n        \"“Algorithms for Inverse Reinforcement Learning\",\n        \"“Learning symbolic models of stochastic domains\",\n        \"“Building relational world models for reinforcement learning\",\n        \"“Efficient Learning of Relational Models for Sequential Decision Making\",\n        \"“Learning relational dynamics of stochastic domains for planning\",\n        \"“Learning Graph-Based Representations for Continuous Reinforcement Learning Domains\",\n        \"M. Lázaro-Gredilla\",\n        \"“Interpretable dynamics models for data-efficient reinforcement learning\",\n        \"“Composable Planning with Attributes\",\n        \"“Search on the replay buffer: Bridging planning and reinforcement learning\",\n        \"“Interaction networks for learning about objects\",\n        \"“Unsupervised learning for physical interaction through video prediction\",\n        \"“Deep visual foresight for planning robot motion\",\n        \"“Object-oriented dynamics predictor\",\n        \"“Object-Oriented Dynamics Learning through Multi-Level Abstraction\",\n        \"“Unsupervised Object-Level Deep Reinforcement Learning\",\n        \"“Learning Agents for Uncertain Environments\",\n        \"“Learning to drive a bicycle using reinforcement learning and shaping\",\n        \"“Value Alignment or Misalignment – What Will Keep Systems Accountable?” in AAAI Workshop\",\n        \"“Alvinn: An autonomous land vehicle in a neural network\",\n        \"“Algorithmic Perspective on Imitation Learning\",\n        \"“A survey of inverse reinforcement learning: Challenges\",\n        \"“Creating advice-taking reinforcement learners\",\n        \"“Guiding Autonomous Agents to Better Behaviors through Human Advice\",\n        \"“Preference Elicitation and Inverse Reinforcement Learning\",\n        \"“Interactive Q-Learning with Ordinal Rewards and Unreliable Tutor\",\n        \"“Inverse reinforcement learning in relational domains\",\n        \"“Relational reinforcement learning for planning with exogenous effects\",\n        \"“Interpretable batch IRL to extract clinician goals in ICU hypotension management\",\n        \"“Q-Learning for robust satisfaction of signal temporal logic specifications\",\n        \"“Environment-Independent Task Specifications via GLTL\",\n        \"“Reinforcement learning with temporal logic rewards\",\n        \"“Teaching Multiple Tasks to an RL Agent using LTL\",\n        \"“Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning\",\n        \"“Deep Reinforcement Learning with Temporal Logics\",\n        \"“Learning Reward Machines for Partially Observable Reinforcement Learning\",\n        \"“Joint Inference of Reward Machines and Policies for Reinforcement Learning\",\n        \"“Reinforcement Learning with Non-Markovian Rewards\",\n        \"“A Boolean Task Algebra for Reinforcement Learning\",\n        \"“Deep Neural Decision Trees\",\n        \"“Interpretable Apprenticeship Learning with Temporal Logic Specifications\",\n        \"“Compositionality of optimal control laws\",\n        \"“Tree-based batch mode reinforcement learning\",\n        \"“Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving\",\n        \"“Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning\",\n        \"“Neural-encoding Human Experts’ Domain Knowledge to Warm Start Reinforcement Learning\",\n        \"“Iterative Bounding MDPs: Learning Interpretable Policies via Non-Interpretable Methods\",\n        \"“Policy tree: Adaptive representation for policy gradient\",\n        \"“Policy Search in a Space of Simple Closed-form Formulas: Towards Interpretability of Reinforcement Learning\",\n        \"“Automatic Discovery of Ranking Formulas for Playing with Multi-armed Bandits\",\n        \"“Interpretable policies for reinforcement learning by genetic programming\",\n        \"“Generating interpretable reinforcement learning policies using genetic programming\",\n        \"“Learning an Interpretable Traffic Signal Control Policy\",\n        \"“Towards reinforcement learning of human readable policies\",\n        \"“Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies\",\n        \"“Learning explanatory rules from noisy data\",\n        \"“Neural Logic Reinforcement Learning\",\n        \"“Inductive Logic Programming via Differentiable Deep Neural Logic Networks\",\n        \"“Learning Algorithms via Neural Logic Networks\",\n        \"“Incorporating Relational Background Knowledge into Reinforcement Learning via Differentiable Inductive Logic Programming\",\n        \"“Differentiable logic machines\",\n        \"“Learn to Explain Efficiently via Neural Logic Inductive Learning\",\n        \"“Interpretable Reinforcement Learning With Neural Symbolic Logic\",\n        \"“Imitation-Projected Programmatic Reinforcement Learning\",\n        \"“Neurosymbolic Reinforcement Learning with Formally Verified Exploration\",\n        \"“Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning\",\n        \"“Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\",\n        \"“Neural Logic Machines\",\n        \"“Relational retrieval using a combination of path-constrained random walks\",\n        \"“Differentiable Learning of Logical Rules for Knowledge Base Reasoning\",\n        \"“Imitation Learning: A Survey of Learning Methods\",\n        \"“Policy distillation\",\n        \"“Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees\",\n        \"“Learning to use selective attention and short-term memory in sequential tasks\",\n        \"“”Why Should I Trust You?”: Explaining the Predictions of Any Classifier\",\n        \"“Verifiable Reinforcement Learning via Policy Extraction\",\n        \"“A Reduction of Imitation Learning and Structured Prediction to No-regret Online Learning\",\n        \"“Conservative Q-Improvement: Reinforcement Learning for an Interpretable Decision-Tree Policy\",\n        \"“MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees\",\n        \"“Imitation learning in relational domains: A functional-gradient boosting approach\",\n        \"“Imitation learning of car driving skills with decision trees and random forests\",\n        \"“Interpretable approximation of a deep reinforcement learning agent as a set of if-then rules\",\n        \"“Programmatically interpretable reinforcement learning\",\n        \"“An inductive synthesis framework for verifiable reinforcement learning\",\n        \"“Safe reinforcement learning via shielding\",\n        \"“From explanation to synthesis: Compositional program induction for learning from demonstration\",\n        \"“Learning Finite State Representations of Recurrent Policy Networks\",\n        \"“Teaching on a Budget: Agents Advising Agents in Reinforcement Learning\",\n        \"“Teacher-Student Framework: A Reinforcement Learning Approach\",\n        \"“Neuroevolution of Self-Interpretable Agents\",\n        \"“Towards Interpretable Reinforcement Learning Using Attention Augmented Agents\",\n        \"“Towards Better Interpretability in Deep Q-Networks\",\n        \"“Attention is not not Explanation\",\n        \"“Attention is not Explanation\",\n        \"“On Identifiability in Transformers\",\n        \"“Fast relational learning using bottom clause propositionalization with artificial neural networks\",\n        \"“Advanced building control via deep reinforcement learning\",\n        \"“Model compression\",\n        \"“L1-regularized Neural Networks are Improperly Learnable in Polynomial Time\",\n        \"“Legibility and predictability of robot motion\",\n        \"“Optimizing for Interpretability in Deep Neural Networks with Tree Regularization\",\n        \"“Semantic-based regularization for learning and inference\",\n        \"“Integrating Deep Learning with Logic Fusion for Information Extraction\",\n        \"“Injecting Logical Background Knowledge into Embeddings for Relation Extraction\",\n        \"“Lifted Rule Injection for Relation Embeddings\",\n        \"“A semantic loss function for deep learning with symbolic knowledge\",\n        \"“Adversarial Sets for Regularising Neural Link Predictors\",\n        \"“Regularizing Black-box Models for Improved Interpretability\",\n        \"“Visualizing and understanding atari agents\",\n        \"B. Krishnamurthy\",\n        \"“Attribution-based Salience Method towards Interpretable Reinforcement Learning\",\n        \"“Self-Supervised Discovering of Interpretable Features for Reinforcement Learning\",\n        \"“Attentional Bottleneck: Towards an Interpretable Deep Driving Network\",\n        \"“Interestingness Elements for Explainable Reinforcement Learning: Understanding Agents’ Capabilities and Limitations\",\n        \"“Improving Robot Controller Transparency Through Autonomous Policy Explanation\",\n        \"K. van den Bosch\",\n        \"“Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents\",\n        \"“Explainable Reinforcement Learning Through a Causal Lens\",\n        \"“Distal Explanations for Model-free Explainable Reinforcement Learning\",\n        \"“Distilling deep reinforcement learning policies in soft decision trees\",\n        \"“TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments\",\n        \"“Explainable reinforcement learning via reward decomposition\",\n        \"“Generation of Policy-Level Explanations for Reinforcement Learning\",\n        \"“Memory-Based Explainable Reinforcement Learning\",\n        \"“Visualizing Data using t-SNE\",\n        \"“A unified approach to interpreting model predictions\",\n        \"“Why these Explanations? Selecting Intelligibility Types for Explanation Goals\",\n        \"“Explaining Explanations in AI\",\n        \"“Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning\",\n        \"“The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to Be More Effective at Data Analysis\",\n        \"“Simple random search of static linear policies is competitive for reinforcement learning\",\n        \"“Program Synthesis\",\n        \"The Human Use of Human Beings\",\n        \"“Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability\",\n        \"“Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing\",\n        \"“Artificial Intelligence\"\n    ],\n    \"reasoning\": \"Selected these 259 references because they represent foundational work and recent advances in A Survey on Interpretable Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}