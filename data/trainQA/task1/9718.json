{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 47 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sampling From Autoencoders‚Äô Latent Space via Quantization And Probability Mass Function Concepts\n\nPaper Abstract: Abstract In this study, we focus on sampling from the latent space of generative models built upon autoencoders so as the reconstructed samples are lifelike images. To do to, we introduce a novel post-training sampling algorithm rooted in the concept of probability mass functions, coupled with a quantization process. Our proposed algorithm establishes a vicinity around each latent vector from the input data and then proceeds to draw samples from these defined neighborhoods. This strategic approach ensures that the sampled latent vectors predominantly inhabit high-probability regions, which, in turn, can be effectively transformed into authentic real-world images. A noteworthy point of comparison for our sampling algorithm is the sampling technique based on Gaussian mixture models (GMM), owing to its inherent capability to represent clusters. Remarkably, we manage to improve the time complexity from the previous ùí™‚Äã(n√ód√ók√ói)ùí™ùëõùëëùëòùëñ\\mathcal{O}(n\\times d\\times k\\times i) associated with GMM sampling to a much more streamlined ùí™‚Äã(n√ód)ùí™ùëõùëë\\mathcal{O}(n\\times d), thereby resulting in substantial speedup during runtime. Moreover, our experimental results, gauged through the Fr√©chet inception distance (FID) for image generation, underscore the superior performance of our sampling algorithm across a diverse range of models and datasets. On the MNIST benchmark dataset, our approach outperforms GMM sampling by yielding a noteworthy improvement of up to 0.890.890.89 in FID value. Furthermore, when it comes to generating images of faces and ocular images, our approach showcases substantial enhancements with FID improvements of 1.691.691.69 and 0.870.870.87 respectively, as compared to GMM sampling, as evidenced on the CelebA and MOBIUS datasets. Lastly, we substantiate our methodology‚Äôs efficacy in estimating latent space distributions in contrast to GMM sampling, particularly through the lens of the Wasserstein distance.\n\n{retrieved_text}\n\nGenerate exactly 47 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 47\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"On the best lattice quantizers\",\n        \"Hamiltonian variational auto-encoder\",\n        \"A geometric perspective on variational autoencoders\",\n        \"Data Augmentation in High Dimensional Low Sample Size Setting Using a Geometry-Based Variational Autoencoder\",\n        \"Pythae: Unifying Generative Autoencoders in Python - A Benchmarking Use Case\",\n        \"IùêºI-Divergence Geometry of Probability Distributions and Minimization Problems\",\n        \"Sinkhorn distances: Lightspeed computation of optimal transport\",\n        \"Hyperspherical Variational Auto-Encoders\",\n        \"PyTorch Lightning\",\n        \"From variational to deterministic autoencoders\",\n        \"Deep Learning\",\n        \"Generative adversarial nets\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"Array programming with NumPy\",\n        \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\n        \"Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\",\n        \"Matplotlib: A 2D Graphics Environment\",\n        \"Auto-Encoding Variational Bayes\",\n        \"Normalizing flows: An introduction and review of current methods\",\n        \"Gradient-Based Learning Applied to Document Recognition\",\n        \"Deep Learning Face Attributes in the Wild\",\n        \"Lattice quantization\",\n        \"Iris-gan: Learning to generate realistic iris images using convolutional gan\",\n        \"The Expectation-Maximization Algorithm\",\n        \"Sdd-fiqa: Unsupervised face image quality assessment with similarity distribution distance\",\n        \"Normalizing flows for probabilistic modeling and inference\",\n        \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n        \"Scikit-learn: Machine Learning in Python\",\n        \"Computational Optimal Transport: With Applications to Data Science\",\n        \"Deep multi-class eye segmentation for ocular biometrics\",\n        \"Deep sclera segmentation and recognition\",\n        \"Improved techniques for training gans\",\n        \"Introduction to differential and riemannian geometry\",\n        \"And the bit goes down: Revisiting the quantization of neural networks\",\n        \"Rethinking the inception architecture for computer vision\",\n        \"Haven‚Äôt i seen you before? assessing identity leakage in synthetic irises\",\n        \"Wasserstein Auto-Encoders\",\n        \"TorchVision: PyTorch‚Äôs Computer Vision library\",\n        \"Neural discrete representation learning\",\n        \"Exploring bias in sclera segmentation models: A group evaluation approach\",\n        \"SSBC 2020: Sclera segmentation benchmarking competition in the mobile environment\",\n        \"A comprehensive investigation into sclera biometrics: A novel dataset and performance study\",\n        \"Generative Latent Flow\",\n        \"Infovae: Balancing learning and inference in variational autoencoders\",\n        \"AE-FLOW: Autoencoders with normalizing flows for medical images anomaly detection\",\n        \"Latent normalizing flows for discrete sequences\",\n        \"Generation textured contact lenses iris images based on 4dcycle-gan\"\n    ],\n    \"reasoning\": \"Selected these 47 references because they represent foundational work and recent advances in Sampling From Autoencoders‚Äô Latent Space via Quantization And Probability Mass Function Concepts, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}