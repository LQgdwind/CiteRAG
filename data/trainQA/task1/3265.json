{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 174 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey of Data Augmentation Approaches for NLP\n\nPaper Abstract: Abstract Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP.\n\n{retrieved_text}\n\nGenerate exactly 174 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 174\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Data augmentation for pipeline-based speech translation\",\n        \"Do not have enough data? Deep learning to the rescue!\",\n        \"Good-enough compositional data augmentation\",\n        \"Logic-guided data augmentation and regularization for consistent question answering\",\n        \"Text augmentation using BERT for image captioning\",\n        \"On adversarial mixup resynthesis\",\n        \"Training with noise is equivalent to Tikhonov regularization\",\n        \"Using Wikipedia edits in low resource grammatical error correction\",\n        \"Neural fuzzy repair: Integrating fuzzy matches into neural machine translation\",\n        \"Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight\",\n        \"Zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking\",\n        \"Mlsmote: Approaching imbalanced multilabel learning through synthetic instance generation\",\n        \"SMOTE: Synthetic minority over-sampling technique\",\n        \"Lexical-constraint-aware neural machine translation via data augmentation\",\n        \"Finding friends and flipping frenemies: Automatic paraphrase dataset augmentation using graph theory\",\n        \"MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification\",\n        \"A group-theoretic framework for data augmentation\",\n        \"A simple framework for contrastive learning of visual representations\",\n        \"AdvAug: Robust adversarial augmentation for neural machine translation\",\n        \"Adapting neural machine translation with parallel synthetic data\",\n        \"A neural grammatical error correction system built on better pre-training and sequential transfer learning\",\n        \"ELECTRA: Pre-training text encoders as discriminators rather than generators\",\n        \"Autoaugment: Learning augmentation policies from data\",\n        \"Randaugment: Practical automated data augmentation with a reduced search space\",\n        \"An analysis of simple data augmentation for named entity recognition\",\n        \"Overview of DUC 2005\",\n        \"A kernel theory of modern data augmentation\",\n        \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Improved regularization of convolutional neural networks with cutout\",\n        \"DAGA: Data augmentation with a generation approach forLow-resource tagging tasks\",\n        \"Counterexample-guided data augmentation\",\n        \"Syntax-aware data augmentation for neural machine translation\",\n        \"Findings of the E2E NLG challenge\",\n        \"Improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation\",\n        \"Data augmentation for low-resource neural machine translation\",\n        \"Data augmentation techniques for the video question answering task\",\n        \"Patchup: A regularization technique for convolutional neural networks\",\n        \"Artificial error generation for translation-based grammatical error correction\",\n        \"GenAug: Data augmentation for finetuning text generators\",\n        \"Keep calm and switch on! Preserving sentiment and fluency in semantic text exchange\",\n        \"SMOTE for learning from imbalanced data: progress and challenges, marking the 15-year anniversary\",\n        \"GenERRate: Generating errors for use in grammatical error detection\",\n        \"Nareor: The narrative reordering problem\",\n        \"Soft contextual data augmentation for neural machine translation\",\n        \"Paraphrase augmented task-oriented dialog generation\",\n        \"The WebNLG challenge: Generating text from RDF data\",\n        \"Simple copy-paste is a strong data augmentation method for instance segmentation\",\n        \"Breaking NLI systems with sentences that require simple lexical inferences\",\n        \"Tradeoffs in data augmentation: An empirical study\",\n        \"Generalizing back-translation in neural machine translation\",\n        \"Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management\",\n        \"Neural grammatical error correction systems with unsupervised pre-training on synthetic data\",\n        \"Colorless green recurrent networks dream hierarchically\",\n        \"Sequence-level mixed sample data augmentation\",\n        \"Nonlinear mixup: Out-of-manifold data augmentation for text classification\",\n        \"It‚Äôs all in the name: Mitigating gender bias with name-based counterfactual data substitution\",\n        \"Low-shot visual recognition by shrinking and hallucinating features\",\n        \"Synthetic data for neural machine translation of spoken-dialects\",\n        \"Findings of the third workshop on neural generation and translation\",\n        \"Teaching machines to read and comprehend\",\n        \"Data augmentation instead of explicit regularization\",\n        \"TaPas: Weakly supervised table parsing via pre-training\",\n        \"Sequence-to-sequence data augmentation for dialogue language understanding\",\n        \"Learning data manipulation for augmentation and weighting\",\n        \"Multimodal continuous emotion recognition with data augmentation using recurrent neural networks\",\n        \"An empirical survey of data augmentation for time series classification with neural networks\",\n        \"Adversarial example generation with syntactically controlled paraphrase networks\",\n        \"Data recombination for neural semantic parsing\",\n        \"Certified robustness to adversarial word substitutions\",\n        \"Augmenting NLP models using latent feature interpolations\",\n        \"Speechmix - augmenting deep sound recognition using hidden space interpolations\",\n        \"Data augmentation for visual question answering\",\n        \"AdvEntuRe: Adversarial training for textual entailment with knowledge-guided examples\",\n        \"Valar nmt : Vastly lacking resources neural machine translation\",\n        \"Quantifying the evaluation of heuristic methods for textual data augmentation\",\n        \"Learning the difference that makes a difference with counterfactually-augmented data\",\n        \"Explaining the efficacy of counterfactually augmented data\",\n        \"A good sample is hard to find: Noise injection sampling and self-training for neural language generation models\",\n        \"Data augmentation by data noising for open-vocabulary slots in spoken language understanding\",\n        \"A syntactic rule-based framework for parallel data synthesis in japanese gec\",\n        \"Contextual augmentation: Data augmentation by words with paradigmatic relations\",\n        \"Syntax-guided controlled generation of paraphrases\",\n        \"Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation\",\n        \"A closer look at feature space data augmentation for few-shot intent classification\",\n        \"Neural data augmentation via example extrapolation\",\n        \"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\",\n        \"Sentence boundary augmentation for neural machine translation robustness\",\n        \"Robust training under linguistic adversity\",\n        \"A diverse data augmentation strategy for low-resource neural machine translation\",\n        \"Improving neural machine translation robustness via data augmentation: Beyond back-translation\",\n        \"Corpora generation for grammatical error correction\",\n        \"A survey of text data augmentation\",\n        \"Data boost: Text data augmentation through reinforcement learning guided conditional generation\",\n        \"Enhanced offensive language detection through data augmentation\",\n        \"An exploration of data augmentation and sampling techniques for domain-agnostic question answering\",\n        \"How effective is task-agnostic data augmentation for pretrained transformers?\",\n        \"Simple is better! lightweight data augmentation for low resource slot filling and intent classification\",\n        \"Preetam Amancharla\",\n        \"Wordnet: a lexical database for english\",\n        \"Mining revision log of language learning SNS for automated Japanese error correction of second language learners\",\n        \"Denoising pre-training and data augmentation strategies for enhanced RDF verbalization with transformers\",\n        \"Improving robustness by augmenting training sentences with predicate-argument structures\",\n        \"Multimodal dialogue state tracking by qa approach with data augmentation\",\n        \"Augmenting neural machine translation with knowledge graphs\",\n        \"SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness\",\n        \"Data diversification: A simple strategy for neural machine translation\",\n        \"Named entity recognition for social media texts with semantic augmentation\",\n        \"Multi-source neural machine translation with data augmentation\",\n        \"Abstract text summarization: A low resource challenge\",\n        \"Data augmentation with manifold exploring geometric transformations for increased performance and robustness\",\n        \"Data augmentation for abstractive query-focused multi-document summarization\",\n        \"Pushpendre Rastogi\",\n        \"Dictionary-based data augmentation for cross-domain neural machine translation\",\n        \"Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp\",\n        \"Effective data augmentation approaches to end-to-end task-oriented dialogue\",\n        \"Textual data augmentation for efficient active learning on tiny datasets\",\n        \"Language models are unsupervised multitask learners\",\n        \"Does data augmentation lead to positive margin?\",\n        \"Learning to Compose Domain-Specific Transformations for Data Augmentation\",\n        \"Multi-Modal Data Augmentation for End-to-end ASR\",\n        \"Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering\",\n        \"Data augmentation via dependency tree morphing for low-resource languages\",\n        \"Œ¥ùõø\\\\delta-encoder: an effective sample synthesis method for few-shot object recognition\",\n        \"Improving Neural Machine Translation Models with Monolingual Data\",\n        \"Substructure Substitution: Structured Data Augmentation for NLP\",\n        \"A survey on Image Data Augmentation for Deep Learning\",\n        \"Xlda: Cross-lingual data augmentation for natural language inference and question answering\",\n        \"Negative data augmentation\",\n        \"Data Augmentation for Copy-Mechanism in Dialogue State Tracking\",\n        \"Data augmentation using back-translation for context-aware neural machine translation\",\n        \"TNT-NLG, System 2: Data repetition and meaning representation manipulation to improve neural generation\",\n        \"Semantic equivalent adversarial data augmentation for visual question answering\",\n        \"Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks\",\n        \"Improving Robustness of Machine Translation with Synthetic Noise\",\n        \"A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages\",\n        \"Manifold mixup: Better representations by interpolating hidden states\",\n        \"Improving Grammatical Error Correction with Data Augmentation by Editing Latent Representation\",\n        \"Controllable Data Synthesis Method for Grammatical Error Correction\",\n        \"Data augmentation for training dialog models robust to speech recognition errors\",\n        \"A survey on face data augmentation\",\n        \"SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation\",\n        \"Few-shot text classification with triplet networks, data augmentation, and curriculum learning\",\n        \"Text augmentation in a multi-task view\",\n        \"EDA: Easy data augmentation techniques for boosting performance on text classification tasks\",\n        \"A Comparative Study of Synthetic Data Generation Methods for Grammatical Error Correction\",\n        \"Low resource multi-modal data augmentation for end-to-end ASR\",\n        \"Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings\",\n        \"Challenges in Data-to-Document Generation\",\n        \"Generalized Data Augmentation for Low-Resource Translation\",\n        \"Unsupervised data augmentation for consistency training\",\n        \"Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction\",\n        \"MDA: Multimodal Data Augmentation Framework for Boosting Performance on Image-Text Sentiment/Emotion Classification Tasks\",\n        \"Erroneous data generation for Grammatical Error Correction\",\n        \"Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering\",\n        \"G-daug: Generative data augmentation for commonsense reasoning\",\n        \"Dialog State Tracking with Reinforced Data Augmentation\",\n        \"Augmenting Image Question Answering Dataset by Exploiting Image Captions\",\n        \"Data Augmentation for Spoken Language Understanding via Joint Variational Generation\",\n        \"POMDP-Based Statistical Spoken Dialog Systems: A Review\",\n        \"Fast and accurate reading comprehension by combining self-attention and convolution\",\n        \"GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing\",\n        \"Cutmix: Regularization strategy to train strong classifiers with localizable features\",\n        \"mixup: Beyond empirical risk minimization\",\n        \"SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup\",\n        \"Editing-based SQL Query Generation for Cross-Domain Context-Dependent Questions\",\n        \"Character-Level Convolutional Networks for Text Classification\",\n        \"Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting\",\n        \"Task-oriented dialog systems that consider multiple appropriate responses under the same context\",\n        \"PAWS: Paraphrase adversaries from word scrambling\",\n        \"Gender bias in coreference resolution: Evaluation and debiasing methods\",\n        \"Data augmentation with atomic templates for spoken language understanding\",\n        \"Grounded adaptation for zero-shot executable semantic parsing\",\n        \"Transforming wikipedia into augmented data for query-focused summarization\",\n        \"Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology\"\n    ],\n    \"reasoning\": \"Selected these 174 references because they represent foundational work and recent advances in A Survey of Data Augmentation Approaches for NLP, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}