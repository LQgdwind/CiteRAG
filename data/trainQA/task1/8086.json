{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Why Target Networks Stabilise Temporal Difference Methods\n\nPaper Abstract: Abstract Integral to many recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. At the same time, a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: “why do target networks stabilise TD learning”? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad–the use of TD updates with (nonlinear) function approximation and off-policy data–which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditioning in the Jacobian of the TD update. Furthermore, we show that under mild regularity conditions and a well tuned target network update frequency, convergence can be guaranteed even in the extremely challenging off-policy sampling and nonlinear function approximation setting.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Construction of bayesian deformable models via a stochastic approximation algorithm: A convergence study\",\n        \"A projected stochastic approximation algorithm\",\n        \"Stability of stochastic approximation under verifiable conditions\",\n        \"Residual algorithms: Reinforcement learning with function approximation\",\n        \"Temporal differences-based policy iteration and applications in neuro-dynamic programming\",\n        \"Improved temporal difference methods with linear function approximation\",\n        \"A finite time analysis of temporal difference learning with linear function approximation\",\n        \"The o.d. e. method for convergence of stochastic approximation and reinforcement learning\",\n        \"Optimization methods for large-scale machine learning\",\n        \"Openai gym\",\n        \"Stochastic approximation and recursive algorithms with applications, 2nd edn by h. j. kushner and g. g. yin\",\n        \"A new convergent variant of q-learning with linear function approximation\",\n        \"Zap q-learning with nonlinear function approximation\",\n        \"Finite sample analysis for td (0) with linear function approximation\",\n        \"On the convergence of stochastic approximations under a subgeometric ergodic Markov dynamic\",\n        \"Tree-based batch mode reinforcement learning\",\n        \"A theoretical analysis of deep q-learning\",\n        \"Bayesian bellman operators\",\n        \"Addressing function approximation error in actor-critic methods\",\n        \"Gradient temporal-difference learning with regularized corrections\",\n        \"Reinforcement learning with deep energy-based policies\",\n        \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\",\n        \"Batch policy learning under constraints\",\n        \"Target-based temporal difference learning\",\n        \"Continuous control with deep reinforcement learning\",\n        \"Convergent temporal-difference learning with arbitrary smooth function approximation\",\n        \"Playing atari with deep reinforcement learning\",\n        \"Human-level control through deep reinforcement learning\",\n        \"Finite-time bounds for fitted value iteration\",\n        \"Least squares policy evaluation algorithms with linear function approximation\",\n        \"Nonconvergence to Unstable Points in Urn Models and Stochastic Approximations\",\n        \"Some methods of speeding up the convergence of iteration methods\",\n        \"Markov decision processes: discrete stochastic dynamic programming\",\n        \"Tdprop: Does jacobi preconditioning help temporal difference learning?\",\n        \"Iterative methods for sparse linear systems\",\n        \"Performance comparison of different momentum techniques on deep reinforcement learning\",\n        \"Finite-time error bounds for linear stochastic approximation and td learning\",\n        \"Learning to predict by the methods of temporal differences\",\n        \"Reinforcement Learning: An Introduction\",\n        \"A convergent o (n) algorithm for off-policy temporal-difference learning with linear function approximation\",\n        \"Fast gradient-descent methods for temporal-difference learning with linear function approximation\",\n        \"An analysis of temporal-difference learning with function approximation\",\n        \"Convergence of stochastic approximation via martingale and converse lyapunov methods\",\n        \"What are the statistical limits of offline rl with linear function approximation?\",\n        \"Instabilities of offline rl with pre-trained neural representation\",\n        \"Analysis of some incremental variants of policy iteration: First steps toward understanding actor-cr\",\n        \"Convergent reinforcement learning with function approximation: A bilevel optimization perspective\",\n        \"Preconditioned temporal difference learning\",\n        \"Breaking the deadly triad with a target network\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Why Target Networks Stabilise Temporal Difference Methods, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}