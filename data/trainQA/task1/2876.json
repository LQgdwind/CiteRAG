{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 214 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Compacting Deep Neural Networks for Internet of Things: Methods and Applications\n\nPaper Abstract: Abstract Deep Neural Networks (DNNs) have shown great success in completing complex tasks. However, DNNs inevitably bring high computational cost and storage consumption due to the complexity of hierarchical structures, thereby hindering their wide deployment in Internet-of-Things (IoT) devices, which have limited computational capability and storage capacity. Therefore, it is a necessity to investigate the technologies to compact DNNs. Despite tremendous advances in compacting DNNs, few surveys summarize compacting-DNNs technologies, especially for IoT applications. Hence, this paper presents a comprehensive study on compacting-DNNs technologies. We categorize compacting-DNNs technologies into three major types: 1) network model compression, 2) Knowledge Distillation (KD), 3) modification of network structures. We also elaborate on the diversity of these approaches and make side-by-side comparisons. Moreover, we discuss the applications of compacted DNNs in various IoT applications and outline future directions.\n\n{retrieved_text}\n\nGenerate exactly 214 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 214\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"What Is the Internet of Things (IoT)?\",\n        \"Big data analytics for large scale wireless networks: Challenges and opportunities\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"Model compression and acceleration for deep neural networks: The principles, progress, and challenges\",\n        \"A survey on deep neural network compression: Challenges, overview, and solutions\",\n        \"A comprehensive survey on model compression and acceleration\",\n        \"Model compression and hardware acceleration for neural networks: A comprehensive survey\",\n        \"Deep learning for cyber security intrusion detection: Approaches, datasets, and comparative study\",\n        \"Efficient processing of deep neural networks: A tutorial and survey\",\n        \"Backpropagation applied to handwritten zip code recognition\",\n        \"Imagenet classification with deep convolutional neural networks\",\n        \"Visualizing and understanding convolutional networks\",\n        \"Very deep convolutional networks for large-scale image recognition\",\n        \"Going deeper with convolutions\",\n        \"Deep residual learning for image recognition\",\n        \"Deep learning in neural networks: An overview\",\n        \"Long short-term memory\",\n        \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\n        \"Reducing the Dimensionality of Data with Neural Networks\",\n        \"Generative adversarial nets\",\n        \"A survey of machine learning techniques in adversarial image forensics\",\n        \"The Internet of Things (IoT): An Overview | Internet Society\",\n        \"A Survey on Network Methodologies for Real-Time Analytics of Massive IoT Data and Open Research Issues\",\n        \"Predicting complex events for pro-active IoT applications\",\n        \"Real-time big data analytics: Applications and challenges\",\n        \"Learning IoT in Edge: Deep Learning for the Internet of Things with Edge Computing\",\n        \"Handover control in wireless systems via asynchronous multiuser deep reinforcement learning\",\n        \"iraf: A deep reinforcement learning approach for collaborative mobile edge computing IoT networks\",\n        \"Deep learning-based beam management and interference coordination in dense mmwave networks\",\n        \"Learning to branch: Accelerating resource allocation in wireless networks\",\n        \"Thermal comfort modeling for smart buildings: A fine-grained deep learning approach\",\n        \"Blockchain-enabled internet of vehicles with cooperative positioning: A deep neural network approach\",\n        \"A deep-learning method for device activity detection in mmtc under imperfect csi based on variational-autoencoder\",\n        \"Cloud computing: state-of-the-art and research challenges\",\n        \"A comprehensive survey on fog computing: State-of-the-art and research challenges\",\n        \"Edge ai: On-demand accelerating deep neural network inference via edge computing\",\n        \"Fog computing and its role in the internet of things\",\n        \"Edge computing: Vision and challenges\",\n        \"ETSI - new white paper: Etsi’s mobile edge computing initiative explained\",\n        \"Fog/edge computing-based IoT (feciot): Architecture, applications, and research issues\",\n        \"Processing assignment of deep learning according to sensor node capacity\",\n        \"In-edge AI: Intelligentizing mobile edge computing, caching and communication by federated learning\",\n        \"Edge intelligence: Paving the last mile of artificial intelligence with edge computing\",\n        \"Cost-driven off-loading for dnn-based applications over cloud, edge, and end devices\",\n        \"An efficient distributed deep learning framework for fog-based IoT systems\",\n        \"Predicting Parameters in Deep Learning\",\n        \"Do deep nets really need to be deep?\",\n        \"Product quantization for nearest neighbor search\",\n        \"Approximate nearest neighbor search by residual vector quantization\",\n        \"Compressing Deep Convolutional Networks using Vector Quantization\",\n        \"“Quantized convolutional neural networks for mobile devices\",\n        \"“ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks\",\n        \"“Towards the Limit of Network Quantization\",\n        \"“Weightless: Lossy Weight Encoding For Deep Neural Network Compression\",\n        \"“Deep learning with low precision by half-wave gaussian quantization\",\n        \"“Dnq: Dynamic network quantization\",\n        \"“Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\",\n        \"“XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\",\n        \"“Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\",\n        \"“Ternary Weight Networks\",\n        \"“Asymmetric ternary networks\",\n        \"“RTN: Reparameterized Ternary Network\",\n        \"K. Gopalakrishnan\",\n        \"“Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization\",\n        \"“Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks\",\n        \"“Quantization Networks\",\n        \"“Binarized convolutional neural networks with separable filters for efficient hardware acceleration\",\n        \"“Loss-aware Binarization of Deep Networks\",\n        \"“Comparing biases for minimal network construction with back-propagation\",\n        \"Advances in Neural Information Processing Systems 2\",\n        \"“Optimal brain surgeon and general network pruning\",\n        \"“Learning both weights and connections for efficient neural networks\",\n        \"“Deep Compression: Compressing Deep Neural Networks with Pruning\",\n        \"“Dynamic Network Surgery for Efficient DNNs\",\n        \"“Pruning Filters for Efficient ConvNets\",\n        \"“Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures\",\n        \"“Structured pruning of convolutional neural networks via l1 regularization\",\n        \"“SNIP: Single-shot Network Pruning based on Connection Sensitivity\",\n        \"“A ”Network Pruning Network” Approach to Deep Model Compression\",\n        \"“Channel pruning for accelerating very deep neural networks\",\n        \"“Exploiting weight-level sparsity in channel pruning with low-rank approximation\",\n        \"“Convolutional neural network channel pruning based on regularized sparse\",\n        \"“Multi-loss-aware channel pruning of deep networks\",\n        \"“A hybrid statistics-based channel pruning method for deep cnns*\",\n        \"“A channel pruning algorithm based on depth-wise separable convolution unit\",\n        \"“Designing energy-efficient convolutional neural networks using energy-aware pruning\",\n        \"“Evaluating extended pruning on object detection neural networks\",\n        \"“Automated pruning for deep neural network compression\",\n        \"“Data-Driven Sparse Structure Selection for Deep Neural Networks\",\n        \"“Speeding up Convolutional Neural Networks with Low Rank Expansions\",\n        \"“Exploiting linear structure within convolutional networks for efficient evaluation\",\n        \"“Efficient and accurate approximations of nonlinear convolutional networks\",\n        \"“Convolutional neural networks with low-rank regularization\",\n        \"“Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications\",\n        \"“Cscc: Convolution split compression calculation algorithm for deep neural network\",\n        \"“Compact and computationally efficient representation of deep neural networks\",\n        \"“Distilling the Knowledge in a Neural Network\",\n        \"“Model compression\",\n        \"“Like What You Like: Knowledge Distill via Neuron Selectivity Transfer\",\n        \"J. van de Weijer\",\n        \"“Improved Knowledge Distillation via Teacher Assistant\",\n        \"“FitNets: Hints for Thin Deep Nets\",\n        \"“A gift from knowledge distillation: Fast optimization\",\n        \"“Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\",\n        \"“Knowledge Projection for Deep Neural Networks\",\n        \"“Correlation congruence for knowledge distillation\",\n        \"“Moonshine: Distilling with Cheap Convolutions\",\n        \"“Relational knowledge distillation\",\n        \"“Learning using privileged information: Similarity control and knowledge transfer\",\n        \"“MEAL: Multi-Model Ensemble via Adversarial Learning\",\n        \"“Variational information distillation for knowledge transfer\",\n        \"“Similarity-preserving knowledge distillation\",\n        \"“Be your own teacher: Improve the performance of convolutional neural networks via self distillation\",\n        \"“Graph-based Knowledge Distillation by Multi-head Attention Network\",\n        \"“Graph Representation Learning via Multi-task Knowledge Distillation\",\n        \"“Contrastive Representation Distillation\",\n        \"“Structured Knowledge Distillation for Dense Prediction\",\n        \"“Residual Knowledge Distillation\",\n        \"“Unifying distillation and privileged information\",\n        \"“Retaining privileged information for multi-task learning\",\n        \"“Kdgan: Knowledge distillation with generative adversarial networks\",\n        \"“Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks\",\n        \"“Revisiting knowledge transfer for training object class detectors\",\n        \"“Large scale distributed neural network training through online distillation\",\n        \"“Born Again Neural Networks\",\n        \"“Learning Global Additive Explanations for Neural Nets Using Model Distillation\",\n        \"“Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons\",\n        \"“Knowledge adaptation for efficient semantic segmentation\",\n        \"“Knowledge Flow: Improve Upon Your Teachers\",\n        \"“Knowledge distillation via adaptive instance normalization\",\n        \"“LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning\",\n        \"“Addressnet: Shift-based primitives for efficient convolutional neural networks\",\n        \"“Shufflenet: An extremely efficient convolutional neural network for mobile devices\",\n        \"“Shift: A zero flop\",\n        \"“Aggregated residual transformations for deep neural networks\",\n        \"L. Van Der Maaten\",\n        \"“Condensenet: An efficient densenet using learned group convolutions\",\n        \"“Interpretable Relative Squeezing bottleneck design for compact convolutional neural networks model\",\n        \"“Sparsely Aggregated Convolutional Networks\",\n        \"“Squeeze-and-excitation networks\",\n        \"“SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n        \"“SEP-Nets: Small and Effective Pattern Networks\",\n        \"“Xception: Deep learning with depthwise separable convolutions\",\n        \"“MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\",\n        \"“Deep convolutional neural network based on densely connected squeeze-and-excitation blocks\",\n        \"“Lightweight Residual Densely Connected Convolutional Neural Network\",\n        \"“A lightweight residual-inception convolutional neural network\",\n        \"“Lightweight Convolutional Neural Network with SE Module for Image Super-Resolution\",\n        \"“Squeezedet: Unified\",\n        \"“SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models\",\n        \"“Iirnet: A lightweight deep neural network using intensely inverted residuals for image recognition\",\n        \"“TanhExp: A Smooth Activation Function with High Convergence Speed for Lightweight Neural Networks\",\n        \"“Learning transferable architectures for scalable image recognition\",\n        \"“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\",\n        \"M. Martínez-García\",\n        \"“Raspberry pi assisted face recognition framework for enhanced law-enforcement services in smart cities\",\n        \"“Lightweight searchable encryption protocol for industrial internet of things\",\n        \"“Blockchain-based mobile crowd sensing in industrial systems\",\n        \"“14.4 a scalable speech recognizer with deep-neural-network acoustic models and voice-activated power gating\",\n        \"“An ultra-low power binarized convolutional neural network-based speech recognition processor with on-chip self-learning\",\n        \"“Indoor positioning based on fingerprint-image and deep learning\",\n        \"“Deep belief network for fingerprinting-based rfid indoor localization\",\n        \"“Indoor localization using channel state information with regression artificial neural networks\",\n        \"“Multicolumn bidirectional long short-term memory for mobile devices-based human activity recognition\",\n        \"“A novel semisupervised deep learning method for human activity recognition\",\n        \"K. Prakah-Asante\",\n        \"“A fuzzy authentication system based on neural network learning and extreme value statistics\",\n        \"“Real-time detection of false data injection attacks in smart grid: A deep learning-based intelligent mechanism\",\n        \"“Wide and deep convolutional neural networks for electricity-theft detection to secure smart grids\",\n        \"“Deepcoin: A novel deep learning and blockchain-based energy exchange framework for smart grids\",\n        \"“A deep learning approach for intrusion detection using recurrent neural networks\",\n        \"“Intrusion detection for iot based on improved genetic algorithm and deep belief network\",\n        \"” Computers & Security\",\n        \"“Privacy-preserving deep learning via additively homomorphic encryption\",\n        \"“Padl: Privacy-aware and asynchronous deep learning for iot applications\",\n        \"“Toward secure and privacy-preserving distributed deep learning in fog-cloud computing\",\n        \"“An ensemble of deep recurrent neural networks for detecting iot cyber attacks using network traffic\",\n        \"“Lightweight convolution neural networks for mobile edge computing in transportation cyber physical systems\",\n        \"” https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/\",\n        \"“Driver pose estimation using recurrent lightweight network and virtual data augmented transfer learning\",\n        \"“Short-term traffic flow prediction based on deep learning network\",\n        \"“Traffic flow prediction based on stack autoencoder and long short-term memory network\",\n        \"“SlimYOLOv3: Narrower\",\n        \"“A lightweight neural network framework for cross-domain road matching\",\n        \"“Slimrgbd: A geographic information photography noise reduction system for aerial remote sensing\",\n        \"“Learning Pose Estimation for UAV Autonomous Navigation and Landing Using Visual-Inertial Sensor Data\",\n        \"“A deep learning model based on multi-objective particle swarm optimization for scene classification in unmanned aerial vehicles\",\n        \"“Emergencynet: Efficient aerial image classification for drone-based emergency monitoring using atrous convolutional feature fusion\",\n        \"“DeepMon: Mobile GPU-Based Deep Learning Framework for Continuous Vision Applications\",\n        \"“DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework\",\n        \"“Lightweight models for weather identification\",\n        \"“Non-temporal lightweight fire detection network for intelligent surveillance systems\",\n        \"“Lightweight deep learning based intelligent edge surveillance techniques\",\n        \"“Human detection in surveillance videos using mobilenet\",\n        \"“Sparsification and separation of deep learning layers for constrained resource inference on wearables\",\n        \"“Transportation mode recognition fusing wearable motion\",\n        \"“Low-cost dnn hardware accelerator for wearable\",\n        \"“One-shot learning hand gesture recognition based on lightweight 3d convolutional neural networks for portable applications on mobile systems\",\n        \"“Multi-level binarized lstm in eeg classification for wearable devices\",\n        \"“Layer-wise training convolutional neural networks with smaller filters for human activity recognition using wearable sensors\",\n        \"“Memory- and communication-aware model compression for distributed deep learning inference on IoT\",\n        \"“Ese: Efficient speech recognition engine with sparse lstm on fpga\",\n        \"“Compressing RNNs for IoT devices by 15-38x using Kronecker Products\",\n        \"“An enhanced stacked lstm method with no random initialization for malware threat hunting in safety and time-critical systems\",\n        \"“Single Path One-Shot Neural Architecture Search with Uniform Sampling\",\n        \"“Towards optimal structured cnn pruning via generative adversarial learning\",\n        \"“A bayesian optimization framework for neural network compression\",\n        \"“GhostNet: More Features from Cheap Operations\",\n        \"“Progressive Neural Architecture Search\",\n        \"“Filter Grafting for Deep Neural Networks\",\n        \"“Lightweight offloading system for mobile edge computing\",\n        \"“Cost-driven off-loading for dnn-based applications over cloud\",\n        \"“Blockchain technologies for the internet of things: Research issues and challenges\",\n        \"“Blockchain for internet of things: A survey\"\n    ],\n    \"reasoning\": \"Selected these 214 references because they represent foundational work and recent advances in Compacting Deep Neural Networks for Internet of Things: Methods and Applications, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}