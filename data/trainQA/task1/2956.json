{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 561 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Attention, please! A survey of Neural Attention Models in Deep Learning\n\nPaper Abstract: Abstract In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.\n\n{retrieved_text}\n\nGenerate exactly 561 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 561\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"An attentional model for intelligent robotics agents\",\n        \"A taxonomy of external and internal attention\",\n        \"The retinotopy of visual spatial attention\",\n        \"Modulation of early sensory processing in human auditory cortex during auditory selective attention\",\n        \"The physiology and psychology of selective attention to touch\",\n        \"Attentional modulation in human primary olfactory cortex\",\n        \"Trying to detect taste in a tasteless solution: modulation of early gustatory cortex by attention to taste\",\n        \"The Principles of Psychology\",\n        \"Computational visual attention systems and their cognitive foundations: A survey\",\n        \"A feature-integration theory of attention\",\n        \"Guided search: an alternative to the feature integration model for visual search\",\n        \"The dynamic representation of scenes\",\n        \"Perception and communication\",\n        \"Toward a theory of memory and attention\",\n        \"Attention and effort, volume 1063\",\n        \"Clam: Closed-loop attention model for visual search\",\n        \"Slam: A connectionist model for attention in visual selection tasks\",\n        \"Shifts in selective visual attention: towards the underlying neural circuitry\",\n        \"A model of saliency-based visual attention for rapid scene analysis\",\n        \"An integrated model of top-down and bottom-up attention for optimizing detection speed\",\n        \"The emergence of attention by population-based inference and its role in distributed processing and cognitive control of vision\",\n        \"Modeling feature-based attention as an active top-down inference process\",\n        \"VOCUS: A visual attention system for object detection and goal-directed search, volume 3899\",\n        \"A selective attention-based method for visual pattern recognition with application to handwritten digit recognition and face recognition\",\n        \"Visual attention: from bio-inspired modeling to real-time implementation\",\n        \"Interactions of visual attention and object recognition: computational modeling, algorithms, and psychophysics\",\n        \"Detection and tracking of objects in underwater video\",\n        \"Modal control of an attentive vision system\",\n        \"A context-dependent attention system for a social robot\",\n        \"Towards the dream of intelligent, visually-guided wheelchairs\",\n        \"Attentive visual servoing\",\n        \"Attentional landmarks and active gaze control for visual slam\",\n        \"Visual attention in a mobile robot\",\n        \"Expectation-based selective attention for visual monitoring and control of a robot vehicle\",\n        \"Draw: A recurrent neural network for image generation\",\n        \"Attention is all you need\",\n        \"Memory networks\",\n        \"Survey on the attention based rnn model and its applications in computer vision\",\n        \"An introductory survey on attention mechanisms in nlp problems\",\n        \"Attention in natural language processing\",\n        \"Attention models in graphs: A survey\",\n        \"An attentive survey of attention models\",\n        \"Neural machine translation by jointly learning to align and translate\",\n        \"Show, attend and tell: Neural image caption generation with visual attention\",\n        \"Recurrent models of visual attention\",\n        \"On the properties of neural machine translation: Encoder-decoder approaches\",\n        \"Listen, attend and spell\",\n        \"Grammar as a foreign language\",\n        \"A neural conversational model\",\n        \"Bidirectional attention flow for machine comprehension\",\n        \"Hierarchical attention networks for document classification\",\n        \"Dynamic coattention networks for question answering\",\n        \"Pointer networks\",\n        \"Get to the point: Summarization with pointer-generator networks\",\n        \"Fusionnet: Fusing via fully-aware attention with application to machine comprehension\",\n        \"Reasoning about entailment with neural attention\",\n        \"Effective approaches to attention-based neural machine translation\",\n        \"Spatial transformer networks\",\n        \"Deep multimodal learning: A survey on recent advances and trends\",\n        \"A survey on deep learning for multimodal data fusion\",\n        \"Describing videos by exploiting temporal structure\",\n        \"Hierarchical attention-based multimodal fusion for video captioning\",\n        \"Memory-augmented attention modelling for videos\",\n        \"A diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism\",\n        \"Adaptive feature abstraction for translating video to text\",\n        \"Global context-aware attention lstm networks for 3d action recognition\",\n        \"Attention in convolutional lstm for gesture recognition\",\n        \"Adding attentiveness to the neurons in recurrent neural networks\",\n        \"Description generation for remote sensing images using attribute attention mechanism\",\n        \"Hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism\",\n        \"Scene classification with recurrent attention of vhr remote sensing images\",\n        \"Spectral-spatial attention networks for hyperspectral image classification\",\n        \"Attention-based multimodal fusion for video description\",\n        \"Deep fusion: An attention guided factorized bilinear pooling for audio-video emotion recognition\",\n        \"Memory fusion network for multi-view sequential learning\",\n        \"Multi-attention recurrent network for human communication comprehension\",\n        \"Relational recurrent neural networks\",\n        \"Neural multimodal belief tracker with adaptive attention for dialogue systems\",\n        \"Dual attention networks for multimodal reasoning and matching\",\n        \"Bi-directional spatial-semantic attention networks for image-text matching\",\n        \"Pointing novel objects in image captioning\",\n        \"Origins of the human pointing gesture: a training study\",\n        \"Improving referring expression grounding with cross-modal attention-guided erasing\",\n        \"Pay attention!-robustifying a deep visuomotor policy through task-focused visual attention\",\n        \"Neural turing machines\",\n        \"Memory-based graph networks\",\n        \"Dynamic memory networks for visual and textual question answering\",\n        \"Video object segmentation with episodic graph memory networks\",\n        \"Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling\",\n        \"End-to-end memory networks\",\n        \"Video object segmentation using space-time memory networks\",\n        \"Ask me anything: Dynamic memory networks for natural language processing\",\n        \"A comprehensive survey on graph neural networks\",\n        \"Key-value memory networks for directly reading documents\",\n        \"Memory graph networks for explainable memory-grounded question answering\",\n        \"Graph attention networks\",\n        \"Heterogeneous graph attention network\",\n        \"Graph attention convolution for point cloud semantic segmentation\",\n        \"Watch your step: Learning node embeddings via graph attention\",\n        \"Relation-aware graph attention network for visual question answering\",\n        \"Weighted transformer network for machine translation\",\n        \"Music transformer\",\n        \"Generating long sequences with sparse transformers\",\n        \"Set transformer: A framework for attention-based permutation-invariant neural networks\",\n        \"Doubly attentive transformer machine translation\",\n        \"Input combination strategies for multi-source transformer decoder\",\n        \"Style transformer: Unpaired text style transfer without disentangled latent representation\",\n        \"Hierarchical transformers for multi-document summarization\",\n        \"Learning multi-level information for dialogue response selection by highway recurrent transformer\",\n        \"Lattice-based transformer encoder for neural machine translation\",\n        \"Neural speech synthesis with transformer network\",\n        \"Phrase-based attentions\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Language models are unsupervised multitask learners\",\n        \"Language models are few-shot learners\",\n        \"Image transformer\",\n        \"Self-attention generative adversarial networks\",\n        \"Generative pretraining from pixels\",\n        \"Dall·e: Creating images from text, 2021\",\n        \"Hyperbolic attention networks\",\n        \"Hyperbolic graph attention network\",\n        \"Temporal graph networks for deep learning on dynamic graphs\",\n        \"Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules\",\n        \"Adaptive computation time for recurrent neural networks\",\n        \"Spatially adaptive computation time for residual networks\",\n        \"Differentiable adaptive computation time for visual reasoning\",\n        \"Deep reinforcement learning with visual attention for vehicle classification\",\n        \"Beyond frame-level cnn: saliency-aware 3-d cnn with lstm for video action recognition\",\n        \"Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding\",\n        \"Squeeze-and-excitation networks\",\n        \"Residual non-local attention networks for image restoration\",\n        \"Cbam: Convolutional block attention module\",\n        \"A^2-nets: Double attention networks\",\n        \"Attention branch network: Learning of attention mechanism for visual explanation\",\n        \"3dviewgraph: Learning global features for 3d shapes from a graph of unordered views with attention\",\n        \"Distant supervision for relation extraction with sentence-level attention and entity descriptions\",\n        \"Neural aggregation network for video face recognition\",\n        \"Inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks\",\n        \"A simple neural attentive meta-learner\",\n        \"Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition\",\n        \"Feature fusion encoder decoder network for automatic liver lesion segmentation\",\n        \"Learning better features for face detection with feature fusion and segmentation supervision\",\n        \"Delta: Deep learning transfer using feature map with attention for convolutional networks\",\n        \"Learning multi-attention convolutional neural network for fine-grained image recognition\",\n        \"A painless attention mechanism for convolutional neural networks\",\n        \"Relation classification via multi-level attention cnns\",\n        \"Abcnn: Attention-based convolutional neural network for modeling sentence pairs\",\n        \"Capacity visual attention networks\",\n        \"A dual-stage attention-based recurrent neural network for time series prediction\",\n        \"Geoman: Multi-level attention networks for geo-sensory time series prediction\",\n        \"Rpan: An end-to-end recurrent pose-attention network for action recognition in videos\",\n        \"Graph2seq: Graph to sequence learning with attention-based neural networks\",\n        \"Multiple object recognition with visual attention\",\n        \"Long short-term memory-networks for machine reading\",\n        \"Sparse attentive backtracking: Temporal creditassignment through reminding\",\n        \"Lstm networks for online cross-network recommendations\",\n        \"Monet: Unsupervised scene decomposition and representation\",\n        \"Learning to generate with memory\",\n        \"Fast adaptation in generative models with generative matching networks\",\n        \"One-shot generalization in deep generative models\",\n        \"Variational memory addressing in generative models\",\n        \"(self-attentive) autoencoder-based universal language representation for machine translation\",\n        \"Universal transformers\",\n        \"Inferring algorithmic patterns with stack-augmented recurrent nets\",\n        \"Matching networks for one shot learning\",\n        \"Transformer-xl: Attentive language models beyond a fixed-length context\",\n        \"Adaptive attention span in transformers\",\n        \"Adaptive input representations for neural language modeling\",\n        \"Frustratingly short attention spans in neural language modeling\",\n        \"Structured attention networks\",\n        \"Neural phrase-to-phrase machine translation\",\n        \"Self-attention with relative position representations, 2018\",\n        \"Online and linear-time attention by enforcing monotonic alignments, 2017\",\n        \"Learning phrase representations using rnn encoder-decoder for statistical machine translation\",\n        \"Neural machine translation of rare words with subword units\",\n        \"Adding interpretable attention to neural translation models improves word alignment\",\n        \"Context-aware self-attention networks\",\n        \"Convolutional self-attention networks\",\n        \"Modeling recurrence for transformer\",\n        \"Sockeye: A toolkit for neural machine translation\",\n        \"Asynchronous bidirectional decoding for neural machine translation\",\n        \"Graph convolutional encoders for syntax-aware neural machine translation\",\n        \"Deep recurrent models with fast-forward connections for neural machine translation\",\n        \"Google’s neural machine translation system: Bridging the gap between human and machine translation\",\n        \"Tree-to-sequence attentional neural machine translation\",\n        \"Information aggregation for multi-head attention with routing-by-agreement\",\n        \"Multi-way, multilingual neural machine translation with a shared attention mechanism\",\n        \"Latent alignment and variational attention\",\n        \"Understanding subtitles by character-level sequence-to-sequence learning\",\n        \"A decomposable attention model for natural language inference\",\n        \"Disan: Directional self-attention network for rnn/cnn-free language understanding\",\n        \"Bi-directional block self-attention for fast and memory-efficient sequence modeling\",\n        \"Learning natural language inference with lstm\",\n        \"Learning natural language inference using bidirectional lstm model and inner-attention\",\n        \"Neural programmer: Inducing latent programs with gradient descent\",\n        \"Hierarchical recurrent attention network for response generation\",\n        \"anmm: Ranking short answer texts with attention-based neural matching model\",\n        \"Densely connected attention propagation for reading comprehension\",\n        \"Qanet: Combining local convolution with global self-attention for reading comprehension\",\n        \"Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering\",\n        \"Multi-turn response selection for chatbots with deep attention matching network\",\n        \"Attentive pooling networks\",\n        \"Coarse-grain fine-grain coattention network for multi-evidence question answering\",\n        \"Generating high-quality and informative conversation responses with sequence-to-sequence models\",\n        \"Sdnet: Contextualized attention-based deep network for conversational question answering\",\n        \"Lstm-based deep learning models for non-factoid answer selection\",\n        \"Gated-attention readers for text comprehension\",\n        \"An end-to-end model for question answering over knowledge base with cross-attention combining global knowledge\",\n        \"Text understanding with the attention sum reader network\",\n        \"Neural tree indexers for text understanding\",\n        \"Semantic sentence matching with densely-connected recurrent and co-attentive information\",\n        \"Commonsense for generative multi-hop question answering tasks\",\n        \"Option comparison network for multiple-choice reading comprehension\",\n        \"Teaching machines to read and comprehend\",\n        \"Topic aware neural response generation\",\n        \"Iterative alternating neural attention for machine reading\",\n        \"Inner attention based recurrent neural networks for answer selection\",\n        \"Attention-based lstm for aspect-level sentiment classification\",\n        \"Interactive attention networks for aspect-level sentiment classification\",\n        \"Aela-dlstms: Attention-enabled and location-aware double lstms for aspect-level sentiment classification\",\n        \"Datastories at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis\",\n        \"Aspect based sentiment analysis with gated convolutional networks\",\n        \"Attention based hierarchical lstm network for context-aware microblog sentiment classification\",\n        \"Attentional encoder network for targeted sentiment classification\",\n        \"Lexicon integrated cnn models with attention for sentiment analysis\",\n        \"Attention-based long short-term memory network using sentiment lexicon embedding for aspect-level sentiment analysis in korean\",\n        \"Convolution-based neural attention with applications to sentiment classification\",\n        \"Recurrent attention network on memory for aspect sentiment analysis\",\n        \"Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm\",\n        \"Attention modeling for targeted sentiment\",\n        \"Neural sentiment classification with user and product attention\",\n        \"Enhancing attention-based lstm with position context for aspect-level sentiment classification\",\n        \"Distant supervision for relation extraction with hierarchical attention and entity descriptions\",\n        \"Learning better discourse representation for implicit discourse relation recognition via attention networks\",\n        \"Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling\",\n        \"Deep semantic role labeling with self-attention\",\n        \"Language to logical form with neural attention\",\n        \"Attention-based convolutional neural networks for chinese relation extraction\",\n        \"Attention-based models for speech recognition\",\n        \"Minimum word error rate training for attention-based sequence-to-sequence models, 2018\",\n        \"Language modeling with deep transformers\",\n        \"Rwth asr systems for librispeech: Hybrid vs attention–w/o data augmentation\",\n        \"Self-attention aligner: A latency-control end-to-end model for asr using self-attention network and chunk-hopping\",\n        \"Self-attention networks for connectionist temporal classification in speech recognition\",\n        \"Stream attention-based multi-array end-to-end speech recognition\",\n        \"Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese\",\n        \"Hybrid ctc/attention architecture for end-to-end speech recognition\",\n        \"End-to-end attention-based large vocabulary speech recognition\",\n        \"End-to-end continuous speech recognition using attention-based recurrent nn: First results\",\n        \"Attention-based models for text-dependent speaker verification\",\n        \"End-to-end anchored speech recognition\",\n        \"Improved training of end-to-end attention models for speech recognition\",\n        \"Joint ctc-attention based end-to-end speech recognition using multi-task learning\",\n        \"Attentive statistics pooling for deep speaker embedding\",\n        \"Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm\",\n        \"Abstractive sentence summarization with attentive recurrent neural networks\",\n        \"A deep reinforced model for abstractive summarization\",\n        \"Abstractive text summarization using sequence-to-sequence rnns and beyond\",\n        \"A neural attention model for abstractive sentence summarization\",\n        \"Hierarchical neural story generation\",\n        \"Generative adversarial network with policy gradient for text summarization\",\n        \"A discourse-aware attention model for abstractive summarization of long documents\",\n        \"Bidirectional recurrent neural networks\",\n        \"A convolutional attention network for extreme summarization of source code\",\n        \"Video-based sign language recognition without temporal segmentation\",\n        \"Automatic speech emotion recognition using recurrent neural networks with local attention\",\n        \"Sarcasm detection using soft attention-based bidirectional long short-term memory model with convolution network\",\n        \"Attention-based convolutional neural network and long short-term memory for short-term detection of mood disorders based on elicited speech responses\",\n        \"Dialoguernn: An attentive rnn for emotion detection in conversations\",\n        \"Attention based fully convolutional network for speech emotion recognition\",\n        \"Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech\",\n        \"Coupled multi-layer attentions for co-extraction of aspect and opinion terms\",\n        \"Bidirectional lstm with attention mechanism and convolutional layer for text classification\",\n        \"Exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed\",\n        \"Multi-stream network with temporal attention for environmental sound classification\",\n        \"Simultaneously self-attending to all mentions for full-abstract biological relation extraction\",\n        \"A single attention-based combination of cnn and rnn for relation classification\",\n        \"Attention-based bidirectional long short-term memory networks for relation classification\",\n        \"Neural relation extraction with selective attention over instances\",\n        \"Position-aware attention and supervised data improve slot filling\",\n        \"Bert for joint intent classification and slot filling\",\n        \"Aila: Attentive interactive labeling assistant for document classification through attention-based deep neural networks\",\n        \"Audio set classification with attention model: A probabilistic perspective\",\n        \"Improving relation extraction by pre-trained language representations\",\n        \"Investigation of enhanced tacotron text-to-speech synthesis systems with self-attention for pitch accent language\",\n        \"Joint training framework for text-to-speech and voice conversion using multi-source tacotron and wavenet\",\n        \"What do recurrent neural network grammars learn about syntax?\",\n        \"Attention-passing models for robust and data-efficient end-to-end speech translation\",\n        \"Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention\",\n        \"Differentiable dynamic programming for structured prediction and attention\",\n        \"Multiresolution graph attention networks for relevance matching\",\n        \"Long-tail relation extraction via knowledge graph embeddings and graph convolution networks\",\n        \"Attention-over-attention neural networks for reading comprehension\",\n        \"R-trans: Rnn transformer network for chinese machine reading comprehension\",\n        \"Consensus attention-based neural networks for chinese reading comprehension\",\n        \"Efficient large-scale domain classification with personalized attention\",\n        \"Learning to transduce with unbounded memory\",\n        \"Leveraging knowledge bases in lstms for improving machine reading\",\n        \"Attention-based recurrent neural network models for joint intent detection and slot filling\",\n        \"Attention-based encoder-decoder networks for spelling and grammatical error correction\",\n        \"Chains of reasoning over entities, relations, and text using recurrent neural networks\",\n        \"Deep joint entity disambiguation with local neural attention\",\n        \"A structured self-attentive sentence embedding\",\n        \"Attentive mimicking: Better word embeddings by attending to informative contexts\",\n        \"Self-attentive speaker embeddings for text-independent speaker verification\",\n        \"Deep biaffine attention for neural dependency parsing\",\n        \"Linguistically-informed self-attention for semantic role labeling\",\n        \"Commonsense knowledge aware conversation generation with graph attention\",\n        \"Sequence-to-sequence acoustic modeling for voice conversion\",\n        \"Automatic question tagging with deep neural networks\",\n        \"Action recognition using visual attention\",\n        \"Video action transformer network\",\n        \"Attentional pooling for action recognition\",\n        \"An end-to-end spatio-temporal attention model for human action recognition from skeleton data\",\n        \"Recurrent mixture density network for spatiotemporal visual attention\",\n        \"Videolstm convolves, attends and flows for action recognition\",\n        \"Attend and interact: Higher-order object interactions for video understanding\",\n        \"Eidetic 3d lstm: A model for video prediction and beyond\",\n        \"Unified spatio-temporal attention networks for action recognition in videos\",\n        \"View adaptive recurrent neural networks for high performance human action recognition from skeleton data\",\n        \"Relational network for skeleton-based action recognition\",\n        \"Two stream lstm: A deep fusion framework for human action recognition\",\n        \"Attention-based multiview re-observation fusion network for skeletal action recognition\",\n        \"An attention enhanced graph convolutional lstm network for skeleton-based action recognition\",\n        \"Lsta: Long short-term attention for egocentric action recognition\",\n        \"Skeleton-based human action recognition with global context-aware attention lstm networks\",\n        \"Decidenet: Counting varying density crowds through attention guided detection and density estimation\",\n        \"Crowd counting using deep recurrent spatial-aware network\",\n        \"Crowd counting using scale-aware attention networks\",\n        \"Multi-resolution attention convolutional neural network for crowd counting\",\n        \"Overcoming catastrophic forgetting with hard attention to the task\",\n        \"Attribute-aware attention model for fine-grained representation learning\",\n        \"Attention for fine-grained categorization\",\n        \"Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization\",\n        \"Residual attention network for image classification\",\n        \"Hierarchical attention networks\",\n        \"Multi-label image recognition by recurrently discovering attentional regions\",\n        \"Attention augmented convolutional networks\",\n        \"Deep attention-based spatially recursive networks for fine-grained visual recognition\",\n        \"Diversified visual attention networks for fine-grained object classification\",\n        \"Learning embedding adaptation for few-shot learning\",\n        \"A cnn–rnn architecture for multi-label weather recognition\",\n        \"Quaternion convolutional neural network for color image classification and forensics\",\n        \"The application of two-level attention models in deep convolutional neural network for fine-grained image classification\",\n        \"Deephint: understanding hiv-1 integration via deep learning with attention\",\n        \"Diagnose like a radiologist: Attention guided convolutional neural network for thorax disease classification\",\n        \"Top-down neural attention by excitation backprop\",\n        \"Attentive fashion grammar network for fashion landmark detection and clothing category classification\",\n        \"Attention-based ensemble for deep metric learning\",\n        \"Incremental few-shot learning with attention attractor networks\",\n        \"Object-part attention model for fine-grained image classification\",\n        \"The pros and cons: Rank-aware temporal attention for skill determination in long videos\",\n        \"Object-part attention driven discriminative localization for fine-grained image classification\",\n        \"Attention-aware generative adversarial networks (ata-gans)\",\n        \"Generative image inpainting with contextual attention\",\n        \"Few-shot autoregressive density estimation: Towards learning to learn distributions\",\n        \"Attention-gan for object transfiguration in wild images\",\n        \"Layoutgan: Generating graphic layouts with wireframe discriminators\",\n        \"Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation\",\n        \"Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism\",\n        \"Pedestrian alignment network for large-scale person re-identification\",\n        \"Hydraplus-net: Attentive deep features for pedestrian analysis\",\n        \"A twofold siamese network for real-time object tracking\",\n        \"Iterative visual reasoning beyond convolutions\",\n        \"Dual attention matching network for context-aware feature sequence based person re-identification\",\n        \"Mask-guided contrastive attention model for person re-identification\",\n        \"Aware attentive multi-view inference for vehicle re-identification\",\n        \"Progressive attention guided recurrent network for salient object detection\",\n        \"Deep feature pyramid reconfiguration for object detection\",\n        \"Cross-modal attentional context learning for rgb-d object detection\",\n        \"Reverse attention for salient object detection\",\n        \"Three-stream attention-aware network for rgb-d salient object detection\",\n        \"Towards universal object detection by domain attention\",\n        \"Harmonious attention network for person re-identification\",\n        \"End-to-end comparative attention networks for person re-identification\",\n        \"Video-based person re-identification via 3d convolutional networks and non-local attention\",\n        \"Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding\",\n        \"Deeply-learned part-aligned representations for person re-identification\",\n        \"See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification\",\n        \"Attention-aware compositional network for person re-identification\",\n        \"Diversity regularized spatiotemporal attention for video-based person re-identification\",\n        \"Re-identification with consistent attentive siamese networks\",\n        \"Video-based person re-identification via spatio-temporal attentional and two-stream fusion convolutional networks\",\n        \"Mancs: A multi-task attentional network with curriculum sampling for person re-identification\",\n        \"Jointly attentive spatial-temporal pooling networks for video-based person re-identification\",\n        \"Scan: Self-and-collaborative attention network for video person re-identification\",\n        \"Dual attention network for scene segmentation\",\n        \"Tell me where to look: Guided attention inference network\",\n        \"End-to-end instance segmentation with recurrent attention\",\n        \"Deep gated attention networks for large-scale street-level scene segmentation\",\n        \"Attention to scale: Scale-aware semantic image segmentation\",\n        \"Learn to pay attention\",\n        \"End-to-end multi-task learning with attention\",\n        \"Ocnet: Object context network for scene parsing\",\n        \"Pyramid attention network for semantic segmentation\",\n        \"Direction-aware spatial context features for shadow detection\",\n        \"Ric-unet: An improved neural network based on unet for nuclei segmentation in histology images\",\n        \"Scene segmentation with dag-recurrent neural networks\",\n        \"Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation\",\n        \"Attention u-net: Learning where to look for the pancreas\",\n        \"Connection sensitive attention u-net for accurate retinal vessel segmentation\",\n        \"Pixel-wise attentional gating for scene parsing\",\n        \"Video object segmentation with joint re-identification and attention-aware mask propagation\",\n        \"Psanet: Point-wise spatial attention network for scene parsing\",\n        \"Recurrent attentional networks for saliency detection\",\n        \"Picanet: Learning pixel-wise contextual attention for saliency detection\",\n        \"Predicting human eye fixations via an lstm-based saliency attentive model\",\n        \"Sac-net: Spatial attenuation context for salient object detection\",\n        \"An end-to-end textspotter with explicit alignment and attention\",\n        \"Focusing attention: Towards accurate text recognition in natural images\",\n        \"Moran: A multi-object rectified attention network for scene text recognition\",\n        \"Convolutional attention networks for scene text recognition\",\n        \"Show, attend and read: A simple and strong baseline for irregular text recognition\",\n        \"Recurrent temporal sparse autoencoder for attention-based action recognition\",\n        \"Attention-based temporal weighted convolutional neural network for action recognition\",\n        \"Temporal attention-gated model for robust sequence classification\",\n        \"A gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition\",\n        \"Stacked semantics-guided attention model for fine-grained zero-shot learning\",\n        \"Attention gated networks: Learning to leverage salient regions in medical images\",\n        \"Mganet: A robust model for quality enhancement of compressed video\",\n        \"Down-scaling with learned kernels in multi-scale deep neural networks for non-uniform single image deblurring\",\n        \"Structured attention guided convolutional neural fields for monocular depth estimation\",\n        \"Attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions\",\n        \"Attentive generative adversarial network for raindrop removal from a single image\",\n        \"Unsupervised attention-guided image-to-image translation\",\n        \"Attention-guided generative adversarial networks for unsupervised image-to-image translation\",\n        \"Deep saliency hashing\",\n        \"Deep attention-guided hashing\",\n        \"Ahcnet: An application of attention mechanism and hybrid connection for liver tumor segmentation in ct volumes\",\n        \"Attention-based deep multiple instance learning\",\n        \"Recursive recurrent nets with attention modeling for ocr in the wild\",\n        \"Multi-context attention for human pose estimation\",\n        \"Image super-resolution using very deep residual channel attention networks\",\n        \"Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer\",\n        \"Learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision\",\n        \"Pay attention to virality: understanding popularity of social media videos with the attention mechanism\",\n        \"Attention clusters: Purely attention based local feature integration for video classification\",\n        \"Robust and high performance face detector\",\n        \"Robust facial landmark detection via recurrent attentive-refinement networks\",\n        \"Deep learning for fall detection: Three-dimensional cnn combined with lstm on video kinematic data\",\n        \"Cross-modality interactive attention network for multispectral pedestrian detection\",\n        \"Occluded pedestrian detection through guided attention in cnns\",\n        \"Vssa-net: vertical spatial sequence attention network for traffic sign detection\",\n        \"Attention-based extraction of structured information from street view imagery\",\n        \"Single shot text detector with regional attention\",\n        \"Script identification in natural scene image and video frames using an attention based convolutional-lstm network\",\n        \"Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition\",\n        \"Deep-emotion: Facial expression recognition using attentional convolutional network\",\n        \"Occlusion aware facial expression recognition using cnn with attention mechanism\",\n        \"Fully convolutional sequence recognition network for water meter number reading\",\n        \"Learning deep structured multi-scale features using attention-gated crfs for contour prediction\",\n        \"Semantic adversarial network with multi-scale pyramid attention for video classification\",\n        \"A multimodal lstm for predicting listener empathic responses over time\",\n        \"Mattnet: Modular attention network for referring expression comprehension\",\n        \"Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays\",\n        \"Attngan: Fine-grained text to image generation with attentional generative adversarial networks\",\n        \"Character-based handwritten text transcription with attention networks\",\n        \"Bottom-up and top-down attention for image captioning and visual question answering\",\n        \"Image captioning with triple-attention and stack parallel lstm\",\n        \"Da-gan: Instance-level image translation by deep attention generative adversarial networks\",\n        \"Knowing when to look: Adaptive attention via a visual sentinel for image captioning\",\n        \"Image captioning with semantic attention\",\n        \"Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning\",\n        \"Vd-san: Visual-densely semantic attention network for image caption generation\",\n        \"From captions to visual concepts and back\",\n        \"Review networks for caption generation\",\n        \"Areas of attention for image captioning\",\n        \"Exploring visual relationship for image captioning\",\n        \"Recurrent topic-transition gan for visual paragraph generation\",\n        \"Grounding of textual phrases in images by reconstruction\",\n        \"One model to learn them all\",\n        \"Stacked cross attention for image-text matching\",\n        \"X-linear attention networks for image captioning\",\n        \"Describing multimedia content using attention-based encoder–decoder networks\",\n        \"End-to-end dense video captioning with masked transformer\",\n        \"Actbert: Learning global-local video-text representations\",\n        \"Video paragraph captioning using hierarchical recurrent neural networks\",\n        \"Dense-captioning events in videos\",\n        \"Describing video with attention-based bidirectional lstm\",\n        \"Hierarchical boundary-aware neural encoder for video captioning\",\n        \"An end-to-end baseline for video captioning\",\n        \"Video summarization with attention-based encoder–decoder networks\",\n        \"Hierarchical lstm with adjusted temporal attention for video captioning\",\n        \"Residual attention-based lstm for video captioning\",\n        \"Video captioning with attention-based lstm and semantic consistency\",\n        \"Bidirectional attentive fusion with context gating for dense video captioning\",\n        \"Modality shifting attention network for multi-modal video question answering\",\n        \"Fantastic answers and where to find them: Immersive question-directed visual attention\",\n        \"Focal visual-text attention for visual question answering\",\n        \"Compositional attention networks for machine reasoning\",\n        \"Dual recurrent attention units for visual question answering\",\n        \"Hierarchical question-image co-attention for visual question answering\",\n        \"Bilinear attention networks\",\n        \"Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering\",\n        \"Deep compositional question answering with neural module networks\",\n        \"Multimodal residual learning for visual qa\",\n        \"Where to look: Focus regions for visual question answering\",\n        \"Visual7w: Grounded question answering in images\",\n        \"Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model\",\n        \"Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering\",\n        \"Don’t just assume; look and answer: Overcoming priors for visual question answering\",\n        \"Learning to count objects in natural images for visual question answering\",\n        \"Multi-step reasoning via recurrent dual attention for visual dialog\",\n        \"Abc-cnn: An attention based convolutional neural network for visual question answering\",\n        \"Ask, attend and answer: Exploring question-guided spatial attention for visual question answering\",\n        \"Interpretable visual question answering by visual grounding from attention supervision mining\",\n        \"Focal visual-text attention for memex question answering\",\n        \"Hypergraph attention networks for multimodal learning\",\n        \"Stacked attention networks for image question answering\",\n        \"Multi-level attention networks for visual question answering\",\n        \"Multi-modal factorized bilinear pooling with co-attention learning for visual question answering\",\n        \"Stamp: short-term attention/memory priority model for session-based recommendation\",\n        \"Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention\",\n        \"Action recognition using visual attention with reinforcement learning\",\n        \"Attention-aware deep reinforcement learning for video face recognition\",\n        \"Deep networks with internal selective attention through feedback connections\",\n        \"Attention-aware face hallucination via deep reinforcement learning\",\n        \"Digital image steganalysis based on visual attention and deep reinforcement learning\",\n        \"End-to-end learning of action detection from frame glimpses in videos\",\n        \"Graph classification using structural attention\",\n        \"Neural map: Structured memory for deep reinforcement learning\",\n        \"Relational deep reinforcement learning\",\n        \"Emergent tool use from multi-agent autocurricula\",\n        \"High dimensions and heavy tails for natural evolution strategies\",\n        \"One-shot imitation learning\",\n        \"Deep visual odometry with adaptive memory\",\n        \"Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume\",\n        \"Dynamic attention-based visual odometry\",\n        \"Exploring self-attention for visual odometry\",\n        \"Attentional separation-and-aggregation network for self-supervised depth-pose learning in dynamic scenes\",\n        \"Transformer guided geometry model for flow-based unsupervised visual odometry\",\n        \"Sophie: An attentive gan for predicting paths compliant to social and physical constraints\",\n        \"Social attention: Modeling attention in human crowds\",\n        \"Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning\",\n        \"Scene memory transformer for embodied agents in long-horizon tasks\",\n        \"Translating navigation instructions in natural language to a high-level plan for behavioral robot navigation\",\n        \"Understanding neural networks through representation erasure\",\n        \"Is attention interpretable?\",\n        \"Analyzing the structure of attention in a transformer language model\",\n        \"Bert rediscovers the classical nlp pipeline\",\n        \"What does bert look at? an analysis of bert’s attention\",\n        \"Attention is not explanation\",\n        \"Attention interpretability across nlp tasks\",\n        \"Attention is not not explanation\",\n        \"Dynamic routing between capsules\",\n        \"Attention routing between capsules\",\n        \"Da-capsnet: dual attention mechanism capsule network\",\n        \"Self-attention capsule networks for image classification\",\n        \"Capsules with inverted dot-product attention routing\",\n        \"Deep learning\",\n        \"A survey on statistical relational learning\",\n        \"Neural-symbolic learning and reasoning: A survey and interpretation\",\n        \"Differentiable learning of logical rules for knowledge base reasoning\",\n        \"Logic attention based neighborhood aggregation for inductive knowledge graph embedding\",\n        \"Probabilistic logic graph attention networks for reasoning\",\n        \"Self-attentional credit assignment for transfer in reinforcement learning\",\n        \"Please stop explaining black box models for high stakes decisions\",\n        \"Human-centered artificial intelligence and machine learning\",\n        \"Attgan: Facial attribute editing by only changing what you want\",\n        \"Battrae: Bidimensional attention-based recursive autoencoders for learning bilingual phrase embeddings\",\n        \"Attention-based autoencoder topic model for short texts\"\n    ],\n    \"reasoning\": \"Selected these 561 references because they represent foundational work and recent advances in Attention, please! A survey of Neural Attention Models in Deep Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}