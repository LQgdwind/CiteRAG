{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 55 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Composition of Relational Features with an Application to Explaining Black-Box Predictors\n\nPaper Abstract: Abstract Three key strengths of relational machine learning programs like those developed in Inductive Logic Programming (ILP) are: (1) The use of an expressive subset of first-order logic that allows models that capture complex relationships amongst data instances; (2) The use of domain-specific relations to guide the construction of models; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. The price for these advantages is that ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. Motivated by the work of McCreath and Sharma (McCreath and Sharma, 1998a; McCreath, 1999) we formulate the notion of a set of MM\\mathrm{M}-simple features in a mode language MM\\mathrm{M} and identify two composition operators (œÅ1subscriptùúå1\\rho_{1} and œÅ2subscriptùúå2\\rho_{2}) from which all possible complex features can be derived. We use these results to implement a form of ‚Äúexplainable neural network‚Äô called Compositional Relational Machines, or CRMs. CRMs are labelled directed-acyclic graphs. The vertex-label for any vertex jùëój in the CRM contains a feature-function fjsubscriptùëìùëóf_{j} and an continuous activation function gjsubscriptùëîùëóg_{j}. If jùëój is a ‚Äúnon-input‚Äù vertex, then fjsubscriptùëìùëóf_{j} is the composition of features associated with vertices in the direct predecessors of jùëój. Our focus is on CRMs in which input vertices (those without any direct predecessors) all have MM\\mathrm{M}-simple features in their vertex-labels. We provide a randomised procedure for constructing the structure of such CRMs, and a procedure for estimating the parameters (the wi‚Äãjsubscriptùë§ùëñùëów_{ij}‚Äôs))) using back-propagation and stochastic gradient descent. Using a notion of explanations based on the compositional structure of features in a CRM, we provide empirical evidence on synthetic data of the ability to identify appropriate explanations; and demonstrate the use of CRMs as ‚Äòexplanation machines‚Äô for black-box models that do not provide explanations for their predictions.\n\n{retrieved_text}\n\nGenerate exactly 55 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 55\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Towards robust interpretability with self-explaining neural networks\",\n        \"On representations of problems of reasoning about actions\",\n        \"Towards explainable deep neural networks (xdnn)\",\n        \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n        \"Learning logical exceptions in chess\",\n        \"Layer-wise relevance propagation for deep neural network architectures\",\n        \"Symbolic logic and mechanical theorem proving\",\n        \"Approximation by superpositions of a sigmoidal function\",\n        \"Large-scale assessment of deep relational machines\",\n        \"Discrete stochastic search and its application to feature-selection for deep relational machines\",\n        \"Incorporating symbolic domain knowledge into graph neural networks\",\n        \"Inclusion of domain-knowledge into gnns using mode-directed inverse entailment\",\n        \"Neuro-symbolic= neural+ logical+ probabilistic\",\n        \"Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning\",\n        \"Topic models with relational features for drug design\",\n        \"Fast relational learning using bottom clause propositionalization with artificial neural networks\",\n        \"Deep learning\",\n        \"Subsumption and implication\",\n        \"Multilayer feedforward networks are universal approximators\",\n        \"Feature construction using theory-guided sampling and randomised search\",\n        \"Adam: A method for stochastic optimization\",\n        \"Propositionalization Approaches to Relational Data Mining\",\n        \"Rsd: Relational subgroup discovery through first-order feature construction\",\n        \"Propositionalization of relational data\",\n        \"Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm\",\n        \"Foundations of logic programming\",\n        \"Data mining the nci cancer cell line compound gi50 values: identifying quinone subtypes effective against melanoma and leukemia cell classes\",\n        \"Induction in first order logic from noisy training examples and fixed example set size\",\n        \"L ime: a system for learning relations\",\n        \"Lime: A system for learning relations\",\n        \"Pattern recognition as rule-guided inductive inference\",\n        \"King and Rook Against King. 1. Historical Background and a Problem on the Infinite Board\",\n        \"To the international computing community: A new east-west challenge\",\n        \"Inverse entailment and progol\",\n        \"Inductive logic programming: Theory and methods\",\n        \"Foundations of inductive logic programming\",\n        \"Logic and artificial intelligence\",\n        \"Pytorch: An imperative style, high-performance deep learning library\",\n        \"Approximation theory of the mlp model in neural networks\",\n        \"Automatic methods of inductive inference\",\n        \"Early stopping-but when?\",\n        \"Discovering rules by induction from large collections of examples\",\n        \"Using ilp to construct features for information extraction from semi-structured text\",\n        \"Explainable deep learning: A field guide for the uninitiated\",\n        \"Learning representations by back-propagating errors\",\n        \"What kinds of relational features are useful for statistical learning?\",\n        \"Lifted relational neural networks: Efficient learning of latent relational structures\",\n        \"An investigation into feature construction to assist word sense disambiguation\",\n        \"The aleph manual\",\n        \"Feature construction with inductive logic programming: A study of quantitative predictions of biological activity aided by structural attributes\",\n        \"Distinguishing exceptions from noise in non-monotonic learning\",\n        \"Logical explanations for deep relational machines using relevance information\",\n        \"The lattice structure and refinement operators for the hypothesis space bounded by a bottom clause\",\n        \"Intelligent machinery\",\n        \"An investigation into the role of domain-knowledge on the use of embeddings\"\n    ],\n    \"reasoning\": \"Selected these 55 references because they represent foundational work and recent advances in Composition of Relational Features with an Application to Explaining Black-Box Predictors, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}