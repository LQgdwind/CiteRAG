{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 309 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: On the Design Fundamentals of Diffusion Models: A Survey\n\nPaper Abstract: Abstract Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.\n\n{retrieved_text}\n\nGenerate exactly 309 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 309\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Deep unsupervised learning using nonequilibrium thermodynamics\",\n        \"Palette: Image-to-image diffusion models\",\n        \"Hierarchical text-conditional image generation with clip latents\",\n        \"Cascaded diffusion models for high fidelity image generation.\",\n        \"Score-based generative modeling through stochastic differential equations\",\n        \"Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation\",\n        \"Non-autoregressive conditional diffusion models for time series prediction\",\n        \"AudioLDM: Text-to-Audio Generation with Latent Diffusion Models\",\n        \"Symbolic music generation with diffusion models\",\n        \"A study on speech enhancement based on diffusion probabilistic model\",\n        \"3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models\",\n        \"Human motion diffusion model\",\n        \"MagicVideo: Efficient Video Generation With Latent Diffusion Models\",\n        \"Conditional diffusion based on discrete graph structures for molecular graph generation\",\n        \"Predicting molecular conformation via dynamic graph score matching\",\n        \"Language-guided traffic simulation via scene-level diffusion\",\n        \"Probabilistic mapping of dark matter by neural score matching\",\n        \"Generative modeling by estimating gradients of the data distribution\",\n        \"Denoising diffusion probabilistic models\",\n        \"Elucidating the design space of diffusion-based generative models\",\n        \"On the mathematics of diffusion models\",\n        \"A survey on generative diffusion model\",\n        \"Diffusion models: A comprehensive survey of methods and applications\",\n        \"Understanding diffusion models: A unified perspective\",\n        \"Generative diffusion models on graphs: Methods and applications\",\n        \"Efficient diffusion models for vision: A survey\",\n        \"Diffusion models in bioinformatics: A new wave of deep learning revolution in action\",\n        \"Diffusion models for medical image analysis: A comprehensive survey\",\n        \"A survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai\",\n        \"A survey on generative diffusion models for structured data\",\n        \"Diffusion models in vision: A survey\",\n        \"Text-to-image diffusion model in generative ai: A survey\",\n        \"Diffusion models in nlp: A survey\",\n        \"How much is enough? a study on diffusion times in score-based generative models\",\n        \"Auto-encoding variational bayes\",\n        \"Understanding Deep Learning\",\n        \"Large scale gan training for high fidelity natural image synthesis\",\n        \"On calibrating diffusion probabilistic models\",\n        \"Vaes meet diffusion models: Efficient and high-fidelity generation\",\n        \"Fundamentals of Stochastic Models\",\n        \"Diffusion models beat gans on image synthesis\",\n        \"Exploring the optimal choice for generative processes in diffusion models: Ordinary vs stochastic differential equations\",\n        \"Maximum likelihood training of implicit nonlinear diffusion models\",\n        \"Reverse-time diffusion equation models\",\n        \"Estimation of non-normalized statistical models by score matching.\",\n        \"On the importance of noise scheduling for diffusion models\",\n        \"Non gaussian denoising diffusion models\",\n        \"Score-based generative modeling with critically-damped langevin diffusion\",\n        \"Soft diffusion: Score matching for general corruptions\",\n        \"Denoising diffusion gamma models\",\n        \"On the generalization of diffusion model\",\n        \"“Convergence of denoising diffusion models under the manifold hypothesis\",\n        \"“Tackling the generative learning trilemma with denoising diffusion gans\",\n        \"“Learning energy-based models in high-dimensional spaces with multi-scale denoising score matching\",\n        \"“Wavegrad: Estimating gradients for waveform generation\",\n        \"“Estimating high order gradients of the data distribution by denoising\",\n        \"“Improved techniques for training score-based generative models\",\n        \"“Improved denoising diffusion probabilistic models\",\n        \"“Variational diffusion models\",\n        \"“Scalable adaptive computation for iterative generation\",\n        \"“Noise2music: Text-conditioned music generation with diffusion models\",\n        \"“Cold diffusion: Inverting arbitrary image transforms without noise\",\n        \"“A connection between score matching and denoising autoencoders\",\n        \"“Score-based denoising diffusion with non-isotropic gaussian noise models\",\n        \"“Preserve your own correlation: A noise prior for video diffusion models\",\n        \"“On fast sampling of diffusion probabilistic models\",\n        \"“Priorgrad: Improving conditional denoising diffusion models with data-driven adaptive prior\",\n        \"“A reparameterized discrete diffusion model for text generation\",\n        \"“Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders\",\n        \"“Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction\",\n        \"“Grad-tts: A diffusion probabilistic model for text-to-speech\",\n        \"“Diffsinger: Singing voice synthesis via shallow diffusion mechanism\",\n        \"“Accelerating diffusion models via early stop of the diffusion process\",\n        \"“Dirichlet diffusion score model for biological sequence generation\",\n        \"“Domain-specific denoising diffusion probabilistic models for brain dynamics\",\n        \"“Dddm-vc: Decoupled denoising diffusion models with disentangled representation and prior mixup for verified robust voice conversion\",\n        \"“Subspace diffusion generative models\",\n        \"“Non-uniform diffusion models\",\n        \"“Compositional visual generation with composable diffusion models\",\n        \"“Score-based generative modeling of graphs via the system of stochastic differential equations\",\n        \"“Conditional image generation with score-based diffusion models\",\n        \"“Clip-diffusion-lm: Apply diffusion model on image captioning\",\n        \"“Riemannian score-based generative modeling\",\n        \"“High-resolution image synthesis with latent diffusion models\",\n        \"“Argmax flows and multinomial diffusion: Learning categorical distributions\",\n        \"“Diffusion probabilistic models for scene-scale 3d categorical data\",\n        \"“Blackout diffusion: Generative diffusion models in discrete-state spaces\",\n        \"“Analog bits: Generating discrete data using diffusion models with self-conditioning\",\n        \"“Diffuseq: Sequence to sequence text generation with diffusion models\",\n        \"C. Durkan et al\",\n        \"“Diffuser: Diffusion via edit-based reconstruction\",\n        \"“Unsupervised discovery of semantic latent directions in diffusion models\",\n        \"“Riemannian diffusion schr\\\\\\\\\\\\backslash” odinger bridge\",\n        \"“Riemannian diffusion models\",\n        \"“Diffusion-Based Representation Learning\",\n        \"“Diffusevae: Efficient\",\n        \"“Diffusion autoencoders: Toward a meaningful and decodable representation\",\n        \"“Diffusion Models already have a Semantic Latent Space\",\n        \"“Emergent correspondence from image diffusion\",\n        \"“Multi-modal latent diffusion\",\n        \"“Unifying diffusion models’ latent space\",\n        \"“Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation\",\n        \"“Exploring vision transformers as diffusion learners\",\n        \"“Progressive distillation for fast sampling of diffusion models\",\n        \"“U-net: Convolutional networks for biomedical image segmentation\",\n        \"“Attention is all you need\",\n        \"“Wavelet diffusion models are fast and scalable image generators\",\n        \"“Introduction to diffusion models for machine learning\",\n        \"“Multi-architecture multi-expert diffusion models\",\n        \"J. Pouget-Abadie\",\n        \"“Deep learning and the information bottleneck principle\",\n        \"“Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications\",\n        \"“Wide residual networks\",\n        \"“Weight normalization: A simple reparameterization to accelerate training of deep neural networks\",\n        \"“Group normalization\",\n        \"“Crossvit: Cross-attention multi-scale vision transformer for image classification\",\n        \"“Photorealistic text-to-image diffusion models with deep language understanding\",\n        \"“Diffusion probabilistic fields\",\n        \"“Vdt: An empirical study on video diffusion with transformers\",\n        \"“Your vit is secretly a hybrid discriminative-generative diffusion model\",\n        \"A. Triantafyllopoulos\",\n        \"“All are worth words: a vit backbone for score-based diffusion models\",\n        \"“Exploring transformer backbones for image diffusion models\",\n        \"” in Computer Vision–ECCV 2022: 17th European Conference\",\n        \"“Scalable diffusion models with transformers\",\n        \"“Positional diffusion: Ordering unordered sets with diffusion probabilistic models\",\n        \"“Vit-tts: Visual text-to-speech with scalable diffusion transformer\",\n        \"“An image is worth 16x16 words: Transformers for image recognition at scale\",\n        \"“One transformer fits all distributions in multi-modal diffusion at scale\",\n        \"“3d equivariant diffusion for target-aware molecule generation and affinity prediction\",\n        \"“Dynamic dual-output diffusion models\",\n        \"“Sliced score matching: A scalable approach to density and score estimation\",\n        \"“A geometric perspective on diffusion models\",\n        \"“How to train your energy-based models\",\n        \"“Training products of experts by minimizing contrastive divergence\",\n        \"“Geodiff: A geometric diffusion model for molecular conformation generation\",\n        \"“A variational perspective on diffusion-based generative models and score matching\",\n        \"“Imagen video: High definition video generation with diffusion models\",\n        \"“Common diffusion noise schedules and sample steps are flawed\",\n        \"“Improved techniques for maximum likelihood estimation for diffusion odes\",\n        \"“Boosting fast and high-quality speech synthesis with linear diffusion\",\n        \"“Diffusion models are minimax optimal distribution estimators\",\n        \"“Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models\",\n        \"“Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models\",\n        \"“Denoising diffusion implicit models\",\n        \"“Estimating the optimal covariance with imperfect mean in diffusion probabilistic models\",\n        \"“High-Fidelity Guided Image Synthesis with Latent Diffusion Models\",\n        \"“Unsupervised semantic correspondence using stable diffusion\",\n        \"“On analyzing generative and denoising capabilities of diffusion-based deep generative models\",\n        \"“A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence\",\n        \"“Addp: Learning general representations for image recognition and generation with alternating denoising diffusion process\",\n        \"“Diffusion hyperfeatures: Searching through time and space for semantic correspondence\",\n        \"“Perception prioritized training of diffusion models\",\n        \"“Are diffusion models vision-and-language reasoners?” 2023\",\n        \"“Training-free structured diffusion guidance for compositional text-to-image synthesis\",\n        \"“Conceptbed: Evaluating concept learning abilities of text-to-image diffusion models\",\n        \"“Beyond surface statistics: Scene representations in a latent diffusion model\",\n        \"“Score-based generative modeling in latent space\",\n        \"“Maximum likelihood training of score-based diffusion models\",\n        \"“Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\",\n        \"“Controlling text-to-image diffusion by orthogonal finetuning\",\n        \"“Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop\",\n        \"R. Piché-Taillefer\",\n        \"“Fast diffusion model\",\n        \"“Self-guided diffusion models\",\n        \"“Towards enhanced controllability of diffusion models\",\n        \"“Learning Data Representations with Joint Diffusion Models\",\n        \"“Diffcap: Exploring continuous diffusion on image captioning\",\n        \"“Raphael: Text-to-image generation via large mixture of diffusion paths\",\n        \"“Adding conditional control to text-to-image diffusion models\",\n        \"“Classifier-free diffusion guidance\",\n        \"A. Schwarzschild\",\n        \"“A simple and effective positional encoding for transformers\",\n        \"“Blended latent diffusion\",\n        \"“3d shape generation and completion through point-voxel diffusion\",\n        \"“A conditional point diffusion-refinement paradigm for 3d point cloud completion\",\n        \"“Unifying human motion synthesis and style transfer with denoising diffusion probabilistic models\",\n        \"“Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models\",\n        \"“Directed diffusion: Direct control of object placement through attention guidance\",\n        \"“Improving sample quality of diffusion models using self-attention guidance\",\n        \"“Towards performant and reliable undersampled mr reconstruction via diffusion model sampling\",\n        \"E. Hajiramezanali\",\n        \"“Syncdiffusion: Coherent montage via synchronized joint diffusions\",\n        \"“Improving diffusion-based image translation using asymmetric gradient guidance\",\n        \"“Simple black-box adversarial attacks\",\n        \"“Gradient-based adversarial attacks against text transformers\",\n        \"“Enhancing diffusion-based image synthesis with robust classifier guidance\",\n        \"“Plug & play generative networks: Conditional iterative generation of images in latent space\",\n        \"“A study and comparison of human and deep learning recognition performance under visual distortions\",\n        \"“Google’s cloud vision api is not robust to noise\",\n        \"“End-to-end diffusion latent optimization improves classifier guidance\",\n        \"“Suboptimal behavior of bayes and mdl in classification under misspecification\",\n        \"“Rethinking the role of gradient-based attribution methods for model interpretability\",\n        \"“Denoising likelihood score matching for conditional score-based data generation\",\n        \"“Null-text guidance in diffusion models is secretly a cartoon-style creator\",\n        \"K. Hofmann et al\",\n        \"“Diffusion self-guidance for controllable image generation\",\n        \"“Conditional generation from unconditional diffusion models using denoiser representations\",\n        \"“T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models\",\n        \"“A closer look at parameter-efficient tuning in diffusion models\",\n        \"“Uni-controlnet: All-in-one control to text-to-image diffusion models\",\n        \"“Norespeech: Knowledge distillation based conditional diffusion model for noise-robust expressive tts\",\n        \"“Theoretical guarantees for sampling and inference in generative models with latent diffusions\",\n        \"“Accelerating diffusion models for inverse problems through shortcut sampling\",\n        \"“Generative adversarial networks for markovian temporal dynamics: Stochastic continuous data generation\",\n        \"“Distilling the knowledge in a neural network\",\n        \"“Accelerating diffusion sampling with classifier-based feature distillation\",\n        \"“Knowledge diffusion for distillation\",\n        \"“Knowledge distillation in iterative generative models for improved sampling speed\",\n        \"K. Azizzadenesheli\",\n        \"“On distillation of guided diffusion models\",\n        \"“Consistency models\",\n        \"“A comprehensive survey on knowledge distillation of diffusion models\",\n        \"“Boot: Data-free distillation of denoising diffusion models with bootstrapping\",\n        \"“Denoising diffusion samplers\",\n        \"“Learning fast samplers for diffusion models by differentiating through sample quality\",\n        \"“Selective guidance: Are all the denoising steps of guided diffusion important?” arXiv preprint arXiv:2305.09847\",\n        \"“Parallel sampling of diffusion models\",\n        \"“Fast sampling of diffusion models with exponential integrator\",\n        \"“It\\\\\\\\\\\\backslash^{{\\\\{o}}\\\\}-taylor sampling scheme for denoising diffusion probabilistic models using ideal derivatives\",\n        \"“Alleviating exposure bias in diffusion models through sampling with shifted time steps\",\n        \"“Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps\",\n        \"“Structural pruning for diffusion models\",\n        \"“Image super-resolution via iterative refinement\",\n        \"“Noise estimation for generative diffusion models\",\n        \"“Optimal linear subspace search: Learning to construct fast and high-quality schedulers for diffusion models\",\n        \"“Learning to efficiently sample from diffusion probabilistic models\",\n        \"“Latent dynamical implicit diffusion processes\",\n        \"“Trans-dimensional generative modeling via jump diffusion models\",\n        \"“Unifying gans and score-based diffusion as generative particle models\",\n        \"“Expressiveness remarks for denoising diffusion models and samplers\",\n        \"“Variational gaussian process diffusion processes\",\n        \"“Phoenix: A federated generative diffusion model\",\n        \"“Diffusion explainer: Visual explanation for text-to-image stable diffusion\",\n        \"“Interpretable style transfer for text-to-speech with controlvae and diffusion bridge\",\n        \"“Amd autoregressive motion diffusion\",\n        \"“Ar-diffusion: Auto-regressive diffusion model for text generation\",\n        \"“Diffpack: A torsional diffusion model for autoregressive protein side-chain packing\",\n        \"“Diffwave: A versatile diffusion model for audio synthesis\",\n        \"“On architectural compression of text-to-image diffusion models\",\n        \"“Training diffusion models with reinforcement learning\",\n        \"“Few-shot diffusion models\",\n        \"“Training data attribution for diffusion models\",\n        \"“Gsure-based diffusion model training with corrupted data\",\n        \"“Ambient diffusion: Learning clean distributions from corrupted data\",\n        \"“Label-retrieval-augmented diffusion models for learning from noisy labels\",\n        \"“Meta-dm: Applications of diffusion models on few-shot learning\",\n        \"“Few-shot image generation with diffusion models\",\n        \"“Dinar: Diffusion inpainting of neural textures for one-shot human avatars\",\n        \"“Relightify: Relightable 3d faces from a single image via diffusion models\",\n        \"“Zero-shot-learning cross-modality data translation through mutual information guided stochastic diffusion\",\n        \"“Zero-shot medical image translation via frequency-guided diffusion models\",\n        \"“Zet-speech: Zero-shot adaptive emotion-controllable text-to-speech synthesis with diffusion and style-based models\",\n        \"“Exploring diffusion models for unsupervised video anomaly detection\",\n        \"“Dds2m: Self-supervised denoising diffusion spatio-spectral model for hyperspectral image restoration\",\n        \"“Diffusion models and semi-supervised learners benefit mutually with few labels\",\n        \"“A brief introduction to weakly supervised learning\",\n        \"“Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models\",\n        \"“3ddesigner: Towards photorealistic 3d object generation and editing with text-guided diffusion models\",\n        \"“Any-to-any generation via composable diffusion\",\n        \"S. Savarese et al\",\n        \"“Card: Classification and regression diffusion models\",\n        \"“The surprising effectiveness of diffusion models for optical flow and monocular depth estimation\",\n        \"“Policy representation via diffusion probability model for reinforcement learning\",\n        \"“Diffusionnag: Task-guided neural architecture generation with diffusion models\",\n        \"“Diffusion models for black-box optimization\",\n        \"“Diffusionner: Boundary diffusion for named entity recognition\",\n        \"“Is synthetic data from diffusion models ready for knowledge distillation?” 2023\",\n        \"“Synthetic ct generation from mri using 3d transformer-based denoising diffusion model\",\n        \"“Towards consistent video editing with text-to-image diffusion models\",\n        \"“A survey on graph diffusion models: Generative ai in science for molecule\",\n        \"“Diffusion models for high-resolution solar forecasts\",\n        \"“End-to-end latent variational diffusion models for inverse problems in high energy physics\",\n        \"“Geometry-complete diffusion for 3d molecule generation\",\n        \"“A latent diffusion model for protein structure generation\",\n        \"“Spontaneous symmetry breaking in generative diffusion models\",\n        \"“Moldiff: Addressing the atom-bond inconsistency problem in 3d molecule diffusion generation\",\n        \"“Se (3) diffusion model with application to protein backbone generation\",\n        \"and re-sample: Enhancing robustness and generalizability in anomaly detection through automatic diffusion models\",\n        \"“Villandiffusion: A unified backdoor attack framework for diffusion models\",\n        \"“Text-to-image diffusion models can be easily backdoored through multimodal data poisoning\",\n        \"“Densepure: Understanding diffusion models towards adversarial robustness\",\n        \"“Stable diffusion is unstable\",\n        \"“On enhancing the robustness of vision transformers: Defensive diffusion\",\n        \"“Robust evaluation of diffusion-based adversarial purification\",\n        \"“Raising the bar for certified adversarial robustness with diffusion models\",\n        \"“Diffusion theory as a scalpel: Detecting and purifying poisonous dimensions in pre-trained language models caused by backdoor or bias\",\n        \"“Diffusion models for imperceptible and transferable adversarial attack\",\n        \"“Diffusion-based adversarial sample generation for improved stealthiness and controllability\",\n        \"and videotape: Deep fakes and free speech delusions\",\n        \"“Level up the deepfake detection: a method to effectively discriminate images generated by gan architectures and diffusion models\",\n        \"“Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models\",\n        \"“Detecting images generated by diffusers\",\n        \"“Protecting the intellectual property of diffusion models by the watermark diffusion process\",\n        \"“Diffusionshield: A watermark for copyright protection against generative diffusion models\",\n        \"“Extracting training data from diffusion models\",\n        \"“A reproducible extraction of training images from diffusion models\",\n        \"“Diffprotect: Generate adversarial examples with diffusion models for facial privacy protection\",\n        \"“Privacy distillation: Reducing re-identification risk of multimodal diffusion models\",\n        \"“Prisampler: Mitigating property inference of diffusion models\",\n        \"“Differentially private latent diffusion models\",\n        \"“Unlearnable examples for diffusion models: Protect data from unauthorized exploitation\",\n        \"“Analyzing bias in diffusion-based face generation models\",\n        \"“Don’t play favorites: Minority guidance for diffusion models\",\n        \"“Generating high fidelity data from low-density regions using diffusion models\",\n        \"“Understanding and mitigating copying in diffusion models\",\n        \"“Diversify your vision datasets with automatic diffusion-based augmentation\",\n        \"“Stable bias: Analyzing societal representations in diffusion models\",\n        \"“A style-based generator architecture for generative adversarial networks\"\n    ],\n    \"reasoning\": \"Selected these 309 references because they represent foundational work and recent advances in On the Design Fundamentals of Diffusion Models: A Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}