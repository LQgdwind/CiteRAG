{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 65 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Disentangling Transfer and Interference in Multi-Domain Learning\n\nPaper Abstract: Abstract Humans are incredibly good at transferring knowledge from one domain to another, enabling rapid learning of new tasks. Likewise, transfer learning has enabled enormous success in many computer vision problems using pretraining. However, the benefits of transfer in multi-domain learning, where a network learns multiple tasks defined by different datasets, has not been adequately studied. Learning multiple domains could be beneficial, or these domains could interfere with each other given limited network capacity. Understanding how deep neural networks of varied capacity facilitate transfer across inputs from different distributions is a critical step towards open world learning. In this work, we decipher the conditions where interference and knowledge transfer occur in multi-domain learning. We propose new metrics disentangling interference and transfer, set up experimental protocols, and examine the roles of network capacity, task grouping, and dynamic loss weighting in reducing interference and facilitating transfer.\n\n{retrieved_text}\n\nGenerate exactly 65 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 65\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A comprehensive study of class incremental learning algorithms for visual tasks\",\n        \"Towards open world recognition\",\n        \"Budget-aware adapters for multi-domain learning\",\n        \"Integrated perception with recurrent multi-task neural networks\",\n        \"Universal representations: The missing link between faces, text, planktons, and cat breeds\",\n        \"Incremental multi-domain learning with network latent tensor factorization\",\n        \"Multitask learning\",\n        \"Riemannian walk for incremental learning: Understanding forgetting and intransigence\",\n        \"Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks\",\n        \"Instance-aware semantic segmentation via multi-task network cascades\",\n        \"Donâ€™t forget, there is more than forgetting: new metrics for Continual Learning\",\n        \"Multi-task self-supervised visual learning\",\n        \"Blitznet: A real-time deep network for scene understanding\",\n        \"Representation similarity analysis for efficient task taxonomy & transfer learning\",\n        \"Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture\",\n        \"Multi-loss weighting with coefficient of variations\",\n        \"Dynamic task prioritization for multitask learning\",\n        \"Depthwise convolution is all you need for learning multiple visual domains\",\n        \"Deep residual learning for image recognition\",\n        \"Scaling laws for transfer\",\n        \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n        \"Towards open world object detection\",\n        \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\",\n        \"Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory\",\n        \"Similarity of neural network representations revisited\",\n        \"Learning multiple layers of features from tiny images\",\n        \"Tiny imagenet visual recognition challenge\",\n        \"Learning Shared Knowledge for Deep Lifelong Learning using Deconvolutional Networks\",\n        \"Self-paced multi-task learning\",\n        \"Efficient multi-domain learning by covariance normalization\",\n        \"Auxiliary tasks in multi-task learning\",\n        \"End-to-end multi-task learning with attention\",\n        \"Learning multiple tasks with multilinear relationship networks\",\n        \"Gradient episodic memory for continual learning\",\n        \"Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification\",\n        \"Piggyback: Adapting a single network to multiple tasks by learning to mask weights\",\n        \"Adding new tasks to a single network with weight transformations using binary masks\",\n        \"Linear Mode Connectivity in Multitask and Continual Learning\",\n        \"Mish: A self regularized non-monotonic neural activation function\",\n        \"Cross-stitch networks for multi-task learning\",\n        \"A wholistic view of continual learning with deep neural networks: Forgotten lessons and the bridge to active and open world learning\",\n        \"Real-time joint semantic segmentation and depth estimation using asymmetric annotations\",\n        \"Continual lifelong learning with neural networks: A review\",\n        \"Multi-task learning with labeled and unlabeled tasks\",\n        \"Micro-Batch Training with Batch-Channel Normalization and Weight Standardization\",\n        \"Learning multiple visual domains with residual adapters\",\n        \"Efficient parametrization of multi-domain deep neural networks\",\n        \"Incremental learning through deep adaptation\",\n        \"Moon: A mixed objective optimization network for the recognition of facial attributes\",\n        \"Multi-task learning as multi-objective optimization\",\n        \"Overfeat: Integrated recognition, localization and detection using convolutional networks\",\n        \"Which tasks should be learned together in multi-task learning?\",\n        \"A deep hierarchical approach to lifelong learning in minecraft\",\n        \"Is learning the n-th thing any easier than learning the first?\",\n        \"Three scenarios for continual learning\",\n        \"Deep visual domain adaptation: A survey\",\n        \"Understanding and Improving Information Transfer in Multi-Task Learning\",\n        \"Group normalization\",\n        \"Open-world learning and application to product classification\",\n        \"Taskonomy: Disentangling task transfer learning\",\n        \"Context encoding for semantic segmentation\",\n        \"Overcoming negative transfer: A survey\",\n        \"Places: A 10 million image database for scene recognition\",\n        \"Pattern-structure diffusion for multi-task learning\",\n        \"A comprehensive survey on transfer learning\"\n    ],\n    \"reasoning\": \"Selected these 65 references because they represent foundational work and recent advances in Disentangling Transfer and Interference in Multi-Domain Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}