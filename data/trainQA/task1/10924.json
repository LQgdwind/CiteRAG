{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation\n\nPaper Abstract: Abstract Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune the ShareGPT4V model for this specific task, aiming to achieve state-of-the-art results in this particular challenge. Although such a model would not be practical in production, as it is incredibly expensive compared to a specialized model like MiVOLO, it could be very useful in some tasks, like data annotation.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Flamingo: a visual language model for few-shot learning\",\n        \"Human age estimation using ranking svm\",\n        \"Rank consistent ordinal regression for neural networks with application to age estimation\",\n        \"Cross-age reference coding for age-invariant face recognition and retrieval\",\n        \"Sharegpt4v: Improving large multi-modal models with better captions\",\n        \"Instructblip: Towards general-purpose vision-language models with instruction tuning\",\n        \"How good is chatgpt at face biometrics? a first look into recognition, soft biometrics, and explainability\",\n        \"Age and gender estimation of unfiltered faces\",\n        \"Guiding instruction-based image editing via multimodal large language models\",\n        \"Open-vocabulary object detection via vision and language knowledge distillation\",\n        \"Hierarchical attention-based age estimation and bias estimation\",\n        \"Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Mivolo: Multi-input transformer for age and gender estimation\",\n        \"The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale\",\n        \"Lisa: Reasoning segmentation via large language model\",\n        \"Age and gender classification using convolutional neural networks\",\n        \"Llava-med: Training a large language-and-vision assistant for biomedicine in one day\",\n        \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\",\n        \"Ordinalclip: Learning rank prompts for language-guided ordinal regression\",\n        \"Fp-age: Leveraging face parsing attention for facial age estimation in the wild\",\n        \"Improved baselines with visual instruction tuning\",\n        \"Llava-next: Improved reasoning, ocr, and world knowledge\",\n        \"Visual instruction tuning\",\n        \"Agenet: Deeply learned regressor and classifier for robust apparent age estimation\",\n        \"Ei-clip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval\",\n        \"Video-chatgpt: Towards detailed video understanding via large vision and language models\",\n        \"ChatGPT: A Large Language Model\",\n        \"Gpt-4 technical report\",\n        \"A call to reflect on evaluation practices for age estimation: Comparative analysis of the state-of-the-art and a unified benchmark\",\n        \"Kosmos-2: Grounding multimodal large language models to the world\",\n        \"Learning transferable visual models from natural language supervision\",\n        \"Denseclip: Language-guided dense prediction with context-aware prompting\",\n        \"Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output cnn\",\n        \"Facial expression and attributes recognition based on multi-task learning of lightweight neural networks\",\n        \"Laion-5b: An open large-scale dataset for training next generation image-text models\",\n        \"Moving window regression: A novel approach to ordinal regression\",\n        \"Czl-ciae: Clip-driven zero-shot learning for correcting inverse age estimation\",\n        \"Masked contrastive graph representation learning for age estimation\",\n        \"Llama: Open and efficient foundation language models\",\n        \"Attention is all you need\",\n        \"Improving face-based age estimation with attention-based dynamic patch fusion\",\n        \"Learning-to-rank meets language: Boosting language-driven ordering alignment for ordinal classification\",\n        \"Visionllm: Large language model is also an open-ended decoder for vision-centric tasks\",\n        \"mplug-owl: Modularization empowers large language models with multimodality\",\n        \"C3ae: Exploring the limits of compact model for age estimation\",\n        \"Quantifying facial age by posterior of age comparisons\",\n        \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}