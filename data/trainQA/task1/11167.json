{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 136 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse Sampling\n\nPaper Abstract: Abstract Eye tracking is becoming an increasingly important task domain in emerging computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye tracking system suffers from long end-to-end tracking latency and can easily eat up half of the power budget of a mobile VR device. Most existing optimization efforts exclusively focus on the computation pipeline by optimizing the algorithm and/or designing dedicated accelerators while largely ignoring the front-end of any eye tracking pipeline: the image sensor. This paper makes a case for co-designing the imaging system with the computing system. In particular, we propose the notion of ``in-sensor sparse sampling'', whereby the pixels are drastically downsampled (by 20√ó\\times) within the sensor. Such in-sensor sampling enhances the overall tracking efficiency by significantly reducing 1) the power consumption of the sensor readout chain and sensor-host communication interfaces, two major power contributors, and 2) the work done on the host, which receives and operates on far fewer pixels. With careful reuse of existing pixel circuitry, our proposed BlissCam requires little hardware augmentation to support the in-sensor operations. Our synthesis results show up to 8.2 √ó\\times energy reduction and 1.4 √ó\\times latency reduction over existing eye tracking pipelines.\n\n{retrieved_text}\n\nGenerate exactly 136 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 136\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"1/1.395 Color CMOS 200 Megapixel (16384x12288) Image Sensor with PureCel¬ÆPlus-S Technology\",\n        \"Battery capacity, battery life and charging time of the oculus quest 2.\",\n        \"Canon High Speed Global Shutter\",\n        \"Does vr use a lot of energy?\",\n        \"Eyelink 1000 plus.\",\n        \"HTC VIVE PRO EYE: Next-generation virtual reality\",\n        \"Introducing smart eye pro 10: Streamlined set up with enhanced user-friendly experience.\",\n        \"Jetson modules.\",\n        \"Meta quest 3 release date, price, and specs.\",\n        \"Micron 178-Ball, Single-Channel Mobile LPDDR3 SDRAM Features.\",\n        \"Micron System Power Calculators.\",\n        \"Most advanced eye tracking system ‚Äî tobii pro spectrum.\",\n        \"MST Family of Products\",\n        \"Nvidia reveals xavier soc details.\",\n        \"OMNIVISION Launches World‚Äôs First Image Sensor for Automotive Viewing Cameras with 140 dB HDR and Top LED Flicker Mitigation Performance\",\n        \"Real Examples of MIPI Interfaces in AR/VR, Mobile, and Other Image Applications.\",\n        \"Sony to Release World's First Intelligent Vision Sensors with AI Processing Functionality\",\n        \"Specifications for Eye Tracker 5.\",\n        \"The Impact of Higher Data Rate Requirements on MIPI CSI and MIPI DSI Designs.\",\n        \"Eye tracking in virtual reality: A broad review of applications and challenges\",\n        \"Event based, near eye gaze tracking beyond 10,000 hz\",\n        \"Saccade, Saccadic Eye Movement\",\n        \"A low-power convolutional neural network face recognition processor and a cis integrated with always-on face detector\",\n        \"14.6 a 0.62 mw ultra-low-power convolutional-neural-network face-recognition processor and a cis integrated with always-on haar-like face detector\",\n        \"A 0.5 error 10 mw cmos image sensor-based gaze estimation processor\",\n        \"Eye-tracking metrics in perception and visual attention research\",\n        \"Unprocessing images for learned raw denoising\",\n        \"A review of eye tracking for understanding and improving diagnostic interpretation\",\n        \"Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information\",\n        \"Stable signal recovery from incomplete and inaccurate measurements\",\n        \"Gaze-contingent retinal speckle suppression for perceptually-matched foveated holographic displays\",\n        \"Eye tracking based human computer interaction: Applications and their uses\",\n        \"A high-frame-rate eye-tracking framework for mobile devices\",\n        \"Ritnet: Real-time semantic segmentation of the eye for gaze tracking\",\n        \"3d gaze estimation with a single camera without ir illumination\",\n        \"High-frequency systolic array-based transformer accelerator on field programmable gate arrays\",\n        \"A spatial-temporal multiresolution cmos image sensor with adaptive frame rates for tracking the moving objects in region-of-interest and suppressing motion blur\",\n        \"An energy/illumination-adaptive cmos image sensor with reconfigurable modes of operations\",\n        \"Design of an always-on image sensor using an analog lightweight convolutional neural network\",\n        \"Image sensors in 3d stacking technology: Retrospective and perspectives from a digital architect‚Äôs point of view\",\n        \"Eye tracking in virtual reality\",\n        \"A study and comparison of human and deep learning recognition performance under visual distortions\",\n        \"Single-pixel imaging via compressive sampling\",\n        \"Color-perception-guided display power reduction for virtual reality\",\n        \"Compressed sensing image reconstruction via recursive spatially adaptive filtering\",\n        \"√úber die von der molekularkinetischen theorie der w√§rme geforderte bewegung von in ruhenden fl√ºssigkeiten suspendierten teilchen\",\n        \"9.6 a 1/2.3 inch 12.3 mpixel with on-chip 4.97 tops/w cnn processor back-illuminated stacked cmos image sensor\",\n        \"Digital camera simulation\",\n        \"Real-time gaze tracking with event-driven eye segmentation\",\n        \"Learned in-sensor visual computing: From compression to eventification\",\n        \"``Asv: Accelerated stereo vision system\",\n        \"K. Daniilidis et al\",\n        \"``Openeds: Open eye dataset\",\n        \"``Run-length encodings (corresp.)\",\n        \"``Mipi white paper: An introductory guide to mipi automotive serdes solutions (mass)\",\n        \"'' in Proceedings of the IEEE international conference on computer vision\",\n        \"``A single camera eye-gaze tracking system with free head motion\",\n        \"``Power-up sram state as an identifying fingerprint and source of true random numbers\",\n        \"``A 2.71 nj/pixel gaze-activated object recognition system for low-power mobile smart glasses\",\n        \"``How robust are wearable eye trackers to slow and fast head and body movements?'' Behavior Research Methods\",\n        \"``Architecting efficient multi-modal aiot systems\",\n        \"'' IEEE Journal of Solid-State Circuits\",\n        \"``Gaze analysis and prediction in virtual reality\",\n        \"S.-G. Wuu et al\",\n        \"A. Berkovich et al\",\n        \"``Assessing saccadic eye movements with head-mounted display virtual reality technology\",\n        \"``Finely tuned eye movements enhance visual acuity\",\n        \"``Interaction-free calibration for optical see-through head-mounted displays based on 3d eye localization\",\n        \"in The mind\",\n        \"``Eye-tracking in educational practice: Investigating visual perception underlying teaching and learning in the classroom\",\n        \"``Real-time 3d reconstruction and 6-dof tracking with an event camera\",\n        \"``Eye tracking-based 360 vr foveated/tiled video rendering\",\n        \"H.-J. Yang et al\",\n        \"``Rhythmic pixel regions: multi-resolution visual sensing system towards high-precision visual computing at low power\",\n        \"``Ellseg: An ellipse segmentation framework for robust gaze tracking\",\n        \"``Progressive compressive sensing of large images with multiscale deep learning reconstruction\",\n        \"``A low-power 65/14nm stacked cmos image sensor\",\n        \"``Energy-efficient video processing for virtual reality\",\n        \"``Etracker: A mobile gaze-tracking system with near-eye display based on a combined gaze-tracking algorithm\",\n        \"``A 128√ó\\\\times128 120db 15Œºùúá\\\\mus latency asynchronous temporal contrast vision sensor\",\n        \"``Redeye: analog convnet image sensor architecture for continuous mobile vision\",\n        \"``Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization\",\n        \"``Augmented reality-the next frontier of image sensors and compute systems\",\n        \"``Patchdropout: Economizing vision transformers using patch dropout\",\n        \"``Leca: In-sensor learned compressive acquisition for efficient machine vision on the edge\",\n        \"``Hogeye: neural approximation of hog feature extraction in rram-based 3d-stacked image sensors\",\n        \"``Camj: Enabling system-level energy modeling and architectural exploration for in-sensor visual computing\",\n        \"P. Vandergheynst\",\n        \"``Gaze-aware displays and interaction\",\n        \"``Neural sensors: Learning pixel exposures for hdr imaging and video compressive sensing with programmable sensors\",\n        \"``Dynamic graph modeling of simultaneous eeg and eye-tracking data for reading task identification\",\n        \"``ishadow: design of a wearable\",\n        \"C. Andriamisaina\",\n        \"``Event-based moving object detection and tracking\",\n        \"Smart CMOS image sensors and applications. CRC press\",\n        \"``Cmos image sensor with per-column œÉùúé\\\\sigmaŒ¥ùõø\\\\delta adc and programmable compressed sensing\",\n        \"``A 640√ó\\\\times640 fully dynamic cmos image sensor for always-on operation\",\n        \"``Ultra-low power cmos image sensor with two-step logical shift algorithm-based correlated double sampling scheme\",\n        \"``Cmos image sensor with two-step single-slope adcs and a detachable super capacitive dac\",\n        \"``Perceptually-based foveated virtual reality\",\n        \"``Towards foveated rendering for gaze-tracked virtual reality\",\n        \"P. De Tournemire\",\n        \"``Automated spatial calibration of hmd systems with unconstrained eye-cameras\",\n        \"``Understanding cognitive and affective mechanisms in social psychology through eye-tracking\",\n        \"``A quality-centered analysis of eye tracking data in foveated rendering\",\n        \"Physics of digital photography. IOP Publishing\",\n        \"``Miniature eye movements enhance fine spatial detail\",\n        \"``Deepscaletool: A tool for the accurate estimation of technology scaling in the deep-submicron era\",\n        \"``Trends and developments in state-of-the-art cmos image sensors\",\n        \"``Online webcam-based eye tracking in cognitive science: A first look\",\n        \"``2.45 e-rms low-random-noise\",\n        \"``Temporal frame filtering for autonomous driving using 3d-stacked global shutter cis with iwo buffer memory and near-pixel compute\",\n        \"``Weighted pointer: Error-aware gaze-based interaction through fallback modalities\",\n        \"``34.2 a 21pj/frame/pixel imager and 34pj/frame/pixel image processor for a low-vision augmented-reality smart contact lens\",\n        \"``Scaling equations for the accurate prediction of cmos device performance from 180 nm to 7 nm\",\n        \"``Eye tracking in human-computer interaction and usability research\",\n        \"``Segmenter: Transformer for semantic segmentation\",\n        \"``A 1.6 pj/bit 96% stable chip-id generating circuit using process variations\",\n        \"D. Yoneyama et al\",\n        \"E. Van der Sluis\",\n        \"``Attention is all you need\",\n        \"``Via: A novel vision-transformer accelerator based on fpga\",\n        \"``Multi-sensor eye-tracking systems and tools for capturing student attention and understanding engagement in learning: A review\",\n        \"``Eyecontact: scleral coil eye tracking for virtual reality\",\n        \"``A black box method for stability analysis of arbitrary sram cell structures\",\n        \"``A review of 3-dimensional wafer level stacked backside illuminated cmos image sensor process technologies\",\n        \"``Senputing: An ultra-low-power always-on vision perception chip featuring the deep fusion of sensing and computing\",\n        \"``Accelerating attention mechanism on fpgas based on efficient reconfigurable systolic array\",\n        \"``Deepvog: Open-source pupil segmentation and gaze estimation in neuroscience using deep learning\",\n        \"``High-speed imaging using cmos image sensor with quasi pixel-wise exposure\",\n        \"``Eyecod: eye tracking system acceleration via flatcam-based algorithm & accelerator co-design\",\n        \"``A data-compressive 1.5/2.75-bit log-gradient qvga image sensor with multi-scale readout for always-on object detection\",\n        \"``Evaluation of appearance-based methods and implications for gaze-based applications\",\n        \"``Eye tracking based control system for natural human-computer interaction\",\n        \"V. Boominathan et al\",\n        \"in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\"\n    ],\n    \"reasoning\": \"Selected these 136 references because they represent foundational work and recent advances in BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse Sampling, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}