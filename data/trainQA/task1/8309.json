{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 56 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Planning Goals for Exploration\n\nPaper Abstract: Abstract Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose “Planning Exploratory Goals” (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent’s goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to “plan goal commands”. In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://penn-pal-lab.github.io/peg/\n\n{retrieved_text}\n\nGenerate exactly 56 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 56\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Hindsight experience replay\",\n        \"Active learning of inverse models with intrinsically motivated goal exploration in robots\",\n        \"Unifying count-based exploration and intrinsic motivation\",\n        \"LEAF: latent exploration along the frontier\",\n        \"Exploration by random network distillation\",\n        \"Explore, discover and learn: Unsupervised discovery of state-covering skills\",\n        \"Goal-conditioned reinforcement learning with imagined subgoals\",\n        \"UCB EXPLORATION VIA q-ENSEMBLES\",\n        \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models\",\n        \"Learning the value of information and reward over time when solving exploration-exploitation problems\",\n        \"A tutorial on the cross-entropy method\",\n        \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control\",\n        \"First return then explore\",\n        \"Diversity is all you need: Learning skills without a reward function\",\n        \"Automatic goal generation for reinforcement learning agents\",\n        \"Memory based trajectory-conditioned policies for learning from sparse rewards\",\n        \"Dream to control: Learning behaviors by latent imagination\",\n        \"Mastering atari with discrete world models\",\n        \"Deep hierarchical planning from pixels\",\n        \"Dynamical distance learning for semi-supervised and unsupervised skill discovery\",\n        \"Explicit explore-exploit algorithms in continuous state spaces\",\n        \"Time-agnostic prediction: Predicting predictable video frames\",\n        \"Direct then diffuse: Incremental unsupervised skill discovery for state covering and goal reaching\",\n        \"Curiosity-driven multi-criteria hindsight experience replay\",\n        \"Towards practical multi-object manipulation using relational reinforcement learning\",\n        \"Discovering and achieving goals with world models\",\n        \"Transformers are sample-efficient world models\",\n        \"Deep dynamics models for learning dexterous manipulation\",\n        \"Overcoming exploration in reinforcement learning with demonstrations\",\n        \"Visual reinforcement learning with imagined goals\",\n        \"Adaptive skip intervals: Temporal abstraction for recurrent dynamical models\",\n        \"Deep exploration via randomized value functions\",\n        \"Intrinsic motivation systems for autonomous mental development\",\n        \"Curiosity-driven exploration by self-supervised prediction\",\n        \"Long-horizon visual planning with goal-conditioned hierarchical predictors\",\n        \"When should agents explore?\",\n        \"Maximum entropy gain exploration for long horizon multi-goal reinforcement learning\",\n        \"Skew-fit: State-covering self-supervised reinforcement learning\",\n        \"An analytic solution to discrete bayesian reinforcement learning\",\n        \"Model-based reinforcement learning via latent-space collocation\",\n        \"Formal theory of creativity, fun, and intrinsic motivation (1990–2010)\",\n        \"Planning to explore via self-supervised world models\",\n        \"Dynamics-aware unsupervised discovery of skills\",\n        \"Model-based active exploration\",\n        \"Balancing exploration and exploitation in reinforcement learning using a value of information criterion\",\n        \"Value of information when an estimated random variable is hidden\",\n        \"Learning more skills through optimistic exploration\",\n        \"Deepmind control suite\",\n        \"Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards\",\n        \"Multi-stage episodic control for strategic exploration in text games\",\n        \"Many-goals reinforcement learning\",\n        \"Model predictive path integral control using covariance variable importance sampling\",\n        \"Frontier-based exploration using multiple robots\",\n        \"Exploration in deep reinforcement learning: a comprehensive survey\",\n        \"When to go, and when to explore: The benefit of post-exploration in intrinsic motivation\",\n        \"Automatic curriculum learning through value disagreement\"\n    ],\n    \"reasoning\": \"Selected these 56 references because they represent foundational work and recent advances in Planning Goals for Exploration, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}