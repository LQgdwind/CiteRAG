{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 49 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Active Preference-Based Gaussian Process Regression for Reward Learning\n\nPaper Abstract: Abstract ††*First two authors contributed equally and are listed in alphabetical order. Designing reward functions is a challenging problem in AI and robotics. Humans usually have a difficult time directly specifying all the desirable behaviors that a robot needs to optimize. One common approach is to learn reward functions from collected expert demonstrations. However, learning reward functions from demonstrations introduces many challenges: some methods require highly structured models, e.g. reward functions that are linear in some predefined set of features, while others adopt less structured reward functions that on the other hand require tremendous amount of data. In addition, humans tend to have a difficult time providing demonstrations on robots with high degrees of freedom, or even quantifying reward values for given demonstrations. To address these challenges, we present a preference-based learning approach, where as an alternative, the human feedback is only in the form of comparisons between trajectories. Furthermore, we do not assume highly constrained structures on the reward function. Instead, we model the reward function using a Gaussian Process (GP) and propose a mathematical formulation to actively find a GP using only human preferences. Our approach enables us to tackle both inflexibility and data-inefficiency problems within a preference-based learning framework. Our results in simulations and a user study suggest that our approach can efficiently learn expressive reward functions for robotics tasks.\n\n{retrieved_text}\n\nGenerate exactly 49 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 49\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Generative adversarial imitation learning\",\n        \"Learning monocular reactive uav control in cluttered natural environments\",\n        \"Multi-agent generative adversarial imitation learning\",\n        \"Third-person imitation learning\",\n        \"Faulty reward functions in the wild, Dec 2016\",\n        \"Policy invariance under reward transformations: Theory and application to reward shaping\",\n        \"Deep reinforcement learning from human preferences\",\n        \"Human preferences for robot-human hand-over configurations\",\n        \"Reward learning from human preferences and demonstrations in atari\",\n        \"Deep bayesian reward learning from preferences\",\n        \"Preference-based learning for exoskeleton gait optimization\",\n        \"Active preference-based learning of reward functions\",\n        \"Bayesian active learning for collaborative task specification using equivalence regions\",\n        \"April: Active preference learning-based reinforcement learning\",\n        \"Bayesian preference elicitation for multiobjective engineering design optimization\",\n        \"Batch active preference-based learning of reward functions\",\n        \"Batch active learning using determinantal point processes\",\n        \"Asking easy questions: A user-friendly approach to active reward learning\",\n        \"Active learning of reward dynamics from hierarchical queries\",\n        \"Learning reward functions by integrating human demonstrations and preferences\",\n        \"Learning an urban air mobility encounter model from expert preferences\",\n        \"The green choice: Learning and influencing human decisions on shared roads\",\n        \"Teacher-aware active robot learning\",\n        \"Gaussian processes for machine learning\",\n        \"Apprenticeship learning via inverse reinforcement learning\",\n        \"Exploration and apprenticeship learning in reinforcement learning\",\n        \"Algorithms for inverse reinforcement learning\",\n        \"Bayesian inverse reinforcement learning\",\n        \"Maximum entropy inverse reinforcement learning\",\n        \"Controlling assistive robots with learned latent actions\",\n        \"Keyframe-based learning from demonstration\",\n        \"Formalizing assistive teleoperation\",\n        \"Shared autonomy via hindsight optimization\",\n        \"Data-driven motion mappings improve transparency in teleoperation\",\n        \"Do you want your autonomous car to drive like you?\",\n        \"Learning robot objectives from physical human interaction\",\n        \"Learning from physical human corrections, one feature at a time\",\n        \"Preferential bayesian optimization\",\n        \"Preference learning with gaussian processes\",\n        \"Collaborative gaussian processes for preference learning\",\n        \"Pairwise judgements and absolute ratings with gaussian process priors\",\n        \"Active learning with gaussian processes for object categorization\",\n        \"Bayesian active learning for classification and preference learning\",\n        \"Active reward learning with a novel acquisition function\",\n        \"Approximations for binary gaussian process classification\",\n        \"Planning for autonomous cars that leverage effects on human actions\",\n        \"Mujoco: A physics engine for model-based control\",\n        \"Fast poisson disk sampling in arbitrary dimensions\",\n        \"Fetch and freight: Standard platforms for service robot applications\"\n    ],\n    \"reasoning\": \"Selected these 49 references because they represent foundational work and recent advances in Active Preference-Based Gaussian Process Regression for Reward Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}