{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 286 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Trustworthy AI: A Computational Perspective\n\nPaper Abstract: Abstract. In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone’s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.\n\n{retrieved_text}\n\nGenerate exactly 286 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 286\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"IEEE Standard for Software Reviews and Audits\",\n        \"The Montreal Declaration of Responsible AI\",\n        \"Federated learning of predictive models from federated Electronic Health Records\",\n        \"Governance Principles for the New Generation Artificial Intelligence–Developing Responsible Artificial Intelligence\",\n        \"Federated AI Technology Enabler\",\n        \"LEAF: A Benchmark for Federated Settings\",\n        \"A list of Homomorphic Encryption libraries, software or resources\",\n        \"A list of MPC software or resources\",\n        \"OenDP: Open Source Tools for Differential Privacy\",\n        \"Opacus: Train PyTorch models with Differential Privacy\",\n        \"Paddle Federated Learning\",\n        \"A Technical Analysis of Confidential Computing\",\n        \"TensorFlow Federated\",\n        \"TensorFlow Privacy\",\n        \"H Brendan McMahan\",\n        \"A survey on homomorphic encryption schemes: Theory and implementation\",\n        \"Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)\",\n        \"Zoubin Ghahramani\",\n        \"Auditing black-box models for indirect influence\",\n        \"Mohammad Javad Azizi\",\n        \"Ali Shahin Shamsabadi\",\n        \"A lambic: a privacy-preserving recommender system for electronic commerce\",\n        \"Threat of adversarial attacks on deep learning in computer vision: A survey\",\n        \"Privacy-preserving machine learning: Threats and solutions\",\n        \"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI\",\n        \"Ai explainability 360: An extensible toolkit for understanding data and machine learning models\",\n        \"The security of machine learning\",\n        \"Marta Ruiz Costa-Jussà\",\n        \"Probabilistic verification of fairness properties via concentration\",\n        \"AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias\",\n        \"Principles and practice of explainable machine learning\",\n        \"A convex framework for fair regression\",\n        \"Fairness in criminal justice risk assessments: The state of the art\",\n        \"Giorgio Giacinto\",\n        \"Poisoning attacks against support vector machines\",\n        \"Pattern recognition and machine learning\",\n        \"Adversarial attacks on node embeddings\",\n        \"Adversarial Attacks on Node Embeddings via Graph Poisoning\",\n        \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n        \"Venkatesh Saligrama\",\n        \"Identifying and reducing gender bias in word-level language models\",\n        \"Jeffrey Sorensen\",\n        \"Compositional fairness constraints for graph embeddings\",\n        \"Analysing and assessing accountability: A conceptual framework 1\",\n        \"Privacy-preserving biometric identification using secure multiparty computation: An overview and recent trends\",\n        \"Toward trustworthy AI development: mechanisms for supporting verifiable claims\",\n        \"Colleen Alkalay-Houlihan\",\n        \"A (very) brief history of artificial intelligence\",\n        \"Notes from the AI frontier: Modeling the impact of AI on the world economy\",\n        \"Joy Buolamwini and Timnit Gebru\",\n        \"How the machine ‘thinks’: Understanding opacity in machine learning algorithms\",\n        \"Dimitrios Stamoulis\",\n        \"Three naive bayes approaches for discrimination-free classification\",\n        \"Why unbiased computational processes can lead to discriminative decision procedures\",\n        \"Nicholas Carlini and David Wagner\",\n        \"Towards Evaluating the Robustness of Neural Networks\",\n        \"Unlabeled Data Improves Adversarial Robustness\",\n        \"Many Cars Tone Deaf To Women’s Voices\",\n        \"Fairness in Machine Learning: A Survey\",\n        \"How to be Fair and Diverse?\",\n        \"Improved adversarial learning for fair classification\",\n        \"Adversarial attacks and defences: A survey\",\n        \"Why is my classifier discriminatory?\",\n        \"Bias and Debias in Recommender System: A Survey and Future Directions\",\n        \"Fast Gradient Attack on Network Embedding\",\n        \"Nicholas Carlini\",\n        \"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning\",\n        \"A survey of accelerator architectures for deep neural networks\",\n        \"Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples\",\n        \"A survey of model compression and acceleration for deep neural networks\",\n        \"Robin Cheong and Robel Daniel\",\n        \"Mariya Georgieva\",\n        \"Towards the limit of network quantization\",\n        \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\",\n        \"Fairer and more accurate, but for whom?\",\n        \"Raviv Cohen and Derek Ruths\",\n        \"Taco Cohen and Max Welling\",\n        \"Independent High-Level Expert Group on Artificial Intelligence (2019)\",\n        \"The measure and mismeasure of fairness: A critical review of fair machine learning\",\n        \"Algorithmic bias: A counterfactual perspective\",\n        \"RobustBench: a standardized adversarial robustness benchmark\",\n        \"Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack\",\n        \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\",\n        \"Adversarial Attack on Graph Structured Data\",\n        \"A survey of the state of explainable AI for natural language processing\",\n        \"Practicing differential privacy in health care: A review\",\n        \"An overview of privacy in machine learning\",\n        \"Universal transformers\",\n        \"Deep learning in natural language processing\",\n        \"Assessing the consequences of text preprocessing decisions\",\n        \"Towards a rigorous science of interpretable machine learning\",\n        \"Techniques for interpretable machine learning\",\n        \"The algorithmic foundations of differential privacy\",\n        \"Sorelle A Friedler\",\n        \"On the connection between adversarial robustness and saliency map interpretability\",\n        \"Robust physical-world attacks on deep learning models\",\n        \"A Graph Neural Network Framework for Social Recommendations\",\n        \"Neil Zhenqiang Gong\",\n        \"Certifying and Removing Disparate Impact\",\n        \"Learning fair representations via an adversarial framework\",\n        \"Adversarial attacks on medical machine learning\",\n        \"AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations\",\n        \"Estimation of energy consumption in machine learning\",\n        \"Suchin Gururangan\",\n        \"Craig Gentry and Shai Halevi\",\n        \"Non-Discriminatory Machine Learning through Convex Fairness Criteria\",\n        \"Mohammad Yaghini\",\n        \"Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them\",\n        \"Hila Gonen and Kellie Webster\",\n        \"Adversarial and clean data are not twins\",\n        \"Race and Social Equity: A Nervous Area of Government\",\n        \"Explaining and harnessing adversarial examples\",\n        \"On the (statistical) detection of adversarial examples\",\n        \"A survey of methods for explaining black box models\",\n        \"A methodology for direct and indirect discrimination prevention in data mining\",\n        \"EIE: Efficient inference engine on compressed deep neural network\",\n        \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\",\n        \"Deep speech: Scaling up end-to-end speech recognition\",\n        \"Equality of opportunity in supervised learning\",\n        \"FedML: A Research Library and Benchmark for Federated Machine Learning\",\n        \"Calibration for the (Computationally-Identifiable) Masses\",\n        \"Nicolas Angelard-Gontier\",\n        \"Privacy as protection of the incomputable self: From agnostic to agonistic machine learning\",\n        \"Distilling the knowledge in a neural network\",\n        \"Loss-aware weight quantization of deep networks\",\n        \"The ugly truth about ourselves and our robot creations: the problem of bias and social inequity\",\n        \"Membership Inference Attacks on Machine Learning: A Survey\",\n        \"Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition\",\n        \"Ben Hutchinson and Margaret Mitchell\",\n        \"FAE: A Fairness-Aware Ensemble Framework\",\n        \"Speeding up convolutional neural networks with low rank expansions\",\n        \"Differential privacy and machine learning: a survey and review\",\n        \"Heinrich Jiang and Ofir Nachum\",\n        \"Drug discovery with explainable artificial intelligence\",\n        \"Adversarial attacks and defenses on graphs: A review and empirical study\",\n        \"Graph Structure Learning for Robust Graph Neural Networks\",\n        \"Constance: Modeling annotation contexts to improve stance classification\",\n        \"Fair Algorithms for Infinite and Contextual Bandits\",\n        \"Censored and fair universal representations using generative adversarial models\",\n        \"Secure, privacy-preserving and federated machine learning in medical imaging\",\n        \"Faisal Kamiran and Toon Calders\",\n        \"Data preprocessing techniques for classification without discrimination\",\n        \"Michael Kapralov and Kunal Talwar\",\n        \"and Ananda Theertha Suresh\",\n        \"Konstantin Mishchenko\",\n        \"Mateo Rojas-Carulla\",\n        \"Sequence-level knowledge distillation\",\n        \"Semi-supervised classification with graph convolutional networks\",\n        \"Examining gender and race bias in two hundred sentiment analysis systems\",\n        \"Artificial intelligence: definition, trends, techniques, and cases\",\n        \"Eleftherios Spyromitros-Xioufis\",\n        \"Quantifying the carbon emissions of machine learning\",\n        \"Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads\",\n        \"Albert: A lite bert for self-supervised learning of language representations\",\n        \"Convolutional networks for images, speech, and time series\",\n        \"David Kohlbrenner\",\n        \"Trust in automation: Designing for appropriate reliance\",\n        \"Discrete Attacks and Submodular Optimization with Applications to Text Classification\",\n        \"DeepRobust: A PyTorch Library for Adversarial Attacks and Defenses\",\n        \"Explainable ai: A review of machine learning interpretability methods\",\n        \"Designing monitoring systems for continuous certification of cloud services: deriving meta-requirements and design guidelines\",\n        \"Say What I Want: Towards the Dark Side of Neural Dialogue Models\",\n        \"The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification\",\n        \"DIG: A Turnkey Library for Diving into Graph Deep Learning Research\",\n        \"Calibrated fairness in bandits\",\n        \"Learning to pivot with adversarial networks\",\n        \"Gender bias in neural natural language processing\",\n        \"A Unified Approach to Interpreting Model Predictions\",\n        \"Parameterized explainer for graph neural network\",\n        \"Attacking Graph Convolutional Networks via Rewiring\",\n        \"Towards deep learning models resistant to adversarial attacks\",\n        \"Collaborative filtering and the missing at random assumption\",\n        \"Ethical implications and accountability of algorithms\",\n        \"On measuring social biases in sentence encoders\",\n        \"An integrative model of organizational trust\",\n        \"A proposal for the dartmouth summer research project on artificial intelligence, august 31, 1955\",\n        \"Advances and Open Problems in Federated Learning\",\n        \"Frank McSherry and Ilya Mironov\",\n        \"Frank McSherry and Kunal Talwar\",\n        \"A survey on bias and fairness in machine learning\",\n        \"The cost of fairness in classification\",\n        \"Aditya Krishna Menon and Robert C Williamson\",\n        \"Are sixteen heads really better than one?\",\n        \"Explanation in artificial intelligence: Insights from the social sciences\",\n        \"Deep learning for healthcare: review, opportunities and challenges\",\n        \"A survey of methods for analyzing and improving GPU energy efficiency\",\n        \"Payman Mohassel and Yupeng Zhang\",\n        \"Interpretable machine learning\",\n        \"Seyed-Mohsen Moosavi-Dezfooli\",\n        \"How police technology aggravates racial inequity: A taxonomy of problems and a path forward\",\n        \"Fair Inference on Outcomes\",\n        \"Stratis Ioannidis\",\n        \"Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models\",\n        \"An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness\",\n        \"InterpretML: A Unified Framework for Machine Learning Interpretability\",\n        \"Asilomar AI principles\",\n        \"Social data: Biases, methodological pitfalls, and ethical boundaries\",\n        \"Google Photos Identified Two Black People as’ Gorillas’\",\n        \"Juan Carlos Medina Serrano\",\n        \"Yakun Sophia Shao\",\n        \"Reducing gender bias in abusive language detection\",\n        \"Assessing gender bias in machine translation: a case study with google translate\",\n        \"Interpretable deep learning in drug discovery\",\n        \"Toward a better trade-off between performance and fairness with kernel-based distribution matching\",\n        \"Induction of decision trees\",\n        \"Certified defenses against adversarial examples\",\n        \"Margaret Mitchell\",\n        \"Deep convolutional neural networks for image classification: A comprehensive review\",\n        \"The future of digital health with federated learning\",\n        \"A survey of privacy attacks in machine learning\",\n        \"A field study of the impact of gender and user’s technical experience on the performance of voice-activated medical tracking application\",\n        \"Fitnets: Hints for thin deep nets\",\n        \"Are face-detection cameras racist\",\n        \"diffpriv: An R Package for Easy Differential Privacy\",\n        \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n        \"Artificial intelligence: a modern approach\",\n        \"Improving smiling detection with race and gender diversity\",\n        \"Mohammed Achemlal\",\n        \"Thomas Schneider\",\n        \"Machine learning for detection and diagnosis of disease\",\n        \"Aequitas: A bias and fairness audit toolkit\",\n        \"Auditing algorithms: Research methods for detecting discrimination on internet platforms\",\n        \"The graph neural network model\",\n        \"Michael Cogswell\",\n        \"Christoph Studer\",\n        \"Adversarial training for free!\",\n        \"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview\",\n        \"H Andrew Schwartz\",\n        \"Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data\",\n        \"Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n        \"Darts: Deceiving autonomous cars with toxic signs\",\n        \"The eu approach to ethics guidelines for trustworthy artificial intelligence\",\n        \"Kamalika Chaudhuri\",\n        \"Anand Krishnan Prakash\",\n        \"Evaluating gender bias in machine translation\",\n        \"Energy and policy considerations for deep learning in NLP\",\n        \"Patient knowledge distillation for bert model compression\",\n        \"Efficient processing of deep neural networks: A tutorial and survey\",\n        \"Intriguing properties of neural networks\",\n        \"Distilling task-specific knowledge from bert into simple neural networks\",\n        \"Google’s speech recognition has a gender bias\",\n        \"Trustworthy artificial intelligence\",\n        \"A survey on explainable artificial intelligence (xai): Toward medical xai\",\n        \"Vaggelis Atlidakis\",\n        \"Michael K Reiter\",\n        \"Robustness may be at odds with accuracy\",\n        \"Big questions for social media big data: Representativeness, validity and other methodological pitfalls\",\n        \"Applications of machine learning in drug discovery and development\",\n        \"Getting gender right in neural machine translation\",\n        \"Counterfactual explanations without opening the black box: Automated decisions and the GDPR\",\n        \"Finprivacy: A privacy-preserving mechanism for fingerprint identification\",\n        \"Ioannis Christos Karakozis\",\n        \"Randomized response: A survey technique for eliminating evasive answer bias\",\n        \"Federated learning with differential privacy: Algorithms and performance analysis\",\n        \"Anna Alexandrova\",\n        \"Why we should have seen that coming: comments on microsoft’s tay “experiment,” and wider implications\",\n        \"No free lunch theorems for optimization\",\n        \"Fast is better than free: Revisiting adversarial training\",\n        \"Adversarial weight perturbation helps robust generalization\",\n        \"FairGAN: Fairness-aware Generative Adversarial Networks\",\n        \"To be Robust or to be Fair: Towards Fairness in Adversarial Training\",\n        \"Adversarial attacks and defenses in images, graphs and text: A review\",\n        \"Federated learning for healthcare informatics\",\n        \"Federated machine learning: Concept and applications\",\n        \"Catherine Yeo and Alyssa Chen\",\n        \"Gnnexplainer: Generating explanations for graph neural networks\",\n        \"Building ethics into artificial intelligence\",\n        \"Explainability in Graph Neural Networks: A Taxonomic Survey\",\n        \"Manuel Gomez Rodriguez\",\n        \"Recurrent neural network regularization\",\n        \"Systematic review of privacy-preserving distributed machine learning from federated databases in health care\",\n        \"Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting\",\n        \"Laurent El Ghaoui\",\n        \"Graph Embedding for Recommendation against Attribute Inference Attacks\",\n        \"Adversarial attacks on deep-learning models in natural language processing: A survey\",\n        \"Explainable recommendation: A survey and new perspectives\",\n        \"Identifying significant predictive bias in classifiers\",\n        \"idlg: Improved deep leakage from gradients\",\n        \"Gender bias in contextualized word embeddings\",\n        \"Men also like shopping: Reducing gender bias amplification using corpus-level constraints\",\n        \"Gender bias in coreference resolution: Evaluation and debiasing methods\",\n        \"Deep leakage from gradients\",\n        \"A survey on measuring indirect discrimination in machine learning\",\n        \"AI can be sexist and racist—it’s time to make it fair\",\n        \"Adversarial attacks on graph neural networks via meta learning\",\n        \"Adversarial Attacks on Neural Networks for Graph Data\"\n    ],\n    \"reasoning\": \"Selected these 286 references because they represent foundational work and recent advances in Trustworthy AI: A Computational Perspective, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}