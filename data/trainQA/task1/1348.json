{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 50 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Boosting Data Reduction for the Maximum Weight Independent Set Problem Using Increasing TransformationsPartially supported by DFG grants SA 933/10-2 and by National Research, Development and Innovation Office NKFIH Fund No. SNN-135643.\n\nPaper Abstract: Abstract Given a vertex-weighted graph, the maximum weight independent set problem asks for a pair-wise non-adjacent set of vertices such that the sum of their weights is maximum. The branch-and-reduce paradigm is the de facto standard approach to solve the problem to optimality in practice. In this paradigm, data reduction rules are applied to decrease the problem size. These data reduction rules ensure that given an optimum solution on the new (smaller) input, one can quickly construct an optimum solution on the original input. We introduce new generalized data reduction and transformation rules for the problem. A key feature of our work is that some transformation rules can increase the size of the input. Surprisingly, these so-called increasing transformations can simplify the problem and also open up the reduction space to yield even smaller irreducible graphs later throughout the algorithm. In experiments, our algorithm computes significantly smaller irreducible graphs on all except one instance, solves more instances to optimality than previously possible, is up to two orders of magnitude faster than the best state-of-the-art solver, and finds higher-quality solutions than heuristic solvers DynWVC and HILS on many instances. While the increasing transformations are only efficient enough for preprocessing at this time, we see this as a critical initial step towards a new branch-and-transform paradigm.\n\n{retrieved_text}\n\nGenerate exactly 50 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 50\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"OpenStreetMap\",\n        \"Branch-and-reduce exponential/FPT algorithms in practice: A case study of vertex cover\",\n        \"Struction revisited\",\n        \"Fast local search for the maximum independent set problem\",\n        \"Submap: aligning metabolic pathways with subnetwork mappings\",\n        \"A fast algorithm for the maximum weight clique problem\",\n        \"Finding a maximum clique in an arbitrary graph\",\n        \"Temporal map labeling: A new unified framework with experiments\",\n        \"A new table of constant weight codes\",\n        \"Using critical sets to solve the maximum independent set problem\",\n        \"Local search with edge weighting and configuration checking heuristics for minimum vertex cover\",\n        \"Improving local search for minimum weight vertex cover by dynamic strategies\",\n        \"Computing a near-maximum independent set in linear time by reducing-peeling\",\n        \"Energy-aware scheduling in disk storage systems\",\n        \"Accelerating local search for the maximum independent set problem\",\n        \"Pseudo-boolean functions and stability of graphs\",\n        \"Some Simplified NP-Complete Problems\",\n        \"Evaluation of labeling strategies for rotating maps\",\n        \"Stability in can-free graphs\",\n        \"The struction of a graph: Application to cn-free graphs\",\n        \"Scalable kernelization for maximum independent sets\",\n        \"WeGotYouCovered: The winning solver from the PACE 2019 challenge, vertex cover track\",\n        \"The struction algorithm for the maximum stable set problem revisited\",\n        \"Finding near-optimal independent sets at scale\",\n        \"Exactly solving the maximum weight independent set problem on large real-world graphs\",\n        \"SNAP Datasets: Stanford large network dataset collection\",\n        \"On minimization of the number of branches in branch-and-bound algorithms for the maximum clique problem\",\n        \"Numwvc: A novel local search for minimum weighted vertex cover problem\",\n        \"An efficient local search algorithm for minimum weighted vertex cover on massive graphs\",\n        \"Matching theory\",\n        \"Conic reduction of graphs for the stable set problem\",\n        \"Maximum weight cliques with mutex constraints for video object segmentation\",\n        \"Predicting structural and functional sites in proteins by searching for maximum-weight cliques\",\n        \"A hybrid iterated local search heuristic for the maximum weight independent set problem\",\n        \"New constant weight codes from linear permutation groups\",\n        \"A fast algorithm for the maximum clique problem\",\n        \"Bounds on the number of vertex independent sets in a graph\",\n        \"Fibonacci numbers of graphs\",\n        \"Peaty: An exact solver for the vertex cover problem\",\n        \"A branch and cut solver for the maximum stable set problem\",\n        \"Efficient traversal of mesh edges using adjacency primitives\",\n        \"On the power of simple reductions for the maximum independent set problem\",\n        \"Combining algorithms for vertex cover and clique search\",\n        \"Maximum weight clique instances\",\n        \"Combinatorial branch-and-bound for the maximum weight independent set problem\",\n        \"A branch, price, and cut approach to solving the maximum weighted independent set problem\",\n        \"A branch-and-price approach for the maximum weight independent set problem\",\n        \"Solving the winner determination problem via a weighted maximum clique heuristic\",\n        \"A new solver for the minimum weighted vertex cover problem\",\n        \"Maximum weighted independent set of links under physical interference model\"\n    ],\n    \"reasoning\": \"Selected these 50 references because they represent foundational work and recent advances in Boosting Data Reduction for the Maximum Weight Independent Set Problem Using Increasing TransformationsPartially supported by DFG grants SA 933/10-2 and by National Research, Development and Innovation Office NKFIH Fund No. SNN-135643., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}