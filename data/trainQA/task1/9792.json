{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts\n\nPaper Abstract: Abstract Detecting online sexual predatory behaviours and abusive language on social media platforms has become a critical area of research due to the growing concerns about online safety, especially for vulnerable populations such as children and adolescents. Researchers have been exploring various techniques and approaches to develop effective detection systems that can identify and mitigate these risks. Recent development of large language models (LLMs) has opened a new opportunity to address this problem more effectively. This paper proposes an approach to detection of online sexual predatory chats and abusive language using the open-source pretrained Llama 2 7B-parameter model, recently released by Meta GenAI. We fine-tune the LLM using datasets with different sizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu). Based on the power of LLMs, our approach is generic and automated without a manual search for a synergy between feature extraction and classifier design steps like conventional methods in this domain. Experimental results show a strong performance of the proposed approach, which performs proficiently and consistently across three distinct datasets with five sets of experiments. This studyâ€™s outcomes indicate that the proposed method can be implemented in real-world applications (even with non-English languages) for flagging sexual predators, offensive or toxic content, hate speech, and discriminatory language in online discussions and comments to maintain respectful internet or digital communities. Furthermore, it can be employed for solving text classification problems with other potential applications such as sentiment analysis, spam and phishing detection, sorting legal documents, fake news detection, language identification, user intent recognition, text-based product categorization, medical record analysis, and resume screening.\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"How to take advantage of behavioral features for the early detection of grooming in online conversations\",\n        \"Offensive language detection for low resource language using deep sequence model\",\n        \"Large-scale analysis of grooming in modern social networks\",\n        \"Exposing patterns of adult solicitor behaviour: towards a theory of control within the cybersexual abuse of youth\",\n        \"Detecting offensive language based on graph attention networks and fusion features\",\n        \"Online offensive behaviour in social media: detection approaches, comprehensive review and future directions\",\n        \"Automatic identification of cyber predators using text analytics and machine learning\",\n        \"Detecting online grooming by simple contrastive chat embeddings\",\n        \"Abusive language detection from social media comments using conventional machine learning and deep learning approaches\",\n        \"Automatic detection of offensive language for Urdu and Roman Urdu\",\n        \"Llama 2: Open foundation and fine-tuned chat models\",\n        \"Language models are few-shot learners\",\n        \"GPT-4 technical report\",\n        \"BLOOM: A 176B-parameter open-access multilingual language model\",\n        \"LLaMA: Open and efficient foundation language models\",\n        \"The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only\",\n        \"Attention is all you need\",\n        \"Root mean square layer normalization\",\n        \"GLU variants improve transformer\",\n        \"RoFormer: Enhanced transformer with rotary position embedding\",\n        \"Neural machine translation of rare words with subword units\",\n        \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing\",\n        \"LoRA: Low-rank adaptation of large language models\",\n        \"PEFT: State-of-the-art parameter-efficient fine-tuning methods\",\n        \"Prefix-tuning: Optimizing continuous prompts for generation\",\n        \"The power of scale for parameter-efficient prompt tuning\",\n        \"Compacter: Efficient low-rank hypercomplex adapter layers\",\n        \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning\",\n        \"BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models\",\n        \"SparseAdapter: An easy approach for improving the parameter-efficiency of adapters\",\n        \"Llama-Adapter: Efficient fine-tuning of language models with zero-init attention\",\n        \"Adaptive budget allocation for parameter-efficient fine-tuning\",\n        \"Scaling down to scale up: A guide to parameter-efficient fine-tuning\",\n        \"Parameter-efficient fine-tuning of large-scale pre-trained language models\",\n        \"Intrinsic dimensionality explains the effectiveness of language model fine-tuning\",\n        \"PAN12 Deception Detection: Sexual Predator Identification\",\n        \"Overview of the international sexual predator identification competition at PAN-2012\",\n        \"On preprocessing the data for improving sexual predator detection: Anonymous for review\",\n        \"Detecting sexual predatory chats by perturbed data and balanced ensembles\",\n        \"Decoupled weight decay regularization\",\n        \"A two-step approach for effective detection of misbehaving users in chats.\",\n        \"Ensemble method for sexual predators identification in online chats\",\n        \"On the impact of sentiment and emotion based features in detecting online sexual predators\",\n        \"Predatory conversation detection\",\n        \"Detection of cyber grooming in online conversation\",\n        \"Detecting predatory conversations in social media by deep convolutional neural networks\",\n        \"Recognizing predatory chat documents using semi-supervised anomaly detection\",\n        \"GQA: Training generalized multi-query transformer models from multi-head checkpoints\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}