{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 59 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Generating Structured Adversarial Attacks Using Frank-Wolfe Method\n\nPaper Abstract: Abstract White box adversarial perturbations are generated via iterative optimization algorithms most often by minimizing an adversarial loss on a ‚Ñìpsubscript‚Ñìùëù\\ell_{p} neighborhood of the original image, the so-called distortion set. Constraining the adversarial search with different norms results in disparately structured adversarial examples. Here we explore several distortion sets with structure-enhancing algorithms. These new structures for adversarial examples might provide challenges for provable and empirical robust mechanisms. Because adversarial robustness is still an empirical field, defense mechanisms should also reasonably be evaluated against differently structured attacks. Besides, these structured adversarial perturbations may allow for larger distortions size than their ‚Ñìpsubscript‚Ñìùëù\\ell_{p} counter-part while remaining imperceptible or perceptible as natural distortions of the image. We will demonstrate in this work that the proposed structured adversarial examples can significantly bring down the classification accuracy of adversarialy trained classifiers while showing low ‚Ñì2subscript‚Ñì2\\ell_{2} distortion rate. For instance, on ImagNet dataset the structured attacks drop the accuracy of adversarial model to near zero with only 50% of ‚Ñì2subscript‚Ñì2\\ell_{2} distortion generated using white-box attacks like PGD. As a byproduct, our finding on structured adversarial examples can be used for adversarial regularization of models to make models more robust or improve their generalization performance on datasets which are structurally different.\n\n{retrieved_text}\n\nGenerate exactly 59 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 59\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Linear convergence of a frank-wolfe type algorithm over trace-norm balls\",\n        \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples\",\n        \"Exact matrix completion via convex optimization\",\n        \"On evaluating adversarial robustness\",\n        \"Towards evaluating the robustness of neural networks\",\n        \"A frank-wolfe framework for efficient and effective adversarial attacks\",\n        \"Projection free rank-drop steps\",\n        \"Certified adversarial robustness via randomized smoothing\",\n        \"Sparse and imperceivable adversarial attacks\",\n        \"Approximate methods in optimization problems\",\n        \"Lifted coordinate descent for learning with trace-norm regularization\",\n        \"Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals\",\n        \"A rotation and a translation suffice: Fooling cnns with simple transformations\",\n        \"A rank minimization heuristic with application to minimum order system approximation\",\n        \"An algorithm for quadratic programming\",\n        \"An extended frank‚Äìwolfe method with ‚Äúin-face‚Äù directions, and its application to low-rank matrix completion\",\n        \"A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization\",\n        \"Playing non-linear games with linear oracles\",\n        \"Faster rates for the frank-wolfe method over strongly-convex sets\",\n        \"Fast generalized conditional gradient method with applications to matrix recovery problems\",\n        \"Controlling perceptual factors in neural style transfer\",\n        \"Motivating the rules of the game for adversarial example research\",\n        \"Explaining and harnessing adversarial examples\",\n        \"Some comments on Wolfe‚Äôs ‚Äòaway step‚Äô\",\n        \"Low frequency adversarial perturbation\",\n        \"Large-scale image classification with trace-norm regularization\",\n        \"Revisiting frank-wolfe: Projection-free sparse convex optimization\",\n        \"A simple algorithm for nuclear norm regularized problems\",\n        \"Frank-wolfe on uniformly convex sets\",\n        \"Frank-wolfe with subsampling oracle\",\n        \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n        \"Adversarial examples in the physical world\",\n        \"An affine invariant linear convergence analysis for frank-wolfe algorithms\",\n        \"On the global linear convergence of Frank‚ÄìWolfe optimization variants\",\n        \"On the effect of low-rank weights on adversarial robustness of neural networks\",\n        \"Practical large-scale optimization for max-norm regularization\",\n        \"Constrained minimization methods\",\n        \"Beyond pixel norm-balls: Parametric adversaries using an analytically differentiable renderer\",\n        \"Decoder network over lightweight reconstructed feature for fast semantic style transfer\",\n        \"Towards imperceptible and robust adversarial example attacks against neural networks\",\n        \"Towards deep learning models resistant to adversarial attacks\",\n        \"Certified defenses against adversarial examples\",\n        \"Foolbox: A python toolbox to benchmark the robustness of machine learning models\",\n        \"Learning what and where to draw\",\n        \"Stable and controllable neural texture synthesis and style transfer using histogram losses\",\n        \"Adversarially robust generalization requires more data\",\n        \"Should adversarial attacks use pixel p-norm?\",\n        \"Large-scale convex minimization with a low-rank constraint\",\n        \"On the suitability of lp-norms for creating and preventing adversarial examples\",\n        \"Disentangling adversarial robustness and generalization\",\n        \"Convex tensor decomposition via structured schatten norm regularization\",\n        \"Provable defenses against adversarial examples via the convex outer adversarial polytope\",\n        \"Learning perturbation sets for robust machine learning\",\n        \"Wasserstein adversarial examples via projected sinkhorn iterations\",\n        \"Structured adversarial attack: Towards general implementation and better interpretability\",\n        \"Subspace attack: Exploiting promising subspaces for query-efficient black-box attacks\",\n        \"Randomized smoothing of all shapes and sizes\",\n        \"Me-net: Towards effective adversarial robustness with matrix estimation\",\n        \"Towards stable and efficient training of verifiably robust neural networks\"\n    ],\n    \"reasoning\": \"Selected these 59 references because they represent foundational work and recent advances in Generating Structured Adversarial Attacks Using Frank-Wolfe Method, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}