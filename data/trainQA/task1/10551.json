{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 609 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey\n\nPaper Abstract: Abstract Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community’s exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.\n\n{retrieved_text}\n\nGenerate exactly 609 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 609\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27\",\n        \"Generations of knowledge graphs: The crazy ideas and the business impact\",\n        \"Head-to-tail: How knowledgeable are large language models (llm)? A.K.A. will llms replace knowledge graphs?\",\n        \"Large language models and knowledge graphs: Opportunities and challenges\",\n        \"Unifying large language models and knowledge graphs: A roadmap\",\n        \"Knowledge graphs\",\n        \"Multi-modal knowledge graph construction and application: A survey\",\n        \"A survey on visual transfer learning using knowledge graphs\",\n        \"A survey on knowledge-enhanced multimodal learning\",\n        \"What is a multi-modal knowledge graph: A survey\",\n        \"Yago: a core of semantic knowledge\",\n        \"Dbpedia: A nucleus for a web of open data\",\n        \"Freebase: a collaboratively created graph database for structuring human knowledge\",\n        \"Ontologies and the semantic web\",\n        \"WordNet: A lexical database for english\",\n        \"Conceptnet 5.5: An open multilingual graph of general knowledge\",\n        \"Zero-shot and few-shot learning with knowledge graphs: A comprehensive survey\",\n        \"The development of embodied cognition: Six lessons from babies\",\n        \"A survey on multi-view learning\",\n        \"Learning robust representations via multi-view information bottleneck\",\n        \"What makes multi-modal learning better than single (provably)\",\n        \"Deep multimodal fusion by channel exchanging\",\n        \"Graph-based visual-semantic entanglement network for zero-shot image recognition\",\n        \"Knowledge-based fine-grained classification for few-shot learning\",\n        \"Ontozsl: Ontology-enhanced zero-shot learning\",\n        \"Bringing back the context: Camera trap species identification as link prediction on multimodal knowledge graphs\",\n        \"FVQA: fact-based visual question answering\",\n        \"OK-VQA: A visual question answering benchmark requiring external knowledge\",\n        \"Mucko: Multi-layer cross-modal knowledge reasoning for fact-based visual question answering\",\n        \"KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA\",\n        \"Zero-shot visual question answering using knowledge graph\",\n        \"Lako: Knowledge-driven visual question answering via late knowledge-to-text injection\",\n        \"Webchild: harvesting and organizing commonsense knowledge from the web\",\n        \"Webchild 2.0 : Fine-grained commonsense knowledge distillation\",\n        \"Know more say less: Image captioning based on scene graphs\",\n        \"Tele-knowledge pre-training for fault analysis\",\n        \"The wikipedia xml corpus\",\n        \"KEPLER: A unified model for knowledge embedding and pre-trained language representation\",\n        \"MMEKG: multi-modal event knowledge graph towards universal representation across modalities\",\n        \"GAIA: A fine-grained multimedia knowledge extraction system\",\n        \"NEIL: extracting visual knowledge from web data\",\n        \"Answering visual-relational queries in web-extracted knowledge graphs\",\n        \"Visualsem: a high-quality knowledge graph for vision and language\",\n        \"Aspectmmkg: A multi-modal knowledge graph with aspect-aware entities\",\n        \"M2conceptbase: A fine-grained aligned multi-modal conceptual knowledge base\",\n        \"Scene-driven multimodal knowledge graph construction for embodied ai\",\n        \"Imagenet large scale visual recognition challenge\",\n        \"Visual genome: Connecting language and vision using crowdsourced dense image annotations\",\n        \"YFCC100M: the new data in multimedia research\",\n        \"Image-embodied knowledge representation learning\",\n        \"“Robust imagegraph: Rank-level feature fusion for image search\",\n        \"“Imgpedia: A linked dataset with content-based analysis of wikimedia images\",\n        \"“MMKG: multi-modal knowledge graphs\",\n        \"“Visual pivoting for (unsupervised) entity alignment\",\n        \"“Cross-lingual entity alignment via joint attribute-preserving embedding\",\n        \"“Learning to exploit long-term relational dependencies in knowledge graphs\",\n        \"“Richpedia: A large-scale\",\n        \"C. Callison-Burch\",\n        \"“Relation-enhanced negative sampling for multimodal knowledge graph completion\",\n        \"“A benchmarking study of embedding-based entity alignment for knowledge graphs\",\n        \"“Multimodal analogical reasoning over knowledge graphs\",\n        \"deduction and alignment: An empirical study on multi-modal knowledge graph alignment\",\n        \"“Uknow: A unified knowledge protocol for common-sense reasoning and vision-language pre-training\",\n        \"“Rethinking uncertainly missing and ambiguous visual modality in multi-modal entity alignment\",\n        \"“TIVA-KG: A multimodal knowledge graph with text\",\n        \"“Mmpedia: A large-scale multi-modal knowledge graph\",\n        \"“VISTA: visual-textual knowledge graph representation learning\",\n        \"“Weakly-supervised learning of visual relations\",\n        \"“Visual relationship detection with language priors\",\n        \"“Learning to detect human-object interactions\",\n        \"“Viske: Visual knowledge extraction and question answering by visual verification of relation phrases\",\n        \"“Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark\",\n        \"“Imagenet: A large-scale hierarchical image database\",\n        \"“Enriching visual knowledge bases via object discovery and segmentation\",\n        \"“Image retrieval using scene graphs\",\n        \"“Stating the obvious: Extracting visual common sense knowledge\",\n        \"“Extracting visual knowledge from the web with multimodal learning\",\n        \"“Translating embeddings for modeling multi-relational data\",\n        \"“Babelnet: The automatic construction\",\n        \"“Wikidata: a free collaborative knowledgebase\",\n        \"“Towards using semantic-web technologies for multi-modal knowledge graph construction\",\n        \"“Effectively filtering images for better multi-modal knowledge graph\",\n        \"“Visualizable or non-visualizable? exploring the visualizability of concepts in multi-modal knowledge graph\",\n        \"“Multi-source semantic graph-based multimodal sarcasm explanation generation\",\n        \"“Common sense knowledge infusion for visual understanding and reasoning: Approaches\",\n        \"“VQA: visual question answering\",\n        \"“From image to language: A critical analysis of visual question answering (VQA) approaches\",\n        \"“Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n        \"“Flamingo: a visual language model for few-shot learning\",\n        \"“BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models\",\n        \"“Ask me anything: Free-form visual question answering based on knowledge from external sources\",\n        \"“KVQA: knowledge-aware visual question answering\",\n        \"“Cross-modal knowledge reasoning for knowledge-based visual question answering\",\n        \"“Conceptbert: Concept-aware representation for visual question answering\",\n        \"“REVIVE: regional visual representation matters in knowledge-based visual question answering\",\n        \"“Explicit knowledge-based reasoning for visual question answering\",\n        \"“Straight to the facts: Learning knowledge base retrieval for factual visual question answering\",\n        \"“Incorporating external knowledge to boost machine comprehension based question answering\",\n        \"“Out of the box: Reasoning with graph convolution nets for factual visual question answering\",\n        \"“Towards knowledge-augmented visual question answering\",\n        \"“Boosting visual question answering with context-aware knowledge aggregation\",\n        \"“Seeing is knowing! fact-based visual question answering using knowledge graph embeddings\",\n        \"“Rich visual knowledge-based augmentation network for visual question answering\",\n        \"“Knowledge base graph embedding module design for visual question answering model\",\n        \"“In factuality: Efficient integration of relevant facts for visual question answering\",\n        \"“Dynamic key-value memory enhanced multi-step graph reasoning for knowledge-based visual question answering\",\n        \"“Can open domain question answering systems answer visual knowledge questions?” CoRR\",\n        \"“Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering\",\n        \"“Multimodal knowledge reasoning for enhanced visual question answering\",\n        \"“Semantic-aware modular capsule routing for visual question answering\",\n        \"“Answering knowledge-based visual questions via the exploration of question purpose\",\n        \"“VLC-BERT: visual question answering with contextualized commonsense knowledge\",\n        \"“A retriever-reader framework with visual entity linking for knowledge-based visual question answering\",\n        \"“Mm-reasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering\",\n        \"“Multi-clue reasoning with memory augmentation for knowledge-based visual question answering\",\n        \"“Modality-aware integration with large language models for knowledge-based visual question answering\",\n        \"“Retrieval augmented visual question answering with outside knowledge\",\n        \"“Multimodal graph transformer for multimodal question answering\",\n        \"“Learning visual knowledge memory networks for visual question answering\",\n        \"O. L. de Lacalle\",\n        \"“KAT: A knowledge augmented transformer for vision-and-language\",\n        \"“Entity-focused dense passage retrieval for outside-knowledge visual question answering\",\n        \"“Prompting vision language model with knowledge from large language model for knowledge-based VQA\",\n        \"“Combo of thinking and observing for outside-knowledge VQA\",\n        \"Natural Language Processing with Python. O’Reilly\",\n        \"“Allennlp: A deep semantic natural language processing platform\",\n        \"“Stanza: A python natural language processing toolkit for many human languages\",\n        \"“A fast and accurate dependency parser using neural networks\",\n        \"“Incorporating non-local information into information extraction systems by gibbs sampling\",\n        \"“Multimodal graph networks for compositional generalization in visual question answering\",\n        \"“Knowledge-routed visual question reasoning: Challenges for deep representation embedding\",\n        \"“Multi-modal answer validation for knowledge-based VQA\",\n        \"search: A new benchmark for knowledge-augmented visual question answering\",\n        \"“Reasoning over vision and language: Exploring the benefits of supplemental knowledge\",\n        \"“Explicit knowledge integration for knowledge-aware visual question answering about named entities\",\n        \"“Hypergraph transformer: Weakly-supervised multi-hop reasoning for knowledge-based visual question answering\",\n        \"a dataset for knowledge-based visual question answering about named entities\",\n        \"“12-in-1: Multi-task vision and language representation learning\",\n        \"“Weakly-supervised visual-retriever-reader for knowledge-based question answering\",\n        \"“Dense passage retrieval for open-domain question answering\",\n        \"“Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering\",\n        \"“Multimodal prompt retrieval for generative visual question answering\",\n        \"“Fact-based visual question answering via dual-process system\",\n        \"“Billion-scale similarity search with gpus\",\n        \"“Knowledge is power: Hierarchical-knowledge embedded meta-learning for visual reasoning in artistic domains\",\n        \"“Res2net: A new multi-scale backbone architecture\",\n        \"“Resolving zero-shot and fact-based visual question answering via enhanced fact retrieval\",\n        \"“AI-VQA: visual question answering based on agent interaction with interpretability\",\n        \"“LXMERT: learning cross-modality encoder representations from transformers\",\n        \"“A unified end-to-end retriever-reader framework for knowledge-based VQA\",\n        \"“A symmetric dual encoding dense retrieval framework for knowledge-intensive visual question answering\",\n        \"“Passage retrieval for outside-knowledge visual question answering\",\n        \"“Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory\",\n        \"“WIT: wikipedia-based image text dataset for multimodal multilingual machine learning\",\n        \"“Perceiver: General perception with iterative attention\",\n        \"“Language models as knowledge bases?” in EMNLP/IJCNLP (1). Association for Computational Linguistics\",\n        \"“(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs\",\n        \"“Filling the image information gap for VQA: prompting large language models to proactively ask questions\",\n        \"“An empirical study of GPT-3 for few-shot knowledge-based VQA\",\n        \"“Prompting large language models with answer heuristics for knowledge-based visual question answering\",\n        \"“Modular visual question answering via code generation\",\n        \"“Glove: Global vectors for word representation\",\n        \"“Adapting language models to compress contexts\",\n        \"“End-to-end case-based reasoning for commonsense knowledge base completion\",\n        \"“Commonsense knowledge base completion with structural and semantic context\",\n        \"“Pytorch-biggraph: A large scale graph embedding system\",\n        \"“Improving and diagnosing knowledge-based visual question answering via entity enhanced knowledge injection\",\n        \"“Promptcap: Prompt-guided task-aware image captioning\",\n        \"“A simple baseline for knowledge-based visual question answering\",\n        \"“Sentence-bert: Sentence embeddings using siamese bert-networks\",\n        \"“MUTAN: multimodal tucker fusion for visual question answering\",\n        \"“Multi-graph attention networks with bilinear convolution for diagnosis of schizophrenia\",\n        \"“Stacked attention networks for image question answering\",\n        \"“BERT: pre-training of deep bidirectional transformers for language understanding\",\n        \"“ALBERT: A lite BERT for self-supervised learning of language representations\",\n        \"“VQA-GNN: reasoning with multimodal semantic graph for visual question answering\",\n        \"“Modeling relational data with graph convolutional networks\",\n        \"“Visualbert: A simple and performant baseline for vision and language\",\n        \"“Ask me anything: Dynamic memory networks for natural language processing\",\n        \"“Attention is all you need\",\n        \"“An empirical study of training end-to-end vision-and-language transformers\",\n        \"“LAMM: language-assisted multi-modal instruction-tuning dataset\",\n        \"“UNITER: universal image-text representation learning\",\n        \"“Vilt: Vision-and-language transformer without convolution or region supervision\",\n        \"“VL-BERT: pre-training of generic visual-linguistic representations\",\n        \"“Unifying vision-and-language tasks via text generation\",\n        \"“Frozen transformers in language models are effective visual encoder layers\",\n        \"“Llama: Open and efficient foundation language models\",\n        \"“Xgboost: A scalable tree boosting system\",\n        \"“Leveraging passage retrieval with generative models for open domain question answering\",\n        \"“Large language models are visual reasoning coordinators\",\n        \"“Roberta: A robustly optimized BERT pretraining approach\",\n        \"“Are emergent abilities of large language models a mirage?” CoRR\",\n        \"“Language guided visual question answering: Elevate your multimodal language model using knowledge-enriched prompts\",\n        \"“Language models are few-shot learners\",\n        \"“Exploring the limits of transfer learning with a unified text-to-text transformer\",\n        \"“Analyzing modular approaches for visual question decomposition\",\n        \"H. P. de Oliveira Pinto\",\n        \"“Chain-of-thought prompting elicits reasoning in large language models\",\n        \"“Verb semantics and lexical selection\",\n        \"“Bleu: a method for automatic evaluation of machine translation\",\n        \"“Cider: Consensus-based image description evaluation\",\n        \"“Evaluating open-domain question answering in the era of large language models\",\n        \"“Training language models to follow instructions with human feedback\",\n        \"“Do dogs have whiskers? A new knowledge base of haspart relations\",\n        \"“CSKG: the commonsense knowledge graph\",\n        \"“Roget’s 21st century thesaurus in dictionary form: the essential reference for home\",\n        \"“The berkeley framenet project\",\n        \"“ATOMIC: an atlas of machine commonsense for if-then reasoning\",\n        \"“Advanced semantics for commonsense knowledge extraction\",\n        \"“A comprehensive evaluation of GPT-4V on knowledge-intensive visual question answering\",\n        \"“R-VQA: learning visual relation facts with semantic attention for visual question answering\",\n        \"“FVQA 2.0: Introducing adversarial samples into fact-based visual question answering\",\n        \"“A-OKVQA: A benchmark for visual question answering using world knowledge\",\n        \"“Outside knowledge visual question answering version 2.0\",\n        \"“Open-set knowledge-based visual question answering with inference paths\",\n        \"“Incorporating external knowledge to answer open-domain visual questions with dynamic memory networks\",\n        \"“Visual7w: Grounded question answering in images\",\n        \"“From recognition to cognition: Visual commonsense reasoning\",\n        \"“Improving commonsense in vision-language models via knowledge graph riddles\",\n        \"“Lora: A logical reasoning augmented dataset for visual question answering\",\n        \"“Learn to explain: Multimodal reasoning via thought chains for science question answering\",\n        \"“Knowit VQA: answering knowledge-based questions about videos\",\n        \"“Knowledge-based embodied question answering\",\n        \"“Knowledge-based visual question generation\",\n        \"“Deconfounded visual question generation with causal inference\",\n        \"“Pre-training multi-modal dense retrievers for outside-knowledge visual question answering\",\n        \"“Generating natural questions about an image\",\n        \"“K-VQG: knowledge-aware visual question generation for common-sense acquisition\",\n        \"“Multimodal dialog systems with dual knowledge-enhanced generative pretrained language model\",\n        \"“Iterative context-aware graph inference for visual dialog\",\n        \"“KBGN: knowledge-bridge graph network for adaptive vision-text reasoning in visual dialogue\",\n        \"“Reasoning visual dialog with sparse graph learning and knowledge transfer\",\n        \"“Unified multimodal model with unlikelihood training for visual dialog\",\n        \"“Heterogeneous knowledge network for visual dialog\",\n        \"“Context-aware graph inference with knowledge distillation for visual dialog\",\n        \"“Skanet: Structured knowledge-aware network for visual dialog\",\n        \"“Knowledge-aware causal inference network for visual dialog\",\n        \"“Reasoning with multi-structure commonsense knowledge in visual dialog\",\n        \"“Counterfactual visual dialog: Robust commonsense knowledge learning from unbiased training\",\n        \"“Knowledge-aware zero-shot learning: Survey and perspective\",\n        \"“Devise: A deep visual-semantic embedding model\",\n        \"“A generative adversarial approach for zero-shot learning from noisy texts\",\n        \"“Feature generating networks for zero-shot learning\",\n        \"“Attribute-based classification for zero-shot visual object categorization\",\n        \"“Benchmarking knowledge-driven zero-shot learning\",\n        \"“Zero-shot recognition via semantic embeddings and knowledge graphs\",\n        \"“Rethinking knowledge graph propagation for zero-shot learning\",\n        \"“Improving zero-shot learning baselines with commonsense knowledge\",\n        \"“Zero-shot learning with common sense knowledge graphs\",\n        \"“I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs\",\n        \"“Disentangled ontology embedding for zero-shot learning\",\n        \"“Ontology-guided semantic composition for zero-shot learning\",\n        \"“Zero-shot image tagging by hierarchical semantic embedding\",\n        \"“Label-embedding for image classification\",\n        \"“Label-embedding for attribute-based classification\",\n        \"“DUET: cross-modal semantic grounding for contrastive zero-shot learning\",\n        \"“TGG: transferable graph generation for zero-shot and few-shot learning\",\n        \"“A zero-shot learning method with a multi-modal knowledge graph\",\n        \"“Explainable zero-shot learning via attentive graph convolutional network and knowledge graphs\",\n        \"“Zero-shot ingredient recognition by multi-relational graph convolutional network\",\n        \"“Zero-shot learning via contrastive learning on dual knowledge graphs\",\n        \"“Recognizing unseen objects via multimodal intensive knowledge graph propagation\",\n        \"“Multi-label zero-shot learning with structured knowledge graphs\",\n        \"“Zero-shot learning - A comprehensive evaluation of the good\",\n        \"“NUS-WIDE: a real-world web image database from national university of singapore\",\n        \"“Fake news detection via knowledge-driven multimodal graph convolutional networks\",\n        \"“Probase: a probabilistic taxonomy for text understanding\",\n        \"“Yolov3: An incremental improvement\",\n        \"“Knowledge-aware multi-modal adaptive graph convolutional networks for fake news detection\",\n        \"“Inconsistent matters: A knowledge-guided dual-consistency network for multi-modal rumor detection\",\n        \"“Emoknow: Emotion- and knowledge-oriented model for COVID-19 fake news detection\",\n        \"“Condensed movies: Story based retrieval with contextual embeddings\",\n        \"“Movienet: A holistic dataset for movie understanding\",\n        \"“Incorporating domain knowledge graph into multimodal movie genre classification with self-supervised attention and contrastive learning\",\n        \"“A comprehensive survey of deep learning for image captioning\",\n        \"“From images to sentences through scene description graphs using commonsense reasoning and knowledge\",\n        \"“Entity-aware image caption generation\",\n        \"“Image captioning with internal and external knowledge\",\n        \"“Relational reasoning using prior knowledge for visual captioning\",\n        \"“Joint commonsense and relation reasoning for image and video captioning\",\n        \"“Integrating image captioning with rule-based entity masking\",\n        \"“Image captioning with transformer and knowledge graph\",\n        \"“Image caption generation based on object detection and knowledge enhancement\",\n        \"“Improving image captioning by leveraging knowledge graphs\",\n        \"“When radiology report generation meets knowledge graph\",\n        \"“Boosting entity-aware image captioning with multi-modal knowledge graph\",\n        \"“Graph attention networks\",\n        \"“Generating image captions with external encyclopedic knowledge\",\n        \"“Expressive scene graph generation using commonsense knowledge infusion for visual understanding and reasoning\",\n        \"“Knowledge-enriched visual storytelling\",\n        \"“Knowledgeable storyteller: A commonsense-driven generative model for visual storytelling\",\n        \"“Commonsense knowledge aware concept selection for diverse and informative visual storytelling\",\n        \"M. Ghazvininejad\",\n        \"“Plot and rework: Modeling storylines for visual storytelling\",\n        \"reason and write: Visual storytelling with graph knowledge and relational reasoning\",\n        \"“Knowledge-enriched attention network with group-wise semantic for visual storytelling\",\n        \"imagine and create: Text-to-image generation from prior knowledge\",\n        \"“Rifegan: Rich feature generation for text-to-image synthesis from prior knowledge\",\n        \"“Rifegan2: Rich feature generation for text-to-image synthesis from constrained prior knowledge\",\n        \"“Prior knowledge guided text to image generation\",\n        \"“Knhigan: Knowledge-enhanced hierarchical generative adversarial network for fine-grained text-to-image synthesis\",\n        \"“Enriching attributes from knowledge graph for fine-grained text-to-image synthesis\",\n        \"“T2TD: text-3d generation model based on prior knowledge guidance\",\n        \"“Improved techniques for training gans\",\n        \"“Attngan: Fine-grained text to image generation with attentional generative adversarial networks\",\n        \"“Structure-clip: Enhance multi-modal language representations with structure knowledge\",\n        \"“A comprehensive survey of scene graphs: Generation and application\",\n        \"“Graphhopper: Multi-hop scene graph reasoning for visual question answering\",\n        \"“Ernie-vil: Knowledge enhanced vision-language representations through scene graphs\",\n        \"“Learning from the scene and borrowing from the rich: Tackling the long tail in scene graph generation\",\n        \"“Neural motifs: Scene graph parsing with global context\",\n        \"“Representation learning for scene graph completion via jointly structural and visual embedding\",\n        \"“Zero-shot scene graph generation with knowledge graph completion\",\n        \"“Learning visual commonsense for robust scene graph generation\",\n        \"“KG-BERT: BERT for knowledge graph completion\",\n        \"“Knowledge-embedded routing network for scene graph generation\",\n        \"“Scene graph generation with external knowledge and image reconstruction\",\n        \"“Bridging knowledge graphs to generate scene graphs\",\n        \"less bias: Unbiasing scene graph generation with explicit ontological adjustment\",\n        \"“Prior knowledge-driven dynamic scene graph generation with causal inference\",\n        \"“Towards confidence-aware commonsense knowledge integration for scene graph generation\",\n        \"read and enrich - learning from scientific figures and their captions\",\n        \"“Image-text retrieval: A survey on recent research and development\",\n        \"“From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\n        \"“Microsoft COCO: common objects in context\",\n        \"“Vinvl: Revisiting visual representations in vision-language models\",\n        \"“BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation\",\n        \"“DRSGN: dual revised semantic graph structured network for image-text matching\",\n        \"“Knowledge aware semantic concept expansion for image-text matching\",\n        \"“Consensus-aware visual-semantic embedding for image-text matching\",\n        \"“CODER: coupled diversity-sensitive momentum contrastive learning for image-text retrieval\",\n        \"“Commonsense-guided semantic and relational consistencies for image-text retrieval\",\n        \"“External knowledge dynamic modeling for image-text retrieval\",\n        \"“SPICE: semantic propositional image caption evaluation\",\n        \"“MACK: multimodal aligned conceptual knowledge for unpaired image-text matching\",\n        \"“MKVSE: multimodal knowledge enhanced visual-semantic embedding for image-text retrieval\",\n        \"“Geometric deep learning on graphs and manifolds using mixture model cnns\",\n        \"“Entity-graph enhanced cross-modal pretraining for instance-level product retrieval\",\n        \"“AGREE: aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models\",\n        \"“Referring expression comprehension: A survey of methods and datasets\",\n        \"“Cross-modal relationship inference for grounding referring expressions\",\n        \"“Relationship-embedded representation learning for grounding referring expressions\",\n        \"“Context disentangling and prototype inheriting for robust visual grounding\",\n        \"“Knowledge aided consistency for weakly supervised phrase grounding\",\n        \"“Improving zero-shot phrase grounding via reasoning on external knowledge and spatial relations\",\n        \"“Give me something to eat: Referring expression comprehension with commonsense knowledge\",\n        \"H. Yannakoudakis\",\n        \"“Segment-level and category-oriented network for knowledge-based referring expression comprehension\",\n        \"“Advancing visual grounding with scene knowledge: Benchmark and method\",\n        \"“Product knowledge graph embedding for e-commerce\",\n        \"“KM-BART: knowledge enhanced multimodal BART for visual commonsense generation\",\n        \"“Comet: Commonsense transformers for automatic knowledge graph construction\",\n        \"“ROSITA: enhancing vision-and-language semantic alignments via cross- and intra-modal knowledge integration\",\n        \"reason and learn: Enhancing medical vision-and-language pre-training with knowledge\",\n        \"“The unified medical language system (UMLS): integrating biomedical terminology\",\n        \"“Align before fuse: Vision and language representation learning with momentum distillation\",\n        \"“ERNIE: enhanced representation through knowledge integration\",\n        \"“Structure pretraining and prompt tuning for knowledge graph transfer\",\n        \"“Rotate: Knowledge graph embedding by relational rotation in complex space\",\n        \"“Observed versus latent features for knowledge base and text inference\",\n        \"“Codex: A comprehensive knowledge graph completion benchmark\",\n        \"“Visual instruction tuning\",\n        \"“Multimodal representation with embedded visual guiding objects for named entity recognition in social media posts\",\n        \"“Rpbert: A text-image relation propagation-based BERT model for multimodal NER\",\n        \"“Multimodal relation extraction with efficient graph alignment\",\n        \"“Multimodal named entity recognition with image attributes and image knowledge\",\n        \"“RIVA: A pre-trained tweet multimodal model based on text-image relation for multimodal NER\",\n        \"“Improving multimodal named entity recognition via entity span detection with unified multimodal transformer\",\n        \"“Multi-modal graph fusion for named entity recognition with targeted visual guidance\",\n        \"“Multimodal named entity recognition with bottleneck fusion and contrastive learning\",\n        \"“Multi-granularity cross-modal representation learning for named entity recognition on social media\",\n        \"“Uamner: uncertainty-aware multimodal named entity recognition in social media posts\",\n        \"“MAF: A general matching and alignment framework for multimodal named entity recognition\",\n        \"“A span-based multimodal variational autoencoder for semi-supervised multimodal named entity recognition\",\n        \"“Good visual guidance make A better extractor: Hierarchical visual prefix for multimodal entity and relation extraction\",\n        \"“Flat multi-modal interaction transformer for named entity recognition\",\n        \"“Query prior matters: A MRC framework for multimodal named entity recognition\",\n        \"“Entity-level interaction via heterogeneous graph for multimodal named entity recognition\",\n        \"“‘what’and ‘where’both matter: dual cross-modal graph convolutional networks for multimodal named entity recognition\",\n        \"“Learning from different text-image pairs: A relation-enhanced graph convolutional network for multimodal NER\",\n        \"“MPMRC-MNER: A unified MRC framework for multimodal named entity recognition based multimodal prompt\",\n        \"“Text-image scene graph fusion for multi-modal named entity recognition\",\n        \"“MNER-QG: an end-to-end MRC framework for multimodal named entity recognition with query grounding\",\n        \"“Reducing the bias of visual objects in multimodal named entity recognition\",\n        \"“Hybrid transformer with multi-level fusion for multimodal knowledge graph completion\",\n        \"“Dynamic graph construction framework for multimodal named entity recognition in social media\",\n        \"“Enhancing multimodal entity and relation extraction with variational information bottleneck\",\n        \"“ITA: image-text alignments for multi-modal named entity recognition\",\n        \"“Promptmner: Prompt-based entity-related visual clue extraction and integration for multimodal named entity recognition\",\n        \"“CAT-MNER: multimodal named entity recognition with knowledge-refined cross-modal attention\",\n        \"“Named entity and relation extraction with multi-modal retrieval\",\n        \"“MGICL: multi-grained interaction contrastive learning for multimodal named entity recognition\",\n        \"“Prompting chatgpt in MNER: enhanced multimodal named entity recognition with auxiliary refined knowledge\",\n        \"“Prompt me up: Unleashing the power of alignments for multimodal entity and relation extraction\",\n        \"“A novel framework for multimodal named entity recognition with multi-level alignments\",\n        \"“MNRE: A challenge multimodal dataset for neural relation extraction with visual evidence in social media posts\",\n        \"“On analyzing the role of image for visual-enhanced relation extraction (student abstract)\",\n        \"“Information screening whilst exploiting! multimodal relation extraction with feature denoising and multimodal topic modeling\",\n        \"“Dual-gated fusion with prefix-tuning for multi-modal relation extraction\",\n        \"“Multimodal relation extraction with cross-modal retrieval and synthesis\",\n        \"“Towards zero-shot relation extraction in web mining: A multimodal approach with relative XML path\",\n        \"“Cross-media structured common space for multimedia event extraction\",\n        \"“Clip-event: Connecting text and images with event structures\",\n        \"“Multimedia event extraction from news with a unified contrastive learning framework\",\n        \"“Training multimedia event extraction with generated images and captions\",\n        \"“Joint multimedia event extraction from video and article\",\n        \"“Cross-modal contrastive learning for event extraction\",\n        \"“Three stream based multi-level event contrastive learning for text-video event extraction\",\n        \"“Few-shot domain-adaptative visually-fused event detection from text\",\n        \"“MMEA: entity alignment for multi-modal knowledge graph\",\n        \"“Multi-modal entity alignment in hyperbolic space\",\n        \"“Multi-modal siamese network for entity alignment\",\n        \"“Multi-modal contrastive representation learning for entity alignment\",\n        \"“Multijaf: Multi-modal joint entity alignment framework for multi-modal knowledge graph\",\n        \"“Attribute-consistent knowledge graph representation learning for multi-modal entity alignment\",\n        \"“Neural entity alignment with cross-modal supervision\",\n        \"“Enhanced entity interaction modeling for multi-modal entity alignment\",\n        \"“Multi-modal knowledge graph transformer framework for multi-modal entity alignment\",\n        \"“Universal multi-modal entity alignment via iteratively fusing modality similarity paths\",\n        \"“Cross-modal graph attention network for entity alignment\",\n        \"“DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment\",\n        \"“Probing the impacts of visual context in multimodal entity alignment\",\n        \"“Meaformer: Multi-modal entity alignment transformer for meta modality hybrid\",\n        \"“Revisit and outstrip entity alignment: A perspective of generative models\",\n        \"“PSNEA: pseudo-siamese network for entity alignment between multi-modal knowledge graphs\",\n        \"“Multimodal named entity disambiguation for noisy social media posts\",\n        \"“Scalable zero-shot entity linking with dense entity retrieval\",\n        \"“Multimodal entity linking for tweets\",\n        \"“Autoregressive entity retrieval\",\n        \"“Multimodal entity linking with gated hierarchical fusion and contrastive training\",\n        \"“Multimodal knowledge learning for named entity disambiguation\",\n        \"“MMEL: A joint learning framework for multi-mention entity linking\",\n        \"“Multi-grained multimodal interaction network for entity linking\",\n        \"“Kuaipedia: a large-scale multi-modal short-video encyclopedia\",\n        \"“Benchmarking diverse-modal entity linking with generative models\",\n        \"“Generative multimodal entity linking\",\n        \"“A dual-way enhanced framework from text matching point of view for multimodal entity linking\",\n        \"“A multimodal translation-based approach for knowledge graph representation learning\",\n        \"“Embedding multimodal relational data for knowledge base completion\",\n        \"“Multimodal data enhanced representation learning for knowledge graphs\",\n        \"“Is visual context really helpful for knowledge graph? A representation learning perspective\",\n        \"“Knowledge graph completion with pre-trained multimodal transformer and twins negative sampling\",\n        \"“OTKGE: multi-modal knowledge graph embeddings via optimal transport\",\n        \"“Mose: Modality split and ensemble for multimodal knowledge graph completion\",\n        \"“MMKRL: A robust embedding approach for multi-modal knowledge graph representation learning\",\n        \"“Modality-aware negative sampling for multi-modal knowledge graph embedding\",\n        \"“IMF: interactive multimodal fusion model for link prediction\",\n        \"“MULTIFORM: few-shot knowledge graph completion via multi-modal contexts\",\n        \"“Multimodal biological knowledge graph completion via triple co-attention mechanism\",\n        \"“Contrastive multi-modal knowledge graph representation learning\",\n        \"“Structure guided multi-modal pre-trained transformer for knowledge graph reasoning\",\n        \"“Hyper-node relational graph attention network for multi-modal knowledge graph completion\",\n        \"“MMKGR: multi-hop multi-modal knowledge graph reasoning\",\n        \"not as I get: Topology-aware multi-hop reasoning on multi-modal knowledge graphs\",\n        \"“Endowing language models with multimodal knowledge graph representations\",\n        \"“Multi-modal knowledge hypergraph for diverse image retrieval\",\n        \"“Self-supervised opinion summarization with multi-modal knowledge graph\",\n        \"“Contrastive language-image pre-training with knowledge graphs\",\n        \"“Graphadapter: Tuning vision-language models with dual knowledge graph\",\n        \"“KGNN: knowledge graph neural network for drug-drug interaction prediction\",\n        \"“Ontoprotein: Protein pretraining with gene ontology embedding\",\n        \"“Molecular contrastive learning with chemical element knowledge graph\",\n        \"“Knowledge graph-enhanced molecular contrastive learning with functional prompt\",\n        \"“Multi-modal protein knowledge graph construction and applications (student abstract)\",\n        \"“Knowledge perceived multi-modal pretraining in e-commerce\",\n        \"“Alime MKG: A multi-modal knowledge graph for live-streaming e-commerce\",\n        \"“Fashionklip: Enhancing e-commerce image-text retrieval with fashion multi-modal conceptual knowledge graph\",\n        \"“Enhancing recommender system with multi-modal knowledge graph\",\n        \"“Multi-modal knowledge graphs for recommender systems\",\n        \"“Cross-modal knowledge graph contrastive learning for machine learning method recommendation\",\n        \"“Fusing visual and textual content for knowledge graph embedding via dual-track model\",\n        \"“Multi-modal knowledge graphs representation learning via multi-headed self-attention\",\n        \"“Knowledge graph incremental embedding for unseen modalities\",\n        \"“A survey of information extraction based on deep learning\",\n        \"“A survey on multimodal knowledge graphs: Construction\",\n        \"“Neural architectures for named entity recognition\",\n        \"“Multimodal named entity recognition for short social media posts\",\n        \"“Visual attention model for name tagging in multimodal social media\",\n        \"“Learning transferable visual models from natural language supervision\",\n        \"“Integrating large pre-trained models into multimodal named entity recognition with evidential fusion\",\n        \"“Fine-grained multimodal named entity recognition and grounding with a generative framework\",\n        \"“Adaptive co-attention network for named entity recognition in tweets\",\n        \"“Joint multimodal entity-relation extraction based on edge-enhanced graph alignment network and word-pair relation tagging\",\n        \"“Event detection and domain adaptation with convolutional neural networks\",\n        \"“Joint event extraction via recurrent neural networks\",\n        \"“Neural cross-lingual event detection with minimal parallel resources\",\n        \"“Event extraction as machine reading comprehension\",\n        \"“Situation recognition with graph neural networks\",\n        \"” in EMNLP/IJCNLP (1). Association for Computational Linguistics\",\n        \"“Deepstruct: Pretraining of language models for structure prediction\",\n        \"“Unified structure generation for universal information extraction\",\n        \"“Grounded situation recognition\",\n        \"“Grounded video situation recognition\",\n        \"“Open visual knowledge extraction via relation-oriented multimodality model prompting\",\n        \"“UMIE: unified multimodal information extraction with instruction tuning\",\n        \"“Ontoea: Ontology-guided entity alignment via joint knowledge graph embedding\",\n        \"“Unsupervised knowledge graph alignment by probabilistic reasoning and semantic embedding\",\n        \"“Understanding and improving knowledge graph embedding for entity alignment\",\n        \"“Deep reinforcement learning for entity alignment\",\n        \"“Revisiting embedding-based entity alignment: A robust and adaptive method\",\n        \"“Clusterea: Scalable entity alignment with stochastic training and normalized mini-batch similarities\",\n        \"“Unsupervised entity alignment for temporal knowledge graphs\",\n        \"“Informed multi-context entity alignment\",\n        \"“Entity alignment with reliable path reasoning and relation-aware heterogeneous graph transformer\",\n        \"“What makes entities similar? A similarity flooding perspective for multi-sourced knowledge graph embeddings\",\n        \"“Leveraging multi-modal information for cross-lingual entity matching across knowledge graphs\",\n        \"“MMIEA: multi-modal interaction entity alignment model for knowledge graphs\",\n        \"“Make it easy: An effective end-to-end entity alignment framework\",\n        \"“Largeea: Aligning entities for large-scale knowledge graphs\",\n        \"“Towards semantic consistency: Dirichlet energy driven robust multi-modal entity alignment\",\n        \"“Knowing the no-match: Entity alignment with dangling cases\",\n        \"“An accurate unsupervised method for joint entity alignment and dangling entity detection\",\n        \"“Dependency-aware self-training for entity alignment\",\n        \"“Entity linking with a knowledge base: Issues\",\n        \"“Entity linking meets deep learning: Techniques and solutions\",\n        \"“Neural entity linking: A survey of models based on deep learning\",\n        \"“Attention-based multimodal entity linking with high-quality images\",\n        \"“Multimodal entity linking: A new dataset and A baseline\",\n        \"“Visual entity linking via multi-modal learning\",\n        \"“Faster zero-shot multi-modal entity linking via visual-linguistic representation\",\n        \"“AMELI: enhancing multimodal entity linking with fine-grained attributes\",\n        \"“Transform and tell: Entity-aware news image captioning\",\n        \"“Universal dependency parsing from scratch\",\n        \"“Fine-tuned llms know more\",\n        \"“Wikidiverse: A multimodal entity linking dataset with diversified contextual topics and entity types\",\n        \"“End-to-end learning on multimodal knowledge graphs\",\n        \"“Reasoning over different types of knowledge graphs: Static\",\n        \"“Multi-task learning for knowledge graph completion with pre-trained language models\",\n        \"“Structure-augmented text representation learning for efficient knowledge graph completion\",\n        \"“Cascader: Cross-modal cascading for knowledge graph link prediction\",\n        \"“Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach\",\n        \"“Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting\",\n        \"“Prompting disentangled embeddings for knowledge graph completion with pre-trained language model\",\n        \"“Sequence-to-sequence knowledge graph completion and question answering\",\n        \"“From discrimination to generation: Knowledge graph completion with generative transformer\",\n        \"“Knowledge is flat: A seq2seq generative framework for various knowledge graph completion\",\n        \"“Exploring large language models for knowledge graph completion\",\n        \"“Making large language models perform better in knowledge graph completion\",\n        \"“Simkgc: Simple contrastive knowledge graph completion with pre-trained language models\",\n        \"“MACO: A modality adversarial and contrastive framework for modality-missing multi-modal knowledge graph completion\",\n        \"A. Krishnamurthy\",\n        \"“Minigpt-4: Enhancing vision-language understanding with advanced large language models\",\n        \"“Knowledge graphs and their applications in drug discovery\",\n        \"“A review of biomedical datasets relating to drug discovery: a knowledge graph perspective\",\n        \"“Uniprot: a worldwide hub of protein knowledge\",\n        \"B. Al-Lazikani et al\",\n        \"“The sider database of drugs and side effects\",\n        \"the signaling network open resource 3.0: 2022 update\",\n        \"“Semmeddb: a pubmed-scale repository of biomedical semantic predications\",\n        \"“A global network of biomedical relationships derived from text\",\n        \"“Drkg - drug repurposing knowledge graph for covid-19\",\n        \"“Open-set image tagging with multi-grained text supervision\",\n        \"“Segment anything\",\n        \"“Mm-llms: Recent advances in multimodal large language models\",\n        \"“Multiway-adapater: Adapting large-scale multi-modal models for scalable image-text retrieval\",\n        \"“Multimodal graph learning for generative tasks\",\n        \"“Exploring boundary of GPT-4V on marine analysis: A preliminary case study\",\n        \"“Knowledgeable preference alignment for llms in domain-specific question answering\",\n        \"“LION : Empowering multimodal large language model with dual-level visual knowledge\",\n        \"“Knowgpt: Black-box knowledge injection for large language models\",\n        \"“Explore-instruct: Enhancing domain-specific instruction coverage through active exploration\",\n        \"“Self-instruct: Aligning language models with self-generated instructions\",\n        \"“Wizardlm: Empowering large language models to follow complex instructions\",\n        \"“What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning\",\n        \"“Narrowing the knowledge evaluation gap: Open-domain question answering with multi-granularity answers\",\n        \"“Sequential modeling enables scalable learning for large vision models\",\n        \"“Generative multimodal models are in-context learners\",\n        \"“Vary: Scaling up the vision vocabulary for large vision-language models\",\n        \"“Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse\",\n        \"“Siren’s song in the AI ocean: A survey on hallucination in large language models\",\n        \"“A survey of hallucination in large foundation models\",\n        \"“Can knowledge graphs reduce hallucinations in llms? : A survey\",\n        \"“OPERA: alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation\",\n        \"“Eyes wide shut? exploring the visual shortcomings of multimodal llms\",\n        \"“A survey on hallucination in large vision-language models\",\n        \"“Bamboo: Building mega-scale vision dataset continually with human-machine synergy\",\n        \"“Unveiling the siren’s song: Towards reliable fact-conflicting hallucination detection\",\n        \"“Fine-grained hallucination detection and editing for language models\",\n        \"“Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting\",\n        \"“Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites\",\n        \"“Urban generative intelligence (UGI): A foundational platform for agents in embodied city environment\",\n        \"“How far are we from believable AI agents? A framework for evaluating the believability of human behavior simulation\",\n        \"“Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment\",\n        \"“Towards top-down reasoning: An explainable multi-agent approach for visual question answering\",\n        \"“AUTOACT: automatic agent learning from scratch via self-planning\",\n        \"“Editing personality for llms\",\n        \"“Characterchat: Learning towards conversational AI with personalized social support\",\n        \"“Graph-guided reasoning for multi-hop question answering in large language models\",\n        \"“Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph\",\n        \"“Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning\",\n        \"“Fine-tuning or retrieval? comparing knowledge injection in llms\",\n        \"“Seven failure points when engineering a retrieval augmented generation system\",\n        \"“Learning to filter context for retrieval-augmented generation\",\n        \"“CAR: consolidation\",\n        \"“V*: Guided visual search as a core mechanism in multimodal llms\",\n        \"“Generate rather than retrieve: Large language models are strong context generators\",\n        \"“Can we edit multimodal large language models?” in EMNLP. Association for Computational Linguistics\",\n        \"“Editing language model-based knowledge graph embeddings\",\n        \"“Understanding the effects of RLHF on LLM generalisation and diversity\",\n        \"“Improving generalization of alignment with human preferences through group invariant learning\",\n        \"L. Aschenbrenner\",\n        \"“Alignment for honesty\",\n        \"“Llmaaa: Making large language models as active annotators\",\n        \"“Interpretable mixture of experts\",\n        \"“Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment\",\n        \"“Llama-moe: Building mixture-of-experts from llama with continual pre-training\",\n        \"“Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models\",\n        \"“Moe-llava: Mixture of experts for large vision-language models\"\n    ],\n    \"reasoning\": \"Selected these 609 references because they represent foundational work and recent advances in Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}