{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 45 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: SOMONITOR: Explainable Marketing Data Processing and Analysis with Large Language Models\n\nPaper Abstract: Abstract. Online marketing faces formidable challenges in managing and interpreting immense volumes of data necessary for competitor analysis, content research, and strategic branding. It is impossible to review hundreds to thousands of transient online content items by hand, and partial analysis often leads to suboptimal outcomes and poorly performing campaigns. We introduce an explainable AI framework SOMONITOR that aims to synergize human intuition with AI-based efficiency, helping marketers across all stages of the marketing funnel, from strategic planning to content creation and campaign execution. SOMONITOR incorporates a CTR prediction and ranking model for advertising content and uses large language models (LLMs) to process high-performing competitor content, identifying core content pillars such as target audiences, customer needs, and product features. These pillars are then organized into broader categories, including communication themes and targeted customer personas. By integrating these insights with data from the brand’s own advertising campaigns, SOMONITOR constructs a narrative for addressing new customer personas and simultaneously generates detailed content briefs in the form of user stories that can be directly applied by marketing teams to streamline content production and campaign execution. The adoption of SOMONITOR in daily operations allows digital marketers to quickly parse through extensive datasets, offering actionable insights that significantly enhance campaign effectiveness and overall job satisfaction.\n\n{retrieved_text}\n\nGenerate exactly 45 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 45\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Introducing the next generation of Claude\",\n        \"Longformer: The Long-Document Transformer\",\n        \"Multi-view personality profiling based on longitudinal data\",\n        \"Hallucination detection: Robustly discerning reliable answers in large language models\",\n        \"Generating Long Sequences with Sparse Transformers\",\n        \"Rethinking Attention with Performers\",\n        \"From LLM to NMT: Advancing Low-Resource Machine Translation with Claude\",\n        \"Mistral 7B\",\n        \"Evaluating Large Language Models Trained on Code\",\n        \"Redemptive Marketing: The Path To Resolving Transparency And Performance Dilemmas In Digital Advertising\",\n        \"Online Advertisements with LLMs: Opportunities and Challenges\",\n        \"Image Matters: Visually Modeling User Behaviors Using Advanced Model Server\",\n        \"Practical Lessons from Predicting Clicks on Ads at Facebook\",\n        \"Imagen Video: High Definition Video Generation with Diffusion Models\",\n        \"Denoising Diffusion Probabilistic Models\",\n        \"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers\",\n        \"Marketing Generative AI Report: A Comprehensive Evaluation of Leading LLMs\",\n        \"Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra\",\n        \"Latent Dirichlet Allocation: Stability and Applications to Studies of User-Generated Content\",\n        \"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models\",\n        \"Mega: Moving Average Equipped Gated Attention\",\n        \"Large Language Models: Their Success and Impact\",\n        \"Persona based marketing strategies: creation of personas through data analytics\",\n        \"Ad Click Prediction: A View from the Trenches\",\n        \"Large Language Models: A Survey\",\n        \"Text and code embeddings by contrastive pre-training\",\n        \"SVD-LDA: Topic Modeling for Full-Text Recommender Systems\",\n        \"GPT-4 Technical Report\",\n        \"Hello GPT-4o\",\n        \"Resurrecting Recurrent Neural Networks for Long Sequences\",\n        \"Training language models to follow instructions with human feedback\",\n        \"Representation Learning-Assisted Click-Through Rate Prediction\",\n        \"X-means: Extending k-means with efficient estimation of the number of clusters.\",\n        \"High-Resolution Image Synthesis With Latent Diffusion Models\",\n        \"Ad Lingua: Text Classification Improves Symbolism Prediction in Image Advertisements\",\n        \"Denoising Diffusion Implicit Models\",\n        \"Video Understanding with Large Language Models: A Survey\",\n        \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\n        \"Leveraging large language models in advertising\",\n        \"Demographic Prediction based on User Reviews about Medications\",\n        \"Exploring convolutional neural networks and topic models for user profiling from drug reviews\",\n        \"Attention Is All You Need\",\n        \"Linformer: Self-Attention with Linear Complexity\",\n        \"Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising\",\n        \"Siren’s song in the AI ocean: a survey on hallucination in large language models\"\n    ],\n    \"reasoning\": \"Selected these 45 references because they represent foundational work and recent advances in SOMONITOR: Explainable Marketing Data Processing and Analysis with Large Language Models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}