{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 55 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Sentiment analysis in tweets: an assessment study from classical to modern text representation models\n\nPaper Abstract: Abstract With the growth of social medias, such as Twitter, plenty of user-generated data emerge daily. The short texts published on Twitter – the tweets – have earned significant attention as a rich source of information to guide many decision-making processes. However, their inherent characteristics, such as the informal, and noisy linguistic style, remain challenging to many natural language processing (NLP) tasks, including sentiment analysis. Sentiment classification is tackled mainly by machine learning-based classifiers. The literature has adopted word representations from distinct natures to transform tweets to vector-based inputs to feed sentiment classifiers. The representations come from simple count-based methods, such as bag-of-words, to more sophisticated ones, such as BERTweet, built upon the trendy BERT architecture. Nevertheless, most studies mainly focus on evaluating those models using only a small number of datasets. Despite the progress made in recent years in language modelling, there is still a gap regarding a robust evaluation of induced embeddings applied to sentiment analysis on tweets. Furthermore, while fine-tuning the model from downstream tasks is prominent nowadays, less attention has been given to adjustments based on the specific linguistic style of the data. In this context, this study fulfils an assessment of existing language models in distinguishing the sentiment expressed in tweets by using a rich collection of 22 datasets from distinct domains and five classification algorithms. The evaluation includes static and contextualized representations. Contexts are assembled from Transformer-based autoencoder models that are also fine-tuned based on the masked language model task, using a plethora of strategies.\n\n{retrieved_text}\n\nGenerate exactly 55 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 55\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Docbert: BERT for document classification\",\n        \"Learning emotion-enriched word representations\",\n        \"Application of Deep Learning Approaches for Sentiment Analysis\",\n        \"Applying BERT to document retrieval with birch\",\n        \"Enhancing deep learning sentiment analysis with ensemble techniques in social applications\",\n        \"Robust sentiment detection on Twitter from biased and noisy data\",\n        \"Learning long-term dependencies with gradient descent is difficult\",\n        \"Determining word-emotion associations from tweets by multi-label classification\",\n        \"Sentiment analysis is a big suitcase\",\n        \"Exploiting Different Types of Features to Improve Classification Effectiveness in Twitter Sentiment Analysis\",\n        \"On the evaluation and combination of state-of-the-art features in Twitter sentiment analysis\",\n        \"Efficientqa : a roberta based phrase-indexed question-answering system\",\n        \"Extracting diverse sentiment expressions with target-dependent polarity from twitter\",\n        \"Bert: Pre-training of deep bidirectional transformers for language understanding\",\n        \"Characterizing debate performance via aggregated twitter sentiment\",\n        \"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping\",\n        \"Adaptive recursive neural network for target-dependent twitter sentiment classification\",\n        \"Summary from the kdd-03 panel - data mining: The next 10 years\",\n        \"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\",\n        \"A Vector Space Model for Automatic Indexing\",\n        \"Target-dependent sentiment classification with bert\",\n        \"Vader: A parsimonious rule-based model for sentiment analysis of social media text\",\n        \"Twitter sentiment classification using distant supervision\",\n        \"Bazinga! caracterizando e detectando sarcasmo e ironia no twitter\",\n        \"Deep Learning\",\n        \"Universal language model fine-tuning for text classification\",\n        \"Albert: A lite bert for self-supervised learning of language representations\",\n        \"Flaubert: Unsupervised language model pre-training for french\",\n        \"Sentiment analysis: Mining opinions, sentiments, and emotions\",\n        \"Roberta: A robustly optimized bert pretraining approach\",\n        \"Short text opinion detection using ensemble of classifiers and semantic indexing\",\n        \"Introduction to Information Retrieval\",\n        \"Advances in pre-training distributed word representations\",\n        \"Distributed representations of words and phrases and their compositionality\",\n        \"Semeval-2018 Task 1: Affect in tweets\",\n        \"SemEval-2016 task 4: Sentiment analysis in Twitter\",\n        \"Language-independent twitter sentiment analysis\",\n        \"Bertweet: A pre-trained language model for english tweets\",\n        \"Twitter as a corpus for sentiment analysis and opinion mining\",\n        \"GloVe: Global vectors for word representation\",\n        \"Deep contextualized word representations\",\n        \"The Edinburgh twitter corpus\",\n        \"Improving language understanding by generative pre-training\",\n        \"SemEval-2017 task 4: Sentiment analysis in Twitter\",\n        \"Semantic sentiment analysis of microblogs\",\n        \"Evaluation datasets for twitter sentiment analysis: A survey and a new dataset, the sts-gold\",\n        \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\",\n        \"Twitter polarity classification with label propagation over lexical links and the follower graph\",\n        \"Learning sentiment-specific word embedding for Twitter sentiment classification\",\n        \"Sentiment strength detection for the social web\",\n        \"Enriching Word Vectors with Subword Information\",\n        \"From frequency to meaning: Vector space models of semantics\",\n        \"Attention is all you need\",\n        \"Emo2Vec: Learning generalized emotion representation by multi-task training\",\n        \"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books\"\n    ],\n    \"reasoning\": \"Selected these 55 references because they represent foundational work and recent advances in Sentiment analysis in tweets: an assessment study from classical to modern text representation models, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}