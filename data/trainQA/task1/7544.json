{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 58 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learningfootnote footnoteAuthor names are in randomized order.\n\nPaper Abstract: Abstract The Forster transform is a method of regularizing a dataset by placing it in radial isotropic position while maintaining some of its essential properties. Forster transforms have played a key role in a diverse range of settings spanning computer science and functional analysis. Prior work had given weakly polynomial time algorithms for computing Forster transforms, when they exist. Our main result is the first strongly polynomial time algorithm to compute an approximate Forster transform of a given dataset or certify that no such transformation exists. By leveraging our strongly polynomial Forster algorithm, we obtain the first strongly polynomial time algorithm for distribution-free PAC learning of halfspaces. This learning result is surprising because proper PAC learning of halfspaces is equivalent to linear programming. Our learning approach extends to give a strongly polynomial halfspace learner in the presence of random classification noise and, more generally, Massart noise.\n\n{retrieved_text}\n\nGenerate exactly 58 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 58\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Operator scaling via geodesically convex optimization, invariant theory and polynomial identity testing\",\n        \"On radial isotropic position: Theory and algorithms\",\n        \"Learning from noisy examples\",\n        \"On a reverse form of the brascamp-lieb inequality\",\n        \"Learnability and the Vapnik-Chervonenkis dimension\",\n        \"New combinatorial and polynomial algorithms for the linear feasibility problem\",\n        \"Efficient algorithms for tensor scaling, quantum marginals, and moment polytopes\",\n        \"A polynomial-time algorithm for learning noisy linear threshold functions\",\n        \"A polynomial time algorithm for learning noisy linear threshold functions\",\n        \"Classification under misspecification: Halfspaces, generalized linear models, and evolvability\",\n        \"Learning noisy perceptrons by a perceptron in polynomial time\",\n        \"Distribution-independent PAC learning of halfspaces with massart noise\",\n        \"A scaling-invariant algorithm for linear programming whose running time depends only on the constraint matrix\",\n        \"Near-optimal statistical query hardness of learning halfspaces with massart noise\",\n        \"Cryptographic hardness of learning halfspaces with massart noise\",\n        \"Forster decomposition and learning halfspaces with noise\",\n        \"Combinatorial methods in density estimation\",\n        \"Revisiting tardosâ€™s framework for linear programming: Faster exact solutions using approximate solvers\",\n        \"Relu regression with massart noise\",\n        \"Superquadratic lower bound for 3-query locally correctable codes over the reals\",\n        \"Optimal outlier removal in high-dimensional spaces\",\n        \"A simple polynomial-time rescaling algorithm for solving linear programs\",\n        \"A linear lower bound on the unbounded error probabilistic communication complexity\",\n        \"A decision-theoretic generalization of on-line learning and an application to boosting\",\n        \"Recent progress on scaling algorithms and applications\",\n        \"Algorithmic and optimization aspects of brascamp-lieb inequalities, via operator scaling\",\n        \"Majority gates vs. general weighted threshold gates\",\n        \"Geometric Algorithms and Combinatorial Optimization, volume 2\",\n        \"A deterministic algorithm for approximating the mixed discriminant and mixed volume, and a combinatorial corollary\",\n        \"Finding minimum-cost circulations by canceling negative cycles\",\n        \"A strongly polynomial algorithm for linear exchange markets\",\n        \"Point location and active learning: Learning halfspaces almost optimally\",\n        \"Algorithms and hardness for robust subspace recovery\",\n        \"The paulsen problem made simple\",\n        \"The paulsen problem, continuous operator scaling, and smoothed analysis\",\n        \"An Introduction to Computational Learning Theory\",\n        \"A deterministic strongly polynomial algorithm for matrix scaling and approximate permanents\",\n        \"Towards a genuinely polynomial algorithm for linear programming\",\n        \"Risk bounds for statistical learning\",\n        \"Perceptrons: an introduction to computational geometry\",\n        \"How fast can a threshold gate learn?\",\n        \"On convergence proofs on perceptrons\",\n        \"Optimal SQ lower bounds for learning halfspaces with Massart noise\",\n        \"Analysis of Boolean Functions\",\n        \"A faster strongly polynomial minimum cost flow algorithm\",\n        \"A simpler and faster strongly polynomial algorithm for generalized flow maximization\",\n        \"The Symmetric Eigenvalue Problem\",\n        \"The Perceptron: a probabilistic model for information storage and organization in the brain\",\n        \"Mathematical problems for the next century\",\n        \"An introduction to support vector machines\",\n        \"A strongly polynomial minimum cost circulation algorithm\",\n        \"A strongly polynomial algorithm to solve combinatorial linear programs\",\n        \"A theory of the learnable\",\n        \"Statistical Learning Theory\",\n        \"A strongly polynomial algorithm for generalized flow maximization\",\n        \"A strongly polynomial algorithm for a class of minimum-cost flow problems with separable convex objectives\",\n        \"A primal-dual interior point method whose running time depends only on the constraint matrix\",\n        \"On ACC and threshold circuits\"\n    ],\n    \"reasoning\": \"Selected these 58 references because they represent foundational work and recent advances in A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learningfootnote footnoteAuthor names are in randomized order., providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}