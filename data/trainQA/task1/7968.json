{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic reference prediction expert. Your task is to predict the most likely references for a given academic paper based on its title, abstract, and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract\n- Output: {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nREQUIREMENTS:\n1. Analyze the paper's title and abstract to understand its research area and focus\n2. Consider the retrieved papers as potential references (when available)\n3. Generate a list of predicted reference titles that would be most relevant and likely to be cited\n4. Provide reasoning for each prediction to prevent hallucination\n5. Output format: JSON object with \"titles\" array and \"reasoning\" string\n6. Generate exactly 48 most relevant reference titles (this should be at least 40 to cover all evaluation metrics)\n7. Ensure titles are realistic and appropriate for the research area\n8. Consider citation patterns in the field\n9. CRITICAL: Each title must be unique - never repeat the same title multiple times\n\nEXAMPLE:\nInput:\nTitle: \"Question Type Classification Methods Comparison\"\nAbstract: \"The paper presents a comparative study of state-of-the-art approaches for question classification task: Logistic Regression, Convolutional Neural Networks (CNN), Long Short-Term Memory Network (LSTM) and Quasi-Recurrent Neural Networks (QRNN). All models use pre-trained GLoVe word embeddings and trained on human-labeled data. The best accuracy is achieved using CNN model with five convolutional layers and various kernel sizes stacked in parallel, followed by one fully connected layer. The model reached 90.7% accuracy on TREC 10 test set.\"\n\nRetrieved papers ({top_r}):\n1. Title: Convolutional Neural Networks for Sentence Classification\n   Abstract: We report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks.\n\n2. Title: Glove: Global vectors for word representation\n   Abstract: We present a new global log-bilinear regression model that combines the advantages of the two major model families in the literature.\n\nOutput:\n{\n  \"titles\": [\n    \"Convolutional Neural Networks for Sentence Classification\",\n    \"Glove: Global vectors for word representation\",\n    \"Quasi-Recurrent Neural Networks\",\n    \"A Comparative Study of Neural Network Models for Sentence Classification\",\n    \"Deep lstm based feature mapping for query classification\"\n  ],\n  \"reasoning\": \"Based on the paper's focus on question classification using neural networks, I predict these references because: 1) CNN paper is directly relevant to the convolutional approach used; 2) GloVe embeddings are mentioned as pre-trained features; 3) QRNN is one of the compared methods; 4) Comparative study papers are common in this field; 5) LSTM-based query classification is closely related to question classification. Additional references cover related neural network architectures, NLP applications, deep learning methodologies, optimization techniques, and advanced neural network methods that would be relevant for this research area.\"\n}\n\nReturn JSON object only."
    },
    {
      "role": "user",
      "content": "Paper Title: Automatic Noise Filtering with Dynamic Sparse Training in Deep Reinforcement Learning\n\nPaper Abstract: Abstract. Tomorrow’s robots will need to distinguish useful information from noise when performing different tasks. A household robot for instance may continuously receive a plethora of information about the home, but needs to focus on just a small subset to successfully execute its current chore. Filtering distracting inputs that contain irrelevant data has received little attention in the reinforcement learning literature. To start resolving this, we formulate a problem setting in reinforcement learning called the extremely noisy environment (ENE), where up to 99% of the input features are pure noise. Agents need to detect which features provide task-relevant information about the state of the environment. Consequently, we propose a new method termed Automatic Noise Filtering (ANF), which uses the principles of dynamic sparse training in synergy with various deep reinforcement learning algorithms. The sparse input layer learns to focus its connectivity on task-relevant features, such that ANF-SAC and ANF-TD3 outperform standard SAC and TD3 by a large margin, while using up to 95% fewer weights. Furthermore, we devise a transfer learning setting for ENEs, by permuting all features of the environment after 1M timesteps to simulate the fact that other information sources can become relevant as the world evolves. Again, ANF surpasses the baselines in final performance and sample complexity. Our code is available online.111See https://github.com/bramgrooten/automatic-noise-filtering\n\n{retrieved_text}\n\nGenerate exactly 48 most likely reference titles for this paper. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL: Each title must appear exactly once. Do not repeat any title.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"titles\": [\n        \"Reference Title 1\",\n        \"Reference Title 2\",\n        \"Reference Title 3\",\n        \"Reference Title 4\",\n        \"Reference Title 48\"\n    ],\n    \"reasoning\": \"Explain why you chose these specific references based on the paper's title, abstract, and retrieved papers. This helps prevent hallucination and ensures relevance.\"\n}\n\nDo not include any text before or after the JSON. Do not include authors, venues, or years - only the paper titles. The reasoning field is required to explain your prediction logic."
    },
    {
      "role": "assistant",
      "content": "{\n    \"titles\": [\n        \"Feature selection for scalable reinforcement learning\",\n        \"Single-Shot Pruning for Offline Reinforcement Learning\",\n        \"Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders\",\n        \"Vision Substitution by Tactile Image Projection\",\n        \"Deep Rewiring: Training very sparse deep networks\",\n        \"Robust Deep Reinforcement Learning for Greenhouse Control and Crop Yield Optimization\",\n        \"Evolutionary Feature Evaluation for Online Reinforcement Learning\",\n        \"Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces\",\n        \"OpenAI Gym\",\n        \"Neuroplasticity in adult human visual cortex\",\n        \"Chasing Sparsity in Vision Transformers: An End-to-End Exploration\",\n        \"Sparsity Winning Twice: Better Robust Generalization from More Efficient Training\",\n        \"Truly Sparse Neural Networks at Scale\",\n        \"Dimensionality Reduced Reinforcement Learning for Assistive Robots\",\n        \"Magnetic control of tokamak plasmas through deep reinforcement learning\",\n        \"Continual Backprop: Stochastic Gradient Descent with Persistent Randomness\",\n        \"Rigging the Lottery: Making All Tickets Winners\",\n        \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\",\n        \"Addressing Function Approximation Error in Actor-Critic Methods\",\n        \"The State of Sparse Training in Deep Reinforcement Learning\",\n        \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n        \"Learning both Weights and Connections for Efficient Neural Networks\",\n        \"Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks\",\n        \"The Hardware Lottery\",\n        \"Adam: A Method for Stochastic Optimization\",\n        \"Automatic Feature Selection for Model-Based Reinforcement Learning in Factored MDPs\",\n        \"Privacy Threats Analysis to Secure Federated Learning\",\n        \"Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers\",\n        \"Sparse evolutionary Deep Learning with over one million artificial neurons on commodity hardware\",\n        \"On improving deep learning generalization with adaptive sparse connectivity\",\n        \"Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training\",\n        \"Sparse Training Theory for Scalable and Efficient Agents\",\n        \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science\",\n        \"Robust Reinforcement Learning: A Review of Foundations and Recent Advances\",\n        \"Electrical synapses and their functional interactions with chemical synapses\",\n        \"From Eye-blinks to State Construction: Diagnostic Benchmarks for Online Representation Learning\",\n        \"Reward is enough\",\n        \"Where to Pay Attention in Sparse Training for Feature Selection?\",\n        \"Dynamic Sparse Training for Deep Reinforcement Learning\",\n        \"The Distracting Control Suite – A Challenging Benchmark for Reinforcement Learning from Pixels\",\n        \"Exploring the Training Robustness of Distributional Reinforcement Learning against Noisy State Observations\",\n        \"RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch\",\n        \"Transfer learning for reinforcement learning domains: A survey\",\n        \"MuJoCo: A physics engine for model-based control\",\n        \"Robust Reinforcement Learning using Adversarial Populations\",\n        \"On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning\",\n        \"Automatic Feature Selection in Neuroevolution\",\n        \"Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch\"\n    ],\n    \"reasoning\": \"Selected these 48 references because they represent foundational work and recent advances in Automatic Noise Filtering with Dynamic Sparse Training in Deep Reinforcement Learning, providing essential background knowledge and methodological foundations relevant to this research.\"\n}"
    }
  ],
  "format": "chatml"
}