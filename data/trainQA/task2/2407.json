{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Topological Perspective on Causal Inference\n\nPaper Abstract: \n\nPaper Section Text:\nIn the background of any investigation into learning algorithms are no-free-lunch phenomena: roughly, the observation that assumption-free statistical learning is infeasible in general (see, e.g., [38, Ch. 5] for a formal statement). Common wisdom is that learning algorithms and architectures must adequately reflect non-trivial features of the data-generating distribution to gain inductive purchase. For many purposes we need to move beyond passive observation, focusing instead on what would happen were we to act upon a given system. Even further, we sometimes desire to explain the behavior of a system, raising questions about what would have occurred had some aspects of a situation been different. Such questions depend not just on the data distribution; they depend on deeper features of underlying data-generating processes or mechanisms. It is thus generally acknowledged that stronger assumptions are required if we want to draw causal conclusions from data     . Whether implicit or explicit, any approach to causal inference involves a space of candidate causal models, viz. data-generating processes. Indeed, a blunt way of incorporating inductive bias is simply to omit some class of possible causal hypotheses from consideration. Many (im)possibility results in the literature can accordingly be understood as pertaining to all models within a class.\nFor instance, if we can restrict attention to Markovian models that satisfy faithfulness, then we can always identify the structure of a model from experimental data (e.g.,  ). If we can restrict attention to Markovian (continuous) models with linear functions and non-Gaussian noise, then every model can be learned even from purely observational data . As a negative example, in the larger class of (not necessarily Markovian) models, no model can ever be determined from observational data alone  [ref]4. At the same time, in many settings it is sensible to aim for results with “nearly universal” force. It is natural to ask, e.g., within the class of all Markovian models, how “typical” are those in which the faithfulness condition is violated? This might tell us, for instance, how typically we could expect failure of a method that depended on these assumptions. A well-known result shows that, fixing any particular causal dependence graph, such violations have measure zero for any smooth (e.g., Lebesgue) measure on the parameter space of distributions consistent with that graph . In fact, the standard notion of statistical consistency itself, which underlies many possibility results in causal inference, requires omission of some purportedly “negligible” set of possible data streams  . There are two standard mathematical approaches to making concepts like “typical” and “negligible” rigorous: measure-theoretic and topological. While the two approaches often agree, they capture slightly different intuitions . One virtue of the measure-theoretic approach is its natural probabilistic interpretation: intuitively, we are exceedingly unlikely to hit upon a set with measure zero. At the same time, the measure-theoretic approach is sometimes criticized in statistical settings for its alleged dependence on a measure, and this has been argued to favor topological approaches (see, e.g.,  on no-free-lunch theorems). The latter of course in turn demands an appropriate topology. In the present work we show how to define a sequence of meaningful topologies on the space of causal models, each corresponding to a progressively coarser level of the so called causal hierarchy ( [ref]4; see Fig. 1 for an abbreviated pictorial summary). We aim to demonstrate that topologizing causal models in this way helps clarify the scope and limits of causal inference under different assumptions, as well as the potential empirical status of those very assumptions, in a highly general setting.\n Our starting point is a canonical topology on the space of Borel probability distributions called the weak topology. The weak topology is grounded in the fundamental notion of weak convergence of probability distributions  and is thereby closely related to problems of statistical inference (see, e.g., ). Recent work has sharpened this correspondence, showing that open sets in the weak topology correspond exactly to the statistical hypotheses that can be naturally deemed verifiable  . We extend the correspondence to higher levels of the causal hierarchy, including the most refined and expansive “top” level consisting of all (well-founded) causal models. Lower levels and natural subspaces (e.g., corresponding to prominent causal assumption classes) emerge as coarsenings and projections of this largest space. As an illustration of the general approach, we prove a topological version of the causal hierarchy theorem from [ref]4. Rather than showing that collapse happens only in a measure zero set as in [ref]4, our Theorem 3 show that collapse is topologically meager. Conceptually, this highlights a different (but complementary) intuition: not only is collapse exceedingly unlikely in the sense of measure, meagerness implies that collapse could never be statistically verified. Correlatively, this implies that any causal assumption that would generally allow us to infer counterfactual probabilities from experimental (or “interventional”) probabilities must itself be statistically unverifiable (Corollary 1). To derive such a result we actually show something slightly stronger (see Lem. 2): even with respect to the subspace of models consistent with a fixed temporal order on variables, the causal hierarchy theorem holds. Merely knowing the temporal order of the variables is not enough to render collapse of the hierarchy a statistically verifiable proposition.\nFurthermore, we show that the witness to collapse can be taken as any of the well-known counterfactual “probabilities of causation” (see, e.g., ): probabilities of necessity, sufficiency, necessity and sufficiency, enablement, or disablement. That is, none of these important quantities are fully determined by experimental data except in a meager set. In §2 we give background on causal models, and in §3 we present a model-theoretic characterization of the causal hierarchy as a sequence of spaces. Topology is introduced in §4, and the main results about collapse appear in §5. For the technical results, we include proof sketches in the main text to provide the core intuitions, relegating some of the details to an exhaustive technical appendix, which also includes additional supplementary material.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: can be learned even from purely observational data . As a negative example, in the larger class of (not necessarily Markovian) models, no model can ever be determined from observational data alone  [ref]4. At the same time, in many settings it is sensible to aim for results with “nearly universal” force. It is natural to ask, e.g., within the class of all Markovian models, how “typical” are\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Absolutely No Free Lunches!\",\n                \"Identification of causal effects using instrumental variables\",\n                \"Identifiability of path-specific effects\",\n                \"Probabilistic evaluation of counterfactual queries\",\n                \"On Pearl’s hierarchy and the foundations of causal inference\",\n                \"Convergence of Probability Measures\",\n                \"Measure Theory\",\n                \"Foundations of Structural Causal Models with Cycles and Latent Variables\",\n                \"Nature’s Capacities and their Measurements\",\n                \"A topological criterion for hypothesis testing\",\n                \"On the consistency of Bayes estimates\",\n                \"Global identifiability of linear structural equation models\",\n                \"On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables\",\n                \"From association to causation via regression\",\n                \"An axiomatic characterization of causal counterfactuals\",\n                \"The Topology of Statistical Inquiry\",\n                \"Statistical undecidability in linear, non-gaussian models in the presence of latent confounders\",\n                \"The Topology of Statistical Verifiability\",\n                \"Statistical decidability in linear, non-Gaussian models\",\n                \"On open-universe causal reasoning\",\n                \"Probabilistic reasoning across the causal hierarchy\",\n                \"Causal Inference in Statistics, Social, and Biomedical Sciences\",\n                \"Classical Descriptive Set Theory\",\n                \"General Topology\",\n                \"General identifiability with arbitrary surrogate experiments\",\n                \"On Learning Causal Structures from Non-Experimental Data without Any Faithfulness Assumption\",\n                \"Strong completeness and faithfulness in Bayesian networks\",\n                \"Right inverses of linear maps on convex sets\",\n                \"Measure and Category\",\n                \"Causal diagrams for empirical research\",\n                \"Probabilities of causation: Three counterfactual interpretations and their identification\",\n                \"Causality\",\n                \"The book of why: The new science of cause and effect\",\n                \"Elements of Causal Inference: Foundations and Learning Algorithms\",\n                \"Single world intervention graphs (SWIGs): A unification of the counterfactual and graphical approaches to causality\",\n                \"The probability of causation under a stochastic model for individual risk\",\n                \"Toward causal representation learning\",\n                \"Understanding Machine Learning: From Theory to Algorithms\",\n                \"A linear non-Gaussian acyclic model for causal discovery\",\n                \"Causation, Prediction, and Search\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of A Topological Perspective on Causal Inference. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}