{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Frontier AI Regulation:Managing Emerging Risks to Public Safety\n\nPaper Abstract: \n\nPaper Section Text:\nFor the purposes of this paper, we define “frontier AI models” as\nhighly capable foundation models222\ndefines “foundation models” as “models (e.g., BERT, DALL-E, GPT-3) that are\ntrained on broad data at scale and are adaptable to a wide range of downstream\ntasks.” See also [ref]24. that could exhibit sufficiently dangerous capabilities.\nSuch harms could take the form of significant physical harm or the disruption\nof key societal functions on a global scale, resulting from intentional\nmisuse or accident  .\nIt would be prudent to assume that next-generation foundation models could\npossess advanced enough capabilities to qualify as frontier AI models, given\nboth the  difficulty\nof predicting when sufficiently dangerous capabilities will\narise and the already significant capabilities of today’s models. Though it is not clear where the line for “sufficiently dangerous capabilities”\nshould be drawn, examples could include: Allowing a non-expert to design and synthesize new biological or chemical weapons.333Such capabilities are starting to emerge. For example, a group of\nresearchers tasked a narrow drug-discovery system to identify maximally toxic\nmolecules. The system identified over 40,000 candidate molecules, including\nboth known chemical weapons and novel molecules that were predicted to be as or\nmore deadly [ref]16. Other researchers are warning that LLMs\ncan be used to aid in discovery and synthesis of compounds. One group attempted\nto create an LLM-based agent, giving it access to the internet, code execution\nabilities, hardware documentation, and remote control of an automated ‘cloud’\nlaboratory. They report finding that it in some cases the model was willing to outline\nand execute on viable methods for synthesizing illegal drugs and chemical\nweapons . Producing and propagating highly persuasive, individually tailored,\nmulti-modal disinformation with minimal user instruction.444Generative AI models may already be useful to generate material for\ndisinformation campaigns   .\nIt is possible that, in the future, models could possess additional\ncapabilities that could enhance the persuasiveness or dissemination of\ndisinformation, such as by making such disinformation more dynamic,\npersonalized, and multimodal; or by autonomously disseminating such\ndisinformation through channels that enhance its persuasive value, such as\ntraditional media. Harnessing unprecedented offensive cyber capabilities that could cause\ncatastrophic harm.555AI systems are already helpful in writing and\ndebugging code, capabilities that can also be applied to software vulnerability\ndiscovery. There is potential for significant harm via automation of\nvulnerability discovery and exploitation. However, vulnerability discovery\ncould ultimately benefit cyberdefense more than -offense, provided defenders\nare able to use such tools to identify and patch vulnerabilities more\neffectively than attackers can find and exploit them\n . Evading human control through means of deception and obfuscation.666If future AI systems develop the ability and the propensity to\ndeceive their users, controlling their behavior could be extremely challenging.\nThough it is unclear whether models will trend in that direction, it seems rash\nto dismiss the possibility and some argue that it might be the default outcome\nof current training paradigms      . This list represents just a few salient possibilities; the possible future\ncapabilities of frontier AI models remains an important area of inquiry. Foundation models, such as large language models (LLMs), are trained on large,\nbroad corpora of natural language and other text (e.g., computer code), usually\nstarting with the simple objective of predicting the next “token”.777A\ntoken can be thought of as a word or part of a word\n. This relatively simple approach produces models\nwith surprisingly broad capabilities.888For example, LLMs achieve\nstate-of-the-art performance in diverse tasks such as question answering,\ntranslation, multi-step reasoning, summarization, and code completion, among\nothers   [ref]36 . Indeed, the term “LLM” is already becoming outdated, as\nseveral leading “LLMs” are in fact multimodal (e.g., possess visual\ncapabilities) [ref]36 . These models\nthus possess more general-purpose functionality999We intentionally avoid\nusing the term “general-purpose AI” to avoid confusion with the use of that\nterm in the EU AI Act and other legislation. Frontier AI systems are a related\nbut narrower class of AI systems with general-purpose functionality, but whose\ncapabilities are relatively advanced and novel. than many other classes of AI\nmodels, such as the recommender systems used to suggest Internet videos or\ngenerative AI models in narrower domains like music. Developers often make\ntheir models available through “broad deployment” via sector-agnostic platforms\nsuch as APIs, chatbots, or via open-sourcing.101010We use “open-source” to\nmean “open release:” that is a model being made freely available online, be it\nwith a license restricting what the system can be used for. An example of such\na license is the Responsible AI License. Our usage of “open-source” differs\nfrom how the term is often used in computer science which excludes instances of\nlicense requirements, though is closer to how many other communities understand\nthe term  . This means that they can be\nintegrated in a large number of diverse downstream applications, possibly\nincluding safety-critical sectors (illustrated in\n Figure 1). A number of features of our definition are worth highlighting. In focusing on\nfoundation models which could have dangerous, emergent capabilities, our\ndefinition of frontier AI excludes narrow models, even when these models could\nhave sufficiently dangerous capabilities.111111However, if a foundation\nmodel could be fine-tuned and adapted to pose severe risk to public safety via\ncapabilities in some narrow domain, it would count as a “frontier AI.” For\nexample, models optimizing for the toxicity of compounds [ref]16\nor the virulence of pathogens could lead to intended (or at least foreseen)\nharms and thus may be more appropriately covered with more targeted\nregulation.121212Indeed, intentionally creating dangerous narrow models\nshould already be covered by various laws and regulators. To the extent that it\nis not clearly covered, modification of those existing laws and regulations\nwould be appropriate and urgent. Further, the difference in mental state of the\ndeveloper makes it much easier to identify and impose liability on developers\nof narrower dangerous models. Our definition focuses on models that could — rather than just those\nthat do — possess dangerous capabilities, as many of the practices we\npropose apply before it is known that a model has dangerous capabilities. One\napproach to identifying models that could possess such capabilities is focusing on\nfoundation models that advance the state-of-the-art of foundation model\ncapabilities. While currently deployed foundation models pose risks\n , they do not yet appear to\npossess dangerous capabilities that pose severe risks to public safety as we\nhave defined them.131313In some cases, these have been explicitly tested\nfor . Given both our  inability to reliably predict\nwhat models will have sufficiently dangerous capabilities and the\nalready significant capabilities today’s models possess, it would be prudent\nfor regulators to assume that next-generation state-of-the-art foundation\nmodels could possess advanced enough capabilities to warrant\nregulation.141414We think it is prudent to anticipate that foundation\nmodels’ capabilities may advance much more quickly than many expect, as has\narguably been the case for many AI capabilities: “[P]rogress on ML benchmarks\nhappened significantly faster than forecasters expected. But forecasters\npredicted faster progress than I did personally, and my sense is that I expect\nsomewhat faster progress than the median ML researcher does.”\n; See  at 9;\n at 11 (Chinchilla and Gopher surpassing forecaster\npredictions for progress on MMLU); [ref]36 (GPT-4 surpassing\nGopher and Chinchilla on MMLU, also well ahead of forecaster predictions);\n   . An\ninitial way to identify potential state-of-the-art foundation models could be\nfocusing on models trained using above some very large amount of computational\nresources.151515Perhaps more than any model that has been trained to date.\nEstimates suggest that 1E26 floating point operations (FLOP) would meet this\ncriteria . Over time, the scope of frontier AI should be further refined. The scope\nshould be sensitive to features other than compute; state-of-the-art\nperformance can be achieved by using high quality data and new algorithmic\ninsights. Further, as systems with sufficiently dangerous capabilities are\nidentified, it will be possible to identify training runs that are likely to\nproduce such capabilities despite not achieving state-of-the-art performance. We acknowledge that our proposed definition is lacking in sufficient precision\nto be used for regulatory purposes and that more work is required to fully\nassess the advantages and limitations of different approaches. Further, it is\nnot our role to determine exactly what should fall within the scope of the\nregulatory proposals outlined – this will require more analysis and input from\na wider range of actors. Rather, the aim of this paper is to present a set of\ninitial proposals which we believe should apply to at least some subset of AI\ndevelopment. We provide a more detailed description of alternative approaches\nand the general complexity of defining “frontier AI” in  Appendix A.\n\nReference markers found: [24]\n\nContext windows for each [ref]:\n[ref]24: foundation models222\ndefines “foundation models” as “models (e.g., BERT, DALL-E, GPT-3) that are\ntrained on broad data at scale and are adaptable to a wide range of downstream\ntasks.” See also [ref]24. that could exhibit sufficiently dangerous capabilities.\nSuch harms could take the form of significant physical harm or the disruption\nof key societal functions on a global scale, resulting\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 24,\n            \"titles\": [\n                \"Model evaluation for extreme risks\",\n                \"Foundation models for generalist medical artificial intelligence\",\n                \"Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine\",\n                \"Large Language Models Encode Clinical Knowledge\",\n                \"Capabilities of GPT-4 on Medical Challenge Problems\",\n                \"Access to A.I. Justice: Avoiding an Inequitable Two-Tiered System of Legal Services\",\n                \"Contracts in the Age of Smart Readers\",\n                \"Predicting Consumer Contracts\",\n                \"Harnessing GPT-4 so that all students benefit\",\n                \"Tackling Climate Change with Machine Learning\",\n                \"DeepMind AI Reduces Google Data Centre Cooling Bill by 40%\",\n                \"De-risking Carbon Capture and Sequestration with Explainable CO2 Leakage Detection in Time-lapse Seismic Monitoring Images\",\n                \"Machine Learning for Sustainable Energy Systems\",\n                \"The medical and societal impact of big data analytics and artificial intelligence applications in combating pandemics: A review focused on Covid-19\",\n                \"The Challenges and Opportunities in Creating an Early Warning System for Global Pandemics\",\n                \"On the Opportunities and Risks of Foundation Models\",\n                \"Dual use of artificial-intelligence-powered drug discovery\",\n                \"The Alignment Problem from a Deep Learning Perspective\",\n                \"Advanced artificial agents intervene in the provision of reward\",\n                \"Unsolved Problems in ML Safety\",\n                \"X-Risk Analysis for AI Research\",\n                \"Is Power-Seeking AI an Existential Risk?\",\n                \"Human Compatible\",\n                \"An alignment problem\",\n                \"Proposal for a regulation of the European Parliament and of the Council on harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts\",\n                \"Thinking About Risks From AI: Accidents, Misuse and Structure\",\n                \"Emergent autonomous scientific research capabilities of large language models\",\n                \"On the Horizon: Interactive and Compositional Deepfakes\",\n                \"Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations\",\n                \"Truth, Lies, and Automation: How Language Models Could Change Disinformation\",\n                \"AI-assisted coding: Experiments with GPT-4\",\n                \"Will AI Make Cyber Swords or Shields?\",\n                \"What are Tokens?\",\n                \"Language Models are Unsupervised Multitask Learners\",\n                \"Language Models are Few-Shot Learners\",\n                \"GPT-4 Technical Report\",\n                \"PaLM: Scaling Language Modeling with Pathways\",\n                \"Flamingo: a Visual Language Model for Few-Shot Learning\",\n                \"Reponsible AI Licenses\",\n                \"The Open Source Definition\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]24 based on the context and research area of Frontier AI Regulation:Managing Emerging Risks to Public Safety. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}