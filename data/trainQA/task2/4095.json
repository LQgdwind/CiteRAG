{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Memory-efficient model-based deep learning with convergence and robustness guarantees\n\nPaper Abstract: \n\nPaper Section Text:\nThe recovery of images from a few noisy measurements is a common problem in several imaging modalities, including MRI , CT , PET , and microscopy . In the undersampled setting, multiple images can give a similar fit to the measured data, making the recovery ill-posed. Compressive sensing (CS) algorithms pose the recovery as a convex optimization problem, where a strongly convex prior is added to the data-consistency term to regularize the recovery . The main benefit of convex priors is in the uniqueness of the solutions. In particular, the strong convexity of the priors guarantees that the overall cost function in (3) is strongly convex, even when ğ€ğ€\\mathbf{A} operator has a large null space. Another desirable property of convex priors in CS is the robustness of the solution to input perturbations. In recent years, several flavors of model-based deep learning algorithms, which combine imaging physics with learned priors, were introduced to significantly improve the performance compared to CS algorithms. For example, plug and play (PnP) methods use denoiser modules to replace the proximal mapping steps in CS algorithms    [ref]9 0 1, and the algorithms are run until convergence. While earlier approaches chose off-the-shelf denoisers such as BM3D 2, recent methods use pre-trained convolutional neural network (CNN) modules [ref]9 0. The pre-trained CNN modules that learn the image prior are agnostic to the forward model, which enables their use in arbitrary inverse problems. These methods come with convergence and uniqueness guarantees when the forward model is full-rank or the data term is strongly convex 3. When the data term is not strongly convex, weaker convergence guarantees are available [ref]9, but uniqueness is not guaranteed.\nAnother category of approaches relies on unrolled optimization; these algorithms unroll finite number of iterative optimization steps in CS algorithms to obtain a deep network, which is composed of CNN blocks and optimization blocks to enforce data consistency; the resulting deep network is trained in an end-to-end fashion 4 5 6 7. A key difference between unrolled methods and PnP methods is that the CNN block is trained end-to-end, assuming a specific forward model; such model-based methods typically offer better performance than PnP methods that are agnostic to the forward model 4 5 8 6 9 7 0. Unlike PnP approaches that run the algorithm until convergence, the number of iterations in unrolled methods are restricted by the memory of the GPU devices during training; this often limits the applicability of unrolled algorithms to large-scale multi-dimensional problems. Several strategies were introduced to overcome the memory limitations of unrolled methods. For an unrolled network with Nğ‘N iterations and shared CNN modules across iterations, the computational complexity and memory demand of backpropagation are ğ’ªâ€‹(N)ğ’ªğ‘\\mathcal{O}(N) and ğ’ªâ€‹(N)ğ’ªğ‘\\mathcal{O}(N), respectively. The forward steps can be recomputed during backpropagation, which reduces the memory demand to ğ’ªâ€‹(1)ğ’ª1\\mathcal{O}(1), while the computational complexity increases to ğ’ªâ€‹(N2)ğ’ªsuperscriptğ‘2\\mathcal{O}(N^{2}). Forward checkpointing 1 saves the variables for every Kğ¾K layers during forward propagation, which reduces the computational demand to ğ’ªâ€‹(Nâ€‹K)ğ’ªğ‘ğ¾\\mathcal{O}(NK), while the memory demand is ğ’ªâ€‹(N/K)ğ’ªğ‘ğ¾\\mathcal{O}(N/K). Reverse recalculation has been proposed to reduce the memory demand to ğ’ªâ€‹(1)ğ’ª1\\mathcal{O}(1) and computational complexity to ğ’ªâ€‹(N)ğ’ªğ‘\\mathcal{O}(N) 2. However, the approach in 2 requires multiple iterations to invert each CNN block, resulting in high computational complexity in practical applications. Gilton et al. recently extended the deep equilibrium (DEQ) model 3 to significantly improve the memory demand 4 of unrolled methods. Unlike unrolled methods, DEQ schemes run the iterations until convergence, similar to PnP algorithms. This property allows one to perform forward and backward propagation using fixed-point iteration involving a single physical layer, which reduces the memory demand to ğ’ªâ€‹(1)ğ’ª1\\mathcal{O}(1), while the computational complexity is ğ’ªâ€‹(N)ğ’ªğ‘\\mathcal{O}(N); this offers better tradeoffs than the alternatives discussed above 1 2. The runtime of DEQ methods that are iterated until convergence are variable compared to unrolled methods, which use a finite number of iterations. In addition, the convergence of the iterative algorithm is crucial for the accuracy of backpropagation steps in DEQ, unlike in unrolled methods. Convergence guarantees were introduced in 0 4 for the alternating direction method of multipliers (ADMM), proximal gradient (PG), and forward-backward DEQ algorithms. The convergence guarantees rely on restrictive conditions on the CNN denoising blocks, which are dependent on the forward models. Unfortunately, when the minimum singular value of the forward operator is small (e.g., highly accelerated parallel MRI) or zero (e.g., super-resolution), the CNN denoiser needs to be close to an identity operator for the iterations to converge. Another challenge associated with DEQ methods is the way the non-expansive constraints on the network are imposed. Most methods 0 4 use spectral normalization of each layer of the network. Our experiments in Fig. 3 show that spectral normalization often translates to networks with lower performance. Another theoretical problem associated with current DEQ methods is the potential non-uniqueness of the fixed point, which can also affect the stability/robustness of the algorithm in the presence of input perturbations.\nWe note that the stability of deep image reconstruction networks is a debated topic. While deep networks are reported to be more fragile to input perturbations than are conventional algorithms 5, some of the recent works have presented a more optimistic view 6 7. The main goal of this work is to introduce a model-based DEQ algorithm that shares the desirable properties of convex CS algorithms, including guaranteed uniqueness of the fixed point solutions, convergence, and robustness to input perturbations. By enabling the training of the CNN modules in an end-to-end fashion, the proposed algorithm can match the performance of unrolled approaches while being significantly more memory efficient. The main contributions of this paper are: We introduce a forward-backward DEQ algorithm (14) involving a learned network â„±â„±\\mathcal{F}. Existing algorithms  6 8 such as MoDL and RED are special cases of this algorithm when the damping parameter Î±=1ğ›¼1\\alpha=1. We show that constraining the CNN module as an m>0ğ‘š0m>0 monotone operator is necessary and sufficient to guarantee the uniqueness of the fixed point of the algorithm. Because the monotone constraint is central to our approach, we term the proposed scheme as the monotone operator learning (MOL) algorithm. We show that an mğ‘šm-monotone operator â„±â„±\\mathcal{F} can be realized as a residual CNN: â„±=â„âˆ’â„‹Î¸â„±â„subscriptâ„‹ğœƒ\\mathcal{F}=\\mathcal{I}-\\mathcal{H}_{\\theta}, where the Lipschitz constant of the denoiser module â„‹Î¸subscriptâ„‹ğœƒ\\mathcal{H}_{\\theta} is Lâ€‹[â„‹Î¸]=1âˆ’mğ¿delimited-[]subscriptâ„‹ğœƒ1ğ‘šL[\\mathcal{H}_{\\theta}]=1-m. We also determine the range of values of Î±ğ›¼\\alpha and Lâ€‹[â„‹Î¸]ğ¿delimited-[]subscriptâ„‹ğœƒL[\\mathcal{H}_{\\theta}] for which the algorithm converges; the analysis and the experiments in Fig. 3 show that the direct application of the MoDL and RED (Î±=1ğ›¼1\\alpha=1) algorithms to the DEQ setting will diverge unless a highly constrained CNN (Lâ€‹[â„‹Î¸]<0.24ğ¿delimited-[]subscriptâ„‹ğœƒ0.24L[\\mathcal{H}_{\\theta}]<0.24) is used, which restricts performance. By contrast, the use of a smaller Î±ğ›¼\\alpha translates to higher Lâ€‹[â„‹Î¸]ğ¿delimited-[]subscriptâ„‹ğœƒL[\\mathcal{H}_{\\theta}] and hence improved performance. We theoretically analyze the worst-case sensitivity of the the resulting DEQ scheme. Our analysis shows that the norm of the perturbations in the reconstructed images are linearly proportional to the norm of the measurement perturbations, with the proportionality dependent on 1/m1ğ‘š1/m. We introduce two implementations of the proposed MOL algorithm. The first approach uses spectral normalization to enforce the monotone constraint in the strict sense. We also introduce an approximate implementation, where we replace Lâ€‹[â„‹Î¸]ğ¿delimited-[]subscriptâ„‹ğœƒL[\\mathcal{H}_{\\theta}] by an approximation lâ€‹[â„‹Î¸]ğ‘™delimited-[]subscriptâ„‹ğœƒl[\\mathcal{H}_{\\theta}]. While the second approach does not satisfy the monotone constraint in the strict sense, our experiments in Fig. 3 shows that the resulting algorithm converges, while Fig. 6 shows that the robustness of both schemes to adversarial and Gaussian noise are comparable. We note that spectral normalization based estimate for Lipschitz constant is very conservative; our experiments in Fig. 4 show that the second approach offers improved performance over the exact approach. We experimentally compare the performance against unrolled algorithms that use similar-sized CNNs in two-dimensional MR imaging problems. Our results show that the performance of the MOL scheme is comparable to that of unrolled algorithms. In addition, the MOL scheme is associated with a ten-fold reduction in memory compared to the unrolling algorithms with ten unrolls. The significant gain in memory demand allows us to extend our algorithm to the 3D or 2D+time setting, where it offers improved performance over unrolled 2D approaches. The experimental results in Fig. 6 and 7 show the improved robustness of the proposed scheme compared to existing unrolled algorithms 6 9 and UNET 9. The recorded run-times in Table I show that MOL has higher computational complexity (â‰ˆ\\approx 2.5 times) compared to unrolling algorithms due to more iterations, when compared with fixed number of unrolls in the latter. Our experiments in Fig. 6 show that the increased computational complexity translate to an improvement in robustness performance over unrolled algorithms, when Lipschitz regularization is applied on the networks.\n\nReference markers found: [9]\n\nContext windows for each [ref]:\n[ref]9: introduced to significantly improve the performance compared to CS algorithms. For example, plug and play (PnP) methods use denoiser modules to replace the proximal mapping steps in CS algorithms    [ref]9 0 1, and the algorithms are run until convergence. While earlier approaches chose off-the-shelf denoisers such as BM3D 2, recent methods use pre-trained convolutional neural network (CNN)\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 9,\n            \"titles\": [\n                \"Plug-and-Play Methods Provably Converge with Properly Trained Denoisers\",\n                \"Model-based image reconstruction for MRI\",\n                \"Statistical image reconstruction for polyenergetic X-ray computed tomography\",\n                \"Dynamic PET reconstruction using wavelet regularization with adapted basis functions\",\n                \"Model-based 2.5-D deconvolution for extended depth of field in brightfield microscopy\",\n                \"Sparse MRI: The application of compressed sensing for rapid MR imaging\",\n                \"Plug-and-play priors for model based reconstruction\",\n                \"Plug-and-play unplugged: Optimization-free reconstruction using consensus equilibrium\",\n                \"The Little Engine that Could Regularization by Denoising (RED)\",\n                \"An Online Plug-and-Play Algorithm for Regularized Image Reconstruction\",\n                \"Scalable Plug-and-Play ADMM with Convergence Guarantees\",\n                \"BM3D image denoising with shape-adaptive principal component analysis\",\n                \"Plug-and-Play Methods for Integrating Physical and Learned Models in Computational Imaging: Theory, algorithms, and applications\",\n                \"Learning fast approximations of sparse coding\",\n                \"Learning a Variational Network for Reconstruction of Accelerated MRI Data\",\n                \"MoDL: Model-based deep learning architecture for inverse problems\",\n                \"FISTA-net: Learning a fast iterative shrinkage thresholding network for inverse problems in imaging\",\n                \"A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image Reconstruction\",\n                \"Deep ADMM-Net for compressive sensing MRI\",\n                \"Deep learning techniques for inverse problems in imaging\",\n                \"Training deep nets with sublinear memory cost\",\n                \"Memory-efficient Learning for Large-scale Computational Imaging\",\n                \"Deep Equilibrium Models\",\n                \"Deep Equilibrium Architectures for Inverse Problems in Imaging\",\n                \"On instabilities of deep learning in image reconstruction and the potential costs of AI\",\n                \"Measuring robustness in deep learning based compressive sensing\",\n                \"Solving Inverse Problems With Deep Neural Networks â€“ Robustness Included?\",\n                \"Machine learning for image reconstruction\",\n                \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n                \"A fast alternating direction method for TVL1-L2 signal reconstruction from partial Fourier data\",\n                \"An iterative thresholding algorithm for linear inverse problems with a sparsity constraint\",\n                \"A fast iterative shrinkage-thresholding algorithm for linear inverse problems\",\n                \"ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing\",\n                \"Primer on monotone operator methods\",\n                \"Spectral Normalization for Generative Adversarial Networks\",\n                \"CLIP: Cheap Lipschitz training of neural networks\",\n                \"Learning maximally monotone operators for image recovery\",\n                \"Lipschitz constant estimation of Neural Networks via sparse polynomial optimization\",\n                \"Exactly Computing the Local Lipschitz Constant of ReLU networks\",\n                \"SENSE: sensitivity encoding for fast MRI\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]9 based on the context and research area of Memory-efficient model-based deep learning with convergence and robustness guarantees. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}