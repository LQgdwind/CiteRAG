{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling\n\nPaper Abstract: \n\nPaper Section Text:\nA knowledge graph (KG) is a heterogeneous graph that includes nodes representing different entities (𝒱𝒱\\mathcal{V}) of |𝒞|𝒞|\\mathcal{C}| classes, e.g., Paper and Author and edge types representing |ℛ|ℛ|\\mathcal{R}| relations, e.g., publishedIn and write, between these entities. 𝒞𝒞\\mathcal{C} and ℛℛ\\mathcal{R} are the sets of node and edge types in the KG, respectively. Real-world KGs, such as Yago  and Wikidata , have hundreds to thousands of node/edge types.\nHeterogeneous graph neural networks (HGNNs) have emerged as a powerful tool for analyzing KGs by defining real-world problems as node classification, such as recommender systems , and entity alignment , or link prediction, such as drug discovery  and fraud detection  tasks. HGNN methods adopt mechanisms from GNNs designed for homogeneous graphs, such as multi-layer graph convolutional networks (GCNs) and sampling in mini-batch training [ref]7 [ref]8  0 1 2.\nUnlike GNNs on homogeneous graphs, the number of GCN layers in HGNNs is associated with the number of edge types or metapaths in the heterogeneous graph to model the distinct relationships effectively. A metapath captures distinct semantic relationships (edge types) between node types. For example, the metapath Author-(write)-Paper-(publishedIn)-Venue (APV) describes the semantic relationships of an author who published a paper in a particular venue.\nIn mini-batch HGNN training, the incorporated sampling mechanisms aim to obtain a subgraph (a subset of nodes and edges) that captures the representative structure of the original KG in general. The sampling is performed without considering node and edge types, where some specific types may have negligible or no effect in training a particular task. For example, a DBPedia 111The DBPedia KG contains information about movies and academic work. subgraph, including instances of APV does not contribute to training a model predicting a movie genre. Hence, HGNN methods are not optimized for identifying subgraphs of a smaller subset of types to reduce the number of GCN layers in training a specific task. The space complexity of training HGNNs includes the memory required to store the graph structures, associated features, and multi-layer GCNs. Additionally, the time complexity includes the sampling process, in the case of mini-batch training, and the message-passing iterations needed to aggregate the embeddings of neighbouring nodes in each GCN.\nThat leads to computationally expensive memory usage and training time with large KGs, where the complexity 222The actual complexity varies depending on the specific implementation. is influenced by the KG’s size, density, |ℛ|ℛ|\\mathcal{R}|, and |𝒞|𝒞|\\mathcal{C}| 3 [ref]7 [ref]8.\nTo alleviates the excessive computation on a large KG, AI practitioners manually handcraft a subgraph of a subset of ℛℛ\\mathcal{R} and 𝒞𝒞\\mathcal{C} from a given KG for training HGNNs 4 5. We refer to this subgraph as a task-oriented subgraph (TOSG). Training the task using TOSG instead of the original KG helps HGNN methods reduce training time and memory usage. For example, the open graph benchmark (OGB) 6 4 provided OGBN-MAG, a subgraph of 21M triples out of 8.4B triples ≈0.2%absentpercent0.2\\approx 0.2\\% that is extracted from the Microsoft Academic Graph (MAG) 7 and is relevant to the node classification task for predicting a paper venue (P​V𝑃𝑉PV). OGBN-MAG contains nodes and edges of only four types. To demonstrate the effectiveness of this methodology,\nwe trained the P​V𝑃𝑉PV task on a super-set of OGBN-MAG that contains 166M triples ≈2%absentpercent2\\approx 2\\% of MAG KG with 42M vertices, 58 node types and 62 edge types; we refer to it as (M​A​G−42​M𝑀𝐴𝐺42𝑀MAG-42M). The O​G​B​N𝑂𝐺𝐵𝑁OGBN−-M​A​G𝑀𝐴𝐺MAG 4 is a subgraph of M​A​G𝑀𝐴𝐺MAG−-42​M42𝑀42M relevant to P​V𝑃𝑉PV, i.e., task-oriented subgraph for P​V𝑃𝑉PV.\nWe used the state-of-the-art GNN methods, namely ShaDowSAINT [ref]8 and SeHGNN  3. As shown in Figure 1, O​G​B​N𝑂𝐺𝐵𝑁OGBN−-M​A​G𝑀𝐴𝐺MAG trades the accuracy to help both methods reduce time and memory usage in training the task.\nFurthermore, crafting the TOSG demands a deep understanding of the KG’s structure and the task’s objectives. Hence, the TOSG extraction is a challenging and time-consuming process and does not guarantee to improve the model performance. This paper proposes KG-TOSA 333\nKG-TOSA stands for KG Task Oriented Sampling. It is available at https://github.com/CoDS-GCS/KGTOSA\n, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that effectively captures both the local and global structure of a KG for a specific task. Our definition is grounded in the theoretical foundation of training HGNNs and considering crucial factors, such as data sufficiency and graph topology.\nThe objective of our graph pattern is to maximize the diversity of neighbour node types while minimizing the average depth between non-target and target vertices.\nWe preserve the local context by identifying an initial set of nodes targeted by the task. Then, we expand our selection to include neighbouring nodes within a specified distance of hℎh hops. Our graph pattern interlinks the local context around each target node to generate a larger subgraph of node and edge types globally related to the task. This approach allows us to effectively capture the intricate relationships and dependencies between different entities in the KG to enhance the representation of the task-specific knowledge captured in the TOSG. We explore different techniques to extract subgraphs matching our graph pattern. Two techniques sample around targeted nodes using biased random walk or influence scores. We also propose a SPARQL-based extraction method leveraging RDF engines’ built-in indices. Our biased random walk sampling performs random walks starting from nodes that match the target vertices in the task. In our influence-based sampling approach, we utilize a Personalized PageRank (PPR) score to measure the relevance or importance of nodes in the KG to the task. However, it is essential to note that these sampling methods can incur a significant extraction overhead, particularly with a large KG.\nThis excessive overhead may overshadow the potential time savings in training using the TOSG generated by both sampling methods instead of the original KG. To mitigate this overhead, we propose a SPARQL-based method that leverages built-in indices in RDF engines. Hence, our SPARQL-based method automates the TOSG extraction with negligible preprocessing overhead.\nFigure 1 motivates the need for our KG-TOSA approach, which helps both ShaDowSAINT and SeHGNN (i) reduce the overall maintenance cost of the multi-layer GCNs, and (ii) improve the modelling performance, including accuracy and inference time. For evaluating GNN methods on node classification and link prediction tasks, existing benchmarks, such as OGB 4, OGB-LSC 6, and Open-HGNN 8, include KGs of few node/edge types. Unlike these benchmarks, we develop a benchmark of real KGs of large sizes, up to hundred millions of triples and hundreds of node and edge types with various tasks for node classification and link prediction. Hence, our benchmark presents challenging settings for the state-of-the-art (SOTA) HGNN methods.\nOur benchmark encompasses KGs from the academic domain, such as MAG 7 and DBLP 9, as well as two versions of YAGO as a general-purpose KG of different sizes, namely YAGO-4  and YAGO-3 0.\nWe integrated KG-TOSA with SOTA GNN methods, namely SeHGNN 3, GraphSAINT [ref]7, ShaDowSAINT [ref]8, RGCN 1, MorsE 1 , and LHGNN 2. Our comprehensive evaluation using large real KGs and various ML tasks shows that KG-TOSA enables GNN methods to converge faster and use less memory for training. Moreover, KG-TOSA empowers GNN methods to enhance and attain comparable levels of accuracy. In summary, our contributions are: the first approach (KG-TOSA) for task-oriented HGNN training based on our generic graph pattern identifying a KG’s local and global context related to a specific task, Section III.  three mechanisms for automating the TOSG extraction using our generic graph pattern based on task-oriented sampling and SPARQL-based query extraction. Our SPARQL-based method outperforms the task-oriented sampling in time and memory usage, Section IV. a comprehensive evaluation using our KG benchmark and six SOTA GNN methods. Our KG benchmark includes four real KGs, six node classification tasks, and three link-prediction tasks. The KGs have up to 42 million nodes and 400 million edges. KG-TOSA achieves up to 70% memory and time reduction while improving the model performance for better accuracy, smaller model size, and less inference time, Section V,\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: and fraud detection  tasks. HGNN methods adopt mechanisms from GNNs designed for homogeneous graphs, such as multi-layer graph convolutional networks (GCNs) and sampling in mini-batch training [ref]7 [ref]8  0 1 2.\nUnlike GNNs on homogeneous graphs, the number of GCN layers in HGNNs is associated with the number of edge types or metapaths in the heterogeneous graph to model the distinct\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Decoupling the Depth and Scope of Graph Neural Networks\",\n                \"YAGO 4: A reason-able knowledge base\",\n                \"Wikidata: a free collaborative knowledge base\",\n                \"Graph Neural Networks in Recommender Systems: A Survey\",\n                \"Cross-lingual knowledge graph alignment via graph convolutional networks\",\n                \"Kgnn: Knowledge graph neural network for drug-drug interaction prediction\",\n                \"Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters\",\n                \"Graphsaint: Graph sampling based inductive learning method\",\n                \"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\",\n                \"Multiplex heterogeneous graph convolutional network\",\n                \"Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding\",\n                \"Link Prediction on Latent Heterogeneous Graphs\",\n                \"Simple and Efficient Heterogeneous Graph Neural Network\",\n                \"Open Graph Benchmark: Datasets for Machine Learning on Graphs\",\n                \"Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks\",\n                \"OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs\",\n                \"The microsoft academic knowledge graph: A linked data source with 8 billion triples of scholarly data\",\n                \"A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources\",\n                \"dblp in rdf\",\n                \"Convolutional 2D Knowledge Graph Embeddings\",\n                \"Modeling relational data with graph convolutional networks\",\n                \"Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs\",\n                \"Pathsim: Meta path-based top-k similarity search in heterogeneous information networks\",\n                \"Sampling methods for efficient training of graph convolutional networks: A survey\",\n                \"Influence-Based Mini-Batching for Graph Neural Networks\",\n                \"Heterogeneous Graph Transformer\",\n                \"Scalable Graph Neural Networks for Heterogeneous Graphs\",\n                \"Graph Neural Networks: Methods, Applications, and Opportunities\",\n                \"Graph neural networks in node classification: survey and evaluation\",\n                \"A survey on knowledge graph embeddings for link prediction\",\n                \"Openhgnn: An open source toolkit for heterogeneous graph neural network\",\n                \"Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments\",\n                \"Heterogeneous graph neural network\",\n                \"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\",\n                \"Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View\",\n                \"Towards powerful graph neural networks: Diversity matters\",\n                \"A survey of information entropy metrics for complex networks\",\n                \"Local graph partitioning using pagerank vectors\",\n                \"Towards a GML-Enabled Knowledge Graph Platform\",\n                \"Demonstration of sparqlml: An interfacing language for supporting graph machine learning for rdf graphs\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}