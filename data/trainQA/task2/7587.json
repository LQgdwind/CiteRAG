{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Neural Codebook Design for Network Beam Management\n\nPaper Abstract: \n\nPaper Section Text:\nIn this paper, we propose a gradient-complete multi-cell MU-MIMO codebook optimization strategy. We previously found that neural codebook design based on beamspace representations is a powerful tool for codebook design . Training the architecture to maximize beam training power was also effective [ref]7, but ultimately focused on an intermediate goal–beam training power–rather than the network goal of spectral efficiency maximization. Conceptually, in this paper, we reframe the learning objective to maximize the sum spectral efficiency in the presence of multi-user and multi-cell interference. In this section, we define the end-to-end neural architecture as shown in Figure 3, beginning with the beamspace preprocessing and including up through codebook generation. We define three important aspects of the neural architecture design process that fundamentally aid the NBL algorithm. Design aspect 1: Encapsulating the beam management framework within the end-to-end learning produces NBL codebooks that maximize system-level performance.\nLearning SE maximizing codebooks requires more than simply training the neural network to predict a specific set of beamformers in a typical supervised fashion. Instead, we capture the beam management process as described in Section II in the backpropagation step. Then we train the neural network to maximize the achievable rate during CSI-RS based on (19). Ultimately, because of the reliance on the CSI-RS codebook for analog beamforming in the data transmission, the codebooks used during beam training should support high data rates rather than simply rank-111 transmissions. The first step in the neural processing pipeline is converting the previous SSB Codebook into a beamspace representation  [ref]7. The beamspace representation provides a consistent learning paradigm where the SSB feedback corresponds to an “image” of the beamforming magnitude over a range of azimuth and elevation directions. In addition to setting up the learning paradigm, the beamspace representation also allows the neural architecture to generalize to any antenna size or number of users by separating those dimensions from the inputs and outputs of the neural network. To be precise, neural networks are often limited by requiring consistent dimensions for inputs and outputs but we circumvent this through the beamspace representation which allows for arbitrarily sizing the input “image” to sampling dimensions NX,O×NY,Osubscript𝑁X,Osubscript𝑁Y,ON_{\\text{X,O}}\\times N_{\\text{Y,O}} in the azimuth and elevation directions for each beamformer. The only requirement is that NX,O≥NXsubscript𝑁X,Osubscript𝑁XN_{\\text{X,O}}\\geq N_{\\text{X}} and NY,O≥NYsubscript𝑁Y,Osubscript𝑁YN_{\\text{Y,O}}\\geq N_{\\text{Y}} to ensure the beamspace is not aliased. The beamspace transformation can be performed at each cell and shared with local cells or a small centralized node during each SSB period (≈20absent20\\approx 20ms) to form a weakly-centralized setting (see Figure 3). The centralization or SSB sharing is motivated by the long SSB periods as well as only sharing the SSB feedback between cells, rather than large packets of information. We define the beamspace representation for each cell c𝑐c as 𝐎cBSC∈ℂ(NX,O+1)×(NY,O+1)×(Lmax)subscriptsuperscript𝐎BSC𝑐superscriptℂsubscript𝑁X,O1subscript𝑁Y,O1subscript𝐿max\\mathbf{O}^{\\text{BSC}}_{c}\\in\\mathbb{C}^{(N_{\\text{X,O}}+1)\\times(N_{\\text{Y,O}}+1)\\times(L_{\\text{{max}}})} formulated in the same way as [ref]7. First, transformation matrices 𝐔N1,N2subscript𝐔subscript𝑁1subscript𝑁2\\mathbf{U}_{N_{1},N_{2}} are calculated from the array responses 𝐚N​(θ)subscript𝐚𝑁𝜃\\mathbf{a}_{N}(\\theta)  for antenna dimension N1subscript𝑁1N_{1} to beamspace sampling dimension N2subscript𝑁2N_{2} Then the beamspace representation is Note that in a centralized scenario, the codebook generation is carried out at a shared compute node and shared with each cell which in turn share the SSB feedback as shown in Figure 3. In the event of a fully disaggregated scenario, the interfering cells cannot be jointly optimized, the nearby cell’s beamspace representations are not shared, and interference instead is treated as noise during the gradient updates. Design aspect 2: Beamspace representations enable generalization in dynamic codebook learning algorirthms.\nThe beamspace preprocessing forms a consistent basis that abstracts the carrier frequency and antenna geometry from the neural network. The abstraction is performed by the beamspace conversion that incorporates the array response, which is a function of the operating frequency and geometry. The beamspace representation allows for the same neural network to be used in a range of deployment strategies for network operators [ref]7. An evaluation of the generalization capabilities is presented in Section VI. After computing the beamspaces for each cell, the concatenated 𝐎BSCsuperscript𝐎BSC\\mathbf{O}^{\\text{BSC}} is fed into a 3+3333+3 layer convolutional autoencoder architecture with (128,128,256128128256128,128,256) filters and 2×2222\\times 2 max pooling between layers. In contrast to our prior work [ref]7, the final 333 layers perform an inverse convolution (typically termed convolution-transpose in deep learning frameworks) instead of using fully connected layers due to the large dimensional outputs which would require significant hardware resources (more than a single GPU) to parameterize in a fully connected architecture. By employing parameter sharing during the 333 encoding and 333 decoding layers, the network only requires ≈2absent2\\approx 2 million parameters for the joint SSB and CSI-RS prediction for all cells. The output of the neural network is a predicted beamspace which is then converted back to the complex codebook entries for the next beam training period. The deconversion process is performed in the same way as the conversion process using the matrix inverses of (28). With the new codebooks 𝐅^SSB,𝐅^CSI-RSsuperscript^𝐅SSBsuperscript^𝐅CSI-RS\\widehat{\\mathbf{F}}^{\\text{SSB}},\\widehat{\\mathbf{F}}^{\\text{CSI-RS}} we can introduce the end-to-end learning strategy for rate-maximized codebook learning. Ultimately, the end-to-end strategy allows for learning complex and arbitrary codebooks that minimize the loss in achievable rate from a perfect CSI, SU-MIMO, interference-free setting. First, the target “labels”, which are the maximum achievable spectral efficiencies, are computed from the channel assuming perfect CSI and SU-MIMO data transmission. The maximum achievable SE is calculated from the singular value decomposition (SVD) for a user u𝑢u with connected BS c𝑐c as For ease of notation, we focus on a single time-frequency resource t,k𝑡𝑘t,k, although the actual implementation is performed over an applicable set of resources. In a single-cell scenario, the system could then calculate the achievable spectral efficiency resulting from serving all active users with different analog precoder selections.\nWhen extending this to the multi-cell setting, however, it is pessimistic to assume that every analog precoder 𝐁subsuperscript𝐁sub\\mathbf{B}^{\\text{sub}} from every BS is active and causing interference. Furthermore, directly assuming all precoders cause interference ignores the timing aspects of beam management, where interference is acceptable during beams that a user does not end up selecting. In essence, the beams from nearby cells should cover non-overlapping regions at a given time. We capture this property by assuming the interference only occurs from the beams used by the interfering BS during the CSI-RS of the selected CSI-RS 𝐢^c,usubscript^𝐢𝑐𝑢\\widehat{\\mathbf{i}}_{c,u} as in (18). Mathematically the achievable spectral efficiency can therefore be calculated as (19). Design aspect 3: Incorporating realistic time-frequency usage within the training allows the algorithm to coordinate beamforming between interfering cells.\nInterference during CSI-RS causes channel estimation errors and CSI-RS beams may be allocated by users for the nearby cells. Assuming interference only during the user-selected CSI-RS resource frees the algorithm to learn analog precoders that do not interfere strongly during the same CSI-RS to improve channel estimation and subsequent data transmission. In a non-centralized format, the neural network would be limited to simply maximizing the signal strength, but this does not necessarily improve the network ESSE (see Figure 13 of Section VI). Then the overall loss function is the mean-squared-error (MSE) of the target and predicted achievable rates In an ideal scenario, the loss would approach 00 with perfect CSI and no noise. Of course, there is always non-zero noise and interference thereby causing the loss function to be strictly positive. Gradient calculation is backpropagated throughout the entire system model for the selected SSB and CSI-RS codewords.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: previously found that neural codebook design based on beamspace representations is a powerful tool for codebook design . Training the architecture to maximize beam training power was also effective [ref]7, but ultimately focused on an intermediate goal–beam training power–rather than the network goal of spectral efficiency maximization. Conceptually, in this paper, we reframe the learning\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Massive MIMO Beam Management in Sub-6 GHz 5G NR\",\n                \"Foundations of MIMO Communication\",\n                \"Massive MIMO in 5G: How beamforming, codebooks, and feedback enable larger arrays\",\n                \"Extreme massive MIMO for macro cell capacity boost in 5G-Advanced and 6G\",\n                \"An overview of signal processing techniques for millimeter wave MIMO systems\",\n                \"Hybrid Beamforming in Massive MIMO for Next-Generation Communication Technology\",\n                \"A tutorial on beam management for 3GPP NR at mmwave frequencies\",\n                \"Hierarchical ML Codebook Design for Extreme MIMO Beam Management\",\n                \"The Impact of Beamwidth on Temporal Channel Variation in Vehicular Channels and its Implications\",\n                \"A review of codebooks for CSI feedback in 5G New Radio and beyond\",\n                \"An overview of the 3GPP study on artificial intelligence for 5G New Radio\",\n                \"WI summary for WI Core part: NR MIMO evolution for downlink and uplink\",\n                \"Machine Learning Paradigms for Next-Generation Wireless Networks\",\n                \"Deep Learning in Mobile and Wireless Networking: A Survey\",\n                \"Six key challenges for beam management in 5.5G and 6G systems\",\n                \"Self-Tuning Sectorization: Deep Reinforcement Learning Meets Broadcast Beam Optimization\",\n                \"Learning site-specific probing beams for fast mmWave beam alignment\",\n                \"Limited Feedback on Measurements: Sharing a Codebook or a Generative Model?\",\n                \"CSI-PPPNet: A One-Sided One-for-All Deep Learning Framework for Massive MIMO CSI Feedback\",\n                \"Practical Denoising Autoencoder for CSI Feedback Without Clean Target in Massive MIMO Networks\",\n                \"Improving the Performance of R17 Type-II Codebook with Deep Learning\",\n                \"Beam codebook based beamforming protocol for multi-Gbps millimeter-wave WPAN systems\",\n                \"Channel estimation and hybrid precoding for millimeter wave cellular systems\",\n                \"Enhanced CSI feedback for FD-MIMO with beamformed CSI-RS in LTE-A Pro systems\",\n                \"Hierarchical Beam Alignment for Millimeter-Wave Communication Systems: A Deep Learning Approach\",\n                \"Deep learning for massive MIMO CSI feedback\",\n                \"Deep learning-based implicit CSI feedback in massive MIMO\",\n                \"Knowledge-driven Meta-learning for CSI feedback\",\n                \"Combining Reciprocity and CSI Feedback in MIMO Systems\",\n                \"Deep learning for hierarchical beam alignment in mmWave communication systems\",\n                \"An online context-aware machine learning algorithm for 5G mmWave vehicular communications\",\n                \"Millimeter Wave Vehicular Communication to Support Massive Automotive Sensing\",\n                \"Inverse Multipath Fingerprinting for Millimeter Wave V2I Beam Alignment\",\n                \"Learning and adaptation for millimeter-wave beam tracking and training: A dual timescale variational framework\",\n                \"Multi-Sector and Multi-Panel Performance in 5G mmWave Cellular Networks\",\n                \"PARAMOUNT: Towards generalizable deeP leARning for mmwAve beaM selectiOn using sUb-6GHz chaNnel measuremenTs\",\n                \"Deep learning-based beam management and interference coordination in dense mmWave networks\",\n                \"Decentralized Interference-Aware Codebook Learning in Millimeter Wave MIMO Systems\",\n                \"Machine learning codebook design for initial access and CSI type-II feedback in sub-6GHz 5G NR\",\n                \"Direct conversion of hybrid precoding and combining from full array architecture to subarray architecture for mmWave MIMO systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of Neural Codebook Design for Network Beam Management. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}