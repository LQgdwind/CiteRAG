{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Optimal Coding Theorems in Time-Bounded Kolmogorov Complexity\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we provide an informal overview of our proofs and techniques.\n Efficient Coding Theorem for 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} (Theorem 1).\nBreaking down the result into its components, Theorem 1 shows that for any polynomial-time sampler A𝐴A, there exist a probabilistic polynomial-time algorithm 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} and an algorithm 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} with the following properties: 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} on input an n𝑛n-bit string x𝑥x and δ𝛿\\delta (which estimates from below the probability with which A𝐴A samples x𝑥x), returns a codeword cxsubscript𝑐𝑥c_{x} of length log⁡(1/δ)+poly⁡(log⁡n)1𝛿poly𝑛\\log(1/\\delta)+\\operatorname{poly}(\\log n) such that 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} with probability ≥0.99absent0.99\\geq 0.99 reconstructs x𝑥x in time 1/δ⋅exp⁡(poly⁡(log⁡n))⋅1𝛿poly𝑛1/\\delta\\cdot\\exp(\\operatorname{poly}(\\log n)). Note that the probabilistic representation of x𝑥x certifying the 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} bound in Theorem 1 can be obtained from the codeword cxsubscript𝑐𝑥c_{x} and 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress}, and that obtaining a running time with a factor of (1/δ)1+o​(1)superscript1𝛿1𝑜1(1/\\delta)^{1+o(1)} is crucial in order to get a final 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} bound of the form (2+o​(1))⋅log⁡(1/δ)⋅2𝑜11𝛿(2+o(1))\\cdot\\log(1/\\delta). (Actually, 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} does not have to depend on A𝐴A, the 0.990.990.99 can be 1−ε1𝜀1-\\varepsilon for arbitrary ε>0𝜀0\\varepsilon>0, and the poly⁡(log⁡n)poly𝑛\\operatorname{poly}(\\log n) term is O​(log⁡n+log2⁡log⁡(1/δ))𝑂𝑛superscript21𝛿O(\\log n+\\log^{2}\\log(1/\\delta)), but we omit these details in our discussion). We explain what are the challenges in obtaining 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} and 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} and how they are overcome. We remark that the construction is different from the approaches described in 9. 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} can run the sampler K:=O​(1/δ)assign𝐾𝑂1𝛿K:=O(1/\\delta) times and obtain a list of elements S∗superscript𝑆S^{*} (the list of suspects) that with high probability contains x𝑥x. 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} has to provide information that allows 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} to prune S∗superscript𝑆S^{*} and find x𝑥x. Since the algorithms do not share randomness, 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} does not know S∗superscript𝑆S^{*}, and so compression has to work for any S⊆{0,1}n𝑆superscript01𝑛S\\subseteq\\{0,1\\}^{n} of size K𝐾K, only assuming that x∈S𝑥𝑆x\\in S. 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} can use a bipartite lossless expander graph G𝐺G, which is a graph with the property that any set S𝑆S of left nodes with size |S|≤K𝑆𝐾|S|\\leq K has at least (1−ε)​D​|S|1𝜀𝐷𝑆(1-\\varepsilon)D|S| neighbors, where D𝐷D is the left degree. Such graphs are called ((1−ε)​D,K)1𝜀𝐷𝐾((1-\\varepsilon)D,K) lossless expanders and they have numerous applications (see e.g.,   ). An extension of Hall’s matching theorem shows that for any set S𝑆S of K𝐾K left nodes, there is a matching that assigns to each x∈S𝑥𝑆x\\in S, (1−ε)​D1𝜀𝐷(1-\\varepsilon)D of its neighbors, so that no right node is assigned twice (i.e., the matching defines a subgraph with no collisions). 𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖢𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Compress} can just pick the codeword cxsubscript𝑐𝑥c_{x} to be one random neighbor of x𝑥x. Then, 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} can do the pruning of S∗superscript𝑆S^{*} as follows. Having S∗superscript𝑆S^{*} and cxsubscript𝑐𝑥c_{x}, it does the matching, and, since with probability 1−ε1𝜀1-\\varepsilon, cxsubscript𝑐𝑥c_{x} is only assigned to x𝑥x, 𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌\\mathsf{Decompress} can find x𝑥x. There is one problem though. The algorithms for maximum matching in general bipartite graphs\ndo not run in linear time (see  , and the references therein). Therefore, the decompression time would have a dependency on δ𝛿\\delta, which is\ntoo large for us. Fortunately, lossless expanders can be used to do “almost” matching faster. [ref]11 introduces invertible functions (see Definition 12) for the more demanding task in which the elements of S𝑆S appear one-by-one and the matching has to be done in the online manner. We do not need online matching, but we take advantage of the construction in [ref]11 to obtain a fast matching algorithm. It follows from [ref]11, that in a lossless expander it is possible to do a greedy-type of “almost” matching, which means that every left node in S𝑆S is matched to (1−ε)​D1𝜀𝐷(1-\\varepsilon)D of its neighbors (exactly what we need), but with poly⁡(log⁡n)poly𝑛\\operatorname{poly}(\\log n) collisions. The collisions can be eliminated with some additional standard hashing (see the discussion on page 13 for details). As we explain on page 13, this leads to decompression time K⋅D⋅poly⁡(n)⋅𝐾𝐷poly𝑛K\\cdot D\\cdot\\operatorname{poly}(n) and the length of the codeword cxsubscript𝑐𝑥c_{x} is log⁡|R|+|hash-code|𝑅hash-code\\log|R|+|\\text{hash-code}|, where R𝑅R is the right set of the lossless expander. To obtain our result, the degree D𝐷D has to be 2poly⁡(log⁡n)superscript2poly𝑛2^{\\operatorname{poly}(\\log n)} and |R|𝑅|R| has to be K⋅2poly⁡(log⁡n)⋅𝐾superscript2poly𝑛K\\cdot 2^{\\operatorname{poly}(\\log n)}. Building on results and techniques from , [ref]11 constructs a ((1−ε)​D,K)1𝜀𝐷𝐾((1-\\varepsilon)D,K) explicit lossless expander with left side {0,1}nsuperscript01𝑛\\{0,1\\}^{n}, degree D=2d𝐷superscript2𝑑D=2^{d} for d=O​(log⁡(n/ε)⋅log⁡k)𝑑𝑂⋅𝑛𝜀𝑘d=O(\\log(n/\\varepsilon)\\cdot\\log k), and right side R𝑅R, with size verifying log⁡|R|=k+log⁡(n/ε)⋅log⁡k𝑅𝑘⋅𝑛𝜀𝑘\\log|R|=k+\\log(n/\\varepsilon)\\cdot\\log k\n(where k:=logK)k:=\\log K). To obtain in Theorem 1 the dependency on n𝑛n to be O​(log⁡n)𝑂𝑛O(\\log n) (which is optimal up to the constant in O​(⋅)𝑂⋅O(\\cdot)), we show the existence of a ((1−ε)​D,K)1𝜀𝐷𝐾((1-\\varepsilon)D,K) explicit expander with d=O​(log⁡n+log⁡(k/ε)⋅log⁡k)𝑑𝑂𝑛⋅𝑘𝜀𝑘d=O(\\log n+\\log(k/\\varepsilon)\\cdot\\log k) and log⁡|R|=k+O​(log⁡n+log⁡(k/ε)⋅log⁡k)𝑅𝑘𝑂𝑛⋅𝑘𝜀𝑘\\log|R|=k+O(\\log n+\\log(k/\\varepsilon)\\cdot\\log k). This lossless expander is constructed by a simple composition of the above lossless expander from [ref]11 with a lossless expander from , with an appropriate choice of parameters (see Section 3.1.1).\n Conditional Lower Bound for Efficient Coding Theorems (Theorem 3). Our goal is to show that there is no efficient coding theorem for 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} that achieves bounds of the form (1+γ−o​(1))⋅log⁡(1/δ)+poly⁡(log⁡n)⋅1𝛾𝑜11𝛿poly𝑛(1+\\gamma-o(1))\\cdot\\log(1/\\delta)+\\operatorname{poly}(\\log n), under the assumption that γ𝛾\\gamma-𝖢𝗋𝗒𝗉𝗍𝗈𝖢𝗋𝗒𝗉𝗍𝗈\\mathsf{Crypto}-𝖤𝖳𝖧𝖤𝖳𝖧\\mathsf{ETH} holds for γ∈(0,1)𝛾01\\gamma\\in(0,1). We build on an idea attributed to L. Levin (see e.g. [26, Section 5.3]). To provide an overview of the argument, let Gn:{0,1}ℓ​(n)→{0,1}n:subscript𝐺𝑛→superscript01ℓ𝑛superscript01𝑛G_{n}\\colon\\{0,1\\}^{\\ell(n)}\\to\\{0,1\\}^{n} be a cryptographic generator of seed length ℓ​(n)=n/2ℓ𝑛𝑛2\\ell(n)=n/2 witnessing that γ𝛾\\gamma-𝖢𝗋𝗒𝗉𝗍𝗈𝖢𝗋𝗒𝗉𝗍𝗈\\mathsf{Crypto}-𝖤𝖳𝖧𝖤𝖳𝖧\\mathsf{ETH} holds. In other words, Gnsubscript𝐺𝑛G_{n} has security 2γ⋅ℓ​(n)superscript2⋅𝛾ℓ𝑛2^{\\gamma\\cdot\\ell(n)} against uniform adversaries. We define a sampler Snsubscript𝑆𝑛S_{n} as follows. On input x∈{0,1}n𝑥superscript01𝑛x\\in\\{0,1\\}^{n}, which we interpret as a random string, it outputs Gn​(x′)subscript𝐺𝑛superscript𝑥′G_{n}(x^{\\prime}), where x′superscript𝑥′x^{\\prime} is the prefix of x𝑥x of length ℓ​(n)ℓ𝑛\\ell(n). We argue that if an efficient algorithm F𝐹F is able to compress every string y𝑦y in the support of 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n}), the distribution induced by the sampler Snsubscript𝑆𝑛S_{n}, to an 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} encoding of complexity (1+γ−ε)⋅log⁡(1/δ′​(y))+C⋅(log⁡n)C⋅1𝛾𝜀1superscript𝛿′𝑦⋅𝐶superscript𝑛𝐶(1+\\gamma-\\varepsilon)\\cdot\\log(1/\\delta^{\\prime}(y))+C\\cdot(\\log n)^{C}, where δ′​(y)superscript𝛿′𝑦\\delta^{\\prime}(y) is a lower bound on δ​(y)𝛿𝑦\\delta(y) (the probability of y𝑦y under 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n})), we can use F𝐹F to break Gnsubscript𝐺𝑛G_{n}. (Note that F𝐹F expects as input n𝑛n, y𝑦y, δ′superscript𝛿′\\delta^{\\prime}, and 𝖼𝗈𝖽𝖾​(S)𝖼𝗈𝖽𝖾𝑆\\mathsf{code}(S).) The (uniform) distinguisher D𝐷D computes roughly as follows. Given a string z∈{0,1}n𝑧superscript01𝑛z\\in\\{0,1\\}^{n}, which might come from the uniform distribution Unsubscript𝑈𝑛U_{n} or from Gn​(Uℓ​(n))≡𝖣𝗂𝗌𝗍​(Sn)subscript𝐺𝑛subscript𝑈ℓ𝑛𝖣𝗂𝗌𝗍subscript𝑆𝑛G_{n}(U_{\\ell(n)})\\equiv\\mathsf{Dist}(S_{n}), D𝐷D attempts to use F𝐹F to compress z𝑧z to a “succinct” representation, then checks if the computed representation decompresses to the original string z𝑧z. If this is the case, it outputs 111, otherwise it outputs 00. (Note that we haven’t specified what “succinct” means, and it is also not immediately clear how to run F𝐹F, since it assumes knowledge of a probability bound δ′superscript𝛿′\\delta^{\\prime}. For simplicity of the exposition, we omit this point here.) We need to argue that a test of this form can be implemented in time 2γ⋅ℓ​(n)superscript2⋅𝛾ℓ𝑛2^{\\gamma\\cdot\\ell(n)}, and that it distinguishes the output of G𝐺G from a random string. To achieve these goals, first note that a typical random string cannot be compressed to representations of length, say, n−𝗉𝗈𝗅𝗒​(log⁡n)𝑛𝗉𝗈𝗅𝗒𝑛n-\\mathsf{poly}(\\log n), even in the much stronger sense of (time-unbounded) Kolmogorov complexity. Therefore, with some flexibility with respect to our threshold for succinctness, the proposed distinguisher is likely to output 00 on a random string. On the other hand, if F𝐹F implements an efficient coding theorem that achieves 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} encodings of complexity (1+γ−ε)⋅log⁡(1/δ′​(y))+𝗉𝗈𝗅𝗒​(log⁡n)⋅1𝛾𝜀1superscript𝛿′𝑦𝗉𝗈𝗅𝗒𝑛(1+\\gamma-\\varepsilon)\\cdot\\log(1/\\delta^{\\prime}(y))+\\mathsf{poly}(\\log n), the following must be true. Using that the expected encoding length of any (prefix-free) encoding scheme is at least H​(𝖣𝗂𝗌𝗍​(Sn))𝐻𝖣𝗂𝗌𝗍subscript𝑆𝑛H(\\mathsf{Dist}(S_{n})), where 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n}) is the distribution of strings sampled by Snsubscript𝑆𝑛S_{n} and H𝐻H is the entropy function, we get (via a slightly stronger version of this result) that a non-trivial measure of strings y𝑦y in the support of 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n}) have 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} encoding length at least (1−ε/4)⋅log⁡(1/δ​(y))⋅1𝜀41𝛿𝑦(1-\\varepsilon/4)\\cdot\\log(1/\\delta(y)). Consequently, for such strings, an upper bound on 𝗋𝖪𝗍𝗋𝖪𝗍\\mathsf{rKt} complexity of (1+γ−ε)⋅log⁡(1/δ′​(y))+𝗉𝗈𝗅𝗒​(log⁡n)⋅1𝛾𝜀1superscript𝛿′𝑦𝗉𝗈𝗅𝗒𝑛(1+\\gamma-\\varepsilon)\\cdot\\log(1/\\delta^{\\prime}(y))+\\mathsf{poly}(\\log n) when δ′​(y)superscript𝛿′𝑦\\delta^{\\prime}(y) is sufficiently close to δ​(y)𝛿𝑦\\delta(y) implies that the running time t𝑡t of the underlying machine satisfies log⁡t≤(γ−ε/2)​log⁡(1/δ​(y))+𝗉𝗈𝗅𝗒​(log⁡n)𝑡𝛾𝜀21𝛿𝑦𝗉𝗈𝗅𝗒𝑛\\log t\\leq(\\gamma-\\varepsilon/2)\\log(1/\\delta(y))+\\mathsf{poly}(\\log n). Using that ℓ​(n)=n/2ℓ𝑛𝑛2\\ell(n)=n/2 and δ​(y)≥2−ℓ​(n)𝛿𝑦superscript2ℓ𝑛\\delta(y)\\geq 2^{-\\ell(n)} for any string y𝑦y in the support of 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n}), it is easy to check that (asymptotically) t≤2(γ−ε/4)⋅ℓ​(n)𝑡superscript2⋅𝛾𝜀4ℓ𝑛t\\leq 2^{(\\gamma-\\varepsilon/4)\\cdot\\ell(n)}. For this reason, we can implement a (slightly modified) distinguisher D𝐷D in time less than 2γ⋅ℓ​(n)superscript2⋅𝛾ℓ𝑛2^{\\gamma\\cdot\\ell(n)}, by trying different approximations δ′​(z)superscript𝛿′𝑧\\delta^{\\prime}(z) for an input string z𝑧z and by running the decompressor on the produced representation for at most t𝑡t steps on each guess for δ​(z)𝛿𝑧\\delta(z). By our previous discussion, a non-trivial measure of strings from 𝖣𝗂𝗌𝗍​(Sn)𝖣𝗂𝗌𝗍subscript𝑆𝑛\\mathsf{Dist}(S_{n}) will be accepted by D𝐷D, while only a negligible fraction of the set of all strings (corresponding to the random case) will be accepted by D𝐷D. Implementing this strategy turns out to be more subtle than this. This happens because F𝐹F is a probabilistic algorithm which does not need to commit to a fixed succinct encoding. We refer to the formal presentation in Section 4 for details, where we also discuss the bound on the seed length ℓ​(n)ℓ𝑛\\ell(n).\n Coding Theorem for 𝗉𝖪tsuperscript𝗉𝖪𝑡\\mathsf{pK}^{t} (Theorem 5) and Unconditional  (Theorem 6). The proof of our optimal coding theorem for 𝗉𝖪tsuperscript𝗉𝖪𝑡\\mathsf{pK}^{t} builds on that of the conditional coding theorem for 𝖪tsuperscript𝖪𝑡\\mathsf{K}^{t} from , which can be viewed as a two-step argument. Roughly speaking, the first step is to show that if there is a polynomial-time sampler that outputs a string x∈{0,1}n𝑥superscript01𝑛x\\in\\left\\{0,1\\right\\}^{n} with probability δ𝛿\\delta, then the polynomial-time-bounded Kolmogorov complexity of x𝑥x is about log⁡(1/δ)+O​(log⁡n)1𝛿𝑂𝑛\\log(1/\\delta)+O(\\log n) if we are given a random string. After this, they “derandomize” the use of random strings using a certain pseudorandom generator, which exists under a strong derandomization assumption. Our key observation is that the use of random strings arises naturally in probabilistic Kolmogorov complexity, and particularly in this case the random strings can be “embedded” into the definition of 𝗉𝖪tsuperscript𝗉𝖪𝑡\\mathsf{pK}^{t}. As a result, we don’t need to perform the afterward derandomization as in original proof of , and hence get rid of the derandomization assumption. Next, we describe how to use Theorem 5, together with other useful properties of 𝗉𝖪tsuperscript𝗉𝖪𝑡\\mathsf{pK}^{t}, to obtain an unconditional version of Antunes and Fortnow’s main result. Let μ𝜇\\mu be a Kolmogorov complexity measure, such as 𝖪𝗉𝗈𝗅𝗒superscript𝖪𝗉𝗈𝗅𝗒\\mathsf{K}^{\\sf poly}, 𝗋𝖪𝗉𝗈𝗅𝗒superscript𝗋𝖪𝗉𝗈𝗅𝗒\\mathsf{rK}^{\\sf poly} or 𝗉𝖪𝗉𝗈𝗅𝗒superscript𝗉𝖪𝗉𝗈𝗅𝗒\\mathsf{pK}^{\\sf poly}. The key notion in the proof is the distribution (in fact, a class of semi-distributions) called mμsubscript𝑚𝜇m_{\\mu}, which is defined as mμ​(x):=1/2μ​(x)assignsubscript𝑚𝜇𝑥1superscript2𝜇𝑥m_{\\mu}(x)\\vcentcolon=1/2^{\\mu(x)}. More specifically, following , it is not hard to show that, for every language L𝐿L, L𝐿L can be decided in polynomial-time on average with respect to mμsubscript𝑚𝜇m_{\\mu} if and only if its worst-case running time is 2O​(μ​(x)−𝖪​(x))superscript2𝑂𝜇𝑥𝖪𝑥2^{O(\\mu(x)-\\mathsf{K}(x))} on input x𝑥x (see Lemma 35). Then, essentially, to show our result we argue that L𝐿L can be decided in polynomial time on average with respect to mμsubscript𝑚𝜇m_{\\mu} if and only if the same holds with respect to all 𝖯𝖯\\mathsf{P}-samplable distributions. Recall that if a distribution 𝒟𝒟\\mathcal{D} dominates another distribution 𝒟′superscript𝒟′\\mathcal{D}^{\\prime} (i.e., 𝒟​(x)≳𝒟′​(x)greater-than-or-equivalent-to𝒟𝑥superscript𝒟′𝑥\\mathcal{\\mathcal{D}}(x)\\gtrsim\\mathcal{\\mathcal{D}}^{\\prime}(x) for all x𝑥x) and L𝐿L is polynomial-time on average with respect to 𝒟𝒟\\mathcal{D}, then the same holds with respect to 𝒟′superscript𝒟′\\mathcal{D}^{\\prime} (see Definition 9 and 10). Therefore, to replace mμsubscript𝑚𝜇m_{\\mu} above with 𝖯𝖯\\mathsf{P}-samplable distributions, it suffices to show that mμsubscript𝑚𝜇m_{\\mu} is “universal” with respect to the class of 𝖯𝖯\\mathsf{P}-samplable distributions, in the following sense. mμsubscript𝑚𝜇m_{\\mu} dominates every 𝖯𝖯\\mathsf{P}-samplable distribution. (This is essentially an optimal source coding theorem for the Kolmogorov measure μ𝜇\\mu.) mμsubscript𝑚𝜇m_{\\mu} is dominated by some 𝖯𝖯\\mathsf{P}-samplable distribution. The above two conditions require two properties of the Kolmogorov measure μ𝜇\\mu that are somewhat conflicting: the first condition requires the notion of μ𝜇\\mu to be general enough so that mμsubscript𝑚𝜇m_{\\mu} can “simulate” every 𝖯𝖯\\mathsf{P}-samplable distribution, while the second condition needs μ𝜇\\mu to be restricted enough so that mμsubscript𝑚𝜇m_{\\mu} can be “simulated” by some 𝖯𝖯\\mathsf{P}-samplable (i.e., simple) distribution. For example, if μ𝜇\\mu is simply the time-unbounded Kolmogorov complexity 𝖪𝖪\\mathsf{K} (or even the polynomial-space-bounded variant), then it is easy to establish an optimal source coding theorem for such a general Kolmogorov measure; however it is unclear how to sample in polynomial-time a string x𝑥x with probability about 1/2𝖪​(x)1superscript2𝖪𝑥1/2^{\\mathsf{K}(x)}, so in this case μ𝜇\\mu does not satisfy the second condition. On the other hand, if μ𝜇\\mu is some restricted notion of time-bounded Kolmogrov complexity measure such as 𝖪𝗉𝗈𝗅𝗒superscript𝖪𝗉𝗈𝗅𝗒\\mathsf{K}^{\\sf poly} or 𝗋𝖪𝗉𝗈𝗅𝗒superscript𝗋𝖪𝗉𝗈𝗅𝗒\\mathsf{rK}^{\\sf poly}, then one can obtain polynomial-time samplers that sample x𝑥x with probability about 1/2𝖪poly​(x)1superscript2superscript𝖪poly𝑥1/2^{\\mathsf{K}^{\\operatorname{poly}}(x)} or 1/2𝖪poly​(x)1superscript2superscript𝖪poly𝑥1/2^{\\mathsf{K}^{\\operatorname{poly}}(x)} (up to a polynomial factor); however, as in , we only know how to show an optimal source coding theorem for 𝖪𝗉𝗈𝗅𝗒superscript𝖪𝗉𝗈𝗅𝗒\\mathsf{K}^{\\sf poly} (or 𝗋𝖪𝗉𝗈𝗅𝗒superscript𝗋𝖪𝗉𝗈𝗅𝗒\\mathsf{rK}^{\\sf poly}) under a derandomization assumption. Therefore, in this case μ𝜇\\mu does not satisfies the first condition. Our key observation is that the notion 𝗉𝖪𝗉𝗈𝗅𝗒superscript𝗉𝖪𝗉𝗈𝗅𝗒\\mathsf{pK}^{\\sf poly}, which sits in between 𝖪𝖪\\mathsf{K} and 𝖪polysuperscript𝖪poly\\mathsf{K}^{\\operatorname{poly}} (or 𝗋𝖪polysuperscript𝗋𝖪poly\\mathsf{rK}^{\\operatorname{poly}}),888We can show that for every x∈{0,1}∗𝑥superscript01x\\in\\left\\{0,1\\right\\}^{*} and every computable time bound t:ℕ→ℕ:𝑡→ℕℕt\\colon\\mathbb{N}\\to\\mathbb{N}, 𝖪​(x)≲𝗉𝖪t​(x)≤𝗋𝖪t​(x)≤𝖪t​(x)less-than-or-similar-to𝖪𝑥superscript𝗉𝖪𝑡𝑥superscript𝗋𝖪𝑡𝑥superscript𝖪𝑡𝑥\\mathsf{K}(x)\\lesssim\\mathsf{pK}^{t}(x)\\leq\\mathsf{rK}^{t}(x)\\leq\\mathsf{K}^{t}(x). satisfies both conditions described above (see Lemmas 37 and 36).\n\nReference markers found: [11]\n\nContext windows for each [ref]:\n[ref]11: the references therein). Therefore, the decompression time would have a dependency on δ𝛿\\delta, which is\ntoo large for us. Fortunately, lossless expanders can be used to do “almost” matching faster. [ref]11 introduces invertible functions (see Definition 12) for the more demanding task in which the elements of S𝑆S appear one-by-one and the matching has to be done in the online manner. We do not\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 11,\n            \"titles\": [\n                \"Tight Quantum Time-Space Tradeoffs for Function Inversion\",\n                \"The equivalence of sampling and searching\",\n                \"Worst-case running times for average-case algorithms\",\n                \"Using depth to capture average-case complexity\",\n                \"Applications of time-bounded Kolmogorov complexity in complexity theory\",\n                \"When worlds collide: Derandomization, lower bounds, and Kolmogorov complexity\",\n                \"The complexity of complexity\",\n                \"On the theory of average case complexity\",\n                \"Resource-bounded Kolmogorov complexity revisited\",\n                \"Language compression and pseudorandom generators\",\n                \"Average-case complexity\",\n                \"Universal almost optimal compression and Slepian-Wolf coding in probabilistic polynomial time\",\n                \"Maximum Flow and Minimum-Cost Flow in Almost-Linear Time\",\n                \"Randomness conductors and constant-degree lossless expanders\",\n                \"Time space tradeoffs for attacks against one-way functions and PRGs\",\n                \"Rigorous time/space trade-offs for inverting functions\",\n                \"Kolmogorov complexity and computational complexity\",\n                \"Probabilistic Kolmogorov complexity with applications to average-case complexity\",\n                \"Unbalanced expanders and randomness extractors from Parvaresh-Vardy codes\",\n                \"Non-black-box worst-case to average-case reductions within NP\",\n                \"Average-case hardness of NP from exponential worst-case hardness assumptions\",\n                \"Expander graphs and their applications\",\n                \"Efficiency improvements in constructing pseudorandom generators from one-way functions\",\n                \"On the complexity of SAT\",\n                \"Information in propositional proofs and algorithmic proof search\",\n                \"Kolmogorov complexity and formula lower bounds\",\n                \"Randomness conservation inequalities; information and independence in mathematical theories\",\n                \"Average case complete problems\",\n                \"An efficient coding theorem via probabilistic representations and its applications\",\n                \"Pseudodeterministic Algorithms and the Structure of Probabilistic Time\",\n                \"On One-way Functions and Kolmogorov Complexity\",\n                \"Average case complexity under the universal distribution equals worst-case complexity\",\n                \"An introduction to Kolmogorov complexity and its applications\",\n                \"Navigating central path with electrical flows: From flows to matchings, and back\",\n                \"Pseudorandom bits for constant depth circuits\",\n                \"Randomness and intractability in Kolmogorov complexity\",\n                \"Hardness of KT characterizes parallel cryptography\",\n                \"Complexity theory column 89: The polynomial hierarchy, random oracles, and boolean circuits\",\n                \"A complexity theoretic approach to randomness\",\n                \"Improved pseudorandom generators from pseudorandom multi-switching lemmas\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]11 based on the context and research area of Optimal Coding Theorems in Time-Bounded Kolmogorov Complexity. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}