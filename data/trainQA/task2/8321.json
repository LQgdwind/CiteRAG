{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Locally Sampleable Uniform Symmetric Distributions\n\nPaper Abstract: \n\nPaper Section Text:\nDespite being one of the simplest models of computation, ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits (i.e., Boolean circuits of constant depth and bounded fan-in) elude a comprehensive understanding. Even very recently, the model has been the subject of active research on the range avoidance problem   , quantum advantages     , proof verification [ref]17 [ref]4 , and more. Pertinent to this paper is the study of the sampling power of ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits. While the general problem was considered at least as early as , interest in the ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} setting has seen a strong uptick lately    [ref]15        .\nAt a high level, it considers what distributions can be (approximately) produced by simple functions on random inputs. More formally, let fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) denote the distribution resulting from applying an ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} function f:{0,1}mâ†’{0,1}n:ğ‘“â†’superscript01ğ‘šsuperscript01ğ‘›f\\colon\\{0,1\\}^{m}\\to\\{0,1\\}^{n} to a random string drawn from ğ’°msuperscriptğ’°ğ‘š\\mathcal{U}^{m}, the uniform distribution over {0,1}msuperscript01ğ‘š\\{0,1\\}^{m}. Typically, mğ‘šm is viewed as being arbitrarily large and nğ‘›n is the parameter of interest. Then the goal is to analyze the distance between fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) and some specific distribution. Aside from being inherently interesting, this question has also played a crucial role in applications ranging from data structure lower bounds        to pseudorandom generators    to extractors  [ref]15    to coding theory . One recurring class of distributions in this line of work is uniform symmetric distributions (i.e., uniform distributions over a symmetric support). Indeed, these are precisely the distributions that arise in an elegant connection to succinct data structures (see [38, Claim 1.8]), for example. Moreover, this seemingly simple class is already rich enough to allow surprisingly powerful results. For example, ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits can sample the uniform distribution over the preimage PARITYâˆ’1â€‹(0)superscriptPARITY10\\mathrm{PARITY}^{-1}(0) (and PARITYâˆ’1â€‹(1)superscriptPARITY11\\mathrm{PARITY}^{-1}(1)), despite a celebrated result of HÃ¥stad  proving that more powerful ğ– ğ–¢ğŸ¢superscriptğ– ğ–¢0\\mathsf{AC^{0}} circuits require an exponential number of gates to compute PARITYPARITY\\mathrm{PARITY}. Perhaps more surprisingly, the strategy to sample a uniform random string with even Hamming weight is extremely simple: map the uniform random bits x1,â€¦,xnsubscriptğ‘¥1â€¦subscriptğ‘¥ğ‘›x_{1},\\ldots,x_{n} to x1âŠ•x2,x2âŠ•x3,â€¦,xnâŠ•x1direct-sumsubscriptğ‘¥1subscriptğ‘¥2direct-sumsubscriptğ‘¥2subscriptğ‘¥3â€¦direct-sumsubscriptğ‘¥ğ‘›subscriptğ‘¥1x_{1}\\oplus x_{2},x_{2}\\oplus x_{3},\\ldots,x_{n}\\oplus x_{1}  . A number of notable prior results already rule out specific distributions from being accurately sampled by ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits. For example, let ğ’Ÿksubscriptğ’Ÿğ‘˜\\mathcal{D}_{k} denote the uniform distribution over all nğ‘›n-bit strings of Hamming weight kğ‘˜k. The influential early paper of  showed that such shallow circuits could not accurately sample ğ’Ÿksubscriptğ’Ÿğ‘˜\\mathcal{D}_{k} for k=Î˜â€‹(n)ğ‘˜Î˜ğ‘›k=\\Theta(n) under certain assumptions about the input length or accuracy tolerance; recent works    have eliminated the need for these assumptions. Additionally, a number of results are known for uniform symmetric distributions over multiple Hamming weights, such as the case of exclusively tail weights , all weights divisible by qğ‘q for fixed 3â‰¤qâ‰ªn3ğ‘much-less-thanğ‘›3\\leq q\\ll\\sqrt{n} , and all weights above n/2ğ‘›2n/2 [ref]17   (see also ). Despite much effort, the previous body of work proceeds in a somewhat ad-hoc fashion, with techniques tailored to rule out specific cases. However, an exciting recent work by Filmus, Leigh, Riazanov, and Sokolov  gave the following bold conjecture about the capabilities of ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits for sampling distributions, unifying prior results. Let f:{0,1}mâ†’{0,1}n:ğ‘“â†’superscript01ğ‘šsuperscript01ğ‘›f\\colon\\{0,1\\}^{m}\\to\\{0,1\\}^{n} be computable by an ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuit.\nIf fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) is Îµğœ€\\varepsilon-close (in total variation distance) to a uniform symmetric distribution and nğ‘›n is sufficiently large, then fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) is Oâ€‹(Îµ)ğ‘‚ğœ€O(\\varepsilon)-close to one of the following six distributions: Point distribution on 0nsuperscript0ğ‘›0^{n}. Point distribution on 1nsuperscript1ğ‘›1^{n}. Uniform distribution over {0n,1n}superscript0ğ‘›superscript1ğ‘›\\left\\{0^{n},1^{n}\\right\\}. Uniform distribution over strings with even Hamming weights. Uniform distribution over strings with odd Hamming weights. Uniform distribution over all strings. All six distributions can be sampled (exactly) by functions whose output bits each depend on at most two input bits. Hence one may informally view the conjecture as asserting that more input dependencies do not substantially increase the ability of ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits to generate uniform symmetric distributions. In this work, we confirm the conjecture of  as follows. Let dğ‘‘d be a fixed constant, and suppose f:{0,1}mâ†’{0,1}n:ğ‘“â†’superscript01ğ‘šsuperscript01ğ‘›f\\colon\\{0,1\\}^{m}\\to\\{0,1\\}^{n} is computable by an ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuit of depth at most dğ‘‘d. If fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) is Îµğœ€\\varepsilon-close (in total variation distance) to a uniform symmetric distribution and nğ‘›n is sufficiently large, then fâ€‹(ğ’°m)ğ‘“superscriptğ’°ğ‘šf(\\mathcal{U}^{m}) is Oâ€‹(Îµ)ğ‘‚ğœ€O(\\varepsilon)-close to one of the six distributions in 1.1. Note that this result is optimal up to the implicit constant in Oâ€‹(Îµ)ğ‘‚ğœ€O(\\varepsilon). We include a more thorough discussion of our resultâ€™s tightness in SectionÂ 4, where we present a quantitative version of TheoremÂ 1.2 parametrized by the locality (i.e., number of input bits each output bit depends on) of fğ‘“f. The following corollary is immediate. For sufficiently large nğ‘›n, the only uniform symmetric distributions over {0,1}nsuperscript01ğ‘›\\{0,1\\}^{n} exactly sampleable by ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} functions are the six distributions in 1.1. As a contrasting example to the limitation given by TheoremÂ 1.2, consider the next simplest class of circuits commonly studied: ğ– ğ–¢ğŸ¢superscriptğ– ğ–¢0\\mathsf{AC^{0}}. Up to some exponentially small error, they are able to sample the uniform distribution over permutations of [n]delimited-[]ğ‘›[n] 9 1. Thus by sampling 1wâ€‹0nâˆ’wsuperscript1ğ‘¤superscript0ğ‘›ğ‘¤1^{w}0^{n-w} for the appropriate distribution over weights wğ‘¤w accepted (or rejected) by a symmetric function fğ‘“f, one can apply a randomly sampled permutation to output the uniform distribution over fâˆ’1â€‹(1)superscriptğ‘“11f^{-1}(1) (or fâˆ’1â€‹(0)superscriptğ‘“10f^{-1}(0)) [38, Lemma 4.3]. We provide a proof overview of TheoremÂ 1.2 in SectionÂ 2. Preliminary definitions and results are given in SectionÂ 3. The full proof of our main result is in SectionÂ 4, with some technical proofs deferred to the appendices.\n\nReference markers found: [17]\n\nContext windows for each [ref]:\n[ref]17: fan-in) elude a comprehensive understanding. Even very recently, the model has been the subject of active research on the range avoidance problem   , quantum advantages     , proof verification [ref]17 [ref]4 , and more. Pertinent to this paper is the study of the sampling power of ğ–­ğ–¢ğŸ¢superscriptğ–­ğ–¢0\\mathsf{NC^{0}} circuits. While the general problem was considered at least as early as ,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 17,\n            \"titles\": [\n                \"Range Avoidance for Constant-Depth Circuits: Hardness and Algorithms\",\n                \"Improved bounds for the sunflower lemma\",\n                \"Random oracles separate PSPACE from the polynomial-time hierarchy\",\n                \"Note on Sunflowers\",\n                \"Verifying proofs in constant depth\",\n                \"Quantum advantage with shallow circuits\",\n                \"Quantum advantage with noisy shallow circuits\",\n                \"Large deviation bounds for decision trees and sampling lower bounds for AC0-circuits\",\n                \"One-way functions and circuit complexity\",\n                \"Concentration inequalities: A nonasymptotic theory of independence\",\n                \"Estimates for Bezout coefficients\",\n                \"The space complexity of sampling\",\n                \"Extractors for near logarithmic min-entropy\",\n                \"Elements of information theory\",\n                \"Explicit two-source extractors and resilient functions\",\n                \"Extractors and lower bounds for locally samplable sources\",\n                \"Sampling and Certifying Symmetric Functions\",\n                \"Verifying and decoding in constant depth\",\n                \"Range avoidance for low-depth circuits and connections to pseudorandomness\",\n                \"A lower bound for sampling disjoint sets\",\n                \"Fast parallel generation of random permutations\",\n                \"Computational limitations for small depth circuits\",\n                \"Probability inequalities for sums of bounded random variables\",\n                \"Random generation of combinatorial structures from a uniform distribution\",\n                \"Small depth proof systems\",\n                \"Locality Bounds for Sampling Hamming Slices\",\n                \"Sum of the first k binomial coefficients for fixed nğ‘›n\",\n                \"Bounded-depth circuits cannot sample good codes\",\n                \"Converting high probability into nearly-constant timeâ€”with applications to parallel hashing\",\n                \"Sums of independent random variables, volumeÂ 82\",\n                \"Coding for sunflowers\",\n                \"The monotone complexity of k-clique on random graphs\",\n                \"On the range avoidance problem for circuits\",\n                \"Explicit codes for poly-size circuits and functions that are hard to sample on low entropy distributions\",\n                \"Classical and almost sure local limit theorems\",\n                \"Upper estimates of maximum probability for sums of independent random vectors\",\n                \"Bit-probe lower bounds for succinct data structures\",\n                \"The complexity of distributions\",\n                \"Extractors for Turing-machine sources\",\n                \"Extractors for circuit sources\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]17 based on the context and research area of Locally Sampleable Uniform Symmetric Distributions. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}