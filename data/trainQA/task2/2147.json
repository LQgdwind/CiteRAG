{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs\n\nPaper Abstract: \n\nPaper Section Text:\nIn Table 1, we report the results for all the previously trained generators, with the different decoding algorithms presented in Section 4. By ‘model’, in the following, we refer to the couple composed by a trained generator and a decoding algorithm. We report BLEU4, ROUGE-1, ROUGE-L along with scores for the discriminators Base and Base+, computed as the percentage of outputs considered as human by a given discriminator model. Base was only trained on MLE+Beam Search outputs. As expected, by further training on the outputs generated by all the different models, Base+ has a higher accuracy, which consistently results in lower scores compared to Base. We start by focusing on the MLE results to compare the different decoding mechanisms.\nWe observe that all the cooperative searches outperform Beam Search.\nRegarding Base and Base+ metrics, DASLocal compares favorably to DASGlobal. We hypothesize that invoking the discriminator to rank at each step can have more impact than using it only once on fully decoded sequences. Finally, our proposed Coop-MCTS obtains the best results by a large margin. Regarding the different GANs, we first compare them given the default decoding mechanism, i.e. Beam Search. The three version of SelfGAN compare favorably to MLE and ColdGAN on both n-gram based metrics and discriminators metrics. Among SelfGANs, SelfGANCoop-MCTS obtains the best results: given a Beam Search decoding, it obtains the best BLEU, ROUGE-1 and ROUGE-L on the two tasks (respectively 17.2; 44.3; 40.6 on QG and 12.3; 38.6; 36.7 on Summarization). The performance in term of Base and Base+ for SelfGANCoop-MCTS is even more important in comparison to the other models (34.1%; 21.9% on QG and 20.2%; 12.7% on Summarization). Both GAN at training time and Cooperative decoding at inference time pursue the same objective: to obtain better outputs that look like human texts.\nWould a generator trained via GAN, coupled with a Cooperative Decoding mechanism for inference result into a cumulative improvement from the two methods? First, on both ColdGAN and three SelfGANs, we can observe that adding a Cooperative Decoding method allows to gain significant improvement on Base and Base+. In particular, it is interesting to note that for SelfGAN an additional pattern seems to emerge: using the same cooperative decoding algorithm both during training and inference seems to provide additional gains.\nThe best performance is achieved with the generator SelfGANCoop-MCTS paired with the decoding Coop-MCTS. Compared to MLE via Beam Search, it obtains a final improvement superior to 1 point in term of ROUGE and BLEU. The relative improvement for Base+ is significant: from 15.2% to 26.2% on QG and from 8.6% to 15.3% on Summarization. This corresponds to almost twice more outputs that sound human according to the Discriminator metric. Human-like features during training\nIn NLG, various rules are often integrated into the Beam Search to improve the quality of the outputs, for instance a length penalty [ref]34 or an interdiction for 3-grams repetitions  [ref]8. Such a need to hard code these rules indicates a discrepancy between the human output characteristics and what the model has learned.\nIn particular, Scialom et al.  reported the difference between DAS and the human reference for:\ni) Length: the average number of tokens per output; ii) Novelty: percentage of tokens in the output that were not present in the source text;\niii) N-gram repetition: percentage of N-grams that occur more than once in the output.\nTo measure how SelfGAN learns these features by itself, we report in\nFigure 1 the evolution of these statistics during training: we observe that SelfGAN constantly reach statistics more similar to human references than ColdGAN. Human Evaluation\nWe conduct a human evaluation to measure the models performances beyond automatic metrics. We limit the evaluation to three generators (MLE, ColdGAN, and SelfGANCoop-MCTS) and two decoding methods (Beam Search and Coop-MCTS), for a total of 6 different models. Three professional English speakers rated 300 sampled summaries and followed the same protocol from Fabbri et al. . Four dimensions are evaluated on a Likert scale from 1 to 5 (the higher the better): Consistency: the proportion of facts in the summary correct w.r.t. the source text; Coherence: how well-structured and well-organized is the summary; Fluency: how fluent the summary is to read; Relevance: the ratio between important and excess information in the summary. From Table 2 we observe significantly better results for SelfGANCoop-MCTS w.r.t. both MLE and ColdGAN.\nWhile Coop-MCTS decoding appears overall beneficial in terms of Coherence and Relevance, but scores lower on Consistency and Fluency, its combination with SelfGANCoop-MCTS allows to obtain significant improvements on the former two dimensions while still maintaining comparable scores on the latter. Analysis\nTo further understand the benefits of selfGAN,\nwe propose to analyze the evolution of the generator and discriminator networks through the learning process. In figure 2 (left), we first plot the average magnitude (L2 norm) of the discriminator gradients w.r.t. its parameters.\nWe observe that C​o​l​d​G​A​N𝐶𝑜𝑙𝑑𝐺𝐴𝑁ColdGAN induces important instabilities for its discriminator over time, with a highly fluctuating gradient magnitude. Conversely, thanks to its cooperative decoding process, SelfGAN produces sequences that form a more compact set for discriminator training, a variance of gradient magnitude twice lower than C​o​l​d​G​A​N𝐶𝑜𝑙𝑑𝐺𝐴𝑁ColdGAN, for a comparable magnitude in average. This discriminator stability is a first explanation for the improvements of the proposed approach. In a second plot, given on the right of Figure 2, we report the collinearity of generator gradients for the generated samples from the model with those for the corresponding human references. Higher values indicate sampling strategies that induce a useful gradient flow for the generator.\nFor ablation purposes, we first report values for a \"SelfGANBeamSearch\" approach, where we used a standard Beam Search to generate the training examples: note that it has no discriminator, hence it is not a GAN anymore. We can observe its divergence, as opposed to SelfGANCoop-MCTS, which emphasizes the importance of the cooperative decoding for producing the example used to train the model.\nFor SelfGANCoop-MCTS and ColdGAN, the gradients become more co-linear with human references through time, indicating a convergence of the process towards the human distribution. We observe that SelfGANCoop-MCTS produces more useful sequences for achieving this convergence. Coop-MCTS as an alternative to the dead-end search\nWhen analysing the behavior for the Coop-MCTS decoding, we observed in different examples that it provides an effective mean to revise generations that eventually ended up to be unlikely. To illustrate this, we report in Table 3 the different MCTS steps for an ambiguous example: the conditioned answer, Super Bowl, occurs at different places of the the input. Therefore, the model has to decide which specific mention of Super Bowl to focus on: at step 17, it considers its current generation as a dead end and decides to start on new node (How). The final output is a question that arguably sounds better than the initial one. ... ... Societal Impact\nReliable NLG models can have significant societal impact with beneficial applications such as efficient information access via automatic summarization or personalized student evaluation through question generation. Still, malicious actors can use the same technology to build tools detrimental to society, e.g. large scale creation of misleading (fake) news [ref]25. As argued by Zellers et al. , keeping this research open and under public scrutiny can be an effective defense.\n\nReference markers found: [34]\n\nContext windows for each [ref]:\n[ref]34: to the Discriminator metric. Human-like features during training\nIn NLG, various rules are often integrated into the Beam Search to improve the quality of the outputs, for instance a length penalty [ref]34 or an interdiction for 3-grams repetitions  [ref]8. Such a need to hard code these rules indicates a discrepancy between the human output characteristics and what the model has learned.\nIn\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 34,\n            \"titles\": [\n                \"On Accurate Evaluation of GANs for Language Generation\",\n                \"Towards Principled Methods for Training Generative Adversarial Networks\",\n                \"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\",\n                \"Language GANs Falling Short\",\n                \"Maximum-Likelihood Augmented Discrete Generative Adversarial Networks\",\n                \"Efficient selectivity and backup operators in monte-carlo tree search\",\n                \"Training Language GANs from Scratch\",\n                \"Residual Energy-Based Models for Text Generation\",\n                \"Unified Language Model Pre-training for Natural Language Understanding and Generation\",\n                \"SummEval: Re-evaluating Summarization Evaluation\",\n                \"Hierarchical neural story generation\",\n                \"Cooperative generator-discriminator networks for abstractive summarization with narrative flow\",\n                \"Distilling the Knowledge in a Neural Network\",\n                \"The curious case of neural text degeneration\",\n                \"Bandit based monte-carlo planning\",\n                \"Human-like natural language generation using monte carlo tree search\",\n                \"Professor forcing: A new algorithm for training recurrent networks\",\n                \"Machine Translation Decoding beyond Beam Search\",\n                \"Adversarial Learning for Neural Dialogue Generation\",\n                \"Rouge: A package for automatic evaluation of summaries\",\n                \"An unsupervised approach to automatic response generation for conversational e-commerce agents using monte carlo tree search\",\n                \"Abstractive text summarization using sequence-to-sequence rnns and beyond\",\n                \"Why we need new evaluation metrics for NLG\",\n                \"Bleu: a method for automatic evaluation of machine translation\",\n                \"A Deep Reinforced Model for Abstractive Summarization\",\n                \"Language models are unsupervised multitask learners\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"Squad: 100,000+ questions for machine comprehension of text\",\n                \"Sequence Level Training with Recurrent Neural Networks\",\n                \"Multi-armed bandits with episode context\",\n                \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\",\n                \"ColdGANs: Taming Language GANs with Cautious Sampling Strategies\",\n                \"Discriminative Adversarial Search for Abstractive Summarization\",\n                \"Probability of error of some adaptive pattern-recognition machines\",\n                \"Get To The Point: Summarization with Pointer-Generator Networks\",\n                \"Self-Attention with Relative Position Representations\",\n                \"Mastering the game of go without human knowledge\",\n                \"A learning algorithm for continually running fully recurrent neural networks\",\n                \"Huggingface’s transformers: State-of-the-art natural language processing\",\n                \"Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]34 based on the context and research area of To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}