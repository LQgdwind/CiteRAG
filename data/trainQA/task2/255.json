{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Bias in Machine Learning - What is it Good for?\n\nPaper Abstract: \n\nPaper Section Text:\nSampling bias occurs when there is an underrepresentation or overrepresentation of observations from a segment of the population .\nSuch bias, which is sometimes called selection bias , or population bias , may result in a classifier that performs bad in general, or bad for certain demographic groups. One example of underrepresentation is a reported case where a New Zealand passport robot rejected an Asian man’s eyes because ‘subject eyes are closed’666CNN World, Dec. 9, 2016. Accessed July 29, 2020..\nA possible reason could have been that the robot was trained with too few pictures of Asian men, and therefor made bad predictions on this demographic group. There are many reasons for sampling bias in a dataset. One\nkind is denoted self-selection bias  and can be exemplified with an online survey about computer use. Such a survey is likely to attract people more interested in technology than is typical for the entire population and therefor creates a bias in data. Another example is a system that predicts crime rates in different parts of a city. Since areas with more crimes typically have more police present, the number of reported arrests would become unfairly high in these areas. If such a system would be used to determine the distribution of police presence, a viscous circle may even be created  . An opposite example demonstrates how the big data era with its automatic data gathering can create ‘dark zones or shadows where some citizens and communities are overlooked’ . The author Kate Crawford points to Street Bump, a phone app that uses the phone’s built in accelerometer to detect and report information about road problems to the city. Due to the uneven distribution of smartphones across different parts of the city, data from Street Bump will have a sampling bias. It is important to note that sampling bias does not only refer to unbalanced categories of humans, and furthermore not even to unbalanced categories. Unbalances may also concern features that have to appear in a balanced fashion.\nOne example is given in [ref]46, and is there denoted dataset bias. Focusing on image data, the authors argue that ‘… computer vision datasets are supposed to be a representation of the world’, but in reality, many commonly used datasets represent the world in a very biased way. Objects may, for example, always appear in the center of the image. This bias makes it hard for a classifier to recognize objects that are not centered in the image [ref]46.\nThe authors compared six common datasets of images used for object detection, and found that performance on another dataset than the one used during training in average was cut to less than half.\nA similar effect is reported in . If all images in a dataset containing a snowmobile also contain snow, a machine learning algorithm may find snow cues useful to detect snowmobiles. While this may work fine for images in the dataset used for training, it becomes problematic to analyze images with snowmobiles placed indoors. Another kind of sampling bias is survivorship bias . It occurs when the sampled data does not represent the population of interest, since some data items ‘died’. One example is when a bank’s stock fund management is assessed by sampling the performance of the bank’s current funds. This leads to a biased assessment since poorly-performing funds are often removed or merged into other funds .\n\nReference markers found: [46]\n\nContext windows for each [ref]:\n[ref]46: refer to unbalanced categories of humans, and furthermore not even to unbalanced categories. Unbalances may also concern features that have to appear in a balanced fashion.\nOne example is given in [ref]46, and is there denoted dataset bias. Focusing on image data, the authors argue that ‘… computer vision datasets are supposed to be a representation of the world’, but in reality, many commonly\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 46,\n            \"titles\": [\n                \"It’s a Man’s Wikipedia? Assessing Gender Inequality in an Online Encyclopedia\",\n                \"False positives, false negatives, and false analyses: A rejoinder to ”machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks”\",\n                \"Learning de-biased representations with biased representations\",\n                \"Big data’s disparate impact\",\n                \"Hyper-parameter optimization in deep learning and transfer learning : applications to medical imaging\",\n                \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n                \"Understanding the Origins of Bias in Word Embeddings\",\n                \"Semantics derived automatically from language corpora necessarily contain human biases\",\n                \"Ai now 2017 report\",\n                \"CRISP-DM 1.0: Step-by-step data mining guide\",\n                \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\",\n                \"Algorithmic discrimination is an information problem\",\n                \"Think again: Big data\",\n                \"Algorithmic bias in autonomous systems\",\n                \"Proxy non-discrimination in data-driven systems\",\n                \"Chapter 6 research design - sampling bias\",\n                \"Fairness through awareness\",\n                \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\",\n                \"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them\",\n                \"European Union regulations on algorithmic decision-making and a “right to explanation”\",\n                \"Truth and Method\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"Handbook of Research on Machine Learning Innovations and Trends\",\n                \"Privacy as protection of the incomputable self: From agnostic to agonistic machine learning\",\n                \"Detecting biased statements in wikipedia\",\n                \"Towards bias detection in online text corpora\",\n                \"Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks\",\n                \"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\",\n                \"Inherent Trade-Offs in the Fair Determination of Risk Scores\",\n                \"Generalization versus discrimination in machine learning\",\n                \"Repair: Removing representation bias by dataset resampling\",\n                \"Causal Reasoning for Algorithmic Fairness\",\n                \"Returns from investing in equity mutual funds 1971 to 1991\",\n                \"Efficient estimation of word representations in vector space\",\n                \"The need for biases in learning generalizations\",\n                \"Social data: Biases, methodological pitfalls, and ethical boundaries\",\n                \"Artificial intelligence and algorithmic bias: implications for health systems\",\n                \"Discrimination-aware data mining\",\n                \"Glove: Global vectors for word representation\",\n                \"A Dictionary of Epidemiology\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]46 based on the context and research area of Bias in Machine Learning - What is it Good for?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}