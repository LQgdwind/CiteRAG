{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Deep Reinforcement Learning for Uplink Scheduling in NOMA-URLLC Networks\n\nPaper Abstract: \n\nPaper Section Text:\n[\ncap = Simulation Settings.,\ncaption = Network Simulation Settings.,\nlabel = tab:simparam,\npos = ht\n]—c—c—c—\n\\tnote[a]For a channel bandwidth of 40 MHz, the signal occupies 38.16 MHz after having excluded guard bands .\n\\tnote[b]Typical delay spread for an indoor hot-spot scenario with carrier frequency 4 GHz .\n\\tnote[c]Normal cyclic prefix duration for symbols not at the start or in the middle of the subframe .\n\\tnote[e]Size of an array of size 565656 corresponding to a maximum deadline of 565656 frames, i.e., 101010 ms, with 333 bits entries giving the number of packets for every deadline.\n\\tnote[d]Headers include 2 bytes of CRC , 1 byte for MAC , 0 byte for RLC 5, 2 bytes for PDCP 6, 0 byte for SDAP 7 and 40 bytes for IPv6 8.\n\nParameter  Notation  Value \nCarrier frequency  fcsubscript𝑓𝑐f_{c} 444 GHz \nBandwidth\\tmark[a]  W𝑊W 38.1638.1638.16 MHz \nSubcarrier spacing  Δ​fΔ𝑓\\Delta f 303030 kHz \nDelay spread\\tmark[b]  Tdsubscript𝑇𝑑T_{d} 100100100 ns \nOFDM symbol information part  Tisubscript𝑇𝑖T_{i} 33.3333.3333.33 μ𝜇\\mus \nOFDM symbol cyclic prefix\\tmark[c]  Tc​psubscript𝑇𝑐𝑝T_{cp} 2.342.342.34 μ𝜇\\mus \nSIC limitation  B𝐵B 333\nInformation length  Lisubscript𝐿𝑖L_{i} 323232 bytes \nHeaders\\tmark[d] length  Lhsubscript𝐿ℎL_{h} 464646 bytes \nBuffer information\\tmark[e]  Lbsubscript𝐿𝑏L_{b} 141414 bytes \nAverage inter-arrival rate  1/λ1𝜆1/\\lambda  2 ms\nDeadline  δ𝛿\\delta 111 ms \nNetwork layout  ℓ×ℓ′ℓsuperscriptℓ′\\ell\\times\\ell^{\\prime} 50×1205012050\\times 120 m2\nNoise Power Spectral Density  N0subscript𝑁0N_{0} −174174-174 dBm/Hz\nBS noise figure  NFsubscript𝑁𝐹N_{F} 555 dB \nBS antenna height  h~bsubscript~ℎ𝑏\\tilde{h}_{b} 333 m \nBS antenna gain  Gbsubscript𝐺𝑏G_{b} 555 dBi \nBS number of antennas  nasubscript𝑛𝑎n_{a}  4 \nDevice transmit power  p𝑝p 232323 dBm \nDevice antenna height  h~dsubscript~ℎ𝑑\\tilde{h}_{d} 1.51.51.5 m \nDevice antenna gain  Gdsubscript𝐺𝑑G_{d} 00 dBi \nDevice speed  v𝑣v 333 km/h \n Our simulation settings (see Table LABEL:tab:simparam) adopt the parameters of the factory automation use case of the 3GPP 5G NR specifications on URLLC  and industrial IoT 9. Our radio frame is made of five time-slots (Tf=5​Tssubscript𝑇𝑓5subscript𝑇𝑠T_{f}=5T_{s}), whose duration Tssubscript𝑇𝑠T_{s} is equivalent to an OFDM symbol in NR. It can be decomposed into an information part of duration Tisubscript𝑇𝑖T_{i} and a cyclic prefix of duration Tc​psubscript𝑇𝑐𝑝T_{cp}, which both depend on the subcarrier spacing Δ​fΔ𝑓\\Delta f: Ts=Ti+Tc​psubscript𝑇𝑠subscript𝑇𝑖subscript𝑇𝑐𝑝T_{s}=T_{i}+T_{cp} with Ti=1/Δ​fsubscript𝑇𝑖1Δ𝑓T_{i}=1/\\Delta f. From the signal bandwidth we substract the subcarriers dedicated to uplink pilots, so that, when there are U𝑈U polled devices and npsubscript𝑛𝑝n_{p} pilots per device, the number of complex channel uses is n=(W−np​U​Δ​f)​Ti𝑛𝑊subscript𝑛𝑝𝑈Δ𝑓subscript𝑇𝑖n=(W-n_{p}U\\Delta f)T_{i} [37, Chapter 5]. The number of pilots per device can be obtained as follows: np=⌈W/Wc⌉subscript𝑛𝑝𝑊subscript𝑊𝑐n_{p}=\\lceil W/W_{c}\\rceil, where Wc=1/(2​Td)subscript𝑊𝑐12subscript𝑇𝑑W_{c}=1/(2T_{d}) [37, Chapter 2] is the coherence bandwidth and Tdsubscript𝑇𝑑T_{d} is the delay spread. Regarding the traffic model, we consider either a deterministic periodic traffic with period 1/λ1𝜆1/\\lambda or a probabilistic aperiodic traffic with average inter-arrival time 1/λ1𝜆1/\\lambda. A packet can be decomposed into an information part of length Lisubscript𝐿𝑖L_{i}, a header part of length Lhsubscript𝐿ℎL_{h}, and a buffer description of length Lbsubscript𝐿𝑏L_{b}, so that L=Li+Lh+Lb𝐿subscript𝐿𝑖subscript𝐿ℎsubscript𝐿𝑏L=L_{i}+L_{h}+L_{b}. In URLLC, headers cannot indeed be neglected with respect to the message length. We assume that the information part, the header and the buffer information are jointly encoded . The traffic parameters of Table LABEL:tab:simparam are taken from the factory automation use case of Release 16 . For realistic numerical experiments, we partly adopt the scenario proposed in [40, Table A.2.2-1] for the factory automation use case, with a single BS. The network layout is a rectangle of size ℓ×ℓ′ℓsuperscriptℓ′\\ell\\times\\ell^{\\prime}; the BS is positioned at its center at a height h~bsubscript~ℎ𝑏\\tilde{h}_{b} and serves devices, each at height h~dsubscript~ℎ𝑑\\tilde{h}_{d} and moving with velocity v𝑣v. Devices are uniformly distributed within the network area. Devices and BS benefit from antenna gains Gbsubscript𝐺𝑏G_{b} and Gdsubscript𝐺𝑑G_{d} respectively. For a speed of v=3𝑣3v=3 km/h, we obtain a coherence time of Tc=c/(8​fc​v)=11.2subscript𝑇𝑐𝑐8subscript𝑓𝑐𝑣11.2T_{c}=c/(8f_{c}v)=11.2 ms, which corresponds to 636363 radio frames. We choose an episode length of 200200200 frames that allows us to consider speeds below 111 km/h. The path-loss model is the ITU InH NLOS [ref]61. The BS has a noise figure NFsubscript𝑁𝐹N_{F}, so that the noise power is σn2=N0​W​NFsuperscriptsubscript𝜎𝑛2subscript𝑁0𝑊subscript𝑁𝐹\\sigma_{n}^{2}=N_{0}WN_{F}, where N0subscript𝑁0N_{0} is the noise power spectral density. Typical values for the channel parameters are given in Table LABEL:tab:simparam. We express the deadlines and inter-arrival time in term of frames. Indeed, given the frame duration Tfsubscript𝑇𝑓T_{f}, we can deduce that the average inter-arrival time of 2 ms corresponds to 11.211.211.2 frames and that the deadline of 111 ms to 5.65.65.6 frames. The parameters of the DRL algorithms are given in Table I. We preprocess the agent state as follows. In order to reduce the dimension of the buffer information, the matrix 𝑩A​(t)superscript𝑩𝐴𝑡\\bm{B}^{A}(t) is transformed into a vector of size K𝐾K of head-of-line delays for each agent. In order to improve stability of speed up training, we normalize 𝝉p,𝝉a,𝝉ssuperscript𝝉𝑝superscript𝝉𝑎superscript𝝉𝑠\\bm{\\tau}^{p},\\bm{\\tau}^{a},\\bm{\\tau}^{s} between 0 and 1 by taking 1/𝝉p,1/𝝉a,1/𝝉s1superscript𝝉𝑝1superscript𝝉𝑎1superscript𝝉𝑠1/\\bm{\\tau}^{p},1/\\bm{\\tau}^{a},1/\\bm{\\tau}^{s}. Finally, the channel threshold η∗superscript𝜂\\eta^{*} is calculated using (8) such that the error probability in absence of interference corresponding to η∗superscript𝜂\\eta^{*} is equal to 10−5superscript10510^{-5}.\n\nReference markers found: [61]\n\nContext windows for each [ref]:\n[ref]61: which corresponds to 636363 radio frames. We choose an episode length of 200200200 frames that allows us to consider speeds below 111 km/h. The path-loss model is the ITU InH NLOS [ref]61. The BS has a noise figure NFsubscript𝑁𝐹N_{F}, so that the noise power is σn2=N0​W​NFsuperscriptsubscript𝜎𝑛2subscript𝑁0𝑊subscript𝑁𝐹\\sigma_{n}^{2}=N_{0}WN_{F}, where N0subscript𝑁0N_{0} is the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 61,\n            \"titles\": [\n                \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\",\n                \"Study on scenarios and requirements for next generation access technologies\",\n                \"Non-orthogonal multiple access (noma) for cellular future radio access\",\n                \"Enabling urllc in 5g nr iiot networks: A full-stack end-to-end analysis\",\n                \"Uplink scheduling for mixed grant-based embb and grant-free urllc traffic in 5g networks\",\n                \"NR; Medium Access Control (MAC) protocol specification\",\n                \"A predictive semi-persistent scheduling scheme for low-latency applications in lte and nr networks\",\n                \"Radio resource allocation and retransmission schemes for urllc over 5g networks\",\n                \"Uplink grant-free access solutions for urllc services in 5g new radio\",\n                \"Slotted aloha with noma for the next generation iot\",\n                \"Grant-free non-orthogonal multiple access for iot: A survey\",\n                \"Analyzing grant-free access for urllc service\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Deep reinforcement learning for internet of things: A comprehensive survey\",\n                \"Distributive dynamic spectrum access through deep reinforcement learning: A reservoir computing-based approach\",\n                \"The application of deep reinforcement learning to distributed spectrum access in dynamic heterogeneous environments with partial observations\",\n                \"Cooperative multi-agent reinforcement learning based distributed dynamic spectrum access in cognitive radio networks\",\n                \"Deep reinforcement learning based massive access management for ultra-reliable low-latency communications\",\n                \"A reliable reinforcement learning for resource allocation in uplink noma-urllc networks\",\n                \"A general deep reinforcement learning framework for grant-free noma optimization in murllc\",\n                \"Deep reinforcement learning for scheduling uplink iot traffic with strict deadlines\",\n                \"Deep Reinforcement Learning in Large Discrete Action Spaces\",\n                \"Continuous control with deep reinforcement learning\",\n                \"Discrete Sequential Prediction of Continuous Actions for Deep RL\",\n                \"Action Branching Architectures for Deep Reinforcement Learning\",\n                \"The optimal control of partially observable Markov processes\",\n                \"Planning and acting in partially observable stochastic domains\",\n                \"Deep Recurrent Q-Learning for Partially Observable MDPs\",\n                \"Deep Variational Reinforcement Learning for POMDPs\",\n                \"Approximate information state for approximate planning and reinforcement learning in partially observed systems\",\n                \"Reinforcement Learning\",\n                \"Proximal policy optimization algorithms\",\n                \"Bayesian transfer reinforcement learning with prior knowledge rules\",\n                \"Joint subcarrier and power allocation in noma: Optimal and approximate algorithms\",\n                \"Joint Power and Blocklength Optimization for URLLC in a Factory Automation Scenario\",\n                \"Joint beamforming and scheduling for a multi-antenna downlink with imperfect transmitter channel knowledge\",\n                \"Microwave mobile communications\",\n                \"Fundamentals of wireless communication\",\n                \"Wireless communications\",\n                \"The effect of imperfect channel estimation on the performance of maximum ratio combining in the presence of cochannel interference\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]61 based on the context and research area of Deep Reinforcement Learning for Uplink Scheduling in NOMA-URLLC Networks. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}