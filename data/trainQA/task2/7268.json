{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A comprehensive study on fidelity metrics for XAI\n\nPaper Abstract: \n\nPaper Section Text:\nDeep learning models have become ubiquitous solutions and are used across multiple fields, yielding astonishing results. These methods outperform other artificial intelligence (AI) models owing to their high complexity, and ability to learn from large amounts of data. However, this complexity gives rise to a major drawback: the inability to know the reasons behind their results. This challenge is commonly known as the “black-box problem” [ref]8. To address this challenge, eXplainable AI (XAI) has emerged. According to Adadi and Berrada , the goal of XAI methods is to “create a suite of techniques that produce more explainable models whilst maintaining high performance levels”. The growing dynamic around XAI has been reflected in several scientific events and the increase in publications as highlighted in several recent reviews about the topic  3    [ref]8 1. In particular, these methods have been used in sensitive fields such as medical tasks   , where XAI methods are extensively used to gain a deeper understanding of models, improve them, and prevent life-costing mistakes. Multiple methods have emerged to achieve this goal. Murdoch et al.  proposed categorizing them into two main categories: model-based and post-hoc. Model-based algorithms refer to AI models that inherently provide insights into the relationships they have learned. The main challenge in model-based explainability lies in developing models that strike a balance between simplicity, making them easily understandable to the audience, and sophistication, enabling them to effectively capture the underlying data. Post-hoc techniques are defined as methods that analyse an externally trained model to provide insights into the learned relationships. These techniques focus on understanding the specific model’s behaviour rather than directly interpreting the model’s internal mechanisms. Owing to their simplicity compared with model-based approaches, post-hoc methods have gained widespread adoption, as demonstrated in various studies that reviewed the existing state of the art (4, 4, ). However, a significant challenge with the post-hoc methods, as highlighted by Adebayo et al. , is that different post-hoc methods can produce varying explanations for the same AI model. Krishna et al. 0 identified and analysed this inconsistency and called it the disagreement problem. This problem emphasises the need to identify correct and incorrect explanations to enhance existing techniques. To achieve this, objective evaluation becomes crucial, as relying solely on subjective human evaluation, as stated by Miller 1, may not yield reliable and consistent results. Tomsett et al. 3 identified fidelity as the main property for detecting whether an XAI algorithm is correct. According to Mohseni et al. 6 fidelity is ”the correctness of an ad-hoc technique in generating the true explanations (e.g., correctness of a saliency map) for model predictions”. The main limitation to calculating it is the inability to have a ground truth of the real explanation. To overcome this limitation, most authors rely on assumptions about the relationship between a correct explanation and the model to measure fidelity. While these proposals may differ in several aspects, all of them involve perturbing the input based on an explanation and analysing the resulting differences in the output of the AI model. Bach et al. [ref]7 proposed perturbing individual pixels from most important to least important and analysing how this perturbation modifies the neural network output, generating a curve that contains the number of pixels perturbed and the output. Samek et al. 2 used an approach similar to that of Bach et al. [ref]7, however, instead of perturbing single pixels they perturbed regions of pixels. In addition, they calculated the Area over the Perturbation Curve (AOPC). Rieger et al. 1 modified the proposal of Samek et al. defining the regions using a superpixel detection algorithm. Bhat et al.  proposed adding perturbations to the original input according to the importance of the explanation, to a completely perturbed image. To obtain the final metric, they proposed using the Pearson correlation coefficient 5 between the differences in the outputs of the model and the extend of importance that was removed. Alvares-Melis et al.  performed the same calculation as Bhat et al. , with the major difference being that they did not accumulate the perturbation, only perturbing one region at a time. Finally, Yeh et al. 7 instead of calculating fidelity proposes calculating the reverse, i.e. infidelity. To do this, they proposed using the expected mean square between the difference in the output when a region is perturbed and the importance of these regions multiplied by the amount of perturbation. The existence of numerous fidelity metrics and the absence of a consensus among them pose a significant challenge, which is reminiscent of the disagreement issues identified in XAI methods by Krishna et al. 0. In addressing this challenge, several authors have advocated the assessment of metric quality, with Hedström et al. 8 characterising this evaluation as a meta-evaluation. To this end, Tomsett et al. 3 introduced three sanity checks for fidelity metrics, establishing essential conditions for ensuring their inherent reliability. The application of these checks to the AOPC metric proposed by Samek et al. 2 and the faithfulness metric proposed by Alvarez-Melis  revealed that both metrics were deemed ”unreliable at measuring saliency map fidelity”. Using a methodology similar to that of Tomsett et al. 3, Hedström et al. 8 introduced a set of conditions for accurate measurements, and subsequently converted them into continuous metrics. The application of these metrics to 10 different fidelity measures indicates that Pixel Flipping by Bach et al. [ref]7 performed the best, albeit without achieving perfect results. Both of these assessment approaches can be categorised as axiomatic evaluations, because they establish a set of axioms and assess whether the metrics align with them. However, a noteworthy limitation of these studies lies in the necessity to assume these axioms, especially when a lack of consensus exists between both proposals. Efforts to reconcile and standardise these axioms are imperative for advancing the field and enhancing the reliability of fidelity metrics. From the insights gleaned from these studies, it becomes apparent that the comprehensive XAI methodology does not necessarily eliminate the need for blind trust in black-box models; instead, it introduces its own set of non-transparent elements that demand a similar degree of trust. These components include the AI model itself, the XAI method used, and the fidelity metric employed. Visualised in Figure 1, we observe how the inclusion of elements aimed at shedding light on the opaqueness of a pipeline actually only adds complexity to the entire system. In this study, we aim to develop a novel method to verify fidelity metrics. To accomplish this, we used a transparent model that allowed us to have a ground truth for the explanation: the well-known decision tree 0. This model allowed us to compare fidelity metrics with the real fidelity of the explanation, surpassing the limitations of the previous approaches and the axiomatic approaches used previously in the literature. The rest of this paper is organised as follows. In the next section, we identify the objectives of this research. In Section 3 we propose a methodology to measure and analyse the different fidelity metrics. In Section 4, we specify the experimental environment and describe the fidelity metrics, models, measures, and statistical tests used for experimentation. In Section 5, we discuss the results of the two experiments defined in the previous section to analyse the different fidelity metrics, and the theoretical and practical implications of the results. Finally, in Section 6 we present the conclusions of the study.\n\nReference markers found: [8]\n\nContext windows for each [ref]:\n[ref]8: large amounts of data. However, this complexity gives rise to a major drawback: the inability to know the reasons behind their results. This challenge is commonly known as the “black-box problem” [ref]8. To address this challenge, eXplainable AI (XAI) has emerged. According to Adadi and Berrada , the goal of XAI methods is to “create a suite of techniques that produce more explainable models\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 8,\n            \"titles\": [\n                \"Evaluating and Aggregating Feature-based Model Explanations\",\n                \"Peeking inside the black-box: a survey on explainable artificial intelligence (xai)\",\n                \"Fair and explainable depression detection in social media\",\n                \"Sanity checks for saliency maps\",\n                \"Towards Robust Interpretability with Self-Explaining Neural Networks\",\n                \"Towards better understanding of gradient-based attribution methods for Deep Neural Networks\",\n                \"Explainable agents and robots: Results from a systematic literature review\",\n                \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n                \"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\",\n                \"Classification and regression trees\",\n                \"A survey on xai and natural language explanations\",\n                \"Describing textures in the wild\",\n                \"Explainable artificial intelligence: A survey\",\n                \"Testing the robustness of attribution methods for convolutional neural networks in MRI-based Alzheimer’s disease classification\",\n                \"Statistics (international student edition)\",\n                \"Metrics for saliency map evaluation of deep learning explanation methods\",\n                \"Evaluating local explanation methods on ground truth\",\n                \"The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus\",\n                \"Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond\",\n                \"The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective\",\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"Explainable artificial intelligence: a comprehensive review\",\n                \"A novel approach to generate datasets with xai ground truth to evaluate image models\",\n                \"Evaluating explainable artificial intelligence for x-ray image analysis\",\n                \"Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets\",\n                \"A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems\",\n                \"Methods for interpreting and understanding deep neural networks\",\n                \"Interpretable machine learning: definitions, methods, and applications\",\n                \"Scikit-learn: Machine learning in Python\",\n                \"Generating perturbation-based explanations with robustness to out-of-distribution data\",\n                \"IROF: a low resource evaluation metric for explanation methods\",\n                \"Evaluating the visualization of what a Deep Neural Network has learned\",\n                \"Sanity Checks for Saliency Metrics\",\n                \"Explainable artificial intelligence (XAI) in deep learning-based medical image analysis\",\n                \"COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images\",\n                \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\",\n                \"On the (In)fidelity and Sensitivity of Explanations\",\n                \"Generated Reference Title 37 for [ref]8\",\n                \"Generated Reference Title 38 for [ref]8\",\n                \"Generated Reference Title 39 for [ref]8\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]8 based on the context and research area of A comprehensive study on fidelity metrics for XAI. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}