{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: New advances in universal approximation with neural networks of minimal width\n\nPaper Abstract: \n\nPaper Section Text:\nThe minimal widths of FNNs with ReLU or leaky ReLU activations for universal approximation properties have been specified exactly by Park et al. , Cai [ref]5 and our 1. Note additionally that, due to the coding scheme construction and because the memorizer has dimension one, the universal approximation results of Park et al.  and ours have minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1. Therefore, while Cai [ref]5 first proved the minimal width for leaky ReLU FNNs, our alternative proof of the upper bound of the minimal width in 1 constructs the approximating sequences explicitly, and shows that there always exists an approximating sequence of minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1. Moreover, Cai [ref]5 use results, connected to the Theorem 2.5 (i) of Brenier and Gangbo , to infer the UAP in Lpsuperscript𝐿𝑝L^{p} with LReLU activations 1 from the UAP  with respect to flow maps for neuralODEs . We, in contrast, directly prove the UAP in Lpsuperscript𝐿𝑝L^{p} and thereby provide a constructive proof of Brenier and Gangbo’s result as a corollary of 4. For proving additional theoretical results regarding the approximation with FNNs with leaky ReLU activations, it might therefore be advantageous to consider the explicit construction obtained in the proof of our 1 and 4. For example, as we constructed the memorizer as a leaky ReLU FNN of input dimension 111, it is clear that, in general, a neural network in our approximating sequences for Lpsuperscript𝐿𝑝L^{p} integrable functions has at least one hidden layer with one neuron, corresponding to a minimal interior dimension dmin=1subscript𝑑1d_{\\min}=1, independent of the input and output dimension dx,dy∈ℕsubscript𝑑𝑥subscript𝑑𝑦ℕd_{x},d_{y}\\in\\mathbb{N}, respectively. In particular, this allowed us to prove universal approximation for autoencoders with arbitrary interior feature dimension in Corollary 2.9. In contrast, Cai [ref]5 shows that for every Lpsuperscript𝐿𝑝L^{p} integrable function there exists an approximating sequence of LReLU FNNs of width max⁡{dx,dy,2}subscript𝑑𝑥subscript𝑑𝑦2\\max\\{d_{x},d_{y},2\\}, but more specifics of the FNNs, such as the minimal interior dimension they attain, are unclear. Moreover, note that Cai [ref]5 showed that the minimal width for universal approximation of FNNs with arbitrary activations is lower bounded by wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}, which means that FNNs with leaky ReLU activations achieve the minimal width over all possible activation functions if min⁡{dx,dy}≥2subscript𝑑𝑥subscript𝑑𝑦2\\min\\{d_{x},d_{y}\\}\\geq 2. Thus, the leaky ReLU activation achieves the minimal width for all but infinitely many choices of input and output dimensions except for the three special cases (dx,dy)∈{(1,1),(1,2),(2,1)}subscript𝑑𝑥subscript𝑑𝑦111221(d_{x},d_{y})\\in\\{(1,1),(1,2),(2,1)\\}. Moreover as the minimal width for universal approximation for FNNs with ReLU activations is given by max⁡{dx+1,dy}subscript𝑑𝑥1subscript𝑑𝑦\\max\\{d_{x}+1,d_{y}\\} by Park et al.  the ReLU fails to achieve this property in the infinitely many cases where dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}. An interesting open question lies in exactly specifying the minimal width for universal approximation of continuous functions on Euclidean spaces with respect to the supremum norm for FNNs with the commonly used activation functions, such as ReLU or leaky ReLU. Cai [ref]5 proved that wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} is the lowest minimal width that any activation function can achieve, both for uniform and Lpsuperscript𝐿𝑝L^{p} universal approximation. Furthermore, our 5 shows that dx+1subscript𝑑𝑥1d_{x}+1 is a lower bound for the minimal width for uniform universal approximation when dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}, for Lipschitz continuous monotone activations, thus including ReLU and leaky ReLU. Park et al.  showed that the minimal width for uniform universal approximation of functions in C0​( ,ℝ2)superscript𝐶001superscriptℝ2C^{0}( ,\\mathbb{R}^{2}) with ReLU FNNs is wmin=3subscript𝑤3w_{\\min}=3, hence the lower bound max⁡{dx,dy}=2subscript𝑑𝑥subscript𝑑𝑦2\\max\\{d_{x},d_{y}\\}=2 is not tight in this case. However, in general, even for ReLU and leaky ReLU activations, it is not known whether these bounds are tight, making it an important topic for further research to extract upper bounds for the universal approximation of continuous functions with respect to the supremum norm, especially for ReLU and leaky ReLU, but also for other activations of practical and theoretical interest. Furthermore, our 5 demonstrates that, in the case dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}, FNNs with leaky ReLU activations are unable to attain the smallest possible minimal width over all activation functions, given by wmin∗=max⁡{dx,dy}=dxsuperscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦subscript𝑑𝑥w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\}=d_{x}, while they achieve it for Lpsuperscript𝐿𝑝L^{p} universal approximation of Lpsuperscript𝐿𝑝L^{p} integrable functions when dx≥2subscript𝑑𝑥2d_{x}\\geq 2 and dy≥2subscript𝑑𝑦2d_{y}\\geq 2, as shown in 1 and by Cai [ref]5. Moreover, it should be noted that Cai [ref]5 proved that the minimal width for uniform universal approximation of continuous functions with FNNs with ReLU+FLOOR activations is given by wmin=max⁡{2,dx,dy}subscript𝑤2subscript𝑑𝑥subscript𝑑𝑦w_{\\min}=\\max\\{2,d_{x},d_{y}\\}. Nevertheless, despite this result being optimal for approximation in theory, it is difficult for practical purposes, as using discontinuous activations in neural networks fundamentally breaks backpropagation and gradient descent. These methods are designed for smooth functions for computing gradients  9, and while there are workarounds, they might be inefficient and impractical for training deep networks with discontinuous activations. This motivates the question of whether there is another continuous (almost everywhere) smooth activation that achieves the minimal width for the uniform universal approximation of continuous functions. According to our 5, such an activation cannot be Lipschitz continuous and monotone, which excludes most of the popular choices 2 , such as ReLU, LReLU, tangens hyperbolicus, logistic sigmoid and many other sigmoidal activations. Some possible candidates for relevant activation functions, as e.g. presented by Dubey et al. 2, that might still be able to achieve uniform UAP of continuous functions with a width of max⁡{dx,dy}subscript𝑑𝑥subscript𝑑𝑦\\max\\{d_{x},d_{y}\\}, could include the swish activation given by swish⁡(x)=x1+exp⁡(−x)swish𝑥𝑥1𝑥\\operatorname{swish}(x)=\\frac{x}{1+\\exp(-x)} or the adaptive piecewise linear (APL) activation, which is more adaptive due to learnable parameters similar to LReLU, both of which are not monotone. Additionally, as of now, the leaky ReLUs are the only known activation functions to achieve universal approximation for Lpsuperscript𝐿𝑝L^{p} integrable functions with the minimal width wmin∗superscriptsubscript𝑤w_{\\min}^{*} over all possible activation functions for all but three choices of input and output dimensions. Therefore another interesting open question for future work is whether there exist other activation functions, besides the leaky ReLU, which achieve the minimal width for Lpsuperscript𝐿𝑝L^{p} universal approximation for all but finitely many choices of the dimensions, or perhaps even for all input and output dimensions. Note that ReLU does not accomplish this, since Park et al.  showed that FNNs with ReLU activations for Lpsuperscript𝐿𝑝L^{p} universal approximation of the Lpsuperscript𝐿𝑝L^{p} integrable function have a minimal width of max⁡{dx+1,dy}subscript𝑑𝑥1subscript𝑑𝑦\\max\\{d_{x}+1,d_{y}\\} and therefore cannot achieve the smallest possible width of wmin∗=max⁡{dx,dy}superscriptsubscript𝑤subscript𝑑𝑥subscript𝑑𝑦w_{\\min}^{*}=\\max\\{d_{x},d_{y}\\} if dx≥dysubscript𝑑𝑥subscript𝑑𝑦d_{x}\\geq d_{y}.\n\nReference markers found: [5]\n\nContext windows for each [ref]:\n[ref]5: The minimal widths of FNNs with ReLU or leaky ReLU activations for universal approximation properties have been specified exactly by Park et al. , Cai [ref]5 and our 1. Note additionally that, due to the coding scheme construction and because the memorizer has dimension one, the universal approximation results of Park et al.  and ours have minimal\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 5,\n            \"titles\": [\n                \"LU-Net: Invertible Neural Networks Based on Matrix Factorization\",\n                \"Analysis III\",\n                \"Learning Deep Architectures for AI\",\n                \"lpsuperscript𝑙𝑝l^{p} approximation of maps by diffeomorphisms\",\n                \"Functional Analysis, Sobolev Spaces and Partial Differential Equations\",\n                \"Achieve the minimum width of neural networks for universal approximation\",\n                \"Neural Ordinary Differential Equations\",\n                \"Approximation by superpositions of a sigmoidal function\",\n                \"Activation functions and their characteristics in deep neural networks\",\n                \"Density estimation using Real NVP\",\n                \"Vanilla feedforward neural networks as a discretization of dynamic systems\",\n                \"Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark\",\n                \"Real Analysis\",\n                \"Approximating Continuous Functions by ReLU Nets of Minimal Width\",\n                \"Analysis 2\",\n                \"Multilayer feedforward networks are universal approximators\",\n                \"Deep, skinny neural networks are not universal approximators\",\n                \"Universal Approximation with Deep Narrow Networks\",\n                \"Glow: Generative Flow with Invertible 1×1 Convolutions\",\n                \"Normalizing Flows: An Introduction and Review of Current Methods\",\n                \"Deep Learning via Dynamical Systems: An Approximation Perspective\",\n                \"Linear Algebra\",\n                \"The Expressive Power of Neural Networks: A View from the Width\",\n                \"Distribution learning via neural differential equations: a nonparametric statistical perspective\",\n                \"Neural networks and deep learning\",\n                \"Applied Nonlinear Functional Analysis\",\n                \"Minimum width for universal approximation\",\n                \"On the difficulty of training recurrent neural networks\",\n                \"Approximation theory of the mlp model in neural networks\",\n                \"Intermediate Calculus\",\n                \"Remarks on a Multivariate Transformation\",\n                \"Neural ode control for classification, approximation, and transport\",\n                \"Optimal transport for applied mathematicians\",\n                \"Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators\",\n                \"Optimal transport: old and new\",\n                \"Generated Reference Title 35 for [ref]5\",\n                \"Generated Reference Title 36 for [ref]5\",\n                \"Generated Reference Title 37 for [ref]5\",\n                \"Generated Reference Title 38 for [ref]5\",\n                \"Generated Reference Title 39 for [ref]5\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]5 based on the context and research area of New advances in universal approximation with neural networks of minimal width. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}