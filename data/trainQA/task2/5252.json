{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Real-Time Neural Light Field on Mobile Devices\n\nPaper Abstract: \n\nPaper Section Text:\nHere we perform the ablation analysis to understand the design choices of the network. We use the scene of Chair from the synthetic 360∘superscript360360^{\\circ} to conduct the analysis. All models are trained for 200200200K iterations. Options for Backbone.\nWe study the two available operators for designing the backbone.\nMLP and 1×1111\\times 1 CONV layer are essentially equivalent operators and perform the same calculations, thus resulting in similar performance. However, we observe around 27%percent2727\\% latency reduction on mobile devices (iPhone 13) when replacing the MLP layer with the 1×1111\\times 1 CONV layer. Specifically, as shown in Tab. 4, we design two networks, i.e., MLP and CONV2D, with only residual blocks as in Fig. 3 but removing the activation, normalization, and super-resolution modules. We use the input size as 100×100100100100\\times 100 for the two models. Since the super-resolution modules are omitted, we train the two networks for generating 100×100100100100\\times 100 images.\nAs can be seen, the CONV2D model has a faster inference speed than MLP with similar performance.\nThis is due to the fact that CONV operation is better-optimized than MLP on mobile devices. Additionally, due to the intrinsic design of our proposed MobileR2L, employing MLP layers requires two additional operators, i.e., Permute and Reshape, before feeding the internal features to super-resolution modules, while Permute and Reshape involve data movement that adds unnecessary overheads on some edge devices [ref]26. Analysis of Activation Function.\nR2L  and NeRF [ref]33 use ReLU  activation as non-linearity function. In our proposed MobileR2L, we use GELU [ref]17 instead. As shown in Tab. 4, by comparing the CONV2D + ReLU and CONV2D + GeLU, which are two networks trained with ReLU and GeLU activations, we notice that GeLU brings about 0.170.170.17 PSNR boost without any additional latency overhead. Similarly, we show that incorporating BatchNorm  layer into the ResBlock is also beneficial for better performance without introducing extra latency, as shown by CONV2D + GeLU + BN in Tab. 4. The three networks in the experiments are also trained to render 100×100100100100\\times 100 images. Analysis on Input Dimension. We further analyze the optimal spatial resolution for the input tensor. Specifically, we benchmark the performance of three approaches, namely, 50×50505050\\times 50 - NeRF Teacher, 100×100100100100\\times 100 - NeRF Teacher, and 200×200200200200\\times 200 - NeRF Teacher with the spatial size of input as the square of 505050, 100100100, and 200200200 respectively. These models contain super-resolution modules to render 800×800800800800\\times 800 images and are trained with the NeRF [ref]33 as a teacher model. Results are presented in Tab. 5. The model using a small input spatial size, i.e., 50×50505050\\times 50, achieves 2×2\\times speedup than the model with a larger size, i.e., 100×100100100100\\times 100, as less computation is required. However, the performance is also degraded by 0.250.250.25 PSNR. Further increasing the input spatial size to 200×200200200200\\times 200 makes the model unable to achieve real-time inference. Thus, we do not report the rendering performance of the models with 200×200200200200\\times 200 input size. Analysis of SR modules. We further show the necessity of using SR modules. We use the spatial size of 800×800800800800\\times 800 as the network input to render images with the same spatial size. We denote the setting as 800×800800800800\\times 800 - w/o SR in Tab. 5. Profiling the latency of such a network leads to compilation errors due to intensive memory usage. Thus, our proposed SR module is essential for high-resolution synthesis without introducing prohibitive computation overhead. Choice of Teacher Models.\nSince both R2L  and MobileR2L  use a teacher model for generating pseudo data to train a lightweight network, we study whether a more powerful teacher model can improve performance. To conduct the experiments, we use MipNeRF  as the teacher model for the training MobileR2L, given MipNeRF shows better performance than NeRF on the synthetic 360∘superscript360360^{\\circ} dataset. We denote the approach as 100×100100100100\\times 100 - MipNeRF Teacher. Through the comparison with 100×100100100100\\times 100 - NeRF Teacher, as in Tab. 5, we notice the quality of the rendered images is improved, e.g., the PSNR is increased by 0.180.180.18, without the extra cost of latency. The comparison demonstrates that our approach has the potential to render higher-quality images when better teacher models are provided. Analysis on Ray Representation.\nHere we analyze how the ray representation affects the latency-performance trade-off of MobileR2L.\nWe follow the same ray representation paradigm as in R2L  and positional encoding as in NeRF [ref]33. Specifically, R2L sample K𝐾K 3D points along the ray, and each point is mapped to a higher dimension by positional encoding with L𝐿L positional coordinates. R2L sets K=16𝐾16K=16 and L=10𝐿10L=10, resulting in a vector with a dimension of 1,00810081,008. We apply the same setting to MobileR2L and denote the model as 100×100100100100\\times 100, K​16𝐾16K16, L​10𝐿10L10 - MipNeRF Teacher in Tab. 5. For our implementation in MobileR2L, we set K=8​and​L=6𝐾8and𝐿6K=8~{}\\text{and}~{}L=6 for the model, which has the dimension per ray as 312312312. The model is 100×100100100100\\times 100 - MipNeRF Teacher. By comparing the performance of the two models, we notice that larger K𝐾K and L𝐿L lead to negligible PSNR improvement (0.070.070.07), yet higher inference latency and bigger model size (more parameters).\nTherefore, we chose K=8​and​L=6𝐾8and𝐿6K=8~{}\\text{and}~{}L=6 for the consideration of model size and latency – they are the more important metrics when deploying neural rendering models on mobile devices. Depth of the Backbone. Lastly, we show the effects of the backbone depth on the model performance. We use the depth as 606060 for our backbone. By increasing the number of residual blocks in the backbone, i.e., setting depth as 100100100, we obverse better model performance at the cost of higher latency, as shown by comparing the last two rows in Tab. 5. Using depth as 100100100 significantly increases the latency and the number of parameters, and the model fails to run in real time. Thus, we chose depth as 606060, given the better trade-off between latency, model size, and performance.\n\nReference markers found: [26]\n\nContext windows for each [ref]:\n[ref]26: i.e., Permute and Reshape, before feeding the internal features to super-resolution modules, while Permute and Reshape involve data movement that adds unnecessary overheads on some edge devices [ref]26. Analysis of Activation Function.\nR2L  and NeRF [ref]33 use ReLU  activation as non-linearity function. In our proposed MobileR2L, we use GELU [ref]17 instead. As shown in Tab. 4, by\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 26,\n            \"titles\": [\n                \"AutoInt: Automatic Integration for Fast Neural Volume Rendering\",\n                \"Lens studio.\",\n                \"Nerf-factory.\",\n                \"Snap ml.\",\n                \"Deep Learning using Rectified Linear Units (ReLU)\",\n                \"Learning neural light fields with ray-space embedding.\",\n                \"​​Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields​\",\n                \"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields\",\n                \"Model compression.\",\n                \"Efficient Geometry-aware 3D Generative Adversarial Networks\",\n                \"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures\",\n                \"Use coremltools to convert models from third-party libraries to core ml.\",\n                \"Plenoxels: Radiance Fields without Neural Networks\",\n                \"Fastnerf: High-fidelity neural rendering at 200fps.\",\n                \"The lumigraph.\",\n                \"StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis\",\n                \"Baking Neural Radiance Fields for Real-Time View Synthesis\",\n                \"Gaussian Error Linear Units (GELUs)\",\n                \"Distilling the Knowledge in a Neural Network\",\n                \"HeadNeRF: A Real-time NeRF-based Parametric Head Model\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\",\n                \"Ray tracing volume densities.\",\n                \"Adam: A method for stochastic optimization.\",\n                \"Light field rendering.\",\n                \"Rethinking Vision Transformers for MobileNet Size and Speed\",\n                \"EfficientFormer: Vision Transformers at MobileNet Speed\",\n                \"NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field\",\n                \"Neural Sparse Voxel Fields\",\n                \"Efficient sparse-winograd convolutional neural networks.\",\n                \"Optical models for direct volume rendering.\",\n                \"Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines\",\n                \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\",\n                \"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\",\n                \"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\",\n                \"DeRF: Decomposed Radiance Fields\",\n                \"Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps.\",\n                \"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering\",\n                \"Attention Is All You Need\",\n                \"Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\",\n                \"R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]26 based on the context and research area of Real-Time Neural Light Field on Mobile Devices. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}