{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nLet us formally introduce the tallying bandit setting. An online learning game is an (m,g,h)𝑚𝑔ℎ(m,g,h)-tallying bandit if the adversary is m𝑚m-memory bounded, g𝑔g-restricted and hℎh-tallying, and if after playing action atsubscript𝑎𝑡a_{t} the player observes a random variable h~at​(yt)∈ [ref]1subscript~ℎsubscript𝑎𝑡subscript𝑦𝑡01\\widetilde{h}_{a_{t}}(y_{t})\\in [ref]1 satisfying where yt=∑t′=max⁡{1,t−m+1}t𝕀​(at′=at)subscript𝑦𝑡superscriptsubscriptsuperscript𝑡′1𝑡𝑚1𝑡𝕀subscript𝑎superscript𝑡′subscript𝑎𝑡y_{t}=\\sum_{t^{\\prime}=\\max\\{1,t-m+1\\}}^{t}\\mathbb{I}(a_{t^{\\prime}}=a_{t}). We assume the cardinality K𝐾K of the action set 𝒳𝒳\\mathcal{X} is finite, and also that m𝑚m is known (although we discuss how to relax this in Section 6). With this definition of the setting in hand, we can restate our goal of efficiently minimizing the CPR. Concretely, we desire an algorithm, which when given an (m,g,h)𝑚𝑔ℎ(m,g,h)-tallying bandit problem, has a CPR bound that is polynomial in K,m𝐾𝑚K,m and is sublinear in T𝑇T. The tallying bandit strictly generalizes the well studied stochastic multi armed bandit (sMAB)  , simply via a choice of m=1𝑚1m=1. Hence, we generalize the study of sMAB to m>1𝑚1m>1. We remark that tallying bandit is a special case of the rested bandit, a general framework for nonstationary MAB where the reward of an arm evolves when it is pulled, and we defer detailed discussion of this to Section 5. Recall that in sMAB, the optimal policy plays the same optimal arm at each timestep. Hence, in sMAB obtaining zero constant action traditional regret is equivalent to obtaining zero constant action policy regret and also zero CPR. Given that the tallying bandit is highly structured and generalizes sMAB via m=1𝑚1m=1, it is natural to question whether minimizing constant action policy (or traditional) regret implies minimizing CPR. The following counterexample answers this question in the negative when m>1𝑚1m>1. More generally, this counterexample shows that even when the adversary is restricted (as in tallying bandits), minimizing the constant action policy (or traditional) regret can lead to solutions whose cumulative loss is Ω~​(T)~Ω𝑇\\widetilde{\\Omega}(T) larger than the minimum achievable total loss. Let 𝒳={x1,x2}𝒳subscript𝑥1subscript𝑥2\\mathcal{X}=\\{x_{1},x_{2}\\} and m=2𝑚2m=2. Define hx1​(1)=hx2​(1)=0subscriptℎsubscript𝑥11subscriptℎsubscript𝑥210h_{x_{1}}(1)=h_{x_{2}}(1)=0 and hx1​(2)=hx2​(2)=1subscriptℎsubscript𝑥12subscriptℎsubscript𝑥221h_{x_{1}}(2)=h_{x_{2}}(2)=1. A policy that fixes either of the two actions, and then plays this action at every timestep, incurs cumulative loss T−1𝑇1T-1 but has zero constant action policy regret and zero constant action traditional regret. Meanwhile, the complete policy regret of this policy is T−1𝑇1T-1, since the optimal policy that alternates playing actions x1subscript𝑥1x_{1} and x2subscript𝑥2x_{2} incurs zero cumulative loss. Thus far, our motivation for the tallying bandit setting has been primarily theoretical, to resolve the gap in our understanding of when we can efficiently minimize CPR. Nevertheless, in similar vein to Heidari et al. , Lindner et al. 3 and Awasthi et al. , we believe that the tallying bandit is a simple approximation for various practical settings. For instance, in recommender systems the reward associated with an action is rarely static, because the stimulus of recommended content influences user preferences  . Moreover, literature on psychology and cognition suggests that humans often forget prior stimuli and do not always encode them in permanent memory 9 . Thus, one way to model a user’s preferences is via an (unknown) function of the number of times a content genre has been recommended in a recent time interval, motivating both the hℎh-tallying structure as well as bounded m𝑚m. Nevertheless, the tallying bandit is just one plausible model, and we suggest possible extensions in Section 6. Let us now discuss potential avenues for efficiently minimizing CPR in tallying bandit problems. One approach is to observe that any tallying bandit problem can be cast as a reinforcement learning (RL) problem, where each state corresponds to a sequence of actions taken in the last m𝑚m timesteps. However, methods for solving such RL problems typically scale with the cardinality of the state space [ref]8 [ref]9 6, and such approaches would suffer Ω~​(Km)~Ωsuperscript𝐾𝑚\\widetilde{\\Omega}(K^{m}) CPR. Hence it is necessary to leverage the additional properties of tallying bandit problems. To gain intuition, let us consider the simplified setting of deterministic bandit feedback, where for each t,a1:t∈𝒳t𝑡subscript𝑎:1𝑡superscript𝒳𝑡t,a_{1:t}\\in\\mathcal{X}^{t} the player observes ft​(a1:t)subscript𝑓𝑡subscript𝑎:1𝑡f_{t}(a_{1:t}) with no noise. Since the loss functions ftsubscript𝑓𝑡f_{t} are fully defined by the functions {hx}x∈𝒳subscriptsubscriptℎ𝑥𝑥𝒳\\{h_{x}\\}_{x\\in\\mathcal{X}}, it is natural to consider the following algorithm, which we denote 𝔸​𝕃​𝔾det𝔸𝕃subscript𝔾det\\mathbb{ALG}_{\\text{det}}. First, the algorithm queries hx​(y)subscriptℎ𝑥𝑦h_{x}(y) at each (x,y)∈𝒳×{1,2​…​m}𝑥𝑦𝒳12…𝑚(x,y)\\in\\mathcal{X}\\times\\{1,2\\dots m\\}. This yields full information about the loss functions ftsubscript𝑓𝑡f_{t}. Then, the algorithm plans offline an optimal sequence of actions for the remaining timesteps. 𝔸​𝕃​𝔾det𝔸𝕃subscript𝔾det\\mathbb{ALG}_{\\text{det}} is formally specified as Algorithm 2 in Appendix C, and the following result shows that its CPR is minimax optimal (upto constant factors). Consider any (m,g,h)𝑚𝑔ℎ(m,g,h)-tallying bandit problem with deterministic bandit feedback. Then the complete policy regret of Algorithm 2 (𝔸​𝕃​𝔾det𝔸𝕃subscript𝔾det\\mathbb{ALG}_{\\text{det}}) is almost surely upper bounded as ℛTcp≤(m+1)​Ksubscriptsuperscriptℛcp𝑇𝑚1𝐾\\mathcal{R}^{\\text{cp}}_{T}\\leq(m+1)K. Moreover, there exists an (m,g,h)𝑚𝑔ℎ(m,g,h)-tallying bandit problem, such that the (expected) complete policy regret of any (possibly randomized) algorithm on this problem with deterministic feedback is lower bounded as 𝔼​[ℛTcp]≥m​K/128𝔼delimited-[]subscriptsuperscriptℛcp𝑇𝑚𝐾128\\mathbb{E}\\left[\\mathcal{R}^{\\text{cp}}_{T}\\right]\\geq mK/128. The proof of Proposition 1 is included in Appendix C. Due to the optimality of 𝔸​𝕃​𝔾det𝔸𝕃subscript𝔾det\\mathbb{ALG}_{\\text{det}} given deterministic feedback, it is reasonable to extend it to handle stochastic feedback. One natural way to do so is via an “explore then exploit” modification, which has been studied even for sMAB . Consider the following “explore then exploit” algorithm, which we denote 𝔸​𝕃​𝔾stoch𝔸𝕃subscript𝔾stoch\\mathbb{ALG}_{\\text{stoch}}. It queries repeatedly to receive stochastic realizations of hx​(y)subscriptℎ𝑥𝑦h_{x}(y) at each (x,y)∈𝒳×{1,2​…​m}𝑥𝑦𝒳12…𝑚(x,y)\\in\\mathcal{X}\\times\\{1,2\\dots m\\}, and constructs tight confidence intervals for these hx​(y)subscriptℎ𝑥𝑦h_{x}(y). It then uses these estimated values of hx​(y)subscriptℎ𝑥𝑦h_{x}(y) to plan offline an optimal sequence of actions for the remaining timesteps. 𝔸​𝕃​𝔾stoch𝔸𝕃subscript𝔾stoch\\mathbb{ALG}_{\\text{stoch}} is provably efficient, in the sense that its CPR is sublinear in T𝑇T and polynomial in m,K𝑚𝐾m,K. However, a standard argument  shows that the dependency on T𝑇T for 𝔸​𝕃​𝔾stoch𝔸𝕃subscript𝔾stoch\\mathbb{ALG}_{\\text{stoch}} scales as Θ~​(T2/3)~Θsuperscript𝑇23\\widetilde{\\Theta}\\left(T^{2/3}\\right), and it is unclear whether this dependency is optimal for tallying bandits. In the forthcoming section, we show that this dependency on T𝑇T can be significantly improved.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: bandit if the adversary is m𝑚m-memory bounded, g𝑔g-restricted and hℎh-tallying, and if after playing action atsubscript𝑎𝑡a_{t} the player observes a random variable h~at​(yt)∈ [ref]1subscript~ℎsubscript𝑎𝑡subscript𝑦𝑡01\\widetilde{h}_{a_{t}}(y_{t})\\in [ref]1 satisfying where\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Congested Bandits: Optimal Routing via Short-term Resets\",\n                \"Best arm identification in multi-armed bandits\",\n                \"Finite-time analysis of the multiarmed bandit problem\",\n                \"The nonstochastic multiarmed bandit problem\",\n                \"Policy Regret in Repeated Games\",\n                \"Online bandit learning against an adaptive adversary: From regret to policy regret\",\n                \"Competing in the dark: An efficient algorithm for bandit linear optimization\",\n                \"Minimax pac bounds on the sample complexity of reinforcement learning with a generative model\",\n                \"Minimax Regret Bounds for Reinforcement Learning\",\n                \"Multi-armed Bandit Problem with Known Trend\",\n                \"Stochastic multi-armed-bandit problem with non-stationary rewards\",\n                \"Blocking Bandits\",\n                \"Online learning with switching costs and other adaptive adversaries\",\n                \"Stochastic bandits with delay-dependent payoffs\",\n                \"Discrepancy-based algorithms for non-stationary rested bandits\",\n                \"Is seeing believing? how recommender system interfaces affect users’ opinions\",\n                \"A neurocognitive model of advertisement content and brand name recall\",\n                \"Bandits with switching costs: T2/3superscript𝑇23T^{2/3} regret\",\n                \"Is a good representation sufficient for sample efficient reinforcement learning?\",\n                \"PAC bounds for multi-armed bandit and markov decision processes\",\n                \"Online convex optimization in the bandit setting: Gradient descent without a gradient\",\n                \"On upper-confidence bound policies for switching bandit problems\",\n                \"Online submodular minimization\",\n                \"Tight policy regret bounds for improving and decaying bandits\",\n                \"Probability inequalities for sums of bounded random variables\",\n                \"Is Q-learning provably efficient?\",\n                \"Recharging bandits\",\n                \"Exploration in metric state spaces\",\n                \"Human memory: structures and processes\",\n                \"A sparse sampling algorithm for near-optimal planning in large markov decision processes\",\n                \"Break your bandit routine with LSD rewards: a last switch dependent analysis of satiation and seasonality\",\n                \"Rotting Bandits\",\n                \"Addressing the Long-term Impact of ML Decisions via Policy Regret\",\n                \"Asymptotically efficient adaptive allocation rules\",\n                \"On sequential strategies for loss functions with memory\",\n                \"Sample efficient reinforcement learning in continuous state spaces: A perspective beyond linearity\",\n                \"Competing with automata-based expert sequences\",\n                \"Recovering Bandits\",\n                \"Reinforcement Learning: An Introduction\",\n                \"Deconvolving Feedback Loops in Recommender Systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}