{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nChemical Properties\nWe implemented the first phase for the following\n22 chemical properties of monomers: \nÂ  autoignition temperature (At), biological half life (BHL),\ncritical pressure (Cp), \nÂ  critical temperature (Ct),\ndissociation constants (Dc),\nflammable limits lower (FlmL), \nÂ  flammable limits upper (FlmU),\nflash point in closed cup (Fp),\nmelting point (Mp), \nÂ  refractive index of trees (RfIdT),\nodor threshold lower (OdrL),\nodor threshold upper (OdrU) \nÂ  and vapor pressure (Vp) \nÂ  provided by HSDB from PubChemÂ [ref]30; \nÂ  solubility (Sl) by ESOLÂ ; \nÂ  autoignition temperature for organic compounds (AtO) by A. Dashti et al.Â ; \nÂ  flammable limits upper for organic compounds (FlmUO) by S.Â Yuan et al.Â ; \nÂ  flammable limits lower for gas (FlmLG) and \nÂ  flammable limits upper for gas (FlmUG) by S.Â Kondo et al.Â [ref]34;\nand \nÂ  energy of highest occupied molecular orbital (Homo), \nÂ  energy of lowest unoccupied molecular orbital (Lumo), \nÂ  the energy difference between Homo and Lumo (Gap) and \nÂ  electric dipole moment (mu) provided by MoleculeNetÂ ,\nwhere all these from Homo\nÂ  to mu are based on a common data set QM9. The data set QM9 contains more than 130,000 compounds.\nIn our experiment, we use a set of 1,000 compounds randomly selected from the data set.\nWe do not exclude any polymer from the original data set as outliers for these properties. Setting Data Sets\nFor each property Ï€ğœ‹\\pi,\nwe first select a set Î›Î›\\Lambda of chemical elements\nand then collect a data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi} on chemical graphs\nover the set Î›Î›\\Lambda of chemical elements.\nTo construct the data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi},\nwe eliminated chemical compounds that do not satisfy\none of the following: the graph is connected,\nthe number of carbon atoms is at least four,\nand the number of non-hydrogen neighbors of each atom is\nat most 4. We set a branch-parameter ÏğœŒ{\\rho} to be 2,\nintroduce linear descriptors defined by the two-layered graph\nin the chemical model without suppressing hydrogen\nand use the sets DÏ€(1)superscriptsubscriptğ·ğœ‹1D_{\\pi}^{(1)} and DÏ€(2)superscriptsubscriptğ·ğœ‹2D_{\\pi}^{(2)}\nof linear and quadratic descriptors\n(see AppendixÂ A for the details). For properties Ï€âˆˆ{\\pi\\in\\{BHL, FlmL, FlmLG, FlmU, FlmUO, FlmUG, OdrL, OdrU, Vp}}\\},\nwe take the logarithmic measurement of observed values aâ€‹(â„‚),â„‚âˆˆğ’ŸÏ€ğ‘â„‚â„‚subscriptğ’Ÿğœ‹a(\\mathbb{C}),\\mathbb{C}\\in\\mathcal{D}_{\\pi}, where\nlogâ¡(aâ€‹(â„‚)+c)ğ‘â„‚ğ‘\\log(a(\\mathbb{C})+c) (c=10âˆ’8ğ‘superscript108c=10^{-8} if Ï€=ğœ‹absent\\pi=Vp and c=0ğ‘0c=0 otherwise)\nis used as the observed value in our experiments. We normalize the range of each linear descriptor and\nthe range of observed values aâ€‹(â„‚),â„‚âˆˆğ’Ï€ğ‘â„‚â„‚subscriptğ’ğœ‹a(\\mathbb{C}),\\mathbb{C}\\in\\mathcal{C}_{\\pi}. We conducted an experiment of comparing the following five methods of constructing a prediction function. LLR: use Lasso linear regression on the set DÏ€(1)superscriptsubscriptğ·ğœ‹1D_{\\pi}^{(1)} of linear descriptors\n(see  for the detail of the implementation); ANN: use ANN on the set DÏ€(1)superscriptsubscriptğ·ğœ‹1D_{\\pi}^{(1)} of linear descriptors\n(see  for the detail of the implementation); ALR: use adjustive linear regression on the set DÏ€(1)superscriptsubscriptğ·ğœ‹1D_{\\pi}^{(1)} of linear descriptors\n(see  for the detail of the implementation); RLR: the learning method proposed by ZhuÂ et al.Â \nthat chooses a set of descriptors from\nthe set DÏ€(1)âˆªDÏ€(2)superscriptsubscriptğ·ğœ‹1superscriptsubscriptğ·ğœ‹2D_{\\pi}^{(1)}\\cup D_{\\pi}^{(2)} of linear and quadratic descriptors and\nthen constructs a prediction function by applying MLR (multidimensional linear regression)\nto the resulting set of descriptors; and HPS:\nour method of computing a hyperplane to split a given data set\ninto two subsets\nand constructing a prediction function to each subset independently.\nBy conducting a preliminary experiment,\nwe predetermine a threshold Î¸ğœƒ\\theta and a hyperplane (w,b),wâˆˆâ„|DÏ€(1)|ğ‘¤ğ‘ğ‘¤superscriptâ„superscriptsubscriptğ·ğœ‹1(w,b),w\\in\\mathbb{R}^{|D_{\\pi}^{(1)}|} and bâˆˆâ„ğ‘â„b\\in\\mathbb{R}\nin the feature space of linear descriptors.\nIn a cross-validation, we split a training data set ğ’^Ï€âŠ†ğ’Ï€subscript^ğ’ğœ‹subscriptğ’ğœ‹\\widehat{\\mathcal{C}}_{\\pi}\\subseteq\\mathcal{C}_{\\pi}\ninto two subsets ğ’^Ï€(j),j=1,2formulae-sequencesuperscriptsubscript^ğ’ğœ‹ğ‘—ğ‘—12\\widehat{\\mathcal{C}}_{\\pi}^{(j)},j=1,2 and construct\na prediction function Ïˆjsubscriptğœ“ğ‘—\\psi_{j} to each subset ğ’^Ï€(j)superscriptsubscript^ğ’ğœ‹ğ‘—\\widehat{\\mathcal{C}}_{\\pi}^{(j)}\nby applying one of the above four methods (i)-(iv). TableÂ 1 shows the size and range of data sets that\nwe prepared for each chemical property to construct a prediction function,\nwhere we denote the following: Ï€ğœ‹\\pi: the name of a chemical property used in the experiment. Î›Î›\\Lambda: a set of selected elements used in the data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi};\nÎ›Î›\\Lambda is one of the following eight sets: \nÎ›1={ğ™·,ğ™²,ğ™¾,ğ™½}subscriptÎ›1ğ™·ğ™²ğ™¾ğ™½\\Lambda_{1}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N}\\};\nÎ›2={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™µ}subscriptÎ›2ğ™·ğ™²ğ™¾ğ™½ğ™µ\\Lambda_{2}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt F}\\};\nÎ›3={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™²ğš•,ğš‚(2),ğš‚(6)}subscriptÎ›3ğ™·ğ™²ğ™¾ğ™½ğ™²ğš•subscriptğš‚2subscriptğš‚6\\Lambda_{3}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt Cl},{\\tt S}_{(2)},{\\tt S}_{(6)}\\};\nÎ›4={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™²ğš•,ğ™¿ğš‹}subscriptÎ›4ğ™·ğ™²ğ™¾ğ™½ğ™²ğš•ğ™¿ğš‹\\Lambda_{4}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt Cl},{\\tt Pb}\\};\nÎ›5={ğ™·,ğ™²,ğ™¾,ğ™½,ğš‚(2),ğš‚(4),ğš‚(6),ğ™²ğš•,ğ™±ğš›,ğ™µ}subscriptÎ›5ğ™·ğ™²ğ™¾ğ™½subscriptğš‚2subscriptğš‚4subscriptğš‚6ğ™²ğš•ğ™±ğš›ğ™µ\\Lambda_{5}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt S}_{(2)},{\\tt S}_{(4)},{\\tt S}_{(6)},{\\tt Cl},{\\tt Br},{\\tt F}\\};\nÎ›6={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™²ğš•,ğ™¿(2),ğ™¿(5)}subscriptÎ›6ğ™·ğ™²ğ™¾ğ™½ğ™²ğš•subscriptğ™¿2subscriptğ™¿5\\Lambda_{6}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt Cl},{\\tt P}_{(2)},{\\tt P}_{(5)}\\};\nÎ›7={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™²ğš•,ğ™±ğš›}subscriptÎ›7ğ™·ğ™²ğ™¾ğ™½ğ™²ğš•ğ™±ğš›\\Lambda_{7}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt Cl},{\\tt Br}\\};\nÎ›8={ğ™·,ğ™²,ğ™¾,ğ™½,ğ™²ğš•,ğ™±ğš›,ğ™µ}subscriptÎ›8ğ™·ğ™²ğ™¾ğ™½ğ™²ğš•ğ™±ğš›ğ™µ\\Lambda_{8}=\\{{\\tt H},{\\tt C},{\\tt O},{\\tt N},{\\tt Cl},{\\tt Br},{\\tt F}\\},\nwhere ğšŠ(i)subscriptğšŠğ‘–{\\tt a}_{(i)} for a chemical element ğšŠğšŠ{\\tt a} and an integer iâ‰¥1ğ‘–1i\\geq 1\nmeans that a chemical element ğšŠğšŠ{\\tt a} with valence iğ‘–i. |ğ’Ï€|subscriptğ’ğœ‹|\\mathcal{C}_{\\pi}|: the size of data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi} over the element set Î›Î›\\Lambda\nfor the property Ï€ğœ‹\\pi. nÂ¯,nÂ¯Â¯ğ‘›Â¯ğ‘›\\underline{n},~{}\\overline{n}:\nthe minimum and maximum values of the number\nnâ€‹(â„‚)ğ‘›â„‚n(\\mathbb{C}) of non-hydrogen atoms in\nthe compounds â„‚â„‚\\mathbb{C} in ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}. aÂ¯,aÂ¯Â¯ğ‘Â¯ğ‘\\underline{a},~{}\\overline{a}: the minimum and maximum values\nof aâ€‹(â„‚)ğ‘â„‚a(\\mathbb{C}) (or the logarithm of the original observed values)\nfor Ï€ğœ‹\\pi over the compounds â„‚â„‚\\mathbb{C} in ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}\nbefore we normalize them between 0 and 1. |Î“|Î“|\\Gamma|:\nthe number of different edge-configurations\nof interior-edges over the compounds inÂ ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}. |â„±|â„±|\\mathcal{F}|: the number of non-isomorphic chemical rooted trees\nin the set of all 2-fringe-trees in the compounds in ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}. K1subscriptğ¾1K_{1}: the size |DÏ€(1)|superscriptsubscriptğ·ğœ‹1|D_{\\pi}^{(1)}| of a set DÏ€(1)superscriptsubscriptğ·ğœ‹1D_{\\pi}^{(1)} of linear descriptors,\nwhere |DÏ€(2)|=(3â€‹K12+K1)/2superscriptsubscriptğ·ğœ‹23superscriptsubscriptğ¾12subscriptğ¾12|D_{\\pi}^{(2)}|=(3K_{1}^{2}+K_{1})/2 holds. Constructing Prediction Functions\nFor each chemical property Ï€ğœ‹\\pi, we construct a prediction function\nby one of the four methods (i)-(iv). For methods (i)-(iv), we used the same implementation\ndue to ZhuÂ et al.Â   \nand omit the details. TablesÂ 2 shows\nthe results on constructing prediction functions,\nwhere we denote the following: Ï€,Î›ğœ‹Î›\\pi,\\Lambda: an instance with a chemical property Ï€ğœ‹\\pi\nand a set Î›Î›\\Lambda of chemical elements selected from the data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}. LLR:\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for prediction functions constructed by the method (i). ANN:\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for prediction functions constructed by the method (ii). ALR:\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for prediction functions constructed by the method (iii). RLR:\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for prediction functions constructed by the method (iv). HPS:\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for the prediction function obtained\nby HPS, where a prediction function is obtained by combining\nthe prediction functions constructed for the first and second subsets. the score of LLR, ANN, ALR, RLR or HPS marked with â€œ*â€ indicates\nthe best performance among the five methods for the property Ï€ğœ‹\\pi; Î¸ğœƒ\\theta: the threshold used in HPS to split the data set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}. amax(1),amin(2)subscriptsuperscriptğ‘1subscriptsuperscriptğ‘2a^{(1)}_{\\max},a^{(2)}_{\\min}:\nthe maximum observed value aâ€‹(â„‚)ğ‘â„‚a(\\mathbb{C}) of a compound â„‚âˆˆğ’(1)â„‚superscriptğ’1\\mathbb{C}\\in\\mathcal{C}^{(1)} and\nthe minimum observed value aâ€‹(â„‚)ğ‘â„‚a(\\mathbb{C}) of a compound â„‚âˆˆğ’(2)â„‚superscriptğ’2\\mathbb{C}\\in\\mathcal{C}^{(2)}\nin HPS: |ğ’(1)|,|ğ’(2)|superscriptğ’1superscriptğ’2|\\mathcal{C}^{(1)}|,|\\mathcal{C}^{(2)}|:\nthe sizes of the first subset ğ’(1)superscriptğ’1\\mathcal{C}^{(1)} and the second subset ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}\nin HPS. ğ’(1)superscriptğ’1\\mathcal{C}^{(1)}-R2:\nthe name of the method (one of (i)-(iv))\nused to construct a prediction function to the first subset in HPS and\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for the prediction function over the first subset. ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}-R2:\nthe name of the method (one of (i)-(iv))\nused to construct a prediction function to the second subset in HPS and\nthe median of test R2superscriptR2\\mathrm{R}^{2}\nin ten 5-fold cross-validations for the prediction function over the second subset. The running time of constructing two prediction functions for subsets ğ’(1)superscriptğ’1\\mathcal{C}^{(1)} and ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}\nin HPS was at most around 13 seconds.\nTo execute RLR, selecting a subset of linear and quadratic descriptors\nis the most time consuming part, which took from 1162 to 44356 seconds. In TableÂ 2, HPS attains the best score of the median test R2\namong the five methods in most cases.\nEspecially the improvement over the methods (i)-(iv) is significant for properties\nAt, FlmL, FlmU, FlmUO, FlmUG, OdrL and OdrU. From the values amax(1),amin(2)subscriptsuperscriptğ‘1subscriptsuperscriptğ‘2a^{(1)}_{\\max},a^{(2)}_{\\min} in TableÂ 2,\nwe see how the original set ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi} is split with a hyperplane\ninto two subsets ğ’(j),j=1,2formulae-sequencesuperscriptğ’ğ‘—ğ‘—12\\mathcal{C}^{(j)},j=1,2 for each property Ï€ğœ‹\\pi.\nFor property At with Î»1subscriptğœ†1\\lambda_{1}, it holds that\namax(1)=0.505>amin(2)=0.293subscriptsuperscriptğ‘10.505subscriptsuperscriptğ‘20.293a^{(1)}_{\\max}=0.505>a^{(2)}_{\\min}=0.293, which implies that\nno hyperplane separates\nğ’â‰¤Î¸={â„‚iâˆˆğ’Ï€âˆ£aiâ‰¤Î¸}subscriptğ’absentğœƒconditional-setsubscriptâ„‚ğ‘–subscriptğ’ğœ‹subscriptğ‘ğ‘–ğœƒ\\mathcal{C}_{\\leq\\theta}=\\{\\mathbb{C}_{i}\\in\\mathcal{C}_{\\pi}\\mid a_{i}\\leq\\theta\\}\nand ğ’>Î¸={â„‚iâˆˆğ’Ï€âˆ£ai>Î¸}subscriptğ’absentğœƒconditional-setsubscriptâ„‚ğ‘–subscriptğ’ğœ‹subscriptğ‘ğ‘–ğœƒ\\mathcal{C}_{>\\theta}=\\{\\mathbb{C}_{i}\\in\\mathcal{C}_{\\pi}\\mid a_{i}>\\theta\\} for Î¸=0.40ğœƒ0.40\\theta=0.40.\nOn the other hand, for property AtO, it holds that\namax(1)=0.447<Î¸=0.45<amin(2)=0.451subscriptsuperscriptğ‘10.447ğœƒ0.45subscriptsuperscriptğ‘20.451a^{(1)}_{\\max}=0.447<\\theta=0.45<a^{(2)}_{\\min}=0.451, which implies that\nğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi} is split with a hyperplane into\nğ’(1)=ğ’â‰¤Î¸superscriptğ’1subscriptğ’absentğœƒ\\mathcal{C}^{(1)}=\\mathcal{C}_{\\leq\\theta}\nand ğ’(2)=ğ’>Î¸superscriptğ’2subscriptğ’absentğœƒ\\mathcal{C}^{(2)}=\\mathcal{C}_{>\\theta}. For some properties such as At with Î»1subscriptğœ†1\\lambda_{1}, the median R2 of HPS\nis considerably larger than the median R2 of\nğ’(1)superscriptğ’1\\mathcal{C}^{(1)}-R2 and ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}-R2.\nFor property Ï€=ğœ‹absent\\pi=Â At with Î»1subscriptğœ†1\\lambda_{1}, the median R2 of HPS is 0.808\nwhereas that of ğ’(1)superscriptğ’1\\mathcal{C}^{(1)} (resp., ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}) is 0.220 (resp., 0.521).\nThis can happen because the range [amin(1)=0,amax(1)=0.505]delimited-[]formulae-sequencesuperscriptsubscriptğ‘10superscriptsubscriptğ‘10.505[a_{\\min}^{(1)}=0,a_{\\max}^{(1)}=0.505]\n(resp., [amin(2)=0.293,amax(2)=1]delimited-[]formulae-sequencesuperscriptsubscriptğ‘20.293superscriptsubscriptğ‘21[a_{\\min}^{(2)}=0.293,a_{\\max}^{(2)}=1])\nof observed values for ğ’(1)superscriptğ’1\\mathcal{C}^{(1)} (resp., ğ’(2)superscriptğ’2\\mathcal{C}^{(2)})\nis again normalized to   \non which a prediction function Ïˆ1subscriptğœ“1\\psi_{1} (resp., Ïˆ2subscriptğœ“2\\psi_{2})\nis constructed and its learning performance\nğ’(1)superscriptğ’1\\mathcal{C}^{(1)}-R2 (resp., ğ’(2)superscriptğ’2\\mathcal{C}^{(2)}-R2)\nis evaluated.\nHowever, in the evaluation of Ïˆğœ“\\psi for the median R2 of HPS,\nthe error caused by each of the prediction functions Ïˆj,j=1,2formulae-sequencesubscriptğœ“ğ‘—ğ‘—12\\psi_{j},j=1,2\nis measured as a relatively smaller value\nover the original wider range of observed values to ğ’Ï€subscriptğ’ğœ‹\\mathcal{C}_{\\pi}.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: in closed cup (Fp),\nmelting point (Mp), \nÂ  refractive index of trees (RfIdT),\nodor threshold lower (OdrL),\nodor threshold upper (OdrU) \nÂ  and vapor pressure (Vp) \nÂ  provided by HSDB from PubChemÂ [ref]30; \nÂ  solubility (Sl) by ESOLÂ ; \nÂ  autoignition temperature for organic compounds (AtO) by A. Dashti et al.Â ; \nÂ  flammable limits upper for organic compounds (FlmUO) by S.Â Yuan et al.Â ;\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê³µê²© ê·¸ë˜í”„ ìƒì„±\",\n                \"Machine learning in chemoinformatics and drug discovery\",\n                \"From big data to artificial intelligence: chemoinformatics meets new challenges\",\n                \"QSAR modeling: where have you been? Where are you going to?\",\n                \"Inverse QSPR/QSAR analysis for chemical structure generation (from y to x)\",\n                \"Bayesian molecular design with a chemical language model\",\n                \"Strategy to discover diverse optimal molecules in the small molecule universe\",\n                \"Neural network and deep-learning algorithms used in QSAR studies: merits and drawbacks\",\n                \"Comprehensive survey of recent drug discovery using deep learning\",\n                \"Semi-Supervised Classification with Graph Convolutional Networks\",\n                \"The art and practice of structure-based drug design: A molecular modeling perspective\",\n                \"Inferring a graph from path frequency\",\n                \"Graph neural networks for automated de novo drug design\",\n                \"Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules\",\n                \"Grammar variational autoencoder\",\n                \"MolGAN: An implicit generative model for small molecular graphs\",\n                \"Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks\",\n                \"ChemTS: An Efficient Python Library for de novo Molecular Generation\",\n                \"GraphNVP: An Invertible Flow Model for Generating Molecular Graphs\",\n                \"GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation\",\n                \"An inverse QSAR method based on a two-layered model and integer programming\",\n                \"A method for molecular design based on linear regression and integer programming\",\n                \"A method for inferring polymers based on linear regression and integer programming\",\n                \"Molecular design based on artificial neural networks, integer programming and grid neighbor search\",\n                \"A novel method for inference of acyclic chemical compounds with bounded branch-height based on artificial neural networks and integer programming\",\n                \"Support-vector networks\",\n                \"Regression shrinkage and selection via the lasso\",\n                \"Adjustive linear regression and its application to the inverse QSAR\",\n                \"Molecular design based on integer programming and quadratic descriptors in a two-layered model\",\n                \"Simple but powerful goal programming models for discriminant problems\",\n                \"Quantitative structure property relationship schemes for estimation of autoignition temperatures of organic compounds\",\n                \"Developing quantitative structureâ€“property relationship models to predict the upper flammability limit using machine learning\",\n                \"Prediction of flammability of gases by using F-number analysis\",\n                \"Generated Reference Title 33 for [ref]30\",\n                \"Generated Reference Title 34 for [ref]30\",\n                \"Generated Reference Title 35 for [ref]30\",\n                \"Generated Reference Title 36 for [ref]30\",\n                \"Generated Reference Title 37 for [ref]30\",\n                \"Generated Reference Title 38 for [ref]30\",\n                \"Generated Reference Title 39 for [ref]30\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}