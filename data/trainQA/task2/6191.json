{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Guessing Winning Policies in LTL Synthesis by Semantic Learning This research was funded in part by the German Research Foundation (DFG) project 427755713 Group-By Objectives in Probabilistic Verification (GOPro)\n\nPaper Abstract: \n\nPaper Section Text:\nTo fairly compare to , we consider the same dataset, i.e. randomly generated LTL formulae, split into three categories: “(Co-)Safety”, “Near (Co-)Safety”, and “Parity”.\nSee  for details on how these are obtained.\nIn essence, the tool randltl [ref]7 is used to generate random formulae with different biases.\nThen, we filter out formulae which need more than 10 minutes to be translated to a parity automaton.\nAs a second dataset, we also use some (original and mutated) SYNTCOMP formulae (the test set described in Section 5.4).\nWe only consider formulae where the corresponding game can be won by system.\nWe do this simply because we can only recommend on games which are winning – otherwise there is no preference on edges since every action is losing by definition.\nIn total, this leaves 262 randomly generated formulae and 123 from SYNTCOMP. We consider two metrics for our comparison.\nFirstly, similar to , we consider the fraction of immediately solved games, i.e. games where following actions recommended by SemML or TrueSI directly yields a winning strategy.\nIn light of our motivation to augment SI solvers, we want to measure how “close” the recommended strategy is to being correct in case is not immediately winning.\nTo this end, we feed it to (a modified version of) the parity game solver Oink  and compute the (relative) distance of the obtained strategy, as follows.\nWe count the number of (reachable) states in which the winning strategy determined by Oink differs from the recommended one, i.e. how many “wrong” choices were recommended, and divide it by the total amount of (reachable) states.\nWe note that this unfortunately induces a slight bias that we cannot measure:\nOink may potentially change winning decisions because of internal details of the algorithm.\nIdeally, we would want to obtain the minimal distance over all winning strategies; however this quantity is intractable to compute due to the exponential size of the strategy space.\nNevertheless, we believe that this measure strongly correlates with the quality of the strategy. We argue that simply measuring the number of iterations required by strategy iteration to converge is a too crude metric:\nOn the one hand, even a “very wrong” strategy can be changed to a winning strategy in a single iteration by changing the choice in every single state.\nOn the other hand, even a nearly correct strategy, requiring only a hand full of changes, may need as many iterations.\nMoreover, this additionally induces the same bias as above. Since our approach incorporates trueness as one of its many features, we expect that our approach should be at least on par with the previous one of .\nAs we also consider long-term temporal information beyond trueness, we particularly expect to outperform TrueSI on larger, more complicated instances. We ran this evaluation on consumer hardware (Intel Core i7-8565U with 16GB RAM).\nWe summarize our findings in Table 1.\nClearly, our approach vastly outperforms the previous one.\nIn particular, while TrueSI perfectly handles (co-)safety formulae, its performance quickly drops when going to more complicated formulae.\nIn comparison, the SemML solves the vast majority of formulae immediately, even on the quite complicated SYNTCOMP dataset.\nWe note that these findings are not “absolute” (as to be expected from machine learning approaches).\nThere are few instances where the previous approach does perform better.\nOur baseline comparison to a random initialization approach validates that both approaches indeed solve a non-trivial problem. Since we are particularly interested in complex, “human written” formulae, we investigate the SYNTCOMP dataset more closely.\nIn Fig. 5, we provide a more detailed view on our two metrics.\nFirst, we investigate how the “immediately solving” performance evolves in comparison to the size of the game, which intuitively correlates with the difficulty of the synthesis question.\nWe observe that SemML solves practically all smaller games and still performs well on larger games, compared to TrueSI, which quickly falls off.\nThe second plot displays the relative distances for each instance which neither recommendation solved immediately.\nWe clearly see that the strategies recommended by SemML are better in almost all cases. This positively answers our first question.\nAside from the direct comparison to the previous approach, the significant percentage of immediately solved games gives us an interesting implication:\nIf SemML solves many games immediately, we can use SemML as a best-effort guidance tool for reactive synthesis questions which are intractably large to solve.\nMoreover, SemML thus presents us with a constant size representation of a winning strategy for many games, effectively described by approximately a few hundred SVM weights compared to a decision table for thousands of states in each game.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: dataset, i.e. randomly generated LTL formulae, split into three categories: “(Co-)Safety”, “Near (Co-)Safety”, and “Parity”.\nSee  for details on how these are obtained.\nIn essence, the tool randltl [ref]7 is used to generate random formulae with different biases.\nThen, we filter out formulae which need more than 10 minutes to be translated to a parity automaton.\nAs a second dataset, we also use\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"From LTL to Deterministic Automata: A Safraless Compositional Approach\",\n                \"Artifact for Guessing Winning Policies in LTL Synthesis by Semantic Learning\",\n                \"Permissive strategies: from parity games to safety games\",\n                \"Deciding parity games in quasi-polynomial time\",\n                \"Iterative Circuit Repair Against Formal Specifications\",\n                \"Oink: An implementation and evaluation of modern parity game solvers\",\n                \"Spot 2.0 - A framework for LTL and \\\\omega -automata manipulation\",\n                \"From LTL and limit-deterministic büchi automata to deterministic parity automata\",\n                \"From linear temporal logic and limit-deterministic büchi automata to deterministic parity automata\",\n                \"One theorem to rule them all: A unified translation of LTL into ω𝜔\\\\omega-automata\",\n                \"A unified translation of linear temporal logic to ω𝜔\\\\omega-automata\",\n                \"Efficient parallel strategy improvement for parity games\",\n                \"An Ordered Approach to Solving Parity Games in Quasi Polynomial Time and Quasi Linear Space\",\n                \"Solving parity games in practice\",\n                \"Rabinizer: Small deterministic automata for ltl(f, G)\",\n                \"Solving parity games on the GPU\",\n                \"The Reactive Synthesis Competition (SYNTCOMP): 2018–2021\",\n                \"Deciding the winner in parity games is in UP \\\\cap co-up\",\n                \"Rabinizer 3: Safraless translation of LTL to small deterministic automata\",\n                \"Rabinizer 2: Small deterministic automata for LTL ∖\\\\setminus GU\",\n                \"Semantic labelling and learning for parity game solving in LTL synthesis\",\n                \"Owl: A library for ω𝜔\\\\omega-words, automata, and LTL\",\n                \"Rabinizer 4: From LTL to your favourite deterministic automaton\",\n                \"Index appearance record with preorders\",\n                \"The blowup in translating LTL to deterministic automata\",\n                \"A recursive approach to solving parity games in quasipolynomial time\",\n                \"LTL satisfiability checking revisited\",\n                \"Learning to rank for information retrieval\",\n                \"Practical synthesis of reactive systems from LTL specifications via parity games\",\n                \"Solving mean-payoff games on the GPU\",\n                \"Strix: Explicit reactive synthesis strikes back!\",\n                \"An introduction to game theory\",\n                \"Scikit-learn: Machine learning in Python\",\n                \"From nondeterministic buchi and streett automata to deterministic parity automata\",\n                \"The temporal logic of programs\",\n                \"On the synthesis of an asynchronous reactive module\",\n                \"On the complexity of omega-automata\",\n                \"Tighter bounds for the determinisation of büchi automata\",\n                \"Neural Circuit Synthesis from Specification Patterns\",\n                \"Limit-deterministic büchi automata for linear temporal logic\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of Guessing Winning Policies in LTL Synthesis by Semantic Learning This research was funded in part by the German Research Foundation (DFG) project 427755713 Group-By Objectives in Probabilistic Verification (GOPro). The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}