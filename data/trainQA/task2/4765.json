{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training\n\nPaper Abstract: \n\nPaper Section Text:\nIn the following we present how a collection of captioned pictures implicitly defines a common space for images and texts through relative representations [ref]13, allowing to build a multimodal model without training.\nHere we focus exclusively on vision and language as modalities due to the wider availability of relevant models and paired data. However, we anticipate that our procedure could be more generally applicable.\nIndeed, subsequent research by other teams has already utilized ASIF as a baseline in other modalities, such as audio .\nBefore delving into the specifics of our method, we will briefly review existing techniques for establishing this common space. Contrastive training to build a common space.   With multimodal models, we refer to architectures that\nembed inputs of diverse modes into the same space. The better is a multimodal model, the closer are representations of different modes of the same object.\nSo far, this common space has been obtained as the result of a contrastive training of one  or both the neural mode encoders [ref]1 . Using a collection of image-text pairs as training set, a contrastive loss promotes the closeness of image and text embeddings from the same pair, while spacing out the ones from distinct pairs. Zhai et al.  train just the text encoder to match the image embeddings of a pretrained visual encoder.\nOnce the encoders are trained, a multimodal model can be adapted to perform any visual classification task just by crafting a caption for each label and selecting the one with the embedding closer to the embedding of the input image (zero-shot image classification). Relative representations.  Our idea to build a common latent space is to use a collection of coupled data as a “rosetta stone” between modalities, and represent each new data point as its similarities to the points of the same modality in the collection. In other words, we compute a relative representation for each new data point: Given an encoder E:X→ℝd:𝐸→𝑋superscriptℝ𝑑{\\color[rgb]{0.27734375,0.73046875,0.69921875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.27734375,0.73046875,0.69921875}{E}}:X\\to{\\color[rgb]{0.27734375,0.73046875,0.69921875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.27734375,0.73046875,0.69921875}{\\mathbb{R}^{d}}} and a subset of samples {x1,…,xn}subscript𝑥1…subscript𝑥𝑛\\{x_{1},\\dots,x_{n}\\} denoted as anchors, we define the relative representation of x′superscript𝑥′x^{\\prime} as the n𝑛n-dimensional vector: for some similarity function sim, e.g. the cosine similarity.\nxi∈Xsubscript𝑥𝑖𝑋x_{i}\\in X are input samples, e.g. images or texts; xi∈ℝdsubscript𝑥𝑖superscriptℝ𝑑{\\color[rgb]{0.27734375,0.73046875,0.69921875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.27734375,0.73046875,0.69921875}{{x_{i}}}}\\in{\\color[rgb]{0.27734375,0.73046875,0.69921875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.27734375,0.73046875,0.69921875}{{\\mathbb{R}^{d}}}} are the embeddings of xisubscript𝑥𝑖x_{i} obtained with the encoder E𝐸{\\color[rgb]{0.27734375,0.73046875,0.69921875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.27734375,0.73046875,0.69921875}{{E}}}, i.e. the absolute representations of xisubscript𝑥𝑖x_{i}; while xi∈ℝnsubscript𝑥𝑖superscriptℝ𝑛{\\color[rgb]{0.4453125,0.6328125,0.04296875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.4453125,0.6328125,0.04296875}{{x_{i}}}}\\in{\\color[rgb]{0.4453125,0.6328125,0.04296875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.4453125,0.6328125,0.04296875}{{\\mathbb{R}^{n}}}} are the relative representations of xisubscript𝑥𝑖x_{i}. We observe that when each anchor is available in two or more modalities, we can compute relative representations of samples from those modalities using the same subset of anchors. Most notably, these relative representations will all live in the same space, even when they are representing samples from different modalities. This foundational insight is illustrated in Figure 2. Relation with Kernel methods. Definition 2.1 may not look surprising to readers familiar with the literature on kernel methods . Instead of presenting x′superscript𝑥′{\\color[rgb]{0.4453125,0.6328125,0.04296875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.4453125,0.6328125,0.04296875}{x^{\\prime}}} as a kernel, we say it is a relative representation to stress that (1) we want to explicitly represent the coordinates in our ASIF procedure as opposed to operating in an implicit feature space and (2) we do not aim at learning regression parameters, although we note this may help with the inference speed. Instead, we rely on a simpler procedure that may be interpreted as a hard version of the Watson-Nadaraya  [ref]18 regression with a distance threshold. Nevertheless, it is worth noting that while the ASIF procedure hinges on the ability to compute similarities between samples, such computation can be achieved using a kernel function, thus sidestepping the need for explicit representations generated by unimodal encoders.\nAlthough integrating kernel methods could offer benefits, as alluded to earlier, delving into these prospects is beyond the scope of this paper. Our central focus remains on illustrating how single-domain pre-trained networks can be aligned without additional training. ASIF: relative representations inducing a meaningful common space.  Consider the embedding spaces of any two good text and visual encoders,\nwe expect captions of images that are close in the visual space to be themselves close in the language space.\nThis fact makes a representation defined in terms of similarities against a set of ground-truth multimodal pairs almost mode-invariant, i.e. an image and its caption share almost the same representation. That is why we can assign the best caption to a new image x∗superscript𝑥x^{*} just by performing nearest neighbors in this new space: we can consider the\nrelative representation of x∗superscript𝑥x^{*} respect to the image collection (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},\\dots,x_{n}) as if it was the relative representation of its ideal caption y∗superscript𝑦y^{*} with respect to the counterpart collection (y1,…,yn)subscript𝑦1…subscript𝑦𝑛(y_{1},\\dots,y_{n}), see Figure 4.\nThe whole procedure to set up an ASIF model and use it to find the best caption for a new image follows. Properties of ASIF models.  The above construction yields several intriguing properties for free: No training and “data centric”.\nAs we have seen, an ASIF model is built on top of two independently pretrained encoders and the embeddings of a multimodal dataset, and so without training or finetuning any neuron.\nBeing deployable or updatable in seconds, an ASIF model is radically “data centric” . For example, it is trivial to adjust the model by adding or forgetting specific samples.\nThe latter use-case is particularly important, as the right to use specific assets may change over time and removing the effect of specific samples from a trained network requires sophisticated forgetting techniques, e.g.     . In our procedure, the encoders should be pre-trained with established data sets that do not change over time, while removing the effect of a multimodal pair is as simple as deleting its embeddings. Data efficiency: Being able to exploit two pretrained encoders, ASIF models require far less ground-truth multimodal pairs to become effective. As confirmed by our experiments, ASIF reaches competitive zero-shot performance on diverse classification benchmarks by using a fraction of the multimodal data of its predecessors, reaching a respectable accuracy even with thousands of pairs (we refer to Section 3 for more details). This is in line with classical work in computer vision, where prototypical networks  are a strong baseline in the extremely few-shot regime. Interpretability: Sparse relative representations make the ASIF models interpretable classifiers. In fact, we can trace back every prediction to a small set of data points in the multimodal dataset –corresponding to the dimensions that are nonzero in both the image and the label relative representations– accountable for the outcome (at most k𝑘k), see Figure 6. This enables visualizations of the relevant samples contributing to a correct or incorrect prediction at no extra cost, in stark contrast with other approaches that are often costly 0 and fragile 1 2. Relation to k-Nearest Neighbors. Like k-NN, ASIF is a non-parametric supervised learning algorithm that requires an explicit representation of every entry in the training dataset at test time. Differently from k-NN, ASIF can perform open-ended classification, as shown e.g. in Figure 4 with two competing brand new captions. Indeed, ASIF is functionally equivalent to CLIP, and can function as a drop-in replacement in applications using CLIP-like models. Implementation that scales.   Clearly, our method pays the price of avoiding training with a larger memory footprint and increased computation at inference time, since we need to compute not only the embeddings but also the cosine similarities against the multimodal dataset.\nAs such, our approach should not be considered a general one-stop replacement for CLIP, although in our experiments we managed to scale ASIF to 1.6M pairs while maintaining a reasonable inference speed. Our non-optimized implementation of ASIF is less than 2x slower than CLIP.\nOn a positive note, there are two well-established techniques that could radically enhance the efficiency of ASIF and potentially enable scalability to billions of entries.\nThe memory footprint required to store all the embeddings of the multimodal dataset can be dramatically reduced using product quantization 3, while inverse indexing 4 can be used to circumvent the need for computing the cosine similarities against the entire dataset at test time. These techniques are both implemented e.g. in the FAISS library 5.\nFinally, we find that the distribution of pairs chosen during inference is rather short-tailed, presenting opportunities to massively prune the model even at deployment time, deleting from memory the data that is never used. It should be noted, however, that the assessment of the performance of large-scale optimized ASIF models is beyond the scope of this work. While it is an interesting direction for future research, the focus of our current study is on establishing the potential the ASIF method.\n\nReference markers found: [13]\n\nContext windows for each [ref]:\n[ref]13: In the following we present how a collection of captioned pictures implicitly defines a common space for images and texts through relative representations [ref]13, allowing to build a multimodal model without training.\nHere we focus exclusively on vision and language as modalities due to the wider availability of relevant models and paired data.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 13,\n            \"titles\": [\n                \"Understanding Shared Speech-Text Representations\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"LiT: Zero-Shot Transfer with Locked-image text Tuning\",\n                \"On the opportunities and risks of foundation models\",\n                \"Explanatory Learning: Beyond Empiricism in Neural Networks\",\n                \"Backpropagation applied to handwritten zip code recognition\",\n                \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era\",\n                \"Mixed-Privacy Forgetting in Deep Networks\",\n                \"Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations\",\n                \"Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks\",\n                \"Making AI Forget You: Data Deletion in Machine Learning\",\n                \"Certified Data Removal from Machine Learning Models\",\n                \"Unbiggen ai\",\n                \"Relative representations enable zero-shot latent space communication\",\n                \"Scaling up visual and vision-language representation learning with noisy text supervision\",\n                \"Kernel methods in machine learning\",\n                \"On estimating regression\",\n                \"Smooth regression analysis\",\n                \"Prototypical Networks for Few-shot Learning\",\n                \"LQF: Linear Quadratic Fine-Tuning\",\n                \"Understanding Black-box Predictions via Influence Functions\",\n                \"Influence Functions in Deep Learning Are Fragile\",\n                \"Product quantization for nearest neighbor search\",\n                \"Video google: a text retrieval approach to object matching in videos\",\n                \"Billion-scale similarity search with GPUs\",\n                \"The barrier of meaning\",\n                \"Analogy as the core of cognition\",\n                \"Improving language models by retrieving from trillions of tokens\",\n                \"Few-shot learning with retrieval augmented language models\",\n                \"Discrete key-value bottleneck\",\n                \"Retrieval-Augmented Reinforcement Learning\",\n                \"Neural Production Systems\",\n                \"Retrieval-augmented generation for code summarization via hybrid gnn\",\n                \"Generalized orthogonal matching pursuit\",\n                \"Matching pursuits with time-frequency dictionaries\",\n                \"A unified optimization view on generalized matching pursuit and frank-wolfe\",\n                \"On Matching Pursuit and Coordinate Descent\",\n                \"Yfcc100m: The new data in multimedia research\",\n                \"Devise: A deep visual-semantic embedding model\",\n                \"Deep Visual-Semantic Alignments for Generating Image Descriptions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]13 based on the context and research area of ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}