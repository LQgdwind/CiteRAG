{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Self-Training Vision Language BERTs with a Unified Conditional Model\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we talk about the self-training algorithm. Figure¬†3 illustrates the training process of our algorithm. We first train a UCM model with the human labeled data from COCO and VG datasets. Then we repeat two steps: generating pseudo labeling on Conceptual Caption unlabeled data and retraining the UCM model with mixed data from COCO, VG and Conceptual Caption. Train UCM with labeled data. We first train UCM using captioning annotations in MSCOCO, dense captioning annotations in Visual Genome and questions in VQA¬† and GQA dataset¬†[ref]35. The trained UCM is able to generate the above three different styles of content. Annotate unlabeled data with trained UCM. We then use the trained UCM to generate pseudo labels on images from Conceptual Captions dataset. Conceptual Caption dataset provides one caption for each image by default, while the default captions are machine-generated, not of good quality¬†[ref]9 and are often used as out-of-domain training data¬†. Therefore, we remove the original captions and use the data as unlabeled image data. To boost the performance of self-training, the generated captions need to be diverse. We introduce 3 methods to generate diverse image captions for each image. First, we perform image-level augmentations. We randomly mask object regions when generating captions. Empirically, each image regional feature is masked with a ratio of 0.5. This method makes sure the model can focus on different visual areas when generating outputs. Second, we perform augmentations when sampling language words. We randomly sample a word from the top-K predictions in each prediction step, such that even for the same image input, the model could generate different captioning outputs. We choose K=5ùêæ5K=5 based on the common design choice of Image Captioning¬†. This is especially useful when generating dense captions. The result shows that the generated dense captions usually focus on one object region. Given one fixed image, the generated dense captions will always be the same without sampling. Compared with beam-search method, the top-K sampling method is faster but may potentially generate noisier captions. In experiments, we observe the performance differences between the two methods are negligible according to finetuning accuracy. One possible reason is that the generated captions are only used as pseudo labels and pseudo labels are noisy labels by default. Some works[ref]4 even purposely add noise to the generation process of pseudo labels. Therefore, we use top-K sampling method to speed up the generation process. We discard the generated questions because they usually contain less information than the captions. Finally, we use the condition flag to control the contents generated. For each image, we generate 5 captions with MSCOCO flag and 10 captions with VG Dense caption flag. The condition flag conditions the style of the generated contents. Train new model by mixing labeled and unlabeled data. After pseudo labels are generated for unlabeled images, we mix the pseudo labeled data and original labeled data to train a new model. Unlike self-training methods in image classification¬†[ref]4, which train new models from scratch, we propose to initialize the model with the last pretrained weight. The design choice is based on 2 considerations. First, vision language BERT pretraining takes a long time. Loading pretrained weight can help to reduce the training time of new models. Second, for image classification tasks, if we directly use soft classification labels to describe the unlabeled image and load previously trained weights, the loss will be zero on pseudo labeled data because the labels are exactly from the model itself. Compared with soft classification labels, generated captions are generated from sampling and do not directly describe the output distribution of previous models. This property reduces the risk that loading the previous model will result in zero loss on pseudo labeled data. This design choice also shares the same spirit as previous self-training works¬† , where the teacher models‚Äô weights are derived from the student models. Iterating the previous steps. Following the common practice of self-training, we iterate the ‚ÄúAnnotate unlabeled data with trained UCM‚Äù and ‚ÄúTrain new model by mixing labeled and unlabeled data‚Äù steps a few times to get better performance. A detailed ablation study is shown in Section ¬†IV.\n\nReference markers found: [35]\n\nContext windows for each [ref]:\n[ref]35: VG and Conceptual Caption. Train UCM with labeled data. We first train UCM using captioning annotations in MSCOCO, dense captioning annotations in Visual Genome and questions in VQA¬† and GQA dataset¬†[ref]35. The trained UCM is able to generate the above three different styles of content. Annotate unlabeled data with trained UCM. We then use the trained UCM to generate pseudo labels on images\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 35,\n            \"titles\": [\n                \"Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\",\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\",\n                \"Bert has a mouth, and it must speak: Bert as a markov random field language model\",\n                \"Self-training with noisy student improves imagenet classification\",\n                \"Rethinking Pre-training and Self-training\",\n                \"Language Models are Few-Shot Learners\",\n                \"LXMERT: Learning Cross-Modality Encoder Representations from Transformers\",\n                \"Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning\",\n                \"Are we pretraining it right? Digging deeper into visio-linguistic pretraining\",\n                \"UNITER: UNiversal Image-TExt Representation Learning\",\n                \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\n                \"Movie question answering via textual memory and plot graph\",\n                \"Action-centric relation transformer network for video question answering\",\n                \"Loss-rescaling vqa: Revisiting language prior problem from a class-imbalance view\",\n                \"FVQA: Fact-based Visual Question Answering\",\n                \"Re-attention for visual question answering\",\n                \"Topic-oriented image captioning based on order-embedding\",\n                \"Image captioning with end-to-end attribute detection and subsequent attributes prediction\",\n                \"Task-adaptive attention for image captioning\",\n                \"Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning\",\n                \"Efficient video grounding with which-where reading comprehension\",\n                \"Attention Is All You Need\",\n                \"12-in-1: Multi-Task Vision and Language Representation Learning\",\n                \"VisualBERT: A Simple and Performant Baseline for Vision and Language\",\n                \"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training\",\n                \"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\",\n                \"Unified Vision-Language Pre-Training for Image Captioning and VQA\",\n                \"Billion-scale semi-supervised learning for image classification\",\n                \"Improving neural machine translation models with monolingual data\",\n                \"Semi-Supervised Learning for Neural Machine Translation\",\n                \"Exploiting monolingual data at scale for neural machine translation\",\n                \"Unified Language Model Pre-training for Natural Language Understanding and Generation\",\n                \"Language models are unsupervised multitask learners\",\n                \"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\",\n                \"Gqa: a new dataset for compositional question answering over real-world images\",\n                \"Improving consistency-based semi-supervised learning with weight averaging\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n                \"Microsoft coco captions: Data collection and evaluation server\",\n                \"Visual genome: Connecting language and vision using crowdsourced dense image annotations\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]35 based on the context and research area of Self-Training Vision Language BERTs with a Unified Conditional Model. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}