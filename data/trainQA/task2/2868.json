{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Neural Network Ensemble Approach to System Identification\n\nPaper Abstract: \n\nPaper Section Text:\nSystem identification refers to the problem of building mathematical models and approximating governing equations using only observed data from the system. Governing laws and equations have traditionally been derived from expert knowledge and first principles, however in recent years the large amount of data available resulted in a growing interest in data-driven models and approaches for automated dynamical systems discovery. The applications of system identification include any system where the inputs and outputs can be measured, such as industrial processes, control systems, economic data and financial systems, biology and the life sciences, medicine, social systems, and many more (see for instance  for more examples of applications). Examples of frequently used approaches for data-driven discovery of nonlinear differential equations are sparse regression, Gaussian processes, applied Koopmanism and dictionary based approaches, among which neural networks. Sparse regression approaches are based on a user-determined library of candidate terms from which the most important ones are selected using sparse regression (see for instance , , [ref]28, ). These methods provide interpretable results, but they are usually sensitive to noise and require the user to choose an ‚Äúappropriate‚Äù sets of basis functions. Identification using Gaussian Processes places a Gaussian prior on the unknown coefficients of the differential equation and infers them via maximum likelihood estimation (see for instance , [ref]24, ). The Koopman approach is based on the idea that non linear system identification in the state space is equivalent to linear identification of the Koopman operator in the infinite-dimensional space of observables. The power of the Koopman approach is that it allows to study non-linear systems using traditional techniques in numerical linear algebra.\nHowever, since the Koopman operator is infinite-dimensional, in practice one computes a projection of the Koopman operator onto a finite-dimensional subspace of the observables. This approximation may result in models of very high dimension and has proven challenging in practical applications (see for instance , , ). In this work we use a different approach based on neural networks.\nSince neural networks are universal approximators, they are a natural choice for nonlinear system identification: depending on the architecture and on the properties of the loss function, they can be used as sparse regression models, they can act as priors on unknown coefficients or completely determine an unknown differential operator (see for instance , , , , , [ref]26, , , ). The common goal among all such methods is learning a nonlinear and potentially\nmulti-variate mapping fùëìf, right-hand-side of the differential equation: that can be used to predict the future system states given a set of data describing the present and past states. Two main approaches can be used to approximate the function fùëìf with a neural network. The first approach aims at approximating the function fùëìf directly, like we did in our previous paper . In this work, inspired by the work of Oberman and Calder in  , we use a Lipschitz regularized neural network to approximate the RHS of the ODE (1), directly from observations of the state vector x‚Äã(t)ùë•ùë°x(t). The target data for the network is made of discrete approximations of the velocity vector xÀô‚Äã(t)Àôùë•ùë°\\dot{x}(t), which act as a prior for fùëìf. To generate the target data we first denoise the trajectory data using spline interpolation, then we approximate the velocity vector using the numerical derivative of the splines. In the rest of the paper we refer to this method as splines method. One limitation of this approach is that, in order to obtain accurate approximations of the function fùëìf, one needs to obtain reliable target data, approximations of the velocity vector, from the observations of x‚Äã(t)ùë•ùë°x(t). This proved to be hard when a large amount of noise (more that 2%) was present in the data or when splines could not approximate the trajectories correctly. When instead we could obtain high quality target data, we empirically proved that, thanks to the Lipschitz regularization, our method was robust to noise and able to provide an accurate approximation of the function fùëìf.\nThe second approach aims at approximating the function fùëìf implicitly by expressing the differential equation (1) in integral form and enforcing that the network that approximates fùëìf satisfies an appropriate update rule. This is the approach used in [ref]26, which we refer to as multistep method, where the authors train the approximating network to satisfy a linear multistep method. An advantage of this approach over the previous one is that the target data used to train the multistep network is composed only of observations of the state vector x‚Äã(t)ùë•ùë°x(t). However, noise in the observations of x‚Äã(t)ùë•ùë°x(t) can still have a strong impact on the quality of the network approximation of fùëìf. \nLater on we will compare these methods with our proposed approach. In this work we build on the second approach and introduce a new idea to overcome the limitations of the methods mentioned above. Similarly to the multistep method, we express the differential equation in integral form and train the network that approximates fùëìf to satisfy Euler update rule (with minimal modifications one can use linear multistep methods as well). This implicit approach overcomes the limitations of the splines method, whose results were strongly dependent on the quality of the velocity vector approximations used as target data. Differently than the multistep method, our proposed approach is based on a Lipschitz regularized ensemble of neural networks and it is able to overcome the sensitivity to noise.\nMore specifically, we consider the system of ODEs (1)\nwhere x‚Äã(t)‚àà‚Ñùdùë•ùë°superscript‚Ñùùëëx(t)\\in\\mathbb{R}^{d} is the state vector of a dùëëd-dimensional dynamical system at time t‚ààI‚äÇ‚Ñùùë°ùêº‚Ñùt\\in I\\subset\\mathbb{R}, xÀô‚Äã(t)‚àà‚ÑùdÀôùë•ùë°superscript‚Ñùùëë\\dot{x}(t)\\in\\mathbb{R}^{d} is the first order time derivative of x‚Äã(t)ùë•ùë°x(t) and f:‚Ñù1+d‚Üí‚Ñùd:ùëì‚Üísuperscript‚Ñù1ùëësuperscript‚Ñùùëëf:\\mathbb{R}^{1+d}\\rightarrow\\mathbb{R}^{d} is a vector-valued function right-hand side of the differential equation. We approximate the unknown function fùëìf with an ensemble of neural networks. A neural network ensemble is a learning paradigm where a finite number of networks are jointly used to solve a problem. An ensemble algorithm is generally constructed in two steps: first multiple component neural networks are trained to produce component predictions; then they are combined to produce a final prediction (for a more precise explanation see ). In their work , Hansen and Salamon showed that the generalization ability of a neural network architecture can be significantly improved through ensembling. This is the reason why we use an ensemble of neural networks, instead of only one network as it was done in [ref]26. Our proposed ensemble architecture is composed of two blocks: the first, which we call target data generator is an ensemble of neural networks whose goal is to produce accurate velocity vector approximations using only observations of x‚Äã(t)ùë•ùë°x(t). To train this ensemble of networks, we express equation (1) in integral form and use Euler method to predict the solution at every successive time step using at each iteration a different neural network as a prior for fùëìf. If MùëÄM denotes the number of time steps at which x‚Äã(t)ùë•ùë°x(t) is observed, then the procedure described above yields M‚àí1ùëÄ1M-1 time-independent networks, each of which approximates the velocity vector xÀô‚Äã(t)Àôùë•ùë°\\dot{x}(t) for a fixed time tùë°t. The second block of the ensemble architecture is the interpolation network. This is a Lipschitz regularized feed forward network NùëÅN as defined in . This network takes as input an observation of the time tùë°t and of the state vector x‚Äã(t)ùë•ùë°x(t) and uses as target data the approximations of the velocity vector generated by the target data generator. Once trained, the interpolation network provides the desired approximation of the RHS function fùëìf on its domain. Finally we want to comment on our choice of using ensembles of neural networks as compared to the other methods listed above for system identification. In our experience and from a literature review, neural networks are a good choice for function approximation because of their ability to learn and model non-linear and complex functions as well as to generalize to unseen data. For example, it has been shown empirically in  that neural networks outperform polynomial regression when complicated interactions are present in the function to approximate. We also show in Section 4.3 that, for noisy data, our Lipschitz regularized ensemble approach outperforms the splines and mutlistep methods as well as polynomial regression and the dictionary based method SINDy (Sparse Identification of Nonlinear Dynamics) .\nSince neural networks are universal approximators, we do not need any prior information about the order or about the analytical form of the differential equation as in , [ref]28, , , ; this allows to accurately recover very general and complex RHS functions even when no information on the target function is available.\n\nSince the proposed ensemble method is based on weak notion of solution using integration (see formula (3)) it can be used to reconstruct non-smooth RHS functions (see Example 4.2.1). This is especially an advantage over models that rely on the notion of classical solution like the Splines Method . The ability of our proposed method to accurately approximate both smooth and non-smooth functions make it an extremely valuable approach when working with real-world data.\nAnother advantage of our ensemble approach is its ability to overcome sensitivity to noise and avoid overfitting. This is due to the fact that we use an ensemble of networks to produce our predictions as well as to the Lipschitz regularization term in the loss function of the interpolation network. The ability of our method to overcome sensitivity to noise is especially an advantage over works that use finite differences and polynomial approximation to extract governing equations from data (, [ref]28), over the Koopman based methods where noise in the data can impact the quality of the finite dimensional approximation of the Koopman operator (3, ), as well as over the multistep method [ref]26.\nFinally, our model is defined componentwise so it can be applied to system of equations of any dimension, making it a valuable approach when dealing with high dimensional real-world data. The flexibility and noise robustness of our approach comes, however, at the cost of loss of interpretability and increased computational cost. Training a neural network ensemble is more computationally expensive than training only one neural network, as it is the case in [ref]26 and , or than using polynomial regression or SINDy. Moreover, the learned ensemble is usually less interpretable than a sparse model based on a dictionary of elementary functions, especially when the number of network learnable parameters is large. However, the trained ensemble produces very accurate results and implements a function which can be easily used in future computations, for example to generate new trajectories like we do in Section 4. The paper is organized as follows: in Section 2 we describe the ensemble architecture and the loss function used in the training; in Section 3 we describe how the synthetic data was generated, the metrics used to evaluate our method and we precisely define the generalization gap; in Section 4 we propose numerical examples, we show how the ensemble method is an improvement over our previous method and we compare it with other methods for system identification. In Section 5 we discuss our numerical results. Finally, in the conclusion Section we summarize our results and describe possible future directions of research.\n\nReference markers found: [28]\n\nContext windows for each [ref]:\n[ref]28: neural networks. Sparse regression approaches are based on a user-determined library of candidate terms from which the most important ones are selected using sparse regression (see for instance , , [ref]28, ). These methods provide interpretable results, but they are usually sensitive to noise and require the user to choose an ‚Äúappropriate‚Äù sets of basis functions. Identification using Gaussian\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 28,\n            \"titles\": [\n                \"Learning Equations for Extrapolation and Control\",\n                \"Learning from data, volume 4\",\n                \"For valid generalization the size of the weights is more important than the size of the network\",\n                \"Data-driven discovery of PDEs in complex datasets\",\n                \"Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains\",\n                \"Discovering governing equations from data by sparse identification of nonlinear dynamical systems\",\n                \"Applied Koopmanism\",\n                \"Bayesian lipschitz constant estimation and quadrature\",\n                \"Data-driven discovery of coordinates and governing equations\",\n                \"A functional approximation comparison between neural networks and polynomial regression\",\n                \"Neural network ensembles\",\n                \"LEARNING PARTIAL DIFFERENTIAL EQUATIONS FROM DATA USING NEURAL NETWORKS\",\n                \"Approximating the Koopman operator using noisy data: noise-resilient extended dynamic mode decomposition\",\n                \"Pose invariant face recognition\",\n                \"Learning with ensembles: How over-fitting can be useful\",\n                \"Deep learning for universal linear embeddings of nonlinear dynamics\",\n                \"Neural networks and dynamical systems\",\n                \"Applied Koopman theory for partial differential equations and data-driven modeling of spatio-temporal systems\",\n                \"System Identification Through Lipschitz Regularized Deep Neural Networks\",\n                \"Lipschitz regularized deep neural networks converge and generalize\",\n                \"Nonlinear systems identification using deep dynamic neural networks\",\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"Data Driven Governing Equations Approximation Using Deep Neural Networks\",\n                \"Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations\",\n                \"Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations\",\n                \"Machine Learning of Linear Differential Equations using Gaussian Processes\",\n                \"Multistep neural networks for data-driven discovery of nonlinear dynamical systems\",\n                \"Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations\",\n                \"Data-driven discovery of partial differential equations\",\n                \"Learning partial differential equations via data discovery and sparse optimization\",\n                \"Sparse dynamics for partial differential equations\",\n                \"Classification of seismic signals by integrating ensembles of neural networks\",\n                \"On robust computation of Koopman operator and prediction in random dynamical systems\",\n                \"SciPy 1.0‚ÄîFundamental Algorithms for Scientific Computing in Python\",\n                \"A fully automated recurrent neural network for unknown dynamic system identification and control\",\n                \"Lung cancer cell identification based on artificial neural network ensembles\",\n                \"Generated Reference Title 36 for [ref]28\",\n                \"Generated Reference Title 37 for [ref]28\",\n                \"Generated Reference Title 38 for [ref]28\",\n                \"Generated Reference Title 39 for [ref]28\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]28 based on the context and research area of A Neural Network Ensemble Approach to System Identification. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}