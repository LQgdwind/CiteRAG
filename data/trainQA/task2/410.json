{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: How to generate random lambda terms?\n\nPaper Abstract: \n\nPaper Section Text:\nFor many years, the exact-size sampling paradigm was the de facto\nstandard in combinatorial generation. Its long-lasting dominance was brought to\nan end with the seminal paper of Duchon, Flajolet, Louchard, and\nSchaefferÂ  who introduced the framework of\nBoltzmann models. Instead of generating random structures of fixed size\nnğ‘›n, Boltzmann models provide a more relaxed, approximate-size sampling\nenvironment which uses the analytic properties of associated generating functions. Fix a class ğ’œğ’œ\\mathcal{A} of combinatorial structures we want to sample. Under the\nBoltzmann model we assign to each object Î±âˆˆğ’œğ›¼ğ’œ\\alpha\\in\\mathcal{A} a\nprobability measure where xğ‘¥x is some aptly chosen, positive real-valued control parameter,\nand |Î±|ğ›¼|\\alpha| denotes the size of object Î±ğ›¼\\alpha. If we sample in accordance\nwith the above Boltzmann distribution, outcome structures of equal size\ninvariably occur with the same probability. Boltzmann models retain\ntherefore the usual requirement of uniformity however now, the size of\nthe outcome object is no longer fixed, but instead varies. Indeed, let Nğ‘N be a\nrandom variable denoting the outcome size of a Boltzmann sampler\ngenerating objects according toÂ (14). Summing over all\nÎ±âˆˆğ’œğ›¼ğ’œ\\alpha\\in\\mathcal{A} of size nğ‘›n we find that We can control the expected size or generated objects by choosing suitable\nvalues of the control parameter xğ‘¥x. The expected average and standard\ndeviation of Nğ‘N satisfy, respectively, Suppose, for instance, that we want to design a Boltzmann model for plane binary\ntrees. Recall that the associated generating function Câ€‹(z)ğ¶ğ‘§C(z) satisfies the\nrelation Câ€‹(z)=1+zâ€‹Câ€‹(z)2ğ¶ğ‘§1ğ‘§ğ¶superscriptğ‘§2C(z)=1+z{C(z)}^{2}. If we want to centre the mean output tree size\naround some fixed value nğ‘›n, we need to find a suitable control parameter xğ‘¥x\nsuch that ğ”¼xâ€‹(N)=nsubscriptğ”¼ğ‘¥ğ‘ğ‘›\\mathbb{E}_{x}(N)=n. A direct computations reveals that we should\nuse So, if we want to obtain a random tree in some admissible size window\n[(1âˆ’Îµ)â€‹n,(1+Îµ)â€‹n]1ğœ€ğ‘›1ğœ€ğ‘›{[(1-\\varepsilon)n,(1+\\varepsilon)n]} with tolerance Îµğœ€\\varepsilon, we\ncalibrate xğ‘¥x according toÂ (17) and reject\ntrees falling outside of the prescribed size window. Boltzmann samplers.\nFor many interesting instances of decomposable combinatorial classes, such as\nrational languages or algebraic data types, it is possible to\nautomatically design appropriate Boltzmann models and corresponding\nsamplers generating random objects in accordance with the underlying Boltzmann\ndistribution,\nseeÂ   [ref]6 . Much like exact-size recursive samplers, the Boltzmann sampler layout follows\nthe inductive structure of the target specification. Suppose that we want to\nsample an object from a (disjoint) union class ğ’œ=â„¬+ğ’ğ’œâ„¬ğ’\\mathcal{A}=\\mathcal{B}+\\mathcal{C}. According to the\nBoltzmann distributionÂ (14) the\nprobability of sampling an object from â„¬â„¬\\mathcal{B} is equal to Bâ€‹(x)/Aâ€‹(x)ğµğ‘¥ğ´ğ‘¥B(x)/A(x).\nLikewise, the probability of sampling an object from ğ’ğ’\\mathcal{C} is equal to Câ€‹(x)/Aâ€‹(x)ğ¶ğ‘¥ğ´ğ‘¥C(x)/A(x). The corresponding Boltzmann sampler Î“â€‹(ğ’œ)Î“ğ’œ\\Gamma(\\mathcal{A}) for ğ’œğ’œ\\mathcal{A}\nperforms therefore a single Bernoulli trial with parameter Bâ€‹(x)/Aâ€‹(x)ğµğ‘¥ğ´ğ‘¥B(x)/A(x). If\nsuccessful, the sampler invokes Î“â€‹(â„¬)Î“â„¬\\Gamma(\\mathcal{B}). Otherwise, it calls the\nremaining sampler Î“â€‹(ğ’)Î“ğ’\\Gamma(\\mathcal{C}). Clearly, such a sampler conforms with the\nBoltzmann distribution. Now, suppose that we want to sample an object from a product class ğ’œ=â„¬Ã—ğ’ğ’œâ„¬ğ’\\mathcal{A}=\\mathcal{B}\\times\\mathcal{C}. According to the Boltzmann distribution, the probability\nmeasure of a product object Î±=(Î²,Î³)âˆˆğ’œğ›¼ğ›½ğ›¾ğ’œ\\alpha=(\\beta,\\gamma)\\in\\mathcal{A} satisfies Therefore, in order to sample an object from ğ’œğ’œ\\mathcal{A} we can independently\ninvoke samplers Î“â€‹(â„¬)Î“â„¬\\Gamma(\\mathcal{B}) and Î“â€‹(ğ’)Î“ğ’\\Gamma(\\mathcal{C}), collect their outcomes,\nand construct a pair out of them. The result is a random object from ğ’œğ’œ\\mathcal{A}. The disjoint union ++ and Cartesian product Ã—\\times suffice to design\nBoltzmann samplers for algebraic specifications, in particular unambiguous\ncontext-free grammars. Let us remark, however, that Boltzmann sampler are\nnot just limited to these two operations.\nOver the years, efficient Boltzmann samplers for both labelled and unlabelled\nstructures were developed. Let us just mention Boltzmann samplers for\nvarious PÃ³lya structuresÂ , first-order\ndifferential specificationsÂ [ref]15, labelled planar\ngraphsÂ , or plane partitionsÂ . In most cases, especially when optimised using anticipated\nrejectionÂ , Boltzmann samplers are remarkably efficient,\nfrequently generating random structures of sizes in the millions. For typical\nalgebraic specifications, Boltzmann samplers calibrated to the target size\nwindow [(1âˆ’Îµ)â€‹n,(1+Îµ)â€‹n]1ğœ€ğ‘›1ğœ€ğ‘›[(1-\\varepsilon)n,(1+\\varepsilon)n] admit, on average, a linear Oâ€‹(n)ğ‘‚ğ‘›O(n)\ntime complexity under the real arithmetic computation\nmodelÂ  . For exact-size\nsampling where the tolerance Îµ=0ğœ€0\\varepsilon=0, the complexity becomes Oâ€‹(n2)ğ‘‚superscriptğ‘›2O(n^{2}). Boltzmann samplers for Î»ğœ†\\lambda-terms were first developed by\nLescanneÂ   for\nTrompâ€™s binary Î»ğœ†\\lambda-calculusÂ .\nTrompâ€™s special encoding of (plain) lambda terms as binary strings, provided\nan unambiguous context-free specification fitting the framework of\nBoltzmann samplers. With their help, it became possible to generate\nÎ»ğœ†\\lambda-terms a couple of orders of magnitude larger than using\nrecursive method. Boltzmann models, especially if used with rejection methods, provide a rich\nsource of random Î»ğœ†\\lambda-terms. Although quite general, Boltzmann\nsamplers cannot be used for all term representations and size notions. Recall\nthat in order to derive a corresponding Boltzmann model, we have to, inter\nalia, evaluate the associated generating functions at the calibration parameter\nxğ‘¥x. For some representations, such as for instance Wangâ€™s closed\nÎ»ğœ†\\lambda-term modelÂ , the associated\ngenerating function is not analytic around the complex plane origin333In\nsuch a case the corresponding counting sequence grows super-exponentially fast,\ni.e.Â asymptotically faster than any exponential function Cnsuperscriptğ¶ğ‘›C^{n}.. Moreover,\nwithout a finite specification we cannot derive a finite set of Boltzmann\nsamplers. In such cases we have to resort to alternative sampling methods\nincluding, e.g.Â rejection-based samplers.\n\nReference markers found: [6]\n\nContext windows for each [ref]:\n[ref]6: data types, it is possible to\nautomatically design appropriate Boltzmann models and corresponding\nsamplers generating random objects in accordance with the underlying Boltzmann\ndistribution,\nseeÂ   [ref]6 . Much like exact-size recursive samplers, the Boltzmann sampler layout follows\nthe inductive structure of the target specification. Suppose that we want to\nsample an object from a (disjoint)\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 6,\n            \"titles\": [\n                \"Statistical properties of lambda terms\",\n                \"Testing AUTOSAR software with QuickCheck\",\n                \"Almost Every Simply Typed Lambda-Term Has a Long Beta-Reduction Sequence\",\n                \"Exact-size Sampling for Motzkin Trees in Linear Time via Boltzmann Samplers and Holonomic Specification\",\n                \"Type-assignment in the Lambda-calculus\",\n                \"Quantitative aspects and generation of random lambda and combinatory logic terms\",\n                \"Polynomial tuning of multiparametric combinatorial samplers\",\n                \"Random generation of closed simply typed Î»ğœ†\\\\lambda-terms: A synergy between logic programming and Boltzmann samplers\",\n                \"A natural counting of lambda terms\",\n                \"Combinatorics of lâ€‹aâ€‹mâ€‹bâ€‹dâ€‹ağ‘™ğ‘ğ‘šğ‘ğ‘‘ğ‘lambda-terms: a natural approach\",\n                \"Random sampling of plane partitions\",\n                \"Asymptotics and random sampling for BCI and BCK lambda terms Generation of Combinatorial Structures\",\n                \"Pointed versus singular Boltzmann samplers: a comparative analysis\",\n                \"Enumerating lambda terms by weighted length of their De Bruijn representation\",\n                \"Boltzmann samplers for first-order differential specifications V Latin American Algorithms, Graphs, and Optimization Symposium â€” Gramado, Brazil\",\n                \"On Uniquely Closable and Uniquely Typable Skeletons of Lambda Terms\",\n                \"On the Number of Unary-Binary Tree-Like Structures with Restrictions on the Unary Height\",\n                \"Lambda calculus notation with nameless dummies, a tool for automatic formula manipulation, with application to the Church-Rosser theorem\",\n                \"Fast and Sound Random Generation for Automated Testing and Benchmarking in Objective Caml\",\n                \"QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs\",\n                \"Counterexample-Guided Abstraction Refinement\",\n                \"Calcul pratique des coefficients de Taylor dâ€™une fonction algebrique\",\n                \"Asymptotically almost all Î»ğœ†\\\\lambda-terms are strongly normalizing\",\n                \"Uniform random generation of decomposable structures using floating-point arithmetic\",\n                \"Boltzmann Samplers for the Random Generation of Combinatorial Structures\",\n                \"Boltzmann Sampling of Unlabelled Structures\",\n                \"Analytic Combinatorics\",\n                \"A calculus for the random generation of labelled combinatorial structures\",\n                \"Quadratic exact size and linear approximate size random generation of planar graphs\",\n                \"On the Number of Lambda Terms With Prescribed Size of Their De Bruijn Representation\",\n                \"Counting and generating lambda terms\",\n                \"Counting and generating terms in the binary lambda calculus\",\n                \"The Art of Computer Programming, Volume 4, Fascicle 4: Generating All Treesâ€“History of Combinatorial Generation (Art of Computer Programming)\",\n                \"The Ultimate Challenge: The 3â€‹x+13ğ‘¥13x+1 Problem\",\n                \"Counterexample to Eulerâ€™s conjecture on sums of like powers\",\n                \"Boltzmann samplers for random generation of lambda terms\",\n                \"On Counting Untyped Lambda Terms\",\n                \"Quantitative aspects of linear and affine closed lambda terms\",\n                \"The Glasgow Haskell Compiler\",\n                \"Effect-Driven QuickChecking of Compilers\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]6 based on the context and research area of How to generate random lambda terms?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}