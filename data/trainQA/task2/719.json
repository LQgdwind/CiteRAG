{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nThe distributed coloring problem is arguably the most intensively studied problem in the area of distributed graph algorithms and certainly also one of the most intensively studied problems in distributed computing more generally. The standard assumption is that the coloring graph â€“ the graph on which we want to compute a coloring â€“ is also the communication network â€“ the graph forming the network topology. We explore in this paper the case when the latter is weaker than the former: the communication is constrained, and direct links are not available to all the â€œneighborsâ€ that are to be colored differently. The primary setting for this is the distance-222 coloring problem in the standard distributed CONGEST model. Given a graph G=(V,E)ğºğ‘‰ğ¸G=(V,E), in the d2-coloring problem on GğºG, the objective is to assign a color xvsubscriptğ‘¥ğ‘£x_{v} to each node vâˆˆVğ‘£ğ‘‰v\\in V such that any two nodes uğ‘¢u and vğ‘£v at distance at most 222 in GğºG are assigned different colors xuâ‰ xvsubscriptğ‘¥ğ‘¢subscriptğ‘¥ğ‘£x_{u}\\neq x_{v}. Equivalently, d2-coloring asks for a coloring of the nodes of GğºG such that for every uâˆˆVğ‘¢ğ‘‰u\\in V, all the nodes in the set {u}âˆªNâ€‹(u)ğ‘¢ğ‘ğ‘¢\\left\\{u\\right\\}\\cup N(u) (where Nâ€‹(u)ğ‘ğ‘¢N(u) denotes the set of neighbors of uğ‘¢u) are assigned distinct colors. Further note that d2-coloring on GğºG is also equivalent to the usual vertex coloring problem on the graph G2superscriptğº2G^{2}, where Vâ€‹(G2)=Vğ‘‰superscriptğº2ğ‘‰V(G^{2})=V and there is an edge {u,v}âˆˆEâ€‹(G2)ğ‘¢ğ‘£ğ¸superscriptğº2\\left\\{u,v\\right\\}\\in E(G^{2}) whenever dGâ€‹(u,v)â‰¤2subscriptğ‘‘ğºğ‘¢ğ‘£2d_{G}(u,v)\\leq 2. The CONGEST model is a standard synchronous message passing modelÂ [ref]31. The graph on which we want to compute a coloring is also assumed to form the network topology. Each node uâˆˆVğ‘¢ğ‘‰u\\in V of the graph has a unique Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n)-bit identifier IDâ¡(u)IDğ‘¢\\operatorname{ID}(u), where n=|V|ğ‘›ğ‘‰n=|V| is the number of nodes of GğºG. Time is divided into synchronous rounds and in each round, each node uâˆˆVğ‘¢ğ‘‰u\\in V of GğºG can do some arbitrary internal computation, send a (potentially different) message to each of its neighbors vâˆˆNâ€‹(u)ğ‘£ğ‘ğ‘¢v\\in N(u), and receive the messages sent by its neighbors in the current round. If the size of the messages is not restricted, the model is known as the LOCAL modelÂ  [ref]31. In the CONGEST model, it is further assumed that each message consists of at most Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n) bits. As our main result, we give an efficient Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n)-time randomized algorithm for d2-coloring GğºG with at most Î”2+1superscriptÎ”21\\Delta^{2}+1 colors, where Î”Î”\\Delta is the maximum degree of GğºG. This improves on a recent Oâ€‹(logâ¡Î”â‹…logâ¡n)ğ‘‚â‹…Î”ğ‘›O(\\log\\Delta\\cdot\\log n)-time algorithm  and it matches the best known bound for ordinary distance-1 (Î”+1)Î”1(\\Delta+1)-coloring in CONGEST as a function of nğ‘›n alone.\nWe further explore more efficient algorithms when Î”â‰ªnmuch-less-thanÎ”ğ‘›\\Delta\\ll n. Combining our main method with a range of powerful recent techniques, we obtain an algorithm that runs in time Oâ€‹(logâ¡Î”)+2Oâ€‹(logâ¡logâ¡n)ğ‘‚Î”superscript2ğ‘‚ğ‘›O(\\log\\Delta)+2^{O(\\sqrt{\\log\\log n})}. Before discussing our results in more detail, we first discuss why we believe d2-coloring is interesting, what is known for the corresponding coloring problems on GğºG and why it is challenging to transform CONGEST algorithms to color GğºG into CONGEST algorithms for d2-coloring. Wireless networking is a major motivation for distance-2 coloring, where nodes with a common neighbor should not simultaneously communicate to avoid a collision at the common neighborÂ 5 . While the coloring is to be used for scheduling, the wireless channel need not be the medium for computing the coloring. With the advent of software-defined radio and hierarchical / heterogeneous networks, it is well motivated to consider coloring computation in a communication model more powerful than radio networks. Yet, asking for the different-message-to/from-all-neighbors feature of CONGEST may be hoping for too much.\nMore generally, we view it as a major question in distributed graph algorithms whether one can relax the communication requirements for graph coloring. We ask: How constrained can the communication structure be to allow for fast (logarithmic, sublogarithmic) distributed graph coloring computation? Distributed d2-coloring is an interesting and important problem for several other reasons. The d2-coloring problem for example also occurs naturally when single-round randomized algorithms are derandomized using the method of conditional expectationÂ .\nd2-coloring in CONGEST is further of special interest as it appears to lie at the edge of what is computable efficiently, i.e., in polylogarithmic time, while distance-3 coloring is even hard to verify . Distance-kğ‘˜k problems have not been addressed widely in a distributed setting, partly because distance-kğ‘˜k communication can be simulated in kğ‘˜k steps of the LOCAL model. In CONGEST, the situation changes drastically as simulating a single round of a distance-1 coloring algorithm can incur a factor Î˜â€‹(Î”kâˆ’1)Î˜superscriptÎ”ğ‘˜1\\Theta(\\Delta^{k-1}) overhead, i.e., even for k=2ğ‘˜2k=2, the overhead can be linear in Î”Î”\\Delta. Even the very simple algorithm where each node picks a random available color cannot be efficiently used for d2-coloring as it is in general not possible to keep track of the set of colors chosen by 222-hop neighbors in time oâ€‹(Î”)ğ‘œÎ”o(\\Delta).\nRecently, HalldÃ³rsson, Kuhn and Maus  treated d2-coloring in CONGEST and gave a randomized algorithm using Î”2+1superscriptÎ”21\\Delta^{2}+1-colors in Oâ€‹(logâ¡Î”â‹…logâ¡n)ğ‘‚â‹…Î”ğ‘›O(\\log\\Delta\\cdot\\log n) rounds, as well as a deterministic algorithm using (1+Ïµ)â€‹Î”21italic-ÏµsuperscriptÎ”2(1+\\epsilon)\\Delta^{2} colors in pâ€‹oâ€‹lâ€‹yâ€‹(logâ¡n)ğ‘ğ‘œğ‘™ğ‘¦ğ‘›poly(\\log n) rounds. Our main approach builds heavily on their framework, while simplifying certain features and strengthening structural properties. Distributed graph optimization problems on G2superscriptğº2G^{2} (with CONGEST-communication in GğºG) such as vertex cover and minimum dominating set have recently been studied in . The standard variant of the distributed coloring problem on GğºG asks for computing a vertex coloring with at most Î”+1Î”1\\Delta+1 colors, which is computed by a simple sequential greedy algorithm.\nThe main focus in the literature on distributed coloring has been on the LOCAL model, where by now the problem is understood relatively well.\nThe best randomized (Î”+1)Î”1(\\Delta+1)-coloring algorithm known in the LOCAL model, due to Chang, Li, and PettieÂ , runs in polyâ¡logâ¡logâ¡npolyğ‘›\\operatorname{poly}\\log\\log n rounds. The complexity given in  is 2Oâ€‹(logâ¡logâ¡n)superscript2ğ‘‚ğ‘›2^{O(\\sqrt{\\log\\log n})}, while the improvement to polyâ¡logâ¡logâ¡npolyğ‘›\\operatorname{poly}\\log\\log n immediately follows from the recent breakthrough work on deterministic network decomposition of RozhoÅˆ and GhaffariÂ .\nFor a more detailed discussion of related work on distributed coloring, we refer to   . While most known distributed coloring algorithms were developed for the LOCAL model,\nmany of them work directly in the CONGEST model, including those in       0    . Still, the best complexity known for coloring in CONGEST, as a function of nğ‘›n alone, is Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n), which is achieved by the following very simple OneShotColoring algorithm:\nInitially all nodes are uncolored. The algorithm runs in synchronous phases, where in each phase, each still uncolored node vğ‘£v chooses a uniform random color among its available colors (i.e., among the colors that have not already been picked by a neighbor) and vğ‘£v keeps the color if no of its uncolored neighbors tries the same color at the same timeÂ  1. The only known published algorithm in CONGEST with a better bound is due to GhaffariÂ 9, who obtains a (Î”+1)Î”1(\\Delta+1)-coloring in time Oâ€‹(logâ¡Î”)+2Oâ€‹(logâ¡logâ¡n)ğ‘‚Î”superscript2ğ‘‚ğ‘›O(\\log\\Delta)+2^{O(\\sqrt{\\log\\log n})}. The second term is due to a network decomposition algorithm also introduced in 9. Unlike for results in the LOCAL model, it is not directly possible to replace this decomposition with the recent construction in  to improve the dependence on nğ‘›n. The reason is that the complexity of the network decomposition construction of  grows at least linearly in the length of the node identifier bit strings. In the LOCAL model, it is possible to use a standard coloring algorithm of  to first map the IDs to Oâ€‹(logâ¡logâ¡n)ğ‘‚ğ‘›O(\\log\\log n)-bit values that are unique up to a sufficient distance so that one can afterwards apply the algorithm of . Subsequent to the publication of our results  improved upon the network decomposition algorithm from  (to deal with large IDs in the CONGEST model) and as a result obtains a Oâ€‹(logâ¡Î”)+polyâ¡logâ¡logâ¡nğ‘‚Î”polyğ‘›O(\\log\\Delta)+\\operatorname{poly}\\log\\log n CONGEST algorithm for (Î”+1)Î”1(\\Delta+1)-coloring. Note that if we have graphs of size Nğ‘N and if IDs and colors can be represented with polyâ¡logâ¡Npolyğ‘\\operatorname{poly}\\log N bits, there is a recent deterministic (ğ‘‘ğ‘’ğ‘”+1)ğ‘‘ğ‘’ğ‘”1(\\mathit{deg}+1)-list coloring algorithm running in polylogâ¡Npolylogğ‘\\operatorname{polylog}N time in CONGEST .\n\nReference markers found: [31]\n\nContext windows for each [ref]:\n[ref]31: and there is an edge {u,v}âˆˆEâ€‹(G2)ğ‘¢ğ‘£ğ¸superscriptğº2\\left\\{u,v\\right\\}\\in E(G^{2}) whenever dGâ€‹(u,v)â‰¤2subscriptğ‘‘ğºğ‘¢ğ‘£2d_{G}(u,v)\\leq 2. The CONGEST model is a standard synchronous message passing modelÂ [ref]31. The graph on which we want to compute a coloring is also assumed to form the network topology. Each node uâˆˆVğ‘¢ğ‘‰u\\in V of the graph has a unique Oâ€‹(logâ¡n)ğ‘‚ğ‘›O(\\log n)-bit identifier\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 31,\n            \"titles\": [\n                \"Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization\",\n                \"A fast and simple randomized parallel algorithm for the maximal independent set problem\",\n                \"Sublinear algorithms for (Î”+1) vertex coloring\",\n                \"Network decomposition and locality in distributed computation\",\n                \"Efficient deterministic distributed coloring with small bandwidth\",\n                \"Distributed approximation on power graphs\",\n                \"Deterministic (Î” + 1)-coloring in sublinear (in Î”) time in static, dynamic and faulty networks\",\n                \"Deterministic distributed vertex coloring in polylogarithmic time\",\n                \"Distributed Graph Coloring: Fundamentals and Recent Developments\",\n                \"Locally-Iterative Distributed (Î”+1)-Coloring below Szegedy-Vishwanathan Barrier, and Applications to Self-Stabilization and to Restricted-Bandwidth Models\",\n                \"Distributed (Î”+1)-coloring in linear (in Î”) time\",\n                \"The locality of distributed symmetry breaking\",\n                \"An algorithmic approach to the LovaÅ›z local lemma\",\n                \"Derandomizing local distributed algorithms under bandwidth restrictions\",\n                \"An optimal distributed (Î”+1)-coloring algorithm?\",\n                \"A spatial-reuse TDMA/FDMA for mobile multi-hop radio networks\",\n                \"Deterministic Distributed Dominating Set Approximation in the CONGEST Model\",\n                \"(2â€‹Î”âˆ’1)-edge-coloring is much easier than maximal matching in the distributed setting\",\n                \"Distributed testing of distance-k colorings\",\n                \"Distributed maximal independent set using small messages\",\n                \"Improved Deterministic Network Decomposition\",\n                \"On derandomizing local distributed algorithms\",\n                \"Improved Network Decompositions using Small Messages with Applications on MIS, Neighborhood Covers, and Beyond\",\n                \"Distance-2 Coloring in the CONGEST Model\",\n                \"Distributed (Î”+1)-coloring in sublogarithmic rounds\",\n                \"Simple distributed Î”+1-coloring of graphs\",\n                \"Faster deterministic distributed coloring through recursive list coloring\",\n                \"On the complexity of distributed graph coloring\",\n                \"Locality in distributed graph algorithms\",\n                \"A simple parallel algorithm for the maximal independent set problem\",\n                \"MAC layer and coloring\",\n                \"Distributed Computing: A Locality-Sensitive Approach\",\n                \"A new technique for distributed symmetry breaking\",\n                \"Generated Reference Title 33 for [ref]31\",\n                \"Generated Reference Title 34 for [ref]31\",\n                \"Generated Reference Title 35 for [ref]31\",\n                \"Generated Reference Title 36 for [ref]31\",\n                \"Generated Reference Title 37 for [ref]31\",\n                \"Generated Reference Title 38 for [ref]31\",\n                \"Generated Reference Title 39 for [ref]31\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]31 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}