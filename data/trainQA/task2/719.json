{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nThe distributed coloring problem is arguably the most intensively studied problem in the area of distributed graph algorithms and certainly also one of the most intensively studied problems in distributed computing more generally. The standard assumption is that the coloring graph – the graph on which we want to compute a coloring – is also the communication network – the graph forming the network topology. We explore in this paper the case when the latter is weaker than the former: the communication is constrained, and direct links are not available to all the “neighbors” that are to be colored differently. The primary setting for this is the distance-222 coloring problem in the standard distributed CONGEST model. Given a graph G=(V,E)𝐺𝑉𝐸G=(V,E), in the d2-coloring problem on G𝐺G, the objective is to assign a color xvsubscript𝑥𝑣x_{v} to each node v∈V𝑣𝑉v\\in V such that any two nodes u𝑢u and v𝑣v at distance at most 222 in G𝐺G are assigned different colors xu≠xvsubscript𝑥𝑢subscript𝑥𝑣x_{u}\\neq x_{v}. Equivalently, d2-coloring asks for a coloring of the nodes of G𝐺G such that for every u∈V𝑢𝑉u\\in V, all the nodes in the set {u}∪N​(u)𝑢𝑁𝑢\\left\\{u\\right\\}\\cup N(u) (where N​(u)𝑁𝑢N(u) denotes the set of neighbors of u𝑢u) are assigned distinct colors. Further note that d2-coloring on G𝐺G is also equivalent to the usual vertex coloring problem on the graph G2superscript𝐺2G^{2}, where V​(G2)=V𝑉superscript𝐺2𝑉V(G^{2})=V and there is an edge {u,v}∈E​(G2)𝑢𝑣𝐸superscript𝐺2\\left\\{u,v\\right\\}\\in E(G^{2}) whenever dG​(u,v)≤2subscript𝑑𝐺𝑢𝑣2d_{G}(u,v)\\leq 2. The CONGEST model is a standard synchronous message passing model [ref]31. The graph on which we want to compute a coloring is also assumed to form the network topology. Each node u∈V𝑢𝑉u\\in V of the graph has a unique O​(log⁡n)𝑂𝑛O(\\log n)-bit identifier ID⁡(u)ID𝑢\\operatorname{ID}(u), where n=|V|𝑛𝑉n=|V| is the number of nodes of G𝐺G. Time is divided into synchronous rounds and in each round, each node u∈V𝑢𝑉u\\in V of G𝐺G can do some arbitrary internal computation, send a (potentially different) message to each of its neighbors v∈N​(u)𝑣𝑁𝑢v\\in N(u), and receive the messages sent by its neighbors in the current round. If the size of the messages is not restricted, the model is known as the LOCAL model  [ref]31. In the CONGEST model, it is further assumed that each message consists of at most O​(log⁡n)𝑂𝑛O(\\log n) bits. As our main result, we give an efficient O​(log⁡n)𝑂𝑛O(\\log n)-time randomized algorithm for d2-coloring G𝐺G with at most Δ2+1superscriptΔ21\\Delta^{2}+1 colors, where ΔΔ\\Delta is the maximum degree of G𝐺G. This improves on a recent O​(log⁡Δ⋅log⁡n)𝑂⋅Δ𝑛O(\\log\\Delta\\cdot\\log n)-time algorithm  and it matches the best known bound for ordinary distance-1 (Δ+1)Δ1(\\Delta+1)-coloring in CONGEST as a function of n𝑛n alone.\nWe further explore more efficient algorithms when Δ≪nmuch-less-thanΔ𝑛\\Delta\\ll n. Combining our main method with a range of powerful recent techniques, we obtain an algorithm that runs in time O​(log⁡Δ)+2O​(log⁡log⁡n)𝑂Δsuperscript2𝑂𝑛O(\\log\\Delta)+2^{O(\\sqrt{\\log\\log n})}. Before discussing our results in more detail, we first discuss why we believe d2-coloring is interesting, what is known for the corresponding coloring problems on G𝐺G and why it is challenging to transform CONGEST algorithms to color G𝐺G into CONGEST algorithms for d2-coloring. Wireless networking is a major motivation for distance-2 coloring, where nodes with a common neighbor should not simultaneously communicate to avoid a collision at the common neighbor 5 . While the coloring is to be used for scheduling, the wireless channel need not be the medium for computing the coloring. With the advent of software-defined radio and hierarchical / heterogeneous networks, it is well motivated to consider coloring computation in a communication model more powerful than radio networks. Yet, asking for the different-message-to/from-all-neighbors feature of CONGEST may be hoping for too much.\nMore generally, we view it as a major question in distributed graph algorithms whether one can relax the communication requirements for graph coloring. We ask: How constrained can the communication structure be to allow for fast (logarithmic, sublogarithmic) distributed graph coloring computation? Distributed d2-coloring is an interesting and important problem for several other reasons. The d2-coloring problem for example also occurs naturally when single-round randomized algorithms are derandomized using the method of conditional expectation .\nd2-coloring in CONGEST is further of special interest as it appears to lie at the edge of what is computable efficiently, i.e., in polylogarithmic time, while distance-3 coloring is even hard to verify . Distance-k𝑘k problems have not been addressed widely in a distributed setting, partly because distance-k𝑘k communication can be simulated in k𝑘k steps of the LOCAL model. In CONGEST, the situation changes drastically as simulating a single round of a distance-1 coloring algorithm can incur a factor Θ​(Δk−1)ΘsuperscriptΔ𝑘1\\Theta(\\Delta^{k-1}) overhead, i.e., even for k=2𝑘2k=2, the overhead can be linear in ΔΔ\\Delta. Even the very simple algorithm where each node picks a random available color cannot be efficiently used for d2-coloring as it is in general not possible to keep track of the set of colors chosen by 222-hop neighbors in time o​(Δ)𝑜Δo(\\Delta).\nRecently, Halldórsson, Kuhn and Maus  treated d2-coloring in CONGEST and gave a randomized algorithm using Δ2+1superscriptΔ21\\Delta^{2}+1-colors in O​(log⁡Δ⋅log⁡n)𝑂⋅Δ𝑛O(\\log\\Delta\\cdot\\log n) rounds, as well as a deterministic algorithm using (1+ϵ)​Δ21italic-ϵsuperscriptΔ2(1+\\epsilon)\\Delta^{2} colors in p​o​l​y​(log⁡n)𝑝𝑜𝑙𝑦𝑛poly(\\log n) rounds. Our main approach builds heavily on their framework, while simplifying certain features and strengthening structural properties. Distributed graph optimization problems on G2superscript𝐺2G^{2} (with CONGEST-communication in G𝐺G) such as vertex cover and minimum dominating set have recently been studied in . The standard variant of the distributed coloring problem on G𝐺G asks for computing a vertex coloring with at most Δ+1Δ1\\Delta+1 colors, which is computed by a simple sequential greedy algorithm.\nThe main focus in the literature on distributed coloring has been on the LOCAL model, where by now the problem is understood relatively well.\nThe best randomized (Δ+1)Δ1(\\Delta+1)-coloring algorithm known in the LOCAL model, due to Chang, Li, and Pettie , runs in poly⁡log⁡log⁡npoly𝑛\\operatorname{poly}\\log\\log n rounds. The complexity given in  is 2O​(log⁡log⁡n)superscript2𝑂𝑛2^{O(\\sqrt{\\log\\log n})}, while the improvement to poly⁡log⁡log⁡npoly𝑛\\operatorname{poly}\\log\\log n immediately follows from the recent breakthrough work on deterministic network decomposition of Rozhoň and Ghaffari .\nFor a more detailed discussion of related work on distributed coloring, we refer to   . While most known distributed coloring algorithms were developed for the LOCAL model,\nmany of them work directly in the CONGEST model, including those in       0    . Still, the best complexity known for coloring in CONGEST, as a function of n𝑛n alone, is O​(log⁡n)𝑂𝑛O(\\log n), which is achieved by the following very simple OneShotColoring algorithm:\nInitially all nodes are uncolored. The algorithm runs in synchronous phases, where in each phase, each still uncolored node v𝑣v chooses a uniform random color among its available colors (i.e., among the colors that have not already been picked by a neighbor) and v𝑣v keeps the color if no of its uncolored neighbors tries the same color at the same time  1. The only known published algorithm in CONGEST with a better bound is due to Ghaffari 9, who obtains a (Δ+1)Δ1(\\Delta+1)-coloring in time O​(log⁡Δ)+2O​(log⁡log⁡n)𝑂Δsuperscript2𝑂𝑛O(\\log\\Delta)+2^{O(\\sqrt{\\log\\log n})}. The second term is due to a network decomposition algorithm also introduced in 9. Unlike for results in the LOCAL model, it is not directly possible to replace this decomposition with the recent construction in  to improve the dependence on n𝑛n. The reason is that the complexity of the network decomposition construction of  grows at least linearly in the length of the node identifier bit strings. In the LOCAL model, it is possible to use a standard coloring algorithm of  to first map the IDs to O​(log⁡log⁡n)𝑂𝑛O(\\log\\log n)-bit values that are unique up to a sufficient distance so that one can afterwards apply the algorithm of . Subsequent to the publication of our results  improved upon the network decomposition algorithm from  (to deal with large IDs in the CONGEST model) and as a result obtains a O​(log⁡Δ)+poly⁡log⁡log⁡n𝑂Δpoly𝑛O(\\log\\Delta)+\\operatorname{poly}\\log\\log n CONGEST algorithm for (Δ+1)Δ1(\\Delta+1)-coloring. Note that if we have graphs of size N𝑁N and if IDs and colors can be represented with poly⁡log⁡Npoly𝑁\\operatorname{poly}\\log N bits, there is a recent deterministic (𝑑𝑒𝑔+1)𝑑𝑒𝑔1(\\mathit{deg}+1)-list coloring algorithm running in polylog⁡Npolylog𝑁\\operatorname{polylog}N time in CONGEST .\n\nReference markers found: [31]\n\nContext windows for each [ref]:\n[ref]31: and there is an edge {u,v}∈E​(G2)𝑢𝑣𝐸superscript𝐺2\\left\\{u,v\\right\\}\\in E(G^{2}) whenever dG​(u,v)≤2subscript𝑑𝐺𝑢𝑣2d_{G}(u,v)\\leq 2. The CONGEST model is a standard synchronous message passing model [ref]31. The graph on which we want to compute a coloring is also assumed to form the network topology. Each node u∈V𝑢𝑉u\\in V of the graph has a unique O​(log⁡n)𝑂𝑛O(\\log n)-bit identifier\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 31,\n            \"titles\": [\n                \"Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization\",\n                \"A fast and simple randomized parallel algorithm for the maximal independent set problem\",\n                \"Sublinear algorithms for (Δ+1) vertex coloring\",\n                \"Network decomposition and locality in distributed computation\",\n                \"Efficient deterministic distributed coloring with small bandwidth\",\n                \"Distributed approximation on power graphs\",\n                \"Deterministic (Δ + 1)-coloring in sublinear (in Δ) time in static, dynamic and faulty networks\",\n                \"Deterministic distributed vertex coloring in polylogarithmic time\",\n                \"Distributed Graph Coloring: Fundamentals and Recent Developments\",\n                \"Locally-Iterative Distributed (Δ+1)-Coloring below Szegedy-Vishwanathan Barrier, and Applications to Self-Stabilization and to Restricted-Bandwidth Models\",\n                \"Distributed (Δ+1)-coloring in linear (in Δ) time\",\n                \"The locality of distributed symmetry breaking\",\n                \"An algorithmic approach to the Lovaśz local lemma\",\n                \"Derandomizing local distributed algorithms under bandwidth restrictions\",\n                \"An optimal distributed (Δ+1)-coloring algorithm?\",\n                \"A spatial-reuse TDMA/FDMA for mobile multi-hop radio networks\",\n                \"Deterministic Distributed Dominating Set Approximation in the CONGEST Model\",\n                \"(2​Δ−1)-edge-coloring is much easier than maximal matching in the distributed setting\",\n                \"Distributed testing of distance-k colorings\",\n                \"Distributed maximal independent set using small messages\",\n                \"Improved Deterministic Network Decomposition\",\n                \"On derandomizing local distributed algorithms\",\n                \"Improved Network Decompositions using Small Messages with Applications on MIS, Neighborhood Covers, and Beyond\",\n                \"Distance-2 Coloring in the CONGEST Model\",\n                \"Distributed (Δ+1)-coloring in sublogarithmic rounds\",\n                \"Simple distributed Δ+1-coloring of graphs\",\n                \"Faster deterministic distributed coloring through recursive list coloring\",\n                \"On the complexity of distributed graph coloring\",\n                \"Locality in distributed graph algorithms\",\n                \"A simple parallel algorithm for the maximal independent set problem\",\n                \"MAC layer and coloring\",\n                \"Distributed Computing: A Locality-Sensitive Approach\",\n                \"A new technique for distributed symmetry breaking\",\n                \"Generated Reference Title 33 for [ref]31\",\n                \"Generated Reference Title 34 for [ref]31\",\n                \"Generated Reference Title 35 for [ref]31\",\n                \"Generated Reference Title 36 for [ref]31\",\n                \"Generated Reference Title 37 for [ref]31\",\n                \"Generated Reference Title 38 for [ref]31\",\n                \"Generated Reference Title 39 for [ref]31\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]31 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}