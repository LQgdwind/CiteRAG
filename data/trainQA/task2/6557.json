{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating\n\nPaper Abstract: \n\nPaper Section Text:\nWith the deeper exploration in action recognition, fine-grained human action recognition has long been a question of great interest in a wide range of fields. The content of videos with fine-grained human action is composed of different combinations of scenes, tools (fixed or non-fixed), objects (dynamic or static), and persons. In recent years, the motion-centered fine-grained action recognition datasets such as [ref]7[ref]45, have paid more attention to creating new action categories with the combinations of tools and human actions[ref]34. Recent developments in fine-grained human action recognition have heightened the need for professional sports. Compared with the existing datasets with different scenes, professional sport is challenging because human action will play an important role in a single scene[ref]7[ref]45. Meanwhile, the size of our dataset and the number of action categories are untouchable by the combination of human action and non-fixed tools (More details will be elaborated in Sec.2.). Therefore, it is easier to show more details of fine-grained actions with non-fixed tools in a single scene. The challenges of fine-grained human action datasets are mainly derived from 1) Annotation quality and 2) Impact of p​v𝑝𝑣pv (pose variation) and t​v𝑡𝑣tv (temporal action variation) on c​l𝑐𝑙cl (change of label). It’s worth noting that t​v𝑡𝑣tv is influenced by the number of repeated action units and the speed variation among actions (one or both will be represented in an action sequence). Such impact can be denoted as P​(c​l|p​v)𝑃conditional𝑐𝑙𝑝𝑣P(cl|pv)(or P​(c​l|t​v)𝑃conditional𝑐𝑙𝑡𝑣P(cl|tv)), in which P𝑃P indicates the probability of label changing under the condition of p​v𝑝𝑣pv or t​v𝑡𝑣tv. The reader should bear in mind that the fine-grained action is based on small inter-class variance. We can divide the fine-grained action into fine-grained semantics and fine-grained complexity. Given the above, the disadvantages of the existing datasets can be listed as follows: Fine-grained semantics. The fine-grained semantics that can be simply described as P​(c​l|p​v)→1→𝑃conditional𝑐𝑙𝑝𝑣1P(cl|pv)\\rightarrow 1 and P​(c​l|t​v)→1→𝑃conditional𝑐𝑙𝑡𝑣1P(cl|tv)\\rightarrow 1 will lead to small intra-class variance. The fine-grained motion-centered action datasets place more emphasis on the quality of action annotation (requires professionalism and expert participation), the number of categories, and temporal fine-grained semantics . Owing to the lack of official document or real-time labeling by experts, most datasets (e.g. dance, Taichi, etc) are weak in labeling, the accuracy and professionalism of labels are limited . Moreover, restricted by fixed tools (e.g. pommel horse in FineGym[ref]7) or strategic objects (e.g. basketball), the number of fine-grained categories in the existing human action datasets is insufficient (see Tab. I). In fact, the relationship between p​v𝑝𝑣pv and c​l𝑐𝑙cl tends to be formulated by P​(c​l|p​v)→1→𝑃conditional𝑐𝑙𝑝𝑣1P(cl|pv)\\rightarrow 1, which means the larger p​v𝑝𝑣pv is, the more the number of categories will be. And this is also what most of the existing datasets adopt to increase the number of fine-grained categories. Yet, t​v𝑡𝑣tv (temporal action variation), which also contributes to ensuring categories, quite goes by the board. That is, the condition P​(c​l|t​v)→1→𝑃conditional𝑐𝑙𝑡𝑣1P(cl|tv)\\rightarrow 1 is rarely met so that the fine granularity would not increase at the temporal level. Fine-grained complexity. The fine-grained complexity is mainly reflected in two aspects: 1) the large duration and speed variance 2) P​(c​l|p​v)→0→𝑃conditional𝑐𝑙𝑝𝑣0P(cl|pv)\\rightarrow 0 and P​(c​l|t​v)→0→𝑃conditional𝑐𝑙𝑡𝑣0P(cl|tv)\\rightarrow 0. Action categories that only contain fine-grained complexity without fine-grained semantics will lead to large intra-class variance. Up to now, most studies in the field of human action datasets[ref]34 have only focused on fine-grained semantics and limited spatial fine-grained complexity (See Fig. 1). There has been no detailed investigation of fine-grained complexity about temporal levels and spatio-temporal levels. For the existing recognition models, it is less challenging to obtain well-trained models from the existing fine-grained human action datasets in the case of the complex spatio-temporal features of fine-grained complexity is inadequate. Modality. There only exist RGB and flow features for most existing fine-grained human action datasets. It is unfortunate that the skeleton features in FineGym dataset [ref]7, which consists of RGB, flow, and skeleton features simultaneously, is exacted incompletely. Accordingly, the development of Fine-grained skeleton-based models is limited in the field of human action recognition. Taken together, reliable action labels are expected to ensure that the change of label (c​l𝑐𝑙cl) impacted by t​v𝑡𝑣tv and p​v𝑝𝑣pv is accurate. The number of fine-grained actions and the intra-class variance are limited. A small action dataset FSD-10  is proposed for fine-grained action analysis with the above characteristics but without independent spatial/temporal fine-grained semantics and large-scale samples. We thus propose a new figure skating dataset named MMFS (Multi-modality Multi-task dataset of Figure Skating), collected from videos with high definition (720P) in the World Figure Skating Championships. Compared with the existing human action datasets, the advantages of MMFS can be summarized as follows: Strong annotation. Weak annotation is labeled by trained people. Medium annotation is indexed by trained people and official documents. Strong annotation is annotated by experts and an official document, which means MMFS is jointly annotated by both real-time expert determination and proficient annotators under the help of an official document, which can be used to guarantee the label is equipped with accuracy and professionalism. Independently Spatial Label (SL) and Temporal Label (TL). MMFS dataset has spatio-temporal fine-grained semantics: Skates, as wearable and non-fixed tools, assist body movements to add richer pose details to actions [ref]34, introducing more complex spatial fine-grained actions. The number of fine-grained actions will be increased by t​v𝑡𝑣tv and p​v𝑝𝑣pv as part of action units change for one given action (please see Fig. 1 for details). To further research action recognition at both spatial and temporal levels, we propose integrally spatial and temporal labels in MMFS. Note that the prediction of temporal labels is more difficult than spatial ones. Temporal semantics indicates more rigorous requirements than spatial semantics because the large duration and speed variance lead to the large intra-class variance. A hierarchical label structure including temporal and spatial labels is built to compare the fine-grained spatial and temporal semantics. High complexity of spatio-temporal fine-grained action categories. 1) In comparison with the other datasets, the large duration and speed variance of actions make temporal granularity could be adequately demonstrated. For instance, the Jump could be completed within 2s, while the StepSequence would last from 12s to 68s. The longer average duration of MMFS indicates that more action units can be included in action (see Fig. 1). 2) There are sufficient cases of P​(c​l|p​v)→0→𝑃conditional𝑐𝑙𝑝𝑣0P(cl|pv)\\rightarrow 0 and P​(c​l|t​v)→0→𝑃conditional𝑐𝑙𝑡𝑣0P(cl|tv)\\rightarrow 0 in our dataset. More action units and complex spatio-temporal features can maintain the large intra-class variance of fine-grained actions, even with the increasing number of fine-grained action categories (see Section III for details). Multi-modality. In addition to the RGB feature, the MMFS dataset has the full-body skeleton feature, which offers a great challenge to design remarkable multi-modality models. Multi-task. MMFS, which includes action recognition and action quality assessment tasks, is currently the largest multi-modality action quality assessment dataset. The score of skating is determined by the quality of the movement and the rules of the International Skating Union (ISU). To be specific, the score of each movement is composed of basic value (BV) and grade of execution (GOE). Therefore, the scoring system is relatively complex, which brings greater challenges to the scoring model. According to the characteristics and challenges of MMFS, extensive experiments are conducted, including state-of-the-art RGB-based and skeleton-based action recognition models with different input modalities (RGB, flow, and skeleton features). The experiments indicate that: 1) The duration and speed variance of the dataset is large, which makes it difficult to recognize tv-dominated actions; 2) The accuracy of semantic fine-grained actions could be more easily enhanced than that of fine-grained complex (P​(c​l|p​v)→0→𝑃conditional𝑐𝑙𝑝𝑣0P(cl|pv)\\rightarrow 0 or P​(c​l|t​v)→0→𝑃conditional𝑐𝑙𝑡𝑣0P(cl|tv)\\rightarrow 0) actions by increasing the number of input frames. Overall, this work contributes to the fine-grained action field in two aspects: (1) To our best knowledge, MMFS is the first fine-grained action dataset with strong annotation, high fine-grained spatio-temporal complexity, multi-modality, and multi-task characteristics. (2) MMFS is challenging to the existing state-of-the-art action recognition models. The dataset can be utilized to exploit more excellent models for action-related tasks, provides inspiration for future exploration in this field. MMFS involves fine-grained action recognition and action quality assessment tasks. According to the characteristics of MMFS, extensive experiments are conducted, including mainstream RGB-based and skeleton-based action recognition models with different input modalities (RGB and skeleton features). The experiments indicate the challenges of our benchmark, which highlights the need for further research on fine-grained action analysis.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: of different combinations of scenes, tools (fixed or non-fixed), objects (dynamic or static), and persons. In recent years, the motion-centered fine-grained action recognition datasets such as [ref]7[ref]45, have paid more attention to creating new action categories with the combinations of tools and human actions[ref]34. Recent developments in fine-grained human action recognition have\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Intra- and Inter-Action Understanding via Temporal Action Parsing\",\n                \"Watch this! observed tool use affects perceived distance\",\n                \"Activitynet: A large-scale video benchmark for human activity understanding\",\n                \"A Short Note about Kinetics-600\",\n                \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, pages 4724–4733\",\n                \"Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition\",\n                \"HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation\",\n                \"FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding\",\n                \"Revisiting Skeleton-based Action Recognition\",\n                \"Actions as space-time shapes\",\n                \"The ”something something” video database for learning and evaluating visual common sense, pages 5843–5851\",\n                \"Human actions analysis: Templates generation, matching and visualization applied to motion capture of highly-skilled karate athletes\",\n                \"P-cnn: Part-based convolutional neural networks for fine-grained visual categorization\",\n                \"Large-scale video classification with convolutional neural networks\",\n                \"Large-scale Video Classification with Convolutional Neural Networks, pages 1725–1732\",\n                \"HMDB: A Large Video Database for Human Motion Recognition, pages 2556–2563\",\n                \"Leveraging Weak Semantic Relevance for Complex Video Event Classification, pages 3667–3676\",\n                \"MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions\",\n                \"TSM: Temporal Shift Module for Efficient Video Understanding\",\n                \"NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding\",\n                \"Fsd-10: A fine-grained classification dataset for figure skating\",\n                \"Identity Preserve Transform: Understand What Activity Classification Models Have Learnt\",\n                \"3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism, pages 2158–2167\",\n                \"Moments in Time Dataset: one million videos for event understanding\",\n                \"Action Quality Assessment Across Multiple Actions\",\n                \"What and how well you performed? a multitask learning approach to action quality assessment\",\n                \"Learning To Score Olympic Events\",\n                \"AViD Dataset: Anonymized Videos from Diverse Countries\",\n                \"Fine-grained Activity Recognition in Baseball Videos, pages 1821–1829\",\n                \"Assessing the quality of actions\",\n                \"Action mach a spatio-temporal maximum average correlation height filter for action recognition\",\n                \"Recognizing human actions: A local SVM approach, pages 32–36\",\n                \"Two-stream adaptive graph convolutional networks for skeleton-based action recognition\",\n                \"A survey of content-aware video analysis for sports\",\n                \"Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition\",\n                \"Ucf101: A dataset of 101 human actions classes from videos in the wild\",\n                \"Taichi: A fine-grained action recognition dataset\",\n                \"WiFinger: Leveraging Commodity WiFi for Fine-grained Finger Gesture Recognition\",\n                \"Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing\",\n                \"Football Action Recognition using Hierarchical LSTM, pages 155–163\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}