{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: ExpFinder: An Ensemble Expert Finding Model Integrating 𝑁-gram Vector Space Model and \\hits\n\nPaper Abstract: \n\nPaper Section Text:\nTo create a 𝐓𝐗𝐓𝐗\\mathbf{TX}, our fundamental is to utilise the definition of the DLM  [ref]7 for expert finding. Thus, we first briefly describe how this DLM can measure the topic-sensitive weight of an expert x∈𝒳𝑥𝒳x\\in\\mathcal{X} given a topic t∈𝒯𝑡𝒯t\\in\\mathcal{T}, denoted as p​(x|t)𝑝conditional𝑥𝑡p(x|t). Formally, it is given as : where p​(x,t)𝑝𝑥𝑡p(x,t) is the joint probability of x𝑥x and t𝑡t, and p​(t)𝑝𝑡p(t) is the probability of t𝑡t. We can ignore p​(t)𝑝𝑡p(t) as this is a consistently constant over all experts 𝒳𝒳\\mathcal{X}. Thus, p​(x|t)𝑝conditional𝑥𝑡p(x|t) is approximated by p​(x,t)𝑝𝑥𝑡p(x,t) that is reformulated considering documents 𝒟xsubscript𝒟𝑥\\mathcal{D}_{x} : In Eq. 3, we observe the following notations [ref]7: p​(d)𝑝𝑑p(d) is the prior probability of d𝑑d that can also be interpreted as the weight (or importance) of d𝑑d. p​(x|d)𝑝conditional𝑥𝑑p(x|d) is the conditional probability of x𝑥x given d𝑑d (e.g. in a simply way, it can be estimated based on the order of x𝑥x in the co-author list in d𝑑d [ref]7). p​(t|d)𝑝conditional𝑡𝑑p(t|d) is the conditional probability of t𝑡t given d𝑑d. In the DLM  [ref]7, it is assumed that a document d𝑑d is described as a collection of terms that appear in d𝑑d. An importance of a term w∈t𝑤𝑡w\\in t within d𝑑d is determined by the proportion of its occurrences. DLMs provide a way of capturing this notion by representing a document as multinomial probability distribution over the vocabulary of terms.\nTo estimate p​(t|d)𝑝conditional𝑡𝑑p(t|d), let θdsubscript𝜃𝑑\\theta_{d} be the document model of d𝑑d, and the probability of t𝑡t in θdsubscript𝜃𝑑\\theta_{d} is p​(t|θd)𝑝conditional𝑡subscript𝜃𝑑p(t|\\theta_{d}). This p​(t|θd)𝑝conditional𝑡subscript𝜃𝑑p(t|\\theta_{d}) indicates how likely we see t𝑡t if we sampled t𝑡t randomly from d𝑑d. Thus, p​(t|d)𝑝conditional𝑡𝑑p(t|d) is rewritten as p​(t|θd)𝑝conditional𝑡subscript𝜃𝑑p(t|\\theta_{d}) taking the product of t𝑡t’s individual term probabilities as follows  [ref]7: where w𝑤w is an individual term in t𝑡t. However, a limitation of Eq. 4 is that unseen terms in d𝑑d would get a zero probability. Thus, it is a common in DLMs to introduce a smoothing factor to assign non-zero probability to the unseen terms.\nTypically, it can be done by reducing the probabilities of the terms seen in the corpus and assigning the additional probability mass to unseen terms. Formally, p​(w|θd)𝑝conditional𝑤subscript𝜃𝑑p(w|\\theta_{d}) is re-expressed as: where p​(w|d)𝑝conditional𝑤𝑑p(w|d) is estimated by the term frequency of w𝑤w in d𝑑d divided by |d|𝑑|d| (the number of terms in d𝑑d), denoted as t​f​(w,d)𝑡𝑓𝑤𝑑tf(w,d), and p​(w|𝒟)𝑝conditional𝑤𝒟p(w|\\mathcal{D}) is the term frequency of w𝑤w in 𝒟𝒟\\mathcal{D} normalised by |𝒟|𝒟|\\mathcal{D}|, i.e., t​f​(w,|𝒟|)𝑡𝑓𝑤𝒟{tf(w,|\\mathcal{D}|)}. The parameter λθsubscript𝜆𝜃\\lambda_{\\theta} controls the influence of the two probabilities. We now present our novelty for estimating p​(t|d)𝑝conditional𝑡𝑑p(t|d) using n𝑛nTFIDF. Since n𝑛nTFIDF is an extension of TFIDF, we briefly describe how p​(t|d)𝑝conditional𝑡𝑑p(t|d) can be estimated using TFIDF in VSM.\nIn a sense, p​(t|d)𝑝conditional𝑡𝑑p(t|d) can also be interpreted using TFIDF .\nNote that TFIDF is a measure based on the distance between two probability distributions, expressed as the cross-entropy: (1) a local distribution of w∈t𝑤𝑡w\\in t in d𝑑d, and (2) a global distribution of w𝑤w in 𝒟𝒟\\mathcal{D}.\nTFIDF is a measure of perplexity between these two distributions.\nA higher perplexity score implies a higher relevance of d𝑑d to w𝑤w. The cross-entropy between distributions pwsubscript𝑝𝑤p_{w} and qwsubscript𝑞𝑤q_{w} is as follows: if we substitute pwsubscript𝑝𝑤p_{w} with t​f​(w,d)𝑡𝑓𝑤𝑑{tf(w,d)} (TF) and 1qi1subscript𝑞𝑖\\frac{1}{q_{i}} with the inverted probability of encountering d𝑑d with a term w𝑤w (IDF), denoted as |D|d​f​(w)𝐷𝑑𝑓𝑤\\frac{|D|}{df(w)}, where d​f​(w)𝑑𝑓𝑤df(w) is the document frequency of w𝑤w, we obtain a TFIDF formula: Thus, as highlighted in , VSM and DLM are actually closely related. The TF component t​f​(w,d)𝑡𝑓𝑤𝑑{tf(w,d)} is exactly same as the probability of seeing a term w𝑤w in DLM.\nThe IDF component |D|d​f​(w)𝐷𝑑𝑓𝑤\\frac{|D|}{df(w)} is implicitly related to a smoothing method in DLM that uses the collection frequency (t​f​(w,|𝒟|)𝑡𝑓𝑤𝒟{tf(w,|\\mathcal{D}|)}: term frequency of w𝑤w in 𝒟𝒟\\mathcal{D} normalised by |𝒟|𝒟|\\mathcal{D}|. Based on the above observation, we now present our approach for estimating p​(t|d)𝑝conditional𝑡𝑑p(t|d) using n𝑛nTFIDF in n𝑛nVSM.\nAlthough some variant forms of TFIDF methods have been proposed, the majority of TFIDF methods use the same IDF function .\nHowever, one drawback of IDF is that it cannot handle N𝑁N-grams, contiguous sequence of N𝑁N terms (for N𝑁N>>1). The reason is that IDF tends to give a higher weight to a term that occurs in fewer documents. Note that typically, phrases occur in fewer documents when their collocations are less common. Thus, uncommon phrases (e.g. noise phrases) are unintentionally assigned high weight, yielding the conflict with the definition of a good phrase that constitutes a succinct conceptual descriptor in text. To address it, N𝑁N-gram IDF for weighting phrases was recently proposed . N𝑁N-gram IDF has shown the ability to accurately estimate weights of dominant phrases of any length, simply using the domain corpus. The key in n𝑛nVSM is the proposed formula n𝑛nTFIDF that uses a combination of the frequency of a topic t𝑡t with t𝑡t’s N𝑁N-gram IDF. As that frequency, we use the average frequencies of the constituent terms in t𝑡t. Formally, using n𝑛nTFIDF, p​(t|d)𝑝conditional𝑡𝑑p(t|d) is defined as: where w1,…,wnsubscript𝑤1…subscript𝑤𝑛w_{1},\\ldots,w_{n} are n𝑛n-constituent terms in t𝑡t,\nt​f​(wi,d)𝑡𝑓subscript𝑤𝑖𝑑tf({w_{i},d}) is the term frequency of wisubscript𝑤𝑖w_{i} in d𝑑d normalised by |d|𝑑|d|,\n|t|𝑡|t| is equal to n𝑛n, and n​i​d​f​(t)𝑛𝑖𝑑𝑓𝑡nidf(t) is the N𝑁N-gram IDF method for t𝑡t .\nEq. 8 applies for all |t|≥1𝑡1|t|\\geq 1, where n​i​d​f​(t)𝑛𝑖𝑑𝑓𝑡nidf(t) is equal to the log-IDF, log⁡|𝒟|d​f​(t)𝒟𝑑𝑓𝑡\\log\\frac{|\\mathcal{D}|}{df(t)}, in Eq. 7, when |t|𝑡|t|=1. Finally, in n𝑛nVSM, p​(x|t)𝑝conditional𝑥𝑡p(x|t) in Eq. 3 is calculated using p​(t|d)𝑝conditional𝑡𝑑p(t|d) in Eq. 8 and is stored into the entry 𝐓𝐗i​(t),i​(x)subscript𝐓𝐗𝑖𝑡𝑖𝑥\\mathbf{TX}_{i(t),i(x)}, where i​(t)𝑖𝑡i(t) and i​(x)𝑖𝑥i(x) indicate the row and column index of t𝑡t and x𝑥x, respectively, in the 𝐓𝐗𝐓𝐗\\mathbf{TX}.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: To create a 𝐓𝐗𝐓𝐗\\mathbf{TX}, our fundamental is to utilise the definition of the DLM  [ref]7 for expert finding. Thus, we first briefly describe how this DLM can measure the topic-sensitive weight of an expert x∈𝒳𝑥𝒳x\\in\\mathcal{X} given a topic t∈𝒯𝑡𝒯t\\in\\mathcal{T}, denoted as\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Wiser: A Semantic Approach for Expert Finding in Academia based on Entity Linking\",\n                \"Expert finding systems: A systematic review\",\n                \"Looking for experts? what can linked data do for you?\",\n                \"Automated expertise retrieval: A taxonomy-based survey and open issues\",\n                \"Combining query terms extension and weight correlative for expert finding\",\n                \"Fuzzy rule based profiling approach for enterprise information seeking and retrieval\",\n                \"A language modeling framework for expert finding\",\n                \"Bmexpert  Mining medline for finding experts in biomedical domains based on language model\",\n                \"Ranking experts using author-document-topic graphs\",\n                \"Expertise identification using email communications\",\n                \"Constructing effective and efficient topic-specific authority networks for expert finding in social media\",\n                \"Expert ranking techniques for online rated forums\",\n                \"Expert finding considering dynamic profiles and trust in social networks\",\n                \"How to identify experts in a community?\",\n                \"IDF for Word N-Grams\",\n                \"A Generalized CO-HITS Algorithm and Its Application to Bipartite Graphs\",\n                \"The LExR Collection for Expertise Retrieval in Academia\",\n                \"Benchmarking domain-specific expert search using workshop program committees\",\n                \"A Social Network-Based Recommender Systems\",\n                \"Finding expert users in community question answering\",\n                \"Using transactional data from erp systems for expert finding\",\n                \"On information-theoretic document-person associations for expert search in academia\",\n                \"Unsupervised, Efficient and Semantic Expertise Retrieval\",\n                \"The probabilistic relevance framework: Bm25 and beyond\",\n                \"Exploiting heterogeneous scientific literature networks to combat ranking bias: Evidence from the computational linguistics area\",\n                \"Ranking methods for entity-oriented semantic web search\",\n                \"Expertise networks in online communities: structure and algorithms\",\n                \"Expertrank: A topic-aware expert finding algorithm for online knowledge communities\",\n                \"Formulation of a hybrid expertise retrieval system in community question answering services\",\n                \"Enhanced models for expertise retrieval using community-aware strategies\",\n                \"Integrating expert profile, reputation and link analysis for expert finding in question-answering websites\",\n                \"Competition-based networks for expert finding\",\n                \"Expert finding by the dempster-shafer theory for evidence combination\",\n                \"Cfinder: An intelligent key concept finder from text for ontology development\",\n                \"Tf-idf uncovered: A study of theories and probabilities\",\n                \"An insight into vector space modeling and language modeling\",\n                \"Authoritative sources in a hyperlinked environment\",\n                \"Formal Models for Expert Finding on DBLP Bibliography Data\",\n                \"SciBert: A Pretrained Language Model for Scientific Text\",\n                \"Voting for candidates: Adapting data fusion techniques for an expert search task\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of ExpFinder: An Ensemble Expert Finding Model Integrating 𝑁-gram Vector Space Model and \\\\hits. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}