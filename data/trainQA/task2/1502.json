{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: ExpFinder: An Ensemble Expert Finding Model Integrating ğ‘-gram Vector Space Model and \\hits\n\nPaper Abstract: \n\nPaper Section Text:\nTo create a ğ“ğ—ğ“ğ—\\mathbf{TX}, our fundamental is to utilise the definition of the DLMÂ  [ref]7 for expert finding. Thus, we first briefly describe how this DLM can measure the topic-sensitive weight of an expert xâˆˆğ’³ğ‘¥ğ’³x\\in\\mathcal{X} given a topic tâˆˆğ’¯ğ‘¡ğ’¯t\\in\\mathcal{T}, denoted as pâ€‹(x|t)ğ‘conditionalğ‘¥ğ‘¡p(x|t). Formally, it is given asÂ : where pâ€‹(x,t)ğ‘ğ‘¥ğ‘¡p(x,t) is the joint probability of xğ‘¥x and tğ‘¡t, and pâ€‹(t)ğ‘ğ‘¡p(t) is the probability of tğ‘¡t. We can ignore pâ€‹(t)ğ‘ğ‘¡p(t) as this is a consistently constant over all experts ğ’³ğ’³\\mathcal{X}. Thus, pâ€‹(x|t)ğ‘conditionalğ‘¥ğ‘¡p(x|t) is approximated by pâ€‹(x,t)ğ‘ğ‘¥ğ‘¡p(x,t) that is reformulated considering documents ğ’Ÿxsubscriptğ’Ÿğ‘¥\\mathcal{D}_{x}Â : In Eq.Â 3, we observe the following notationsÂ [ref]7: pâ€‹(d)ğ‘ğ‘‘p(d) is the prior probability of dğ‘‘d that can also be interpreted as the weight (or importance) of dğ‘‘d. pâ€‹(x|d)ğ‘conditionalğ‘¥ğ‘‘p(x|d) is the conditional probability of xğ‘¥x given dğ‘‘d (e.g. in a simply way, it can be estimated based on the order of xğ‘¥x in the co-author list in dğ‘‘dÂ [ref]7). pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) is the conditional probability of tğ‘¡t given dğ‘‘d. In the DLMÂ  [ref]7, it is assumed that a document dğ‘‘d is described as a collection of terms that appear in dğ‘‘d. An importance of a term wâˆˆtğ‘¤ğ‘¡w\\in t within dğ‘‘d is determined by the proportion of its occurrences. DLMs provide a way of capturing this notion by representing a document as multinomial probability distribution over the vocabulary of terms.\nTo estimate pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d), let Î¸dsubscriptğœƒğ‘‘\\theta_{d} be the document model of dğ‘‘d, and the probability of tğ‘¡t in Î¸dsubscriptğœƒğ‘‘\\theta_{d} is pâ€‹(t|Î¸d)ğ‘conditionalğ‘¡subscriptğœƒğ‘‘p(t|\\theta_{d}). This pâ€‹(t|Î¸d)ğ‘conditionalğ‘¡subscriptğœƒğ‘‘p(t|\\theta_{d}) indicates how likely we see tğ‘¡t if we sampled tğ‘¡t randomly from dğ‘‘d. Thus, pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) is rewritten as pâ€‹(t|Î¸d)ğ‘conditionalğ‘¡subscriptğœƒğ‘‘p(t|\\theta_{d}) taking the product of tğ‘¡tâ€™s individual term probabilities as followsÂ  [ref]7: where wğ‘¤w is an individual term in tğ‘¡t. However, a limitation of Eq.Â 4 is that unseen terms in dğ‘‘d would get a zero probability. Thus, it is a common in DLMs to introduce a smoothing factor to assign non-zero probability to the unseen terms.\nTypically, it can be done by reducing the probabilities of the terms seen in the corpus and assigning the additional probability mass to unseen terms. Formally, pâ€‹(w|Î¸d)ğ‘conditionalğ‘¤subscriptğœƒğ‘‘p(w|\\theta_{d}) is re-expressed as: where pâ€‹(w|d)ğ‘conditionalğ‘¤ğ‘‘p(w|d) is estimated by the term frequency of wğ‘¤w in dğ‘‘d divided by |d|ğ‘‘|d| (the number of terms in dğ‘‘d), denoted as tâ€‹fâ€‹(w,d)ğ‘¡ğ‘“ğ‘¤ğ‘‘tf(w,d), and pâ€‹(w|ğ’Ÿ)ğ‘conditionalğ‘¤ğ’Ÿp(w|\\mathcal{D}) is the term frequency of wğ‘¤w in ğ’Ÿğ’Ÿ\\mathcal{D} normalised by |ğ’Ÿ|ğ’Ÿ|\\mathcal{D}|, i.e., tâ€‹fâ€‹(w,|ğ’Ÿ|)ğ‘¡ğ‘“ğ‘¤ğ’Ÿ{tf(w,|\\mathcal{D}|)}. The parameter Î»Î¸subscriptğœ†ğœƒ\\lambda_{\\theta} controls the influence of the two probabilities. We now present our novelty for estimating pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) using nğ‘›nTFIDF. Since nğ‘›nTFIDF is an extension of TFIDF, we briefly describe how pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) can be estimated using TFIDF in VSM.\nIn a sense, pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) can also be interpreted using TFIDFÂ .\nNote that TFIDF is a measure based on the distance between two probability distributions, expressed as the cross-entropy: (1) a local distribution of wâˆˆtğ‘¤ğ‘¡w\\in t in dğ‘‘d, and (2) a global distribution of wğ‘¤w in ğ’Ÿğ’Ÿ\\mathcal{D}.\nTFIDF is a measure of perplexity between these two distributions.\nA higher perplexity score implies a higher relevance of dğ‘‘d to wğ‘¤w. The cross-entropy between distributions pwsubscriptğ‘ğ‘¤p_{w} and qwsubscriptğ‘ğ‘¤q_{w} is as follows: if we substitute pwsubscriptğ‘ğ‘¤p_{w} with tâ€‹fâ€‹(w,d)ğ‘¡ğ‘“ğ‘¤ğ‘‘{tf(w,d)} (TF) and 1qi1subscriptğ‘ğ‘–\\frac{1}{q_{i}} with the inverted probability of encountering dğ‘‘d with a term wğ‘¤w (IDF), denoted as |D|dâ€‹fâ€‹(w)ğ·ğ‘‘ğ‘“ğ‘¤\\frac{|D|}{df(w)}, where dâ€‹fâ€‹(w)ğ‘‘ğ‘“ğ‘¤df(w) is the document frequency of wğ‘¤w, we obtain a TFIDF formula: Thus, as highlighted inÂ , VSM and DLM are actually closely related. The TF component tâ€‹fâ€‹(w,d)ğ‘¡ğ‘“ğ‘¤ğ‘‘{tf(w,d)} is exactly same as the probability of seeing a term wğ‘¤w in DLM.\nThe IDF component |D|dâ€‹fâ€‹(w)ğ·ğ‘‘ğ‘“ğ‘¤\\frac{|D|}{df(w)} is implicitly related to a smoothing method in DLM that uses the collection frequency (tâ€‹fâ€‹(w,|ğ’Ÿ|)ğ‘¡ğ‘“ğ‘¤ğ’Ÿ{tf(w,|\\mathcal{D}|)}: term frequency of wğ‘¤w in ğ’Ÿğ’Ÿ\\mathcal{D} normalised by |ğ’Ÿ|ğ’Ÿ|\\mathcal{D}|. Based on the above observation, we now present our approach for estimating pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) using nğ‘›nTFIDF in nğ‘›nVSM.\nAlthough some variant forms of TFIDF methods have been proposed, the majority of TFIDF methods use the same IDF functionÂ .\nHowever, one drawback of IDF is that it cannot handle Nğ‘N-grams, contiguous sequence of Nğ‘N terms (for Nğ‘N>>1). The reason is that IDF tends to give a higher weight to a term that occurs in fewer documents. Note that typically, phrases occur in fewer documents when their collocations are less common. Thus, uncommon phrases (e.g. noise phrases) are unintentionally assigned high weight, yielding the conflict with the definition of a good phrase that constitutes a succinct conceptual descriptor in text. To address it, Nğ‘N-gram IDF for weighting phrases was recently proposedÂ . Nğ‘N-gram IDF has shown the ability to accurately estimate weights of dominant phrases of any length, simply using the domain corpus. The key in nğ‘›nVSM is the proposed formula nğ‘›nTFIDF that uses a combination of the frequency of a topic tğ‘¡t with tğ‘¡tâ€™s Nğ‘N-gram IDF. As that frequency, we use the average frequencies of the constituent terms in tğ‘¡t. Formally, using nğ‘›nTFIDF, pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) is defined as: where w1,â€¦,wnsubscriptğ‘¤1â€¦subscriptğ‘¤ğ‘›w_{1},\\ldots,w_{n} are nğ‘›n-constituent terms in tğ‘¡t,\ntâ€‹fâ€‹(wi,d)ğ‘¡ğ‘“subscriptğ‘¤ğ‘–ğ‘‘tf({w_{i},d}) is the term frequency of wisubscriptğ‘¤ğ‘–w_{i} in dğ‘‘d normalised by |d|ğ‘‘|d|,\n|t|ğ‘¡|t| is equal to nğ‘›n, and nâ€‹iâ€‹dâ€‹fâ€‹(t)ğ‘›ğ‘–ğ‘‘ğ‘“ğ‘¡nidf(t) is the Nğ‘N-gram IDF method for tğ‘¡t .\nEq.Â 8 applies for all |t|â‰¥1ğ‘¡1|t|\\geq 1, where nâ€‹iâ€‹dâ€‹fâ€‹(t)ğ‘›ğ‘–ğ‘‘ğ‘“ğ‘¡nidf(t) is equal to the log-IDF, logâ¡|ğ’Ÿ|dâ€‹fâ€‹(t)ğ’Ÿğ‘‘ğ‘“ğ‘¡\\log\\frac{|\\mathcal{D}|}{df(t)}, in Eq.Â 7, when |t|ğ‘¡|t|=1. Finally, in nğ‘›nVSM, pâ€‹(x|t)ğ‘conditionalğ‘¥ğ‘¡p(x|t) in Eq.Â 3 is calculated using pâ€‹(t|d)ğ‘conditionalğ‘¡ğ‘‘p(t|d) in Eq.Â 8 and is stored into the entry ğ“ğ—iâ€‹(t),iâ€‹(x)subscriptğ“ğ—ğ‘–ğ‘¡ğ‘–ğ‘¥\\mathbf{TX}_{i(t),i(x)}, where iâ€‹(t)ğ‘–ğ‘¡i(t) and iâ€‹(x)ğ‘–ğ‘¥i(x) indicate the row and column index of tğ‘¡t and xğ‘¥x, respectively, in the ğ“ğ—ğ“ğ—\\mathbf{TX}.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: To create a ğ“ğ—ğ“ğ—\\mathbf{TX}, our fundamental is to utilise the definition of the DLMÂ  [ref]7 for expert finding. Thus, we first briefly describe how this DLM can measure the topic-sensitive weight of an expert xâˆˆğ’³ğ‘¥ğ’³x\\in\\mathcal{X} given a topic tâˆˆğ’¯ğ‘¡ğ’¯t\\in\\mathcal{T}, denoted as\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Wiser: A Semantic Approach for Expert Finding in Academia based on Entity Linking\",\n                \"Expert finding systems: A systematic review\",\n                \"Looking for experts? what can linked data do for you?\",\n                \"Automated expertise retrieval: A taxonomy-based survey and open issues\",\n                \"Combining query terms extension and weight correlative for expert finding\",\n                \"Fuzzy rule based profiling approach for enterprise information seeking and retrieval\",\n                \"A language modeling framework for expert finding\",\n                \"Bmexpertâ€… Mining medline for finding experts in biomedical domains based on language model\",\n                \"Ranking experts using author-document-topic graphs\",\n                \"Expertise identification using email communications\",\n                \"Constructing effective and efficient topic-specific authority networks for expert finding in social media\",\n                \"Expert ranking techniques for online rated forums\",\n                \"Expert finding considering dynamic profiles and trust in social networks\",\n                \"How to identify experts in a community?\",\n                \"IDF for Word N-Grams\",\n                \"A Generalized CO-HITS Algorithm and Its Application to Bipartite Graphs\",\n                \"The LExR Collection for Expertise Retrieval in Academia\",\n                \"Benchmarking domain-specific expert search using workshop program committees\",\n                \"A Social Network-Based Recommender Systems\",\n                \"Finding expert users in community question answering\",\n                \"Using transactional data from erp systems for expert finding\",\n                \"On information-theoretic document-person associations for expert search in academia\",\n                \"Unsupervised, Efficient and Semantic Expertise Retrieval\",\n                \"The probabilistic relevance framework: Bm25 and beyond\",\n                \"Exploiting heterogeneous scientific literature networks to combat ranking bias: Evidence from the computational linguistics area\",\n                \"Ranking methods for entity-oriented semantic web search\",\n                \"Expertise networks in online communities: structure and algorithms\",\n                \"Expertrank: A topic-aware expert finding algorithm for online knowledge communities\",\n                \"Formulation of a hybrid expertise retrieval system in community question answering services\",\n                \"Enhanced models for expertise retrieval using community-aware strategies\",\n                \"Integrating expert profile, reputation and link analysis for expert finding in question-answering websites\",\n                \"Competition-based networks for expert finding\",\n                \"Expert finding by the dempster-shafer theory for evidence combination\",\n                \"Cfinder: An intelligent key concept finder from text for ontology development\",\n                \"Tf-idf uncovered: A study of theories and probabilities\",\n                \"An insight into vector space modeling and language modeling\",\n                \"Authoritative sources in a hyperlinked environment\",\n                \"Formal Models for Expert Finding on DBLP Bibliography Data\",\n                \"SciBert: A Pretrained Language Model for Scientific Text\",\n                \"Voting for candidates: Adapting data fusion techniques for an expert search task\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of ExpFinder: An Ensemble Expert Finding Model Integrating ğ‘-gram Vector Space Model and \\\\hits. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}