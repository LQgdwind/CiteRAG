{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning\n\nPaper Abstract: \n\nPaper Section Text:\nGroup fairness ensures that different groups receive the same predictions with close to equal probability. This is related to the philosophical idea of egalitarianism, particularly from a collectivist standpoint [ref]20 . A practical implementation of group fairness is affirmative action policies. Statistical parity (sometimes called demographic parity) is a common formalization of group fairness [ref]23. This notion can be measured by the statistical parity difference. Definition 3: Statistical Parity Difference [ref]23 The statistical parity difference of a model is The model achieves perfect fairness if S​P​D=0𝑆𝑃𝐷0SPD=0. We can similarly measure a model’s group fairness using the equal opportunity difference, which is simply the difference in true positive rates for protected and unprotected groups [ref]20. Definition 4: Equal Opportunity Difference  Formally, for a predictor Y^^𝑌\\hat{Y} with true label Y𝑌Y, the equal opportunity difference is  The model achieves perfect fairness if E​O​D=0𝐸𝑂𝐷0EOD=0. This measurement indicates how well the model performs on the entire group of males versus females. Another, similar metric of group fairness is the average odds difference, which additionally incorporates false positive rate. Definition 5: Average Odds Difference  The average odds difference of a model Y^^𝑌\\hat{Y} is where F​P​R𝐹𝑃𝑅FPR is the false positive rate and T​P​R𝑇𝑃𝑅TPR is the true positive rate. The model achieves perfect fairness if A​O​D=0𝐴𝑂𝐷0AOD=0 In other words, this is a measure of how different the true versus false positive rates are for each group. Another widely used group fairness metric is disparate impact [ref]8. Definition 6: Disparate Impact [ref]8 The disparate impact of a model Y^^𝑌\\hat{Y} is The model achieves perfect fairness if D​I=1𝐷𝐼1DI=1. Notice that this metric measures the same relationship as statistical parity difference, just formulated as a proportion. We include it here in addition to statistical parity difference as a commonly reported way to quantify the same effect.\n\nReference markers found: [20]\n\nContext windows for each [ref]:\n[ref]20: ensures that different groups receive the same predictions with close to equal probability. This is related to the philosophical idea of egalitarianism, particularly from a collectivist standpoint [ref]20 . A practical implementation of group fairness is affirmative action policies. Statistical parity (sometimes called demographic parity) is a common formalization of group fairness [ref]23.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 20,\n            \"titles\": [\n                \"A Survey on Bias and Fairness in Machine Learning\",\n                \"Optimal taxation and insurance using machine learning — sufficient statistics and beyond\",\n                \"Crowdsourcing city government: Using tournaments to improve inspection accuracy\",\n                \"Data-Driven Decisions for Reducing Readmissions for Heart Failure: General Methodology and Case Study\",\n                \"Criminal Justice Forecasts of Risk: A Machine Learning Approach\",\n                \"Systematic bias and nontransparency in us social security administration forecasts\",\n                \"The challenges and limits of big data algorithms in technocratic governance\",\n                \"Learning Optimal Fair Policies\",\n                \"Certifying and removing disparate impact\",\n                \"Mitigating Unwanted Biases with Adversarial Learning\",\n                \"Balancing Competing Objectives with Noisy Data Score-Based Classifiers for Welfare-Aware Machine Learning\",\n                \"Gender shades: Intersectional accuracy disparities in commercial gender classification\",\n                \"Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products\",\n                \"Assessing Gender Bias in Machine Translation – A Case Study with Google Translate\",\n                \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n                \"Automated experiments on ad privacy settings: A tale of opacity, choice, and discrimination\",\n                \"Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare\",\n                \"Fair Housing Act\",\n                \"Equal Credit Opportunity Act\",\n                \"Fairness definitions explained\",\n                \"On Formalizing Fairness in Prediction with Machine Learning\",\n                \"On the Apparent Conflict Between Individual and Group Fairness\",\n                \"Counterfactual Fairness\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias\",\n                \"How machine learning advances will improve the fairness of algorithms\",\n                \"UCI machine learning repository\",\n                \"A survey of bias in machine learning through the prism of statistical parity for the adult data set\",\n                \"Fairness-Aware Classifier with Prejudice Remover Regularizer\",\n                \"Fair inference on outcomes\",\n                \"A causal framework for discovering and removing direct and indirect discrimination\",\n                \"Optimized pre-processing for discrimination prevention\",\n                \"Fairness Constraints: Mechanisms for Fair Classification\",\n                \"Fairness-aware Classification: Criterion, Convexity, and Bounds\",\n                \"On Fairness and Calibration\",\n                \"Algorithmic decision making and the cost of fairness\",\n                \"Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks.\",\n                \"Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction\",\n                \"Nonconvex optimization for regression with fairness constraints\",\n                \"Debiasing Community Detection: The Importance of Lowly-Connected Nodes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]20 based on the context and research area of End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}