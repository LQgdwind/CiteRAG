{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: One Explanation Does Not Fit All\n\nPaper Abstract: \n\nPaper Section Text:\nGiven the opaque, ‚Äúblack-box‚Äù nature of complex Machine Learning (ML) systems, their deployment in mission-critical domains is limited by the extent to which they can be interpreted or validated. In particular, predictions, (trained) models and (training) data should be accounted for. One way to achieve this is by ‚Äútransparency by design‚Äù, so that all components of a predictive system are ‚Äúglass boxes‚Äù, i.e., ante-hoc transparency¬†. Alternatively, transparency might be achieved with post-hoc tools, which have the advantage of not limiting the choice of a predictive model in advance¬†. The latter approaches can either be model-specific or model-agnostic¬†. Despite this wide range of available tools and techniques, many of them are non-interactive, providing the explainee (a recipient of an explanation) with a single explanation that has been optimised according to some predefined metric. While some of these methods simply cannot be customised by the end user without an in-depth understanding of their inner workings, others can take direct input from users with a varying level of domain expertise: from a lay audience ‚Äì e.g., selecting regions of an image in order to query their influence on the classification outcome ‚Äì to domain experts ‚Äì e.g., tuning explanation parameters such as the importance of neighbouring data points. A particular risk of a lack of interaction and personalisation mechanisms is that the explanation may not always align with users‚Äô expectations, reducing its overall value and usefulness. Allowing the user to guide and customise an explanation can benefit the transparency of a predictive system by making it more suitable and appealing to the explainee, for example, by adjusting its content and complexity. Therefore, personalisation can be understood as modifying an explanation or an explanatory process to answer user-specific questions. For counterfactual explanations of the form: ‚Äúhad feature XùëãX been different, the prediction of the model would have been different too‚Äù, these can be user-defined constrains on the number and type of features (XùëãX) that can and cannot appear in the conditional statement. Delegating the task of customising and personalising explanations to the end user via interaction mitigates the need for the difficult process of modelling the user‚Äôs mental model beforehand, rendering the task feasible and making the whole process feel more natural, engaging and less frustrating. In human interactions, understanding is naturally achieved via an explanatory dialogue¬†[ref]26, possibly supported with visual aids. Mirroring this explanatory process for ML transparency would make it attractive and accessible to a wide audience. Furthermore, allowing the user to customise explanations extends their utility beyond ML transparency. The explainee can steer the explanatory process to inspect fairness (e.g., identify biases towards protected groups111A protected group is a sub-population in a data set created by fixing a value of a protected attribute such as age, gender or ethnicity, which discriminating upon is illegal.)¬†, assess accountability (e.g., identify model errors such as non-monotonic predictions with respect to monotonic features)¬† or debug predictive models¬†[ref]20 . In contrast to ML tasks¬† ‚Äì where any interaction may be impeded by human-incomprehensible internal representations utilised by a predictive model ‚Äì interacting with explainability systems is feasible as the representation has to be human-understandable in the first place, thereby enabling a bi-directional communication. Interaction with explanatory systems also allows incorporating new knowledge into the underlying ML algorithm and building a mental model of the explainee, which will help to customise the resulting explanations. Consider the example of explaining an image with a local surrogate method that relies upon super-pixel segmentation (e.g., LIME algorithm introduced by Ribeiro et¬†al. ). While super-pixel discovery may be good at separating colour patches based on their edges, these segments do not often correspond to meaningful concepts such as ears or a tail for a dog image ‚Äì see Figure¬†1 for an example. The explanation can be personalised by allowing the explainee to merge and split segments before analysing their influence on the output of a black-box model, thereby implicitly answering what prompted the explainee to alter the segmentation. User input is a welcome addition given the complexity of images; a similar approach is possible for tabular and text data, although user input is often unnecessary in these two cases. For tabular data the explainee may select certain feature values that are of interest or create meaningful binning for some of the continuous features; for text data (treated as a bag of words) the user may group some words into a phrase that conveys the correct meaning in that particular sentence. This exchange of knowledge between the explainee and the explainability system can considerably increase the quality of explanations, but also poses a significant safety, security and privacy risk. A malicious explainee may use such a system to reveal sensitive data used to train the underlying predictive (or explanatory) model, extract proprietary model components, or learn its behaviour in an attempt to game it (see Section¬†3.2). After Miller‚Äôs¬†[ref]26 seminal work ‚Äì inspired by explanation research in the social sciences ‚Äì drew attention to the lack of human-aspect considerations in the eXplainable Artificial Intelligence (XAI) literature ‚Äì with many such systems being designed by the technical community for the technical community¬† ‚Äì researchers started acknowledging the end user when designing XAI solutions. While this has advanced human-centred design and validation of explanations produced by XAI systems, another of Miller‚Äôs insights received relatively little attention: the interactive, dialogue-like nature of explanations. Many of the state-of-the-art explainability approaches are static, one-off systems that do not take user input or preferences into consideration beyond the initial configuration and parametrisation¬† [ref]9    .222To clarify, the notion of interaction is with respect to the explanation, e.g., the ability of the explainee to personalise it, and not the overall interactiveness of the explainability system. While sometimes the underlying explanatory algorithms are simply incapable of a meaningful interaction, others do apply a technique or utilise an explanatory artefact that can support it in principle. Part of this trend can be attributed to the lack of a well-defined protocol for evaluating interactive explanations and the challenging process of assessing their quality and effectiveness, which ‚Äì in contrast to a one-shot evaluation ‚Äì is a software system engineering challenge333Building such systems requires a range of diverse components: user interface, natural language processing unit, natural language generation module, conversation management system and a suitable and well-designed XAI algorithm. Furthermore, most of these components are domain-specific and cannot be generalised beyond the selected data set and use case. and requires time- and resource-consuming user studies. Schneider and Handali  noted that bespoke explanations in AI ‚Äì achieved through interaction or otherwise ‚Äì are largely absent within the existing literature. Research in this space usually touches upon three aspects of ‚Äúpersonalised‚Äù explanations. First, there are interactive machine learning systems where the user input is harnessed to improve performance of a predictive model or align the data processing with its operator‚Äôs prior beliefs. While the classic active learning paradigm dominates this space, Kulesza et¬†al. [ref]20 designed a system that presents its users with classification explanations to help them refine and personalise the predictive task, hence focusing the interaction on the underlying ML model and not the explanations. Similarly, Kim et¬†al.  introduced an interactive ML system with an explainability component, allowing its users to alter the data clustering based on their preferences. Secondly, the work of Krause et¬†al.  and Weld and Bansal  focused on interactive (multi-modal) explainability systems. Here, the interaction allows the explainee to elicit more information about an ML system by receiving a range of diverse explanations derived from a collection of XAI algorithms such as Partial Dependence (PD)¬† and Individual Conditional Expectation (ICE)¬†[ref]9 plots. While this body of research illustrates what such an interaction (with multiple explanatory modalities) might look like and persuasively argues its benefits¬†, the advocated interaction is mostly with respect to the presentation medium itself ‚Äì e.g., an interactive PD plot ‚Äì and cannot be used to customise and personalise the explanation per se. Thirdly, Madumal et¬†al.  and Schneider and Handali  developed theoretical frameworks for interactive, personalised explainability that prescribe the interaction protocol and design of such systems. However, these theoretical foundations have not yet been utilised to design and implement an interactive explainability system coherent with XAI desiderata outlined by Miller [ref]26, which could offer customisable explanations. A more detailed overview and discussion of the relevant literature is given in Section¬†2. In this paper we propose an architecture of a truly interactive explainability system, demonstrate how to build such a system, analyse its desiderata, and examine how a diverse range of explanations can be personalised (Section¬†3). Furthermore, we discuss lessons learnt from presenting it to both a technical and a lay audience, and provide a plan for future research in this direction (Section¬†4). As a first attempt to build an XAI system that allows the explainee to customise and personalise the explanations, we decided to use a decision tree as the underlying predictive model. This choice simplifies many steps of our initial study, allowing us to validate (and guarantee correctness of) the explanations and reduce the overall complexity of the explanation generation and tuning process by inspecting the structure of the underlying decision tree. Using ante-hoc explanations derived from a single predictive model also allows us to mitigate engineering challenges that come with combining multiple independent XAI algorithms as proposed by Weld and Bansal . Furthermore, a decision tree can provide a wide range of diverse explanation types, many of which can be customised and personalised. Specifically, for global model explanations we provide model visualisation, and feature importance; while as prediction explanations we provide decision rule ‚Äì extracted from a root-to-leaf path, counterfactual ‚Äì achieved by comparing decision rules for different tree leaves, and exemplar ‚Äì a similar training data point extracted from the tree leaves. When presented to the user, all of these explanations span a wide range of explanatory artefacts in visualisation (images) and textualisation (natural language) domains, thereby allowing us to test the extent to which they can be interactively personalised. Contrastive explanations, in particular class-contrastive counterfactual statements, are the foundation of our system. These take the form of: ‚Äúhad one of the attributes been different in a particular way, the classification outcome would have changed as follows‚Ä¶.‚Äù Arguably, they are the most suitable, natural and appealing explanations targeted at humans¬†[ref]26 . In addition to all of their desired properties grounded in the social sciences¬†[ref]26 and legal considerations¬†, they can be easily adapted to an interactive dialogue aimed at personalisation, which is not widely utilised. In our system they are delivered in an interactive dialogue ‚Äì a natural language conversation, which is the most intuitive explanatory mechanism¬†[ref]26. In summary, our approach aims to build a holistic and diverse interactive XAI system where the interaction is focused on personalising explanations (in accordance with Miller‚Äôs¬†[ref]26 notion of XAI interactivity) as opposed to simply building an XAI system that provides explanations interactively (to explain different aspects of a black-box system using a range of XAI algorithms) ‚Äì a subtle but significant difference.\n\nReference markers found: [26]\n\nContext windows for each [ref]:\n[ref]26: rendering the task feasible and making the whole process feel more natural, engaging and less frustrating. In human interactions, understanding is naturally achieved via an explanatory dialogue¬†[ref]26, possibly supported with visual aids. Mirroring this explanatory process for ML transparency would make it attractive and accessible to a wide audience. Furthermore, allowing the user to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 26,\n            \"titles\": [\n                \"Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences\",\n                \"Natural Language Interaction with Explainable AI Models\",\n                \"Power to the people: The role of humans in interactive machine learning\",\n                \"Formalizing explanatory dialogues\",\n                \"Justification narratives for individual classifications\",\n                \"Wizard of Oz studies ‚Äî why and how\",\n                \"Assumption-based argumentation\",\n                \"Bringing transparency design into practice\",\n                \"Greedy function approximation: a gradient boosting machine\",\n                \"Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"DARPA‚Äôs Explainable Artificial Intelligence Program\",\n                \"Towards a Generic Framework for Black-box Explanation Methods\",\n                \"Interactive optimization for steering machine classification\",\n                \"iBCM: Interactive Bayesian case model empowering humans via intuitive interaction\",\n                \"Using visual analytics to interpret predictive machine learning models\",\n                \"Interacting with predictions: Visual inspection of black-box machine learning models\",\n                \"Explanatory debugging: Supporting end-user debugging of machine-learned programs\",\n                \"Tell me more?: the effects of mental model soundness on personalizing an intelligent agent\",\n                \"Too much, too little, or just right? Ways explanations impact end users‚Äô mental models\",\n                \"Principles of explanatory debugging to personalize interactive machine learning\",\n                \"Counterfactual Fairness\",\n                \"Faithful and customizable explanations of black box models\",\n                \"The Doctor Just Won‚Äôt Accept That!\",\n                \"A Unified Approach to Interpreting Model Predictions\",\n                \"A Grounded Interaction Protocol for Explainable Artificial Intelligence\",\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"Scikit-learn: Machine Learning in Python\",\n                \"FACE: Feasible and Actionable Counterfactual Explanations\",\n                \"‚ÄúWhy Should I Trust You?‚Äù Explaining the Predictions of Any Classifier\",\n                \"Anchors: High-precision model-agnostic explanations\",\n                \"Perturbation-Based Explanations of Prediction Models\",\n                \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n                \"Personalized Explanation for Machine Learning: a Conceptualization\",\n                \"Counterfactual Explanations of Machine Learning Predictions: Opportunities and Challenges for AI Safety\",\n                \"Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches\",\n                \"Glass-Box: Explaining AI Decisions With Counterfactual Statements Through Conversation With a Voice-enabled Virtual Assistant\",\n                \"bLIMEy: Surrogate Prediction Explanations Beyond LIME\",\n                \"Contrastive Explanations with Local Foil Trees\",\n                \"Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]26 based on the context and research area of One Explanation Does Not Fit All. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}