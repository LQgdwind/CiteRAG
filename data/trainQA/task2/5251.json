{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Real-Time Neural Light Field on Mobile Devices\n\nPaper Abstract: \n\nPaper Section Text:\nNeural Radiance Field (NeRF). NeRF [ref]33 shows the possibility of representing a scene with a simple multi-layer perceptron (MLP) network. Going forward, many extensions follow up in improving rendering quality (e.g., MipNeRF , MipNeRF 360 , and Ref-NeRF ), rendering efficiency (e.g., NSVF , Nex , AutoInt , FastNeRF , Baking , Plenoctree [ref]45, KiloNeRF [ref]37, DeRF , DoNeRF , R2L , and MobileNeRF ) and training efficiency (e.g., Plenoxels , and Instant-NGP ). Efficient NeRF Rendering.\nSince this paper falls into the category of improving rendering efficiency as we target real-time rendering on mobile devices, we single out the papers of this group and discuss them in length here. There are generally four groups.\n(1) The first group trades speed with space, i.e., they precompute and cache scene representations and the rendering reduces to table lookup. Efficient data structure like sparse octree, e.g., Plenoctree [ref]45, is usually utilized to make the rendering even faster.\n(2) The second attempts reduce the number of sampled points along the camera ray during rendering as it is the root cause of prohibitively slow rendering speed. Fewer sampled points typically lead to performance degradation, so as compensation, they usually introduce extra information, such as depth, e.g., DoNeRF , or mesh, e.g., MobileNeRF , to maintain the visual quality.\n(3) The third group takes a “divide and conquer” strategy. DeRF  decomposes the scene spatially to Voronoi diagrams and learns each diagram with a small network. KiloNeRF [ref]37 also employs a decomposition scheme. Differently, they decompose the scene into thousands of small regular grids. Each is addressed with a small network. Such decomposition poses challenges to parallelism. Thus they utilize customized parallelism implementation to obtain speedup.\n(4) The fourth group is a newly surging one, represented by the recent works RSEN  and R2L . They achieve rendering efficiency by representing the scene with NeLF (neural light field) instead of NeRF. NeLF avoids the dense sampling on camera ray, resulting in a much faster rendering speed than NeRF. On the downside, NeLF is typically much harder to learn than NeRF. As a remedy, these works (such as ) typically integrate a pre-trained NeRF model as a teacher to synthesize additional pseudo data for distillation  . Therefore, the resulting model is fast with a reasonably small representation size, i.e., the model size. Neural Light Field (NeLF). Light field is a different way of representing scenes. The idea has a long history in the computer graphics community, e.g., Light fields  and Lumigraphs  cache plenty of images and enable real-time rendering at the cost of limited camera pose and excessive storage overhead. One of the most intriguing properties of NeLF is that rendering one image only requires one network forward, resulting in a significantly faster speed than NeRF-based methods. With the recent surge of the neural radiance field, some works attempt to revive the idea of the neural light field for efficient neural rendering. Sitzmann et al.  materialize the idea of using a neural network to model the scene, and the rendering process reduces to a single network forward. Despite the encouraging idea, their method has only been evaluated on scenes with simple shapes, not matching the quality of NeRF on complex real-world scenes. Later, RSEN  and R2L  are introduced. RSEN divides the space into many voxel grids. Only in each grid, it is a NeLF, which needs alpha-composition to render the final color, making their method a mixture of NeLF and NeRF. R2L  is a pure NeLF network that avoids the alpha-composition step in rendering, which is also one of the most relevant works to this paper. However, R2L is still not compact and fast enough for mobile devices. Based on our empirical study, an R2L model runs for around three seconds per frame on iPhone 13 even for low-resolution like 200×200200200200\\times 200. This paper is meant to push the NeRF-to-NeLF idea even further, making it able to perform real-time rendering on mobile devices. We will mainly compare to MobileNeRF  in this paper as it is the only method, to our best knowledge, that can run on mobile devices with matching quality to NeRF [ref]33.\n\nReference markers found: [33]\n\nContext windows for each [ref]:\n[ref]33: Neural Radiance Field (NeRF). NeRF [ref]33 shows the possibility of representing a scene with a simple multi-layer perceptron (MLP) network. Going forward, many extensions follow up in improving rendering quality (e.g., MipNeRF ,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 33,\n            \"titles\": [\n                \"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\",\n                \"Lens studio.\",\n                \"Nerf-factory.\",\n                \"Snap ml.\",\n                \"Deep Learning using Rectified Linear Units (ReLU)\",\n                \"Learning neural light fields with ray-space embedding.\",\n                \"​​Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields​\",\n                \"Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields\",\n                \"Model compression.\",\n                \"Efficient Geometry-aware 3D Generative Adversarial Networks\",\n                \"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures\",\n                \"Use coremltools to convert models from third-party libraries to core ml.\",\n                \"Plenoxels: Radiance Fields without Neural Networks\",\n                \"Fastnerf: High-fidelity neural rendering at 200fps.\",\n                \"The lumigraph.\",\n                \"StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis\",\n                \"Baking Neural Radiance Fields for Real-Time View Synthesis\",\n                \"Gaussian Error Linear Units (GELUs)\",\n                \"Distilling the Knowledge in a Neural Network\",\n                \"HeadNeRF: A Real-time NeRF-based Parametric Head Model\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\",\n                \"Ray tracing volume densities.\",\n                \"Adam: A method for stochastic optimization.\",\n                \"Light field rendering.\",\n                \"Rethinking Vision Transformers for MobileNet Size and Speed\",\n                \"EfficientFormer: Vision Transformers at MobileNet Speed\",\n                \"AutoInt: Automatic Integration for Fast Neural Volume Rendering\",\n                \"NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field\",\n                \"Neural Sparse Voxel Fields\",\n                \"Efficient sparse-winograd convolutional neural networks.\",\n                \"Optical models for direct volume rendering.\",\n                \"Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines\",\n                \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\",\n                \"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\",\n                \"DeRF: Decomposed Radiance Fields\",\n                \"Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps.\",\n                \"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering\",\n                \"Attention Is All You Need\",\n                \"Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\",\n                \"R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]33 based on the context and research area of Real-Time Neural Light Field on Mobile Devices. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}