{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Theory and Applications of Probabilistic Kolmogorov Complexity\n\nPaper Abstract: \n\nPaper Section Text:\nThe coding theorem for Kolmogorov complexity roughly states that if a string xğ‘¥x can be sampled with probability Î´ğ›¿\\delta by some algorithm Ağ´A, then its Kolmogorov complexity ğ–ªâ€‹(x)ğ–ªğ‘¥\\mathsf{K}(x) is at most logâ¡(1/Î´)+OAâ€‹(1)1ğ›¿subscriptğ‘‚ğ´1\\log(1/\\delta)+O_{A}(1). In particular, strings that can be generated with non-trivial probability by a program of small description length admit shorter representations. The coding theorem is a fundamental result in Kolmogorov complexity theory that has found many applications in theoretical computer science (see, e.g.,    ). In fact,  regards the coding theorem as one of the four pillars of Kolmogorov complexity.999The other three are incompressibility, language compression, and symmetry of information. The proof of the coding theorem crucially explores the time-unbounded feature of the Kolmogorov complexity measure, and it is unclear how it can be extended to the time-bounded setting. Ideally, we would like to show that if a string xğ‘¥x can be generated with probability Î´ğ›¿\\delta by some efficiently samplable distribution, then its time-bounded Kolmogorov complexity ğ–ªğ—â€‹(x)ğ–ªğ—ğ‘¥\\mathsf{Kt}(x) is about logâ¡(1/Î´)1ğ›¿\\log(1/\\delta). One reason why such a time-bounded coding theorem is hopeful is that it can be proven under certain strong derandomisation assumption .101010The assumption in  states that there is a language Lâˆˆğ–³ğ–¨ğ–¬ğ–¤â€‹[2Oâ€‹(n)]ğ¿ğ–³ğ–¨ğ–¬ğ–¤delimited-[]superscript2ğ‘‚ğ‘›L\\in{\\sf TIME}\\!\\left[2^{O(n)}\\right] that requires Boolean circuits of size 2Î©â€‹(n)superscript2Î©ğ‘›2^{\\Omega(n)} for all but finitely many nğ‘›n, even in the presence of oracle gates to a Î£2psubscriptsuperscriptÎ£ğ‘2\\Sigma^{p}_{2}-complete problem in the circuit. In particular, under such an assumption, if a polynomial-time samplable distribution outputs a string xğ‘¥x with probability at least Î´ğ›¿\\delta, then ğ–ªğ—â€‹(x)â‰¤logâ¡(1/Î´)+Oâ€‹(logâ¡n)ğ–ªğ—ğ‘¥1ğ›¿ğ‘‚ğ‘›\\mathsf{Kt}(x)\\leq\\log(1/\\delta)+O(\\log n). However, the latter result is only conditional, in the sense that it relies on an unproven assumption that seems far beyond the reach of currently known techniques. Moreover, strong assumptions of this form could even be false. While it remains unclear whether we can obtain a coding theorem for ğ–ªğ—ğ–ªğ—\\mathsf{Kt}, [ref]44 considered the problem of establishing an unconditional coding theorem in the randomised time-bounded setting. Somewhat surprisingly, it can be shown unconditionally that if a string xğ‘¥x can be sampled efficiently with probability Î´ğ›¿\\delta, then ğ—‹ğ–ªğ—â€‹(x)â‰¤Oâ€‹(logâ¡1/Î´)+Oâ€‹(logâ¡n)ğ—‹ğ–ªğ—ğ‘¥ğ‘‚1ğ›¿ğ‘‚ğ‘›\\mathsf{rKt}(x)\\leq O(\\log 1/\\delta)+O(\\log n). In a subsequent work [ref]46, this result is further improved to ğ—‹ğ–ªğ—â€‹(x)â‰¤(2+oâ€‹(1))â‹…logâ¡1/Î´+Oâ€‹(logâ¡n)ğ—‹ğ–ªğ—ğ‘¥â‹…2ğ‘œ11ğ›¿ğ‘‚ğ‘›\\mathsf{rKt}(x)\\leq(2+o(1))\\cdot\\log 1/\\delta+O(\\log n). Suppose there is an efficient algorithm Ağ´A for sampling strings such that Aâ€‹(1n)ğ´superscript1ğ‘›A(1^{n}) outputs a string xâˆˆ{0,1}nğ‘¥superscript01ğ‘›x\\in\\{0,1\\}^{n} with probability at least Î´ğ›¿\\delta. Then where the constant behind the Oâ€‹(â‹…)ğ‘‚â‹…O(\\cdot) depends on Ağ´A and is independent of the remaining parameters. Moreover, given xğ‘¥x, the code of Ağ´A, and Î´ğ›¿\\delta, it is possible to compute in time ğ—‰ğ—ˆğ—…ğ—’â€‹(n,|A|)ğ—‰ğ—ˆğ—…ğ—’ğ‘›ğ´\\mathsf{poly}(n,|A|), with probability â‰¥0.99absent0.99\\geq 0.99, a probabilistic representation of xğ‘¥x that satisfies this ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}-complexity bound. (The running time of this algorithm does not depend on the time complexity of Ağ´A.) Similar to the results in the previous section that are concerned with the compressibility of prime numbers, the results of [ref]44 [ref]46 again show the power of utilizing randomness in Kolmogorov complexity, which enables us to establish results for time-bounded Kolmogorov complexity that seem very difficult to show in the deterministic setting. We refer to these papers for a discussion of the techniques employed to show an unconditional coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}. We note that (as in previous work of [ref]44) the coding theorem in TheoremÂ 10 has an unexpected constructive feature: it gives a polynomial-time probabilistic algorithm that, when given xğ‘¥x, the code of the sampler, and Î´ğ›¿\\delta, outputs a probabilistic representation of xğ‘¥x that certifies the claimed ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} complexity bound. (Additionally, the running time of this algorithm does not depend on the running time of the sampler.) Such an efficient coding theorem has interesting implications for search-to-decision reductions for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}. Recall that a search-to-decision reduction is an efficient procedure that allows one to find solutions to a problem from the mere ability to decide when a solution exists. Using results from [ref]44 [ref]46, one can show the following search-to-decision reduction for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}. Let ğ’ªğ’ª\\mathcal{O} be a function that linearly approximates ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} complexity. That is, for every xâˆˆ{0,1}âˆ—ğ‘¥superscript01x\\in\\{0,1\\}^{*}, Then there is a randomised polynomial-time algorithm with access to ğ’ªğ’ª\\mathcal{O} that, when given an input string xğ‘¥x, outputs with probability â‰¥0.99absent0.99\\geq 0.99 a valid ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation of xğ‘¥x of complexity Oâ€‹(ğ—‹ğ–ªğ—â€‹(x))ğ‘‚ğ—‹ğ–ªğ—ğ‘¥O(\\mathsf{rKt}(x)). Furthermore, this algorithm makes a single query qğ‘q to ğ’ªğ’ª\\mathcal{O}, where q=xğ‘ğ‘¥q=x. We would like to invoke TheoremÂ 10 to efficiently compute an ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation of xğ‘¥x, but the â€œmoreoverâ€ part of this result requires the explicit code of a sampler. The idea is to construct a â€œuniversalâ€ sampler that outputs xğ‘¥x with the desired probability, then to hit this sampler with an appropriate coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}. For simplicity, suppose we knew the exact value k=ğ—‹ğ–ªğ—â€‹(x)âˆˆâ„•ğ‘˜ğ—‹ğ–ªğ—ğ‘¥â„•k=\\mathsf{rKt}(x)\\in\\mathbb{N}. Consider the following sampler Ağ´A:\n Aâ€‹(1n)ğ´superscript1ğ‘›A(1^{n}): Randomly selects a randomised program Mğ‘€M of length kğ‘˜k among all strings in {0,1}ksuperscript01ğ‘˜\\{0,1\\}^{k}. Run Mğ‘€M for at most 2ksuperscript2ğ‘˜2^{k} steps, then output the nğ‘›n-bit string that Mğ‘€M outputs during this simulation (or the string 0nsuperscript0ğ‘›0^{n} if Mğ‘€M does not stop or its output is not an nğ‘›n-bit string).\n Note that Ağ´A runs in time t=ğ—‰ğ—ˆğ—…ğ—’â€‹(n,2k)=ğ—‰ğ—ˆğ—…ğ—’â€‹(2k)ğ‘¡ğ—‰ğ—ˆğ—…ğ—’ğ‘›superscript2ğ‘˜ğ—‰ğ—ˆğ—…ğ—’superscript2ğ‘˜t=\\mathsf{poly}(n,2^{k})=\\mathsf{poly}(2^{k}) (since kâ‰¥logâ¡nğ‘˜ğ‘›k\\geq\\log n for any nğ‘›n-bit string), and that it outputs xğ‘¥x with probability at least Î´=2âˆ’kâ‹…2/3ğ›¿â‹…superscript2ğ‘˜23\\delta=2^{-k}\\cdot 2/3, since by the definition of kğ‘˜k at least one such program prints xğ‘¥x with probability at least 2/3232/3. By the coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} from [ref]44 (which is stated in a slightly more general form than TheoremÂ 10), one obtains that ğ—‹ğ–ªğ—â€‹(x)=Oâ€‹(logâ¡(1/Î´))+Oâ€‹(logâ¡t)=Oâ€‹(k)ğ—‹ğ–ªğ—ğ‘¥ğ‘‚1ğ›¿ğ‘‚ğ‘¡ğ‘‚ğ‘˜\\mathsf{rKt}(x)=O(\\log(1/\\delta))+O(\\log t)=O(k). Since Ağ´A is an explicit algorithm, crucially, its â€œmoreoverâ€ part implies that we can efficiently output an ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}-representation of xğ‘¥x of complexity Oâ€‹(k)ğ‘‚ğ‘˜O(k). This completes the sketch of the proof. We refer to [44, Section 4] for the formal proof of TheoremÂ 11, which is a simple adaptation of the idea described here.\nâˆ An interesting feature of the above search-to-decision reduction is that it is instance-wise in the sense that to produce\na near-optimal ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation of xğ‘¥x, we only need to make a single query to a decision oracle for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} on the same xğ‘¥x.111111This is also called a search-to-profile reduction in some references in Kolmogorov complexity . Note that there are known search-to-decision reductions in the context of time-bounded Kolmogorov complexity with respect to various notions of complexity (e.g., 6  0 2  1), but they require an oracle to the decision problem that is correct on all or at least on a large fraction of inputs. As a consequence of this feature, we can easily derive the following result.121212Results of this form were previously known in time-unbounded Kolmogorov complexity (see 3). Given a string xğ‘¥x of length nğ‘›n, it is possible to compute with probability â‰¥0.99absent0.99\\geq 0.99 and in polynomial time a collection of at most â„“=logâ¡(n)â„“ğ‘›\\ell=\\log(n) strings M1,â€¦,Mâ„“subscriptğ‘€1â€¦subscriptğ‘€â„“M_{1},\\ldots,M_{\\ell} such that at least one of these strings is a valid ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation of xğ‘¥x of complexity Oâ€‹(ğ—‹ğ–ªğ—â€‹(x))ğ‘‚ğ—‹ğ–ªğ—ğ‘¥O(\\mathsf{rKt}(x)). We run the instance-wise search-to-decision reduction on the input xğ‘¥x. While it is not clear how to efficiently estimate ğ—‹ğ–ªğ—â€‹(x)ğ—‹ğ–ªğ—ğ‘¥\\mathsf{rKt}(x), we can still â€œguessâ€ the ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} complexity of xğ‘¥x to be of order 2isuperscript2ğ‘–2^{i}, for each iâˆˆ{1,2,â€¦,logâ¡n}ğ‘–12â€¦ğ‘›i\\in\\{1,2,\\ldots,\\log n\\}. We run the procedure on each possible guess, obtaining a list of strings M1,â€¦,Mâ„“subscriptğ‘€1â€¦subscriptğ‘€â„“M_{1},\\ldots,M_{\\ell}, where â„“=logâ¡nâ„“ğ‘›\\ell=\\log n. Since there is at least one value iğ‘–i such that 2i=Î˜â€‹(ğ—‹ğ–ªğ—â€‹(x))superscript2ğ‘–Î˜ğ—‹ğ–ªğ—ğ‘¥2^{i}=\\Theta(\\mathsf{rKt}(x)), we have the guarantee that in this case the reduction outputs with probability at least 0.990.990.99 a valid ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation of xğ‘¥x of similar complexity. Therefore, the list contains with probability at least 0.990.990.99 a representation of the desired form.\nâˆ While the above coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} is a novel development after a long gap in an area with only conditional results, it has an important drawback: the ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} upper bound is at least 2log(1/Î´))2\\log(1/\\delta)) and hence is sub-optimal. In contrast, the bounds in the time-unbounded setting and in the conditional result of  mentioned above have the form logâ¡(1/Î´)1ğ›¿\\log(1/\\delta). A natural question then is whether we can show a coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} with an optimal dependence on the probability parameter Î´ğ›¿\\delta, which is crucial in many applications of the result. It turns out that under a certain hypothesis about the security of cryptographic pseudorandom generators131313The hypothesis states that there is a pseudorandom generator G:{0,1}â„“â€‹(n)â†’{0,1}n:ğºâ†’superscript01â„“ğ‘›superscript01ğ‘›G\\colon\\{0,1\\}^{\\ell(n)}\\to\\{0,1\\}^{n}, where (logâ¡n)Ï‰â€‹(1)â‰¤â„“â€‹(n)â‰¤n/2superscriptğ‘›ğœ”1â„“ğ‘›ğ‘›2(\\log n)^{\\omega(1)}\\leq\\ell(n)\\leq n/2, computable in time polyâ¡(n)polyğ‘›\\operatorname{poly}(n) that is secure against uniform algorithms running in time 2(1âˆ’Î©â€‹(1))â‹…â„“â€‹(n)superscript2â‹…1Î©1â„“ğ‘›2^{(1-\\Omega(1))\\cdot\\ell(n)}. Note that every candidate PRG of seed length â„“â€‹(n)â„“ğ‘›\\ell(n) can be broken in time 2â„“â€‹(n)â‹…ğ—‰ğ—ˆğ—…ğ—’â€‹(n)â‹…superscript2â„“ğ‘›ğ—‰ğ—ˆğ—…ğ—’ğ‘›2^{\\ell(n)}\\cdot\\mathsf{poly}(n) by trying all possible seeds. This hypothesis can be viewed as a cryptographic analogue of the well-known strong exponential time hypothesis (SETH) about the complexity of kğ‘˜k-CNF SAT 3., the ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} bound in TheoremÂ 10 is essentially optimal if we consider only coding theorems that are efficient, i.e., where an ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} representation can be constructed in polynomial time regardless of the running time of the sampler. In particular, [ref]46 showed that in this case, there is no efficient coding theorem that can achieve a bound of the form ğ—‹ğ–ªğ—â€‹(x)â‰¤(2âˆ’oâ€‹(1))â‹…logâ¡(1/Î´)+polyâ¡(logâ¡n)ğ—‹ğ–ªğ—ğ‘¥â‹…2ğ‘œ11ğ›¿polyğ‘›\\mathsf{rKt}(x)\\leq(2-o(1))\\cdot\\log(1/\\delta)+\\operatorname{poly}(\\log n). On the other hand, the conditional coding theorem for ğ–ªğ—ğ–ªğ—\\mathsf{Kt} in  is not efficient. This leads to the following open problem on (unconditionally) showing an existential coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} with optimal parameters. Show that if there is an efficient algorithm Ağ´A for sampling strings such that Aâ€‹(1n)ğ´superscript1ğ‘›A(1^{n}) outputs a string xâˆˆ{0,1}nğ‘¥superscript01ğ‘›x\\in\\{0,1\\}^{n} with probability at least Î´ğ›¿\\delta, then ğ—‹ğ–ªğ—â€‹(x)â‰¤logâ¡(1/Î´)+polyâ¡(logâ¡n)ğ—‹ğ–ªğ—ğ‘¥1ğ›¿polyğ‘›\\mathsf{rKt}(x)\\leq\\log(1/\\delta)+\\operatorname{poly}(\\log n). To this point, we have mentioned the existence of an optimal coding theorem for time-unbounded Kolmogorov complexity and an optimal conditional coding theorem for ğ–ªğ—ğ–ªğ—\\mathsf{Kt} (in fact, the conditional result holds even for ğ–ªtsuperscriptğ–ªğ‘¡\\mathsf{K}^{t} for t=ğ—‰ğ—ˆğ—…ğ—’â€‹(n)ğ‘¡ğ—‰ğ—ˆğ—…ğ—’ğ‘›t=\\mathsf{poly}(n)). Also, an unconditional coding theorem can be obtained for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} but its dependency on the probability parameter Î´ğ›¿\\delta is not logâ¡(1/Î´)1ğ›¿\\log(1/\\delta) (TheoremÂ 10). Note that ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} can be viewed as a â€œrelaxedâ€ notion of ğ–ªğ—ğ–ªğ—\\mathsf{Kt} and is intermediate between ğ–ªğ–ª\\mathsf{K} and ğ–ªğ—ğ–ªğ—\\mathsf{Kt}. If we consider some further relaxed notion of time-bounded Kolmogorov complexity, can we show a coding theorem that is both unconditional and optimal? Note that the time-bounded measure ğ—‰ğ–ªğ—‰ğ–ª\\mathsf{pK} can be viewed as an intermediate notion between time-unbounded Kolmogorov complexity and time-bounded ğ—‹ğ–ªğ—‹ğ–ª\\mathsf{rK}. It turns out that ğ—‰ğ–ªtsuperscriptğ—‰ğ–ªğ‘¡\\mathsf{pK}^{t} admits an optimal coding theorem. Suppose there is a randomised algorithm Ağ´A for sampling strings such that Aâ€‹(1n)ğ´superscript1ğ‘›A(1^{n}) runs in time Tâ€‹(n)â‰¥nğ‘‡ğ‘›ğ‘›T(n)\\geq n and outputs a string xâˆˆ{0,1}nğ‘¥superscript01ğ‘›x\\in\\{0,1\\}^{n} with probability at least Î´>0ğ›¿0\\delta>0. Then where tâ€‹(n)=polyâ¡(Tâ€‹(n))ğ‘¡ğ‘›polyğ‘‡ğ‘›t(n)=\\operatorname{poly}\\!\\left(T(n)\\right) and the constant behind the Oâ€‹(â‹…)ğ‘‚â‹…O(\\cdot) depends on |A|ğ´|A| and is independent of the remaining parameters. The proof of TheoremÂ 14 is similar in spirit to that of the conditional coding theorem for ğ–ªpolysuperscriptğ–ªpoly\\mathsf{K}^{\\operatorname{poly}} in . As an application of the latter,  showed a conditional characterisation of the worst-case running times of languages that are in average polynomial time over all samplable distributions. Using TheoremÂ 14, [ref]46 provided an unconditional characterisation, and this will be discussed in SectionÂ 6. Finally, we can connect the time-bounded coding theorems discussed in this section to the compressibility of prime numbers discussed in the previous section, via the following equivalence. Let Î´:â„•â†’ :ğ›¿â†’â„•01\\delta\\colon\\mathbb{N}\\to  be a time-constructible function. The following statements are equivalent. Samplability. There is a randomised algorithm Ağ´A for sampling strings such that, for infinitely many (resp.Â all but finitely many) nğ‘›n, Aâ€‹(1n)ğ´superscript1ğ‘›A(1^{n}) runs in time (1/Î´â€‹(n))Oâ€‹(1)superscript1ğ›¿ğ‘›ğ‘‚1(1/\\delta(n))^{O(1)} and outputs an nğ‘›n-bit prime qnsubscriptğ‘ğ‘›q_{n} with probability at least Î´â€‹(n)Oâ€‹(1)ğ›¿superscriptğ‘›ğ‘‚1\\delta(n)^{O(1)}. Compressibility. For infinitely many (resp.Â all but finitely many) nğ‘›n, there is an nğ‘›n-bit prime pnsubscriptğ‘ğ‘›p_{n} with ğ—‹ğ–ªğ—â€‹(pn)=Oâ€‹(logâ¡(1/Î´â€‹(n)))ğ—‹ğ–ªğ—subscriptğ‘ğ‘›ğ‘‚1ğ›¿ğ‘›\\mathsf{rKt}(p_{n})=O(\\log(1/\\delta(n))). The implication from (i) to (ii) relies on the existing coding theorem for ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt}. The other direction employs a universal sampler in the spirit of the proof of TheoremÂ 11 sketched above. See [ref]44 for the details.\nâˆ TheoremÂ 15 can be seen as an analogue of the relation between deterministically constructing large primes and obtaining ğ–ªğ—ğ–ªğ—\\mathsf{Kt} upper bounds for primes, which was explained in SectionÂ 1. Using this result, the problem of showing that prime numbers have smaller ğ—‹ğ–ªğ—ğ—‹ğ–ªğ—\\mathsf{rKt} complexity (Problem 9) can be reduced to showing the existence of a faster sampling algorithm for primes. In particular, if we can sample an nğ‘›n-bit prime pnsubscriptğ‘ğ‘›p_{n} in time 2polyâ¡(logâ¡n)superscript2polyğ‘›2^{\\operatorname{poly}(\\log n)} with probability at least 2âˆ’polyâ¡(logâ¡n)superscript2polyğ‘›2^{-\\operatorname{poly}(\\log n)}, then ğ—‹ğ–ªğ—â€‹(pn)â‰¤polyâ¡(logâ¡n)ğ—‹ğ–ªğ—subscriptğ‘ğ‘›polyğ‘›\\mathsf{rKt}(p_{n})\\leq\\operatorname{poly}(\\log n). We remark that an even tighter equivalence between samplability and compressibility can be established using ğ—‰ğ–ªtsuperscriptğ—‰ğ–ªğ‘¡\\mathsf{pK}^{t} complexity, thanks to the optimality of TheoremÂ 14.\n\nReference markers found: [44]\n\nContext windows for each [ref]:\n[ref]44: beyond the reach of currently known techniques. Moreover, strong assumptions of this form could even be false. While it remains unclear whether we can obtain a coding theorem for ğ–ªğ—ğ–ªğ—\\mathsf{Kt}, [ref]44 considered the problem of establishing an unconditional coding theorem in the randomised time-bounded setting. Somewhat surprisingly, it can be shown unconditionally that if a string xğ‘¥x can\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 44,\n            \"titles\": [\n                \"Pseudodeterministic Algorithms and the Structure of Probabilistic Time\",\n                \"The equivalence of sampling and searching\",\n                \"Power from random strings\",\n                \"Worst-case running times for average-case algorithms\",\n                \"Computational depth\",\n                \"PRIMES is in P\",\n                \"Applications of time-bounded Kolmogorov complexity in complexity theory\",\n                \"When worlds collide: Derandomization, lower bounds, and Kolmogorov complexity\",\n                \"The complexity of complexity\",\n                \"Vaughan Jones, Kolmogorov complexity, and the new complexity landscape around circuit minimization\",\n                \"A probabilistic-time hierarchy theorem for â€œslightly non-uniformâ€ algorithms\",\n                \"Some results on derandomization\",\n                \"Language compression and pseudorandom generators\",\n                \"Short lists with short programs in short time\",\n                \"Average-case complexity\",\n                \"Average-case hardness of NP and PH from worst-case fine-grained assumptions\",\n                \"Learning algorithms from natural proofs\",\n                \"Kolmogorov complexity and computational complexity\",\n                \"Hierarchy theorems for probabilistic polynomial time\",\n                \"Probabilistic search algorithms with unique answers and their cryptographic applications\",\n                \"A simpler proof of the worst-case to average-case reduction for polynomial hierarchy via symmetry of information\",\n                \"Probabilistic Kolmogorov complexity with applications to average-case complexity\",\n                \"Non-black-box worst-case to average-case reductions within NP\",\n                \"Characterizing average-case complexity of PH by worst-case meta-complexity\",\n                \"Unexpected hardness results for Kolmogorov complexity under uniform reductions\",\n                \"Average-case hardness of NP from exponential worst-case hardness assumptions\",\n                \"Meta-computational average-case complexity: A new paradigm toward excluding heuristica\",\n                \"Symmetry of information in heuristica\",\n                \"On worst-case learning in relativized heuristica\",\n                \"Errorless versus error-prone average-case complexity\",\n                \"Connecting Perebor conjectures: Towards a search to decision reduction for minimizing formulas\",\n                \"The minimum formula size problem is (ETH) hard\",\n                \"NP-hardness of circuit minimization for multi-output functions\",\n                \"On the complexity of SAT\",\n                \"Hardness on any samplable distribution suffices: New characterizations of one-way functions by meta-complexity\",\n                \"P = BPP if E requires exponential circuits: Derandomizing the XOR lemma\",\n                \"Improper learning by refuting\",\n                \"On the complexity of learning minimum time-bounded Turing machines\",\n                \"The Hardest Explicit Construction\",\n                \"Information in propositional proofs and algorithmic proof search\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]44 based on the context and research area of Theory and Applications of Probabilistic Kolmogorov Complexity. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}