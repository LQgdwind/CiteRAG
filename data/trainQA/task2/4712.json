{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Optimization of FPGA-based CNN Accelerators Using Metaheuristics\n\nPaper Abstract: \n\nPaper Section Text:\nIn the first experiment, we study the effectiveness of the proposed optimization framework in finding the optimal configurations that maximize the throughput of the Multi-CLP design. Hence, this experiment uses the computational performance and resource usage models as well as the maximum resources available on the FPGA to find the solution with the minimum execution cycles. The behavior of a single run of SA algorithm for AlexNet CNN acceleration using multiple CLPs on 485T FPGA is shown in Figure 7. The figure shows the variation in the execution time, which is defined as the total number of cycles divided by the operating frequency. The number of cycles is the cost function, illustrated in Equation (19), that SA algorithm tries to optimize for 1,00010001,000 iterations when run with the parameters mentioned in the previous section. We refer to this approach as SA-based Multi-CLP. As can be seen, since the algorithm starts with a high temperature, the fluctuation in costs is large, which gets reduced over time as the temperature is reduced and better solutions are discovered with minimal cost variations toward the end. Final solution configurations can be found in Table III. Similarly, the behavior of a single run of TS algorithm for the same CNN and the same FPGA board is also illustrated. This approach to designing a Multi-CLP based on TS algorithm is indicated by the TS-based Multi-CLP. As can be seen from Figure 7, the cost reduces very quickly in the first few iterations as there are several neighboring candidates to choose from in each iteration, and then, it reduces at a slower rate in subsequent iterations with hill-climbing behavior, until reaching the final solution shown in Table III. From the figure, we can deduce the effectiveness of these algorithms in finding the optimal solution to the Multi-CLP design problem. Both algorithms were able to efficiently explore the intractable design space and find Multi-CLP design solutions with approximately the same low level of execution time, but each has a different final solution. It is noteworthy that the optimization framework is very fast, it explores the search space in less than a minute on a general-purpose processor. Next, we use the SA-based Multi-CLP and TS-based Multi-CLP approaches to accelerate AlexNet architecture on 485T FPGA. The obtained designs are then compared with the state-of-the-art Single-CLP  design and Multi-CLP  design, as shown in Table III. The table shows the number of CLPs adopted by each technique and the parallelism factors for each CLP. Additionally, it shows the assignment of CONV layers to CLPs. The number of cycles each CLP spends in processing its layers is also provided. Note that when layer assignment column contains more than one layer in a row, the cycle count provided is the total cycles needed to process all of those layers. The table also shows the execution time of an image’s CONV layer, which represents the time interval in which the system can receive a new image for processing as discussed in Section III-B. The execution time is calculated by dividing design cycles count, illustrated in Equation (19), over design operating frequency. Specifically, the cycle count for each CLP is the number of cycles required to execute all of its assigned layers. On the other hand, CLPs in Multi-CLP design work concurrently. Therefore, the overall cycle count for such a design is the maximum cycle count for its CLPs. The last two columns in Table III present the number of DSP slices and BRAMs utilized for each technique. As evidenced by the results, the number of cycles required in the case of a single CLP implementation is almost 1.3×1.3\\times more than that of the multiple CLPs implementations. The results also show that SA-based Multi-CLP and TS-based Multi-CLP were able to find implementation configurations with a fewer number of cycles (less execution time) compared to Multi-CLP even though all designs use the same number of CLPs and DSP slices. This demonstrates the effectiveness of the proposed SA-based Multi-CLP and TS-based Multi-CLP in balancing the computational resources and workloads assigned to each CLPs such that each CLP can be kept busy most of the time. Although the implementation configurations obtained using SA and TS metaheuristic algorithms show small improvements, the results indicate the importance of using metaheuristics in finding different configurations for a given CNN using a systematic methodology. Table IV shows similar experiments performed for AlexNet on 690T FPGA. The number of cycles required in the case of a single CLP implementation is about 1.5×1.5\\times more than that of multiple CLPs implementations. For the implementation configuration obtained using SA-based Multi-CLP, we got the same number of cycles as obtained by Multi-CLP. Although both techniques result in designs with same number of cycles and CLPs, these designs have completely different configurations for their adopted CLPs. On the other hand, implementation configuration obtained by TS-based Multi-CLP has used only 444 CLPs and results in a higher execution time. In Table V, we summarize the results presented in Tables III and IV to get an insight into the required bandwidth, arithmetic unit utilization, CONV layers throughput, and performance. We can see that SA-/TS-based Multi-CLP provides 1.34×1.34\\times and 1.54×1.54\\times more throughput improvement than Single-CLP design on 485T and 690T FPGAs, respectively. This improvement comes from its ability to make better use of the available arithmetic units. Specifically, Single-CLP can provide a useful work to the multipliers and adders only 747474% and 646464% of the time on 485T and 690T FPGAs, respectively, whereas SA-/TS-based Multi-CLP improves the arithmetic units utilization to about 969696% and 989898% on 485T and 690T FPGAs, respectively. Note that the arithmetic utilization is computed as discussed in Equation (18). Furthermore, SA-/TS-based Multi-CLP improves CONV layer throughput of Multi-CLP design by 1.02×1.02\\times on 485T. On the other hand, there is a trade-off between off-chip memory bandwidth and on-chip buffer size. Using large buffers saves off-chip memory bandwidth, whereas adopting small buffers results in a high bandwidth requirement. Here, we must emphasize that the proposed optimization framework gives a higher cost to bandwidth requirement than that given to buffer size. In other words, when more than one feasible design with the same low number of execution cycles is encountered, the design with minimum bandwidth requirement is chosen as optimal. We have adopted this strategy to minimize power expenses since on-chip memory modules consume less power than off-chip memory modules . The results for single CLP and multiple CLPs accelerators for SqueezeNet 1.11.11.1 on 485T and 690T FPGAs with 161616-bit fixed-point data are shown in Table VI. For SqueezeNet 1.11.11.1 accelerators on 485T FPGA, the single CLP implementation requires about 1.9×1.9\\times more cycles than the multiple CLPs implementation. Even more importantly, though, the results show that as the number of CONV layers increases, the search space also increases, making it more challenging to obtain an optimal multiple CLPs design using conventional iterative algorithms. Specifically, SA-based Multi-CLP processes images in a 1.041.041.04 ms and a 1.431.431.43 ms shorter time than Multi-CLP on 485T and 690T FPGAs, respectively. Additionally, although TS-based Multi-CLP produced a solution that requires more cycles than that of SA-based Multi-CLP, it processes images in a shorter time and with fewer DSP slices than Multi-CLP. On the other hand, when comparing the performance of SA-based Multi-CLP in accelerating SqueezeNet 1.11.11.1 on VC707 and VC709 boards, we can notice that it used the available resources to the maximum extent to improve the network throughput. Specifically, with a 1.29×1.29\\times increase in computational resources in VC709 compared to VC707, SA-based Multi-CLP increased SqueezeNet 1.11.11.1 throughput by 1.30×1.30\\times, which demonstrates the scalability of the proposed accelerator. In Tables VII and\nVIII, we show the configurations of single CLP and multiple CLPs accelerators designed for VGGNet and GoogLeNet on 690T FPGA with 161616-bit fixed-point representation. Note that no results have been reported in Single-CLP  and Multi-CLP  accelerators for these two large CNNs. Therefore, with the sake of brevity, we only present the results for SA-based Single-/Multi-CLP as it turns out from previous experiments that SA algorithm provides more optimized solutions than TS algorithm in this specific problem. For VGGNet, SA-based Multi-CLP reduces the total cycles compared to SA-based Single-CLP by about 676×103676superscript103{676\\times 10^{3}} cycles. Here, we see a significant difference compared to the results from AlexNet, which is due to the variance in the dimensions between CONV layers in VGGNet, where, for example, the first CONV layer has ⟨N,M⟩𝑁𝑀\\langle N,M\\rangle as ⟨3,64⟩364\\langle 3,64\\rangle, and those for last CONV layer is ⟨512,512⟩512512\\langle 512,512\\rangle. Hence, using a single CLP results in a large DSP resource under-utilization during the processing of some layers. For GoogLeNet, it requires around 2×2\\times more cycles for the single CLP implementation than that of multiple CLPs. For large CNNs, such as GoogLeNet which has 575757 CONV layers, it becomes more challenging to find an optimized multiple CLPs configuration without using metaheuristics algorithms. Additionally, we modeled the behavior of multiple CLPs design in Verilog and synthesized it using Xilinx Vivado Design Suite targeting the Xilinx Virtex-7 485T FPGA. We kept all synthesis properties to their default. Table IX shows a summary of the implementation and performance results for the proposed SA-based Multi-CLP approach and compares them with those reported in Single-CLP , GA-based Single-CLP [ref]17, and Multi-CLP  approaches when they are used to accelerate AlexNet architecture. For each technique, the table provides the FPGA platform used for implementation, operating frequency, precision adopted to represent FMs and weights, design entry employed to describe the accelerator, hardware resource usage after placement and routing in terms of DSPs, BRAMs, FFs, and LUTs, and power efficiency (a.k.a., performance per Watt). When comparing resource usage estimates with resource usage from Vivado’s implementation report for SA-based Multi-CLP, one can note that the model underestimated the DSP slices count by 535353 DSP slices per CLP, on average. The reason is that Vivado’s utilization report for SA-based Multi-CLP design accounts not only for DSPs used in CLP’s computational engine, but also for those used in the control logic, address calculations, and loop indexing as well. When considering only those DSPs used to implement the computational engines for SA-based Multi-CLP, we found that the estimates match those reported by Vivado utilization report. On the other hand, the implemented and predicted BRAM counts are perfectly matched. This is because in the implemented parametrized Verilog modules, we instantiate the exact required memory type, mode, and count using Xilinx HDL language templates. However, this does not apply to Single-CLP, GA-based Single-CLP, and Multi-CLP because they use high-level synthesis tools to compile C, OpenCL, and C++ codes into HDL codes, respectively. As can be seen from the results, designing with multiple CLPs makes better use of available resources than a single CLP design. Due to the dimensional mismatch issue in Single-CLP design, this design scheme could not take advantage of more than 828282% of DSP slices. One can also note that the proposed SA-based Multi-CLP increases the usage of FF and LUT resources in Single-CLP by about 555% for each additional CLP. All this increase in resource usage contributes to improving the power efficiency by 3.65×3.65\\times. Compared to GA-based Single-CLP, the proposed SA-based Multi-CLP has effectively made use of the available resources resulting in a 3.19×3.19\\times increase in power efficiency even though GA-based Single-CLP uses 888-bit fixed-point weights and 161616-bit fixed-point FMs while our accelerator uses 323232-bit floating-point data and operates at a lower frequency. This is due to the high-level synthesis tool employed in [ref]17 which constrains Ti​nsubscript𝑇𝑖𝑛T_{in} to be from the set {1,2,4,8,16}124816\\{1,2,4,8,16\\} and To​u​tsubscript𝑇𝑜𝑢𝑡T_{out} to be integer multiplicative of Ti​nsubscript𝑇𝑖𝑛T_{in}, where To​u​tsubscript𝑇𝑜𝑢𝑡T_{out} and Ti​nsubscript𝑇𝑖𝑛T_{in} are the unified unrolling factors for the output matrix of CONV operation and the input vectors to CLP structures, respectively, which are determined by GA to minimize execution time. In contrast, this work allows unrolling factors to be whatever value yields the highest performance. On the other hand, the usage of DSP, FF, and LUT resources for Multi-CLP and the proposed SA-based Multi-CLP is almost consistent. However, the additional 168168168 BRAMs used in Multi-CLP led to a 0.40.40.4 Watt increase in its power consumption, which also caused its power efficiency to be about 0.870.870.87 GOPs/Watt lower than that of the proposed SA-based Multi-CLP.\n\nReference markers found: [17]\n\nContext windows for each [ref]:\n[ref]17: default. Table IX shows a summary of the implementation and performance results for the proposed SA-based Multi-CLP approach and compares them with those reported in Single-CLP , GA-based Single-CLP [ref]17, and Multi-CLP  approaches when they are used to accelerate AlexNet architecture. For each technique, the table provides the FPGA platform used for implementation, operating frequency,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 17,\n            \"titles\": [\n                \"Maximizing CNN Accelerator Efficiency Through Resource Partitioning\",\n                \"Mandarin speech recognition using convolution neural network with augmented tone features\",\n                \"A convolutional neural network approach for face verification\",\n                \"Multi-view Face Detection Using Deep Convolutional Neural Networks\",\n                \"CNN Based Vehicle Counting with Virtual Coil in Traffic Surveillance Video\",\n                \"A pedestrian and vehicle rapid identification model based on convolutional neural network\",\n                \"Malaysia traffic sign recognition with convolutional neural network\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review\",\n                \"Computer vision algorithms and hardware implementations: A survey\",\n                \"A Survey on Efficient Convolutional Neural Networks and Hardware Acceleration\",\n                \"Minimizing computation in convolutional neural networks\",\n                \"Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n                \"The Smart Image Recognition Mechanism for Crop Harvesting System in Intelligent Agriculture\",\n                \"IoT-based agriculture: Deep learning in detecting apple fruit diseases\",\n                \"A high performance FPGA-based accelerator for large-scale convolutional neural networks\",\n                \"Optimizing FPGA-based accelerator design for deep convolutional neural networks\",\n                \"Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks\",\n                \"FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks\",\n                \"Meta-heuristics: an overview\",\n                \"Simulated annealing algorithm for deep learning\",\n                \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Going deeper with convolutions\",\n                \"FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation\",\n                \"FPGA-Based Convolutional Neural Network Accelerator with Resource-Optimized Approximate Multiply - Accumulate Unit\",\n                \"Polyhedral-based data reuse optimization for configurable computing\",\n                \"Roofline: an insightful visual performance model for multicore architectures\",\n                \"Vivado Design Suite Product Guide: Floating-Point Operator v7.1 [Online]\",\n                \"User Guide: 7 Series FPGAs Memory Resources [Online]\",\n                \"Iterative Computer Algorithms with Applications in Engineering: Solving Combinatorial Optimization Problems\",\n                \"VLSI physical design automation: theory and practice\",\n                \"Optimization by simulated annealing\",\n                \"Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm\",\n                \"Equation of state calculations by fast computing machines\",\n                \"Evolutionary algorithms, simulated annealing and tabu search: a comparative study\",\n                \"Tabu search—part I\",\n                \"Tabu search—part II\",\n                \"A user’s guide to tabu search\",\n                \"Tabu search\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]17 based on the context and research area of Optimization of FPGA-based CNN Accelerators Using Metaheuristics. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}