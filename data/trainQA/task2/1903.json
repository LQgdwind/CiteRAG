{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: HYPER2: Hyperbolic PoincarÃ© Embedding for Hyper-Relational Link Prediction\n\nPaper Abstract: \n\nPaper Section Text:\nThe emerging of large-scale Knowledge Graphs(KGs), such as Freebase, Wikidata and Googleâ€™s Knowledge Graph power a plethora of downstream tasks ranging from recommendation systems to semantic search and from question answering to natural language understanding[ref]7. KGs are generally represented as a set of triplets in the form of (head entity, relation, tail entity) where a relation links an entity pair. Despite containing rich information, KGs are still in face of incompleteness issue. To our knowledge, 71% of all people in Freebase have no pâ€‹lâ€‹aâ€‹câ€‹eâ€‹_â€‹oâ€‹fâ€‹_â€‹bâ€‹iâ€‹râ€‹tâ€‹hğ‘ğ‘™ğ‘ğ‘ğ‘’_ğ‘œğ‘“_ğ‘ğ‘–ğ‘Ÿğ‘¡â„place\\_of\\_birth attribute. Link prediction task, seeking to predict new links between entities in the graph based on existing ones, tackles this issue. A variety of representitive link prediction techniques such as RESCAL0, TransE1 and ConvE2 have been devised to work over triplet based KGs. And in the past decade, we have witnessed gratifying progress in this realm. Nevertheless, when lots of efforts are poured into binary facts(i.e. triplets), we pay less attention to the ubiquitous n-ary facts involving more than two entities. For instance, award_nomination relation often involves an awarding institution, a receiver, an award and a work. And several performers can also co-occur in a performance.\nAs a matter of fact, in Freebase, more than 1/3 entities participate in n-ary facts3. These higher-arity facts with more knowledge are much closer to natural language, link prediction in hyper-relational KGs provides an excellent potential for lots of downstream NLP applications4. Existing triple based models decompose n-ary facts into a set of triplets, irreversibly resulting in internal structural and semantic loss5. To deal with the more challenging n-ary facts without breaking them into triplets, m-TransH3, a translational distance model based on TransH is first proposed. Along with the model, the authors put forward the first hyper-relational KG JF17K, which is derived from Freebase with frequent presence of n-ary facts. The work is ground breaking, but it combines relations and entities in sequence so as to get a so-called meta-relation. The strict position restriction would bring information loss, for in some cases, two or more entities changing position doesnâ€™t necessarily lead to semantic deviation. Take the sentence Andrea Bocelli sang Time To Say Goodbye and Canto Della Terra with Sarah Brightman in the concert Vivere: Live In Tuscany as an example, exchanging position of Time To Say Goodbye and Canto Della Terra expresses exactly the same semantic meaning. RAE6 further takes multi-layer perceptron into m-TransH, the incremental work brings constant progress. But both of the two tranlational models are limited by the translation nature and suffer from weak expressiveness,\nleaving much room for improvement. Besides, these two models cannot predict relations. Deep models recently gain popularity in n-ary link prediction just as they prevail in many other tasks. NaLP and a new dataset WikiPeople, extracted from Wikidata mainly focusing on people-related facts, are proposed in 7. NaLP outperforms m-TransH and RAE on both dataset, but as shown in Figure 2(c), it breaks an n-ary fact into nğ‘›n key-value pairs and needs to compute relatedness Cn2superscriptsubscriptğ¶ğ‘›2C_{n}^{2} times for a single fact. Authors of HINGE5 advocate that primary triple keeps the main sturcture and semantic information of an n-ary fact.\nExperiments shows that Hinge greatly outperforms NaLP, validating the significance of primary triple. But as dipicted in Figure 2(b), HINGE views an n-ary fact as the combiantion of a primary triple and nâˆ’2ğ‘›2n-2 3-ary tuples, it captures triple and quintuple features with convolution network. It has been proved in 8 that some n-ary facts are in possesseion of indecomposibility. Take the mâ€‹eâ€‹dâ€‹iâ€‹câ€‹aâ€‹lâ€‹_â€‹râ€‹eâ€‹câ€‹oâ€‹râ€‹dğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘ğ‘™_ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘medical\\_record relation as an example, A dâ€‹oâ€‹câ€‹tâ€‹oâ€‹rğ‘‘ğ‘œğ‘ğ‘¡ğ‘œğ‘Ÿdoctor will record the râ€‹eâ€‹aâ€‹câ€‹tâ€‹iâ€‹oâ€‹nğ‘Ÿğ‘’ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›reaction of a pâ€‹aâ€‹tâ€‹iâ€‹eâ€‹nâ€‹tğ‘ğ‘ğ‘¡ğ‘–ğ‘’ğ‘›ğ‘¡patient after he takes mâ€‹eâ€‹dâ€‹iâ€‹câ€‹iâ€‹nâ€‹eğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘–ğ‘›ğ‘’medicine. Râ€‹eâ€‹aâ€‹câ€‹tâ€‹iâ€‹oâ€‹nğ‘…ğ‘’ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Reaction of a pâ€‹aâ€‹tâ€‹iâ€‹eâ€‹nâ€‹tğ‘ğ‘ğ‘¡ğ‘–ğ‘’ğ‘›ğ‘¡patient is tightly correlated to what mâ€‹eâ€‹dâ€‹iâ€‹câ€‹iâ€‹nâ€‹eğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘–ğ‘›ğ‘’medicine he takes, itâ€™s not reasonable to seperate râ€‹eâ€‹aâ€‹câ€‹tâ€‹iâ€‹oâ€‹nğ‘Ÿğ‘’ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›reaction from mâ€‹eâ€‹dâ€‹iâ€‹câ€‹iâ€‹nâ€‹eğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘–ğ‘›ğ‘’medicine. Despite the remarkable performance improvement, n-ary facts in these deep models still suffer from non-integrity issue. Moreover, deep models always sustain relatively high computational complexity. HSimplE9 and HyPE9 are convolutional models, but they can only deal with 2-ary to 6-ary data. The tensor decomposition based model GTED0 also sustains inflexibility issue, itâ€™s only applicable to 3-ary and 4-ary facts. Moreover, the three models donâ€™t explicitly distinguish entities within and beyond triple, neglecting the importance of the primary triple. In this work, we represent an n-ary fact as a whole, keeping the integrity of the fact and maintain the vital role that the primary triple plays in an n-ary fact. Recently, advance in binary link prediction task illustrates that hyperbolic PoincarÃ© model shows superiority and lower complexity over its counterparts1. We further learn that PoincarÃ© ball excels in representing knowledge with hierarchy for the exponential growth of distance on the boundary of the hyperbolic space. As shown in Figure 1, containing rich geometric structures, a PoincarÃ© ball is endowed with strong expressiveness in representing complex graph-structured data. Itâ€™s also notable that hyperbolic PoincarÃ© embedding method has an edge in complexity in that it involves much less\nmatrix addition and multiplication operations compared to existing deep models. To our konwledge, it has not been studied in hyper-relational KG. Considering the expressiveness and low complexity, we deem that hyperbolic PoincarÃ© embedding method, which to the best of our knowledge has not been studied in hyper-relational KGs yet, is promising for a both efficient and effective model. In this work, we propose a generalized hyperbolic PoincarÃ© embedding framework that is appilicable to arbitary arity facts, termed as HYPER2.\nIn HYPER2, all entities are first randomly initialized on the hyperbolic space, then through vector projection between hyperbolic space and tangent space and aggregation of information on the tangent space, HYPER2 captures the interaction between the primary triple and the affiliated entities. HYPER2 achieves SOTA on two representative hyper-relational datasets, improving HITS@1, HITS@10, MRR by 34.5%, 21.9%, 29.1% on WikiPeople, and 23.1%, 14.4%, 18.9% on JF17K respectively, which validates the effectiveness and superiority of our model. Besides, HYPER2 is less computationally complicated, 49-61 times quicker in comparison with its analogues. Moreover,\nour experiments verify literals (numeric values, date-time instances or other string) impede embedding learning. Our contribution can be summarized as follows: HYPER2 employs hyper-relation to totally shrink from tearing an n-ary facts into smaller tuples, simultaneously considering the vital role that the primary triple plays in an n-ary fact and the indecomposibility of some n-ary facts. To the best of our knowledge, our proposed HYPER2 is the first model that employs hyperbolic PoincarÃ© ball for hyper-relational KGs, HYPER2 generalize hyperbolic PoincarÃ© method from binary to arbitary arity facts, illustrating high flexibility. Extensive experiments on two representative hyper-relational KGs demonstrate that HYPER2 realizes superior performance to its counterparts, improving SOTA by a surprisingly large margin, demonstrating absolute superiority. We validate the side effect brought by literals in Wikipeople dataset. And we theoretically and experimentally compare the parameter size and the computational complexity of HYPER2 against several previous best-performing baselines. Experiments show our model greatly overwhelms its counterparts, unveiling low complexity.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: Wikidata and Googleâ€™s Knowledge Graph power a plethora of downstream tasks ranging from recommendation systems to semantic search and from question answering to natural language understanding[ref]7. KGs are generally represented as a set of triplets in the form of (head entity, relation, tail entity) where a relation links an entity pair. Despite containing rich information, KGs are\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"A Review of Relational Machine Learning for Knowledge Graphs\",\n                \"Freebase: A collaboratively created graph database for structuring human knowledge\",\n                \"Wikidata\",\n                \"Google\",\n                \"Collaborative knowledge base embedding for recommender systems\",\n                \"Explicit semantic ranking for academic search via knowledge graph embedding\",\n                \"Semantic parsing via staged query graph generation: Question answering with knowledge base\",\n                \"Ernie 2.0: A continual pre-training framework for language understanding\",\n                \"Knowledge graph embedding: A survey of approaches and applications\",\n                \"A three-way model for collective learning on multi-relational data\",\n                \"Translating embeddings for modeling multi-relational data\",\n                \"Convolutional 2D Knowledge Graph Embeddings\",\n                \"On the representation and embedding of knowledge bases beyond binary relations\",\n                \"Highlife: Higher-arity fact harvesting\",\n                \"Beyond triplets: Hyper-relational knowledge graph embedding for link prediction\",\n                \"Scalable instance reconstruction in knowledge bases via relatedness affiliated embedding\",\n                \"Link prediction on n-ary relational data\",\n                \"Hyper-Path-Based Representation Learning for Hyper-Networks\",\n                \"Knowledge hypergraphs: Prediction beyond binary relations\",\n                \"Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases\",\n                \"Multi-relational PoincarÃ© Graph Embeddings\",\n                \"PoincarÃ© Embeddings for Learning Hierarchical Representations\",\n                \"Learning continuous hierarchies in the lorentz model of hyperbolic geometry\",\n                \"Hyperbolic Neural Networks\",\n                \"Hyperbolic attention networks\",\n                \"Hyperbolic graph convolutional neural networks\",\n                \"Learning mixed-curvature representations in products of model spaces\",\n                \"PoincarÃ© GloVe: Hyperbolic Word Embeddings\",\n                \"Low-Dimensional Hyperbolic Knowledge Graph Embeddings\",\n                \"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space\",\n                \"Knowledge graph embedding by translating on hyperplanes\",\n                \"Learning entity type embeddings for knowledge graph completion\",\n                \"Transms: Knowledge graph embedding for complex relations by multidirectional semantics\",\n                \"A novel embedding model for knowledge base completion based on convolutional neural network\",\n                \"Learning to exploit long-term relational dependencies in knowledge graphs\",\n                \"Modeling relational data with graph convolutional networks\",\n                \"Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\",\n                \"A Survey on Knowledge Graphs: Representation, Acquisition and Applications\",\n                \"Embedding entities and relations for learning and inference in knowledge bases\",\n                \"Complex Embeddings for Simple Link Prediction\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of HYPER2: Hyperbolic PoincarÃ© Embedding for Hyper-Relational Link Prediction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}