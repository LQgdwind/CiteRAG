{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Deep Attributed Network Representation Learning via Attribute Enhanced Neighborhood\n\nPaper Abstract: \n\nPaper Section Text:\n1) A Joint Optimization Framework of the DANRL-ANE Model The entire model consists of three coupled modules, which share the same encoder component. The encoder aims at mapping the input data into the representation space by one or multiple layers of non-linear functions. In our model, the input-output relationship of each layer of the encoder is defined as\n where Risubscript𝑅𝑖R_{i} is the i𝑖i-th row of the input matrix R𝑅R, and records the reconstructed neighbor relationship of node visubscript𝑣𝑖v_{i}. The symbol δ(.)\\delta(.) denotes the non-linear activation functions, and we choose the best suitable one based on their performance on different tasks and datasets. The model parameters W(k)superscript𝑊𝑘W^{(k)} and b(k)superscript𝑏𝑘b^{(k)} represent the transformation matrix and bias vector in the k𝑘k-th layer, respectively, and K𝐾K is the layer number of the encoder. The joint optimization objective of the DANRL-ANE model with where LF​o​Psubscript𝐿𝐹𝑜𝑃L_{FoP}, La​eMsuperscriptsubscript𝐿𝑎𝑒𝑀L_{ae}^{M}, Ls​gN​Ssuperscriptsubscript𝐿𝑠𝑔𝑁𝑆L_{sg}^{NS} are the loss function of the self-built first-order proximity preserved module, the attribute enhanced neighborhood autoencoder module and the community-aware skip-gram module, respectively, as well as α𝛼\\alpha and β𝛽\\beta are the hyperparameters that balance the effect of the modules. The hyperparameter γ𝛾\\gamma is the coefficient of the l2subscript𝑙2l_{2} norm regularizer Lr​e​gsubscript𝐿𝑟𝑒𝑔L_{reg} that is used to prevent overfitting. The number of layers in the encoder and decoder is K𝐾K. The matrix W(k)superscript𝑊𝑘W^{(k)} and W^(k)superscript^𝑊𝑘\\hat{W}^{(k)} represents the weight matrix of the encoder and decoder in the k𝑘k-th layer, respectively. We optimize Equation (4) by the stochastic gradient algorithm adopted by ANRL [ref]18. Next, we give the detailed description of each module. 2) Self-built First-order Proximity Preserved Module\nThe first-order proximity can reveal the similarity between nodes intuitively and simply. We here propose a self-built first-order proximity preserved module. To model the first-order proximity, inspired by the LINE  and DANE [ref]15 model, we define the joint probability wi​j​(vi,vj)subscript𝑤𝑖𝑗subscript𝑣𝑖subscript𝑣𝑗w_{ij}(v_{i},v_{j}) between nodes visubscript𝑣𝑖v_{i} and vjsubscript𝑣𝑗v_{j} with the sigmoid function σ​(x)=11+exp⁡(−x)𝜎𝑥11𝑥\\sigma(x)=\\frac{1}{1+\\exp(-x)}. Let x=yi(K)​yj(K)T𝑥superscriptsubscript𝑦𝑖𝐾superscriptsubscript𝑦𝑗superscript𝐾Tx=y_{i}^{(K)}y_{j}^{(K)^{\\mathrm{T}}}, then we obtain where yi(K)superscriptsubscript𝑦𝑖𝐾y_{i}^{(K)} and yj(K)superscriptsubscript𝑦𝑗𝐾y_{j}^{(K)} denote the representation of nodes visubscript𝑣𝑖v_{i} and vjsubscript𝑣𝑗v_{j}, respectively. Essentially, the first-order proximity on undirected and unweighted networks describes the existence or non-existence of edge between node pairs, which is equivalent to a binomial classification problem. The sigmoid cross-entropy loss function is a typical objective function for a binomial classification, which is defined as\n where t𝑡t represents the label of a sample, and can be either 111 or 00. If the sample belongs to the positive class, t=1𝑡1t=1, else t=0𝑡0t=0. The probability ps​(s)subscript𝑝𝑠𝑠p_{s}(s) indicates the possibility that the sample is predicted to belong to the positive class, and is calculated by the sigmoid function. Here, t=ai​j𝑡subscript𝑎𝑖𝑗t=a_{ij} and ps​(s)=wi​j​(vi,vj)subscript𝑝𝑠𝑠subscript𝑤𝑖𝑗subscript𝑣𝑖subscript𝑣𝑗p_{s}(s)=w_{ij}(v_{i},v_{j}), equivalently. Because only the information about the existing edges is useful for the network representation, we set t=1𝑡1t=1, and take the first term of Equation (7) into consideration. We average all the losses when the edges exist, and thus obtain the objective of the self-built first-order proximity preserved module where ai​jsubscript𝑎𝑖𝑗a_{ij} represents the element of the i𝑖i-th row and the j𝑗j-th column of the adjacency matrix A𝐴A. 3) Attribute Enhanced Neighborhood Autoencoder Module The deep autoencoder model is widely used to mine the proximity between the neighborhood structure of node pairs , since it could smoothly capture the data manifolds, and preserve the similarity between samples. The autoencoder consists of two parts, i.e. the encoder and decoder. The decoder is the inverse calculation process of the encoder. The representation in the representation space is mapped into the reconstruction space in the decoder process. The purpose of the autoencoder is to minimize the reconstruction error between the input data and the reconstructed data, so that the abstract representation of the mid-layer output can capture the manifold structure in the input data. To be specific, the objective of the autoencoder is where n𝑛n is the number of the nodes in the networks, and Ri^^subscript𝑅𝑖\\hat{R_{i}} represents the reconstructed output of the input data Risubscript𝑅𝑖R_{i}. Although the direct neighborhood of each node gets enhanced after integrating the attribute semantics and adjacency matrix, the reconstructed adjacency matrix R𝑅R could still be a sparse matrix, that is, the number of non-zero elements is far less than that of zero elements. A sparse input matrix could further make the autoencoder tend to preserve more zero elements. Since the non-zero elements record the connections between nodes, it is more essential to preserve non-zero elements as much as possible instead of zero elements. Inspired by SDNE , we employ the Hadamard product as the penalty factor, and extend the loss function of the autoencoder. The modified objective is\n where ⊙direct-product\\odot indicates the Hadamard product, bi={bi,j}j=1nsubscript𝑏𝑖superscriptsubscriptsubscript𝑏𝑖𝑗𝑗1𝑛b_{i}=\\{b_{i,j}\\}_{j=1}^{n}. If Ri,j=0subscript𝑅𝑖𝑗0R_{i,j}=0, bi,j=1subscript𝑏𝑖𝑗1b_{i,j}=1, else bi,j=χ>1subscript𝑏𝑖𝑗𝜒1b_{i,j}=\\chi>1. The attribute enhanced neighborhood autoencoder module takes each row vector of the reconstructed adjacency matrix R𝑅R as the sample input, where the row vector denotes the neighbor structural information with the attribute semantics of the corresponding node. In other words, the autoencoder could preserve the second-order proximity. 4) Community-aware skip-gram module Inspired by DeepWalk , we design the community-aware skip-gram module for capturing the high-order proximity in this work. To reduce the time complexity, we adopt the node sequences sampling procedure performed by node2vec[ref]3, where the return parameter pn=1.0subscript𝑝𝑛1.0p_{n}=1.0 and in-out parameter qn=1.0subscript𝑞𝑛1.0q_{n}=1.0, and use the negative sampling to approximate the following loss function where n𝑛n is the number of nodes in the network, c∈C𝑐𝐶c\\in C denotes the node sequences sampled by the random walk, b𝑏b is the size of the window. The input data Risubscript𝑅𝑖R_{i} occupies the i𝑖i-th row of the input matrix R𝑅R. The node vi+jsubscript𝑣𝑖𝑗v_{i+j} is the context node of the current node visubscript𝑣𝑖v_{i} located in the generated random sequences in the window b𝑏b. The node representation yi(K)superscriptsubscript𝑦𝑖𝐾y_{i}^{(K)} is the output of the sample input Risubscript𝑅𝑖R_{i} through the K𝐾K layer encoder. The matrix H′superscript𝐻′H^{\\prime} is the transition matrix between the representation output layer of the autoencoder and the output layer of the skip-gram, and hi′superscriptsubscriptℎ𝑖′h_{i}^{\\prime} is in Row i𝑖i of the transition matrix H′superscript𝐻′H^{\\prime}. Then, we obtain the following Equation (12) where σ​(x)=11+exp⁡(−x)𝜎𝑥11𝑥\\sigma(x)=\\frac{1}{1+\\exp(-x)} is the sigmoid function, |n​e​g|𝑛𝑒𝑔|neg| denotes the number of the sampled negative samples. The sampling distribution Pn​(v)∝dv3/4proportional-tosubscript𝑃𝑛𝑣superscriptsubscript𝑑𝑣34P_{n}(v)\\propto d_{v}^{3/4} is set as suggested in  and dvsubscript𝑑𝑣d_{v} represents the degree of node vnsubscript𝑣𝑛v_{n}. Minimizing Equation (12), we can get the result that if the two nodes co-occur, they have similar embedding vectors. Algorithm 1 describes the learning process of the entire model and all model parameters are denoted as ΘΘ\\Theta.\n\nReference markers found: [18]\n\nContext windows for each [ref]:\n[ref]18: represents the weight matrix of the encoder and decoder in the k𝑘k-th layer, respectively. We optimize Equation (4) by the stochastic gradient algorithm adopted by ANRL [ref]18. Next, we give the detailed description of each module. 2) Self-built First-order Proximity Preserved Module\nThe first-order proximity can reveal the similarity between nodes intuitively and\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 18,\n            \"titles\": [\n                \"Network Representation Learning: A Survey\",\n                \"Unifying Virtual and Physical World: Learning Toward Local and Global Consistency\",\n                \"Anomaly Detection: A Survey\",\n                \"node2vec: Scalable Feature Learning for Networks\",\n                \"DeepWalk: Online Learning of Social Representations\",\n                \"Collective Classification in Network Data Articles\",\n                \"Label-dependent Node Classification in The Network\",\n                \"The Link-Prediction Problem for Social Networks\",\n                \"On the Relation between Low Density Separation, Spectral Clustering and Graph Cuts\",\n                \"Visualizing Data using GTSNE\",\n                \"Graph Embedding and Extensions: A General Framework for Dimensionality Reduction\",\n                \"Structural Deep Network Embedding\",\n                \"Network Studies of Social Influence\",\n                \"Birds of A Feather: Homophily in Social Networks\",\n                \"Homogeneity in Confiding Relations\",\n                \"Deep Attributed Network Embedding\",\n                \"Attributed Social Network Embedding\",\n                \"Multimodal Deep Network Embedding with Integrated Structure and Attribute Information\",\n                \"ANRL: Attributed Network Representation Learning via Deep Neural Networks\",\n                \"GraRep: Learning Graph Representations with Global Structural Information\",\n                \"Graph Embedding Techniques, Applications, and Performance: A Survey\",\n                \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\",\n                \"A Global Geometric Framework for Nonlinear Dimensionality Reduction\",\n                \"BiasedWalk: Biased Sampling for Representation Learning on Graphs\",\n                \"LINE: Large-scale Information Network Embedding\",\n                \"GAT2VEC: Representation Learning for Attributed Graphs\",\n                \"A Scalable Attribute-aware Network Embedding System\",\n                \"Impact of Similarity Measures on Web-page Clustering\",\n                \"Semantic Hashing\",\n                \"Efficient Estimation of Word Representations in Vector Space\",\n                \"Label Informed Attributed Network Embedding\",\n                \"Representation Learning on Graphs: Methods and Applications\",\n                \"An Introduction to ROC Analysis\",\n                \"Generated Reference Title 33 for [ref]18\",\n                \"Generated Reference Title 34 for [ref]18\",\n                \"Generated Reference Title 35 for [ref]18\",\n                \"Generated Reference Title 36 for [ref]18\",\n                \"Generated Reference Title 37 for [ref]18\",\n                \"Generated Reference Title 38 for [ref]18\",\n                \"Generated Reference Title 39 for [ref]18\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]18 based on the context and research area of Deep Attributed Network Representation Learning via Attribute Enhanced Neighborhood. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}