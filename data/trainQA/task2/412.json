{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework\n\nPaper Abstract: \n\nPaper Section Text:\nVirtual memory is a core component of modern computing systems   . Virtual memory was originally designed for systems whose memory hierarchy fit a simple two-level model of small-but-fast main memory that can be directly accessed via CPU instructions and large-but-slow external storage accessed with the help of the operating system (OS). In such a configuration, the OS can easily abstract away the underlying memory architecture details and present applications with a unified view of memory. However, continuing to efficiently support the conventional virtual memory framework requires significant effort due to (1) emerging memory technologies (e.g., DRAM–NVM hybrid memories), (2) diverse system architectures, and (3) diverse memory requirements of modern applications.\nThe OS must now efficiently meet the wide range of application memory requirements that leverage the advantages offered by emerging memory architectures and new system designs while simultaneously hiding the complexity of the underlying memory and system architecture from the applications. Unfortunately, this is a difficult problem to tackle in a generalized manner. We describe three examples of challenges that arise when adapting conventional virtual memory frameworks to today’s diverse system configurations. Virtualized Environments.\nIn a virtual machine, the guest OS performs virtual memory management on the emulated “physical memory” while the host OS performs a second round of memory management to map the emulated physical memory to the actual physical memory. This extra level of indirection results in three problems:\n(1) two-dimensional page walks      , where the number of memory accesses required to serve a TLB miss increases dramatically\n(e.g., up to 24 accesses in x86-64 with 4-level page tables);\n(2) performance loss in case of miscoordination between the guest and host OS mapping and allocation mechanisms (e.g., when the guest supports superpages, but the host does not); and\n(3) inefficiency in virtualizing increasingly complex physical memory architectures (e.g., hybrid memory systems) for the guest OS. These problems worsen with more page table levels , and in systems that support nested virtualization (i.e., a virtual machine running inside another)  . Address Translation.\nIn existing virtual memory frameworks, the OS manages virtual-to-physical address mapping. However, the hardware must be able to traverse these mappings to handle memory access operations (e.g., TLB lookups). This arrangement requires using rigid address-translation structures that are shared between and understood by both the hardware and the OS. Prior works show that many applications can benefit from flexible page tables, which cater to the application’s actual memory footprint and access patterns    . Unfortunately, enabling such flexibility in conventional virtual memory frameworks requires more complex address translation structures every time a new address translation approach is proposed. For example, a recent work  proposes using direct segments to accelerate big-memory applications. However, in order to support direct segments, the virtual memory contract needs to change to enable the OS to specify which regions of memory are directly mapped to physical memory.\nDespite the potential performance benefits, this approach is not easily scalable to today’s increasingly diverse system architectures. Memory Heterogeneity.\nPrior works propose many performance-enhancing techniques that require (1) dynamically mapping data to different physical memory regions according to application requirements (e.g., mapping frequently-accessed data to fast memory), and (2) migrating data when those requirements change (e.g.,                 ). Efficiently implementing such functionality faces two challenges. First, a customized data mapping requires the OS to be aware of microarchitectural properties of the underlying memory. Second, even if this can be achieved, the OS has low visibility into rich fine-grained runtime memory behavior information (e.g., access pattern, memory bandwidth availability), especially at the main memory level. While hardware has access to such fine-grained information, informing the OS frequently enough such that it can react to changes in the memory behavior of an application in a timely manner is challenging     . A wide body of research (e.g.,   1   8         7 7      6 7    0 1   02 0 2 6 33  5  0 39 7  27  05 5  1 [ref]2 23 7) proposes mechanisms to alleviate the overheads of conventional memory allocation and address translation by exploiting specific trends observed in modern systems (e.g., the behavior of emerging applications). Despite notable improvements, these solutions have two major shortcomings. First, these solutions mainly exploit specific system or workload characteristics and, thus, are applicable to a limited set of problems or applications. Second, each solution requires specialized and not necessarily compatible changes to both the OS and hardware. Therefore, implementing all of these proposals at the same time in a system is a daunting prospect. Our goal in this work is to design a general-purpose alternative virtual memory framework that naturally supports and better extracts performance from a wide variety of new system configurations, while still providing the key features of conventional virtual memory frameworks. To this end, we propose the Virtual Block Interface (VBI), an alternative approach to memory virtualization that is inspired by the logical block abstraction used by solid-state drives to hide the underlying device details from the rest of the system. In a similar way, we envision the memory controller as the primary provider of an abstract interface that hides the details of the underlying physical memory architecture, including the physical addresses of the memory locations. VBI is based on three guiding principles. First, programs should be allowed to choose the size of their virtual address space, to mitigate translation overheads associated with very large virtual address spaces. Second, address translation should be decoupled from memory protection, since they are logically separate and need not be managed at the same granularity by the same structures. Third, software should be allowed to communicate semantic information about application data to the hardware, so that the hardware can more intelligently manage the underlying hardware resources. VBI introduces a globally-visible address space called the VBI Address Space, that consists of a large set of virtual blocks (VBs) of different sizes. For any semantically meaningful\nunit of information (e.g., a data structure, a shared library), the program can choose a VB of appropriate size, and tag the VB with properties that describe the contents of the VB. The key idea of VBI is to delegate physical memory allocation and address translation to a hardware-based Memory Translation Layer (MTL) at the memory controller. This idea is enabled by the fact that the globally-visible VBI address space provides VBI with system-wide unique VBI addresses that can be directly used by on-chip caches without requiring address translation. In VBI, the OS no longer needs to manage address translation and memory allocation for the physical memory devices. Instead, the OS (1) retains full control over access protection by controlling which programs have access to which virtual blocks, and (2) uses VB properties to communicate the data’s memory requirements (e.g., latency sensitivity) and characteristics (e.g., access pattern) to the memory controller. Figure 1 illustrates the differences between virtual memory management in state-of-the-art production Intel x86-64 systems  and in VBI. In x86-64 (Figure 1a), the OS manages a single private virtual address space (VAS) for each process ( 1), providing each process with a fixed-size 256 TB VAS irrespective of the actual memory requirements of the process ( 2). The OS uses a set of page tables, one per process, to define how each VAS maps to physical memory ( 3). In contrast, VBI (Figure 1b) makes all virtual blocks (VBs) visible to all processes, and the OS controls which processes can access which VBs ( 1). Therefore, a process’ total virtual address space is defined by which VBs are attached to it, i.e., by the process’ actual memory needs ( 2). In VBI, the MTL has full control over mapping of data from each VB to physical memory, invisibly to the system software ( 3). VBI seamlessly and efficiently supports important optimizations that improve overall system performance, including: (1) enabling benefits akin to using virtually-indexed virtually-tagged (VIVT) caches (e.g., reduced address translation overhead), (2) eliminating two-dimensional page table walks in virtual machine environments, (3) delaying physical memory allocation until the first dirty last-level cache line eviction, and (4) flexibly supporting different virtual-to-physical address translation structures for different memory regions. §3.5 describes these optimizations in detail. We evaluate VBI for two important and emerging use-cases. First, we demonstrate that VBI significantly reduces the address translation overhead both for natively-running programs and for programs running inside a virtual machine (VM programs). Quantitative evaluations using workloads from SPEC CPU 2006 25, SPEC CPU 2017 26, TailBench 8, and Graph 500 4 show that a simplified version of VBI that maps VBs using 4 KB granularity only improves the performance of native programs by 2.18×\\times and VM programs by 3.8×\\times. Even when enabling support for large pages for all data, which significantly lowers translation overheads, VBI improves performance by 77% for native programs and 89% for VM programs. Second, we demonstrate that VBI significantly improves the performance of heterogeneous memory architectures by evaluating two heterogeneous memory systems (PCM–DRAM  and Tiered-Latency-DRAM ). We show that VBI, by intelligently mapping frequently-accessed data to the low-latency region of memory, improves overall performance of these two systems by 33% and 21% respectively, compared to systems that employ a heterogeneity-unaware data mapping scheme. §7 describes our methodology, results, and insights from these evaluations. We make the following key contributions: To our knowledge, this is the first work to propose a virtual memory framework that relieves the OS of explicit physical memory management and delegates this duty to the hardware, i.e., the memory controller. We propose VBI, a new virtual memory framework that efficiently enables memory-controller-based memory management by exposing a purely virtual memory interface to applications, the OS, and the hardware caches. VBI naturally and seamlessly supports several optimizations (e.g., low-cost page walks in virtual machines, purely virtual caches, delayed physical memory allocation), and integrates well with a wide range of system designs. We provide a detailed reference implementation of VBI, including required changes to the user applications, system software, ISA, and hardware. We quantitatively evaluate VBI using two concrete use cases: (1) address translation improvements for native execution and virtual machines, and (2) two different heterogeneous memory architectures. Our evaluations show that VBI significantly improves performance in both use cases.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: to changes in the memory behavior of an application in a timely manner is challenging     . A wide body of research (e.g.,   1   8         7 7      6 7    0 1   02 0 2 6 33  5  0 39 7  27  05 5  1 [ref]2 23 7) proposes mechanisms to alleviate the overheads of conventional memory allocation and address translation by exploiting specific trends observed in modern systems (e.g., the behavior of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines\",\n                \"“Separating Translation from Protection in Address Spaces with Dynamic Remapping\",\n                \"Separating Translation from Protection in Address Spaces with Dynamic Remapping\",\n                \"Virtual Memory System\",\n                \"Do-It-Yourself Virtual Memory Translation\",\n                \"Arm® Architecture Reference Manual: ARMv8, for ARMv8-A Architecture Profile\",\n                \"Pinnacle: IBM MXT in a Memory Controller Chip\",\n                \"Mosaic: A GPU Memory Manager with Application-Transparent Support for Multiple Page Sizes\",\n                \"MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency\",\n                \"Translation Caching: Skip, Don’t Walk (the Page Table)\",\n                \"SpecTLB: A Mechanism for Speculative Address Translation\",\n                \"Efficient Virtual Memory for Big Memory Servers\",\n                \"The Multikernel: A New OS Architecture for Scalable Multicore Systems\",\n                \"Scalable Distributed Shared Last-Level TLBs Using Low-Latency Interconnects\",\n                \"Accelerating Two-Dimensional Page Walks for Virtualized Systems\",\n                \"Large-Reach Memory Management Unit Caches\",\n                \"Shared Last-Level TLBs for Chip Multiprocessors\",\n                \"Characterizing the TLB Behavior of Emerging Parallel Workloads on Chip Multiprocessors\",\n                \"Virtual-Address Caches Part 1: Problems and Solutions in Uniprocessors\",\n                \"Virtual-Address Caches Part 2: Multiprocessor Issues\",\n                \"A High-Performance Memory Allocator for Object-Oriented Systems\",\n                \"Architectural Support for Dynamic Memory Management\",\n                \"Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization\",\n                \"Low-Cost Inter-Linked Subarrays (LISA): Enabling Fast Inter-Subarray Data Movement in DRAM\",\n                \"Sharing and Protection in a Single-Address-Space Operating System\",\n                \"Lightweight Shared Objects in a 64-bit Operating System\",\n                \"Efficient Address Translation for Architectures with Multiple Page Sizes\",\n                \"Dynamo: Amazon’s Highly Available Key-Value Store\",\n                \"Virtual Memory\",\n                \"PDRAM: A Hybrid PRAM and DRAM Main Memory System\",\n                \"Supporting Superpages in Non-Contiguous Physical Memory\",\n                \"SpaceJMP: Programming with Multiple Virtual Address Spaces\",\n                \"AVM: Application-Level Virtual Memory\",\n                \"Exokernel: An Operating System Architecture for Application-Level Resource Management\",\n                \"Exploiting Parallelization on Address Translation: Shared Page Walk Cache\",\n                \"RocksDB: A Persistent Key–Value Store\",\n                \"Nested Virtualization in Azure\",\n                \"Distributed Caching with Memcached\",\n                \"Dynamic Storage Allocation in the Atlas Computer, Including an Automatic Use of a Backing Store\",\n                \"Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}