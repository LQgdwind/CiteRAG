{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: AKF-SR: Adaptive Kalman Filtering-based Successor Representation\n\nPaper Abstract: \n\nPaper Section Text:\nHuman and animals have the learning capabilities for evaluating consequences of their actions; therefore, can adapt their behavior based on the received reward after each action [ref]1  [ref]3.\nSuch an adaptive behavior can be achieved in the context of Artificial Intelligence (AI) via an optimal control policy, which can be obtained by Reinforcement Learning (RL) algorithms. Generally speaking, RL is a class of Machine Learning (ML) algorithms enabling an autonomous agent to learn a task to maximize expected future discounted rewards while interacting with its environment   [ref]6. Unlike supervised learning, RL approaches do not need labeled data, which is generally difficult to acquire, for performing their learning processes. Traditionally, RL algorithms are categorized into two main classes: (i) Model-Free (MF) methods    , which learn the value function using sample trajectories, and; (ii) Model-Based (MB) methods    that estimate transition and reward functions through search trees or dynamic programming. Algorithms belonging to the former category (MF algorithms) are particularly slow to adjust an agent to changes in its task (e.g., changes in the goal locations or reward function). The MB algorithms, on the other hand, can quickly adjust the agent to the changes. Such a rapid adaptation, however, comes with a high computational cost  . Reusing previous knowledge to facilitate learning of new tasks has been proposed as an appealing solution to the mentioned adaptation problems of MB and MF algorithms . To improve speed of an agent for learning a new task, which results from changes in its learned tasks, without considerable increase in computational cost, the agent should be able to identify the changes and reuse its knowledge from previous tasks. Reusing the transferred information learned from the previous tasks would not be feasible if the tasks are completely irrelevant. Therefore, in this paper, we focus on the changes in the reward function while the environment remains same. Successor Representation (SR) methods   are proposed as a potential solution to aforementioned adaptation problems of both MF and MB categories of RL algorithms in the case of changes in the reward function. The SR-based algorithms are faster to adapt to changes than MF algorithm, and provide more efficient computation than MB algorithms. The SR-based algorithms learn the expected immediate reward received after each action, together with the expected discounted future state occupancy (referred to as the SR)   . The value function is then factorized into the SR and the immediate reward in each of successor states. This factorization enables rapid policy evaluation under changes in the reward conditions as only the reward function needs to be relearned in new tasks. When the number of states are limited, the SR and the reward function (consequently, the value function) can be computed for each state. However, computation of the value function is not possible in RL problems with large number of states or when states are continuous; therefore, value function needs to be approximated. The function approximators in RL problems can be generally categorized into linear and non-linear function approximators  . In both cases, value of the approximate function is defined by a set of tunable parameters.\nNon-linear function approximators, such as artificial neural network   0 1 algorithms suffer from different issues including sensitivity of the model’s performance to a large number of parameters, lack of theoretical convergence guarantees, and also the need for a large number of episodes to achieve acceptable results. The linear function approximators transform the approximation problem into the problem of computing weights to fuse several local estimators. Since linear function approximators are simpler and better understood than the non-linear ones; therefore, several convergence guarantees have been provided  2 3. In this regard, Cerebellar Model Articulation Controllers (CMACs) 4 and Radial Basis Functions (RBFs) 5 are the commonly used linear estimators. It has been discovered that the RBFs can better represent gradual-continuous transitions in the functions to be approximated 6 7 8 9. Due to this advantage, in this paper, we use RBFs for the SR and reward function linear approximation task. Gershman et al.  have established that the temporal context model, a model of episodic memory, is actually direct estimation of the SR via Temporal Difference (TD) learning algorithms. Kulkarni et al.  proposed a deep learning method based on the combination of TD learning with Deep Neural Networks (DNNs) for estimation of the SR. Some other works     focused on the development of TD learning method by using different DNNs algorithms to represent the SR. CTDL framework proposed in  combines a DNN with a Self-Organising Map (SOM) to calculate action values to improve the performance and robustness of a DNN-based RL agent. Ma et al.  proposed a DNN framework, referred to as Universal Successor Representation (USR), to approximate the SR and incorporate it with actor-critic method to learn the SR. The methods proposed in     , which are based on the combination of the standard TD learning and DNNs, however, do not consider uncertainty of the SR (consequently, uncertainty in the value function), which exists at the heart of RL problems in the real world. When there is uncertainty about the environment, the agent should not be overconfident of its knowledge and exploit it all the time, but instead explore other available actions, which might be better and reduce uncertainty. The optimal solution for the exploration/exploitation trade-off is computationally intractable, but it has been shown that uncertainty can cause exploration through two different ways: by adding randomness to the value function or directing actions toward uncertain ones . The estimated uncertainty of the value function, therefore, is known as a useful information for quandary between exploration and exploitation dilemma  . Lehnert et al.,  proposed a different method from the TD-based SR learning algorithms, where the transition probability of the RL problem is first learned; the SR is then approximated based on the learned transition model. Although the proposed framework is more sample efficient than the TD-based frameworks, the trajectory for finding the transition model is far more complex due to the difficulty in learning the accurate transition model, which makes the TD-based methods more favorable in the literature. Machado et al.  has recently proposed a framework, known as the Substochastic Successor Representation (SSR), which uses the norm of the SR as an exploration bonus. In the proposed scheme, first, the SSR is learned by minimizing the TD error through a DNN. The value function is then learned through a DNN structured similar to DQN framework , which adopts the estimated SSR for exploration/exploitation trade-off. Geist et al.  proposed Kalman Temporal Difference (KTD) framework for uncertainty estimation of the value function, but in the context of MF methods. The proposed KTD framework also benefits from less sensitivity of the framework’s performance to model’s parameters and reduced time and memory requirement for finding and learning of the best model in comparison to DNN-based frameworks     . Less sensitivity to parameter settings improves the reproducibility aspect of a reliable algorithm to regenerate more consistent performances across multiple learning runs; therefore, reduces the risk of generating unpredictable performances in different practical applications . Geerts et al.  and Salimibeni et al. 0 applied KTD framework for the SR estimation in RL problems, respectively, with discrete state spaces and multiple agents. The proposed algorithms, however, do not use the uncertainty information of the estimated SR, which can be achieved from KTD algorithm, for the action selection process. Since value function is computed as a dot product between the SR and the reward function, there is a need for approximation of the reward function. However, Geerts et al.  do not discuss the reward function learning process. In this paper, as an initial step, we adopt the KTD framework for the SR learning process and propose a reward learning algorithm. Then, an innovative action selection scheme benefiting from the achieved uncertainty/belief of the estimated SR in order to deal with exploration/exploitation dilemma. As stated previously, within the SR domain, for computation of the value function, both the SR and reward function need to be learned. So far, we proposed application of KTD within this context for the SR learning. To learn the reward function, on the other hand, we develop a Kalman Filter (KF) to estimate the RBFs’ weights. The challenge here is selection of KF’s parameters. Performance of KF-based algorithms are highly dependent on the filter’s parameters. It has been shown that measurement noise covariances of a KF is one of the most important parameters of the filter and its improper selection can significantly degrade the filter’s act and even cause divergence of the filter 1 2. In the earlier studies, the underlying parameter assumed to be constant during the estimation process with its value being adapted manually by trial and error. Such a bruteforce approach, however, is significantly challenging due to variation of the parameter value. Lately, multiple studies have proposed to adjust the parameter value at each step via Adaptive Kalman Filter (AKF) approaches to enhance the overall accuracy of the filter 3. Generally speaking, AKF approaches can be categorized into two main groups: (i) Innovation-based Adaptive Estimation (IAE) methods 4, and; (ii) Multiple Model Adaptive Estimation (MMAE) methods 5. The former category uses a single Extended Kalman Filter (EKF) to adapt measurement noise covariances based on the innovation or the residual sequence. In the IAE methods, perfect knowledge of the system’s model and an appropriate window size is required for the parameter computation. The MMAE methodologies, on the other hand, use a weighted sum of multiple KFs running in parallel for the parameter adaptation addressing the aforementioned issue. Consequently, the MMAE methods have attracted more attention owing to their capabilities to manage parametric uncertainty and their independence from imposing specific requirements on the system model. Capitalizing on the success of MMAE frameworks, in this paper, we develop an innovative MMAE-based modeling framework for learning of the reward function within the SR context. Different algorithms were proposed for fusion of the KFs in MMAE systems 6, among which the classical scheme is used in this paper due to its exponentially fast convergence speed to the best candidate model (filter). In the classical MMAE scheme, weight of each filter is achieved from a Bayesian approach. Duration of the learning process and quality of the learned policies, which are obtained based on the proposed combination of KTD and MMAE techniques, highly depend on managing a trade-off between exploration and exploitation. Too much exploration prevents from maximizing the immediate rewards since the selected actions may result in a negative reward from the environment. On the other hand, exploiting uncertain knowledge offered by the environment reduces the expected future rewards because actions are selected given current information; therefore, may not be the optimal ones 7. As the value function is modeled as a function of stochastic variables; therefore, there is a stochastic variable for each state-action pair. The dilemma between exploration and exploitation should profit from such uncertain information . In this paper, we propose an active learning scheme, which uses the KTD framework to tackle the uncertainty computation, an important issue which has been overlooked in the literature. In summary, this paper proposes a SR-based framework, referred to as Adaptive Kalman Filtering-based Successor Representations (AKF-SR), which can adapt quickly to changes in the reward function or goal locations faster than MF methods and with the lower computational cost compared to MB algorithms. The following 444 key contributions are made in this paper: RBFs estimators are incorporated within the AKF-SR framework to project continuous states into feature vectors such that the SR and the reward function can be modeled as linear functions of the feature vectors. Within the proposed AKF-SR framework, we adopt MMAE and gradient descent-based schemes for the reward function estimation (learning) via KF, which respectively compensate for the insufficient information about the measurement noise covariance and measurement mapping function of the KF as the most important parameters of a KF. The KTD framework is used within the AKF-SR framework, which casts the SR learning procedure into a filtering problem to estimate uncertainty of the learned SR. Moreover, by applying KTD, we benefit from decreases in memory and time spent for the SR learning and also sensitivity of the framework’s performance to its parameters (i.e., more reliable) when compared with DNN-based algorithms. An active learning scheme is exploited to balance the amount of exploration and exploitation based on uncertainty information of the value function obtained from KTD algorithm of the SR learning. The proposed active learning mechanism effectively improves performance of the proposed AKF-SR framework in terms of cumulative reward as shown via three RL platforms. The remainder of the paper is organized as follows: In Section 2, an overview of RL and SR is presented. The proposed AKF-SR framework is developed in Section 3. Section 4 presents experimental results obtained based on three benchmark RL platforms illustrating effectiveness of the proposed AKF-SR framework. Finally, Section 5 provides conclusion of the paper.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: Human and animals have the learning capabilities for evaluating consequences of their actions; therefore, can adapt their behavior based on the received reward after each action [ref]1  [ref]3.\nSuch an adaptive behavior can be achieved in the context of Artificial Intelligence (AI) via an optimal control policy, which can be obtained by Reinforcement Learning (RL)\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Universal Successor Representations for Transfer Reinforcement Learning\",\n                \"An Efficient Hardware Implementation of Reinforcement Learning: The Q-Learning Algorithm\",\n                \"Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards\",\n                \"MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning\",\n                \"Modeling behavior of Computer Generated Forces with Machine Learning Techniques, the NATO Task Group approach\",\n                \"Machine Learning Techniques for Autonomous Agents in Military Simulations - Multum in Parvo\",\n                \"Recovering Robustness in Model-Free Reinforcement Learning\",\n                \"Plume Tracing via Model-Free Reinforcement Learning Method\",\n                \"Kalman meets Bellman: Improving Policy Evaluation through Value Tracking\",\n                \"Information Theoretic MPC for Model-based Reinforcement Learning\",\n                \"Agnostic System Identification for Model-based Reinforcement Learning\",\n                \"Model-Based Reinforcement Learning with Value-Targeted Regression\",\n                \"A neurally plausible model learns successor representations in partially observable environments\",\n                \"A Complementary Learning Systems Approach to Temporal Difference Learning\",\n                \"Improving generalization for temporal difference learning: The successor representation\",\n                \"The Successor Representation as a model of behavioural flexibility\",\n                \"Deep Successor Reinforcement Learning\",\n                \"Neural Fitted Q Iteration-first Experiences with a Data Efficient Neural Reinforcement Learning Method\",\n                \"Flow Splitter: A Deep Reinforcement Learning-Based Flow Scheduler for Hybrid Optical-Electrical Data Center Network\",\n                \"Unexpected Collision Avoidance Driving Strategy Using Deep Reinforcement Learning\",\n                \"Deep reinforcement learning with optimized reward functions for robotic trajectory planning\",\n                \"An analysis of temporal-difference learning with function approximation\",\n                \"Improved temporal difference methods with linear function approximation\",\n                \"Cmas: An Associative Neural Network Alternative to Backpropagation\",\n                \"Neural Networks: A Comprehensive Foundation\",\n                \"Basis Function Adaptation in Temporal Difference Reinforcement Learning\",\n                \"Restricted Gradient-descent Algorithm for Value-function Approximation in Reinforcement Learning\",\n                \"Meta-cognitive neural network for classification problems in a sequential learning framework\",\n                \"Comparison of CMACs and radial basis functions for local function approximators in reinforcement learning\",\n                \"The successor representation and temporal context\",\n                \"The successor representation in human reinforcement learning\",\n                \"Predictive representations can link model-based reinforcement learning to model-free mechanisms\",\n                \"Reinforcement Learning: An Introduction\",\n                \"Kalman Temporal Differences\",\n                \"Successor features combine elements of model-free and model-based reinforcement learning\",\n                \"Count-Based Exploration with the Successor Representation\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Measuring the Reliability of Reinforcement Learning Algorithms\",\n                \"Probabilistic successor representations with Kalman temporal differences\",\n                \"Makf-Sr: Multi-Agent Adaptive Kalman Filtering-Based Successor Representations\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of AKF-SR: Adaptive Kalman Filtering-based Successor Representation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}