{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent\n\nPaper Abstract: \n\nPaper Section Text:\nZeroth-order optimization (ZOO) algorithms solve optimization problems with implicit function information by estimating gradients via zeroth-order observation (function evaluations) at judiciously chosen points . They have been extensively employed as model-free reinforcement learning (RL) algorithms for black-box off-line optimization , online optimization , neural networks training [ref]4, and model-free linear quadratic regulator (LQR) problems    . Although, ZOO algorithms are applicable to a broad range of problems, they suffer from high variance and slow convergence rate especially when the problem is of a large size. To improve the performance of ZOO algorithms, besides the classical one-point feedback  , other types of estimation schemes have been proposed, e.g., average of multiple one-point feedback , two-point feedback [ref]9  [ref]4 0 , average of multiple two-point feedback , and one-point residual feedback 1. These methods have been shown to reduce variance and increase convergence rate efficiently. However, all these methods are still based on the evaluation of the global optimization objective, which may cause significantly large errors and high variance of gradient estimates when encountering a large-size problem with a high dimensional variable. Multi-agent networks are one of the most representative systems that have broad applications and usually induce large-size optimization problems 2. In recent years, distributed zeroth-order convex and non-convex optimizations on multi-agent networks have been extensively studied, e.g., 3 4 5 6 7, all of which decompose the original cost function into multiple functions and assign them to the agents. Unfortunately, the variable dimension for each agent is the same as that for the original problem. As a result, the agents are expected to eventually reach consensus on the optimization variables. Such a setting indeed divides the global cost into local costs, but the high variance of the gradient estimate caused by the high variable dimension remains unchanged. In 8, each agent has a lower dimensional variable. However, to estimate the local gradient, the authors employed a consensus protocol for each agent to estimate the global cost function value. Such a framework maintains a similar gradient estimate to that of the centralized ZOO algorithm, which still suffers a high variance when the network is of a large scale. Block coordinate descent (BCD) are known to be efficient for large-size optimization problems due to the low per-iteration cost 9 0. The underlying idea is to solve optimizations by updating only partial components (one block) of the entire variable in each iteration, 1 9 2 3 0 4. Recently, zeroth-order BCD algorithms have been proposed to craft adversarial attacks for neural networks [ref]4 5, where the convergence analysis was given under the convexity assumption on the objective function in 5. A non-convex optimization was addressed by a zeroth-order BCD algorithm in 6, whereas the feasible domain was assumed to be convex. In this paper, we consider a general stochastic locally smooth non-convex optimization with a possibly non-convex feasible domain, and propose a distributed ZOO algorithm based on the BCD method. Out of the aforementioned situations, we design local cost functions involving only partial agents by utilizing the network structure embedded in the optimization objective over a multi-agent network. This is reasonable in model-free control problems because although the dynamics of each agent is unknown, the coupling structure between different agents may be known and the control objective with an inherent network structure is usually artificially designed. Our algorithm allows each agent to update its optimization variable by evaluating a local cost independently, without requiring agents to reach consensus on their optimization variables. This formulation is applicable to the distributed LQR control and many multi-agent cooperative control problems such as formation control 7, network localization 8 and cooperative transportation 9, where the variable of each agent converges to a desired local set corresponding to a part of the minimizer of the global cost function. Our main contributions are listed as follows. (i). We propose a distributed accelerated zeroth-order algorithm with asynchronous sample and update schemes (Algorithm 2). The multi-agent system is divided into multiple clusters, where agents in different clusters evaluate their local costs asynchronously, and the evaluations for different agents are completely independent. It is shown that with appropriate parameters, the algorithm converges to an approximate stationary point of the optimization with high probability. The sample complexity is polynomial with the reciprocal of the convergence error, and is dominated by the number of clusters (see Theorem 1). When compared to ZOO based on the global cost evaluation, our algorithm produces gradient estimates with lower variance and thus results in faster convergence (see the variance analysis in Subsection III-E). (ii). We further consider a model-free multi-agent distributed LQR problem, where multiple agents with decoupled dynamics111Our algorithm applies to multi-agent systems with coupled dynamics. Please refer to Remark 4 for more details. cooperatively minimize a cost function involving all the agents. The optimal LQR controller is desired to be distributed, i.e., the control of each agent involves only its neighbors. We describe the couplings in the cost function and the structural constraint by a cost graph and a sensing graph, respectively. To achieve distributed learning, we design a local cost function for each agent, and introduce a learning graph that describes the required agent-to-agent interaction relationship for local cost evaluations. Specifically, the local cost function for each agent is designed such that its gradient w.r.t. the local control gain is the same as that of the global cost function. The learning graph is determined by the cost and sensing graphs and is needed only during the learning stage. By implementing our asynchronous RL algorithm, the agents optimize their local costs and are able to learn a distributed controller corresponding to a stationary point of the optimization distributively. The design of local costs and the learning graph plays the key role in enabling distributed learning. The learning graph is typically denser than the cost and sensing graphs, which is a trade-off for not using a consensus algorithm. The comparisons between our work and existing related results are listed as follows. (i). In comparison to centralized ZOO  [ref]9  [ref]4 1 and distributed ZOO 3 4 5 6 7, we reduce the variable dimension for each agent, and construct local cost involving only partial agents. Such a framework avoids the influence of the convergence error and convergence rate of the consensus algorithm, and results in reduced variance and high scalability to large-scale networks. Moreover, since the agents do not need to reach agreement on any global index, our algorithm benefits for privacy preserving. (ii). In comparison to ZOO algorithms for LQR, e.g.,   , our problem has a structural constraint on the desired control gain, thus the cost function is not gradient dominated. (iii) Existing BCD algorithms in 1 9 2 3 0 4 always require global smoothness of the objective function. When only zeroth-order information is available, existing BCD algorithms require convexity of either the objective function 5 or the feasible domain 6. In contrast, we propose a zeroth-order BCD algorithm for a stochastic non-convex optimization with a locally smooth objective and a possibly non-convex feasible set. A clustering strategy and a cluster-wise update scheme are proposed as well, which further significantly improve the algorithm convergence rate. (iv) The distributed LQR problem is complicated and challenging because both the objective function and the feasible set are non-convex, and the number of locally optimal solutions grow exponentially in system dimension 0 1. Existing results for distributed LQR include model-based centralized approaches 2 3, model-free centralized approaches  4, and model-free distributed approaches 8 5. All of them focus on finding a sub-optimal distributed controller for an approximate problem or converging to a stationary point of the optimization via policy gradient. Therefore, the obtained solutions may be a sub-optimal solution to the distributed LQR problem. Our algorithm is a derivative-free distributed policy gradient algorithm, which also seeks a stationary point of the problem and yields a sub-optimal solution.  (v). The sample complexity of our algorithm is higher than that in    because their optimization problems have the gradient domination property, and   assume that the infinite horizon LQR cost can be obtained perfectly. The sample complexity of our algorithm for LQR is slightly lower than that in 8. However, two consensus algorithms are employed in 8 for distributed sampling and global cost estimation, respectively, which may slow down the gradient estimation process especially for large-scale problems. Moreover, the gradient estimation in 8 is essentially based on global cost evaluation, while we adopt local cost evaluation, which improves the scalability to large-scale networks significantly. (vi). Distributed RL has also been extensively studied via a Markov decision process (MDP) formulation, e.g., 6 7 8 9 . However, in comparison to our work, they require more information for each agent or are essentially based on global cost evaluation. More specifically, in 6 8 9 , the global state is assumed to be available for all the agents. It has been shown in 8 that naively applying policy gradient methods to multi-agent settings exhibits high variance gradient estimates. 7 describes a distributed learning framework with partial observations, but only from an empirical perspective. This paper is structured as follows. Section II describes the optimization problem. Section III presents our zeroth-order BCD algorithm with convergence analysis. Section IV introduces the application of our algorithm to the model-free multi-agent LQR problem. Section V shows asimulation example. Section VI concludes the whole paper. Notations. Given function f​(x)𝑓𝑥f(x) with x=(x1⊤,…,xN⊤)⊤𝑥superscriptsuperscriptsubscript𝑥1top…superscriptsubscript𝑥𝑁toptopx=(x_{1}^{\\top},...,x_{N}^{\\top})^{\\top}, denote f​(yi,x−i)=f​(x′)𝑓subscript𝑦𝑖subscript𝑥𝑖𝑓superscript𝑥′f(y_{i},x_{-i})=f(x^{\\prime}) with x′=(x′1⊤,…,x′N⊤)⊤superscript𝑥′superscriptsuperscriptsubscriptsuperscript𝑥′1top…superscriptsubscriptsuperscript𝑥′𝑁toptopx^{\\prime}=({x^{\\prime}}_{1}^{\\top},...,{x^{\\prime}}_{N}^{\\top})^{\\top}, xi′=yisubscriptsuperscript𝑥′𝑖subscript𝑦𝑖x^{\\prime}_{i}=y_{i} and xj′=xjsubscriptsuperscript𝑥′𝑗subscript𝑥𝑗x^{\\prime}_{j}=x_{j} for all j≠i𝑗𝑖j\\neq i. Let ℝnsuperscriptℝ𝑛\\mathbb{R}^{n} and ℝ≥0subscriptℝabsent0\\mathbb{R}_{\\geq 0} denote the n−limit-from𝑛n-dimensional Euclidean space and the space of nonnegative real numbers. The norm ∥⋅∥\\|\\cdot\\| denotes the l𝑙l2-norm for vectors, and denotes the spectral norm for matrices, ∥⋅∥F\\|\\cdot\\|_{F} is the Frobenius norm. The pair 𝒢=(𝒱,ℰ)𝒢𝒱ℰ\\mathcal{G}=(\\mathcal{V},\\mathcal{E}) denotes a directed or an undirected graph, where 𝒱𝒱\\mathcal{V} is the set of vertices, ℰ⊂𝒱2ℰsuperscript𝒱2\\mathcal{E}\\subset\\mathcal{V}^{2} is the set of edges. A pair (i,j)∈ℰ𝑖𝑗ℰ(i,j)\\in\\mathcal{E} implies that there exists an edge from i𝑖i to j𝑗j. A path from vertex i𝑖i to vertex j𝑗j in 𝒢𝒢\\mathcal{G} is a sequence of pairs (i,i1)𝑖subscript𝑖1(i,i_{1}), (i1,i2)subscript𝑖1subscript𝑖2(i_{1},i_{2}), …, (is,j)∈ℰsubscript𝑖𝑠𝑗ℰ(i_{s},j)\\in\\mathcal{E}. The symbol 𝔼​[⋅]𝔼delimited-[]⋅\\mathbb{E}[\\cdot] denotes expectation, Cov​(x)Cov𝑥\\mathrm{Cov}(x) denotes the covariance matrix of vector x𝑥x. Given a set S𝑆S, Uni​(S)Uni𝑆\\text{Uni}(S) represents the uniform distribution in S𝑆S. Given two matrices X𝑋X and Y𝑌Y, let ⟨X,Y⟩=trace​(X⊤​Y)𝑋𝑌tracesuperscript𝑋top𝑌\\langle X,Y\\rangle=\\text{trace}(X^{\\top}Y) be the Frobenius inner product.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: chosen points . They have been extensively employed as model-free reinforcement learning (RL) algorithms for black-box off-line optimization , online optimization , neural networks training [ref]4, and model-free linear quadratic regulator (LQR) problems    . Although, ZOO algorithms are applicable to a broad range of problems, they suffer from high variance and slow convergence rate\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator\",\n                \"Introduction to stochastic search and optimization: estimation, simulation, and control\",\n                \"Random gradient-free minimization of convex functions\",\n                \"Online convex optimization in the bandit setting: gradient descent without a gradient\",\n                \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models\",\n                \"Derivative-free methods for policy optimization: Guarantees for linear quadratic systems\",\n                \"Learning the globally optimal distributed lq regulator\",\n                \"On the linear convergence of random search for discrete-time lqr\",\n                \"Optimal algorithms for online convex optimization with multi-point bandit feedback.\",\n                \"An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback\",\n                \"A New One-Point Residual-Feedback Oracle For Black-Box Learning and Control\",\n                \"Survey of recent multi-agent reinforcement learning algorithms utilizing centralized training\",\n                \"Distributed non-convex first-order optimization and information processing: Lower complexity bounds and rate optimal algorithms\",\n                \"Zone: Zeroth-order nonconvex multiagent optimization over networks\",\n                \"Privacy-preserving distributed zeroth-order optimization\",\n                \"Distributed zero-order algorithms for nonconvex multi-agent optimization\",\n                \"Distributed zero-order optimization under adversarial noise\",\n                \"Distributed Reinforcement Learning for Decentralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach\",\n                \"Efficiency of coordinate descent methods on huge-scale optimization problems\",\n                \"Coordinate friendly structures, algorithms and applications\",\n                \"Cyclic coordinate descent: A robotics algorithm for protein loop closure\",\n                \"Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function\",\n                \"Coordinate descent algorithms\",\n                \"A globally convergent algorithm for nonconvex optimization based on block coordinate update\",\n                \"A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization\",\n                \"Zeroth-order stochastic block coordinate type methods for nonconvex optimization\",\n                \"A survey of multi-agent formation control\",\n                \"Angle-Displacement Rigidity Theory with Application to Distributed Network Localization\",\n                \"Distributed decision making and control for cooperative transportation using mobile robots\",\n                \"On the exponential number of connected components for the feasible set of optimal decentralized control problems\",\n                \"On topological and metrical properties of stabilizing feedback gains: the mimo case\",\n                \"Distributed lqr design for identical dynamically decoupled systems\",\n                \"LQR through the Lens of First Order Methods: Discrete-time Case\",\n                \"Learning Distributed Stabilizing Controllers for Multi-Agent Systems\",\n                \"Model-Free Optimal Control of Linear Multi-Agent Systems via Decomposition and Hierarchical Approximation\",\n                \"𝒬​𝒟𝒬𝒟{{\\\\cal Q}{\\\\cal D}}-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus+innovationsconsensusinnovations{\\\\rm consensus}+{\\\\rm innovations}\",\n                \"Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability\",\n                \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\n                \"Fully decentralized multi-agent reinforcement learning with networked agents\",\n                \"Communication-Efficient Policy Gradient Methods for Distributed Reinforcement Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}